ction) {
        strcpy( ConnectionInformation, "Server Accepting Connection" );
        }
    else {
        strcpy( ConnectionInformation, "Server Rejecting Connection" );
        }

    Msg->h.u1.s1.DataLength = strlen( ConnectionInformation ) + 1;
    Msg->h.u1.s1.TotalLength = Msg->h.u1.s1.DataLength + sizeof( Msg->h );
    Status = NtAcceptConnectPort( &ServerClientPortHandles[ CountServerClientPortHandles ],
                                  (PVOID)(CountServerClientPortHandles+1),
                                  &Msg->h,
                                  AcceptConnection,
                                  &ServerView,
                                  &ClientView
                                );

    if (ShowHandleOrStatus( Status, ServerClientPortHandles[ CountServerClientPortHandles ] )) {
        fprintf( stderr, "    ServerView: Base=%lx, Size=%lx, RemoteBase: %lx\n",
                  ServerView.ViewBase,
                  ServerView.ViewSize,
                  ServerView.ViewRemoteBase
                );
        fprintf( stderr, "    ClientView: Base=%lx, Size=%lx\n",
                  ClientView.ViewBase,
                  ClientView.ViewSize
                );

        ClientMemoryBase = ServerView.ViewBase;
        ClientMemorySize = ServerView.ViewSize;
        ServerMemoryBase = ServerView.ViewRemoteBase;
        ServerMemoryDelta = (ULONG)ServerMemoryBase -
                            (ULONG)ClientMemoryBase;
        p = (PULONG)(ClientView.ViewBase);
        i =ClientView.ViewSize;
        while (i) {
            *p = (ULONG)p;
            fprintf( stderr, "Server setting ClientView[ %lx ] = %lx\n",
                      p,
                      *p
                    );
            p += (0x1000/sizeof(ULONG));
            i -= 0x1000;
            }

        p = (PULONG)(ServerView.ViewBase);
        i = ServerView.ViewSize;
        while (i) {
            *p = (ULONG)p - ServerMemoryDelta;
            fprintf( stderr, "Server setting ServerView[ %lx ] = %lx\n",
                      p,
                      *p
                    );
            p += (0x1000/sizeof(ULONG));
            i -= 0x1000;
            }
        Status = NtCompleteConnectPort( ServerClientPortHandles[ CountServerClientPortHandles ] );
        CountServerClientPortHandles++;
        }

    return;
}

DWORD
ServerThread(
    LPVOID Context
    )
{
    NTSTATUS Status;
    CHAR ThreadName[ 64 ];
    TLPC_PORTMSG Msg;
    PTLPC_PORTMSG ReplyMsg;
    HANDLE ReplyPortHandle;
    ULONG PortContext;
    PTEB Teb = NtCurrentTeb();

    Teb->ActiveRpcHandle = NULL;

    strcpy( ThreadName, "Server Thread Id: " );
    RtlIntegerToChar( (ULONG)Teb->ClientId.UniqueProcess, 16, 9,
                    ThreadName + strlen( ThreadName )
                  );
    strcat( ThreadName, "." );
    RtlIntegerToChar( (ULONG)Teb->ClientId.UniqueThread, 16, 9,
                    ThreadName + strlen( ThreadName )
                  );

    EnterThread( ThreadName, (ULONG)Context );

    ReplyMsg = NULL;
    ReplyPortHandle = ServerConnectionPortHandle;
    while (TRUE) {
        fprintf( stderr, "%s waiting for message...\n", ThreadName );
        Status = NtReplyWaitReceivePort( ReplyPortHandle,
                                         (PVOID)&PortContext,
                                         (PPORT_MESSAGE)ReplyMsg,
                                         (PPORT_MESSAGE)&Msg
                                       );

        ReplyMsg = NULL;
        ReplyPortHandle = ServerConnectionPortHandle;
        fprintf( stderr, "%s Receive (%s)  Id: %u", ThreadName, LpcMsgTypes[ Msg.h.u2.s2.Type ], Msg.h.MessageId );
        PortContext -= 1;
        if (!NT_SUCCESS( Status )) {
            fprintf( stderr, " (Status == %08x)\n", Status );
            }
        else
        if (Msg.h.u2.s2.Type == LPC_CONNECTION_REQUEST) {
            ServerHandleConnectionRequest( &Msg );
            continue;
            }
        else
        if (PortContext >= CountServerClientPortHandles) {
            fprintf( stderr, "*** Invalid PortContext (%lx) received\n",
                     PortContext
                   );
            }
        else
        if (Msg.h.u2.s2.Type == LPC_PORT_CLOSED ||
            Msg.h.u2.s2.Type == LPC_CLIENT_DIED
           ) {
            fprintf( stderr, " - disconnect for client %08x\n", PortContext );
            CloseHandle( ServerClientPortHandles[ (ULONG)PortContext ] );
            CountClosedServerClientPortHandles += 1;
            if (CountClosedServerClientPortHandles == CountServerClientPortHandles) {
                break;
                }
            }
        else
        if (Msg.h.u2.s2.Type == LPC_REQUEST) {
            CheckTlpcMsg( Status, &Msg );
            ReplyMsg = &Msg;
            ReplyPortHandle = ServerClientPortHandles[ PortContext ];
            if (TestCallBacks && (Msg.h.u1.s1.DataLength > 30)) {
                Status = SendRequest( 1,
                                      ThreadName,
                                      ReplyPortHandle,
                                      Context,
                                      Msg.h.u1.s1.DataLength >> 1,
                                      ReplyMsg,
                                      TRUE
                                    );
                }
            }
        }

    fprintf( stderr, "Exiting %s\n", ThreadName );

    return RtlNtStatusToDosError( Status );
}



VOID
Usage( VOID )
{
    fprintf( stderr, "usage: USERVER #threads\n" );
    ExitProcess( 1 );
}


int
_cdecl
main(
    int argc,
    char *argv[]
    )
{
    NTSTATUS Status;
    DWORD rc;
    ULONG i, NumberOfThreads;
    OBJECT_ATTRIBUTES ObjectAttributes;

    Status = STATUS_SUCCESS;

    fprintf( stderr, "Entering USERVER User Mode LPC Test Program\n" );

    TestCallBacks = FALSE;
    if (argc < 2) {
        NumberOfThreads = 1;
        }
    else {
        NumberOfThreads = atoi( argv[ 1 ] );
        if (NumberOfThreads >= MAX_REQUEST_THREADS) {
            Usage();
            }

        if (argc > 2) {
            TestCallBacks = TRUE;
            }
        }

    RtlInitUnicodeString( &PortName, PORT_NAME );
    fprintf( stderr, "Creating %wZ connection port", (PUNICODE_STRING)&PortName );
    InitializeObjectAttributes( &ObjectAttributes, &PortName, 0, NULL, NULL );
    Status = NtCreatePort( &ServerConnectionPortHandle,
                           &ObjectAttributes,
                           40,
                           sizeof( TLPC_PORTMSG ),
                           sizeof( TLPC_PORTMSG ) * 32
                         );
    ShowHandleOrStatus( Status, ServerConnectionPortHandle );
    rc = RtlNtStatusToDosError( Status );
    if (rc == NO_ERROR) {
        ServerThreadHandles[ 0 ] = GetCurrentThread();
        ServerThreadClientIds[ 0 ] = GetCurrentThreadId();
        for (i=1; i<NumberOfThreads; i++) {
            fprintf( stderr, "Creating Server Request Thread %ld\n", i+1 );
            rc = NO_ERROR;
            ServerThreadHandles[ i ] = CreateThread( NULL,
                                                     0,
                                                     (LPTHREAD_START_ROUTINE)ServerThread,
                                                     (LPVOID)(i+1),
                                                     CREATE_SUSPENDED,
                                                     &ServerThreadClientIds[ i ]
                                                   );
            if (ServerThreadHandles[ i ] == NULL) {
                rc = GetLastError();
                break;
                }
            }

        if (rc == NO_ERROR) {
            for (i=1; i<NumberOfThreads; i++) {
                ResumeThread( ServerThreadHandles[ i ] );
                }

            ServerThread( 0 );
            }
        }

    if (rc != NO_ERROR) {
        fprintf( stderr, "USERVER: Initialization Failed - %u\n", rc );
        }

    ExitProcess( rc );
    return( rc );
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\alpha\lpcmove.s ===
//      TITLE("LPC Move Message Support")
//++
//
// Copyright (c) 1990  Microsoft Corporation
// Copyright (c) 1992  Digital Equipment Corporation
//
// Module Name:
//
//    lpcmove.s
//
// Abstract:
//
//    This module implements functions to support the efficient movement of
//    LPC Message blocks.
//
// Author:
//
//    David N. Cutler (davec) 11-Apr-1990
//
// Environment:
//
//    Kernel mode only.
//
// Revision History:
//
//    Thomas Van Baak (tvb) 19-May-1992
//
//        Adapted for Alpha AXP.
//
//--

#include "ksalpha.h"

        SBTTL("Move Message")
//++
//
// VOID
// LpcpMoveMessage (
//    OUT PPORT_MESSAGE DstMsg
//    IN PPORT_MESSAGE SrcMsg
//    IN PUCHAR SrcMsgData
//    IN ULONG MsgType OPTIONAL,
//    IN PCLIENT_ID ClientId OPTIONAL
//    )
//
// Routine Description:
//
//    This function moves an LPC message block and optionally sets the message
//    type and client id to the specified values.
//
// Arguments:
//
//    DstMsg (a0) - Supplies a pointer to the destination message.
//
//    SrcMsg (a1) - Supplies a pointer to the source message.
//
//    SrcMsgData (a2) - Supplies a pointer to the source message data to
//       copy to destination.
//
//    MsgType (a3) - If non-zero, then store in type field of the destination
//       message.
//
//    ClientId (a4) - If non-NULL, then points to a ClientId to copy to
//       the destination message.
//
//    N.B. The messages are assumed to be quadword aligned.
//
// Return Value:
//
//    None.
//
//--

        LEAF_ENTRY(LpcpMoveMessage)

        ldq     t0, PmLength(a1)        // load first quadword of source

//
// The message length is in the low word of the first quadword.
//

        addq    t0, 3, t1               // round length to
        bic     t1, 3, t0               // nearest 4-byte multiple

//
// The message type is in the low half of the high longword. If a message
// type was specified, use it instead of the message type in the source
// message.
//

        sll     a3, 32, t2              // shift message type into position
        zap     t0, 0x30, t3            // clear message type field
        or      t3, t2, t1              // merge into new field
        cmovne  a3, t1, t0              // if a3!=0 use new message field
        stq     t0, PmLength(a0)        // store first quadword of destination

//
// The client id is the third and fourth items. If a client id was
// specified, use it instead of the client id in the source message.
//

        lda     t3, PmClientId(a1)      // get address of source client id
        cmovne  a4, a4, t3              // if a4!=0, use client id address in a4

//
//	Move the process and thread ids into place.  Note that for axp32, (t3) isn't
// necessarily quadword aligned.
// 

        LDP     t2, CidUniqueProcess(t3)// load low part of client id
        LDP     t1, CidUniqueThread(t3) // load high part of client id
        STP     t2, PmProcess(a0)       // store third longword of destination
        STP     t1, PmThread(a0)        // store fourth longword of destination
		  
#if defined(_AXP64_)

//
// Copy MessageId and ClientViewSize.  
//

        ldl     t2,PmMessageId(a1)
		  stl	    t2,PmMessageId(a0)
		  ldq     t2,PmClientViewSize(a1)
		  stq     t2,PmClientViewSize(a0)

#else

//
// PmClientViewSize is adjacent to PmMessageId, both can be moved at once
// with a single quadword move.
//

        ldq     t2,PmMessageId(a1)      // get next quadword of source
        stq     t2,PmMessageId(a0)      // set next quadword of destination

#endif

        and     t0, 0xfff8, t3          // isolate quadword move count
        beq     t3,20f                  // if eq, no quadwords to move
        and     a2, 7, t1               // check if source is quadword aligned
        bne     t1, UnalignedSource     // if ne, not quadword aligned

//
// Source and destination are both quadword aligned, use ldq/stq.  Use of
// the constant "PortMessageLength-8" is used in lieu of an additional
// instruction to increment a0 by that amount before entering the copy
// loop.
//

5:      ldq     t1, 0(a2)               // get source qword
        ADDP    a0, 8, a0               // advance destination address
        ADDP    a2, 8, a2               // advance source address
        subq    t3, 8, t3               // decrement number of bytes remaining
        stq     t1, PortMessageLength-8(a0) // store destination qword
        bne     t3, 5b                  // if ne, more quadwords to store
        br      zero, 20f               // move remaining longword

//
// We know that the destination is quadword aligned, but the source is
// not.  Use ldq_u to load the low and high parts of the source quadword,
// merge them with EXTQx and store them as one quadword.
//
// By reusing the result of the second ldq_u as the source for the
// next quadword's EXTQL we end up doing one ldq_u/stq for each quadword,
// regardless of the source's alignment.
//

UnalignedSource:                        //
        ldq_u   t1, 0(a2)               // prime t1 with low half of qword
10:     extql   t1, a2, t2              // t2 is aligned low part
        ADDP    a0, 8, a0               // advance destination address
        subq    t3, 8, t3               // reduce number of bytes remaining
        ldq_u   t1, 7(a2)               // t1 has high part
        extqh   t1, a2, t4              // t4 is aligned high part
        ADDP    a2, 8, a2               // advance source address
        bis     t2, t4, t5              // merge high and low parts
        stq     t5, PortMessageLength-8(a0) // store result
        bne     t3, 10b                 // if ne, more quadwords to move

//
// Move remaining longword (if any)
//

20:     and     t0, 4, t0               // check if longword to move
        beq     t0, 50f                 // if eq, no longword to move
        ldl     t1, 0(a2)               // move last longword to move
        stl     t1, PortMessageLength(a0) //
50:     ret     zero, (ra)              // return

        .end    LpcpMoveMessage
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\up\makefile.inc ===
..\userver.c: ..\ulpc.h

..\uclient.c: ..\ulpc.h

..\lpcclose.c: ..\lpcp.h

..\lpcpriv.c: ..\lpcp.h

..\lpcsend.c: ..\lpcp.h

..\lpcrecv.c: ..\lpcp.h

..\lpcreply.c: ..\lpcp.h

..\lpcqueue.c: ..\lpcp.h
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\amd64\lpcmove.asm ===
title  "LPC Move Message Support"
;++
;
; Copyright (c) 2000  Microsoft Corporation
;
; Module Name:
;
;   lpcmove.asm
;
; Abstract:
;
;   This module implements functions to support the efficient movement of
;   LPC Message blocks
;
; Author:
;
;   David N. Cutler (davec) 25-Jun-2000
;
; Environment:
;
;    Kernel mode only.
;
;--

include ksamd64.inc

        subttl  "Move Message"
;++
;
; VOID
; LpcpMoveMessage (
;     OUT PPORT_MESSAGE DstMsg,
;     IN PPORT_MESSAGE SrcMsg,
;     IN PUCHAR SrcMsgData,
;     IN ULONG MsgType OPTIONAL,
;     IN PCLIENT_ID ClientId OPTIONAL
;     )
;
; Routine Description:
;
;   This function moves an LPC message block and optionally sets the message
;   type and client id to the specified values.
;
; Arguments:
;
;   DstMsg (rcx) - Supplies pointer to the destination message.
;
;   SrcMsg (rdx) - Supplies a pointer to the source message.
;
;   SrcMsgData (r8) - Supplies a pointer to the source message data to copy
;       to destination.
;
;   MsgType (r9) - If nonzero, then store in type field on the destination
;       message.
;
;   ClientId (40[rsp]) - If nonNULL, then supplies a pointer to a client id
;       to copy to the desination message.
;
; Return Value:
;
;   None.
;
;--

MvFrame struct
        SavedRdi dq ?                   ; saved register RDI
        SavedRsi dq ?                   ; saved register RSI
        Fill    dq ?                    ; fill to 8 mod 16
MvFrame ends

ClientId equ ((sizeof MvFrame) + (5 * 8)) ; offset to client id parameter

        NESTED_ENTRY LpcpMoveMessage, _PAGE

        push_reg rsi                    ; save nonvolatile registers
        push_reg rdi                    ;
        alloc_stack (sizeof MvFrame - (2 * 8)) ; allocate stack frame

        END_PROLOGUE

        mov     rdi, rcx                ; set destination address
        mov     rsi, rdx                ; set source address

;
; Copy the message length.
;

        mov     eax, PmLength[rsi]      ; copy message length
        mov     PmLength[rdi], eax      ;

;
; Round the message length up and compute message length in dwords.
;

        lea     rcx, 3[rax]             ; round length to dwords
        and     rcx, 0fffch             ;
        shr     rcx, 2                  ;

;
; Copy data offset and message id. If the specified message id is nonzero,
; then insert the specified message id.
;

        mov     eax, PmZeroInit[rsi]    ; get data offset and message type
        test    r9w, r9w                ; test if message type specified
        cmovnz  ax, r9w                 ; if nz, set specified message id
        mov     PmZeroInit[rdi], eax    ; set offset and message type

;
; Copy the client id. If the specified client id pointer is nonNULL, then
; insert the specified client id. Otherwise, copy the client id from the
; source message.
;

        lea     rax, PmClientId[rsi]    ; get source client id address
        mov     rdx, ClientId[rsp]      ; get specified client id address
        test    rdx, rdx                ; check if client id specified
        cmovz   rdx, rax                ; if z, set source client id address
        mov     rax, CidUniqueProcess[rdx] ; copy low part of client id
        mov     PmProcess[rdi], rax     ;
        mov     rax, CidUniqueThread[rdx] ; copy high part of client id
        mov     PmThread[rdi], rax      ;

;
; Copy the message id and the client view size.
;

        mov     eax, PmMessageId[rsi]   ; copy message id
        mov     PmMessageId[rdi], eax   ;
        mov     rax, PmClientViewSize[rsi] ; copy view size
        mov     PmClientViewSize[rdi], rax ;

;
; Copy the message data.
;

        mov     rsi, r8                 ; set address of source
        lea     rdi, PortMessageLength[rdi] ; set address of destination
        rep     movsd                   ; Copy the data portion of message
        add     rsp, sizeof MvFrame - (2 * 8) ; deallocate stack frame
        pop     rdi                     ; restore nonvolatile register
        pop     rsi                     ;
        ret                             ; return

        NESTED_END LpcpMoveMessage, _PAGE

        end
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\ia64\lpcmove.c ===
/**
***  Copyright  (C) 1996-97 Intel Corporation. All rights reserved.
***
*** The information and source code contained herein is the exclusive
*** property of Intel Corporation and may not be disclosed, examined
*** or reproduced in whole or in part without explicit written authorization
*** from the company.
**/

/*++

Copyright (c) 1995 Intel Corporation

Module Name:

    lpcmove.c

Abstract:

    This module implements functions to support the efficient movement
    of LPC message blocks.

    There is a corresponding .s version that is hand optimized.
    Need to evaluate and install one or the other.

Author:

    Roy D'Souza (rdsouza) 5-May-96

Revision History:

--*/

#include "lpcp.h"


VOID
LpcpMoveMessage (
    OUT PPORT_MESSAGE DstMsg,
    IN PPORT_MESSAGE SrcMsg,
    IN PUCHAR SrcMsgData,
    IN ULONG MsgType OPTIONAL,
    IN PCLIENT_ID ClientId OPTIONAL
    )

/*++

Routine Description:

    This function moves an LPC message block and optionally sets the message
    type and client id to the specified values.

Arguments:

    DstMsg     - Supplies a pointer to the destination message.

    SrcMsg     - Supplies a pointer to the source message.

    SrcMsgData - Supplies a pointer to the source message data to
                 copy to destination.

    MsgType    - If non-zero, then store in type field of the
                 destination message.

    ClientId   - If non-NULL, then points to a ClientId to copy to
                 the destination message.

Return Value:

    None

--*/

{
    ULONG NumberDwords;

    //
    // Extract the data length and copy over the first dword
    //

    *((PULONG)DstMsg)++ = NumberDwords = *((PULONG)SrcMsg)++;
    NumberDwords = ((0x0000FFFF & NumberDwords) + 3) >> 2;

    //
    // Set the message type properly and update the second dword
    //

    *((PULONG)DstMsg)++ = MsgType == 0 ? *((PULONG)SrcMsg)++ :
                         *((PULONG)SrcMsg)++ & 0xFFFF0000 | MsgType & 0xFFFF;

    //
    // Set the ClientId appropriately and update the third dword
    //

    *((PULONG_PTR)DstMsg)++ = ClientId == NULL ? *((PULONG_PTR)SrcMsg) :
            *((PULONG_PTR)ClientId)++;
    ((PULONG_PTR)SrcMsg)++;

    *((PULONG_PTR)DstMsg)++ = ClientId == NULL ? *((PULONG_PTR)SrcMsg) :
            *((PULONG_PTR)ClientId);
    ((PULONG_PTR)SrcMsg)++;

    //
    // Update the final two longwords in the header
    //

    *((PULONG_PTR)DstMsg)++ = *((PULONG_PTR)SrcMsg)++;
    *((PULONG_PTR)DstMsg)++ = *((PULONG_PTR)SrcMsg)++;

    //
    // Copy the data
    //

    if (NumberDwords > 0) {

        RtlCopyMemory(DstMsg, SrcMsgData, NumberDwords*sizeof(ULONG));
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\mp\makefile.inc ===
!INCLUDE ..\up\makefile.inc
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\acceschk.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   acceschk.c

Abstract:

    This module contains the access check routines for memory management.

Author:

    Lou Perazzoli (loup) 10-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if defined(_WIN64)
#include "wow64t.h"

#pragma alloc_text(PAGE, MiCheckForUserStackOverflow)

#if PAGE_SIZE != PAGE_SIZE_X86NT
#define EMULATE_USERMODE_STACK_4K       1
#endif
#endif

//
// MmReadWrite yields 0 if no-access, 10 if read-only, 11 if read-write.
// It is indexed by a page protection.  The value of this array is added
// to the !WriteOperation value.  If the value is 10 or less an access
// violation is issued (read-only - write_operation) = 9,
// (read_only - read_operation) = 10, etc.
//

CCHAR MmReadWrite[32] = {1, 10, 10, 10, 11, 11, 11, 11,
                         1, 10, 10, 10, 11, 11, 11, 11,
                         1, 10, 10, 10, 11, 11, 11, 11,
                         1, 10, 10, 10, 11, 11, 11, 11 };


NTSTATUS
MiAccessCheck (
    IN PMMPTE PointerPte,
    IN ULONG_PTR WriteOperation,
    IN KPROCESSOR_MODE PreviousMode,
    IN ULONG Protection,
    IN BOOLEAN CallerHoldsPfnLock
    )

/*++

Routine Description:



Arguments:

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    WriteOperation - Supplies nonzero if the operation is a write, 0 if
                     the operation is a read.

    PreviousMode - Supplies the previous mode, one of UserMode or KernelMode.

    Protection - Supplies the protection mask to check.

    CallerHoldsPfnLock - Supplies TRUE if the PFN lock is held, FALSE otherwise.

Return Value:

    Returns TRUE if access to the page is allowed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    MMPTE PteContents;
    KIRQL OldIrql;
    PMMPFN Pfn1;

    //
    // Check to see if the owner bit allows access to the previous mode.
    // Access is not allowed if the owner is kernel and the previous
    // mode is user.  Access is also disallowed if the write operation
    // is true and the write field in the PTE is false.
    //

    //
    // If both an access violation and a guard page violation could
    // occur for the page, the access violation must be returned.
    //

    if (PreviousMode == UserMode) {
        if (PointerPte > MiHighestUserPte) {
            return STATUS_ACCESS_VIOLATION;
        }
    }

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 1) {

        //
        // Valid pages cannot be guard page violations.
        //

        if (WriteOperation != 0) {
            if ((PteContents.u.Hard.Write == 1) ||
                (PteContents.u.Hard.CopyOnWrite == 1)) {
                return STATUS_SUCCESS;
            }
            return STATUS_ACCESS_VIOLATION;
        }

        return STATUS_SUCCESS;
    }

    if (WriteOperation != 0) {
        WriteOperation = 1;
    }

    if ((MmReadWrite[Protection] - (CCHAR)WriteOperation) < 10) {
        return STATUS_ACCESS_VIOLATION;
    }

    //
    // Check for a guard page fault.
    //

    if (Protection & MM_GUARD_PAGE) {

        //
        // If this thread is attached to a different process,
        // return an access violation rather than a guard
        // page exception.  The prevents problems with unwanted
        // stack expansion and unexpected guard page behavior
        // from debuggers.
        //

        if (KeIsAttachedProcess()) {
            return STATUS_ACCESS_VIOLATION;
        }

        //
        // Check to see if this is a transition PTE. If so, the
        // PFN database original contents field needs to be updated.
        //

        if ((PteContents.u.Soft.Transition == 1) &&
            (PteContents.u.Soft.Prototype == 0)) {

            //
            // Acquire the PFN lock and check to see if the
            // PTE is still in the transition state. If so,
            // update the original PTE in the PFN database.
            //

            //
            // Initializing OldIrql is not needed for correctness but
            // without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            OldIrql = PASSIVE_LEVEL;

            if (CallerHoldsPfnLock == FALSE) {
                LOCK_PFN (OldIrql);
            }

            PteContents = *(volatile MMPTE *)PointerPte;
            if ((PteContents.u.Soft.Transition == 1) &&
                (PteContents.u.Soft.Prototype == 0)) {

                //
                // Still in transition, update the PFN database.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                //
                // Note that forked processes using guard pages only take the
                // guard page fault when the first thread in either process
                // access the address.  This seems to be the best behavior we
                // can provide users of this API as we must allow the first
                // thread to make forward progress and the guard attribute is
                // stored in the shared fork prototype PTE.
                //

                if (PteContents.u.Soft.Protection == MM_NOACCESS) {
                    ASSERT ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MiLocateCloneAddress (PsGetCurrentProcess (), Pfn1->PteAddress) != NULL));
                    if (CallerHoldsPfnLock == FALSE) {
                        UNLOCK_PFN (OldIrql);
                    }
                    return STATUS_ACCESS_VIOLATION;
                }

                ASSERT ((Pfn1->u3.e1.PrototypePte == 0) ||
                        (MiLocateCloneAddress (PsGetCurrentProcess (), Pfn1->PteAddress) != NULL));
                Pfn1->OriginalPte.u.Soft.Protection =
                                      Protection & ~MM_GUARD_PAGE;
            }
            if (CallerHoldsPfnLock == FALSE) {
                UNLOCK_PFN (OldIrql);
            }
        }

        PointerPte->u.Soft.Protection = Protection & ~MM_GUARD_PAGE;

        return STATUS_GUARD_PAGE_VIOLATION;
    }

    return STATUS_SUCCESS;
}

NTSTATUS
FASTCALL
MiCheckForUserStackOverflow (
    IN PVOID FaultingAddress
    )

/*++

Routine Description:

    This routine checks to see if the faulting address is within
    the stack limits and if so tries to create another guard
    page on the stack.  A stack overflow is returned if the
    creation of a new guard page fails or if the stack is in
    the following form:


    stack   +----------------+
    growth  |                |  StackBase
      |     +----------------+
      v     |                |
            |   allocated    |
            |                |
            |    ...         |
            |                |
            +----------------+
            | old guard page | <- faulting address is in this page.
            +----------------+
            |                |
            +----------------+
            |                | last page of stack (always no access)
            +----------------+

    In this case, the page before the last page is committed, but
    not as a guard page and a STACK_OVERFLOW condition is returned.

Arguments:

    FaultingAddress - Supplies the virtual address of the page which
                      was a guard page.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode. No mutexes held.

--*/

{
    PTEB Teb;
    PPEB Peb;
    ULONG_PTR NextPage;
    SIZE_T RegionSize;
    NTSTATUS status;
    PVOID DeallocationStack;
    PVOID *StackLimit;
    PETHREAD Thread;
    ULONG_PTR PageSize;
    PEPROCESS Process;
    ULONG OldProtection;
    ULONG ExecuteFlags;
    ULONG ProtectionFlags;
    LOGICAL RevertExecuteFlag;
    ULONG StackProtection;
#if defined(_WIN64)
    PTEB32 Teb32;

    Teb32 = NULL;
#endif

    //
    // Make sure we are not recursing with the address space mutex held.
    //

    Thread = PsGetCurrentThread ();

    if (Thread->AddressSpaceOwner == 1) {
        ASSERT (KeGetCurrentIrql () == APC_LEVEL);
        return STATUS_GUARD_PAGE_VIOLATION;
    }

    Process = NULL;

    Teb = Thread->Tcb.Teb;

    //
    // Create an exception handler as the TEB is within the user's
    // address space.
    //

    try {

        //
        // Initialize default protections early so that they can be used on
        // all code paths.
        //

        ProtectionFlags = PAGE_READWRITE | PAGE_GUARD;
        RevertExecuteFlag = FALSE;
        StackProtection = PAGE_READWRITE;

#if defined(_IA64_)

      if ((Teb->NtTib.StackBase <= FaultingAddress) &&
          (Teb->DeallocationBStore > FaultingAddress)) {

        //
        // Check to see if the faulting address is within
        // the bstore limits and if so try to create another guard
        // page in the bstore.
        //
        //
        //          +----------------+
        //          |                | last page of stack (always no access)
        //          +----------------+
        //          |                |
        //          |                |
        //          |                |
        //          +----------------+
        //          | old guard page | <- faulting address is in this page.               |
        //          +----------------+
        //  bstore  |                |
        //  growth  |    ......      |
        //          |                |
        //    ^     |   allocated    |
        //    |     |                |  StackBase
        //          +----------------+
        //
        //

        NextPage = (ULONG_PTR)PAGE_ALIGN(FaultingAddress) + PAGE_SIZE;

        RegionSize = PAGE_SIZE;

        if ((NextPage + PAGE_SIZE) >= (ULONG_PTR)PAGE_ALIGN(Teb->DeallocationBStore)) {

            //
            // There is no more room for expansion, attempt to
            // commit the page before the last page of the
            // stack.
            //

            NextPage = (ULONG_PTR)PAGE_ALIGN(Teb->DeallocationBStore) - PAGE_SIZE;

            status = ZwAllocateVirtualMemory (NtCurrentProcess(),
                                              (PVOID *)&NextPage,
                                              0,
                                              &RegionSize,
                                              MEM_COMMIT,
                                              PAGE_READWRITE);
            if ( NT_SUCCESS(status) ) {
                Teb->BStoreLimit = (PVOID)( (PUCHAR)NextPage);
            }

            return STATUS_STACK_OVERFLOW;
        }

        Teb->BStoreLimit = (PVOID)((PUCHAR)(NextPage));

      }
      else {
#endif

        DeallocationStack = Teb->DeallocationStack;
        StackLimit = &Teb->NtTib.StackLimit;

        //
        // The stack base and the stack limit are both within the stack.
        //

        if ((Teb->NtTib.StackBase <= FaultingAddress) ||
            (DeallocationStack > FaultingAddress)) {

#if defined(_WIN64)

            //
            // Also check for the 32-bit native stack on NT64.
            //

            Teb32 = (PTEB32) Teb->NtTib.ExceptionList;

            if (Teb32 != NULL) {
                ProbeForReadSmallStructure(Teb32, sizeof(TEB32), sizeof(ULONG));
                if ((ULONG_PTR)Teb32->NtTib.StackBase > (ULONG_PTR)FaultingAddress &&
                    (ULONG_PTR)Teb32->DeallocationStack <= (ULONG_PTR)FaultingAddress) {
                    DeallocationStack = (PVOID)ULongToPtr(Teb32->DeallocationStack);

                    StackLimit = (PVOID *)&Teb32->NtTib.StackLimit;
                }
                else {
                    //
                    // Not within the stack.
                    //

                    return STATUS_GUARD_PAGE_VIOLATION;
                }
            }
            else
#endif
              {
                //
                // Not within the stack.
                //

                return STATUS_GUARD_PAGE_VIOLATION;
            }
        }

        //
        // If the image was marked for no stack extensions we will return
        // stack overflow immediately.
        //

        Process = PsGetCurrentProcessByThread (Thread);

        Peb = Process->Peb;

        if (Peb->NtGlobalFlag & FLG_DISABLE_STACK_EXTENSION) {
            return STATUS_STACK_OVERFLOW;
        }

        //
        // Add execute permission if necessary. We do not need to change anything
        // for the old guard page because either it is the first guard page of the
        // current thread and it will get correct protection during user mode thread
        // initialization (see LdrpInitialize in base\ntdll\ldrinit.c) or it is a 
        // guard page created by this function during stack growth and in this case
        // it gets correct protection. We do not do anything for a wow64 process.
        //

#if defined(_WIN64)
        if (Teb32 == NULL) {
#endif            
            ExecuteFlags = Peb->ExecuteOptions;

            if (ExecuteFlags & (MEM_EXECUTE_OPTION_STACK | MEM_EXECUTE_OPTION_DATA)) {

                if (ExecuteFlags & MEM_EXECUTE_OPTION_STACK) {

                    StackProtection = PAGE_EXECUTE_READWRITE;
                    ProtectionFlags = PAGE_EXECUTE_READWRITE | PAGE_GUARD;
                }
                else {

                    //
                    // The stack must be made non-executable.  The
                    // ZwAllocateVirtualMemory call below will make it executable
                    // because this process is marked as wanting executable data
                    // and ZwAllocate cannot tell this is really a stack
                    // allocation.
                    //

                    ASSERT (ExecuteFlags & MEM_EXECUTE_OPTION_DATA);
                    RevertExecuteFlag = TRUE;
                }
            }
#if defined(_WIN64)
        }
#endif

        //
        // This address is within the current stack, check to see
        // if there is ample room for another guard page and
        // if so attempt to commit a new guard page.
        //

#if EMULATE_USERMODE_STACK_4K

        if (Teb32 != NULL)
        {
            NextPage = ((ULONG_PTR)PAGE_4K_ALIGN(FaultingAddress) - PAGE_4K);
            DeallocationStack = PAGE_4K_ALIGN(DeallocationStack);
            PageSize = RegionSize = PAGE_4K;
            
            //
            // Don't set the 'G' bit on the native PTE. Let's just set
            // 'G' bit on the AltPte.
            //

            ProtectionFlags &= ~PAGE_GUARD;
        }
        else
#endif
        {
            NextPage = ((ULONG_PTR)PAGE_ALIGN(FaultingAddress) - PAGE_SIZE);
            DeallocationStack = PAGE_ALIGN(DeallocationStack);
            PageSize = RegionSize = PAGE_SIZE;
        }

        if ((NextPage - PageSize) <= (ULONG_PTR)DeallocationStack) {

            //
            // There is no more room for expansion, attempt to
            // commit the page before the last page of the
            // stack.
            //

            NextPage = (ULONG_PTR)DeallocationStack + PageSize;

            status = ZwAllocateVirtualMemory (NtCurrentProcess(),
                                              (PVOID *)&NextPage,
                                              0,
                                              &RegionSize,
                                              MEM_COMMIT,
                                              StackProtection);

            if (NT_SUCCESS(status)) {

#if defined(_WIN64)
                if (Teb32) {
                    // update the 32-bit stacklimit
                    *(ULONG *)StackLimit = PtrToUlong((PUCHAR)NextPage);
                }
                else {
                    *StackLimit = (PVOID)( (PUCHAR)NextPage);
                }
#else
                *StackLimit = (PVOID)( (PUCHAR)NextPage);
#endif

                //
                // Revert the EXECUTE bit with an extra protect() call
                // if we get it by default but it is not desired.
                //

                if (RevertExecuteFlag) {

                    status = ZwProtectVirtualMemory (NtCurrentProcess(),
                                                     (PVOID *)&NextPage,
                                                     &RegionSize,
                                                     StackProtection,
                                                     &OldProtection);

                    ASSERT (StackProtection & PAGE_READWRITE);
                }
            }

            return STATUS_STACK_OVERFLOW;
        }
#if defined(_WIN64)
        if (Teb32 != NULL) {

            //
            // Update the 32-bit stack limit.
            //

            *(ULONG *)StackLimit = PtrToUlong((PUCHAR)(NextPage + PageSize));
        }
        else {
            *StackLimit = (PVOID)((PUCHAR)(NextPage + PAGE_SIZE));
        }
#else
        *StackLimit = (PVOID)((PUCHAR)(NextPage + PAGE_SIZE));
#endif

#if defined(_IA64_)
      }
#endif // _IA64_

       //
       // Set the guard page. For wow64 processes the protection
       // will not contain the PAGE_GUARD bit. This is ok since in these
       // cases we will set the bit for the top emulated 4K page.
       //

       status = ZwAllocateVirtualMemory (NtCurrentProcess(),
                                         (PVOID *)&NextPage,
                                         0,
                                         &RegionSize,
                                         MEM_COMMIT,
                                         ProtectionFlags);

       if (NT_SUCCESS(status) || (status == STATUS_ALREADY_COMMITTED)) {

            //
            // Revert the EXECUTE bit with an extra protect() call
            // if we get it by default but it is not desired.
            //

            if (RevertExecuteFlag) {

                if (ProtectionFlags & PAGE_GUARD) {
                    ProtectionFlags = PAGE_READWRITE | PAGE_GUARD;
                }
                else {
                    ProtectionFlags = PAGE_READWRITE;
                }

                status = ZwProtectVirtualMemory (NtCurrentProcess(),
                                                 (PVOID *)&NextPage,
                                                 &RegionSize,
                                                 ProtectionFlags,
                                                 &OldProtection);
            }

#if EMULATE_USERMODE_STACK_4K
            if (Teb32 != NULL) {
                
                LOCK_ADDRESS_SPACE (Process);

                MiProtectFor4kPage ((PVOID)NextPage,
                                    RegionSize,
                                    (MM_READWRITE | MM_GUARD_PAGE),
                                    ALT_CHANGE,
                                    Process);

                UNLOCK_ADDRESS_SPACE (Process);
            }
#endif


            //
            // The guard page is now committed or stack space is
            // already present, return success.
            //

            return STATUS_PAGE_FAULT_GUARD_PAGE;
        }

        return STATUS_STACK_OVERFLOW;

    } except (EXCEPTION_EXECUTE_HANDLER) {

        //
        // An exception has occurred during the referencing of the
        // TEB or TIB, just return a guard page violation and
        // don't deal with the stack overflow.
        //

        return STATUS_GUARD_PAGE_VIOLATION;
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\lpc\i386\lpcmove.asm ===
title  "LPC Move Message Support"
;++
;
; Copyright (c) 1989  Microsoft Corporation
;
; Module Name:
;
;    lpcmove.asm
;
; Abstract:
;
;    This module implements functions to support the efficient movement of
;    LPC Message blocks
;
; Author:
;
;    Steven R. Wood (stevewo) 30-Jun-1989
;
; Environment:
;
;    Kernel mode only.
;
; Revision History:
;
;   6-Mar-90 bryanwi
;       Ported to the 386.
;
;--

.386p

include callconv.inc            ; calling convention macros

        page ,132
        subttl  "Update System Time"

_PAGE SEGMENT DWORD PUBLIC 'CODE'
	ASSUME	DS:FLAT, ES:FLAT, SS:NOTHING, FS:NOTHING, GS:NOTHING

;++
;
; VOID
; LpcpMoveMessage (
;    OUT PPORT_MESSAGE DstMsg
;    IN PPORT_MESSAGE SrcMsg
;    IN ULONG MsgType OPTIONAL,
;    IN PCLIENT_ID ClientId OPTIONAL
;    )
;
; Routine Description:
;
;    This function moves an LPC message block.
;
; Arguments:
;
;    DstMsg (TOS) - Supplies pointer to where to move the message block to.
;
;    SrcMsg (TOS+4) - Supplies a pointer to the message to move.
;
;    MsgType (TOS+8) - If non-zero, then store in Type field of DstMsg
;
;    ClientId (TOS+12) - If non-NULL, then points to a ClientId to copy to dst
;
; Return Value:
;
;    None
;
;--

DstMsg      equ [esp + 12]
SrcMsg      equ [esp + 16]
SrcMsgData  equ [esp + 20]
MsgType     equ [esp + 24]
ClientId    equ [esp + 28]

cPublicProc _LpcpMoveMessage    ,5

        push    esi                     ; Save non-volatile registers
        push    edi

        mov     edi,DstMsg              ; (edi)->Destination
        cld
        mov     esi,SrcMsg              ; (esi)->Source

        lodsd                           ; (eax)=length
        stosd
        lea     ecx,3[eax]
        and     ecx,0FFFCH              ; (ecx)=length rounded up to 4
        shr     ecx,2                   ; (ecx)=length in dwords

        lodsd                           ; (eax)=DataInfoOffset U Type
        mov     edx,MsgType             ; (edx)=MsgType
        or      edx,edx
        jz      lmm10                   ; No MsgType, go do straight copy
        mov     ax,dx                   ; (eax low 16)=MsgType
lmm10:  stosd

        mov     edx,ClientId            ; (edx)=ClientId
        or      edx,edx
        jz      lmm20                   ; No Clientid to set, go do copy
        mov     eax,[edx]               ; Get new ClientId
        stosd                           ; and store in DstMsg->ClientId
        mov     eax,[edx+4]
        stosd
        add     esi,8                   ; and skip over SrcMsg->ClientId
        jmp     short lmm30

lmm20:  movsd                           ; Copy ClientId
        movsd

;
;   At this point, all of the control structures are copied, all we
;   need to copy is the data.
;

lmm30:
        movsd                           ; Copy MessageId
        movsd                           ; Copy ClientViewSize

        mov     esi,SrcMsgData          ; Copy data directly from user buffer
        rep     movsd                   ; Copy the data portion of message

        pop     edi
        pop     esi                     ; Restore non-volatile registers

        stdRET    _LpcpMoveMessage

stdENDP _LpcpMoveMessage

_PAGE ends
        end

=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\allocpag.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   allocpag.c

Abstract:

    This module contains the routines which allocate and deallocate
    one or more pages from paged or nonpaged pool.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

PVOID
MiFindContiguousMemoryInPool (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN PVOID CallingAddress
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MiInitializeNonPagedPool)

#pragma alloc_text(PAGE, MmAvailablePoolInPages)
#pragma alloc_text(PAGELK, MiFindContiguousMemory)
#pragma alloc_text(PAGELK, MiFindContiguousMemoryInPool)

#pragma alloc_text(PAGE, MiCheckSessionPoolAllocations)
#pragma alloc_text(PAGE, MiSessionPoolVector)
#pragma alloc_text(PAGE, MiSessionPoolMutex)
#pragma alloc_text(PAGE, MiInitializeSessionPool)
#pragma alloc_text(PAGE, MiFreeSessionPoolBitMaps)

#pragma alloc_text(POOLMI, MiAllocatePoolPages)
#pragma alloc_text(POOLMI, MiFreePoolPages)

#if DBG || (i386 && !FPO)
#pragma alloc_text(PAGELK, MmSnapShotPool)
#endif // DBG || (i386 && !FPO)
#endif

ULONG MmPagedPoolCommit;        // used by the debugger

PFN_NUMBER MmAllocatedNonPagedPool;
PFN_NUMBER MiStartOfInitialPoolFrame;
PFN_NUMBER MiEndOfInitialPoolFrame;

PVOID MmNonPagedPoolEnd0;
PVOID MmNonPagedPoolExpansionStart;

LIST_ENTRY MmNonPagedPoolFreeListHead[MI_MAX_FREE_LIST_HEADS];

extern POOL_DESCRIPTOR NonPagedPoolDescriptor;

#define MM_SMALL_ALLOCATIONS 4

#if DBG

ULONG MiClearCache;

//
// Set this to a nonzero (ie: 10000) value to cause every pool allocation to
// be checked and an ASSERT fires if the allocation is larger than this value.
//

ULONG MmCheckRequestInPages = 0;

//
// Set this to a nonzero (ie: 0x23456789) value to cause this pattern to be
// written into freed nonpaged pool pages.
//

ULONG MiFillFreedPool = 0;
#endif

PFN_NUMBER MiExpansionPoolPagesInUse;
PFN_NUMBER MiExpansionPoolPagesInitialCharge;

ULONG MmUnusedSegmentForceFreeDefault = 30;

extern ULONG MmUnusedSegmentForceFree;

//
// For debugging purposes.
//

typedef enum _MM_POOL_TYPES {
    MmNonPagedPool,
    MmPagedPool,
    MmSessionPagedPool,
    MmMaximumPoolType
} MM_POOL_TYPES;

typedef enum _MM_POOL_PRIORITIES {
    MmHighPriority,
    MmNormalPriority,
    MmLowPriority,
    MmMaximumPoolPriority
} MM_POOL_PRIORITIES;

typedef enum _MM_POOL_FAILURE_REASONS {
    MmNonPagedNoPtes,
    MmPriorityTooLow,
    MmNonPagedNoPagesAvailable,
    MmPagedNoPtes,
    MmSessionPagedNoPtes,
    MmPagedNoPagesAvailable,
    MmSessionPagedNoPagesAvailable,
    MmPagedNoCommit,
    MmSessionPagedNoCommit,
    MmNonPagedNoResidentAvailable,
    MmNonPagedNoCommit,
    MmMaximumFailureReason
} MM_POOL_FAILURE_REASONS;

ULONG MmPoolFailures[MmMaximumPoolType][MmMaximumPoolPriority];
ULONG MmPoolFailureReasons[MmMaximumFailureReason];

typedef enum _MM_PREEMPTIVE_TRIMS {
    MmPreemptForNonPaged,
    MmPreemptForPaged,
    MmPreemptForNonPagedPriority,
    MmPreemptForPagedPriority,
    MmMaximumPreempt
} MM_PREEMPTIVE_TRIMS;

ULONG MmPreemptiveTrims[MmMaximumPreempt];


VOID
MiProtectFreeNonPagedPool (
    IN PVOID VirtualAddress,
    IN ULONG SizeInPages
    )

/*++

Routine Description:

    This function protects freed nonpaged pool.

Arguments:

    VirtualAddress - Supplies the freed pool address to protect.

    SizeInPages - Supplies the size of the request in pages.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    MMPTE PteContents;
    PMMPTE PointerPte;

    //
    // Prevent anyone from touching the free non paged pool
    //

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress) == 0) {
        PointerPte = MiGetPteAddress (VirtualAddress);

        for (i = 0; i < SizeInPages; i += 1) {

            PteContents = *PointerPte;

            PteContents.u.Hard.Valid = 0;
            PteContents.u.Soft.Prototype = 1;
    
            KeFlushSingleTb (VirtualAddress,
                             TRUE,
                             TRUE,
                             (PHARDWARE_PTE)PointerPte,
                             PteContents.u.Flush);
            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            PointerPte += 1;
        }
    }
}


LOGICAL
MiUnProtectFreeNonPagedPool (
    IN PVOID VirtualAddress,
    IN ULONG SizeInPages
    )

/*++

Routine Description:

    This function unprotects freed nonpaged pool.

Arguments:

    VirtualAddress - Supplies the freed pool address to unprotect.

    SizeInPages - Supplies the size of the request in pages - zero indicates
                  to keep going until there are no more protected PTEs (ie: the
                  caller doesn't know how many protected PTEs there are).

Return Value:

    TRUE if pages were unprotected, FALSE if not.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;
    MMPTE PteContents;
    ULONG PagesDone;

    PagesDone = 0;

    //
    // Unprotect the previously freed pool so it can be manipulated
    //

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress) == 0) {

        PointerPte = MiGetPteAddress((PVOID)VirtualAddress);

        PteContents = *PointerPte;

        while (PteContents.u.Hard.Valid == 0 && PteContents.u.Soft.Prototype == 1) {

            PteContents.u.Hard.Valid = 1;
            PteContents.u.Soft.Prototype = 0;
    
            MI_WRITE_VALID_PTE (PointerPte, PteContents);

            PagesDone += 1;

            if (PagesDone == SizeInPages) {
                break;
            }

            PointerPte += 1;
            PteContents = *PointerPte;
        }
    }

    if (PagesDone == 0) {
        return FALSE;
    }

    return TRUE;
}


VOID
MiProtectedPoolInsertList (
    IN PLIST_ENTRY ListHead,
    IN PLIST_ENTRY Entry,
    IN LOGICAL InsertHead
    )

/*++

Routine Description:

    This function inserts the entry into the protected list.

Arguments:

    ListHead - Supplies the list head to add onto.

    Entry - Supplies the list entry to insert.

    InsertHead - If TRUE, insert at the head otherwise at the tail.

Return Value:

    None.

Environment:

    Kernel mode.

--*/
{
    PVOID FreeFlink;
    PVOID FreeBlink;
    PVOID VirtualAddress;

    //
    // Either the flink or the blink may be pointing
    // at protected nonpaged pool.  Unprotect now.
    //

    FreeFlink = (PVOID)0;
    FreeBlink = (PVOID)0;

    if (IsListEmpty(ListHead) == 0) {

        VirtualAddress = (PVOID)ListHead->Flink;
        if (MiUnProtectFreeNonPagedPool (VirtualAddress, 1) == TRUE) {
            FreeFlink = VirtualAddress;
        }
    }

    if (((PVOID)Entry == ListHead->Blink) == 0) {
        VirtualAddress = (PVOID)ListHead->Blink;
        if (MiUnProtectFreeNonPagedPool (VirtualAddress, 1) == TRUE) {
            FreeBlink = VirtualAddress;
        }
    }

    if (InsertHead == TRUE) {
        InsertHeadList (ListHead, Entry);
    }
    else {
        InsertTailList (ListHead, Entry);
    }

    if (FreeFlink) {
        //
        // Reprotect the flink.
        //

        MiProtectFreeNonPagedPool (FreeFlink, 1);
    }

    if (FreeBlink) {
        //
        // Reprotect the blink.
        //

        MiProtectFreeNonPagedPool (FreeBlink, 1);
    }
}


VOID
MiProtectedPoolRemoveEntryList (
    IN PLIST_ENTRY Entry
    )

/*++

Routine Description:

    This function unlinks the list pointer from protected freed nonpaged pool.

Arguments:

    Entry - Supplies the list entry to remove.

Return Value:

    None.

Environment:

    Kernel mode.

--*/
{
    PVOID FreeFlink;
    PVOID FreeBlink;
    PVOID VirtualAddress;

    //
    // Either the flink or the blink may be pointing
    // at protected nonpaged pool.  Unprotect now.
    //

    FreeFlink = (PVOID)0;
    FreeBlink = (PVOID)0;

    if (IsListEmpty(Entry) == 0) {

        VirtualAddress = (PVOID)Entry->Flink;
        if (MiUnProtectFreeNonPagedPool (VirtualAddress, 1) == TRUE) {
            FreeFlink = VirtualAddress;
        }
    }

    if (((PVOID)Entry == Entry->Blink) == 0) {
        VirtualAddress = (PVOID)Entry->Blink;
        if (MiUnProtectFreeNonPagedPool (VirtualAddress, 1) == TRUE) {
            FreeBlink = VirtualAddress;
        }
    }

    RemoveEntryList (Entry);

    if (FreeFlink) {
        //
        // Reprotect the flink.
        //

        MiProtectFreeNonPagedPool (FreeFlink, 1);
    }

    if (FreeBlink) {
        //
        // Reprotect the blink.
        //

        MiProtectFreeNonPagedPool (FreeBlink, 1);
    }
}


VOID
MiTrimSegmentCache (
    VOID
    )

/*++

Routine Description:

    This function initiates trimming of the segment cache.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.

--*/

{
    KIRQL OldIrql;
    LOGICAL SignalDereferenceThread;
    LOGICAL SignalSystemCache;

    SignalDereferenceThread = FALSE;
    SignalSystemCache = FALSE;

    LOCK_PFN2 (OldIrql);

    if (MmUnusedSegmentForceFree == 0) {

        if (!IsListEmpty(&MmUnusedSegmentList)) {

            SignalDereferenceThread = TRUE;
            MmUnusedSegmentForceFree = MmUnusedSegmentForceFreeDefault;
        }
        else {
            if (!IsListEmpty(&MmUnusedSubsectionList)) {
                SignalDereferenceThread = TRUE;
                MmUnusedSegmentForceFree = MmUnusedSegmentForceFreeDefault;
            }

            if (MiUnusedSubsectionPagedPool < 4 * PAGE_SIZE) {

                //
                // No unused segments and tossable subsection usage is low as
                // well.  Start unmapping system cache views in an attempt
                // to get back the paged pool containing its prototype PTEs.
                //
    
                SignalSystemCache = TRUE;
            }
        }
    }

    UNLOCK_PFN2 (OldIrql);

    if (SignalSystemCache == TRUE) {
        if (CcHasInactiveViews() == TRUE) {
            if (SignalDereferenceThread == FALSE) {
                LOCK_PFN2 (OldIrql);
                if (MmUnusedSegmentForceFree == 0) {
                    SignalDereferenceThread = TRUE;
                    MmUnusedSegmentForceFree = MmUnusedSegmentForceFreeDefault;
                }
                UNLOCK_PFN2 (OldIrql);
            }
        }
    }

    if (SignalDereferenceThread == TRUE) {
        KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
    }
}


POOL_TYPE
MmDeterminePoolType (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function determines which pool a virtual address resides within.

Arguments:

    VirtualAddress - Supplies the virtual address to determine which pool
                     it resides within.

Return Value:

    Returns the POOL_TYPE (PagedPool, NonPagedPool, PagedPoolSession or
            NonPagedPoolSession).

Environment:

    Kernel Mode Only.

--*/

{
    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress <= MmPagedPoolEnd)) {
        return PagedPool;
    }

    if (MI_IS_SESSION_POOL_ADDRESS (VirtualAddress) == TRUE) {
        return PagedPoolSession;
    }

    return NonPagedPool;
}


PVOID
MiSessionPoolVector (
    VOID
    )

/*++

Routine Description:

    This function returns the session pool descriptor for the current session.

Arguments:

    None.

Return Value:

    Pool descriptor.

--*/

{
    PAGED_CODE ();

    return (PVOID)&MmSessionSpace->PagedPool;
}

VOID
MiSessionPoolAllocated (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    This function charges the new pool allocation for the current session.
    On session exit, this charge must be zero.

    Interlocks are used here despite the fact that synchronization is provided
    anyway by our caller.  This is so the path where the pool is freed can
    occur caller-lock-free.

Arguments:

    VirtualAddress - Supplies the allocated pool address.

    NumberOfBytes - Supplies the number of bytes allocated.

    PoolType - Supplies the type of the above pool allocation.

Return Value:

    None.

Environment:

    Called both from Mm and executive pool.

    Holding no pool resources when called from pool.  Unfortunately, pool
    resources are held when called from Mm.

--*/

{
#if !DBG
    UNREFERENCED_PARAMETER (VirtualAddress);
#endif

    if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {
        ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
        ASSERT (MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) == FALSE);

        InterlockedExchangeAddSizeT (&MmSessionSpace->NonPagedPoolBytes,
                                     NumberOfBytes);

        InterlockedIncrement ((PLONG)&MmSessionSpace->NonPagedPoolAllocations);
    }
    else {
        ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
        ASSERT (MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) == TRUE);

        InterlockedExchangeAddSizeT (&MmSessionSpace->PagedPoolBytes,
                                     NumberOfBytes);

        InterlockedIncrement ((PLONG)&MmSessionSpace->PagedPoolAllocations);
    }
}


VOID
MiSessionPoolFreed (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    This function returns the specified pool allocation for the current session.
    On session exit, this charge must be zero.

Arguments:

    VirtualAddress - Supplies the pool address being freed.

    NumberOfBytes - Supplies the number of bytes being freed.

    PoolType - Supplies the type of the above pool allocation.

Return Value:

    None.

Environment:

    DISPATCH_LEVEL or below for nonpaged pool allocations,
    APC_LEVEL or below for paged pool.

--*/

{
#if !DBG
    UNREFERENCED_PARAMETER (VirtualAddress);
#endif

    if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {
        ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
        ASSERT (MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) == FALSE);

        InterlockedExchangeAddSizeT (&MmSessionSpace->NonPagedPoolBytes,
                                     0-NumberOfBytes);

        InterlockedDecrement ((PLONG)&MmSessionSpace->NonPagedPoolAllocations);
    }
    else {
        ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
        ASSERT (MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) == TRUE);

        InterlockedExchangeAddSizeT (&MmSessionSpace->PagedPoolBytes,
                                     0-NumberOfBytes);

        InterlockedDecrement ((PLONG)&MmSessionSpace->PagedPoolAllocations);
    }
}


SIZE_T
MmAvailablePoolInPages (
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    This function returns the number of pages available for the given pool.
    Note that it does not account for any executive pool fragmentation.

Arguments:

    PoolType - Supplies the type of pool to retrieve information about.

Return Value:

    The number of full pool pages remaining.

Environment:

    PASSIVE_LEVEL, no mutexes or locks held.

--*/

{
    SIZE_T FreePoolInPages;
    SIZE_T FreeCommitInPages;

#if !DBG
    UNREFERENCED_PARAMETER (PoolType);
#endif

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (PoolType == PagedPool);

    FreePoolInPages = (MmSizeOfPagedPoolInBytes >> PAGE_SHIFT) - MmPagedPoolInfo.AllocatedPagedPool;

    FreeCommitInPages = MmTotalCommitLimitMaximum - MmTotalCommittedPages;

    if (FreePoolInPages > FreeCommitInPages) {
        FreePoolInPages = FreeCommitInPages;
    }

    return FreePoolInPages;
}


LOGICAL
MmResourcesAvailable (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN EX_POOL_PRIORITY Priority
    )

/*++

Routine Description:

    This function examines various resources to determine if this
    pool allocation should be allowed to proceed.

Arguments:

    PoolType - Supplies the type of pool to retrieve information about.

    NumberOfBytes - Supplies the number of bytes to allocate.

    Priority - Supplies an indication as to how important it is that this
               request succeed under low available resource conditions.                       
Return Value:

    TRUE if the pool allocation should be allowed to proceed, FALSE if not.

--*/

{
    PFN_NUMBER NumberOfPages;
    SIZE_T FreePoolInBytes;
    LOGICAL Status;
    MM_POOL_PRIORITIES Index;

    ASSERT (Priority != HighPoolPriority);
    ASSERT ((PoolType & MUST_SUCCEED_POOL_TYPE_MASK) == 0);

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {
        FreePoolInBytes = MmMaximumNonPagedPoolInBytes - (MmAllocatedNonPagedPool << PAGE_SHIFT);
    }
    else if (PoolType & SESSION_POOL_MASK) {
        FreePoolInBytes = MmSessionPoolSize - MmSessionSpace->PagedPoolBytes;
    }
    else {
        FreePoolInBytes = MmSizeOfPagedPoolInBytes - (MmPagedPoolInfo.AllocatedPagedPool << PAGE_SHIFT);
    }

    Status = FALSE;

    //
    // Check available VA space.
    //

    if (Priority == NormalPoolPriority) {
        if ((SIZE_T)NumberOfBytes + 512*1024 > FreePoolInBytes) {
            if (PsGetCurrentThread()->MemoryMaker == 0) {
                goto nopool;
            }
        }
    }
    else {
        if ((SIZE_T)NumberOfBytes + 2*1024*1024 > FreePoolInBytes) {
            if (PsGetCurrentThread()->MemoryMaker == 0) {
                goto nopool;
            }
        }
    }

    //
    // Paged allocations (session and normal) can also fail for lack of commit.
    //

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {
        if (MmTotalCommittedPages + NumberOfPages > MmTotalCommitLimitMaximum) {
            if (PsGetCurrentThread()->MemoryMaker == 0) {
                MiIssuePageExtendRequestNoWait (NumberOfPages);
                goto nopool;
            }
        }
    }

    //
    // If a substantial amount of free pool is still available, return TRUE now.
    //

    if (((SIZE_T)NumberOfBytes + 10*1024*1024 < FreePoolInBytes) ||
        (MmNumberOfPhysicalPages < 256 * 1024)) {
        return TRUE;
    }

    //
    // This pool allocation is permitted, but because we're starting to run low,
    // trigger a round of dereferencing in parallel before returning success.
    // Note this is only done on machines with at least 1GB of RAM as smaller
    // configuration machines will already trigger this due to physical page
    // consumption.
    //

    Status = TRUE;

nopool:

    //
    // Running low on pool - if this request is not for session pool,
    // force unused segment trimming when appropriate.
    //

    if ((PoolType & SESSION_POOL_MASK) == 0) {

        if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {
            MmPreemptiveTrims[MmPreemptForNonPagedPriority] += 1;
        }
        else {
            MmPreemptiveTrims[MmPreemptForPagedPriority] += 1;
        }

        if (MI_UNUSED_SEGMENTS_SURPLUS()) {
            KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
        }
        else {
            MiTrimSegmentCache ();
        }
    }

    if (Status == FALSE) {

        //
        // Log this failure for debugging purposes.
        //

        if (Priority == NormalPoolPriority) {
            Index = MmNormalPriority;
        }
        else {
            Index = MmLowPriority;
        }

        if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {
            MmPoolFailures[MmNonPagedPool][Index] += 1;
        }
        else if (PoolType & SESSION_POOL_MASK) {
            MmPoolFailures[MmSessionPagedPool][Index] += 1;
            MmSessionSpace->SessionPoolAllocationFailures[0] += 1;
        }
        else {
            MmPoolFailures[MmPagedPool][Index] += 1;
        }

        MmPoolFailureReasons[MmPriorityTooLow] += 1;
    }

    return Status;
}


VOID
MiFreeNonPagedPool (
    IN PVOID StartingAddress,
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    This function releases virtually mapped nonpaged expansion pool.

Arguments:

    StartingAddress - Supplies the starting address.

    NumberOfPages - Supplies the number of pages to free.

Return Value:

    None.

Environment:

    These functions are used by the internal Mm page allocation/free routines
    only and should not be called directly.

    Mutexes guarding the pool databases must be held when calling
    this function.

--*/

{
    PFN_NUMBER i;
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PFN_NUMBER ResAvailToReturn;
    PFN_NUMBER PageFrameIndex;
    ULONG Count;
    PVOID FlushVa[MM_MAXIMUM_FLUSH_COUNT];

    MI_MAKING_MULTIPLE_PTES_INVALID (TRUE);

    Count = 0;
    PointerPte = MiGetPteAddress (StartingAddress);

    //
    // Return commitment.
    //

    MiReturnCommitment (NumberOfPages);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NONPAGED_POOL_EXPANSION,
                     NumberOfPages);

    ResAvailToReturn = 0;

    LOCK_PFN_AT_DPC ();

    if (MiExpansionPoolPagesInUse > MiExpansionPoolPagesInitialCharge) {
        ResAvailToReturn = MiExpansionPoolPagesInUse - MiExpansionPoolPagesInitialCharge;
    }
    MiExpansionPoolPagesInUse -= NumberOfPages;

    for (i = 0; i < NumberOfPages; i += 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        //
        // Set the pointer to the PTE as empty so the page
        // is deleted when the reference count goes to zero.
        //

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u2.ShareCount == 1);
        Pfn1->u2.ShareCount = 0;
        MI_SET_PFN_DELETED (Pfn1);
#if DBG
        Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif //DBG
        MiDecrementReferenceCount (PageFrameIndex);

        if (Count != MM_MAXIMUM_FLUSH_COUNT) {
            FlushVa[Count] = StartingAddress;
            Count += 1;
        }

        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

        StartingAddress = (PVOID)((PCHAR)StartingAddress + PAGE_SIZE);
        PointerPte += 1;
    }

    //
    // Generally there is no need to update resident available
    // pages at this time as it has all been done during initialization.
    // However, only some of the expansion pool was charged at init, so
    // calculate how much (if any) resident available page charge to return.
    //

    if (ResAvailToReturn > NumberOfPages) {
        ResAvailToReturn = NumberOfPages;
    }

    if (ResAvailToReturn != 0) {
        MmResidentAvailablePages += ResAvailToReturn;
        MM_BUMP_COUNTER(23, ResAvailToReturn);
    }

    //
    // The PFN lock is not needed for the TB flush - the caller either holds
    // the nonpaged pool lock or nothing, but regardless the address range
    // cannot be reused until the PTEs are released below.
    //

    UNLOCK_PFN_FROM_DPC ();

    if (Count < MM_MAXIMUM_FLUSH_COUNT) {
        KeFlushMultipleTb (Count,
                           &FlushVa[0],
                           TRUE,
                           TRUE,
                           NULL,
                           *(PHARDWARE_PTE)&ZeroPte.u.Flush);
    }
    else {
        KeFlushEntireTb (TRUE, TRUE);
    }

    KeLowerIrql (DISPATCH_LEVEL);

    PointerPte -= NumberOfPages;

    MiReleaseSystemPtes (PointerPte,
                         (ULONG)NumberOfPages,
                         NonPagedPoolExpansion);
}

LOGICAL
MiFreeAllExpansionNonPagedPool (
    IN LOGICAL PoolLockHeld
    )

/*++

Routine Description:

    This function releases all virtually mapped nonpaged expansion pool.

Arguments:

    NonPoolLockHeld - Supplies TRUE if the nonpaged pool lock is already held,
                      FALSE if not.

Return Value:

    TRUE if pages were freed, FALSE if not.

Environment:

    Kernel mode.  NonPagedPool lock optionally may be held, PFN lock is NOT held.

--*/

{
    ULONG Index;
    KIRQL OldIrql;
    PLIST_ENTRY Entry;
    LOGICAL FreedPool;
    PMMFREE_POOL_ENTRY FreePageInfo;

    FreedPool = FALSE;

    if (PoolLockHeld == FALSE) {
        OldIrql = ExLockPool (NonPagedPool);
    }
    else {

        //
        // Initializing OldIrql is not needed for correctness, but without it
        // the compiler cannot compile this code W4 to check for use of
        // uninitialized variables.
        //

        OldIrql = PASSIVE_LEVEL;
    }

    for (Index = 0; Index < MI_MAX_FREE_LIST_HEADS; Index += 1) {

        Entry = MmNonPagedPoolFreeListHead[Index].Flink;

        while (Entry != &MmNonPagedPoolFreeListHead[Index]) {

            if (MmProtectFreedNonPagedPool == TRUE) {
                MiUnProtectFreeNonPagedPool ((PVOID)Entry, 0);
            }

            //
            // The list is not empty, see if this one is virtually
            // mapped.
            //

            FreePageInfo = CONTAINING_RECORD(Entry,
                                             MMFREE_POOL_ENTRY,
                                             List);

            if ((!MI_IS_PHYSICAL_ADDRESS(FreePageInfo)) &&
                ((PVOID)FreePageInfo >= MmNonPagedPoolExpansionStart)) {

                if (MmProtectFreedNonPagedPool == FALSE) {
                    RemoveEntryList (&FreePageInfo->List);
                }
                else {
                    MiProtectedPoolRemoveEntryList (&FreePageInfo->List);
                }

                MmNumberOfFreeNonPagedPool -= FreePageInfo->Size;
                ASSERT ((LONG)MmNumberOfFreeNonPagedPool >= 0);

                FreedPool = TRUE;

                MiFreeNonPagedPool ((PVOID)FreePageInfo,
                                    FreePageInfo->Size);

                Index = (ULONG)-1;
                break;
            }

            Entry = FreePageInfo->List.Flink;

            if (MmProtectFreedNonPagedPool == TRUE) {
                MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                           (ULONG)FreePageInfo->Size);
            }
        }
    }

    if (PoolLockHeld == FALSE) {
        ExUnlockPool (NonPagedPool, OldIrql);
    }

    return FreedPool;
}

PVOID
MiAllocatePoolPages (
    IN POOL_TYPE PoolType,
    IN SIZE_T SizeInBytes,
    IN ULONG IsLargeSessionAllocation
    )

/*++

Routine Description:

    This function allocates a set of pages from the specified pool
    and returns the starting virtual address to the caller.

Arguments:

    PoolType - Supplies the type of pool from which to obtain pages.

    SizeInBytes - Supplies the size of the request in bytes.  The actual
                  size returned is rounded up to a page boundary.

    IsLargeSessionAllocation - Supplies nonzero if the allocation is a single
                               large session allocation.  Zero otherwise.

Return Value:

    Returns a pointer to the allocated pool, or NULL if no more pool is
    available.

Environment:

    These functions are used by the general pool allocation routines
    and should not be called directly.

    Mutexes guarding the pool databases must be held when calling
    these functions.

    Kernel mode, IRQL at DISPATCH_LEVEL.

--*/

{
    PETHREAD Thread;
    PFN_NUMBER SizeInPages;
    ULONG StartPosition;
    ULONG EndPosition;
    PMMPTE StartingPte;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PVOID BaseVa;
    KIRQL OldIrql;
    KIRQL SessionIrql;
    PFN_NUMBER i;
    PFN_NUMBER j;
    PLIST_ENTRY Entry;
    PLIST_ENTRY ListHead;
    PLIST_ENTRY LastListHead;
    PMMFREE_POOL_ENTRY FreePageInfo;
    PMM_SESSION_SPACE SessionSpace;
    PMM_PAGED_POOL_INFO PagedPoolInfo;
    PVOID VirtualAddress;
    PVOID VirtualAddressSave;
    ULONG Index;
    PMMPTE SessionPte;
    WSLE_NUMBER WsEntry;
    WSLE_NUMBER WsSwapEntry;
    ULONG PageTableCount;
    LOGICAL AddressIsPhysical;

    SizeInPages = BYTES_TO_PAGES (SizeInBytes);

#if DBG
    if (MmCheckRequestInPages != 0) {
        ASSERT (SizeInPages < MmCheckRequestInPages);
    }
#endif

    if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {

        Index = (ULONG)(SizeInPages - 1);

        if (Index >= MI_MAX_FREE_LIST_HEADS) {
            Index = MI_MAX_FREE_LIST_HEADS - 1;
        }

        //
        // NonPaged pool is linked together through the pages themselves.
        //

        ListHead = &MmNonPagedPoolFreeListHead[Index];
        LastListHead = &MmNonPagedPoolFreeListHead[MI_MAX_FREE_LIST_HEADS];

        do {

            Entry = ListHead->Flink;

            while (Entry != ListHead) {

                if (MmProtectFreedNonPagedPool == TRUE) {
                    MiUnProtectFreeNonPagedPool ((PVOID)Entry, 0);
                }
    
                //
                // The list is not empty, see if this one has enough space.
                //
    
                FreePageInfo = CONTAINING_RECORD(Entry,
                                                 MMFREE_POOL_ENTRY,
                                                 List);
    
                ASSERT (FreePageInfo->Signature == MM_FREE_POOL_SIGNATURE);
                if (FreePageInfo->Size >= SizeInPages) {
    
                    //
                    // This entry has sufficient space, remove
                    // the pages from the end of the allocation.
                    //
    
                    FreePageInfo->Size -= SizeInPages;
    
                    BaseVa = (PVOID)((PCHAR)FreePageInfo +
                                            (FreePageInfo->Size  << PAGE_SHIFT));
    
                    if (MmProtectFreedNonPagedPool == FALSE) {
                        RemoveEntryList (&FreePageInfo->List);
                    }
                    else {
                        MiProtectedPoolRemoveEntryList (&FreePageInfo->List);
                    }

                    if (FreePageInfo->Size != 0) {
    
                        //
                        // Insert any remainder into the correct list.
                        //
    
                        Index = (ULONG)(FreePageInfo->Size - 1);
                        if (Index >= MI_MAX_FREE_LIST_HEADS) {
                            Index = MI_MAX_FREE_LIST_HEADS - 1;
                        }

                        if (MmProtectFreedNonPagedPool == FALSE) {
                            InsertTailList (&MmNonPagedPoolFreeListHead[Index],
                                            &FreePageInfo->List);
                        }
                        else {
                            MiProtectedPoolInsertList (&MmNonPagedPoolFreeListHead[Index],
                                                       &FreePageInfo->List,
                                                       FALSE);

                            MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                                       (ULONG)FreePageInfo->Size);
                        }
                    }
    
                    //
                    // Adjust the number of free pages remaining in the pool.
                    //
    
                    MmNumberOfFreeNonPagedPool -= SizeInPages;
                    ASSERT ((LONG)MmNumberOfFreeNonPagedPool >= 0);
    
                    //
                    // Mark start and end of allocation in the PFN database.
                    //
    
                    if (MI_IS_PHYSICAL_ADDRESS(BaseVa)) {
    
                        //
                        // On certain architectures, virtual addresses
                        // may be physical and hence have no corresponding PTE.
                        //
    
                        AddressIsPhysical = TRUE;
                        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (BaseVa);

                        //
                        // Initializing PointerPte is not needed for correctness
                        // but without it the compiler cannot compile this code
                        // W4 to check for use of uninitialized variables.
                        //

                        PointerPte = NULL;
                    }
                    else {
                        AddressIsPhysical = FALSE;
                        PointerPte = MiGetPteAddress(BaseVa);
                        ASSERT (PointerPte->u.Hard.Valid == 1);
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                    }
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    
                    ASSERT (Pfn1->u3.e1.StartOfAllocation == 0);
                    ASSERT (Pfn1->u4.VerifierAllocation == 0);
    
                    Pfn1->u3.e1.StartOfAllocation = 1;
    
                    if (PoolType & POOL_VERIFIER_MASK) {
                        Pfn1->u4.VerifierAllocation = 1;
                    }

                    //
                    // Mark this as a large session allocation in the PFN database.
                    //
    
                    if (IsLargeSessionAllocation != 0) {
                        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
    
                        Pfn1->u3.e1.LargeSessionAllocation = 1;
    
                        MiSessionPoolAllocated (BaseVa,
                                                SizeInPages << PAGE_SHIFT,
                                                NonPagedPool);
                    }
    
                    //
                    // Calculate the ending PTE's address.
                    //
    
                    if (SizeInPages != 1) {
                        if (AddressIsPhysical == TRUE) {
                            Pfn1 += SizeInPages - 1;
                        }
                        else {
                            PointerPte += SizeInPages - 1;
                            ASSERT (PointerPte->u.Hard.Valid == 1);
                            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                        }
                    }
    
                    ASSERT (Pfn1->u3.e1.EndOfAllocation == 0);
    
                    Pfn1->u3.e1.EndOfAllocation = 1;
    
                    MmAllocatedNonPagedPool += SizeInPages;
                    return BaseVa;
                }
    
                Entry = FreePageInfo->List.Flink;
    
                if (MmProtectFreedNonPagedPool == TRUE) {
                    MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                               (ULONG)FreePageInfo->Size);
                }
            }

            ListHead += 1;

        } while (ListHead < LastListHead);

        //
        // No more entries on the list, expand nonpaged pool if
        // possible to satisfy this request.
        //

        //
        // If pool is starting to run low then free some paged cache up now.
        // While this can never prevent pool allocations from failing, it does
        // give drivers a better chance to always see success.
        //

        if (MmMaximumNonPagedPoolInBytes - (MmAllocatedNonPagedPool << PAGE_SHIFT) < 3 * 1024 * 1024) {
            MmPreemptiveTrims[MmPreemptForNonPaged] += 1;
            MiTrimSegmentCache ();
        }

#if defined (_WIN64)
        if (SizeInPages >= _4gb) {
            return NULL;
        }
#endif

        //
        // Try to find system PTEs to expand the pool into.
        //

        StartingPte = MiReserveSystemPtes ((ULONG)SizeInPages,
                                           NonPagedPoolExpansion);

        if (StartingPte == NULL) {

            //
            // There are no free physical PTEs to expand nonpaged pool.
            //
            // Check to see if there are too many unused segments lying
            // around.  If so, set an event so they get deleted.
            //

            if (MI_UNUSED_SEGMENTS_SURPLUS()) {
                KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
            }

            //
            // If there are any cached expansion PTEs, free them now in
            // an attempt to get enough contiguous VA for our caller.
            //

            if ((SizeInPages > 1) && (MmNumberOfFreeNonPagedPool != 0)) {

                if (MiFreeAllExpansionNonPagedPool (TRUE) == TRUE) {

                    StartingPte = MiReserveSystemPtes ((ULONG)SizeInPages,
                                                       NonPagedPoolExpansion);
                }
            }

            if (StartingPte == NULL) {

                MmPoolFailures[MmNonPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmNonPagedNoPtes] += 1;

                //
                // Running low on pool - force unused segment trimming.
                //
            
                MiTrimSegmentCache ();

                return NULL;
            }
        }

        //
        // Charge commitment as nonpaged pool uses physical memory.
        //

        if (MiChargeCommitmentCantExpand (SizeInPages, FALSE) == FALSE) {
            if (PsGetCurrentThread()->MemoryMaker == 1) {
                MiChargeCommitmentCantExpand (SizeInPages, TRUE);
            }
            else {
                MiReleaseSystemPtes (StartingPte,
                                     (ULONG)SizeInPages,
                                     NonPagedPoolExpansion);

                MmPoolFailures[MmNonPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmNonPagedNoCommit] += 1;
                MiTrimSegmentCache ();
                return NULL;
            }
        }

        PointerPte = StartingPte;
        TempPte = ValidKernelPte;
        i = SizeInPages;

        MmAllocatedNonPagedPool += SizeInPages;

        LOCK_PFN_AT_DPC ();

        //
        // Make sure we have 1 more than the number of pages
        // requested available.
        //

        if (MmAvailablePages <= SizeInPages) {

            UNLOCK_PFN_FROM_DPC ();

            //
            // There are no free physical pages to expand nonpaged pool.
            //

            MmPoolFailureReasons[MmNonPagedNoPagesAvailable] += 1;

            MmPoolFailures[MmNonPagedPool][MmHighPriority] += 1;

            MmAllocatedNonPagedPool -= SizeInPages;

            MiReturnCommitment (SizeInPages);

            MiReleaseSystemPtes (StartingPte,
                                 (ULONG)SizeInPages,
                                 NonPagedPoolExpansion);

            MiTrimSegmentCache ();

            return NULL;
        }

        //
        // Charge resident available pages now for any excess.
        //

        MiExpansionPoolPagesInUse += SizeInPages;
        if (MiExpansionPoolPagesInUse > MiExpansionPoolPagesInitialCharge) {
            j = MiExpansionPoolPagesInUse - MiExpansionPoolPagesInitialCharge;
            if (j > SizeInPages) {
                j = SizeInPages;
            }
            if (MI_NONPAGABLE_MEMORY_AVAILABLE() >= (SPFN_NUMBER)j) {
                MmResidentAvailablePages -= j;
                MM_BUMP_COUNTER(24, j);
            }
            else {
                MiExpansionPoolPagesInUse -= SizeInPages;
                UNLOCK_PFN_FROM_DPC ();
                MmPoolFailureReasons[MmNonPagedNoResidentAvailable] += 1;

                MmPoolFailures[MmNonPagedPool][MmHighPriority] += 1;

                MmAllocatedNonPagedPool -= SizeInPages;

                MiReturnCommitment (SizeInPages);

                MiReleaseSystemPtes (StartingPte,
                                    (ULONG)SizeInPages,
                                    NonPagedPoolExpansion);

                MiTrimSegmentCache ();

                return NULL;
            }
        }
    
        MM_TRACK_COMMIT (MM_DBG_COMMIT_NONPAGED_POOL_EXPANSION, SizeInPages);

        //
        // Expand the pool.
        //

        do {
            PageFrameIndex = MiRemoveAnyPage (
                                MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            Pfn1->u3.e2.ReferenceCount = 1;
            Pfn1->u2.ShareCount = 1;
            Pfn1->PteAddress = PointerPte;
            Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
            Pfn1->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPte));

            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiCached;
            Pfn1->u3.e1.LargeSessionAllocation = 0;
            Pfn1->u4.VerifierAllocation = 0;

            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);
            PointerPte += 1;
            SizeInPages -= 1;
        } while (SizeInPages > 0);

        Pfn1->u3.e1.EndOfAllocation = 1;

        Pfn1 = MI_PFN_ELEMENT (StartingPte->u.Hard.PageFrameNumber);
        Pfn1->u3.e1.StartOfAllocation = 1;

        ASSERT (Pfn1->u4.VerifierAllocation == 0);

        if (PoolType & POOL_VERIFIER_MASK) {
            Pfn1->u4.VerifierAllocation = 1;
        }

        //
        // Mark this as a large session allocation in the PFN database.
        //

        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);

        if (IsLargeSessionAllocation != 0) {
            Pfn1->u3.e1.LargeSessionAllocation = 1;
        }

        UNLOCK_PFN_FROM_DPC ();

        BaseVa = MiGetVirtualAddressMappedByPte (StartingPte);

        if (IsLargeSessionAllocation != 0) {
            MiSessionPoolAllocated(MiGetVirtualAddressMappedByPte (StartingPte),
                                   i << PAGE_SHIFT,
                                   NonPagedPool);
        }

        return BaseVa;
    }

    //
    // Paged Pool.
    //

    if ((PoolType & SESSION_POOL_MASK) == 0) {
        SessionSpace = NULL;
        PagedPoolInfo = &MmPagedPoolInfo;

        //
        // If pool is starting to run low then free some paged cache up now.
        // While this can never prevent pool allocations from failing, it does
        // give drivers a better chance to always see success.
        //

        if (MmSizeOfPagedPoolInBytes - (MmPagedPoolInfo.AllocatedPagedPool << PAGE_SHIFT) < 5 * 1024 * 1024) {
            MmPreemptiveTrims[MmPreemptForPaged] += 1;
            MiTrimSegmentCache ();
        }
#if DBG
        if (MiClearCache != 0) {
            MmPreemptiveTrims[MmPreemptForPaged] += 1;
            MiTrimSegmentCache ();
        }
#endif
    }
    else {
        SessionSpace = MmSessionSpace;
        PagedPoolInfo = &SessionSpace->PagedPoolInfo;
    }

    StartPosition = RtlFindClearBitsAndSet (
                               PagedPoolInfo->PagedPoolAllocationMap,
                               (ULONG)SizeInPages,
                               PagedPoolInfo->PagedPoolHint
                               );

    if ((StartPosition == NO_BITS_FOUND) &&
        (PagedPoolInfo->PagedPoolHint != 0)) {

        if (MI_UNUSED_SEGMENTS_SURPLUS()) {
            KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
        }

        //
        // No free bits were found, check from the start of the bit map.

        StartPosition = RtlFindClearBitsAndSet (
                                   PagedPoolInfo->PagedPoolAllocationMap,
                                   (ULONG)SizeInPages,
                                   0
                                   );
    }

    if (StartPosition == NO_BITS_FOUND) {

        //
        // No room in pool - attempt to expand the paged pool.
        //

        StartPosition = (((ULONG)SizeInPages - 1) / PTE_PER_PAGE) + 1;

        //
        // Make sure there is enough space to create the prototype PTEs.
        //

        if (((StartPosition - 1) + PagedPoolInfo->NextPdeForPagedPoolExpansion) >
            MiGetPteAddress (PagedPoolInfo->LastPteForPagedPool)) {

            //
            // Can't expand pool any more.  If this request is not for session
            // pool, force unused segment trimming when appropriate.
            //

            if (SessionSpace == NULL) {

                MmPoolFailures[MmPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmPagedNoPtes] += 1;

                //
                // Running low on pool - force unused segment trimming.
                //
            
                MiTrimSegmentCache ();

                return NULL;
            }

            MmPoolFailures[MmSessionPagedPool][MmHighPriority] += 1;
            MmPoolFailureReasons[MmSessionPagedNoPtes] += 1;

            MmSessionSpace->SessionPoolAllocationFailures[1] += 1;

            return NULL;
        }

        PageTableCount = StartPosition;

        if (SessionSpace) {
            TempPte = ValidKernelPdeLocal;
        }
        else {
            TempPte = ValidKernelPde;
        }

        //
        // Charge commitment for the pagetable pages for paged pool expansion.
        //

        if (MiChargeCommitmentCantExpand (StartPosition, FALSE) == FALSE) {
            if (PsGetCurrentThread()->MemoryMaker == 1) {
                MiChargeCommitmentCantExpand (StartPosition, TRUE);
            }
            else {
                MmPoolFailures[MmPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmPagedNoCommit] += 1;
                MiTrimSegmentCache ();

                return NULL;
            }
        }

        EndPosition = (ULONG)((PagedPoolInfo->NextPdeForPagedPoolExpansion -
                          MiGetPteAddress(PagedPoolInfo->FirstPteForPagedPool)) *
                          PTE_PER_PAGE);

        //
        // Expand the pool.
        //

        RtlClearBits (PagedPoolInfo->PagedPoolAllocationMap,
                      EndPosition,
                      (ULONG) StartPosition * PTE_PER_PAGE);

        PointerPte = PagedPoolInfo->NextPdeForPagedPoolExpansion;
        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
        VirtualAddressSave = VirtualAddress;
        PagedPoolInfo->NextPdeForPagedPoolExpansion += StartPosition;

        LOCK_PFN (OldIrql);

        //
        // Make sure we have 1 more than the number of pages
        // requested available.
        //

        if (MmAvailablePages <= StartPosition) {

            //
            // There are no free physical pages to expand paged pool.
            //

            UNLOCK_PFN (OldIrql);

            PagedPoolInfo->NextPdeForPagedPoolExpansion -= StartPosition;

            RtlSetBits (PagedPoolInfo->PagedPoolAllocationMap,
                        EndPosition,
                        (ULONG) StartPosition * PTE_PER_PAGE);

            MiReturnCommitment (StartPosition);

            if (SessionSpace == NULL) {
                MmPoolFailures[MmPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmPagedNoPagesAvailable] += 1;
            }
            else {
                MmPoolFailures[MmSessionPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmSessionPagedNoPagesAvailable] += 1;
                MmSessionSpace->SessionPoolAllocationFailures[2] += 1;
            }

            return NULL;
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGED_POOL_PAGETABLE, StartPosition);

        //
        // Update the count of available resident pages.
        //

        MmResidentAvailablePages -= StartPosition;
        MM_BUMP_COUNTER(1, StartPosition);

        //
        // Allocate the page table pages for the pool expansion.
        //

        do {
            ASSERT (PointerPte->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage (
                                MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            //
            // Map valid PDE into system (or session) address space.
            //

#if (_MI_PAGING_LEVELS >= 3)

            MiInitializePfn (PageFrameIndex, PointerPte, 1);

#else

            if (SessionSpace) {

                Index = (ULONG)(PointerPte - MiGetPdeAddress (MmSessionBase));
                ASSERT (MmSessionSpace->PageTables[Index].u.Long == 0);
                MmSessionSpace->PageTables[Index] = TempPte;

                MiInitializePfnForOtherProcess (PageFrameIndex,
                                                PointerPte,
                                                MmSessionSpace->SessionPageDirectoryIndex);

                MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC1, 1);
            }
            else {
                MmSystemPagePtes [((ULONG_PTR)PointerPte &
                    (PD_PER_SYSTEM * (sizeof(MMPTE) * PDE_PER_PAGE) - 1)) / sizeof(MMPTE)] = TempPte;
                MiInitializePfnForOtherProcess (PageFrameIndex,
                                                PointerPte,
                                                MmSystemPageDirectory[(PointerPte - MiGetPdeAddress(0)) / PDE_PER_PAGE]);
            }
#endif

            KeFillEntryTb ((PHARDWARE_PTE) PointerPte, VirtualAddress, FALSE);

            PointerPte += 1;
            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            StartPosition -= 1;

        } while (StartPosition > 0);

        UNLOCK_PFN (OldIrql);

        MiFillMemoryPte (VirtualAddressSave,
                         PageTableCount * PAGE_SIZE,
                         MM_KERNEL_NOACCESS_PTE);

        if (SessionSpace) {

            PointerPte -= PageTableCount;

            InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                         PageTableCount);

            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_PAGETABLE_ALLOC, PageTableCount);
            Thread = PsGetCurrentThread ();

            LOCK_SESSION_SPACE_WS (SessionIrql, Thread);

            MmSessionSpace->NonPagablePages += PageTableCount;

            do {
                Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
    
                ASSERT (Pfn1->u1.Event == 0);
                Pfn1->u1.Event = (PVOID) Thread;
    
                SessionPte = MiGetVirtualAddressMappedByPte (PointerPte);
    
                MiAddValidPageToWorkingSet (SessionPte,
                                            PointerPte,
                                            Pfn1,
                                            0);
    
                WsEntry = MiLocateWsle (SessionPte,
                                        MmSessionSpace->Vm.VmWorkingSetList,
                                        Pfn1->u1.WsIndex);
    
                if (WsEntry >= MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic) {
        
                    WsSwapEntry = MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic;
        
                    if (WsEntry != MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic) {
        
                        //
                        // Swap this entry with the one at first dynamic.
                        //
        
                        MiSwapWslEntries (WsEntry, WsSwapEntry, &MmSessionSpace->Vm);
                    }
        
                    MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic += 1;
                }
                else {
                    WsSwapEntry = WsEntry;
                }
        
                //
                // Indicate that the page is locked.
                //
        
                MmSessionSpace->Wsle[WsSwapEntry].u1.e1.LockedInWs = 1;
    
                PointerPte += 1;
                PageTableCount -= 1;
            } while (PageTableCount > 0);
            UNLOCK_SESSION_SPACE_WS (SessionIrql);
        }

        StartPosition = RtlFindClearBitsAndSet (
                                   PagedPoolInfo->PagedPoolAllocationMap,
                                   (ULONG)SizeInPages,
                                   EndPosition
                                   );

        ASSERT (StartPosition != NO_BITS_FOUND);
    }

    //
    // This is paged pool, the start and end can't be saved
    // in the PFN database as the page isn't always resident
    // in memory.  The ideal place to save the start and end
    // would be in the prototype PTE, but there are no free
    // bits.  To solve this problem, a bitmap which parallels
    // the allocation bitmap exists which contains set bits
    // in the positions where an allocation ends.  This
    // allows pages to be deallocated with only their starting
    // address.
    //
    // For sanity's sake, the starting address can be verified
    // from the 2 bitmaps as well.  If the page before the starting
    // address is not allocated (bit is zero in allocation bitmap)
    // then this page is obviously a start of an allocation block.
    // If the page before is allocated and the other bit map does
    // not indicate the previous page is the end of an allocation,
    // then the starting address is wrong and a bug check should
    // be issued.
    //

    if (SizeInPages == 1) {
        PagedPoolInfo->PagedPoolHint = StartPosition + (ULONG)SizeInPages;
    }

    //
    // If paged pool has been configured as nonpagable, commitment has
    // already been charged so just set the length and return the address.
    //

    if ((MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) &&
        (SessionSpace == NULL)) {

        BaseVa = (PVOID)((PUCHAR)MmPageAlignedPoolBase[PagedPool] +
                                ((ULONG_PTR)StartPosition << PAGE_SHIFT));

#if DBG
        PointerPte = MiGetPteAddress (BaseVa);
        for (i = 0; i < SizeInPages; i += 1) {
            ASSERT (PointerPte->u.Hard.Valid == 1);
            PointerPte += 1;
        }
#endif
    
        EndPosition = StartPosition + (ULONG)SizeInPages - 1;
        RtlSetBit (PagedPoolInfo->EndOfPagedPoolBitmap, EndPosition);
    
        if (PoolType & POOL_VERIFIER_MASK) {
            RtlSetBit (VerifierLargePagedPoolMap, StartPosition);
        }
    
        PagedPoolInfo->AllocatedPagedPool += SizeInPages;
    
        return BaseVa;
    }

    if (MiChargeCommitmentCantExpand (SizeInPages, FALSE) == FALSE) {
        if (PsGetCurrentThread()->MemoryMaker == 1) {
            MiChargeCommitmentCantExpand (SizeInPages, TRUE);
        }
        else {
            RtlClearBits (PagedPoolInfo->PagedPoolAllocationMap,
                          StartPosition,
                          (ULONG)SizeInPages);
    
            //
            // Could not commit the page(s), return NULL indicating
            // no pool was allocated.  Note that the lack of commit may be due
            // to unused segments and the MmSharedCommit, prototype PTEs, etc
            // associated with them.  So force a reduction now.
            //
    
            MiIssuePageExtendRequestNoWait (SizeInPages);

            MiTrimSegmentCache ();

            if (SessionSpace == NULL) {
                MmPoolFailures[MmPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmPagedNoCommit] += 1;
            }
            else {
                MmPoolFailures[MmSessionPagedPool][MmHighPriority] += 1;
                MmPoolFailureReasons[MmSessionPagedNoCommit] += 1;
                MmSessionSpace->SessionPoolAllocationFailures[3] += 1;
            }

            return NULL;
        }
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGED_POOL_PAGES, SizeInPages);

    if (SessionSpace) {
        InterlockedExchangeAddSizeT (&SessionSpace->CommittedPages,
                                     SizeInPages);
        MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_PAGEDPOOL_PAGES, (ULONG)SizeInPages);
        BaseVa = (PVOID)((PCHAR)SessionSpace->PagedPoolStart +
                                ((ULONG_PTR)StartPosition << PAGE_SHIFT));
    }
    else {
        MmPagedPoolCommit += (ULONG)SizeInPages;
        BaseVa = (PVOID)((PUCHAR)MmPageAlignedPoolBase[PagedPool] +
                                ((ULONG_PTR)StartPosition << PAGE_SHIFT));
    }

#if DBG
    PointerPte = MiGetPteAddress (BaseVa);
    for (i = 0; i < SizeInPages; i += 1) {
        if (*(ULONG *)PointerPte != MM_KERNEL_NOACCESS_PTE) {
            DbgPrint("MiAllocatePoolPages: PP not zero PTE (%p %p %p)\n",
                BaseVa, PointerPte, *PointerPte);
            DbgBreakPoint();
        }
        PointerPte += 1;
    }
#endif
    PointerPte = MiGetPteAddress (BaseVa);
    MiFillMemoryPte (PointerPte,
                     SizeInPages * sizeof(MMPTE),
                     MM_KERNEL_DEMAND_ZERO_PTE);

    PagedPoolInfo->PagedPoolCommit += SizeInPages;
    EndPosition = StartPosition + (ULONG)SizeInPages - 1;
    RtlSetBit (PagedPoolInfo->EndOfPagedPoolBitmap, EndPosition);

    //
    // Mark this as a large session allocation in the PFN database.
    //

    if (IsLargeSessionAllocation != 0) {
        RtlSetBit (PagedPoolInfo->PagedPoolLargeSessionAllocationMap,
                   StartPosition);

        MiSessionPoolAllocated (BaseVa,
                                SizeInPages << PAGE_SHIFT,
                                PagedPool);
    }
    else if (PoolType & POOL_VERIFIER_MASK) {
        RtlSetBit (VerifierLargePagedPoolMap, StartPosition);
    }

    PagedPoolInfo->AllocatedPagedPool += SizeInPages;

    return BaseVa;
}

ULONG
MiFreePoolPages (
    IN PVOID StartingAddress
    )

/*++

Routine Description:

    This function returns a set of pages back to the pool from
    which they were obtained.  Once the pages have been deallocated
    the region provided by the allocation becomes available for
    allocation to other callers, i.e. any data in the region is now
    trashed and cannot be referenced.

Arguments:

    StartingAddress - Supplies the starting address which was returned
                      in a previous call to MiAllocatePoolPages.

Return Value:

    Returns the number of pages deallocated.

Environment:

    These functions are used by the general pool allocation routines
    and should not be called directly.

    Mutexes guarding the pool databases must be held when calling
    these functions.

--*/

{
    ULONG StartPosition;
    ULONG Index;
    PFN_NUMBER i;
    PFN_NUMBER NumberOfPages;
    POOL_TYPE PoolType;
    PMMPTE PointerPte;
    PMMPTE StartPte;
    PMMPFN Pfn1;
    PMMPFN StartPfn;
    PMMFREE_POOL_ENTRY Entry;
    PMMFREE_POOL_ENTRY NextEntry;
    PMMFREE_POOL_ENTRY LastEntry;
    PMM_PAGED_POOL_INFO PagedPoolInfo;
    PMM_SESSION_SPACE SessionSpace;
    LOGICAL SessionAllocation;
    LOGICAL AddressIsPhysical;
    MMPTE LocalNoAccessPte;
    PFN_NUMBER PagesFreed;
    MMPFNENTRY OriginalPfnFlags;
    ULONG_PTR VerifierAllocation;
    PULONG BitMap;
#if DBG
    PMMPTE DebugPte;
    PMMPFN DebugPfn;
    PMMPFN LastDebugPfn;
#endif

    //
    // Determine Pool type base on the virtual address of the block
    // to deallocate.
    //
    // This assumes NonPagedPool starts at a higher virtual address
    // then PagedPool.
    //

    if ((StartingAddress >= MmPagedPoolStart) &&
        (StartingAddress <= MmPagedPoolEnd)) {
        PoolType = PagedPool;
        SessionSpace = NULL;
        PagedPoolInfo = &MmPagedPoolInfo;
        StartPosition = (ULONG)(((PCHAR)StartingAddress -
                          (PCHAR)MmPageAlignedPoolBase[PoolType]) >> PAGE_SHIFT);
    }
    else if (MI_IS_SESSION_POOL_ADDRESS (StartingAddress) == TRUE) {
        PoolType = PagedPool;
        SessionSpace = MmSessionSpace;
        ASSERT (SessionSpace);
        PagedPoolInfo = &SessionSpace->PagedPoolInfo;
        StartPosition = (ULONG)(((PCHAR)StartingAddress -
                          (PCHAR)SessionSpace->PagedPoolStart) >> PAGE_SHIFT);
    }
    else {

        if (StartingAddress < MM_SYSTEM_RANGE_START) {
            KeBugCheckEx (BAD_POOL_CALLER,
                          0x40,
                          (ULONG_PTR)StartingAddress,
                          (ULONG_PTR)MM_SYSTEM_RANGE_START,
                          0);
        }

        PoolType = NonPagedPool;
        SessionSpace = NULL;
        PagedPoolInfo = &MmPagedPoolInfo;
        StartPosition = (ULONG)(((PCHAR)StartingAddress -
                          (PCHAR)MmPageAlignedPoolBase[PoolType]) >> PAGE_SHIFT);
    }

    //
    // Check to ensure this page is really the start of an allocation.
    //

    if (PoolType == NonPagedPool) {

        //
        // The nonpaged pool being freed may be the target of a delayed unlock.
        // Since these pages may be immediately released, force any pending
        // delayed actions to occur now.
        //

#if !defined(MI_MULTINODE)
        if (MmPfnDeferredList != NULL) {
            MiDeferredUnlockPages (0);
        }
#else
        //
        // Each and every node's deferred list would have to be checked so
        // we might as well go the long way and just call.
        //

        MiDeferredUnlockPages (0);
#endif

        if (MI_IS_PHYSICAL_ADDRESS (StartingAddress)) {

            //
            // On certain architectures, virtual addresses
            // may be physical and hence have no corresponding PTE.
            //

            Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (StartingAddress));
            ASSERT (StartPosition < MmExpandedPoolBitPosition);
            AddressIsPhysical = TRUE;

            //
            // Initializing PointerPte & StartPte is not needed for correctness
            // but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            PointerPte = NULL;
            StartPte = NULL;

            if ((StartingAddress < MmNonPagedPoolStart) ||
                (StartingAddress >= MmNonPagedPoolEnd0)) {
                KeBugCheckEx (BAD_POOL_CALLER,
                              0x42,
                              (ULONG_PTR)StartingAddress,
                              0,
                              0);
            }
        }
        else {
            PointerPte = MiGetPteAddress (StartingAddress);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            AddressIsPhysical = FALSE;
            StartPte = PointerPte;

            if (((StartingAddress >= MmNonPagedPoolExpansionStart) &&
                (StartingAddress < MmNonPagedPoolEnd)) ||
                ((StartingAddress >= MmNonPagedPoolStart) &&
                (StartingAddress < MmNonPagedPoolEnd0))) {
                    NOTHING;
            }
            else {
                KeBugCheckEx (BAD_POOL_CALLER,
                              0x43,
                              (ULONG_PTR)StartingAddress,
                              0,
                              0);
            }
        }

        if (Pfn1->u3.e1.StartOfAllocation == 0) {
            KeBugCheckEx (BAD_POOL_CALLER,
                          0x41,
                          (ULONG_PTR)StartingAddress,
                          (ULONG_PTR)(Pfn1 - MmPfnDatabase),
                          MmHighestPhysicalPage);
        }

        StartPfn = Pfn1;

        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        OriginalPfnFlags = Pfn1->u3.e1;
        VerifierAllocation = Pfn1->u4.VerifierAllocation;

        Pfn1->u3.e1.StartOfAllocation = 0;
        Pfn1->u3.e1.LargeSessionAllocation = 0;
        Pfn1->u4.VerifierAllocation = 0;

#if DBG
        if ((Pfn1->u3.e2.ReferenceCount > 1) &&
            (Pfn1->u3.e1.WriteInProgress == 0)) {
            DbgPrint ("MM: MiFreePoolPages - deleting pool locked for I/O %p\n",
                 Pfn1);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        }
#endif

        //
        // Find end of allocation and release the pages.
        //

        if (AddressIsPhysical == TRUE) {
            while (Pfn1->u3.e1.EndOfAllocation == 0) {
                Pfn1 += 1;
#if DBG
                if ((Pfn1->u3.e2.ReferenceCount > 1) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {
                        DbgPrint ("MM:MiFreePoolPages - deleting pool locked for I/O %p\n", Pfn1);
                    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
                }
#endif
            }
            NumberOfPages = Pfn1 - StartPfn + 1;
        }
        else {
            while (Pfn1->u3.e1.EndOfAllocation == 0) {
                PointerPte += 1;
                Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
#if DBG
                if ((Pfn1->u3.e2.ReferenceCount > 1) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {
                        DbgPrint ("MM:MiFreePoolPages - deleting pool locked for I/O %p\n", Pfn1);
                    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
                }
#endif
            }
            NumberOfPages = PointerPte - StartPte + 1;
        }

        MmAllocatedNonPagedPool -= NumberOfPages;

        if (VerifierAllocation != 0) {
            VerifierFreeTrackedPool (StartingAddress,
                                     NumberOfPages << PAGE_SHIFT,
                                     NonPagedPool,
                                     FALSE);
        }

        if (OriginalPfnFlags.LargeSessionAllocation != 0) {
            MiSessionPoolFreed (StartingAddress,
                                NumberOfPages << PAGE_SHIFT,
                                NonPagedPool);
        }

        Pfn1->u3.e1.EndOfAllocation = 0;

#if DBG
        if (MiFillFreedPool != 0) {
            RtlFillMemoryUlong (StartingAddress,
                                PAGE_SIZE * NumberOfPages,
                                MiFillFreedPool);
        }
#endif

        if (StartingAddress > MmNonPagedPoolExpansionStart) {

            //
            // This page was from the expanded pool, should
            // it be freed?
            //
            // NOTE: all pages in the expanded pool area have PTEs
            // so no physical address checks need to be performed.
            //

            if ((NumberOfPages > 3) ||
                (MmNumberOfFreeNonPagedPool > 5) ||
                ((MmResidentAvailablePages < 200) &&
                 (MiExpansionPoolPagesInUse > MiExpansionPoolPagesInitialCharge))) {

                //
                // Free these pages back to the free page list.
                //

                MiFreeNonPagedPool (StartingAddress, NumberOfPages);

                return (ULONG)NumberOfPages;
            }
        }

        //
        // Add the pages to the list of free pages.
        //

        MmNumberOfFreeNonPagedPool += NumberOfPages;

        //
        // Check to see if the next allocation is free.
        // We cannot walk off the end of nonpaged expansion
        // pages as the highest expansion allocation is always
        // virtual and guard-paged.
        //

        i = NumberOfPages;

        ASSERT (MiEndOfInitialPoolFrame != 0);

        if ((PFN_NUMBER)(Pfn1 - MmPfnDatabase) == MiEndOfInitialPoolFrame) {
            PointerPte += 1;
            Pfn1 = NULL;
        }
        else if (AddressIsPhysical == TRUE) {
            Pfn1 += 1;
            ASSERT ((PCHAR)StartingAddress + NumberOfPages < (PCHAR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes);
        }
        else {
            PointerPte += 1;
            ASSERT ((PCHAR)StartingAddress + NumberOfPages <= (PCHAR)MmNonPagedPoolEnd);

            //
            // Unprotect the previously freed pool so it can be merged.
            //

            if (MmProtectFreedNonPagedPool == TRUE) {
                MiUnProtectFreeNonPagedPool (
                    (PVOID)MiGetVirtualAddressMappedByPte(PointerPte),
                    0);
            }

            if (PointerPte->u.Hard.Valid == 1) {
                Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            }
            else {
                Pfn1 = NULL;
            }
        }

        if ((Pfn1 != NULL) && (Pfn1->u3.e1.StartOfAllocation == 0)) {

            //
            // This range of pages is free.  Remove this entry
            // from the list and add these pages to the current
            // range being freed.
            //

            Entry = (PMMFREE_POOL_ENTRY)((PCHAR)StartingAddress
                                        + (NumberOfPages << PAGE_SHIFT));
            ASSERT (Entry->Signature == MM_FREE_POOL_SIGNATURE);
            ASSERT (Entry->Owner == Entry);

#if DBG
            if (AddressIsPhysical == TRUE) {

                ASSERT (MI_IS_PHYSICAL_ADDRESS(StartingAddress));

                //
                // On certain architectures, virtual addresses
                // may be physical and hence have no corresponding PTE.
                //

                DebugPfn = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (Entry));
                DebugPfn += Entry->Size;
                if ((PFN_NUMBER)((DebugPfn - 1) - MmPfnDatabase) != MiEndOfInitialPoolFrame) {
                    ASSERT (DebugPfn->u3.e1.StartOfAllocation == 1);
                }
            }
            else {
                DebugPte = PointerPte + Entry->Size;
                if ((DebugPte-1)->u.Hard.Valid == 1) {
                    DebugPfn = MI_PFN_ELEMENT ((DebugPte-1)->u.Hard.PageFrameNumber);
                    if ((PFN_NUMBER)(DebugPfn - MmPfnDatabase) != MiEndOfInitialPoolFrame) {
                        if (DebugPte->u.Hard.Valid == 1) {
                            DebugPfn = MI_PFN_ELEMENT (DebugPte->u.Hard.PageFrameNumber);
                            ASSERT (DebugPfn->u3.e1.StartOfAllocation == 1);
                        }
                    }

                }
            }
#endif

            i += Entry->Size;
            if (MmProtectFreedNonPagedPool == FALSE) {
                RemoveEntryList (&Entry->List);
            }
            else {
                MiProtectedPoolRemoveEntryList (&Entry->List);
            }
        }

        //
        // Check to see if the previous page is the end of an allocation.
        // If it is not the end of an allocation, it must be free and
        // therefore this allocation can be tagged onto the end of
        // that allocation.
        //
        // We cannot walk off the beginning of expansion pool because it is
        // guard-paged.  If the initial pool is superpaged instead, we are also
        // safe as the must succeed pages always have EndOfAllocation set.
        //

        Entry = (PMMFREE_POOL_ENTRY)StartingAddress;

        ASSERT (MiStartOfInitialPoolFrame != 0);

        if ((PFN_NUMBER)(StartPfn - MmPfnDatabase) == MiStartOfInitialPoolFrame) {
            Pfn1 = NULL;
        }
        else if (AddressIsPhysical == TRUE) {
            ASSERT (MI_IS_PHYSICAL_ADDRESS(StartingAddress));
            ASSERT (StartingAddress != MmNonPagedPoolStart);

            Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (
                                    (PVOID)((PCHAR)Entry - PAGE_SIZE)));

        }
        else {
            PointerPte -= NumberOfPages + 1;

            //
            // Unprotect the previously freed pool so it can be merged.
            //

            if (MmProtectFreedNonPagedPool == TRUE) {
                MiUnProtectFreeNonPagedPool (
                    (PVOID)MiGetVirtualAddressMappedByPte(PointerPte),
                    0);
            }

            if (PointerPte->u.Hard.Valid == 1) {
                Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            }
            else {
                Pfn1 = NULL;
            }
        }
        if (Pfn1 != NULL) {
            if (Pfn1->u3.e1.EndOfAllocation == 0) {

                //
                // This range of pages is free, add these pages to
                // this entry.  The owner field points to the address
                // of the list entry which is linked into the free pool
                // pages list.
                //

                Entry = (PMMFREE_POOL_ENTRY)((PCHAR)StartingAddress - PAGE_SIZE);
                ASSERT (Entry->Signature == MM_FREE_POOL_SIGNATURE);
                Entry = Entry->Owner;

                //
                // Unprotect the previously freed pool so we can merge it
                //

                if (MmProtectFreedNonPagedPool == TRUE) {
                    MiUnProtectFreeNonPagedPool ((PVOID)Entry, 0);
                }

                //
                // If this entry became larger than MM_SMALL_ALLOCATIONS
                // pages, move it to the tail of the list.  This keeps the
                // small allocations at the front of the list.
                //

                if (Entry->Size < MI_MAX_FREE_LIST_HEADS - 1) {

                    if (MmProtectFreedNonPagedPool == FALSE) {
                        RemoveEntryList (&Entry->List);
                    }
                    else {
                        MiProtectedPoolRemoveEntryList (&Entry->List);
                    }

                    //
                    // Add these pages to the previous entry.
                    //
    
                    Entry->Size += i;

                    Index = (ULONG)(Entry->Size - 1);
            
                    if (Index >= MI_MAX_FREE_LIST_HEADS) {
                        Index = MI_MAX_FREE_LIST_HEADS - 1;
                    }

                    if (MmProtectFreedNonPagedPool == FALSE) {
                        InsertTailList (&MmNonPagedPoolFreeListHead[Index],
                                        &Entry->List);
                    }
                    else {
                        MiProtectedPoolInsertList (&MmNonPagedPoolFreeListHead[Index],
                                          &Entry->List,
                                          Entry->Size < MM_SMALL_ALLOCATIONS ?
                                              TRUE : FALSE);
                    }
                }
                else {

                    //
                    // Add these pages to the previous entry.
                    //
    
                    Entry->Size += i;
                }
            }
        }

        if (Entry == (PMMFREE_POOL_ENTRY)StartingAddress) {

            //
            // This entry was not combined with the previous, insert it
            // into the list.
            //

            Entry->Size = i;

            Index = (ULONG)(Entry->Size - 1);
    
            if (Index >= MI_MAX_FREE_LIST_HEADS) {
                Index = MI_MAX_FREE_LIST_HEADS - 1;
            }

            if (MmProtectFreedNonPagedPool == FALSE) {
                InsertTailList (&MmNonPagedPoolFreeListHead[Index],
                                &Entry->List);
            }
            else {
                MiProtectedPoolInsertList (&MmNonPagedPoolFreeListHead[Index],
                                      &Entry->List,
                                      Entry->Size < MM_SMALL_ALLOCATIONS ?
                                          TRUE : FALSE);
            }
        }

        //
        // Set the owner field in all these pages.
        //

        ASSERT (i != 0);
        NextEntry = (PMMFREE_POOL_ENTRY)StartingAddress;
        LastEntry = (PMMFREE_POOL_ENTRY)((PCHAR)NextEntry + (i << PAGE_SHIFT));

        do {
            NextEntry->Owner = Entry;
#if DBG
            NextEntry->Signature = MM_FREE_POOL_SIGNATURE;
#endif

            NextEntry = (PMMFREE_POOL_ENTRY)((PCHAR)NextEntry + PAGE_SIZE);
        } while (NextEntry != LastEntry);

#if DBG
        NextEntry = Entry;

        if (AddressIsPhysical == TRUE) {
            ASSERT (MI_IS_PHYSICAL_ADDRESS(StartingAddress));
            DebugPfn = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (NextEntry));
            LastDebugPfn = DebugPfn + Entry->Size;

            for ( ; DebugPfn < LastDebugPfn; DebugPfn += 1) {
                ASSERT ((DebugPfn->u3.e1.StartOfAllocation == 0) &&
                        (DebugPfn->u3.e1.EndOfAllocation == 0));
                ASSERT (NextEntry->Owner == Entry);
                NextEntry = (PMMFREE_POOL_ENTRY)((PCHAR)NextEntry + PAGE_SIZE);
            }
        }
        else {

            for (i = 0; i < Entry->Size; i += 1) {

                DebugPte = MiGetPteAddress (NextEntry);
                DebugPfn = MI_PFN_ELEMENT (DebugPte->u.Hard.PageFrameNumber);
                ASSERT ((DebugPfn->u3.e1.StartOfAllocation == 0) &&
                        (DebugPfn->u3.e1.EndOfAllocation == 0));
                ASSERT (NextEntry->Owner == Entry);
                NextEntry = (PMMFREE_POOL_ENTRY)((PCHAR)NextEntry + PAGE_SIZE);
            }
        }
#endif

        //
        // Prevent anyone from touching non paged pool after freeing it.
        //

        if (MmProtectFreedNonPagedPool == TRUE) {
            MiProtectFreeNonPagedPool ((PVOID)Entry, (ULONG)Entry->Size);
        }

        return (ULONG)NumberOfPages;
    }

    //
    // Paged pool.  Need to verify start of allocation using
    // end of allocation bitmap.
    //

    if (!RtlCheckBit (PagedPoolInfo->PagedPoolAllocationMap, StartPosition)) {
        KeBugCheckEx (BAD_POOL_CALLER,
                      0x50,
                      (ULONG_PTR)StartingAddress,
                      (ULONG_PTR)StartPosition,
                      MmSizeOfPagedPoolInBytes);
    }

#if DBG
    if (StartPosition > 0) {
        if (RtlCheckBit (PagedPoolInfo->PagedPoolAllocationMap, StartPosition - 1)) {
            if (!RtlCheckBit (PagedPoolInfo->EndOfPagedPoolBitmap, StartPosition - 1)) {

                //
                // In the middle of an allocation... bugcheck.
                //

                DbgPrint("paged pool in middle of allocation\n");
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x41286,
                              (ULONG_PTR)PagedPoolInfo->PagedPoolAllocationMap,
                              (ULONG_PTR)PagedPoolInfo->EndOfPagedPoolBitmap,
                              StartPosition);
            }
        }
    }
#endif

    i = StartPosition;
    PointerPte = PagedPoolInfo->FirstPteForPagedPool + i;

    //
    // Find the last allocated page and check to see if any
    // of the pages being deallocated are in the paging file.
    //

    BitMap = PagedPoolInfo->EndOfPagedPoolBitmap->Buffer;

    while (!MI_CHECK_BIT (BitMap, i)) {
        i += 1;
    }

    NumberOfPages = i - StartPosition + 1;

    if (SessionSpace) {

        //
        // This is needed purely to verify no one leaks pool.  This
        // could be removed if we believe everyone was good.
        //

        BitMap = PagedPoolInfo->PagedPoolLargeSessionAllocationMap->Buffer;

        if (MI_CHECK_BIT (BitMap, StartPosition)) {

            MI_CLEAR_BIT (BitMap, StartPosition);

            MiSessionPoolFreed (MiGetVirtualAddressMappedByPte (PointerPte),
                                NumberOfPages << PAGE_SHIFT,
                                PagedPool);
        }

        SessionAllocation = TRUE;
    }
    else {
        SessionAllocation = FALSE;

        if (VerifierLargePagedPoolMap) {

            BitMap = VerifierLargePagedPoolMap->Buffer;

            if (MI_CHECK_BIT (BitMap, StartPosition)) {

                MI_CLEAR_BIT (BitMap, StartPosition);

                VerifierFreeTrackedPool (MiGetVirtualAddressMappedByPte (PointerPte),
                                         NumberOfPages << PAGE_SHIFT,
                                         PagedPool,
                                         FALSE);
            }
        }

        //
        // If paged pool has been configured as nonpagable, only
        // virtual address space is released.
        //
        
        if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

            //
            // Clear the end of allocation bit in the bit map.
            //
    
            RtlClearBit (PagedPoolInfo->EndOfPagedPoolBitmap, (ULONG)i);
    
            PagedPoolInfo->AllocatedPagedPool -= NumberOfPages;
        
            //
            // Clear the allocation bits in the bit map.
            //
        
            RtlClearBits (PagedPoolInfo->PagedPoolAllocationMap,
                          StartPosition,
                          (ULONG)NumberOfPages);
        
            if (StartPosition < PagedPoolInfo->PagedPoolHint) {
                PagedPoolInfo->PagedPoolHint = StartPosition;
            }
        
            return (ULONG)NumberOfPages;
        }
    }

    LocalNoAccessPte.u.Long = MM_KERNEL_NOACCESS_PTE;

    PagesFreed = MiDeleteSystemPagableVm (PointerPte,
                                          NumberOfPages,
                                          LocalNoAccessPte,
                                          SessionAllocation,
                                          NULL);

    ASSERT (PagesFreed == NumberOfPages);

    if (SessionSpace) {
        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - NumberOfPages);
   
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_COMMIT_POOL_FREED,
                              (ULONG)NumberOfPages);
    }
    else {
        MmPagedPoolCommit -= (ULONG)NumberOfPages;
    }

    MiReturnCommitment (NumberOfPages);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PAGED_POOL_PAGES, NumberOfPages);

    //
    // Clear the end of allocation bit in the bit map.
    //

    BitMap = PagedPoolInfo->EndOfPagedPoolBitmap->Buffer;
    MI_CLEAR_BIT (BitMap, i);

    PagedPoolInfo->PagedPoolCommit -= NumberOfPages;
    PagedPoolInfo->AllocatedPagedPool -= NumberOfPages;

    //
    // Clear the allocation bits in the bit map.
    //

    RtlClearBits (PagedPoolInfo->PagedPoolAllocationMap,
                  StartPosition,
                  (ULONG)NumberOfPages);

    if (StartPosition < PagedPoolInfo->PagedPoolHint) {
        PagedPoolInfo->PagedPoolHint = StartPosition;
    }

    return (ULONG)NumberOfPages;
}

VOID
MiInitializeNonPagedPool (
    VOID
    )

/*++

Routine Description:

    This function initializes the NonPaged pool.

    NonPaged Pool is linked together through the pages.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, during initialization.

--*/

{
    ULONG PagesInPool;
    ULONG Size;
    ULONG Index;
    PMMFREE_POOL_ENTRY FreeEntry;
    PMMFREE_POOL_ENTRY FirstEntry;
    PMMPTE PointerPte;
    PVOID EndOfInitialPool;
    PFN_NUMBER PageFrameIndex;

    PAGED_CODE();

    //
    // Initialize the list heads for free pages.
    //

    for (Index = 0; Index < MI_MAX_FREE_LIST_HEADS; Index += 1) {
        InitializeListHead (&MmNonPagedPoolFreeListHead[Index]);
    }

    //
    // Set up the non paged pool pages.
    //

    FreeEntry = (PMMFREE_POOL_ENTRY) MmNonPagedPoolStart;
    FirstEntry = FreeEntry;

    PagesInPool = BYTES_TO_PAGES (MmSizeOfNonPagedPoolInBytes);

    //
    // Set the location of expanded pool.
    //

    MmExpandedPoolBitPosition = BYTES_TO_PAGES (MmSizeOfNonPagedPoolInBytes);

    MmNumberOfFreeNonPagedPool = PagesInPool;

    Index = (ULONG)(MmNumberOfFreeNonPagedPool - 1);
    if (Index >= MI_MAX_FREE_LIST_HEADS) {
        Index = MI_MAX_FREE_LIST_HEADS - 1;
    }

    InsertHeadList (&MmNonPagedPoolFreeListHead[Index], &FreeEntry->List);

    FreeEntry->Size = PagesInPool;
#if DBG
    FreeEntry->Signature = MM_FREE_POOL_SIGNATURE;
#endif
    FreeEntry->Owner = FirstEntry;

    while (PagesInPool > 1) {
        FreeEntry = (PMMFREE_POOL_ENTRY)((PCHAR)FreeEntry + PAGE_SIZE);
#if DBG
        FreeEntry->Signature = MM_FREE_POOL_SIGNATURE;
#endif
        FreeEntry->Owner = FirstEntry;
        PagesInPool -= 1;
    }

    //
    // Initialize the first nonpaged pool PFN.
    //

    if (MI_IS_PHYSICAL_ADDRESS(MmNonPagedPoolStart)) {
        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (MmNonPagedPoolStart);
    }
    else {
        PointerPte = MiGetPteAddress(MmNonPagedPoolStart);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    }
    MiStartOfInitialPoolFrame = PageFrameIndex;

    //
    // Set the last nonpaged pool PFN so coalescing on free doesn't go
    // past the end of the initial pool.
    //


    MmNonPagedPoolEnd0 = (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes);
    EndOfInitialPool = (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes - 1);

    if (MI_IS_PHYSICAL_ADDRESS(EndOfInitialPool)) {
        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (EndOfInitialPool);
    }
    else {
        PointerPte = MiGetPteAddress(EndOfInitialPool);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    }
    MiEndOfInitialPoolFrame = PageFrameIndex;

    //
    // Set up the system PTEs for nonpaged pool expansion.
    //

    PointerPte = MiGetPteAddress (MmNonPagedPoolExpansionStart);
    ASSERT (PointerPte->u.Hard.Valid == 0);

    Size = BYTES_TO_PAGES(MmMaximumNonPagedPoolInBytes -
                            MmSizeOfNonPagedPoolInBytes);

    //
    // Insert a guard PTE at the bottom of expanded nonpaged pool.
    //

    Size -= 1;
    PointerPte += 1;

    ASSERT (MiExpansionPoolPagesInUse == 0);

    //
    // Initialize the nonpaged pool expansion resident available initial charge.
    // Note that MmResidentAvailablePages & MmAvailablePages are not initialized
    // yet, but this amount is subtracted when MmResidentAvailablePages is
    // initialized later.
    //

    MiExpansionPoolPagesInitialCharge = Size;
    if (Size > MmNumberOfPhysicalPages / 6) {
        MiExpansionPoolPagesInitialCharge = MmNumberOfPhysicalPages / 6;
    }

    MiInitializeSystemPtes (PointerPte, Size, NonPagedPoolExpansion);

    //
    // A guard PTE is built at the top by our caller.  This allows us to
    // freely increment virtual addresses in MiFreePoolPages and just check
    // for a blank PTE.
    //
}

#if DBG || (i386 && !FPO)

//
// This only works on checked builds, because the TraceLargeAllocs array is
// kept in that case to keep track of page size pool allocations.  Otherwise
// we will call ExpSnapShotPoolPages with a page size pool allocation containing
// arbitrary data and it will potentially go off in the weeds trying to
// interpret it as a suballocated pool page.  Ideally, there would be another
// bit map that identified single page pool allocations so
// ExpSnapShotPoolPages would NOT be called for those.
//

NTSTATUS
MmSnapShotPool(
    IN POOL_TYPE PoolType,
    IN PMM_SNAPSHOT_POOL_PAGE SnapShotPoolPage,
    IN PSYSTEM_POOL_INFORMATION PoolInformation,
    IN ULONG Length,
    IN OUT PULONG RequiredLength
    )
{
    ULONG Index;
    NTSTATUS Status;
    NTSTATUS xStatus;
    PCHAR p, pStart;
    ULONG Size;
    ULONG BusyFlag;
    ULONG CurrentPage, NumberOfPages;
    PSYSTEM_POOL_ENTRY PoolEntryInfo;
    PLIST_ENTRY Entry;
    PMMFREE_POOL_ENTRY FreePageInfo;
    ULONG StartPosition;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    LOGICAL NeedsReprotect;

    Status = STATUS_SUCCESS;
    PoolEntryInfo = &PoolInformation->Entries[0];

    if (PoolType == PagedPool) {
        PoolInformation->TotalSize = (PCHAR)MmPagedPoolEnd -
                                     (PCHAR)MmPagedPoolStart;
        PoolInformation->FirstEntry = MmPagedPoolStart;
        p = MmPagedPoolStart;
        CurrentPage = 0;
        while (p < (PCHAR)MmPagedPoolEnd) {
            pStart = p;
            BusyFlag = RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, CurrentPage);
            while (~(BusyFlag ^ RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, CurrentPage))) {
                p += PAGE_SIZE;
                if (RtlCheckBit (MmPagedPoolInfo.EndOfPagedPoolBitmap, CurrentPage)) {
                    CurrentPage += 1;
                    break;
                }

                CurrentPage += 1;
                if (p > (PCHAR)MmPagedPoolEnd) {
                    break;
               }
            }

            Size = (ULONG)(p - pStart);
            if (BusyFlag) {
                xStatus = (*SnapShotPoolPage)(pStart,
                                              Size,
                                              PoolInformation,
                                              &PoolEntryInfo,
                                              Length,
                                              RequiredLength
                                              );
                if (xStatus != STATUS_COMMITMENT_LIMIT) {
                    Status = xStatus;
                }
            }
            else {
                PoolInformation->NumberOfEntries += 1;
                *RequiredLength += sizeof (SYSTEM_POOL_ENTRY);
                if (Length < *RequiredLength) {
                    Status = STATUS_INFO_LENGTH_MISMATCH;
                }
                else {
                    PoolEntryInfo->Allocated = FALSE;
                    PoolEntryInfo->Size = Size;
                    PoolEntryInfo->AllocatorBackTraceIndex = 0;
                    PoolEntryInfo->TagUlong = 0;
                    PoolEntryInfo += 1;
                    Status = STATUS_SUCCESS;
                }
            }
        }
    }
    else if (PoolType == NonPagedPool) {
        PoolInformation->TotalSize = MmSizeOfNonPagedPoolInBytes;
        PoolInformation->FirstEntry = MmNonPagedPoolStart;

        p = MmNonPagedPoolStart;
        while (p < (PCHAR)MmNonPagedPoolEnd) {

            //
            // NonPaged pool is linked together through the pages themselves.
            //

            NeedsReprotect = FALSE;
            FreePageInfo = NULL;

            for (Index = 0; Index < MI_MAX_FREE_LIST_HEADS; Index += 1) {

                Entry = MmNonPagedPoolFreeListHead[Index].Flink;
    
                while (Entry != &MmNonPagedPoolFreeListHead[Index]) {
    
                    if (MmProtectFreedNonPagedPool == TRUE) {
                        MiUnProtectFreeNonPagedPool ((PVOID)Entry, 0);
                        NeedsReprotect = TRUE;
                    }

                    FreePageInfo = CONTAINING_RECORD (Entry,
                                                      MMFREE_POOL_ENTRY,
                                                      List);
    
                    ASSERT (FreePageInfo->Signature == MM_FREE_POOL_SIGNATURE);
    
                    if (p == (PCHAR)FreePageInfo) {
    
                        Size = (ULONG)(FreePageInfo->Size << PAGE_SHIFT);
                        PoolInformation->NumberOfEntries += 1;
                        *RequiredLength += sizeof( SYSTEM_POOL_ENTRY );
                        if (Length < *RequiredLength) {
                            Status = STATUS_INFO_LENGTH_MISMATCH;
                        }
                        else {
                            PoolEntryInfo->Allocated = FALSE;
                            PoolEntryInfo->Size = Size;
                            PoolEntryInfo->AllocatorBackTraceIndex = 0;
                            PoolEntryInfo->TagUlong = 0;
                            PoolEntryInfo += 1;
                            Status = STATUS_SUCCESS;
                        }
    
                        p += Size;
                        Index = MI_MAX_FREE_LIST_HEADS;
                        break;
                    }
    
                    Entry = FreePageInfo->List.Flink;
    
                    if (NeedsReprotect == TRUE) {
                        MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                                   (ULONG)FreePageInfo->Size);
                        NeedsReprotect = FALSE;
                    }
                }
            }

            StartPosition = BYTES_TO_PAGES((PCHAR)p -
                  (PCHAR)MmPageAlignedPoolBase[NonPagedPool]);
            if (StartPosition >= MmExpandedPoolBitPosition) {
                if (NeedsReprotect == TRUE) {
                    MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                               (ULONG)FreePageInfo->Size);
                }
                break;
            }

            if (MI_IS_PHYSICAL_ADDRESS(p)) {
                //
                // On certain architectures, virtual addresses
                // may be physical and hence have no corresponding PTE.
                //
                PointerPte = NULL;
                Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (p));
            }
            else {
                PointerPte = MiGetPteAddress (p);
                Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            }
            ASSERT (Pfn1->u3.e1.StartOfAllocation != 0);

            //
            // Find end of allocation and determine size.
            //

            NumberOfPages = 1;
            while (Pfn1->u3.e1.EndOfAllocation == 0) {
                NumberOfPages += 1;
                if (PointerPte == NULL) {
                    Pfn1 += 1;
                }
                else {
                    PointerPte += 1;
                    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                }
            }

            Size = NumberOfPages << PAGE_SHIFT;

            xStatus = (*SnapShotPoolPage) (p,
                                           Size,
                                           PoolInformation,
                                           &PoolEntryInfo,
                                           Length,
                                           RequiredLength);

            if (NeedsReprotect == TRUE) {
                MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                           (ULONG)FreePageInfo->Size);
            }

            if (xStatus != STATUS_COMMITMENT_LIMIT) {
                Status = xStatus;
            }

            p += Size;
        }
    }
    else {
        Status = STATUS_NOT_IMPLEMENTED;
    }

    return Status;
}

#endif // DBG || (i386 && !FPO)


VOID
MiCheckSessionPoolAllocations (
    VOID
    )

/*++

Routine Description:

    Ensure that the current session has no pool allocations since it is about
    to exit.  All session allocations must be freed prior to session exit.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    PMMPTE StartPde;
    PMMPTE EndPde;
    PMMPTE PointerPte;
    PVOID VirtualAddress;

    PAGED_CODE();

    if (MmSessionSpace->NonPagedPoolBytes || MmSessionSpace->PagedPoolBytes) {

        //
        // All page tables for this session's paged pool must be freed by now.
        // Being here means they aren't - this is fatal.  Force in any valid
        // pages so that a debugger can show who the guilty party is.
        //

        StartPde = MiGetPdeAddress (MmSessionSpace->PagedPoolStart);
        EndPde = MiGetPdeAddress (MmSessionSpace->PagedPoolEnd);

        while (StartPde <= EndPde) {

            if (StartPde->u.Long != 0 && StartPde->u.Long != MM_KERNEL_NOACCESS_PTE) {
                //
                // Hunt through the page table page for valid pages and force
                // them in.  Note this also forces in the page table page if
                // it is not already.
                //

                PointerPte = MiGetVirtualAddressMappedByPte (StartPde);

                for (i = 0; i < PTE_PER_PAGE; i += 1) {
                    if (PointerPte->u.Long != 0 && PointerPte->u.Long != MM_KERNEL_NOACCESS_PTE) {
                        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                        *(volatile UCHAR *)VirtualAddress = *(volatile UCHAR *)VirtualAddress;

#if DBG
                        DbgPrint("MiCheckSessionPoolAllocations: Address %p still valid\n",
                            VirtualAddress);
#endif
                    }
                    PointerPte += 1;
                }

            }

            StartPde += 1;
        }

#if DBG
        DbgPrint ("MiCheckSessionPoolAllocations: This exiting session (ID %d) is leaking pool !\n",  MmSessionSpace->SessionId);

        DbgPrint ("This means win32k.sys, rdpdd.sys, atmfd.sys or a video/font driver is broken\n");

        DbgPrint ("%d nonpaged allocation leaks for %d bytes and %d paged allocation leaks for %d bytes\n",
            MmSessionSpace->NonPagedPoolAllocations,
            MmSessionSpace->NonPagedPoolBytes,
            MmSessionSpace->PagedPoolAllocations,
            MmSessionSpace->PagedPoolBytes);
#endif

        KeBugCheckEx (SESSION_HAS_VALID_POOL_ON_EXIT,
                      (ULONG_PTR)MmSessionSpace->SessionId,
                      MmSessionSpace->PagedPoolBytes,
                      MmSessionSpace->NonPagedPoolBytes,
#if defined (_WIN64)
                      (MmSessionSpace->NonPagedPoolAllocations << 32) |
                        (MmSessionSpace->PagedPoolAllocations)
#else
                      (MmSessionSpace->NonPagedPoolAllocations << 16) |
                        (MmSessionSpace->PagedPoolAllocations)
#endif
                    );
    }

    ASSERT (MmSessionSpace->NonPagedPoolAllocations == 0);
    ASSERT (MmSessionSpace->PagedPoolAllocations == 0);
}

NTSTATUS
MiInitializeAndChargePfn (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PFN_NUMBER ContainingPageFrame,
    IN LOGICAL SessionAllocation
    )

/*++

Routine Description:

    Nonpaged wrapper to allocate, initialize and charge for a new page.

Arguments:

    PageFrameIndex - Returns the page frame number which was initialized.

    PointerPde - Supplies the pointer to the PDE to initialize.

    ContainingPageFrame - Supplies the page frame number of the page
                          directory page which contains this PDE.

    SessionAllocation - Supplies TRUE if this allocation is in session space,
                        FALSE otherwise.

Return Value:

    Status of the page initialization.

--*/

{
    MMPTE TempPte;
    KIRQL OldIrql;

    if (SessionAllocation == TRUE) {
        TempPte = ValidKernelPdeLocal;
    }
    else {
        TempPte = ValidKernelPde;
    }

    LOCK_PFN2 (OldIrql);

    if ((MmAvailablePages <= MM_MEDIUM_LIMIT) || (MI_NONPAGABLE_MEMORY_AVAILABLE() <= 1)) {
        UNLOCK_PFN2 (OldIrql);
        return STATUS_NO_MEMORY;
    }

    MmResidentAvailablePages -= 1;

    MiEnsureAvailablePageOrWait (NULL, NULL);

    //
    // Ensure no other thread handled this while this one waited.  If one has,
    // then return STATUS_RETRY so the caller knows to try again.
    //

    if (PointerPde->u.Hard.Valid == 1) {
        MmResidentAvailablePages += 1;
        UNLOCK_PFN2 (OldIrql);
        return STATUS_RETRY;
    }

    //
    // Allocate and map in the page at the requested address.
    //

    *PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
    TempPte.u.Hard.PageFrameNumber = *PageFrameIndex;
    MI_WRITE_VALID_PTE (PointerPde, TempPte);

    MiInitializePfnForOtherProcess (*PageFrameIndex,
                                    PointerPde,
                                    ContainingPageFrame);

    //
    // This page will be locked into working set and assigned an index when
    // the working set is set up on return.
    //

    ASSERT (MI_PFN_ELEMENT(*PageFrameIndex)->u1.WsIndex == 0);

    UNLOCK_PFN2 (OldIrql);

    return STATUS_SUCCESS;
}


VOID
MiSessionPageTableRelease (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    Nonpaged wrapper to release a session pool page table page.

Arguments:

    PageFrameIndex - Returns the page frame number which was initialized.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    LOCK_PFN (OldIrql);

    ASSERT (MmSessionSpace->SessionPageDirectoryIndex == Pfn1->u4.PteFrame);
    ASSERT (Pfn1->u2.ShareCount == 1);

    MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCountOnly (PageFrameIndex);

    MmResidentAvailablePages += 1;
    MM_BUMP_COUNTER(51, 1);

    UNLOCK_PFN (OldIrql);
}


NTSTATUS
MiInitializeSessionPool (
    VOID
    )

/*++

Routine Description:

    Initialize the current session's pool structure.

Arguments:

    None.

Return Value:

    Status of the pool initialization.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPde, PointerPte;
    PFN_NUMBER PageFrameIndex;
    PPOOL_DESCRIPTOR PoolDescriptor;
    PMM_SESSION_SPACE SessionGlobal;
    PMM_PAGED_POOL_INFO PagedPoolInfo;
    MMPTE PreviousPte;
    NTSTATUS Status;
#if (_MI_PAGING_LEVELS < 3)
    ULONG Index;
#endif
#if DBG
    PMMPTE StartPde;
    PMMPTE EndPde;
#endif

    PAGED_CODE ();

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    ExInitializeFastMutex (&SessionGlobal->PagedPoolMutex);

    PoolDescriptor = &MmSessionSpace->PagedPool;

    ExInitializePoolDescriptor (PoolDescriptor,
                                PagedPoolSession,
                                0,
                                0,
                                &SessionGlobal->PagedPoolMutex);

    MmSessionSpace->PagedPoolStart = (PVOID)MiSessionPoolStart;
    MmSessionSpace->PagedPoolEnd = (PVOID)(MiSessionPoolEnd -1);

    PagedPoolInfo = &MmSessionSpace->PagedPoolInfo;
    PagedPoolInfo->PagedPoolCommit = 0;
    PagedPoolInfo->PagedPoolHint = 0;
    PagedPoolInfo->AllocatedPagedPool = 0;

    //
    // Build the page table page for paged pool.
    //

    PointerPde = MiGetPdeAddress (MmSessionSpace->PagedPoolStart);
    MmSessionSpace->PagedPoolBasePde = PointerPde;

    PointerPte = MiGetPteAddress (MmSessionSpace->PagedPoolStart);

    PagedPoolInfo->FirstPteForPagedPool = PointerPte;
    PagedPoolInfo->LastPteForPagedPool = MiGetPteAddress (MmSessionSpace->PagedPoolEnd);

#if DBG
    //
    // Session pool better be unused.
    //

    StartPde = MiGetPdeAddress (MmSessionSpace->PagedPoolStart);
    EndPde = MiGetPdeAddress (MmSessionSpace->PagedPoolEnd);

    while (StartPde <= EndPde) {
        ASSERT (StartPde->u.Long == 0);
        StartPde += 1;
    }
#endif

    //
    // Mark all PDEs as empty.
    //

    MiFillMemoryPte (PointerPde,
                     sizeof(MMPTE) *
                         (1 + MiGetPdeAddress (MmSessionSpace->PagedPoolEnd) - PointerPde),
                     ZeroKernelPte.u.Long);

    if (MiChargeCommitment (1, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        return STATUS_NO_MEMORY;
    }

    Status = MiInitializeAndChargePfn (&PageFrameIndex,
                                       PointerPde,
                                       MmSessionSpace->SessionPageDirectoryIndex,
                                       TRUE);

    if (!NT_SUCCESS(Status)) {
        MiReturnCommitment (1);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        return Status;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES, 1);

    MM_BUMP_COUNTER(42, 1);
    MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC, 1);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPde, PointerPte, FALSE);

#if (_MI_PAGING_LEVELS < 3)

    Index = MiGetPdeSessionIndex (MmSessionSpace->PagedPoolStart);

    ASSERT (MmSessionSpace->PageTables[Index].u.Long == 0);
    MmSessionSpace->PageTables[Index] = *PointerPde;

#endif

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_POOL_CREATE, 1);
    MmSessionSpace->NonPagablePages += 1;

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 1);

    MiFillMemoryPte (PointerPte, PAGE_SIZE, MM_KERNEL_NOACCESS_PTE);

    PagedPoolInfo->NextPdeForPagedPoolExpansion = PointerPde + 1;

    //
    // Initialize the bitmaps.
    //

    MiCreateBitMap (&PagedPoolInfo->PagedPoolAllocationMap,
                    MmSessionPoolSize >> PAGE_SHIFT,
                    NonPagedPool);

    if (PagedPoolInfo->PagedPoolAllocationMap == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        goto Failure;
    }

    //
    // We start with all pages in the virtual address space as "busy", and
    // clear bits to make pages available as we dynamically expand the pool.
    //

    RtlSetAllBits( PagedPoolInfo->PagedPoolAllocationMap );

    //
    // Indicate first page worth of PTEs are available.
    //

    RtlClearBits (PagedPoolInfo->PagedPoolAllocationMap, 0, PTE_PER_PAGE);

    //
    // Create the end of allocation range bitmap.
    //

    MiCreateBitMap (&PagedPoolInfo->EndOfPagedPoolBitmap,
                    MmSessionPoolSize >> PAGE_SHIFT,
                    NonPagedPool);

    if (PagedPoolInfo->EndOfPagedPoolBitmap == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        goto Failure;
    }

    RtlClearAllBits (PagedPoolInfo->EndOfPagedPoolBitmap);

    //
    // Create the large session allocation bitmap.
    //

    MiCreateBitMap (&PagedPoolInfo->PagedPoolLargeSessionAllocationMap,
                    MmSessionPoolSize >> PAGE_SHIFT,
                    NonPagedPool);

    if (PagedPoolInfo->PagedPoolLargeSessionAllocationMap == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        goto Failure;
    }

    RtlClearAllBits (PagedPoolInfo->PagedPoolLargeSessionAllocationMap);

    return STATUS_SUCCESS;

Failure:

    MiFreeSessionPoolBitMaps ();

    MiSessionPageTableRelease (PageFrameIndex);

    MI_FLUSH_SINGLE_SESSION_TB (MiGetPteAddress(PointerPde),
                                TRUE,
                                TRUE,
                                (PHARDWARE_PTE)PointerPde,
                                ZeroKernelPte.u.Flush,
                                PreviousPte);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_POOL_CREATE_FAILED, 1);
    MmSessionSpace->NonPagablePages -= 1;

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, -1);

    MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_FREE_FAIL1, 1);

    MiReturnCommitment (1);

    MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES, 1);

    return STATUS_NO_MEMORY;
}


VOID
MiFreeSessionPoolBitMaps (
    VOID
    )

/*++

Routine Description:

    Free the current session's pool bitmap structures.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PAGED_CODE();

    if (MmSessionSpace->PagedPoolInfo.PagedPoolAllocationMap ) {
        ExFreePool (MmSessionSpace->PagedPoolInfo.PagedPoolAllocationMap);
        MmSessionSpace->PagedPoolInfo.PagedPoolAllocationMap = NULL;
    }

    if (MmSessionSpace->PagedPoolInfo.EndOfPagedPoolBitmap ) {
        ExFreePool (MmSessionSpace->PagedPoolInfo.EndOfPagedPoolBitmap);
        MmSessionSpace->PagedPoolInfo.EndOfPagedPoolBitmap = NULL;
    }

    if (MmSessionSpace->PagedPoolInfo.PagedPoolLargeSessionAllocationMap) {
        ExFreePool (MmSessionSpace->PagedPoolInfo.PagedPoolLargeSessionAllocationMap);
        MmSessionSpace->PagedPoolInfo.PagedPoolLargeSessionAllocationMap = NULL;
    }
}

#if DBG

#define MI_LOG_CONTIGUOUS  100

typedef struct _MI_CONTIGUOUS_ALLOCATORS {
    PVOID BaseAddress;
    SIZE_T NumberOfBytes;
    PVOID CallingAddress;
} MI_CONTIGUOUS_ALLOCATORS, *PMI_CONTIGUOUS_ALLOCATORS;

ULONG MiContiguousIndex;
MI_CONTIGUOUS_ALLOCATORS MiContiguousAllocators[MI_LOG_CONTIGUOUS];

VOID
MiInsertContiguousTag (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes,
    IN PVOID CallingAddress
    )
{
    KIRQL OldIrql;

#if !DBG
    if ((NtGlobalFlag & FLG_POOL_ENABLE_TAGGING) == 0) {
        return;
    }
#endif

    OldIrql = ExLockPool (NonPagedPool);

    if (MiContiguousIndex >= MI_LOG_CONTIGUOUS) {
        MiContiguousIndex = 0;
    }

    MiContiguousAllocators[MiContiguousIndex].BaseAddress = BaseAddress;
    MiContiguousAllocators[MiContiguousIndex].NumberOfBytes = NumberOfBytes;
    MiContiguousAllocators[MiContiguousIndex].CallingAddress = CallingAddress;

    MiContiguousIndex += 1;

    ExUnlockPool (NonPagedPool, OldIrql);
}
#else
#define MiInsertContiguousTag(a, b, c) (c) = (c)
#endif


PVOID
MiFindContiguousMemoryInPool (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN PVOID CallingAddress
    )

/*++

Routine Description:

    This function searches nonpaged pool for contiguous pages to satisfy the
    request.  Note the pool address returned maps these pages as MmCached.

Arguments:

    LowestPfn - Supplies the lowest acceptable physical page number.

    HighestPfn - Supplies the highest acceptable physical page number.

    BoundaryPfn - Supplies the page frame number multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    SizeInPages - Supplies the number of pages to allocate.

    CallingAddress - Supplies the calling address of the allocator.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/
{
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PVOID BaseAddress;
    PVOID BaseAddress2;
    KIRQL OldIrql;
    PMMFREE_POOL_ENTRY FreePageInfo;
    PLIST_ENTRY Entry;
    ULONG Index;
    PFN_NUMBER BoundaryMask;
    ULONG AllocationPosition;
    PVOID Va;
    LOGICAL AddressIsPhysical;
    PFN_NUMBER SpanInPages;
    PFN_NUMBER SpanInPages2;

    PAGED_CODE ();

    //
    // Initializing SpanInPages* is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    SpanInPages = 0;
    SpanInPages2 = 0;

    BaseAddress = NULL;

    BoundaryMask = ~(BoundaryPfn - 1);

    //
    // A suitable pool page was not allocated via the pool allocator.
    // Grab the pool lock and manually search for a page which meets
    // the requirements.
    //

    MmLockPagableSectionByHandle (ExPageLockHandle);

    OldIrql = ExLockPool (NonPagedPool);

    //
    // Trace through the page allocator's pool headers for a page which
    // meets the requirements.
    //
    // NonPaged pool is linked together through the pages themselves.
    //

    Index = (ULONG)(SizeInPages - 1);

    if (Index >= MI_MAX_FREE_LIST_HEADS) {
        Index = MI_MAX_FREE_LIST_HEADS - 1;
    }

    while (Index < MI_MAX_FREE_LIST_HEADS) {

        Entry = MmNonPagedPoolFreeListHead[Index].Flink;
    
        while (Entry != &MmNonPagedPoolFreeListHead[Index]) {
    
            if (MmProtectFreedNonPagedPool == TRUE) {
                MiUnProtectFreeNonPagedPool ((PVOID)Entry, 0);
            }
    
            //
            // The list is not empty, see if this one meets the physical
            // requirements.
            //
    
            FreePageInfo = CONTAINING_RECORD(Entry,
                                             MMFREE_POOL_ENTRY,
                                             List);
    
            ASSERT (FreePageInfo->Signature == MM_FREE_POOL_SIGNATURE);
            if (FreePageInfo->Size >= SizeInPages) {
    
                //
                // This entry has sufficient space, check to see if the
                // pages meet the physical requirements.
                //
    
                Va = MiCheckForContiguousMemory (PAGE_ALIGN(Entry),
                                                 FreePageInfo->Size,
                                                 SizeInPages,
                                                 LowestPfn,
                                                 HighestPfn,
                                                 BoundaryPfn,
                                                 MiCached);
     
                if (Va != NULL) {

                    //
                    // These pages meet the requirements.  The returned
                    // address may butt up on the end, the front or be
                    // somewhere in the middle.  Split the Entry based
                    // on which case it is.
                    //

                    Entry = PAGE_ALIGN(Entry);
                    if (MmProtectFreedNonPagedPool == FALSE) {
                        RemoveEntryList (&FreePageInfo->List);
                    }
                    else {
                        MiProtectedPoolRemoveEntryList (&FreePageInfo->List);
                    }
    
                    //
                    // Adjust the number of free pages remaining in the pool.
                    // The TotalBigPages calculation appears incorrect for the
                    // case where we're splitting a block, but it's done this
                    // way because ExFreePool corrects it when we free the
                    // fragment block below.  Likewise for
                    // MmAllocatedNonPagedPool and MmNumberOfFreeNonPagedPool
                    // which is corrected by MiFreePoolPages for the fragment.
                    //
    
                    NonPagedPoolDescriptor.TotalBigPages += (ULONG)FreePageInfo->Size;
                    MmAllocatedNonPagedPool += FreePageInfo->Size;
                    MmNumberOfFreeNonPagedPool -= FreePageInfo->Size;
    
                    ASSERT ((LONG)MmNumberOfFreeNonPagedPool >= 0);
    
                    if (Va == Entry) {

                        //
                        // Butted against the front.
                        //

                        AllocationPosition = 0;
                    }
                    else if (((PCHAR)Va + (SizeInPages << PAGE_SHIFT)) == ((PCHAR)Entry + (FreePageInfo->Size << PAGE_SHIFT))) {

                        //
                        // Butted against the end.
                        //

                        AllocationPosition = 2;
                    }
                    else {

                        //
                        // Somewhere in the middle.
                        //

                        AllocationPosition = 1;
                    }

                    //
                    // Pages are being removed from the front of
                    // the list entry and the whole list entry
                    // will be removed and then the remainder inserted.
                    //
    
                    //
                    // Mark start and end for the block at the top of the
                    // list.
                    //
    
                    if (MI_IS_PHYSICAL_ADDRESS(Va)) {
    
                        //
                        // On certain architectures, virtual addresses
                        // may be physical and hence have no corresponding PTE.
                        //
    
                        AddressIsPhysical = TRUE;
                        Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (Va));

                        //
                        // Initializing PointerPte is not needed for correctness
                        // but without it the compiler cannot compile this code
                        // W4 to check for use of uninitialized variables.
                        //

                        PointerPte = NULL;
                    }
                    else {
                        AddressIsPhysical = FALSE;
                        PointerPte = MiGetPteAddress(Va);
                        ASSERT (PointerPte->u.Hard.Valid == 1);
                        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                    }
    
                    ASSERT (Pfn1->u4.VerifierAllocation == 0);
                    ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
                    ASSERT (Pfn1->u3.e1.StartOfAllocation == 0);
                    Pfn1->u3.e1.StartOfAllocation = 1;
    
                    //
                    // Calculate the ending PFN address, note that since
                    // these pages are contiguous, just add to the PFN.
                    //
    
                    Pfn1 += SizeInPages - 1;
                    ASSERT (Pfn1->u4.VerifierAllocation == 0);
                    ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
                    ASSERT (Pfn1->u3.e1.EndOfAllocation == 0);
                    Pfn1->u3.e1.EndOfAllocation = 1;
    
                    if (SizeInPages == FreePageInfo->Size) {
    
                        //
                        // Unlock the pool and return.
                        //
                        BaseAddress = (PVOID)Va;
                        ExUnlockPool (NonPagedPool, OldIrql);
                        goto Done;
                    }
    
                    BaseAddress = NULL;

                    if (AllocationPosition != 2) {

                        //
                        // The end piece needs to be freed as the removal
                        // came from the front or the middle.
                        //

                        BaseAddress = (PVOID)((PCHAR)Va + (SizeInPages << PAGE_SHIFT));
                        SpanInPages = FreePageInfo->Size - SizeInPages -
                            (((ULONG_PTR)Va - (ULONG_PTR)Entry) >> PAGE_SHIFT);
    
                        //
                        // Mark start and end of the allocation in the PFN database.
                        //
        
                        if (AddressIsPhysical == TRUE) {
        
                            //
                            // On certain architectures, virtual addresses
                            // may be physical and hence have no corresponding PTE.
                            //
        
                            Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (BaseAddress));
                        }
                        else {
                            PointerPte = MiGetPteAddress(BaseAddress);
                            ASSERT (PointerPte->u.Hard.Valid == 1);
                            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                        }
        
                        ASSERT (Pfn1->u4.VerifierAllocation == 0);
                        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
                        ASSERT (Pfn1->u3.e1.StartOfAllocation == 0);
                        Pfn1->u3.e1.StartOfAllocation = 1;
        
                        //
                        // Calculate the ending PTE's address, can't depend on
                        // these pages being physically contiguous.
                        //
        
                        if (AddressIsPhysical == TRUE) {
                            Pfn1 += (SpanInPages - 1);
                        }
                        else {
                            PointerPte += (SpanInPages - 1);
                            ASSERT (PointerPte->u.Hard.Valid == 1);
                            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                        }
                        ASSERT (Pfn1->u3.e1.EndOfAllocation == 0);
                        Pfn1->u3.e1.EndOfAllocation = 1;
        
                        ASSERT (((ULONG_PTR)BaseAddress & (PAGE_SIZE -1)) == 0);
        
                        SpanInPages2 = SpanInPages;
                    }
        
                    BaseAddress2 = BaseAddress;
                    BaseAddress = NULL;

                    if (AllocationPosition != 0) {

                        //
                        // The front piece needs to be freed as the removal
                        // came from the middle or the end.
                        //

                        BaseAddress = (PVOID)Entry;

                        SpanInPages = ((ULONG_PTR)Va - (ULONG_PTR)Entry) >> PAGE_SHIFT;
    
                        //
                        // Mark start and end of the allocation in the PFN database.
                        //
        
                        if (AddressIsPhysical == TRUE) {
        
                            //
                            // On certain architectures, virtual addresses
                            // may be physical and hence have no corresponding PTE.
                            //
        
                            Pfn1 = MI_PFN_ELEMENT (MI_CONVERT_PHYSICAL_TO_PFN (BaseAddress));
                        }
                        else {
                            PointerPte = MiGetPteAddress(BaseAddress);
                            ASSERT (PointerPte->u.Hard.Valid == 1);
                            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                        }
        
                        ASSERT (Pfn1->u4.VerifierAllocation == 0);
                        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
                        ASSERT (Pfn1->u3.e1.StartOfAllocation == 0);
                        Pfn1->u3.e1.StartOfAllocation = 1;
        
                        //
                        // Calculate the ending PTE's address, can't depend on
                        // these pages being physically contiguous.
                        //
        
                        if (AddressIsPhysical == TRUE) {
                            Pfn1 += (SpanInPages - 1);
                        }
                        else {
                            PointerPte += (SpanInPages - 1);
                            ASSERT (PointerPte->u.Hard.Valid == 1);
                            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
                        }
                        ASSERT (Pfn1->u3.e1.EndOfAllocation == 0);
                        Pfn1->u3.e1.EndOfAllocation = 1;
        
                        ASSERT (((ULONG_PTR)BaseAddress & (PAGE_SIZE -1)) == 0);
                    }
        
                    //
                    // Unlock the pool.
                    //
    
                    ExUnlockPool (NonPagedPool, OldIrql);
    
                    //
                    // Free the split entry at BaseAddress back into the pool.
                    // Note that we have overcharged the pool - the entire free
                    // chunk has been billed.  Here we return the piece we
                    // didn't use and correct the momentary overbilling.
                    //
                    // The start and end allocation bits of this split entry
                    // which we just set up enable ExFreePool and his callees
                    // to correctly adjust the billing.
                    //
    
                    if (BaseAddress) {
                        ExInsertPoolTag ('tnoC',
                                         BaseAddress,
                                         SpanInPages << PAGE_SHIFT,
                                         NonPagedPool);
                        ExFreePool (BaseAddress);
                    }
                    if (BaseAddress2) {
                        ExInsertPoolTag ('tnoC',
                                         BaseAddress2,
                                         SpanInPages2 << PAGE_SHIFT,
                                         NonPagedPool);
                        ExFreePool (BaseAddress2);
                    }
                    BaseAddress = Va;
                    goto Done;
                }
            }
            Entry = FreePageInfo->List.Flink;
            if (MmProtectFreedNonPagedPool == TRUE) {
                MiProtectFreeNonPagedPool ((PVOID)FreePageInfo,
                                           (ULONG)FreePageInfo->Size);
            }
        }
        Index += 1;
    }

    //
    // No entry was found in free nonpaged pool that meets the requirements.
    //

    ExUnlockPool (NonPagedPool, OldIrql);

Done:

    MmUnlockPagableImageSection (ExPageLockHandle);

    if (BaseAddress) {

        MiInsertContiguousTag (BaseAddress,
                               SizeInPages << PAGE_SHIFT,
                               CallingAddress);

        ExInsertPoolTag ('tnoC',
                         BaseAddress,
                         SizeInPages << PAGE_SHIFT,
                         NonPagedPool);
    }

    return BaseAddress;
}

PVOID
MiFindContiguousMemory (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN MEMORY_CACHING_TYPE CacheType,
    IN PVOID CallingAddress
    )

/*++

Routine Description:

    This function searches nonpaged pool and the free, zeroed,
    and standby lists for contiguous pages that satisfy the
    request.

Arguments:

    LowestPfn - Supplies the lowest acceptable physical page number.

    HighestPfn - Supplies the highest acceptable physical page number.

    BoundaryPfn - Supplies the page frame number multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    SizeInPages - Supplies the number of pages to allocate.

    CacheType - Supplies the type of cache mapping that will be used for the
                memory.

    CallingAddress - Supplies the calling address of the allocator.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/
{
    PMMPTE PointerPte;
    PMMPTE DummyPte;
    PMMPFN StartPfn;
    PMMPFN Pfn1;
    PVOID BaseAddress;
    KIRQL OldIrql;
    ULONG start;
    PFN_NUMBER i;
    PFN_NUMBER count;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    PFN_NUMBER found;
    PFN_NUMBER BoundaryMask;
    PFN_NUMBER StartPage;
    PHYSICAL_ADDRESS PhysicalAddress;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    PAGED_CODE ();

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    if (CacheAttribute == MiCached) {

        BaseAddress = MiFindContiguousMemoryInPool (LowestPfn,
                                                    HighestPfn,
                                                    BoundaryPfn,
                                                    SizeInPages,
                                                    CallingAddress);
        //
        // An existing range of nonpaged pool satisfies the requirements
        // so return it now.
        //

        if (BaseAddress != NULL) {
            return BaseAddress;
        }
    }

    BaseAddress = NULL;

    BoundaryMask = ~(BoundaryPfn - 1);

    //
    // A suitable pool page was not allocated via the pool allocator.
    // Grab the pool lock and manually search for a page which meets
    // the requirements.
    //

    MmLockPagableSectionByHandle (ExPageLockHandle);

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    //
    // Charge commitment.
    //
    // Then search the PFN database for pages that meet the requirements.
    //

    if (MiChargeCommitmentCantExpand (SizeInPages, FALSE) == FALSE) {
        goto Done;
    }

    start = 0;

    //
    // Charge resident available pages.
    //

    LOCK_PFN (OldIrql);

    MiDeferredUnlockPages (MI_DEFER_PFN_HELD);

    if ((SPFN_NUMBER)SizeInPages > MI_NONPAGABLE_MEMORY_AVAILABLE()) {
        UNLOCK_PFN (OldIrql);
        goto Done1;
    }

    //
    // Systems utilizing memory compression may have more
    // pages on the zero, free and standby lists than we
    // want to give out.  Explicitly check MmAvailablePages
    // instead (and recheck whenever the PFN lock is released
    // and reacquired).
    //

    if (SizeInPages > MmAvailablePages) {
        UNLOCK_PFN (OldIrql);
        goto Done1;
    }

    MmResidentAvailablePages -= SizeInPages;
    MM_BUMP_COUNTER(3, SizeInPages);

    UNLOCK_PFN (OldIrql);

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        //
        // Close the gaps, then examine the range for a fit.
        //

        LastPage = Page + count; 

        if (LastPage - 1 > HighestPfn) {
            LastPage = HighestPfn + 1;
        }
    
        if (Page < LowestPfn) {
            Page = LowestPfn;
        }

        if ((count != 0) && (Page + SizeInPages <= LastPage)) {
    
            //
            // A fit may be possible in this run, check whether the pages
            // are on the right list.
            //

            found = 0;

            i = 0;
            Pfn1 = MI_PFN_ELEMENT (Page);
            LOCK_PFN (OldIrql);
            do {

                if ((Pfn1->u3.e1.PageLocation <= StandbyPageList) &&
                    (Pfn1->u1.Flink != 0) &&
                    (Pfn1->u2.Blink != 0) &&
                    (Pfn1->u3.e2.ReferenceCount == 0) &&
                    ((CacheAttribute == MiCached) || (!MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (Page)))) {

                    //
                    // Before starting a new run, ensure that it
                    // can satisfy the boundary requirements (if any).
                    //
                    
                    if ((found == 0) && (BoundaryPfn != 0)) {
                        if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) != 0) {
                            //
                            // This run's physical address does not meet the
                            // requirements.
                            //

                            goto NextPage;
                        }
                    }

                    found += 1;
                    if (found == SizeInPages) {

                        //
                        // A match has been found, remove these pages,
                        // map them and return.
                        //

                        //
                        // Systems utilizing memory compression may have more
                        // pages on the zero, free and standby lists than we
                        // want to give out.  Explicitly check MmAvailablePages
                        // instead (and recheck whenever the PFN lock is
                        // released and reacquired).
                        //

                        if (MmAvailablePages < SizeInPages) {
                            goto Failed;
                        }

                        Page = 1 + Page - found;
                        StartPage = Page;

                        MM_TRACK_COMMIT (MM_DBG_COMMIT_CONTIGUOUS_PAGES, SizeInPages);
                        StartPfn = MI_PFN_ELEMENT (Page);
                        Pfn1 = StartPfn - 1;

                        DummyPte = MiGetPteAddress (MmNonPagedPoolExpansionStart);
                        do {
                            Pfn1 += 1;
                            if (Pfn1->u3.e1.PageLocation == StandbyPageList) {
                                MiUnlinkPageFromList (Pfn1);
                                MiRestoreTransitionPte (Page);
                            }
                            else {
                                MiUnlinkFreeOrZeroedPage (Page);
                            }

                            Pfn1->u3.e2.ReferenceCount = 1;
                            Pfn1->u2.ShareCount = 1;
                            Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                            Pfn1->u3.e1.PageLocation = ActiveAndValid;
                            Pfn1->u3.e1.CacheAttribute = CacheAttribute;
                            Pfn1->u3.e1.StartOfAllocation = 0;
                            Pfn1->u3.e1.EndOfAllocation = 0;
                            Pfn1->u4.VerifierAllocation = 0;
                            Pfn1->u3.e1.LargeSessionAllocation = 0;
                            Pfn1->u3.e1.PrototypePte = 0;

                            //
                            // Initialize PteAddress so an MiIdentifyPfn scan
                            // won't crash.  The real value is put in after the
                            // loop.
                            //

                            Pfn1->PteAddress = DummyPte;

                            Page += 1;
                            found -= 1;
                        } while (found != 0);

                        StartPfn->u3.e1.StartOfAllocation = 1;
                        Pfn1->u3.e1.EndOfAllocation = 1;

                        UNLOCK_PFN (OldIrql);

                        LastPage = StartPage + SizeInPages;

                        PhysicalAddress.QuadPart = StartPage;
                        PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;

                        BaseAddress = MmMapIoSpace (PhysicalAddress,
                                                    SizeInPages << PAGE_SHIFT,
                                                    CacheType);

                        if (BaseAddress == NULL) {

                            //
                            // Release the actual pages.
                            //

                            LOCK_PFN2 (OldIrql);

                            StartPfn->u3.e1.StartOfAllocation = 0;
                            Pfn1->u3.e1.EndOfAllocation = 0;

                            do {
                                MI_SET_PFN_DELETED (StartPfn);
                                MiDecrementShareCount (StartPage);
                                StartPage += 1;
                                StartPfn += 1;
                            } while (StartPage < LastPage);

                            UNLOCK_PFN2 (OldIrql);
                            goto Failed;
                        }

                        PointerPte = MiGetPteAddress (BaseAddress);
                        do {
                            StartPfn->PteAddress = PointerPte;
                            StartPfn->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPte));
                            StartPfn += 1;
                            PointerPte += 1;
                        } while (StartPfn <= Pfn1);

                        goto Done;
                    }
                }
                else {

                    //
                    // Nothing magic about the divisor here - just releasing
                    // the PFN lock periodically to give other processors
                    // and DPCs a chance to execute.
                    //
            
                    i += 1;
                    if ((i & 0xFFFF) == 0) {
                        UNLOCK_PFN (OldIrql);
                        found = 0;
                        LOCK_PFN (OldIrql);
                    }
                    else {
                        found = 0;
                    }
                }
NextPage:
                Page += 1;
                Pfn1 += 1;
            } while (Page < LastPage);
            UNLOCK_PFN (OldIrql);
        }
        start += 1;
    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // The desired physical pages could not be allocated so free the PTEs.
    //

Failed:

    ASSERT (BaseAddress == NULL);

    LOCK_PFN (OldIrql);
    MmResidentAvailablePages += SizeInPages;
    MM_BUMP_COUNTER(32, SizeInPages);
    UNLOCK_PFN (OldIrql);

Done1:

    MiReturnCommitment (SizeInPages);

Done:

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    MmUnlockPagableImageSection (ExPageLockHandle);

    if (BaseAddress != NULL) {

        MiInsertContiguousTag (BaseAddress,
                               SizeInPages << PAGE_SHIFT,
                               CallingAddress);
    }

    return BaseAddress;
}

LOGICAL
MmIsSessionAddress (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function returns TRUE if a session address is specified.
    FALSE is returned if not.

Arguments:

    VirtualAddress - Supplies the address in question.

Return Value:

    See above.

Environment:

    Kernel mode.

--*/

{
    return MI_IS_SESSION_ADDRESS (VirtualAddress);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\allocvm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   allocvm.c

Abstract:

    This module contains the routines which implement the
    NtAllocateVirtualMemory service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if DBG
PEPROCESS MmWatchProcess;
VOID MmFooBar(VOID);
#endif // DBG

const ULONG MMVADKEY = ' daV'; //Vad

//
// This registry-settable variable provides a way to give all applications
// virtual address ranges from the highest address downwards.  The idea is to
// make it easy to test 3GB-aware apps on 32-bit machines and 64-bit apps on
// NT64.
//

ULONG MmAllocationPreference;

NTSTATUS
MiResetVirtualMemory (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

LOGICAL
MiCreatePageTablesForPhysicalRange (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

VOID
MiFlushAcquire (
    IN PCONTROL_AREA ControlArea
    );

VOID
MiFlushRelease (
    IN PCONTROL_AREA ControlArea
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtAllocateVirtualMemory)
#pragma alloc_text(PAGELK,MiCreatePageTablesForPhysicalRange)
#pragma alloc_text(PAGELK,MiDeletePageTablesForPhysicalRange)
#pragma alloc_text(PAGELK,MiResetVirtualMemory)
#endif

SIZE_T MmTotalProcessCommit;        // Only used for debugging


NTSTATUS
NtAllocateVirtualMemory(
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN ULONG_PTR ZeroBits,
    IN OUT PSIZE_T RegionSize,
    IN ULONG AllocationType,
    IN ULONG Protect
    )

/*++

Routine Description:

    This function creates a region of pages within the virtual address
    space of a subject process.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the allocated region of pages.
                  If the initial value of this argument is not null,
                  then the region will be allocated starting at the
                  specified virtual address rounded down to the next
                  host page size address boundary. If the initial
                  value of this argument is null, then the operating
                  system will determine where to allocate the region.
        
    ZeroBits - Supplies the number of high order address bits that
               must be zero in the base address of the section view. The
               value of this argument must be less than or equal to the
               maximum number of zero bits and is only used when memory
               management determines where to allocate the view (i.e. when
               BaseAddress is null).

               If ZeroBits is zero, then no zero bit constraints are applied.

               If ZeroBits is greater than 0 and less than 32, then it is
               the number of leading zero bits from bit 31.  Bits 63:32 are
               also required to be zero.  This retains compatibility
               with 32-bit systems.
                
               If ZeroBits is greater than 32, then it is considered as
               a mask and then number of leading zero are counted out
               in the mask.  This then becomes the zero bits argument.

    RegionSize - Supplies a pointer to a variable that will receive
                 the actual size in bytes of the allocated region
                 of pages. The initial value of this argument
                 specifies the size in bytes of the region and is
                 rounded up to the next host page size boundary.

    AllocationType - Supplies a set of flags that describe the type
                     of allocation that is to be performed for the
                     specified region of pages. Flags are:
            
         MEM_COMMIT - The specified region of pages is to be committed.

         MEM_RESERVE - The specified region of pages is to be reserved.

         MEM_TOP_DOWN - The specified region should be created at the
                        highest virtual address possible based on ZeroBits.

         MEM_RESET - Reset the state of the specified region so
                     that if the pages are in page paging file, they
                     are discarded and pages of zeroes are brought in.
                     If the pages are in memory and modified, they are marked
                     as not modified so they will not be written out to
                     the paging file.  The contents are NOT zeroed.

                     The Protect argument is ignored, but a valid protection
                     must be specified.

         MEM_PHYSICAL - The specified region of pages will map physical memory
                        directly via the AWE APIs.

         MEM_WRITE_WATCH - The specified private region is to be used for
                           write-watch purposes.

    Protect - Supplies the protection desired for the committed region of pages.

         PAGE_NOACCESS - No access to the committed region
                         of pages is allowed. An attempt to read,
                         write, or execute the committed region
                         results in an access violation.

         PAGE_EXECUTE - Execute access to the committed
                        region of pages is allowed. An attempt to
                        read or write the committed region results in
                        an access violation.

         PAGE_READONLY - Read only and execute access to the
                         committed region of pages is allowed. An
                         attempt to write the committed region results
                         in an access violation.

         PAGE_READWRITE - Read, write, and execute access to
                          the committed region of pages is allowed. If
                          write access to the underlying section is
                          allowed, then a single copy of the pages are
                          shared. Otherwise the pages are shared read
                          only/copy on write.

         PAGE_NOCACHE - The region of pages should be allocated
                        as non-cachable.

Return Value:

    Various NTSTATUS codes.

--*/

{
    ULONG Locked;
    PMMVAD Vad;
    PMMVAD FoundVad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    PVOID StartingAddress;
    PVOID EndingAddress;
    NTSTATUS Status;
    PVOID TopAddress;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    SIZE_T NumberOfPages;
    PMMPTE PointerPte;
    PMMPTE CommitLimitPte;
    ULONG ProtectionMask;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE StartingPte;
    MMPTE TempPte;
    ULONG OldProtect;
    SIZE_T QuotaCharge;
    SIZE_T QuotaFree;
    SIZE_T CopyOnWriteCharge;
    LOGICAL Attached;
    LOGICAL ChargedExactQuota;
    MMPTE DecommittedPte;
    ULONG ChangeProtection;
    PVOID UsedPageTableHandle;
    PUCHAR Va;
    LOGICAL ChargedJobCommit;
    PMI_PHYSICAL_VIEW PhysicalView;
    PRTL_BITMAP BitMap;
    ULONG BitMapSize;
    ULONG BitMapBits;
    KAPC_STATE ApcState;
    SECTION Section;
    LARGE_INTEGER NewSize;
    PCONTROL_AREA ControlArea;
    PSEGMENT Segment;
#if defined(_MIALT4K_)
    PVOID OriginalBase;
    SIZE_T OriginalRegionSize;
    PVOID WowProcess;
    PVOID StartingAddressFor4k;
    PVOID EndingAddressFor4k;
    SIZE_T CapturedRegionSizeFor4k;
    ULONG OriginalProtectionMask;
    ULONG AltFlags;
    ULONG NativePageProtection;
#endif
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;
    ULONG ExecutePermission;

    PAGED_CODE();

    Attached = FALSE;

    //
    // Check the zero bits argument for correctness.
    //

#if defined (_WIN64)

    if (ZeroBits >= 32) {

        //
        // ZeroBits is a mask instead of a count.  Translate it to a count now.
        //

        ZeroBits = 64 - RtlFindMostSignificantBit (ZeroBits) -1;        
    }
    else if (ZeroBits) {
        ZeroBits += 32;
    }

#endif

    if (ZeroBits > MM_MAXIMUM_ZERO_BITS) {
        return STATUS_INVALID_PARAMETER_3;
    }

    //
    // Check the AllocationType for correctness.
    //

    if ((AllocationType & ~(MEM_COMMIT | MEM_RESERVE | MEM_PHYSICAL |
                            MEM_TOP_DOWN | MEM_RESET | MEM_WRITE_WATCH)) != 0) {
        return STATUS_INVALID_PARAMETER_5;
    }

    //
    // One of MEM_COMMIT, MEM_RESET or MEM_RESERVE must be set.
    //

    if ((AllocationType & (MEM_COMMIT | MEM_RESERVE | MEM_RESET)) == 0) {
        return STATUS_INVALID_PARAMETER_5;
    }

    if ((AllocationType & MEM_RESET) && (AllocationType != MEM_RESET)) {

        //
        // MEM_RESET may not be used with any other flag.
        //

        return STATUS_INVALID_PARAMETER_5;
    }

    if (AllocationType & MEM_WRITE_WATCH) {

        //
        // Write watch address spaces can only be created with MEM_RESERVE.
        //

        if ((AllocationType & MEM_RESERVE) == 0) {
            return STATUS_INVALID_PARAMETER_5;
        }
    }

    if (AllocationType & MEM_PHYSICAL) {

        //
        // MEM_PHYSICAL must be used with MEM_RESERVE and no other flags.
        // This memory is always read-write when allocated.
        //

        if (AllocationType != (MEM_RESERVE | MEM_PHYSICAL)) {
            return STATUS_INVALID_PARAMETER_5;
        }

        if (Protect != PAGE_READWRITE) {
            return STATUS_INVALID_PARAMETER_6;
        }
    }

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (Protect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ChangeProtection = FALSE;

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

#if defined(_MIALT4K_)

    OriginalBase = CapturedBase;
    OriginalRegionSize = CapturedRegionSize;

#endif

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_VAD_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if ((((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) - (ULONG_PTR)CapturedBase) <
            CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_4;
    }

    if (CapturedRegionSize == 0) {

        //
        // Region size cannot be 0.
        //

        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // If the specified process is not the current process, attach
        // to the specified process.
        //

        if (CurrentProcess != Process) {
            KeStackAttachProcess (&Process->Pcb, &ApcState);
            Attached = TRUE;
        }
    }

    //
    // Add execute permission if necessary.
    //

#if defined (_WIN64)
    if (Process->Wow64Process == NULL && AllocationType & MEM_COMMIT)
#elif defined (_X86PAE_)
    if (AllocationType & MEM_COMMIT)
#else
    if (FALSE)
#endif
    {

        if (Process->Peb != NULL) {

            ExecutePermission = 0;

            try {
                ExecutePermission = Process->Peb->ExecuteOptions & MEM_EXECUTE_OPTION_DATA;
            } except (EXCEPTION_EXECUTE_HANDLER) {
                Status = GetExceptionCode();
                goto ErrorReturn1;
            }

            if (ExecutePermission != 0) {

                switch (Protect & 0xF) {
                    case PAGE_READONLY:
                        Protect &= ~PAGE_READONLY;
                        Protect |= PAGE_EXECUTE_READ;
                        break;
                    case PAGE_READWRITE:
                        Protect &= ~PAGE_READWRITE;
                        Protect |= PAGE_EXECUTE_READWRITE;
                        break;
                    case PAGE_WRITECOPY:
                        Protect &= ~PAGE_WRITECOPY;
                        Protect |= PAGE_EXECUTE_WRITECOPY;
                        break;
                    default:
                        break;
                }

                //
                // Recheck protection.
                //

                ProtectionMask = MiMakeProtectionMask (Protect);

                if (ProtectionMask == MM_INVALID_PROTECTION) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto ErrorReturn1;
                }
            }
        }
    }
              
    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    QuotaCharge = 0;

    if ((CapturedBase == NULL) || (AllocationType & MEM_RESERVE)) {

        //
        // PAGE_WRITECOPY is not valid for private pages.
        //

        if ((Protect & PAGE_WRITECOPY) ||
            (Protect & PAGE_EXECUTE_WRITECOPY)) {
            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorReturn1;
        }

        //
        // Reserve the address space.
        //

        if (CapturedBase == NULL) {

            //
            // No base address was specified.  This MUST be a reserve or
            // reserve and commit.
            //

            CapturedRegionSize = ROUND_TO_PAGES (CapturedRegionSize);

            //
            // If the number of zero bits is greater than zero, then calculate
            // the highest address.
            //

            if (ZeroBits != 0) {
                TopAddress = (PVOID)(((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT) >> ZeroBits);

                //
                // Keep the top address below the highest user vad address
                // regardless.
                //

                if (TopAddress > MM_HIGHEST_VAD_ADDRESS) {
                    Status = STATUS_INVALID_PARAMETER_3;
                    goto ErrorReturn1;
                }

            }
            else {
                TopAddress = (PVOID)MM_HIGHEST_VAD_ADDRESS;
            }

            //
            // Check whether the registry indicates that all applications
            // should be given virtual address ranges from the highest
            // address downwards in order to test 3GB-aware apps on 32-bit
            // machines and 64-bit apps on NT64.
            //

            ASSERT ((MmAllocationPreference == 0) ||
                    (MmAllocationPreference == MEM_TOP_DOWN));

#if defined (_WIN64)
            if (Process->Wow64Process == NULL)
#endif
            AllocationType |= MmAllocationPreference;

            //
            // Note this calculation assumes the starting address will be
            // allocated on at least a page boundary.
            //

            NumberOfPages = BYTES_TO_PAGES (CapturedRegionSize);

            //
            // Initializing StartingAddress and EndingAddress is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            StartingAddress = NULL;
            EndingAddress = NULL;
        }
        else {

            //
            // A non-NULL base address was specified.  Check to make sure
            // the specified base address to ending address is currently
            // unused.
            //

            EndingAddress = (PVOID)(((ULONG_PTR)CapturedBase +
                                  CapturedRegionSize - 1L) | (PAGE_SIZE - 1L));

            //
            // Align the starting address on a 64k boundary.
            //

            StartingAddress = (PVOID)MI_64K_ALIGN(CapturedBase);

            NumberOfPages = BYTES_TO_PAGES ((PCHAR)EndingAddress -
                                            (PCHAR)StartingAddress);

            //
            // Initializing TopAddress is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            TopAddress = NULL;
        }

        BitMapSize = 0;

        //
        // Allocate resources up front before acquiring mutexes to reduce
        // contention.
        //

        Vad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_SHORT), 'SdaV');

        if (Vad == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn1;
        }

        Vad->u.LongFlags = 0;

        //
        // Calculate the page file quota for this address range.
        //

        if (AllocationType & MEM_COMMIT) {
            QuotaCharge = NumberOfPages;
            Vad->u.VadFlags.MemCommit = 1;
        }

        if (AllocationType & MEM_PHYSICAL) {
            Vad->u.VadFlags.UserPhysicalPages = 1;
        }

        Vad->u.VadFlags.Protection = ProtectionMask;
        Vad->u.VadFlags.PrivateMemory = 1;

        Vad->u.VadFlags.CommitCharge = QuotaCharge;

        //
        // Initializing BitMap & PhysicalView is not needed for
        // correctness but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        BitMap = NULL;
        PhysicalView = NULL;

        if (AllocationType & MEM_PHYSICAL) {

            if (AllocationType & MEM_WRITE_WATCH) {
                ExFreePool (Vad);
                Status = STATUS_INVALID_PARAMETER_5;
                goto ErrorReturn1;
            }

            if ((Process->AweInfo == NULL) && (MiAllocateAweInfo () == NULL)) {
                ExFreePool (Vad);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }

            PhysicalView = (PMI_PHYSICAL_VIEW) ExAllocatePoolWithTag (
                                                   NonPagedPool,
                                                   sizeof(MI_PHYSICAL_VIEW),
                                                   MI_PHYSICAL_VIEW_KEY);

            if (PhysicalView == NULL) {
                ExFreePool (Vad);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }

            PhysicalView->Vad = Vad;
            PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_AWE;
        }
        else if (AllocationType & MEM_WRITE_WATCH) {

            ASSERT (AllocationType & MEM_RESERVE);

#if defined (_WIN64)
            if (NumberOfPages >= _4gb) {

                //
                // The bitmap package only handles 32 bits.
                //

                ExFreePool (Vad);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }
#endif

            BitMapBits = (ULONG)NumberOfPages;

            BitMapSize = sizeof(RTL_BITMAP) + (ULONG)(((BitMapBits + 31) / 32) * 4);
            BitMap = ExAllocatePoolWithTag (NonPagedPool, BitMapSize, 'wwmM');

            if (BitMap == NULL) {
                ExFreePool (Vad);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }

            //
            // Charge quota for the nonpaged pool for the bitmap.  This is
            // done here rather than by using ExAllocatePoolWithQuota
            // so the process object is not referenced by the quota charge.
            //
    
            Status = PsChargeProcessNonPagedPoolQuota (Process,
                                                       BitMapSize);
    
            if (!NT_SUCCESS(Status)) {
                ExFreePool (Vad);
                ExFreePool (BitMap);
                goto ErrorReturn1;
            }

            PhysicalView = (PMI_PHYSICAL_VIEW) ExAllocatePoolWithTag (
                                                   NonPagedPool,
                                                   sizeof(MI_PHYSICAL_VIEW),
                                                   MI_WRITEWATCH_VIEW_KEY);

            if (PhysicalView == NULL) {
                ExFreePool (Vad);
                ExFreePool (BitMap);
                PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }

            RtlInitializeBitMap (BitMap, (PULONG)(BitMap + 1), BitMapBits);
    
            RtlClearAllBits (BitMap);

            PhysicalView->Vad = Vad;
            PhysicalView->u.BitMap = BitMap;

            Vad->u.VadFlags.WriteWatch = 1;
        }

        //
        // Now acquire mutexes, check ranges and insert.
        //

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so,
        // return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReleaseVad;
        }

        //
        // Find a (or validate the) starting address.
        //

        if (CapturedBase == NULL) {

            if (AllocationType & MEM_TOP_DOWN) {

                //
                // Start from the top of memory downward.
                //

                Status = MiFindEmptyAddressRangeDown (Process->VadRoot,
                                                      CapturedRegionSize,
                                                      TopAddress,
                                                      X64K,
                                                      &StartingAddress);
            }
            else {

                Status = MiFindEmptyAddressRange (CapturedRegionSize,
                                                  X64K,
                                                  (ULONG)ZeroBits,
                                                  &StartingAddress);
            }

            if (!NT_SUCCESS (Status)) {
                goto ErrorReleaseVad;
            }

            //
            // Calculate the ending address based on the top address.
            //

            EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                  CapturedRegionSize - 1L) | (PAGE_SIZE - 1L));

            if (EndingAddress > TopAddress) {

                //
                // The allocation does not honor the zero bits argument.
                //

                Status = STATUS_NO_MEMORY;
                goto ErrorReleaseVad;
            }
        }
        else {

            //
            // See if a VAD overlaps with this starting/ending address pair.
            //

            if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {

                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReleaseVad;
            }
        }

        //
        // An unoccupied address range has been found, finish initializing
        // the virtual address descriptor to describe this range, then
        // insert it into the tree.
        //

        Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
        Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);

        LOCK_WS_UNSAFE (Process);

        Status = MiInsertVad (Vad);

        if (!NT_SUCCESS(Status)) {

            UNLOCK_WS_UNSAFE (Process);

ErrorReleaseVad:

            //
            // The quota charge in InsertVad failed, deallocate the pool
            // and return an error.
            //

            UNLOCK_ADDRESS_SPACE (Process);

            ExFreePool (Vad);

            if (AllocationType & MEM_PHYSICAL) {
                ExFreePool (PhysicalView);
            }
            else if (BitMapSize != 0) {
                ExFreePool (PhysicalView);
                ExFreePool (BitMap);
                PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
            }

            goto ErrorReturn1;
        }

        //
        // Initialize page directory and table pages for the physical range.
        //

        if (AllocationType & MEM_PHYSICAL) {

            if (MiCreatePageTablesForPhysicalRange (Process,
                                                    StartingAddress,
                                                    EndingAddress) == FALSE) {

                PreviousVad = MiGetPreviousVad (Vad);
                NextVad = MiGetNextVad (Vad);

                MiRemoveVad (Vad);

                //
                // Return commitment for page table pages if possible.
                //

                MiReturnPageTablePageCommitment (StartingAddress,
                                                 EndingAddress,
                                                 Process,
                                                 PreviousVad,
                                                 NextVad);

                UNLOCK_WS_AND_ADDRESS_SPACE (Process);
                ExFreePool (Vad);
                ExFreePool (PhysicalView);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn1;
            }

            PhysicalView->StartVa = StartingAddress;
            PhysicalView->EndVa = EndingAddress;

            //
            // Insert the physical view into this process' list using a
            // nonpaged wrapper since the PFN lock is required.
            //

            ASSERT (PhysicalView->u.LongFlags == MI_PHYSICAL_VIEW_AWE);
            MiAweViewInserter (Process, PhysicalView);
        }
        else if (BitMapSize != 0) {

            PhysicalView->StartVa = StartingAddress;
            PhysicalView->EndVa = EndingAddress;

            MiPhysicalViewInserter (Process, PhysicalView);
        }

        //
        // Unlock the working set lock, page faults can now be taken.
        //

        UNLOCK_WS_UNSAFE (Process);

        //
        // Update the current virtual size in the process header, the
        // address space lock protects this operation.
        //

        CapturedRegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
        Process->VirtualSize += CapturedRegionSize;

        if (Process->VirtualSize > Process->PeakVirtualSize) {
            Process->PeakVirtualSize = Process->VirtualSize;
        }

#if defined(_MIALT4K_)

        if (Process->Wow64Process != NULL) {

            if (OriginalBase == NULL) {

                OriginalRegionSize = ROUND_TO_4K_PAGES(OriginalRegionSize);

                EndingAddress =  (PVOID)(((ULONG_PTR) StartingAddress +
                                OriginalRegionSize - 1L) | (PAGE_4K - 1L));

            }
            else {

                EndingAddress = (PVOID)(((ULONG_PTR)OriginalBase +
                                OriginalRegionSize - 1L) | (PAGE_4K - 1L));
            }

            CapturedRegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;

            //
            // Set the alternate permission table
            //

            AltFlags = (AllocationType & MEM_COMMIT) ? ALT_COMMIT : 0;

            MiProtectFor4kPage (StartingAddress,
                                CapturedRegionSize,
                                ProtectionMask,
                                ALT_ALLOCATE|AltFlags,
                                Process);
        }

#endif

        //
        // Release the address space lock, lower IRQL, detach, and dereference
        // the process object.
        //

        UNLOCK_ADDRESS_SPACE(Process);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }

        //
        // Establish an exception handler and write the size and base
        // address.
        //

        try {

            *RegionSize = CapturedRegionSize;
            *BaseAddress = StartingAddress;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // Return success at this point even if the results
            // cannot be written.
            //

            NOTHING;
        }

        return STATUS_SUCCESS;
    }

    //
    // Commit previously reserved pages.  Note that these pages could
    // be either private or a section.
    //

    if (AllocationType == MEM_RESET) {

        //
        // Round up to page boundaries so good data is not reset.
        //

        EndingAddress = (PVOID)((ULONG_PTR)PAGE_ALIGN ((ULONG_PTR)CapturedBase +
                                    CapturedRegionSize) - 1);
        StartingAddress = (PVOID)PAGE_ALIGN((PUCHAR)CapturedBase + PAGE_SIZE - 1);
        if (StartingAddress > EndingAddress) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            goto ErrorReturn1;
        }
    }
    else {
        EndingAddress = (PVOID)(((ULONG_PTR)CapturedBase +
                                CapturedRegionSize - 1) | (PAGE_SIZE - 1));
        StartingAddress = (PVOID)PAGE_ALIGN(CapturedBase);
    }

    CapturedRegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1;

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so,
    // return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn0;
    }

    FoundVad = MiCheckForConflictingVad (Process, StartingAddress, EndingAddress);

    if (FoundVad == NULL) {

        //
        // No virtual address is reserved at the specified base address,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn0;
    }

    if (FoundVad->u.VadFlags.UserPhysicalPages == 1) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn0;
    }

    //
    // Ensure that the starting and ending addresses are all within
    // the same virtual address descriptor.
    //

    if ((MI_VA_TO_VPN (StartingAddress) < FoundVad->StartingVpn) ||
        (MI_VA_TO_VPN (EndingAddress) > FoundVad->EndingVpn)) {

        //
        // Not within the section virtual address descriptor,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn0;
    }

    if (FoundVad->u.VadFlags.CommitCharge == MM_MAX_COMMIT) {

        //
        // This is a special VAD, don't let any commits occur.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn0;
    }

#if defined(_MIALT4K_)

    WowProcess = Process->Wow64Process;
    OriginalProtectionMask = 0;

    if (WowProcess != NULL) {

        OriginalProtectionMask = MiMakeProtectionMask (Protect);

        if (OriginalProtectionMask == MM_INVALID_PROTECTION) {
            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorReturn0;
        }

        if (StartingAddress >= (PVOID)MM_MAX_WOW64_ADDRESS) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            goto ErrorReturn0;
        }

        //
        // If protection changes on this region are allowed then proceed.
        //

        if (FoundVad->u.VadFlags.NoChange == 0) {

            NativePageProtection = MiMakeProtectForNativePage (StartingAddress,
                                                               Protect,
                                                               Process);

            ProtectionMask = MiMakeProtectionMask (NativePageProtection);

            if (ProtectionMask == MM_INVALID_PROTECTION) {
                Status = STATUS_INVALID_PAGE_PROTECTION;
                goto ErrorReturn0;
            }
        }
    }

#endif

    if (AllocationType == MEM_RESET) {

        LOCK_WS_UNSAFE (Process);

        Status = MiResetVirtualMemory (StartingAddress,
                                       EndingAddress,
                                       FoundVad,
                                       Process);

        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
        goto done;
    }

    if (FoundVad->u.VadFlags.PrivateMemory == 0) {

        Status = STATUS_SUCCESS;

        //
        // The no cache option is not allowed for sections.
        //

        if (Protect & PAGE_NOCACHE) {
            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorReturn0;
        }

        if (FoundVad->u.VadFlags.NoChange == 1) {

            //
            // An attempt is made at changing the protection
            // of a SEC_NO_CHANGE section.
            //

            Status = MiCheckSecuredVad (FoundVad,
                                        CapturedBase,
                                        CapturedRegionSize,
                                        ProtectionMask);

            if (!NT_SUCCESS (Status)) {
                goto ErrorReturn0;
            }
        }

        if (FoundVad->ControlArea->FilePointer != NULL) {
            if (FoundVad->u2.VadFlags2.ExtendableFile == 0) {

                //
                // Only page file backed sections can be committed.
                //

                Status = STATUS_ALREADY_COMMITTED;
                goto ErrorReturn0;
            }

            //
            // Commit the requested portions of the extendable file.
            //

            RtlZeroMemory (&Section, sizeof(SECTION));
            ControlArea = FoundVad->ControlArea;
            Section.Segment = ControlArea->Segment;
            Section.u.LongFlags = ControlArea->u.LongFlags;
            Section.InitialPageProtection = PAGE_READWRITE;
            NewSize.QuadPart = FoundVad->u2.VadFlags2.FileOffset;
            NewSize.QuadPart = NewSize.QuadPart << 16;
            NewSize.QuadPart += 1 +
                   ((PCHAR)EndingAddress - (PCHAR)MI_VPN_TO_VA (FoundVad->StartingVpn));
        
            //
            // The working set and address space mutexes must be
            // released prior to calling MmExtendSection otherwise
            // a deadlock with the filesystem can occur.
            //
            // Prevent the control area from being deleted while
            // the (potential) extension is ongoing.
            //

            MiFlushAcquire (ControlArea);

            UNLOCK_ADDRESS_SPACE (Process);
            
            Status = MmExtendSection (&Section, &NewSize, FALSE);
        
            MiFlushRelease (ControlArea);

            if (NT_SUCCESS(Status)) {

                LOCK_ADDRESS_SPACE (Process);

                //
                // The Vad and/or the control area may have been changed
                // or deleted before the mutexes were regained above.
                // So everything must be revalidated.  Note that
                // if anything has changed, success is silently
                // returned just as if the protection change had failed.
                // It is the caller's fault if any of these has gone
                // away and they will suffer.
                //

                if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
                    // Status = STATUS_PROCESS_IS_TERMINATING;
                    goto ErrorReturn0;
                }

                FoundVad = MiCheckForConflictingVad (Process,
                                                     StartingAddress,
                                                     EndingAddress);
        
                if (FoundVad == NULL) {
        
                    //
                    // No virtual address is reserved at the specified
                    // base address, return an error.
                    //
                    // Status = STATUS_CONFLICTING_ADDRESSES;

                    goto ErrorReturn0;
                }
        
                if (ControlArea != FoundVad->ControlArea) {
                    goto ErrorReturn0;
                }

                if (FoundVad->u.VadFlags.UserPhysicalPages == 1) {
                    // Status = STATUS_CONFLICTING_ADDRESSES;

                    goto ErrorReturn0;
                }
        
                if (FoundVad->u.VadFlags.CommitCharge == MM_MAX_COMMIT) {
                    //
                    // This is a special VAD, no commits are allowed.
                    //
                    // Status = STATUS_CONFLICTING_ADDRESSES;

                    goto ErrorReturn0;
                }
        
                //
                // Ensure that the starting and ending addresses are
                // all within the same virtual address descriptor.
                //
        
                if ((MI_VA_TO_VPN (StartingAddress) < FoundVad->StartingVpn) ||
                    (MI_VA_TO_VPN (EndingAddress) > FoundVad->EndingVpn)) {
        
                    //
                    // Not within the section virtual address
                    // descriptor, return an error.
                    //
                    // Status = STATUS_CONFLICTING_ADDRESSES;

                    goto ErrorReturn0;
                }

                if (FoundVad->u.VadFlags.NoChange == 1) {
    
                    //
                    // An attempt is made at changing the protection
                    // of a SEC_NO_CHANGE section.
                    //
    
                    NTSTATUS Status2;

                    Status2 = MiCheckSecuredVad (FoundVad,
                                                 CapturedBase,
                                                 CapturedRegionSize,
                                                 ProtectionMask);
    
                    if (!NT_SUCCESS (Status2)) {
                        goto ErrorReturn0;
                    }
                }
    
                if (FoundVad->ControlArea->FilePointer == NULL) {
                    goto ErrorReturn0;
                }

                if (FoundVad->u2.VadFlags2.ExtendableFile == 0) {
                    goto ErrorReturn0;
                }

#if defined(_MIALT4K_)

                if (WowProcess != NULL) {

                   StartingAddressFor4k = (PVOID)PAGE_4K_ALIGN(OriginalBase);

                   EndingAddressFor4k = (PVOID)(((ULONG_PTR)OriginalBase +
                                         OriginalRegionSize - 1) | (PAGE_4K - 1));

                   CapturedRegionSizeFor4k = (ULONG_PTR)EndingAddressFor4k -
                                        (ULONG_PTR)StartingAddressFor4k + 1L;

                   if ((FoundVad->u.VadFlags.ImageMap == 1) ||
                       (FoundVad->u2.VadFlags2.CopyOnWrite == 1)) {

                       //
                       // Only set the MM_PROTECTION_COPY_MASK if the new protection includes
                       // MM_PROTECTION_WRITE_MASK, otherwise, it will be considered as MM_READ
                       // inside MiProtectFor4kPage().
                       //

                       if ((OriginalProtectionMask & MM_PROTECTION_WRITE_MASK) == MM_PROTECTION_WRITE_MASK) {
                           OriginalProtectionMask |= MM_PROTECTION_COPY_MASK;
                       }
                   }

                   MiProtectFor4kPage (StartingAddressFor4k,
                                       CapturedRegionSizeFor4k,
                                       OriginalProtectionMask,
                                       ALT_COMMIT,
                                       Process);
                }
#endif

                MiSetProtectionOnSection (Process,
                                          FoundVad,
                                          StartingAddress,
                                          EndingAddress,
                                          Protect,
                                          &OldProtect,
                                          TRUE,
                                          &Locked);

                //
                //      ***  WARNING ***
                //
                // The alternate PTE support routines called by
                // MiSetProtectionOnSection may have deleted the old (small)
                // VAD and replaced it with a different (large) VAD - if so,
                // the old VAD is freed and cannot be referenced.
                //

                UNLOCK_ADDRESS_SPACE (Process);
            }

            goto ErrorReturn1;
        }

        StartingPte = MiGetProtoPteAddress (FoundVad,
                                            MI_VA_TO_VPN(StartingAddress));
        LastPte = MiGetProtoPteAddress (FoundVad,
                                        MI_VA_TO_VPN(EndingAddress));

#if 0
        if (AllocationType & MEM_CHECK_COMMIT_STATE) {

            //
            // Make sure none of the pages are already committed.
            //

            ExAcquireFastMutexUnsafe (&MmSectionCommitMutex);

            PointerPte = StartingPte;

            while (PointerPte <= LastPte) {

                //
                // Check to see if the prototype PTE is committed.
                // Note that prototype PTEs cannot be decommitted so
                // the PTEs only need to be checked for zeroes.
                //

                if (PointerPte->u.Long != 0) {
                    ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);
                    UNLOCK_ADDRESS_SPACE (Process);
                    Status = STATUS_ALREADY_COMMITTED;
                    goto ErrorReturn1;
                }
                PointerPte += 1;
            }

            ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);
        }

#endif //0

        //
        // Check to ensure these pages can be committed if this
        // is a page file backed segment.  Note that page file quota
        // has already been charged for this.
        //

        PointerPte = StartingPte;
        QuotaCharge = 1 + LastPte - StartingPte;

        CopyOnWriteCharge = 0;

        if (MI_IS_PTE_PROTECTION_COPY_WRITE(ProtectionMask)) {

            //
            // If the protection is copy on write, charge for
            // the copy on writes.
            //

            CopyOnWriteCharge = QuotaCharge;
        }

        //
        // Charge commitment for the range.
        //

        ChargedExactQuota = FALSE;
        ChargedJobCommit = FALSE;

        if (CopyOnWriteCharge != 0) {

            Status = PsChargeProcessPageFileQuota (Process, CopyOnWriteCharge);

            if (!NT_SUCCESS (Status)) {
                UNLOCK_ADDRESS_SPACE (Process);
                goto ErrorReturn1;
            }

            //
            // Note this job charging is unusual because it is not
            // followed by an immediate process charge.
            //

            if (Process->CommitChargeLimit) {
                if (Process->CommitCharge + CopyOnWriteCharge > Process->CommitChargeLimit) {
                    if (Process->Job) {
                        PsReportProcessMemoryLimitViolation ();
                    }
                    UNLOCK_ADDRESS_SPACE (Process);
                    PsReturnProcessPageFileQuota (Process, CopyOnWriteCharge);
                    Status = STATUS_COMMITMENT_LIMIT;
                    goto ErrorReturn1;
                }
            }

            if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                if (PsChangeJobMemoryUsage(CopyOnWriteCharge) == FALSE) {
                    UNLOCK_ADDRESS_SPACE (Process);
                    PsReturnProcessPageFileQuota (Process, CopyOnWriteCharge);
                    Status = STATUS_COMMITMENT_LIMIT;
                    goto ErrorReturn1;
                }
                ChargedJobCommit = TRUE;
            }
        }

        do {
            if (MiChargeCommitment (QuotaCharge + CopyOnWriteCharge, NULL) == TRUE) {
                break;
            }

            //
            // Reduce the charge we are asking for if possible.
            //

            if (ChargedExactQuota == TRUE) {

                //
                // We have already tried for the precise charge,
                // so just return an error.
                //

                ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);

                if (CopyOnWriteCharge != 0) {

                    if (ChargedJobCommit == TRUE) {
                        PsChangeJobMemoryUsage (-(SSIZE_T)CopyOnWriteCharge);
                    }
                    UNLOCK_ADDRESS_SPACE (Process);
                    PsReturnProcessPageFileQuota (Process, CopyOnWriteCharge);
                }
                else {
                    UNLOCK_ADDRESS_SPACE (Process);
                }
                Status = STATUS_COMMITMENT_LIMIT;
                goto ErrorReturn1;
            }

            //
            // The commitment charging of quota failed, calculate the
            // exact quota taking into account pages that may already be
            // committed and retry the operation.
            //

            ExAcquireFastMutexUnsafe (&MmSectionCommitMutex);

            while (PointerPte <= LastPte) {

                //
                // Check to see if the prototype PTE is committed.
                // Note that prototype PTEs cannot be decommitted so
                // PTEs only need to be checked for zeroes.
                //

                if (PointerPte->u.Long != 0) {
                    QuotaCharge -= 1;
                }
                PointerPte += 1;
            }

            PointerPte = StartingPte;

            ChargedExactQuota = TRUE;

            //
            // If the entire range is committed then there's nothing to charge.
            //

            if (QuotaCharge + CopyOnWriteCharge == 0) {
                ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);
                QuotaFree = 0;
                goto FinishedCharging;
            }

        } while (TRUE);

        if (ChargedExactQuota == FALSE) {
            ExAcquireFastMutexUnsafe (&MmSectionCommitMutex);
        }

        //
        // Commit all the pages.
        //

        Segment = FoundVad->ControlArea->Segment;
        TempPte = Segment->SegmentPteTemplate;
        ASSERT (TempPte.u.Long != 0);

        QuotaFree = 0;

        while (PointerPte <= LastPte) {

            if (PointerPte->u.Long != 0) {

                //
                // Page is already committed, back out commitment.
                //

                QuotaFree += 1;
            }
            else {
                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            PointerPte += 1;
        }

        //
        // Subtract out any excess, then update the segment charges.
        // Note only segment commit is excess - process commit must
        // remain fully charged.
        //

        if (ChargedExactQuota == FALSE) {
            ASSERT (QuotaCharge >= QuotaFree);
            QuotaCharge -= QuotaFree;

            //
            // Return the QuotaFree excess commitment after the
            // mutexes are released to remove needless contention.
            //
        }
        else {

            //
            // Exact quota was charged so zero this to signify
            // there is no excess to return.
            //

            QuotaFree = 0;
        }

        if (QuotaCharge != 0) {
            Segment->NumberOfCommittedPages += QuotaCharge;
            InterlockedExchangeAddSizeT (&MmSharedCommit, QuotaCharge);

            MM_TRACK_COMMIT (MM_DBG_COMMIT_ALLOCVM_SEGMENT, QuotaCharge);
        }

        ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);

        //
        // Update the per-process charges.
        //

        if (CopyOnWriteCharge != 0) {
            FoundVad->u.VadFlags.CommitCharge += CopyOnWriteCharge;
            Process->CommitCharge += CopyOnWriteCharge;

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (CopyOnWriteCharge);

            if (Process->CommitCharge > Process->CommitChargePeak) {
                Process->CommitChargePeak = Process->CommitCharge;
            }

            MM_TRACK_COMMIT (MM_DBG_COMMIT_ALLOCVM_PROCESS, CopyOnWriteCharge);
        }

FinishedCharging:

#if defined(_MIALT4K_)

        //
        // Update the alternate table before PTEs are created
        // for the protection change.
        //

        if (WowProcess != NULL) {

            StartingAddressFor4k = (PVOID)PAGE_4K_ALIGN(OriginalBase);

            EndingAddressFor4k = (PVOID)(((ULONG_PTR)OriginalBase +
                                       OriginalRegionSize - 1) | (PAGE_4K - 1));

            CapturedRegionSizeFor4k = (ULONG_PTR)EndingAddressFor4k -
                (ULONG_PTR)StartingAddressFor4k + 1L;

            if ((FoundVad->u.VadFlags.ImageMap == 1) ||
                (FoundVad->u2.VadFlags2.CopyOnWrite == 1)) {

                //
                // Only set the MM_PROTECTION_COPY_MASK if the new protection includes
                // MM_PROTECTION_WRITE_MASK, otherwise, it will be considered as MM_READ
                // inside MiProtectFor4kPage().
                //

                if ((OriginalProtectionMask & MM_PROTECTION_WRITE_MASK) == MM_PROTECTION_WRITE_MASK) {
                    OriginalProtectionMask |= MM_PROTECTION_COPY_MASK;
                }

            }

            //
            // Set the alternate permission table.
            //

            MiProtectFor4kPage (StartingAddressFor4k,
                                CapturedRegionSizeFor4k,
                                OriginalProtectionMask,
                                ALT_COMMIT,
                                Process);
        }
        else {

            //
            // Initializing these is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            StartingAddressFor4k = NULL;
            CapturedRegionSizeFor4k = 0;
        }

#endif

        //
        // Change all the protections to be protected as specified.
        //

        MiSetProtectionOnSection (Process,
                                  FoundVad,
                                  StartingAddress,
                                  EndingAddress,
                                  Protect,
                                  &OldProtect,
                                  TRUE,
                                  &Locked);
    
        //
        //      ***  WARNING ***
        //
        // The alternate PTE support routines called by
        // MiSetProtectionOnSection may have deleted the old (small)
        // VAD and replaced it with a different (large) VAD - if so,
        // the old VAD is freed and cannot be referenced.
        //

        UNLOCK_ADDRESS_SPACE (Process);

        //
        // Return any excess segment commit that may have been charged.
        //

        if (QuotaFree != 0) {
            MiReturnCommitment (QuotaFree);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_ALLOCVM_SEGMENT, QuotaFree);
        }

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }

#if defined(_MIALT4K_)
        if (WowProcess != NULL) {
            CapturedRegionSize = CapturedRegionSizeFor4k;
            StartingAddress = StartingAddressFor4k;
        }
#endif

        try {
            *RegionSize = CapturedRegionSize;
            *BaseAddress = StartingAddress;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // Return success at this point even if the results
            // cannot be written.
            //

            NOTHING;
        }

        return STATUS_SUCCESS;
    }

    //
    // PAGE_WRITECOPY is not valid for private pages.
    //

    if ((Protect & PAGE_WRITECOPY) ||
        (Protect & PAGE_EXECUTE_WRITECOPY)) {
        Status = STATUS_INVALID_PAGE_PROTECTION;
        goto ErrorReturn0;
    }

    //
    // Ensure none of the pages are already committed as described
    // in the virtual address descriptor.
    //
#if 0
    if (AllocationType & MEM_CHECK_COMMIT_STATE) {
        if ( !MiIsEntireRangeDecommitted(StartingAddress,
                                         EndingAddress,
                                         FoundVad,
                                         Process)) {

            //
            // Previously reserved pages have been committed, or
            // an error occurred, release mutex and return status.
            //

            Status = STATUS_ALREADY_COMMITTED;
            goto ErrorReturn0;
        }
    }
#endif //0

    //
    // Build a demand zero PTE with the proper protection.
    //

    TempPte = ZeroPte;
    TempPte.u.Soft.Protection = ProtectionMask;

    DecommittedPte = ZeroPte;
    DecommittedPte.u.Soft.Protection = MM_DECOMMIT;

    if (FoundVad->u.VadFlags.MemCommit) {
        CommitLimitPte = MiGetPteAddress (MI_VPN_TO_VA (FoundVad->EndingVpn));
    }
    else {
        CommitLimitPte = NULL;
    }

    //
    // The address range has not been committed, commit it now.
    // Note that for private pages, commitment is handled by
    // explicitly updating PTEs to contain Demand Zero entries.
    //

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Check to ensure these pages can be committed.
    //

    QuotaCharge = 1 + LastPte - PointerPte;

    //
    // Charge quota and commitment for the range.
    //

    ChargedExactQuota = FALSE;

    do {

        ChargedJobCommit = FALSE;

        if (Process->CommitChargeLimit) {
            if (Process->CommitCharge + QuotaCharge > Process->CommitChargeLimit) {
                if (Process->Job) {
                    PsReportProcessMemoryLimitViolation ();
                }
                Status = STATUS_COMMITMENT_LIMIT;
                goto Failed;
            }
        }
        if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
            if (PsChangeJobMemoryUsage(QuotaCharge) == FALSE) {
                Status = STATUS_COMMITMENT_LIMIT;
                goto Failed;
            }
            ChargedJobCommit = TRUE;
        }

        if (MiChargeCommitment (QuotaCharge, NULL) == FALSE) {
            Status = STATUS_COMMITMENT_LIMIT;
            goto Failed;
        }

        Status = PsChargeProcessPageFileQuota (Process, QuotaCharge);
        if (!NT_SUCCESS (Status)) {
            MiReturnCommitment (QuotaCharge);
            goto Failed;
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_ALLOCVM_PROCESS2, QuotaCharge);

        FoundVad->u.VadFlags.CommitCharge += QuotaCharge;
        Process->CommitCharge += QuotaCharge;

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (QuotaCharge);

        if (Process->CommitCharge > Process->CommitChargePeak) {
            Process->CommitChargePeak = Process->CommitCharge;
        }

        //
        // Successful so break out now.
        //

        break;

Failed:
        //
        // Charging of commitment failed.  Release the held mutexes and return
        // the failure status to the user.
        //

        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage (0 - QuotaCharge);
        }

        if (ChargedExactQuota == TRUE) {

            //
            // We have already tried for the precise charge,
            // return an error.
            //

            goto ErrorReturn;
        }

        LOCK_WS_UNSAFE (Process);

        //
        // Quota charge failed, calculate the exact quota
        // taking into account pages that may already be
        // committed, subtract this from the total and retry the operation.
        //

        QuotaFree = MiCalculatePageCommitment (StartingAddress,
                                               EndingAddress,
                                               FoundVad,
                                               Process);

        if (QuotaFree == 0) {
            goto ErrorReturn;
        }

        ChargedExactQuota = TRUE;
        QuotaCharge -= QuotaFree;
        ASSERT ((SSIZE_T)QuotaCharge >= 0);

        if (QuotaCharge == 0) {

            //
            // All the pages are already committed so just march on.
            // Explicitly set status to success as code above may have
            // generated a failure status when overcharging.
            //

            Status = STATUS_SUCCESS;
            break;
        }

    } while (TRUE);

    QuotaFree = 0;

    if (ChargedExactQuota == FALSE) {
        LOCK_WS_UNSAFE (Process);
    }

    //
    // Fill in all the page directory and page table pages with the
    // demand zero PTE.
    //

    MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPteAddress (PointerPte);

            //
            // Pointing to the next page table page, make
            // a page table page exist and make it valid.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);
        }

        if (PointerPte->u.Long == 0) {

            if (PointerPte <= CommitLimitPte) {

                //
                // This page is implicitly committed.
                //

                QuotaFree += 1;

            }

            //
            // Increment the count of non-zero page table entries
            // for this page table and the number of private pages
            // for the process.
            //

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
        }
        else {
            if (PointerPte->u.Long == DecommittedPte.u.Long) {

                //
                // Only commit the page if it is already decommitted.
                //

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                QuotaFree += 1;

                //
                // Make sure the protection for the page is right.
                //

                if (!ChangeProtection &&
                    (Protect != MiGetPageProtection (PointerPte,
                                                     Process,
                                                     FALSE))) {
                    ChangeProtection = TRUE;
                }
            }
        }
        PointerPte += 1;
    }

    UNLOCK_WS_UNSAFE (Process);

#if defined(_MIALT4K_)

    if (WowProcess != NULL) {

        StartingAddress = (PVOID) PAGE_4K_ALIGN(OriginalBase);

        EndingAddress = (PVOID)(((ULONG_PTR)OriginalBase +
                                OriginalRegionSize - 1) | (PAGE_4K - 1));

        CapturedRegionSize = (ULONG_PTR)EndingAddress -
                                  (ULONG_PTR)StartingAddress + 1L;

        //
        // Update the alternate permission table.
        //

        MiProtectFor4kPage (StartingAddress,
                            CapturedRegionSize,
                            OriginalProtectionMask,
                            ALT_COMMIT,
                            Process);
    }
#endif

    if ((ChargedExactQuota == FALSE) && (QuotaFree != 0)) {

        FoundVad->u.VadFlags.CommitCharge -= QuotaFree;
        ASSERT ((LONG_PTR)FoundVad->u.VadFlags.CommitCharge >= 0);
        Process->CommitCharge -= QuotaFree;
        UNLOCK_ADDRESS_SPACE (Process);

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - QuotaFree);

        MiReturnCommitment (QuotaFree);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_ALLOCVM2, QuotaFree);

        PsReturnProcessPageFileQuota (Process, QuotaFree);
        if (ChargedJobCommit) {
            PsChangeJobMemoryUsage (-(SSIZE_T)QuotaFree);
        }
    }
    else {
        UNLOCK_ADDRESS_SPACE (Process);
    }

    //
    // Previously reserved pages have been committed or an error occurred.
    // Detach, dereference process and return status.
    //

done:

    if (ChangeProtection) {
        PVOID Start;
        SIZE_T Size;
        ULONG LastProtect;

        Start = StartingAddress;
        Size = CapturedRegionSize;
        MiProtectVirtualMemory (Process,
                                &Start,
                                &Size,
                                Protect,
                                &LastProtect);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *RegionSize = CapturedRegionSize;
        *BaseAddress = StartingAddress;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    return Status;

ErrorReturn:
        UNLOCK_WS_UNSAFE (Process);

ErrorReturn0:
        UNLOCK_ADDRESS_SPACE (Process);

ErrorReturn1:
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        return Status;
}

NTSTATUS
MiResetVirtualMemory (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:


Arguments:

    StartingAddress - Supplies the starting address of the range.

    RegionsSize - Supplies the size.

    Process - Supplies the current process.

Return Value:

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE ProtoPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    MMPTE PteContents;
    ULONG Waited;
    LOGICAL PfnHeld;
    ULONG First;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;

    if (Vad->u.VadFlags.PrivateMemory == 0) {

        if (Vad->ControlArea->FilePointer != NULL) {

            //
            // Only page file backed sections can be committed.
            //

            return STATUS_USER_MAPPED_FILE;
        }
    }

    PfnHeld = FALSE;

    //
    // Initializing OldIrql is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

    First = TRUE;
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Examine all the PTEs in the range.
    //

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte) || (First)) {

            if (MiIsPteOnPpeBoundary (PointerPte)) {

                if (MiIsPteOnPxeBoundary (PointerPte)) {

                    PointerPxe = MiGetPpeAddress (PointerPte);

                    if (!MiDoesPxeExistAndMakeValid(PointerPxe,
                                                    Process,
                                                    PfnHeld,
                                                    &Waited)) {

                        //
                        // This extended page directory parent entry is empty,
                        // go to the next one.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        continue;
                    }
                }

                PointerPpe = MiGetPdeAddress (PointerPte);

                if (!MiDoesPpeExistAndMakeValid(PointerPpe,
                                                Process,
                                                PfnHeld,
                                                &Waited)) {

                    //
                    // This page directory parent entry is empty,
                    // go to the next one.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    continue;
                }
            }

            //
            // Pointing to the next page table page, make
            // a page table page exist and make it valid.
            //

            First = FALSE;
            PointerPde = MiGetPteAddress (PointerPte);
            if (!MiDoesPdeExistAndMakeValid(PointerPde,
                                            Process,
                                            PfnHeld,
                                            &Waited)) {

                //
                // This page directory entry is empty, go to the next one.
                //

                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                continue;
            }
        }

        PteContents = *PointerPte;
        ProtoPte = NULL;

        if ((PteContents.u.Hard.Valid == 0) &&
            (PteContents.u.Soft.Prototype == 1))  {

            //
            // This is a prototype PTE, evaluate the prototype PTE.  Note that
            // the fact it is a prototype PTE does not guarantee that this is a
            // regular or long VAD - it may be a short VAD in a forked process,
            // so check PrivateMemory before referencing the FirstPrototypePte
            // field.
            //

            if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                (Vad->FirstPrototypePte != NULL)) {
                    ProtoPte = MiGetProtoPteAddress(Vad,
                                            MI_VA_TO_VPN (
                                            MiGetVirtualAddressMappedByPte(PointerPte)));
            }
            else {
                CloneBlock = (PMMCLONE_BLOCK)MiPteToProto (PointerPte);
                ProtoPte = (PMMPTE) CloneBlock;
                CloneDescriptor = MiLocateCloneAddress (Process, (PVOID)CloneBlock);
                ASSERT (CloneDescriptor != NULL);
            }

            if (!PfnHeld) {
                PfnHeld = TRUE;
                LOCK_PFN (OldIrql);
            }

            //
            // The working set mutex may be released in order to make the
            // prototype PTE which resides in paged pool resident.  If this
            // occurs, the page directory and/or page table of the original
            // user address may get trimmed.  Account for that here.
            //

            if (MiMakeSystemAddressValidPfnWs (ProtoPte, Process) != 0) {

                //
                // Working set mutex was released, restart from the top.
                //

                First = TRUE;
                continue;
            }

            PteContents = *ProtoPte;
        }
        if (PteContents.u.Hard.Valid == 1) {
            if (!PfnHeld) {
                PfnHeld = TRUE;
                LOCK_PFN (OldIrql);
                continue;
            }

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            if (Pfn1->u3.e2.ReferenceCount == 1) {

                //
                // Only this process has the page mapped.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x20);
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
            }

            if ((!ProtoPte) && (MI_IS_PTE_DIRTY (PteContents))) {

                //
                // Clear the dirty bit and flush tb if it is NOT a prototype
                // PTE.
                //

                MI_SET_PTE_CLEAN (PteContents);
                KeFlushSingleTb (MiGetVirtualAddressMappedByPte (PointerPte),
                                 TRUE,
                                 FALSE,
                                 (PHARDWARE_PTE)PointerPte,
                                 PteContents.u.Flush);
            }

        }
        else if (PteContents.u.Soft.Transition == 1) {
            if (!PfnHeld) {
                PfnHeld = TRUE;
                LOCK_PFN (OldIrql);
                continue;
            }
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
            if ((Pfn1->u3.e1.PageLocation == ModifiedPageList) &&
                (Pfn1->u3.e2.ReferenceCount == 0)) {

                //
                // Remove from the modified list, release the page
                // file space and insert on the standby list.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x21);
                MiUnlinkPageFromList (Pfn1);
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                MiInsertPageInList (&MmStandbyPageListHead,
                                    MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
            }
        }
        else {
            if (PteContents.u.Soft.PageFileHigh != 0) {
                if (!PfnHeld) {
                    LOCK_PFN (OldIrql);
                }
                MiReleasePageFileSpace (PteContents);
                if (ProtoPte) {
                    ProtoPte->u.Soft.PageFileHigh = 0;
                }
                UNLOCK_PFN (OldIrql);
                PfnHeld = FALSE;
                if (!ProtoPte) {
                    PointerPte->u.Soft.PageFileHigh = 0;
                }
            }
            else {
                if (PfnHeld) {
                    UNLOCK_PFN (OldIrql);
                }
                PfnHeld = FALSE;
            }
        }
        PointerPte += 1;
    }
    if (PfnHeld) {
        UNLOCK_PFN (OldIrql);
    }

    MmUnlockPagableImageSection (ExPageLockHandle);

    return STATUS_SUCCESS;
}

LOGICAL
MiCreatePageTablesForPhysicalRange (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    This routine initializes page directory and page table pages for a
    user-controlled physical range of pages.

Arguments:

    Process - Supplies the current process.

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

Return Value:

    TRUE if the page tables were created, FALSE if not.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE LastPpe;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PVOID UsedPageTableHandle;
    LOGICAL FirstTime;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PFN_NUMBER PagesNeeded;

    FirstTime = TRUE;
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Charge resident available pages for all of the page directory and table
    // pages as they will not be paged until the VAD is freed.
    //

    if (LastPte != PointerPte) {
        PagesNeeded = MI_COMPUTE_PAGES_SPANNED (PointerPte,
                                                LastPte - PointerPte);

#if (_MI_PAGING_LEVELS >= 3)
        if (LastPde != PointerPde) {
            PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPde,
                                                     LastPde - PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
            if (LastPpe != PointerPpe) {
                PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPpe,
                                                         LastPpe - PointerPpe);
            }
#endif
        }
#endif
    }
    else {
        PagesNeeded = 1;
#if (_MI_PAGING_LEVELS >= 3)
        PagesNeeded += 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PagesNeeded += 1;
#endif
    }

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)PagesNeeded > MI_NONPAGABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);
        MmUnlockPagableImageSection (ExPageLockHandle);
        return FALSE;
    }

    MmResidentAvailablePages -= PagesNeeded;
    MM_BUMP_COUNTER(58, PagesNeeded);
    UNLOCK_PFN (OldIrql);

    //
    // Initializing UsedPageTableHandle is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    UsedPageTableHandle = NULL;

    //
    // Fill in all the page table pages with the zero PTE.
    //

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte) || FirstTime == TRUE) {

            PointerPde = MiGetPteAddress (PointerPte);

            //
            // Pointing to the next page table page, make
            // a page table page exist and make it valid.
            //
            // Note this ripples sharecounts through the paging hierarchy so
            // there is no need to up sharecounts to prevent trimming of the
            // page directory (and parent) page as making the page table
            // valid below does this automatically.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);

            //
            // Up the sharecount so the page table page will not get
            // trimmed even if it has no currently valid entries.
            //

            PteContents = *PointerPde;
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            LOCK_PFN (OldIrql);
            Pfn1->u2.ShareCount += 1;
            UNLOCK_PFN (OldIrql);

            FirstTime = FALSE;
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);
        }

        ASSERT (PointerPte->u.Long == 0);

        //
        // Increment the count of non-zero page table entries
        // for this page table - even though this entry is still zero,
        // this is a special case.
        //

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        PointerPte += 1;
        StartingAddress = (PVOID)((PUCHAR)StartingAddress + PAGE_SIZE);
    }
    MmUnlockPagableImageSection (ExPageLockHandle);
    return TRUE;
}

VOID
MiDeletePageTablesForPhysicalRange (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    This routine deletes page directory and page table pages for a
    user-controlled physical range of pages.

    Even though PTEs may be zero in this range, UsedPageTable counts were
    incremented for these special ranges and must be decremented now.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PVOID TempVa;
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE LastPpe;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER PagesNeeded;
    PEPROCESS CurrentProcess;
    PVOID UsedPageTableHandle;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPTE PointerPpe;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    CurrentProcess = PsGetCurrentProcess();

    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);

    //
    // Each PTE is already zeroed - just delete the containing pages.
    //
    // Restore resident available pages for all of the page directory and table
    // pages as they can now be paged again.
    //

    if (LastPte != PointerPte) {
        PagesNeeded = MI_COMPUTE_PAGES_SPANNED (PointerPte,
                                                LastPte - PointerPte);
#if (_MI_PAGING_LEVELS >= 3)
        if (LastPde != PointerPde) {
            PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPde,
                                                     LastPde - PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
            if (LastPpe != PointerPpe) {
                PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPpe,
                                                         LastPpe - PointerPpe);
            }
#endif
        }
#endif
    }
    else {
        PagesNeeded = 1;
#if (_MI_PAGING_LEVELS >= 3)
        PagesNeeded += 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PagesNeeded += 1;
#endif
    }

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    MmResidentAvailablePages += PagesNeeded;
    MM_BUMP_COUNTER(59, PagesNeeded);

    while (PointerPte <= LastPte) {

        ASSERT (PointerPte->u.Long == 0);

        PointerPte += 1;

        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        if ((MiIsPteOnPdeBoundary(PointerPte)) || (PointerPte > LastPte)) {

            //
            // The virtual address is on a page directory boundary or it is
            // the last address in the entire range.
            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.
            //

            PointerPde = MiGetPteAddress (PointerPte - 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            //
            // Down the sharecount on the finished page table page.
            //

            PteContents = *PointerPde;
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            ASSERT (Pfn1->u2.ShareCount > 1);
            Pfn1->u2.ShareCount -= 1;

            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.
            //

            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                ASSERT (PointerPde->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 3)
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte(PointerPde);
                MiDeletePte (PointerPde,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL);

#if (_MI_PAGING_LEVELS >= 3)
                if ((MiIsPteOnPpeBoundary(PointerPte)) || (PointerPte > LastPte)) {
    
                    PointerPpe = MiGetPteAddress (PointerPde);
                    ASSERT (PointerPpe->u.Hard.Valid == 1);
    
                    //
                    // If all the entries have been eliminated from the previous
                    // page directory page, delete the page directory page too.
                    //
    
                    if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                        ASSERT (PointerPpe->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde - 1);
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                        TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     CurrentProcess,
                                     NULL,
                                     NULL);

#if (_MI_PAGING_LEVELS >= 4)
                        PointerPxe = MiGetPdeAddress (PointerPde);
                        if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                            ASSERT (PointerPxe->u.Long != 0);
                            TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                            MiDeletePte (PointerPxe,
                                         TempVa,
                                         FALSE,
                                         CurrentProcess,
                                         NULL,
                                         NULL);
                        }
#endif    
                    }
                }
#endif
            }

            if (PointerPte > LastPte) {
                break;
            }

            //
            // Release the PFN lock.  This prevents a single thread
            // from forcing other high priority threads from being
            // blocked while a large address range is deleted.
            //

            UNLOCK_PFN (OldIrql);
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE ((PVOID)((PUCHAR)StartingAddress + PAGE_SIZE));
            LOCK_PFN (OldIrql);
        }

        StartingAddress = (PVOID)((PUCHAR)StartingAddress + PAGE_SIZE);
    }

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);

    //
    // All done, return.
    //

    return;
}


//
// Commented out, no longer used.
//
#if 0
LOGICAL
MiIsEntireRangeDecommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine examines the range of pages from the starting address
    up to and including the ending address and returns TRUE if every
    page in the range is either not committed or decommitted, FALSE otherwise.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the virtual address descriptor which describes the range.

    Process - Supplies the current process.

Return Value:

    TRUE if the entire range is either decommitted or not committed.
    FALSE if any page within the range is committed.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    ULONG Waited;
    ULONG FirstTime;
    PVOID Va;

    FirstTime = TRUE;
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Set the Va to the starting address + 8, this solves problems
    // associated with address 0 (NULL) being used as a valid virtual
    // address and NULL in the VAD commitment field indicating no pages
    // are committed.
    //

    Va = (PVOID)((PCHAR)StartingAddress + 8);

    //
    // A page table page exists, examine the individual PTEs to ensure
    // none are in the committed state.
    //

    while (PointerPte <= LastPte) {

        //
        // Check to see if a page table page (PDE) exists if the PointerPte
        // address is on a page boundary or this is the first time through
        // the loop.
        //

        if (MiIsPteOnPdeBoundary (PointerPte) || (FirstTime)) {

            //
            // This is a PDE boundary, check to see if the entire
            // PDE page exists.
            //

            FirstTime = FALSE;
            PointerPde = MiGetPteAddress (PointerPte);

            while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                Process,
                                                FALSE,
                                                &Waited)) {

                //
                // No PDE exists for the starting address, check the VAD
                // to see whether the pages are committed or not.
                //

                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                if (PointerPte > LastPte) {

                    //
                    // No page table page exists, if explicit commitment
                    // via VAD indicates PTEs of zero should be committed,
                    // return an error.
                    //

                    if (EndingAddress <= Vad->CommittedAddress) {

                        //
                        // The entire range is committed, return an error.
                        //

                        return FALSE;
                    }
                    else {

                        //
                        // All pages are decommitted, return TRUE.
                        //

                        return TRUE;
                    }
                }

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                //
                // Make sure the range thus far is not committed.
                //

                if (Va <= Vad->CommittedAddress) {

                    //
                    // This range is committed, return an error.
                    //

                    return FALSE;
                }
            }
        }

        //
        // The page table page exists, check each PTE for commitment.
        //

        if (PointerPte->u.Long == 0) {

            //
            // This PTE for the page is zero, check the VAD.
            //

            if (Va <= Vad->CommittedAddress) {

                //
                // The entire range is committed, return an error.
                //

                return FALSE;
            }
        }
        else {

            //
            // Has this page been explicitly decommitted?
            //

            if (!MiIsPteDecommittedPage (PointerPte)) {

                //
                // This page is committed, return an error.
                //

                return FALSE;
            }
        }
        PointerPte += 1;
        Va = (PVOID)((PCHAR)(Va) + PAGE_SIZE);
    }
    return TRUE;
}
#endif //0

#if DBG
VOID
MmFooBar(VOID){}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\checkpfn.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   checkpfn.c

Abstract:

    This module contains routines for sanity checking the PFN database.

Author:

    Lou Perazzoli (loup) 25-Apr-1989

Revision History:

--*/

#include "mi.h"

#if DBG

PRTL_BITMAP CheckPfnBitMap;


VOID
MiCheckPfn (
            )

/*++

Routine Description:

    This routine checks each physical page in the PFN database to ensure
    it is in the proper state.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PEPROCESS Process;
    PMMPFN Pfn1;
    PFN_NUMBER Link, Previous;
    ULONG i;
    PMMPTE PointerPte;
    KIRQL PreviousIrql;
    KIRQL OldIrql;
    USHORT ValidCheck[4];
    USHORT ValidPage[4];
    PMMPFN PfnX;

    ValidCheck[0] = ValidCheck[1] = ValidCheck[2] = ValidCheck[3] = 0;
    ValidPage[0] = ValidPage[1] = ValidPage[2] = ValidPage[3] = 0;

    if (CheckPfnBitMap == NULL) {
        MiCreateBitMap ( &CheckPfnBitMap, MmNumberOfPhysicalPages, NonPagedPool);
    }
    RtlClearAllBits (CheckPfnBitMap);

    Process = PsGetCurrentProcess ();

    //
    // Walk free list.
    //

    KeRaiseIrql (APC_LEVEL, &PreviousIrql);
    LOCK_PFN (OldIrql);

    Previous = MM_EMPTY_LIST;
    Link = MmFreePageListHead.Flink;
    for (i=0; i < MmFreePageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("free list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            KeLowerIrql (PreviousIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on free list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != FreePageList) {
            DbgPrint("page location not freelist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on free list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("free list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk zeroed list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmZeroedPageListHead.Flink;
    for (i=0; i < MmZeroedPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("zero list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            KeLowerIrql (PreviousIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on zero list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != ZeroedPageList) {
            DbgPrint("page location not zerolist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on zero list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("zero list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Bad list.
    //
    Previous = MM_EMPTY_LIST;
    Link = MmBadPageListHead.Flink;
    for (i=0; i < MmBadPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Bad list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            KeLowerIrql (PreviousIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Bad list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != BadPageList) {
            DbgPrint("page location not Badlist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Bad list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Bad list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Standby list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmStandbyPageListHead.Flink;
    for (i=0; i < MmStandbyPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Standby list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            KeLowerIrql (PreviousIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Standby list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != StandbyPageList) {
            DbgPrint("page location not Standbylist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Standby list\n");
            MiFormatPfn(Pfn1);
        }

        //
        // Check to see if referenced PTE is okay.
        //
        if (MI_IS_PFN_DELETED (Pfn1)) {
            DbgPrint("Invalid pteaddress in standby list\n");
            MiFormatPfn(Pfn1);

        } else {

            OldIrql = 99;
            if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
                PointerPte = Pfn1->PteAddress;
            } else {
                PointerPte = MiMapPageInHyperSpace (Process,
                                                    Pfn1->u4.PteFrame,
                                                    &OldIrql);
                PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
            }
            if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) != Link) {
                DbgPrint("Invalid PFN - PTE address is wrong in standby list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (PointerPte->u.Soft.Transition == 0) {
                DbgPrint("Pte not in transition for page on standby list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (OldIrql != 99) {
                MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
                OldIrql = 99;
            }

        }

        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Standby list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Modified list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmModifiedPageListHead.Flink;
    for (i=0; i < MmModifiedPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Modified list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            KeLowerIrql (PreviousIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Modified list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != ModifiedPageList) {
            DbgPrint("page location not Modifiedlist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Modified list\n");
            MiFormatPfn(Pfn1);
        }
        //
        // Check to see if referenced PTE is okay.
        //
        if (MI_IS_PFN_DELETED (Pfn1)) {
            DbgPrint("Invalid pteaddress in modified list\n");
            MiFormatPfn(Pfn1);

        } else {

            if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
                PointerPte = Pfn1->PteAddress;
            } else {
                PointerPte = MiMapPageInHyperSpace (Process,
                                                    Pfn1->u4.PteFrame,
                                                    &OldIrql);

                PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
            }

            if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) != Link) {
                DbgPrint("Invalid PFN - PTE address is wrong in modified list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (PointerPte->u.Soft.Transition == 0) {
                DbgPrint("Pte not in transition for page on modified list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }

            if (OldIrql != 99) {
                MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
                OldIrql = 99;
            }
        }

        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Modified list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }
    //
    // All non active pages have been scanned.  Locate the
    // active pages and make sure they are consistent.
    //

    //
    // set bit zero as page zero is reserved for now
    //

    RtlSetBits (CheckPfnBitMap, 0L, 1L);

    Link = RtlFindClearBitsAndSet (CheckPfnBitMap, 1L, 0);
    while (Link != 0xFFFFFFFF) {
        Pfn1 = MI_PFN_ELEMENT (Link);

        //
        // Make sure the PTE address is okay
        //

        if ((Pfn1->PteAddress >= (PMMPTE)HYPER_SPACE)
                && (Pfn1->u3.e1.PrototypePte == 0)) {
            DbgPrint("Pfn with illegal PTE address\n");
            MiFormatPfn(Pfn1);
            break;
        }

        if (Pfn1->PteAddress < (PMMPTE)PTE_BASE) {
            DbgPrint("Pfn with illegal PTE address\n");
            MiFormatPfn(Pfn1);
            break;
        }

#if defined(_IA64_)

        //
        // ignore PTEs mapped to IA64 kernel BAT.
        //

        if (MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(Pfn1->PteAddress))) {

            goto NoCheck;
        }

#endif // _IA64_

        //
        // Check to make sure the referenced PTE is for this page.
        //

        if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
            PointerPte = Pfn1->PteAddress;
        } else {
            PointerPte = MiMapPageInHyperSpace (Process,
                                                Pfn1->u4.PteFrame,
                                                &OldIrql);

            PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) != Link) {
            DbgPrint("Invalid PFN - PTE address is wrong in active list\n");
            MiFormatPfn(Pfn1);
            MiFormatPte(PointerPte);
        }
        if (PointerPte->u.Hard.Valid == 0) {
            //
            // if the page is a page table page it could be out of
            // the working set yet a transition page is keeping it
            // around in memory (ups the share count).
            //

            if ((Pfn1->PteAddress < (PMMPTE)PDE_BASE) ||
                (Pfn1->PteAddress > (PMMPTE)PDE_TOP)) {

                DbgPrint("Pte not valid for page on active list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
        }

        if (Pfn1->u3.e2.ReferenceCount != 1) {
            DbgPrint("refcount not 1\n");
            MiFormatPfn(Pfn1);
        }


        //
        // Check to make sure the PTE count for the frame is okay.
        //

        if (Pfn1->u3.e1.PrototypePte == 1) {
            PfnX = MI_PFN_ELEMENT(Pfn1->u4.PteFrame);
            for (i = 0; i < 4; i++) {
                if (ValidPage[i] == 0) {
                    ValidPage[i] = (USHORT)Pfn1->u4.PteFrame;
                }
                if (ValidPage[i] == (USHORT)Pfn1->u4.PteFrame) {
                    ValidCheck[i] += 1;
                    break;
                }
            }
        }
        if (OldIrql != 99) {
            MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
            OldIrql = 99;
        }

#if defined(_IA64_)

NoCheck:

#endif

        Link = RtlFindClearBitsAndSet (CheckPfnBitMap, 1L, 0);

    }

    for (i = 0; i < 4; i++) {
        if (ValidPage[i] == 0) {
            break;
        }
        PfnX = MI_PFN_ELEMENT(ValidPage[i]);
    }

    UNLOCK_PFN (OldIrql);
    KeLowerIrql (PreviousIrql);
    return;

}

VOID
MiDumpPfn ( )

{
    ULONG i;
    PMMPFN Pfn1;

    Pfn1 = MI_PFN_ELEMENT (MmLowestPhysicalPage);

    for (i=0; i < MmNumberOfPhysicalPages; i++) {
        MiFormatPfn (Pfn1);
        Pfn1++;
    }
    return;
}

VOID
MiFormatPfn (
    IN PMMPFN PointerPfn
    )

{

    MMPFN Pfn;
    ULONG i;

    Pfn = *PointerPfn;
    i = (ULONG)(PointerPfn - MmPfnDatabase);

    DbgPrint("***PFN %lx  flink %p  blink %p  ptecount-refcnt %lx\n",
        i,
        Pfn.u1.Flink,
        Pfn.u2.Blink,
        Pfn.u3.e2.ReferenceCount);

    DbgPrint("   pteaddr %p  originalPTE %p  flags %lx \n",
        Pfn.PteAddress,
        Pfn.OriginalPte,
        Pfn.u3.e2.ShortFlags);

    return;

}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\checkpte.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   checkpte.c

Abstract:

    This module contains routines for sanity checking the page directory.

Author:

    Lou Perazzoli (loup) 25-Apr-1989

Revision History:

--*/

#include "mi.h"

#if DBG
#if !defined (_WIN64)

VOID
CheckValidPte (
    IN PMMPTE PointerPte
    );

VOID
CheckInvalidPte (
    IN PMMPTE PointerPte
    );


VOID
MiCheckPte (
    VOID
    )

/*++

Routine Description:

    This routine checks each page table page in an address space to
    ensure it is in the proper state.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    ULONG i;
    ULONG j;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    ULONG ValidCount;
    ULONG TransitionCount;
    KIRQL OldIrql;
    PEPROCESS TargetProcess;
    USHORT UsedPages;
    ULONG PdeValidCount;

    TargetProcess = PsGetCurrentProcess ();

    PointerPde = MiGetPdeAddress(0);

    UsedPages = 0;
    PdeValidCount = 1;

    LOCK_WS (TargetProcess);
    LOCK_PFN (OldIrql);

    for (i = 0; i < PDE_PER_PAGE; i += 1) {
        if (PointerPde->u.Hard.Valid) {

            if ((i < 512) || (i == 769) || (i== 896) ) {
                PdeValidCount += 1;
            }

            ValidCount = 0;
            TransitionCount = 0;
            CheckValidPte (PointerPde);

            PointerPte = MiGetPteAddress (i<<22);

            for (j=0; j < PTE_PER_PAGE; j += 1) {

                if ((PointerPte >= MiGetPteAddress(HYPER_SPACE)) &&
                        (PointerPte < MiGetPteAddress(WORKING_SET_LIST))) {
                    goto endloop;
                }

                if (PointerPte->u.Hard.Valid) {
                    ValidCount += 1;
                    CheckValidPte (PointerPte);

                } else {
                    CheckInvalidPte (PointerPte);

                    if ((PointerPte->u.Soft.Transition == 1) &&
                        (PointerPte->u.Soft.Prototype == 0)) {

                        //
                        // Transition PTE, up the transition count.
                        //

                        TransitionCount += 1;

                    }
                }

                if (PointerPte->u.Long != 0) {
                    UsedPages += 1;
                }
endloop:
                PointerPte += 1;
            }
            if ((i < 512) || (i == 896)) {
                if (MmWorkingSetList->UsedPageTableEntries[i] != UsedPages) {
                   DbgPrint("used pages and page table used not equal %lx %lx %lx\n",
                    i,MmWorkingSetList->UsedPageTableEntries[i], UsedPages);
                }
            }

            //
            // Check the share count for the page table page.
            //
            if ((i < 511) || (i == 896)) {
                Pfn1 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
                if (Pfn1->u2.ShareCount != ((ULONG)1+ValidCount+TransitionCount)) {
                    DbgPrint("share count for page table page bad - %lx %lx %lx\n",
                        i,ValidCount, TransitionCount);
                    MiFormatPfn(Pfn1);
                }
            }
        }
        PointerPde += 1;
        UsedPages = 0;
    }

    PointerPde = MiGetPteAddress (PDE_BASE);
    Pfn1 = MI_PFN_ELEMENT(PointerPde->u.Hard.PageFrameNumber);

    UNLOCK_PFN (OldIrql);
    UNLOCK_WS (TargetProcess);

    return;
}

VOID
CheckValidPte (
    IN PMMPTE PointerPte
    )

{
    PMMPFN Pfn1;
    PMMPTE PointerPde;

    if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) > MmNumberOfPhysicalPages) {
        return;
    }


    Pfn1 = MI_PFN_ELEMENT(PointerPte->u.Hard.PageFrameNumber);

    if (PointerPte->u.Hard.PageFrameNumber == 0) {
        DbgPrint("physical page zero mapped\n");
        MiFormatPte(PointerPte);
        MiFormatPfn(Pfn1);
    }

    if (Pfn1->u3.e1.PageLocation != ActiveAndValid) {
        DbgPrint("valid PTE with page frame not active and valid\n");
        MiFormatPfn(Pfn1);
        MiFormatPte(PointerPte);
    }

    if (Pfn1->u3.e1.PrototypePte == 0) {
        //
        // This is not a prototype PTE.
        //
        if (Pfn1->PteAddress != PointerPte) {
            DbgPrint("checkpte - Pfn PTE address and PTE address not equal\n");
            MiFormatPte(PointerPte);
            MiFormatPfn(Pfn1);
            return;
        }

    }

    if (!MmIsAddressValid(Pfn1->PteAddress)) {
        return;
    }

    PointerPde = MiGetPteAddress (Pfn1->PteAddress);
    if (PointerPde->u.Hard.Valid == 1) {

        if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPde) != Pfn1->u4.PteFrame) {
                DbgPrint("checkpte - pteframe not right\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
                MiFormatPte(PointerPde);
        }
    }

    return;
}

#endif

VOID
CheckInvalidPte (
    IN PMMPTE PointerPte
    )
{
    UNREFERENCED_PARAMETER (PointerPte);
    return;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\addrsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    addrsup.c

Abstract:

    This module contains the routine to manipulate the virtual address
    descriptor tree.

Author:

    Lou Perazzoli (loup) 19-May-1989
    Landy Wang (landyw) 02-June-1997

    Ripped off and modified from timersup.c
    The support for siblings was removed and a routine to locate
    the corresponding virtual address descriptor for a given address
    was added.

Environment:

    Kernel mode only, working set mutex held, APCs disabled.

Revision History:

--*/

#include "mi.h"

#if (_MSC_VER >= 800)
#pragma warning(disable:4010)        // Allow pretty pictures without the noise
#endif

VOID
MiReorderTree (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiGetFirstNode)
#pragma alloc_text(PAGE,MiInsertNode)
#pragma alloc_text(PAGE,MiGetNextNode)
#pragma alloc_text(PAGE,MiGetPreviousNode)
#pragma alloc_text(PAGE,MiCheckForConflictingNode)
#pragma alloc_text(PAGE,MiFindEmptyAddressRangeInTree)
#pragma alloc_text(PAGE,MiFindEmptyAddressRangeDownTree)
#pragma alloc_text(PAGE,MiReorderTree)
#pragma alloc_text(PAGE,NodeTreeWalk)
#endif


VOID
MiReorderTree (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    )

/*++

Routine Description:

    This function reorders the Node tree by applying various splay functions
    to the tree. This is a local function that is called by the insert Node
    routine.

Arguments:

    Node - Supplies a pointer to a virtual address descriptor.

Return Value:

    None.

--*/

{
    PMMADDRESS_NODE GrandParent;
    PMMADDRESS_NODE Parent;
    PMMADDRESS_NODE SplayNode;

    //
    // Reorder Node tree to make it as balanced as possible with as little
    // work as possible.
    //

    SplayNode = Node;

    while (SplayNode != *Root) {

        Parent = SplayNode->Parent;
        if (Parent == *Root) {

            //
            // Splay node's parent is the root of the tree. Rotate the tree
            // left or right depending on whether the splay node is the left
            // of right child of its parent.
            //
            // Pictorially:
            //
            //            Right                 Left
            //
            //          P        X          P          X
            //         / \      / \        / \        / \
            //        X   C -> A   P      C   X  ->  P   A
            //       / \          / \        / \    / \
            //      A   B        B   C      B   A  C   B
            //

            *Root = SplayNode;
            SplayNode->Parent = (PMMADDRESS_NODE)NULL;
            Parent->Parent = SplayNode;
            if (SplayNode == Parent->LeftChild) {

                //
                // Splay node is the left child of its parent. Rotate tree
                // right.
                //

                Parent->LeftChild = SplayNode->RightChild;
                if (SplayNode->RightChild) {
                    SplayNode->RightChild->Parent = Parent;
                }
                SplayNode->RightChild = Parent;
            } else {

                //
                // Splay node is the right child of its parent. Rotate tree
                // left.
                //

                Parent->RightChild = SplayNode->LeftChild;
                if (SplayNode->LeftChild) {
                    SplayNode->LeftChild->Parent = Parent;
                }
                SplayNode->LeftChild = Parent;
            }
            break;
        } else {
            GrandParent = Parent->Parent;
            if ((SplayNode == Parent->LeftChild) &&
               (Parent == GrandParent->LeftChild)) {

                //
                // Both the splay node and the parent node are left children
                // of their parents. Rotate tree right and make the parent
                // the root of the new subtree.
                //
                // Pictorially:
                //
                //        G                 P
                //       / \              /   \
                //      P   D            X     G
                //     / \       ->     / \   / \
                //    X   C            A   B C   D
                //   / \
                //  A   B
                //

                if (GrandParent == *Root) {
                    *Root = Parent;
                    Parent->Parent = (PMMADDRESS_NODE)NULL;
                } else {
                    Parent->Parent = GrandParent->Parent;
                    if (GrandParent == GrandParent->Parent->LeftChild) {
                        GrandParent->Parent->LeftChild = Parent;
                    } else {
                        GrandParent->Parent->RightChild = Parent;
                    }
                }
                GrandParent->LeftChild = Parent->RightChild;
                if (Parent->RightChild) {
                    Parent->RightChild->Parent = GrandParent;
                }
                GrandParent->Parent = Parent;
                Parent->RightChild = GrandParent;
                SplayNode = Parent;
            } else if ((SplayNode == Parent->RightChild) &&
                      (Parent == GrandParent->RightChild)) {

                //
                // Both the splay node and the parent node are right children
                // of their parents. Rotate tree left and make the parent
                // the root of the new subtree.
                //
                // Pictorially:
                //
                //        G                 P
                //       / \              /   \
                //      D   P            G     X
                //         / \   ->     / \   / \
                //        C   X        D   C B   A
                //           / \
                //          A   B
                //

                if (GrandParent == *Root) {
                    *Root = Parent;
                    Parent->Parent = (PMMADDRESS_NODE)NULL;
                } else {
                    Parent->Parent = GrandParent->Parent;
                    if (GrandParent == GrandParent->Parent->LeftChild) {
                        GrandParent->Parent->LeftChild = Parent;
                    } else {
                        GrandParent->Parent->RightChild = Parent;
                    }
                }
                GrandParent->RightChild = Parent->LeftChild;
                if (Parent->LeftChild) {
                    Parent->LeftChild->Parent = GrandParent;
                }
                GrandParent->Parent = Parent;
                Parent->LeftChild = GrandParent;
                SplayNode = Parent;
            } else if ((SplayNode == Parent->LeftChild) &&
                      (Parent == GrandParent->RightChild)) {

                //
                // Splay node is the left child of its parent and parent is
                // the right child of its parent. Rotate tree left and make
                // splay node the root of the new subtree.
                //
                // Pictorially:
                //
                //      G                 X
                //     / \              /   \
                //    A   P            G     P
                //       / \    ->    / \   / \
                //      X   D        A   B C   D
                //     / \
                //    B   C
                //

                if (GrandParent == *Root) {
                    *Root = SplayNode;
                    SplayNode->Parent = (PMMADDRESS_NODE)NULL;
                } else {
                    SplayNode->Parent = GrandParent->Parent;
                    if (GrandParent == GrandParent->Parent->LeftChild) {
                        GrandParent->Parent->LeftChild = SplayNode;
                    } else {
                        GrandParent->Parent->RightChild = SplayNode;
                    }
                }
                Parent->LeftChild = SplayNode->RightChild;
                if (SplayNode->RightChild) {
                    SplayNode->RightChild->Parent = Parent;
                }
                GrandParent->RightChild = SplayNode->LeftChild;
                if (SplayNode->LeftChild) {
                    SplayNode->LeftChild->Parent = GrandParent;
                }
                Parent->Parent = SplayNode;
                GrandParent->Parent = SplayNode;
                SplayNode->LeftChild = GrandParent;
                SplayNode->RightChild = Parent;
            } else {

                //
                // Splay node is the right child of its parent and parent is
                // the left child of its parent. Rotate tree right and make
                // splay node the root of the new subtree.
                //
                // Pictorially:
                //
                //       G                 X
                //      / \              /   \
                //     P   A            P     G
                //    / \        ->    / \   / \
                //   D   X            D   C B   A
                //      / \
                //     C   B
                //

                if (GrandParent == *Root) {
                    *Root = SplayNode;
                    SplayNode->Parent = (PMMADDRESS_NODE)NULL;
                } else {
                    SplayNode->Parent = GrandParent->Parent;
                    if (GrandParent == GrandParent->Parent->LeftChild) {
                        GrandParent->Parent->LeftChild = SplayNode;
                    } else {
                        GrandParent->Parent->RightChild = SplayNode;
                    }
                }
                Parent->RightChild = SplayNode->LeftChild;
                if (SplayNode->LeftChild) {
                    SplayNode->LeftChild->Parent = Parent;
                }
                GrandParent->LeftChild = SplayNode->RightChild;
                if (SplayNode->RightChild) {
                    SplayNode->RightChild->Parent = GrandParent;
                }
                Parent->Parent = SplayNode;
                GrandParent->Parent = SplayNode;
                SplayNode->LeftChild = Parent;
                SplayNode->RightChild = GrandParent;
            }
        }
    }
    return;
}

PMMADDRESS_NODE
FASTCALL
MiGetNextNode (
    IN PMMADDRESS_NODE Node
    )

/*++

Routine Description:

    This function locates the virtual address descriptor which contains
    the address range which logically follows the specified address range.

Arguments:

    Node - Supplies a pointer to a virtual address descriptor.

Return Value:

    Returns a pointer to the virtual address descriptor containing the
    next address range, NULL if none.

--*/

{
    PMMADDRESS_NODE Next;
    PMMADDRESS_NODE Parent;
    PMMADDRESS_NODE Left;

    Next = Node;

    if (Next->RightChild == (PMMADDRESS_NODE)NULL) {

        while ((Parent = Next->Parent) != (PMMADDRESS_NODE)NULL) {

            //
            // Locate the first ancestor of this node of which this
            // node is the left child of and return that node as the
            // next element.
            //

            if (Parent->LeftChild == Next) {
                return Parent;
            }

            Next = Parent;

        }

        return (PMMADDRESS_NODE)NULL;
    }

    //
    // A right child exists, locate the left most child of that right child.
    //

    Next = Next->RightChild;

    while ((Left = Next->LeftChild) != (PMMADDRESS_NODE)NULL) {
        Next = Left;
    }
    return Next;

}

PMMADDRESS_NODE
FASTCALL
MiGetPreviousNode (
    IN PMMADDRESS_NODE Node
    )

/*++

Routine Description:

    This function locates the virtual address descriptor which contains
    the address range which logically precedes the specified virtual
    address descriptor.

Arguments:

    Node - Supplies a pointer to a virtual address descriptor.

Return Value:

    Returns a pointer to the virtual address descriptor containing the
    next address range, NULL if none.

--*/

{
    PMMADDRESS_NODE Previous;

    Previous = Node;

    if (Previous->LeftChild == (PMMADDRESS_NODE)NULL) {


        while (Previous->Parent != (PMMADDRESS_NODE)NULL) {

            //
            // Locate the first ancestor of this node of which this
            // node is the right child of and return that node as the
            // Previous element.
            //

            if (Previous->Parent->RightChild == Previous) {
                return Previous->Parent;
            }

            Previous = Previous->Parent;

        }
        return (PMMADDRESS_NODE)NULL;
    }

    //
    // A left child exists, locate the right most child of that left child.
    //

    Previous = Previous->LeftChild;
    while (Previous->RightChild != (PMMADDRESS_NODE)NULL) {
        Previous = Previous->RightChild;
    }
    return Previous;
}

PMMADDRESS_NODE
FASTCALL
MiGetFirstNode (
    IN PMMADDRESS_NODE Root
    )

/*++

Routine Description:

    This function locates the virtual address descriptor which contains
    the address range which logically is first within the address space.

Arguments:

    None.

Return Value:

    Returns a pointer to the virtual address descriptor containing the
    first address range, NULL if none.

--*/

{
    PMMADDRESS_NODE First;

    First = Root;

    if (First == (PMMADDRESS_NODE)NULL) {
        return (PMMADDRESS_NODE)NULL;
    }

    while (First->LeftChild != (PMMADDRESS_NODE)NULL) {
        First = First->LeftChild;
    }

    return First;
}

VOID
FASTCALL
MiInsertNode (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    )

/*++

Routine Description:

    This function inserts a virtual address descriptor into the tree and
    reorders the splay tree as appropriate.

Arguments:

    Node - Supplies a pointer to a virtual address descriptor


Return Value:

    None.

--*/

{
    ULONG Level = 0;
    PMMADDRESS_NODE Parent;

    //
    // Initialize virtual address descriptor child links.
    //

    Node->LeftChild = (PMMADDRESS_NODE)NULL;
    Node->RightChild = (PMMADDRESS_NODE)NULL;

    //
    // If the tree is empty, then establish this virtual address descriptor
    // as the root of the tree.
    // Otherwise descend the tree to find the correct place to
    // insert the descriptor.
    //

    Parent = *Root;
    if (!Parent) {
        *Root = Node;
        Node->Parent = (PMMADDRESS_NODE)NULL;
    } else {

        for (;;) {

            Level += 1;
            if (Level == 15) {
                MiReorderTree(Parent, Root);
            }

            //
            // If the starting address for this virtual address descriptor
            // is less than the parent starting address, then
            // follow the left child link. Else follow the right child link.
            //

            if (Node->StartingVpn < Parent->StartingVpn) {

                //
                // Starting address of the virtual address descriptor is less
                // than the parent starting virtual address.
                // Follow left child link if not null. Otherwise
                // insert the descriptor as the left child of the parent and
                // reorder the tree.
                //

                if (Parent->LeftChild) {
                    Parent = Parent->LeftChild;
                } else {
                    Parent->LeftChild = Node;
                    Node->Parent = Parent;
                    // MiReorderTree(Node, Root);
                    break;
                }
            } else {

                //
                // Starting address of the virtual address descriptor is greater
                // than the parent starting virtual address.
                // Follow right child link if not null. Otherwise
                // insert the descriptor as the right child of the parent and
                // reorder the tree.
                //

                if (Parent->RightChild) {
                    Parent = Parent->RightChild;
                } else {
                    Parent->RightChild = Node;
                    Node->Parent = Parent;
                    // MiReorderTree(Node, Root);
                    break;
                }
            }
        }
    }
    return;
}

VOID
FASTCALL
MiRemoveNode (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    )

/*++

Routine Description:

    This function removes a virtual address descriptor from the tree and
    reorders the splay tree as appropriate.

Arguments:

    Node - Supplies a pointer to a virtual address descriptor.

Return Value:

    None.

--*/

{

    PMMADDRESS_NODE LeftChild;
    PMMADDRESS_NODE RightChild;
    PMMADDRESS_NODE SplayNode;


    LeftChild = Node->LeftChild;
    RightChild = Node->RightChild;


    //
    // If the Node is the root of the tree, then establish new root. Else
    // isolate splay case and perform splay tree transformation.
    //

    if (Node == *Root) {

        //
        // This Node is the root of the tree. There are four cases to
        // handle:
        //
        //  1. the descriptor has no children
        //  2. the descriptor has a left child but no right child
        //  3. the descriptor has a right child but no left child
        //  4. the descriptor has both a right child and a left child
        //

        if (LeftChild) {
            if (RightChild) {

                //
                // The descriptor has both a left child and a right child.
                //

                if (LeftChild->RightChild) {

                    //
                    // The left child has a right child. Make the right most
                    // descendent of the right child of the left child the
                    // new root of the tree.
                    //
                    // Pictorially:
                    //
                    //      R          R
                    //      |          |
                    //      X          Z
                    //     / \        / \
                    //    A   B  ->  A   B
                    //     \          \
                    //      .          .
                    //       \
                    //        Z
                    //

                    SplayNode = LeftChild->RightChild;
                    while (SplayNode->RightChild) {
                        SplayNode = SplayNode->RightChild;
                    }
                    *Root = SplayNode;
                    SplayNode->Parent->RightChild = SplayNode->LeftChild;
                    if (SplayNode->LeftChild) {
                        SplayNode->LeftChild->Parent = SplayNode->Parent;
                    }
                    SplayNode->Parent = (PMMADDRESS_NODE)NULL;
                    LeftChild->Parent = SplayNode;
                    RightChild->Parent = SplayNode;
                    SplayNode->LeftChild = LeftChild;
                    SplayNode->RightChild = RightChild;
                } else if (RightChild->LeftChild) {

                    //
                    // The right child has a left child. Make the left most
                    // descendent of the left child of the right child the
                    // new root of the tree.
                    //
                    // Pictorially:
                    //
                    //      R          R
                    //      |          |
                    //      X          Z
                    //     / \        / \
                    //    A   B  ->  A   B
                    //       /          /
                    //      .          .
                    //     /
                    //    Z
                    //

                    SplayNode = RightChild->LeftChild;
                    while (SplayNode->LeftChild) {
                        SplayNode = SplayNode->LeftChild;
                    }
                    *Root = SplayNode;
                    SplayNode->Parent->LeftChild = SplayNode->RightChild;
                    if (SplayNode->RightChild) {
                        SplayNode->RightChild->Parent = SplayNode->Parent;
                    }
                    SplayNode->Parent = (PMMADDRESS_NODE)NULL;
                    LeftChild->Parent = SplayNode;
                    RightChild->Parent = SplayNode;
                    SplayNode->LeftChild = LeftChild;
                    SplayNode->RightChild = RightChild;
                } else {

                    //
                    // The left child of the descriptor does not have a right child,
                    // and the right child of the descriptor does not have a left
                    // child. Make the left child of the descriptor the new root of
                    // the tree.
                    //
                    // Pictorially:
                    //
                    //      R          R
                    //      |          |
                    //      X          A
                    //     / \        / \
                    //    A   B  ->  .   B
                    //   /          /
                    //  .
                    //

                    *Root = LeftChild;
                    LeftChild->Parent = (PMMADDRESS_NODE)NULL;
                    LeftChild->RightChild = RightChild;
                    LeftChild->RightChild->Parent = LeftChild;
                }
            } else {

                //
                // The descriptor has a left child, but does not have a right child.
                // Make the left child the new root of the tree.
                //
                // Pictorially:
                //
                //       R      R
                //       |      |
                //       X  ->  A
                //      /
                //     A
                //

                *Root = LeftChild;
                LeftChild->Parent = (PMMADDRESS_NODE)NULL;
            }
        } else if (RightChild) {

            //
            // The descriptor has a right child, but does not have a left child.
            // Make the right child the new root of the tree.
            //
            // Pictorially:
            //
            //       R         R
            //       |         |
            //       X    ->   A
            //        \
            //         A
            //

            *Root = RightChild;
            RightChild->Parent = (PMMADDRESS_NODE)NULL;
            while (RightChild->LeftChild) {
                RightChild = RightChild->LeftChild;
            }
        } else {

            //
            // The descriptor has neither a left child nor a right child. The
            // tree will be empty after removing the descriptor.
            //
            // Pictorially:
            //
            //      R      R
            //      |  ->
            //      X
            //

            *Root = NULL;
        }
    } else if (LeftChild) {
        if (RightChild) {

            //
            // The descriptor has both a left child and a right child.
            //

            if (LeftChild->RightChild) {

                //
                // The left child has a right child. Make the right most
                // descendent of the right child of the left child the new
                // root of the subtree.
                //
                // Pictorially:
                //
                //        P      P
                //       /        \
                //      X          X
                //     / \        / \
                //    A   B  or  A   B
                //     \          \
                //      .          .
                //       \          \
                //        Z          Z
                //
                //           |
                //           v
                //
                //        P      P
                //       /        \
                //      Z          Z
                //     / \        / \
                //    A   B  or  A   B
                //     \          \
                //      .          .
                //

                SplayNode = LeftChild->RightChild;
                while (SplayNode->RightChild) {
                    SplayNode = SplayNode->RightChild;
                }
                SplayNode->Parent->RightChild = SplayNode->LeftChild;
                if (SplayNode->LeftChild) {
                    SplayNode->LeftChild->Parent = SplayNode->Parent;
                }
                SplayNode->Parent = Node->Parent;
                if (Node == Node->Parent->LeftChild) {
                    Node->Parent->LeftChild = SplayNode;
                } else {
                    Node->Parent->RightChild = SplayNode;
                }
                LeftChild->Parent = SplayNode;
                RightChild->Parent = SplayNode;
                SplayNode->LeftChild = LeftChild;
                SplayNode->RightChild = RightChild;
            } else if (RightChild->LeftChild) {

                //
                // The right child has a left child. Make the left most
                // descendent of the left child of the right child the
                // new root of the subtree.
                //
                // Pictorially:
                //
                //        P      P
                //       /        \
                //      X          X
                //     / \        / \
                //    A   B  or  A   B
                //       /          /
                //      .          .
                //     /          /
                //    Z          Z
                //
                //           |
                //           v
                //
                //        P      P
                //       /        \
                //      Z          Z
                //     / \        / \
                //    A   B  or  A   B
                //       /          /
                //      .          .
                //

                SplayNode = RightChild->LeftChild;
                while (SplayNode->LeftChild) {
                    SplayNode = SplayNode->LeftChild;
                }
                SplayNode->Parent->LeftChild = SplayNode->RightChild;
                if (SplayNode->RightChild) {
                    SplayNode->RightChild->Parent = SplayNode->Parent;
                }
                SplayNode->Parent = Node->Parent;
                if (Node == Node->Parent->LeftChild) {
                    Node->Parent->LeftChild = SplayNode;
                } else {
                    Node->Parent->RightChild = SplayNode;
                }
                LeftChild->Parent = SplayNode;
                RightChild->Parent = SplayNode;
                SplayNode->LeftChild = LeftChild;
                SplayNode->RightChild = RightChild;
            } else {

                //
                // The left child of the descriptor does not have a right child,
                // and the right child of the descriptor does node have a left
                // child. Make the left child of the descriptor the new root of
                // the subtree.
                //
                // Pictorially:
                //
                //        P      P
                //       /        \
                //      X          X
                //     / \        / \
                //    A   B  or  A   B
                //   /          /
                //  .          .
                //
                //           |
                //           v
                //
                //        P      P
                //       /        \
                //      A          A
                //     / \        / \
                //    .   B  or  .   B
                //   /          /
                //

                SplayNode = LeftChild;
                SplayNode->Parent = Node->Parent;
                if (Node == Node->Parent->LeftChild) {
                    Node->Parent->LeftChild = SplayNode;
                } else {
                    Node->Parent->RightChild = SplayNode;
                }
                SplayNode->RightChild = RightChild;
                RightChild->Parent = SplayNode;
            }
        } else {

            //
            // The descriptor has a left child, but does not have a right child.
            // Make the left child the new root of the subtree.
            //
            // Pictorially:
            //
            //        P   P
            //       /     \
            //      X   or  X
            //     /       /
            //    A       A
            //
            //          |
            //          v
            //
            //        P   P
            //       /     \
            //      A       A
            //

            LeftChild->Parent = Node->Parent;
            if (Node == Node->Parent->LeftChild) {
                Node->Parent->LeftChild = LeftChild;
            } else {
                Node->Parent->RightChild = LeftChild;
            }
        }
    } else if (RightChild) {

        //
        // descriptor has a right child, but does not have a left child. Make
        // the right child the new root of the subtree.
        //
        // Pictorially:
        //
        //        P   P
        //       /     \
        //      X   or  X
        //       \       \
        //        A       A
        //
        //          |
        //          v
        //
        //        P   P
        //       /     \
        //      A       A
        //

        RightChild->Parent = Node->Parent;
        if (Node == Node->Parent->LeftChild) {
            Node->Parent->LeftChild = RightChild;
        } else {
            Node->Parent->RightChild = RightChild;
        }
    } else {

        //
        // The descriptor has neither a left child nor a right child. Delete
        // the descriptor from the tree and adjust its parent right or left
        // link.
        //
        // Pictorially:
        //
        //        P   P
        //       /     \
        //      X   or  X
        //
        //          |
        //          v
        //
        //        P   P
        //

        if (Node == Node->Parent->LeftChild) {
            Node->Parent->LeftChild = (PMMADDRESS_NODE)NULL;
        } else {
            Node->Parent->RightChild = (PMMADDRESS_NODE)NULL;
        }
    }
    return;
}

PMMADDRESS_NODE
FASTCALL
MiLocateAddressInTree (
    IN ULONG_PTR Vpn,
    IN PMMADDRESS_NODE *Root
    )

/*++

Routine Description:

    The function locates the virtual address descriptor which describes
    a given address.

Arguments:

    Vpn - Supplies the virtual page number to locate a descriptor for.

Return Value:

    Returns a pointer to the virtual address descriptor which contains
    the supplied virtual address or NULL if none was located.

--*/

{

    PMMADDRESS_NODE Parent;
    ULONG Level = 0;

    Parent = *Root;

    for (;;) {

        if (Parent == (PMMADDRESS_NODE)NULL) {
            return (PMMADDRESS_NODE)NULL;
        }

        if (Level == 20) {

            //
            // There are 20 nodes above this point, reorder the
            // tree with this node as the root.  Note this reorder
            // cannot be done unless the address creation mutex is held,
            // and it is not held on faults.
#if 0
            MiReorderTree(Parent, Root);
#endif
        }

        if (Vpn < Parent->StartingVpn) {
            Parent = Parent->LeftChild;
            Level += 1;

        } else if (Vpn > Parent->EndingVpn) {
            Parent = Parent->RightChild;
            Level += 1;

        } else {

            //
            // The address is within the start and end range.
            //

            return Parent;
        }
    }
}

PMMADDRESS_NODE
MiCheckForConflictingNode (
    IN ULONG_PTR StartVpn,
    IN ULONG_PTR EndVpn,
    IN PMMADDRESS_NODE Root
    )

/*++

Routine Description:

    The function determines if any addresses between a given starting and
    ending address is contained within a virtual address descriptor.

Arguments:

    StartVpn - Supplies the virtual address to locate a containing
                      descriptor.

    EndVpn - Supplies the virtual address to locate a containing
                      descriptor.

Return Value:

    Returns a pointer to the first conflicting virtual address descriptor
    if one is found, otherwise a NULL value is returned.

--*/

{
    PMMADDRESS_NODE Node;

    Node = Root;

    for (;;) {

        if (Node == (PMMADDRESS_NODE)NULL) {
            return (PMMADDRESS_NODE)NULL;
        }

        if (StartVpn > Node->EndingVpn) {
            Node = Node->RightChild;

        } else if (EndVpn < Node->StartingVpn) {
            Node = Node->LeftChild;

        } else {

            //
            // The starting address is less than or equal to the end VA
            // and the ending address is greater than or equal to the
            // start va.  Return this node.
            //

            return Node;
        }
    }
}

NTSTATUS
MiFindEmptyAddressRangeInTree (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN PMMADDRESS_NODE Root,
    OUT PMMADDRESS_NODE *PreviousVad,
    OUT PVOID *Base
    )

/*++

Routine Description:

    The function examines the virtual address descriptors to locate
    an unused range of the specified size and returns the starting
    address of the range.

Arguments:

    SizeOfRange - Supplies the size in bytes of the range to locate.

    Alignment - Supplies the alignment for the address.  Must be
                 a power of 2 and greater than the page_size.

    Root - Supplies the root of the tree to search through.

    PreviousVad - Supplies the Vad which is before this the found
                  address range.

    Base - Receives the starting address of a suitable range on success.

Return Value:

    NTSTATUS.

--*/

{
    PMMADDRESS_NODE Node;
    PMMADDRESS_NODE NextNode;
    ULONG_PTR AlignmentVpn;
    ULONG_PTR SizeOfRangeVpn;

    AlignmentVpn = Alignment >> PAGE_SHIFT;

    //
    // Locate the Node with the lowest starting address.
    //

    ASSERT (SizeOfRange != 0);
    SizeOfRangeVpn = (SizeOfRange + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
    ASSERT (SizeOfRangeVpn != 0);

    Node = Root;

    if (Node == (PMMADDRESS_NODE)NULL) {
        *Base = MM_LOWEST_USER_ADDRESS;
        return STATUS_SUCCESS;
    }
    while (Node->LeftChild != (PMMADDRESS_NODE)NULL) {
        Node = Node->LeftChild;
    }

    //
    // Check to see if a range exists between the lowest address VAD
    // and lowest user address.
    //

    if (Node->StartingVpn > MI_VA_TO_VPN (MM_LOWEST_USER_ADDRESS)) {
        if ( SizeOfRangeVpn <
            (Node->StartingVpn - MI_VA_TO_VPN (MM_LOWEST_USER_ADDRESS))) {

            *PreviousVad = NULL;
            *Base = MM_LOWEST_USER_ADDRESS;
            return STATUS_SUCCESS;
        }
    }

    for (;;) {

        NextNode = MiGetNextNode (Node);

        if (NextNode != (PMMADDRESS_NODE)NULL) {

            if (SizeOfRangeVpn <=
                ((ULONG_PTR)NextNode->StartingVpn -
                                MI_ROUND_TO_SIZE(1 + Node->EndingVpn,
                                                 AlignmentVpn))) {

                //
                // Check to ensure that the ending address aligned upwards
                // is not greater than the starting address.
                //

                if ((ULONG_PTR)NextNode->StartingVpn >
                        MI_ROUND_TO_SIZE(1 + Node->EndingVpn,
                                         AlignmentVpn)) {

                    *PreviousVad = Node;
                    *Base = (PVOID) MI_ROUND_TO_SIZE(
                                (ULONG_PTR)MI_VPN_TO_VA_ENDING(Node->EndingVpn),
                                    Alignment);
                    return STATUS_SUCCESS;
                }
            }

        } else {

            //
            // No more descriptors, check to see if this fits into the remainder
            // of the address space.
            //

            if ((((ULONG_PTR)Node->EndingVpn + MI_VA_TO_VPN(X64K)) <
                    MI_VA_TO_VPN (MM_HIGHEST_VAD_ADDRESS))
                        &&
                (SizeOfRange <=
                    ((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS -
                         (ULONG_PTR)MI_ROUND_TO_SIZE(
                         (ULONG_PTR)MI_VPN_TO_VA(Node->EndingVpn), Alignment)))) {

                *PreviousVad = Node;
                *Base = (PVOID) MI_ROUND_TO_SIZE(
                            (ULONG_PTR)MI_VPN_TO_VA_ENDING(Node->EndingVpn),
                                Alignment);
                return STATUS_SUCCESS;
            }
            return STATUS_NO_MEMORY;
        }
        Node = NextNode;
    }
}

NTSTATUS
MiFindEmptyAddressRangeDownTree (
    IN SIZE_T SizeOfRange,
    IN PVOID HighestAddressToEndAt,
    IN ULONG_PTR Alignment,
    IN PMMADDRESS_NODE Root,
    OUT PVOID *Base
    )

/*++

Routine Description:

    The function examines the virtual address descriptors to locate
    an unused range of the specified size and returns the starting
    address of the range.  The function examines from the high
    addresses down and ensures that starting address is less than
    the specified address.

Arguments:

    SizeOfRange - Supplies the size in bytes of the range to locate.

    HighestAddressToEndAt - Supplies the virtual address that limits
                            the value of the ending address.  The ending
                            address of the located range must be less
                            than this address.

    Alignment - Supplies the alignment for the address.  Must be
                 a power of 2 and greater than the page_size.

    Root - Supplies the root of the tree to search through.

    Base - Receives the starting address of a suitable range on success.

Return Value:

    NTSTATUS.

--*/

{
    PMMADDRESS_NODE Node;
    PMMADDRESS_NODE PreviousNode;
    ULONG_PTR AlignedEndingVa;
    PVOID OptimalStart;
    ULONG_PTR OptimalStartVpn;
    ULONG_PTR HighestVpn;
    ULONG_PTR AlignmentVpn;

    SizeOfRange = MI_ROUND_TO_SIZE (SizeOfRange, PAGE_SIZE);

    if (((ULONG_PTR)HighestAddressToEndAt + 1) < SizeOfRange) {
        return STATUS_NO_MEMORY;
    }

    ASSERT (HighestAddressToEndAt != NULL);
    ASSERT (HighestAddressToEndAt <= (PVOID)((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1));

    HighestVpn = MI_VA_TO_VPN (HighestAddressToEndAt);

    //
    // Locate the Node with the highest starting address.
    //

    OptimalStart = (PVOID)(MI_ALIGN_TO_SIZE(
                           (((ULONG_PTR)HighestAddressToEndAt + 1) - SizeOfRange),
                           Alignment));
    Node = Root;

    if (Node == (PMMADDRESS_NODE)NULL) {

        //
        // The tree is empty, any range is okay.
        //

        *Base = OptimalStart;
        return STATUS_SUCCESS;
    }

    //
    // See if an empty slot exists to hold this range, locate the largest
    // element in the tree.
    //

    while (Node->RightChild != (PMMADDRESS_NODE)NULL) {
        Node = Node->RightChild;
    }

    //
    // Check to see if a range exists between the highest address VAD
    // and the highest address to end at.
    //

    AlignedEndingVa = (ULONG_PTR)MI_ROUND_TO_SIZE ((ULONG_PTR)MI_VPN_TO_VA_ENDING (Node->EndingVpn),
                                               Alignment);

    if (AlignedEndingVa < (ULONG_PTR)HighestAddressToEndAt) {

        if ( SizeOfRange < ((ULONG_PTR)HighestAddressToEndAt - AlignedEndingVa)) {

            *Base = MI_ALIGN_TO_SIZE(
                                  ((ULONG_PTR)HighestAddressToEndAt - SizeOfRange),
                                  Alignment);
            return STATUS_SUCCESS;
        }
    }

    //
    // Walk the tree backwards looking for a fit.
    //

    OptimalStartVpn = MI_VA_TO_VPN (OptimalStart);
    AlignmentVpn = MI_VA_TO_VPN (Alignment);

    for (;;) {

        PreviousNode = MiGetPreviousNode (Node);

        if (PreviousNode != (PMMADDRESS_NODE)NULL) {

            //
            // Is the ending Va below the top of the address to end at.
            //

            if (PreviousNode->EndingVpn < OptimalStartVpn) {
                if ((SizeOfRange >> PAGE_SHIFT) <=
                    ((ULONG_PTR)Node->StartingVpn -
                    (ULONG_PTR)MI_ROUND_TO_SIZE(1 + PreviousNode->EndingVpn,
                                            AlignmentVpn))) {

                    //
                    // See if the optimal start will fit between these
                    // two VADs.
                    //

                    if ((OptimalStartVpn > PreviousNode->EndingVpn) &&
                        (HighestVpn < Node->StartingVpn)) {
                        *Base = OptimalStart;
                        return STATUS_SUCCESS;
                    }

                    //
                    // Check to ensure that the ending address aligned upwards
                    // is not greater than the starting address.
                    //

                    if ((ULONG_PTR)Node->StartingVpn >
                            (ULONG_PTR)MI_ROUND_TO_SIZE(1 + PreviousNode->EndingVpn,
                                                    AlignmentVpn)) {

                        *Base = MI_ALIGN_TO_SIZE(
                                            (ULONG_PTR)MI_VPN_TO_VA (Node->StartingVpn) - SizeOfRange,
                                            Alignment);
                        return STATUS_SUCCESS;
                    }
                }
            }
        } else {

            //
            // No more descriptors, check to see if this fits into the remainder
            // of the address space.
            //

            if (Node->StartingVpn > MI_VA_TO_VPN (MM_LOWEST_USER_ADDRESS)) {
                if ((SizeOfRange >> PAGE_SHIFT) <=
                    ((ULONG_PTR)Node->StartingVpn - MI_VA_TO_VPN (MM_LOWEST_USER_ADDRESS))) {

                    //
                    // See if the optimal start will fit between these
                    // two VADs.
                    //

                    if (HighestVpn < Node->StartingVpn) {
                        *Base = OptimalStart;
                        return STATUS_SUCCESS;
                    }

                    *Base = MI_ALIGN_TO_SIZE(
                                  (ULONG_PTR)MI_VPN_TO_VA (Node->StartingVpn) - SizeOfRange,
                                  Alignment);
                    return STATUS_SUCCESS;
                }
            }
            return STATUS_NO_MEMORY;
        }
        Node = PreviousNode;
    }
}
#if DBG
VOID
NodeTreeWalk (
    PMMADDRESS_NODE Start
    )

{
    if (Start == (PMMADDRESS_NODE)NULL) {
        return;
    }

    NodeTreeWalk(Start->LeftChild);

    DbgPrint("Node at 0x%p start 0x%p  end 0x%p \n",
                    (ULONG_PTR)Start,
                    MI_VPN_TO_VA(Start->StartingVpn),
                    (ULONG_PTR)MI_VPN_TO_VA (Start->EndingVpn) | (PAGE_SIZE - 1));


    NodeTreeWalk(Start->RightChild);
    return;
}
#endif //DBG
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\buildmdl.c ===
/*++

Copyright (c) 1999 Microsoft Corporation

Module Name:

    buildmdl.c

Abstract:

    This module contains the Mm support routines for the cache manager to
    prefetching groups of pages from secondary storage using logical file
    offets instead of virtual addresses.  This saves the cache manager from
    having to map pages unnecessarily.

    The caller builds a list of various file objects and logical block offsets,
    passing them to MmPrefetchPagesIntoLockedMdl.  The code here then examines
    the internal pages, reading in those that are not already valid or in
    transition.  These pages are read with a single read, using a dummy page
    to bridge gaps of pages that were valid or transition prior to the I/O
    being issued.

    Upon conclusion of the I/O, control is returned to the calling thread.
    All pages are referenced counted as though they were probed and locked,
    regardless of whether they are currently valid or transition.

Author:

    Landy Wang (landyw) 12-Feb-2001

Revision History:

--*/

#include "mi.h"

#if DBG

ULONG MiCcDebug;

#define MI_CC_FORCE_PREFETCH    0x1     // Trim all user pages to force prefetch
#define MI_CC_DELAY             0x2     // Delay hoping to trigger collisions

#endif

typedef struct _MI_READ_INFO {

    PCONTROL_AREA ControlArea;
    PFILE_OBJECT FileObject;
    LARGE_INTEGER FileOffset;
    PMMINPAGE_SUPPORT InPageSupport;
    PMDL IoMdl;
    PMDL ApiMdl;
    PMMPFN DummyPagePfn;
    PSUBSECTION FirstReferencedSubsection;
    PSUBSECTION LastReferencedSubsection;
    SIZE_T LengthInBytes;

} MI_READ_INFO, *PMI_READ_INFO;

VOID
MiCcReleasePrefetchResources (
    IN PMI_READ_INFO MiReadInfo,
    IN NTSTATUS Status
    );

NTSTATUS
MiCcPrepareReadInfo (
    IN PMI_READ_INFO MiReadInfo
    );

NTSTATUS
MiCcPutPagesInTransition (
    IN PMI_READ_INFO MiReadInfo
    );

NTSTATUS
MiCcCompletePrefetchIos (
    PMI_READ_INFO MiReadInfo
    );

VOID
MiRemoveUserPages (
    VOID
    );

VOID
MiPfFreeDummyPage (
    IN PMMPFN DummyPagePfn
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text (PAGE, MmPrefetchPagesIntoLockedMdl)
#pragma alloc_text (PAGE, MiCcPrepareReadInfo)
#pragma alloc_text (PAGE, MiCcReleasePrefetchResources)
#endif


NTSTATUS
MmPrefetchPagesIntoLockedMdl (
    IN PFILE_OBJECT FileObject,
    IN PLARGE_INTEGER FileOffset,
    IN SIZE_T Length,
    OUT PMDL *MdlOut
    )

/*++

Routine Description:

    This routine fills an MDL with pages described by the file object's
    offset and length.

    This routine is for cache manager usage only.

Arguments:

    FileObject - Supplies a pointer to the file object for a file which was
                 opened with NO_INTERMEDIATE_BUFFERING clear, i.e., for
                 which CcInitializeCacheMap was called by the file system.

    FileOffset - Supplies the byte offset in the file for the desired data.

    Length - Supplies the length of the desired data in bytes.

    MdlOut - On output it returns a pointer to an Mdl describing
             the desired data.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    MI_READ_INFO MiReadInfo;
    NTSTATUS status;
    KIRQL OldIrql;
    LOGICAL ApcNeeded;
    PETHREAD CurrentThread;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    RtlZeroMemory (&MiReadInfo, sizeof(MiReadInfo));

    MiReadInfo.FileObject = FileObject;
    MiReadInfo.FileOffset = *FileOffset;
    MiReadInfo.LengthInBytes = Length;

    //
    // Prepare for the impending read : allocate MDLs, inpage blocks,
    // reference count subsections, etc.
    //

    status = MiCcPrepareReadInfo (&MiReadInfo);

    if (!NT_SUCCESS (status)) {
        MiCcReleasePrefetchResources (&MiReadInfo, status);
        return status;
    }

    ASSERT (MiReadInfo.InPageSupport != NULL);

    //
    // APCs must be disabled once we put a page in transition.  Otherwise
    // a thread suspend will stop us from issuing the I/O - this will hang
    // any other threads that need the same page.
    //

    CurrentThread = PsGetCurrentThread();
    ApcNeeded = FALSE;

    ASSERT ((PKTHREAD)CurrentThread == KeGetCurrentThread ());
    KeEnterCriticalRegionThread ((PKTHREAD)CurrentThread);

    //
    // The nested fault count protects this thread from deadlocks where a
    // special kernel APC fires and references the same user page(s) we are
    // putting in transition.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    CurrentThread->NestedFaultCount += 1;
    KeLowerIrql (OldIrql);

    //
    // Allocate physical memory, lock down all the pages and issue any
    // I/O that may be needed.  When MiCcPutPagesInTransition returns
    // STATUS_SUCCESS or STATUS_ISSUE_PAGING_IO, it guarantees that the
    // ApiMdl contains reference-counted (locked-down) pages.
    //

    status = MiCcPutPagesInTransition (&MiReadInfo);

    if (NT_SUCCESS (status)) {

        //
        // No I/O was issued because all the pages were already resident and
        // have now been locked down.
        //

        ASSERT (MiReadInfo.ApiMdl != NULL);
    }
    else if (status == STATUS_ISSUE_PAGING_IO) {

        //
        // Wait for the I/O to complete.  Note APCs must remain disabled.
        //

        ASSERT (MiReadInfo.InPageSupport != NULL);
    
        status = MiCcCompletePrefetchIos (&MiReadInfo);
    }
    else {

        //
        // Some error occurred (like insufficient memory, etc) so fail
        // the request by falling through.
        //
    }

    //
    // Release acquired resources like pool, subsections, etc.
    //

    MiCcReleasePrefetchResources (&MiReadInfo, status);

    //
    // Only now that the I/O have been completed (not just issued) can
    // APCs be re-enabled.  This prevents a user-issued suspend APC from
    // keeping a shared page in transition forever.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);

    ASSERT (CurrentThread->NestedFaultCount == 1);

    CurrentThread->NestedFaultCount -= 1;

    if (CurrentThread->ApcNeeded == 1) {
        ApcNeeded = TRUE;
        CurrentThread->ApcNeeded = 0;
    }

    KeLowerIrql (OldIrql);

    KeLeaveCriticalRegionThread ((PKTHREAD)CurrentThread);

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    ASSERT (CurrentThread->ApcNeeded == 0);

    if (ApcNeeded == TRUE) {
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        IoRetryIrpCompletions ();
        KeLowerIrql (OldIrql);
    }

    *MdlOut = MiReadInfo.ApiMdl;

    return status;
}

VOID
MiCcReleasePrefetchResources (
    IN PMI_READ_INFO MiReadInfo,
    IN NTSTATUS Status
    )

/*++

Routine Description:

    This routine releases all resources consumed to handle a system cache
    logical offset based prefetch.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PSUBSECTION FirstReferencedSubsection;
    PSUBSECTION LastReferencedSubsection;

    //
    // Release all subsection prototype PTE references. 
    //

    FirstReferencedSubsection = MiReadInfo->FirstReferencedSubsection;
    LastReferencedSubsection = MiReadInfo->LastReferencedSubsection;

    while (FirstReferencedSubsection != LastReferencedSubsection) {
        MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION) FirstReferencedSubsection,
                                         FirstReferencedSubsection->PtesInSubsection);
        FirstReferencedSubsection = FirstReferencedSubsection->NextSubsection;
    }

    if (MiReadInfo->IoMdl != NULL) {
        ExFreePool (MiReadInfo->IoMdl);
    }

    //
    // Note successful returns yield the ApiMdl so don't free it here.
    //

    if (!NT_SUCCESS (Status)) {
        if (MiReadInfo->ApiMdl != NULL) {
            ExFreePool (MiReadInfo->ApiMdl);
        }
    }

    if (MiReadInfo->InPageSupport != NULL) {

#if DBG
        MiReadInfo->InPageSupport->ListEntry.Next = NULL;
#endif

        MiFreeInPageSupportBlock (MiReadInfo->InPageSupport);
    }

    //
    // Put DummyPage back on the free list.
    //

    if (MiReadInfo->DummyPagePfn != NULL) {
        MiPfFreeDummyPage (MiReadInfo->DummyPagePfn);
    }
}


NTSTATUS
MiCcPrepareReadInfo (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine constructs MDLs that describe the pages in the argument
    read-list. The caller will then issue the I/O on return.

Arguments:

    MiReadInfo - Supplies a pointer to the read-list.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    ULONG PteOffset;
    NTSTATUS Status;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PMMPTE *ProtoPteArray;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMMINPAGE_SUPPORT InPageSupport;
    PMDL Mdl;
    PMDL IoMdl;
    PMDL ApiMdl;
    ULONG i;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MiReadInfo->FileOffset.LowPart, MiReadInfo->LengthInBytes);

    //
    // Translate the section object into the relevant control area.
    //

    ControlArea = (PCONTROL_AREA)MiReadInfo->FileObject->SectionObjectPointer->DataSectionObject;

    //
    // Initialize the internal Mi readlist.
    //

    MiReadInfo->ControlArea = ControlArea;

    //
    // Allocate and initialize an inpage support block for this run.
    //

    InPageSupport = MiGetInPageSupportBlock (FALSE, PREFETCH_PROCESS);
    
    if (InPageSupport == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }
    
    MiReadInfo->InPageSupport = InPageSupport;

    //
    // Allocate and initialize an MDL to return to our caller.  The actual
    // frame numbers are filled in when all the pages are reference counted.
    //

    ApiMdl = MmCreateMdl (NULL, NULL, NumberOfPages << PAGE_SHIFT);

    if (ApiMdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    ApiMdl->MdlFlags |= MDL_PAGES_LOCKED;

    MiReadInfo->ApiMdl = ApiMdl;

    //
    // Allocate and initialize an MDL to use for the actual transfer (if any).
    //

    IoMdl = MmCreateMdl (NULL, NULL, NumberOfPages << PAGE_SHIFT);

    if (IoMdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MiReadInfo->IoMdl = IoMdl;
    Mdl = IoMdl;

    //
    // If the section is backed by a ROM, then there's no need to prefetch
    // anything as it would waste RAM.
    //

    if (ControlArea->u.Flags.Rom == 1) {
		ASSERT (XIPConfigured == TRUE);

#if 0
        //
        // Calculate the offset to read into the file.
        //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
        //

        StartingOffset.QuadPart = MiReadInfo.FileOffset;

        TempOffset = MiEndingOffset(Subsection);

        ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

        PageFrameIndex = (PFN_NUMBER) (StartingOffset.QuadPart >> PAGE_SHIFT);
        PageFrameIndex += ((PLARGE_CONTROL_AREA)ControlArea)->StartingFrame;

        //
        // Increment the PFN reference count in the control area for
        // the subsection (the PFN lock is required to modify this field).
        //

        ControlArea->NumberOfPfnReferences += 1;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u3.e1.Rom == 1);

        if (Pfn1->u3.e1.PageLocation != 0) {

            ASSERT (Pfn1->u3.e1.PageLocation == StandbyPageList);

            MiUnlinkPageFromList (Pfn1);

            //
            // Update the PFN database - the reference count must be
            // incremented as the share count is going to go from zero to 1.
            //

            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

            Pfn1->u3.e2.ReferenceCount += 1;
            Pfn1->u2.ShareCount += 1;
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            ASSERT (Pfn1->PteAddress == PointerPte);
            ASSERT (Pfn1->u1.Event == NULL);

            //
            // Determine the page frame number of the page table page which
            // contains this PTE.
            //

            PteFramePointer = MiGetPteAddress (PointerPte);
            if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x61940,
                              (ULONG_PTR)PointerPte,
                              (ULONG_PTR)PteFramePointer->u.Long,
                              0);
#if (_MI_PAGING_LEVELS < 3)
                }
#endif
            }

            PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
            ASSERT (Pfn1->PteFrame == PteFramePage);

            //
            // Increment the share count for the page table page containing
            // this PTE as the PTE is going to be made valid.
            //

            ASSERT (PteFramePage != 0);
            Pfn2 = MI_PFN_ELEMENT (PteFramePage);
            Pfn2->u2.ShareCount += 1;
        }
        else {
            ASSERT (Pfn1->u3.e1.InPageError == 0);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);
            ASSERT (Pfn1->u1.Event == NULL);
            MiInitializePfn (PageFrameIndex, PointerPte, 0);
        }

        //
        // Put the prototype PTE into the valid state.
        // LWFIX: don't make proto valid
        //

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           PointerPte->u.Soft.Protection,
                           PointerPte);

        MI_WRITE_VALID_PTE (PointerPte, TempPte);
#endif

        // LWFIX: not success, but what ? and who frees pool

        return STATUS_SUCCESS;
    }

    //
    // Make sure the section is really prefetchable - physical and
    // pagefile-backed sections are not.
    //

    if ((ControlArea->u.Flags.PhysicalMemory) ||
         (ControlArea->u.Flags.Image == 1) ||
         (ControlArea->FilePointer == NULL)) {

        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Start the read at the proper file offset.
    //

    InPageSupport->ReadOffset = MiReadInfo->FileOffset;
    ASSERT (BYTE_OFFSET (InPageSupport->ReadOffset.LowPart) == 0);
    InPageSupport->FilePointer = MiReadInfo->FileObject;

    //
    // Stash a pointer to the start of the prototype PTE array (the values
    // in the array are not contiguous as they may cross subsections)
    // in the inpage block so we can walk it quickly later when the pages
    // are put into transition.
    //

    ProtoPteArray = (PMMPTE *)(Mdl + 1);

    InPageSupport->BasePte = (PMMPTE) ProtoPteArray;

    //
    // Data (but not image) reads use the whole page and the filesystems
    // zero fill any remainder beyond valid data length so we don't
    // bother to handle this here.  It is important to specify the
    // entire page where possible so the filesystem won't post this
    // which will hurt perf.  LWFIX: must use CcZero to make this true.
    //

    ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
    InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);

    //
    // Initialize the prototype PTE pointers.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

#if DBG
    if (MiCcDebug & MI_CC_FORCE_PREFETCH) {
        MiRemoveUserPages ();
    }
#endif

    //
    // Calculate the first prototype PTE address.
    //

    PteOffset = (ULONG)(MiReadInfo->FileOffset.QuadPart >> PAGE_SHIFT);

    //
    // Make sure the PTEs are not in the extended part of the segment.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
    }

    Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION) Subsection,
                                          Subsection->PtesInSubsection);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    MiReadInfo->FirstReferencedSubsection = Subsection;
    MiReadInfo->LastReferencedSubsection = Subsection;

    ProtoPte = &Subsection->SubsectionBase[PteOffset];
    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    for (i = 0; i < NumberOfPages; i += 1) {

        //
        // Calculate which PTE maps the given logical block offset.
        //
        // Always look forwards (as an optimization) in the subsection chain.
        //
        // A quick check is made first to avoid recalculations and loops where
        // possible.
        //
    
        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.  Increment the view count for
            // every subsection spanned by this request, creating prototype
            // PTEs if needed.
            //

            ASSERT (i != 0);

            Subsection = Subsection->NextSubsection;

            Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION) Subsection,
                                                  Subsection->PtesInSubsection);

            if (!NT_SUCCESS (Status)) {
                return Status;
            }

            MiReadInfo->LastReferencedSubsection = Subsection;

            ProtoPte = Subsection->SubsectionBase;

            LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }

        *ProtoPteArray = ProtoPte;
        ProtoPteArray += 1;

        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiCcPutPagesInTransition (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine allocates physical memory for the specified read-list and
    puts all the pages in transition (so collided faults from other threads
    for these same pages remain coherent).  I/O for any pages not already
    resident are issued here.  The caller must wait for their completion.

Arguments:

    MiReadInfo - Supplies a pointer to the read-list.

Return Value:

    STATUS_SUCCESS - all the pages were already resident, reference counts
                     have been applied and no I/O needs to be waited for.

    STATUS_ISSUE_PAGING_IO - the I/O has been issued and the caller must wait.

    Various other failure status values indicate the operation failed.

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    NTSTATUS status;
    PMMPTE LocalPrototypePte;
    PVOID StartingVa;
    PFN_NUMBER MdlPages;
    KIRQL OldIrql;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER ResidentAvailableCharge;
    PPFN_NUMBER IoPage;
    PPFN_NUMBER ApiPage;
    PPFN_NUMBER Page;
    PPFN_NUMBER DestinationPage;
    ULONG PageColor;
    PMMPTE PointerPte;
    PMMPTE *ProtoPteArray;
    PMMPTE *EndProtoPteArray;
    PFN_NUMBER DummyPage;
    PMDL Mdl;
    PMDL FreeMdl;
    PMMPFN PfnProto;
    PMMPFN Pfn1;
    PMMPFN DummyPfn1;
    ULONG i;
    PFN_NUMBER DummyTrim;
    ULONG NumberOfPagesNeedingIo;
    MMPTE TempPte;
    PMMPTE PointerPde;
    PEPROCESS CurrentProcess;
    PMMINPAGE_SUPPORT InPageSupport;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    MiReadInfo->DummyPagePfn = NULL;

    FreeMdl = NULL;
    CurrentProcess = PsGetCurrentProcess();

    PfnProto = NULL;
    PointerPde = NULL;

    InPageSupport = MiReadInfo->InPageSupport;
    
    Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
    ASSERT (Mdl == MiReadInfo->IoMdl);

    IoPage = (PPFN_NUMBER)(Mdl + 1);
    ApiPage = (PPFN_NUMBER)(MiReadInfo->ApiMdl + 1);

    StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
    MdlPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               Mdl->ByteCount);

    if (MdlPages + 1 > MAXUSHORT) {

        //
        // The PFN ReferenceCount for the dummy page could wrap, refuse the
        // request.
        //

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    NumberOfPagesNeedingIo = 0;

    ProtoPteArray = (PMMPTE *)InPageSupport->BasePte;
    EndProtoPteArray = ProtoPteArray + MdlPages;

    ASSERT (*ProtoPteArray != NULL);

    LOCK_PFN (OldIrql);

    //
    // Ensure sufficient pages exist for the transfer plus the dummy page.
    //

    if ((MmAvailablePages <= MdlPages) ||
        (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)MdlPages)) {

        UNLOCK_PFN (OldIrql);

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Charge resident available immediately as the PFN lock may get released
    // and reacquired below before all the pages have been locked down.
    // Note the dummy page is immediately charged separately.
    //

    MmResidentAvailablePages -= MdlPages;

    MM_BUMP_COUNTER(46, MdlPages);

    ResidentAvailableCharge = MdlPages;

    //
    // Allocate a dummy page to map discarded pages that aren't skipped.
    //

    DummyPage = MiRemoveAnyPage (0);
    Pfn1 = MI_PFN_ELEMENT (DummyPage);

    ASSERT (Pfn1->u2.ShareCount == 0);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    MiInitializePfnForOtherProcess (DummyPage, MI_PF_DUMMY_PAGE_PTE, 0);

    //
    // Always bias the reference count by 1 and charge for this locked page
    // up front so the myriad increments and decrements don't get slowed
    // down with needless checking.
    //

    MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 42);
    Pfn1->u3.e2.ReferenceCount += 1;

    Pfn1->u3.e1.ReadInProgress = 1;
    Pfn1->u3.e1.PrototypePte = 0;

    MiReadInfo->DummyPagePfn = Pfn1;

    DummyPfn1 = Pfn1;

    DummyPfn1->u3.e2.ReferenceCount =
        (USHORT)(DummyPfn1->u3.e2.ReferenceCount + MdlPages);

    //
    // Properly initialize the inpage support block fields we overloaded.
    //

    InPageSupport->BasePte = *ProtoPteArray;

    //
    // Build the proper InPageSupport and MDL to describe this run.
    //

    for (; ProtoPteArray < EndProtoPteArray; ProtoPteArray += 1, IoPage += 1, ApiPage += 1) {
    
        //
        // Fill the MDL entry for this RLE.
        //
    
        PointerPte = *ProtoPteArray;

        ASSERT (PointerPte != NULL);

        //
        // The PointerPte better be inside a prototype PTE allocation
        // so that subsequent page trims update the correct PTEs.
        //

        ASSERT (((PointerPte >= (PMMPTE)MmPagedPoolStart) &&
                (PointerPte <= (PMMPTE)MmPagedPoolEnd)) ||
                ((PointerPte >= (PMMPTE)MmSpecialPoolStart) && (PointerPte <= (PMMPTE)MmSpecialPoolEnd)));

        //
        // Check the state of this prototype PTE now that the PFN lock is held.
        // If the page is not resident, the PTE must be put in transition with
        // read in progress before the PFN lock is released.
        //

        //
        // Lock page containing prototype PTEs in memory by
        // incrementing the reference count for the page.
        // Unlock any page locked earlier containing prototype PTEs if
        // the containing page is not the same for both.
        //

        if (PfnProto != NULL) {

            if (PointerPde != MiGetPteAddress (PointerPte)) {

                ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 43);
                PfnProto = NULL;
            }
        }

        if (PfnProto == NULL) {

            ASSERT (!MI_IS_PHYSICAL_ADDRESS (PointerPte));
   
            PointerPde = MiGetPteAddress (PointerPte);
 
            if (PointerPde->u.Hard.Valid == 0) {
                MiMakeSystemAddressValidPfn (PointerPte);
            }

            PfnProto = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
            MI_ADD_LOCKED_PAGE_CHARGE(PfnProto, 44);
            PfnProto->u3.e2.ReferenceCount += 1;
            ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
        }

recheck:
        PteContents = *PointerPte;

        // LWFIX: are zero or dzero ptes possible here ?
        ASSERT (PteContents.u.Long != ZeroKernelPte.u.Long);

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);
            MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 45);
            Pfn1->u3.e2.ReferenceCount += 1;
            *ApiPage = PageFrameIndex;
            *IoPage = DummyPage;
            continue;
        }

        if ((PteContents.u.Soft.Prototype == 0) &&
            (PteContents.u.Soft.Transition == 1)) {

            //
            // The page is in transition.  If there is an inpage still in
            // progress, wait for it to complete.  Reference the PFN and
            // then march on.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);

            if (Pfn1->u4.InPageError) {

                //
                // There was an in-page read error and there are other
                // threads colliding for this page, delay to let the
                // other threads complete and then retry.
                //

                UNLOCK_PFN (OldIrql);
                KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
                LOCK_PFN (OldIrql);
                goto recheck;
            }

            if (Pfn1->u3.e1.ReadInProgress) {
                    // LWFIX - start with temp\aw.c
            }

            //
            // PTE refers to a normal transition PTE.
            //

            ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);

            if (MmAvailablePages == 0) {

                //
                // This can only happen if the system is utilizing a hardware
                // compression cache.  This ensures that only a safe amount
                // of the compressed virtual cache is directly mapped so that
                // if the hardware gets into trouble, we can bail it out.
                //

                UNLOCK_PFN (OldIrql);
                KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
                LOCK_PFN (OldIrql);
                goto recheck;
            }

            //
            // The PFN reference count will be 1 already here if the
            // modified writer has begun a write of this page.  Otherwise
            // it's ordinarily 0.
            //

            MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 46);

            Pfn1->u3.e2.ReferenceCount += 1;

            *IoPage = DummyPage;
            *ApiPage = PageFrameIndex;
            continue;
        }

        ASSERT (PteContents.u.Soft.Prototype == 1);

        if (MiEnsureAvailablePageOrWait (NULL, NULL)) {

            //
            // Had to wait so recheck all state.
            //

            goto recheck;
        }

        NumberOfPagesNeedingIo += 1;

        //
        // Allocate a physical page.
        //

        PageColor = MI_PAGE_COLOR_VA_PROCESS (
                        MiGetVirtualAddressMappedByPte (PointerPte),
                        &CurrentProcess->NextPageColor);

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (PointerPte->u.Hard.Valid == 0);

        //
        // Initialize read-in-progress PFN.
        //
    
        MiInitializePfn (PageFrameIndex, PointerPte, 0);

        //
        // These pieces of MiInitializePfn initialization are overridden
        // here as these pages are only going into prototype
        // transition and not into any page tables.
        //

        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 47);
        Pfn1->u2.ShareCount -= 1;
        Pfn1->u3.e1.PageLocation = ZeroedPageList;

        //
        // Initialize the I/O specific fields.
        //
    
        Pfn1->u1.Event = &InPageSupport->Event;
        Pfn1->u3.e1.ReadInProgress = 1;
        ASSERT (Pfn1->u4.InPageError == 0);
        Pfn1->u3.e1.PrototypePte = 1;

        //
        // Increment the PFN reference count in the control area for
        // the subsection.
        //

        MiReadInfo->ControlArea->NumberOfPfnReferences += 1;
    
        //
        // Put the prototype PTE into the transition state.
        //

        MI_MAKE_TRANSITION_PTE (TempPte,
                                PageFrameIndex,
                                PointerPte->u.Soft.Protection,
                                PointerPte);

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        *IoPage = PageFrameIndex;
        *ApiPage = PageFrameIndex;
    }
    
    //
    // Return the upfront resident available charge as the individual charges
    // have all been made at this point.
    //

    MmResidentAvailablePages += ResidentAvailableCharge;

    MM_BUMP_COUNTER(47, MdlPages);

    //
    // If all the pages were resident, dereference the dummy page references
    // now and notify our caller that I/O is not necessary.
    //
    
    if (NumberOfPagesNeedingIo == 0) {
        ASSERT (DummyPfn1->u3.e2.ReferenceCount > MdlPages);
        DummyPfn1->u3.e2.ReferenceCount =
            (USHORT)(DummyPfn1->u3.e2.ReferenceCount - MdlPages);

        //
        // Unlock page containing prototype PTEs.
        //

        if (PfnProto != NULL) {
            ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 48);
        }

        UNLOCK_PFN (OldIrql);

        return STATUS_SUCCESS;
    }

    //
    // Carefully trim leading dummy pages.
    //

    Page = (PPFN_NUMBER)(Mdl + 1);

    DummyTrim = 0;
    for (i = 0; i < MdlPages - 1; i += 1) {
        if (*Page == DummyPage) {
            DummyTrim += 1;
            Page += 1;
        }
        else {
            break;
        }
    }

    if (DummyTrim != 0) {

        Mdl->Size = (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
        Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
        ASSERT (Mdl->ByteCount != 0);
        InPageSupport->ReadOffset.QuadPart += (DummyTrim * PAGE_SIZE);
        DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);

        //
        // Shuffle down the PFNs in the MDL.
        // Recalculate BasePte to adjust for the shuffle.
        //

        Pfn1 = MI_PFN_ELEMENT (*Page);

        ASSERT (Pfn1->PteAddress->u.Hard.Valid == 0);
        ASSERT ((Pfn1->PteAddress->u.Soft.Prototype == 0) &&
                 (Pfn1->PteAddress->u.Soft.Transition == 1));

        InPageSupport->BasePte = Pfn1->PteAddress;

        DestinationPage = (PPFN_NUMBER)(Mdl + 1);

        do {
            *DestinationPage = *Page;
            DestinationPage += 1;
            Page += 1;
            i += 1;
        } while (i < MdlPages);

        MdlPages -= DummyTrim;
    }

    //
    // Carefully trim trailing dummy pages.
    //

    ASSERT (MdlPages != 0);

    Page = (PPFN_NUMBER)(Mdl + 1) + MdlPages - 1;

    if (*Page == DummyPage) {

        ASSERT (MdlPages >= 2);

        //
        // Trim the last page specially as it may be a partial page.
        //

        Mdl->Size -= sizeof(PFN_NUMBER);
        if (BYTE_OFFSET(Mdl->ByteCount) != 0) {
            Mdl->ByteCount &= ~(PAGE_SIZE - 1);
        }
        else {
            Mdl->ByteCount -= PAGE_SIZE;
        }
        ASSERT (Mdl->ByteCount != 0);
        DummyPfn1->u3.e2.ReferenceCount -= 1;

        //
        // Now trim any other trailing pages.
        //

        Page -= 1;
        DummyTrim = 0;
        while (Page != ((PPFN_NUMBER)(Mdl + 1))) {
            if (*Page != DummyPage) {
                break;
            }
            DummyTrim += 1;
            Page -= 1;
        }
        if (DummyTrim != 0) {
            ASSERT (Mdl->Size > (USHORT)(DummyTrim * sizeof(PFN_NUMBER)));
            Mdl->Size = (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
            Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
            DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);
        }

        ASSERT (MdlPages > DummyTrim + 1);
        MdlPages -= (DummyTrim + 1);

#if DBG
        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        ASSERT (MdlPages == ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                               Mdl->ByteCount));
#endif
    }

    //
    // If the MDL is not already embedded in the inpage block, see if its
    // final size qualifies it - if so, embed it now.
    //

    if ((Mdl != &InPageSupport->Mdl) &&
        (Mdl->ByteCount <= (MM_MAXIMUM_READ_CLUSTER_SIZE + 1) * PAGE_SIZE)){

#if DBG
        RtlFillMemoryUlong (&InPageSupport->Page[0],
                            (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof (PFN_NUMBER),
                            0xf1f1f1f1);
#endif

        RtlCopyMemory (&InPageSupport->Mdl, Mdl, Mdl->Size);

        FreeMdl = Mdl;

        Mdl = &InPageSupport->Mdl;

        ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
        InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);
    }

    ASSERT (MdlPages != 0);

    ASSERT (Mdl->Size - sizeof(MDL) == BYTES_TO_PAGES(Mdl->ByteCount) * sizeof(PFN_NUMBER));

    DummyPfn1->u3.e2.ReferenceCount =
        (USHORT)(DummyPfn1->u3.e2.ReferenceCount - NumberOfPagesNeedingIo);
    
    MmInfoCounters.PageReadIoCount += 1;
    MmInfoCounters.PageReadCount += NumberOfPagesNeedingIo;

    //
    // Unlock page containing prototype PTEs.
    //

    if (PfnProto != NULL) {
        ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 49);
    }

    UNLOCK_PFN (OldIrql);

    if (FreeMdl != NULL) {
        ASSERT (MiReadInfo->IoMdl == FreeMdl);
        MiReadInfo->IoMdl = NULL;
        ExFreePool (FreeMdl);
    }

#if DBG

    if (MiCcDebug & MI_CC_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

    //
    // Finish initialization of the prefetch MDL (and the API MDL).
    //
    
    ASSERT ((Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
    Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

    ASSERT (InPageSupport->u1.e1.Completed == 0);
    ASSERT (InPageSupport->Thread == PsGetCurrentThread());
    ASSERT64 (InPageSupport->UsedPageTableEntries == 0);
    ASSERT (InPageSupport->WaitCount >= 1);
    ASSERT (InPageSupport->u1.e1.PrefetchMdlHighBits != 0);

    //
    // The API caller expects an MDL containing all the locked pages so
    // it can be used for a transfer.
    //
    // Note that an extra reference count is not taken on each page -
    // rather when the Io MDL completes, its reference counts are not
    // decremented (except for the dummy page).  This combined with the
    // reference count already taken on the resident pages keeps the
    // accounting correct.  Only if an error occurs will the Io MDL
    // completion decrement the reference counts.
    //

    //
    // Initialize the inpage support block Pfn field.
    //

    LocalPrototypePte = InPageSupport->BasePte;

    ASSERT (LocalPrototypePte->u.Hard.Valid == 0);
    ASSERT ((LocalPrototypePte->u.Soft.Prototype == 0) &&
             (LocalPrototypePte->u.Soft.Transition == 1));

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(LocalPrototypePte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    InPageSupport->Pfn = Pfn1;

    //
    // Issue the paging I/O.
    //

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    status = IoAsynchronousPageRead (InPageSupport->FilePointer,
                                     Mdl,
                                     &InPageSupport->ReadOffset,
                                     &InPageSupport->Event,
                                     &InPageSupport->IoStatus);

    if (!NT_SUCCESS (status)) {

        //
        // Set the event as the I/O system doesn't set it on errors.
        // This way our caller will automatically unroll the PFN reference
        // counts, etc, when the MiWaitForInPageComplete returns this status.
        //

        InPageSupport->IoStatus.Status = status;
        InPageSupport->IoStatus.Information = 0;
        KeSetEvent (&InPageSupport->Event, 0, FALSE);
    }

#if DBG

    if (MiCcDebug & MI_CC_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

    return STATUS_ISSUE_PAGING_IO;
}


NTSTATUS
MiCcCompletePrefetchIos (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine waits for a series of page reads to complete
    and completes the requests.

Arguments:

    MiReadInfo - Pointer to the read-list.

Return Value:

    NTSTATUS of the I/O request.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PMDL Mdl;
    PMMPFN Pfn1;
    PMMPFN PfnClusterPage;
    PPFN_NUMBER Page;
    NTSTATUS status;
    LONG NumberOfBytes;
    PMMINPAGE_SUPPORT InPageSupport;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    InPageSupport = MiReadInfo->InPageSupport;

    ASSERT (InPageSupport->Pfn != 0);

    Pfn1 = InPageSupport->Pfn;
    Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
    Page = (PPFN_NUMBER)(Mdl + 1);

    status = MiWaitForInPageComplete (InPageSupport->Pfn,
                                      InPageSupport->BasePte,
                                      NULL,
                                      InPageSupport->BasePte,
                                      InPageSupport,
                                      PREFETCH_PROCESS);

    //
    // MiWaitForInPageComplete RETURNS WITH THE PFN LOCK HELD!!!
    //

    NumberOfBytes = (LONG)Mdl->ByteCount;

    while (NumberOfBytes > 0) {

        //
        // Only decrement reference counts if an error occurred.
        //

        PfnClusterPage = MI_PFN_ELEMENT (*Page);

#if DBG
        if (PfnClusterPage->u4.InPageError) {

            //
            // If the page is marked with an error, then the whole transfer
            // must be marked as not successful as well.  The only exception
            // is the prefetch dummy page which is used in multiple
            // transfers concurrently and thus may have the inpage error
            // bit set at any time (due to another transaction besides
            // the current one).
            //

            ASSERT ((status != STATUS_SUCCESS) ||
                    (PfnClusterPage->PteAddress == MI_PF_DUMMY_PAGE_PTE));
        }
#endif
        if (PfnClusterPage->u3.e1.ReadInProgress != 0) {

            ASSERT (PfnClusterPage->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
            PfnClusterPage->u3.e1.ReadInProgress = 0;

            if (PfnClusterPage->u4.InPageError == 0) {
                PfnClusterPage->u1.Event = (PKEVENT)NULL;
            }
        }

        //
        // Note the reference count for each page is NOT decremented unless
        // the I/O failed, in which case it is done below.  This allows the
        // MmPrefetchPagesIntoLockedMdl API to return a locked page MDL.
        //

        Page += 1;
        NumberOfBytes -= PAGE_SIZE;
    }

    if (status != STATUS_SUCCESS) {

        //
        // An I/O error occurred during the page read
        // operation.  All the pages which were just
        // put into transition must be put onto the
        // free list if InPageError is set, and their
        // PTEs restored to the proper contents.
        //

        Page = (PPFN_NUMBER)(Mdl + 1);
        NumberOfBytes = (LONG)Mdl->ByteCount;

        while (NumberOfBytes > 0) {

            PfnClusterPage = MI_PFN_ELEMENT (*Page);

            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnClusterPage, 50);

            if (PfnClusterPage->u4.InPageError == 1) {

                if (PfnClusterPage->u3.e2.ReferenceCount == 0) {

                    ASSERT (PfnClusterPage->u3.e1.PageLocation ==
                                                    StandbyPageList);

                    MiUnlinkPageFromList (PfnClusterPage);
                    MiRestoreTransitionPte (*Page);
                    MiInsertPageInFreeList (*Page);
                }
            }
            Page += 1;
            NumberOfBytes -= PAGE_SIZE;
        }
    }

    //
    // All the relevant prototype PTEs should be in the transition or
    // valid states and all page frames should be referenced.
    // LWFIX: add code to checked build to verify this.
    //

    ASSERT (InPageSupport->WaitCount >= 1);
    UNLOCK_PFN (PASSIVE_LEVEL);

#if DBG
    InPageSupport->ListEntry.Next = NULL;
#endif

    MiFreeInPageSupportBlock (InPageSupport);
    MiReadInfo->InPageSupport = NULL;

    return status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\compress.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    compress.c

Abstract:

    This module contains the routines to support allow hardware to
    transparently compress physical memory.

Author:

    Landy Wang (landyw) 21-Oct-2000

Revision History:

--*/

#include "mi.h"

#if defined (_MI_COMPRESSION)

Enable the #if 0 code in cmdat3.c to allow Ratio specification.

//
// Compression public interface.
//

#define MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION      0x1

typedef 
NTSTATUS
(*PMM_SET_COMPRESSION_THRESHOLD) (
    IN ULONGLONG CompressionByteThreshold
    );

typedef struct _MM_COMPRESSION_CONTEXT {
    ULONG Version;
    ULONG SizeInBytes;
    ULONGLONG ReservedBytes;
    PMM_SET_COMPRESSION_THRESHOLD SetCompressionThreshold;
} MM_COMPRESSION_CONTEXT, *PMM_COMPRESSION_CONTEXT;

#define MM_COMPRESSION_VERSION_INITIAL  1
#define MM_COMPRESSION_VERSION_CURRENT  1

NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    );

NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    );

//
// This defaults to 75% but can be overridden in the registry.  At this
// percentage of *real* physical memory in use, an interrupt is generated so
// that memory management can zero pages to make more memory available.
//

#define MI_DEFAULT_COMPRESSION_THRESHOLD    75

ULONG MmCompressionThresholdRatio;

PFN_NUMBER MiNumberOfCompressionPages;

PMM_SET_COMPRESSION_THRESHOLD MiSetCompressionThreshold;

#if DBG
KIRQL MiCompressionIrql;
#endif

//
// Note there is also code in dynmem.c that is dependent on this #define.
//

#if defined (_MI_COMPRESSION_SUPPORTED_)

typedef struct _MI_COMPRESSION_INFO {
    ULONG IsrPageProcessed;
    ULONG DpcPageProcessed;
    ULONG IsrForcedDpc;
    ULONG IsrFailedDpc;

    ULONG IsrRan;
    ULONG DpcRan;
    ULONG DpcsFired;
    ULONG IsrSkippedZeroedPage;

    ULONG DpcSkippedZeroedPage;
    ULONG CtxswapForcedDpcInsert;
    ULONG CtxswapFailedDpcInsert;
    ULONG PfnForcedDpcInsert;

    ULONG PfnFailedDpcInsert;

} MI_COMPRESSION_INFO, *PMI_COMPRESSION_INFO;

MI_COMPRESSION_INFO MiCompressionInfo;      // LWFIX - temp remove.

PFN_NUMBER MiCompressionOverHeadInPages;

PKDPC MiCompressionDpcArray;
CCHAR MiCompressionProcessors;

VOID
MiCompressionDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    );

PVOID
MiMapCompressionInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiUnmapCompressionInHyperSpace (
    VOID
    );

SIZE_T
MiMakeCompressibleMemoryAtDispatch (
    IN SIZE_T NumberOfBytes OPTIONAL
    );


NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )

/*++

Routine Description:

    This routine notifies memory management that compression hardware exists
    in the system.  Memory management responds by initializing compression
    support here.

Arguments:

    Context - Supplies the compression context pointer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER OverHeadInPages;
    CCHAR Processor;
    CCHAR NumberProcessors;
    PKDPC CompressionDpcArray;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    if (Context->Version != MM_COMPRESSION_VERSION_CURRENT) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (Context->SizeInBytes < sizeof (MM_COMPRESSION_CONTEXT)) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // If the subsequent hot-add cannot succeed then fail this API now.
    //

    if (MmDynamicPfn == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    //
    // Hardware that can't generate a configurable interrupt is not supported.
    //

    if (Context->SetCompressionThreshold == NULL) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // ReservedBytes indicates the number of reserved bytes required by the
    // underlying hardware.  For example, some hardware might have:
    //                    
    //  1.  translation tables which are 1/64 of the fictional RAM total.
    //
    //  2.  the first MB of memory is never compressed.
    //
    //  3.  an L3 which is never compressed.
    //
    //  etc.
    //
    //  ReservedBytes would be the sum of all of these types of ranges.
    //

    OverHeadInPages = (PFN_COUNT)(Context->ReservedBytes / PAGE_SIZE);

    if (MmResidentAvailablePages < (SPFN_NUMBER) OverHeadInPages) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (MmAvailablePages < OverHeadInPages) {
        MmEmptyAllWorkingSets ();
        if (MmAvailablePages < OverHeadInPages) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    //
    // Create a DPC for every processor in the system as servicing the
    // compression interrupt is critical.
    //

    NumberProcessors = KeNumberProcessors;

    CompressionDpcArray = ExAllocatePoolWithTag (NonPagedPool,
                                             NumberProcessors * sizeof (KDPC),
                                             'pDmM');

    if (CompressionDpcArray == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    for (Processor = 0; Processor < NumberProcessors; Processor += 1) {

        KeInitializeDpc (CompressionDpcArray + Processor, MiCompressionDispatch, NULL);

        //
        // Set importance so this DPC always gets queued at the head.
        //

        KeSetImportanceDpc (CompressionDpcArray + Processor, HighImportance);

        KeSetTargetProcessorDpc (CompressionDpcArray + Processor, Processor);
    }

    LOCK_PFN (OldIrql);

    if (MmCompressionThresholdRatio == 0) {
        MmCompressionThresholdRatio = MI_DEFAULT_COMPRESSION_THRESHOLD;
    }
    else if (MmCompressionThresholdRatio > 100) {
        MmCompressionThresholdRatio = 100;
    }

    if ((MmResidentAvailablePages < (SPFN_NUMBER) OverHeadInPages) ||
        (MmAvailablePages < OverHeadInPages)) {

        UNLOCK_PFN (OldIrql);
        ExFreePool (CompressionDpcArray);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MmResidentAvailablePages -= OverHeadInPages;
    MmAvailablePages -= (PFN_COUNT) OverHeadInPages;

    //
    // Snap our own copy to prevent busted drivers from causing overcommits
    // if they deregister improperly.
    //

    MiCompressionOverHeadInPages += OverHeadInPages;

    ASSERT (MiNumberOfCompressionPages == 0);

    ASSERT (MiSetCompressionThreshold == NULL);
    MiSetCompressionThreshold = Context->SetCompressionThreshold;

    if (MiCompressionDpcArray == NULL) {
        MiCompressionDpcArray = CompressionDpcArray;
        CompressionDpcArray = NULL;
        MiCompressionProcessors = NumberProcessors;
    }

    UNLOCK_PFN (OldIrql);

    if (CompressionDpcArray != NULL) {
        ExFreePool (CompressionDpcArray);
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MiArmCompressionInterrupt (
    VOID
    )

/*++

Routine Description:

    This routine arms the hardware-generated compression interrupt.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    NTSTATUS Status;
    PFN_NUMBER RealPages;
    ULONGLONG ByteThreshold;

    MM_PFN_LOCK_ASSERT();

    if (MiSetCompressionThreshold == NULL) {
        return STATUS_SUCCESS;
    }

    RealPages = MmNumberOfPhysicalPages - MiNumberOfCompressionPages - MiCompressionOverHeadInPages;

    ByteThreshold = (RealPages * MmCompressionThresholdRatio) / 100;
    ByteThreshold *= PAGE_SIZE;

    //
    // Note this callout is made with the PFN lock held !
    //

    Status = (*MiSetCompressionThreshold) (ByteThreshold);

    if (!NT_SUCCESS (Status)) {

        //
        // If the hardware fails, all is lost.
        //

        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x61941, 
                      MmNumberOfPhysicalPages,
                      RealPages,
                      MmCompressionThresholdRatio);
    }

    return Status;
}


NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )

/*++

Routine Description:

    This routine notifies memory management that compression hardware is
    being removed.  Note the compression driver must have already SUCCESSFULLY
    called MmRemovePhysicalMemoryEx.

Arguments:

    Context - Supplies the compression context pointer.

Return Value:

    STATUS_SUCCESS if compression support is initialized properly.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_COUNT OverHeadInPages;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    OverHeadInPages = (PFN_COUNT)(Context->ReservedBytes / PAGE_SIZE);

    LOCK_PFN (OldIrql);

    if (OverHeadInPages > MiCompressionOverHeadInPages) {
        UNLOCK_PFN (OldIrql);
        return STATUS_INVALID_PARAMETER;
    }

    MmResidentAvailablePages += OverHeadInPages;
    MmAvailablePages += OverHeadInPages;

    ASSERT (MiCompressionOverHeadInPages == OverHeadInPages);

    MiCompressionOverHeadInPages -= OverHeadInPages;

    MiSetCompressionThreshold = NULL;

    UNLOCK_PFN (OldIrql);

    return STATUS_SUCCESS;
}

VOID
MiCompressionDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )
/*++

Routine Description:

    Called to make memory compressible if the PFN lock could not be
    acquired during the original device interrupt.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    SystemArgument1 - Supplies the number of bytes to make compressible.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL.

--*/
{
    SIZE_T NumberOfBytes;

    UNREFERENCED_PARAMETER (Dpc);
    UNREFERENCED_PARAMETER (DeferredContext);
    UNREFERENCED_PARAMETER (SystemArgument2);

    NumberOfBytes = (SIZE_T) SystemArgument1;

    MiCompressionInfo.DpcsFired += 1;

    MiMakeCompressibleMemoryAtDispatch (NumberOfBytes);
}

SIZE_T
MmMakeCompressibleMemory (
    IN SIZE_T NumberOfBytes OPTIONAL
    )

/*++

Routine Description:

    This routine attempts to move pages from transition to zero so that
    hardware compression can reclaim the physical memory.

Arguments:

    NumberOfBytes - Supplies the number of bytes to make compressible.
                    Zero indicates as much as possible.

Return Value:

    Returns the number of bytes made compressible.

Environment:

    Kernel mode.  Any IRQL as this is called from device interrupt service
    routines.

--*/

{
    KIRQL OldIrql;
    BOOLEAN Queued;
#if !defined(NT_UP)
    PFN_NUMBER PageFrameIndex;
    MMLISTS MemoryList;
    PMMPFNLIST ListHead;
    PMMPFN Pfn1;
    CCHAR Processor;
    PFN_NUMBER Total;
    PVOID ZeroBase;
    PKPRCB Prcb;
    PFN_NUMBER RequestedPages;
    PFN_NUMBER ActualPages;
    PKSPIN_LOCK_QUEUE LockQueuePfn;
    PKSPIN_LOCK_QUEUE LockQueueContextSwap;
#endif

    //
    // LWFIX: interlocked add in the request size above so overlapping
    // requests can be processed.
    //

    OldIrql = KeGetCurrentIrql();

    if (OldIrql <= DISPATCH_LEVEL) {
        return MiMakeCompressibleMemoryAtDispatch (NumberOfBytes);
    }

#if defined(NT_UP)

    //
    // In uniprocessor configurations, there is no indication as to whether
    // various locks of interest (context swap & PFN) are owned because the
    // uniprocessor kernel macros these into merely IRQL raises.  Therefore
    // this routine must be conservative when called above DISPATCH_LEVEL and
    // assume the lock is owned and just always queue a DPC in these cases.
    //

    Queued = KeInsertQueueDpc (MiCompressionDpcArray,
                               (PVOID) NumberOfBytes,
                               NULL);
    if (Queued == TRUE) {
        MiCompressionInfo.CtxswapForcedDpcInsert += 1;
    }
    else {
        MiCompressionInfo.CtxswapFailedDpcInsert += 1;
    }

    return 0;

#else

#if DBG
    //
    // Make sure this interrupt always comes in at the same device IRQL.
    //

    ASSERT ((MiCompressionIrql == 0) || (OldIrql == MiCompressionIrql));
    MiCompressionIrql = OldIrql;
#endif

    Prcb = KeGetCurrentPrcb();

    LockQueueContextSwap = &Prcb->LockQueue[LockQueueContextSwapLock];

    //
    // The context swap lock is needed for TB flushes.  The interrupted thread
    // may already own this in the midst of doing a TB flush without the PFN
    // lock being held.  Since there is no safe way to tell if the current
    // processor owns the context swap lock, just do a try-acquire followed
    // by an immediate release to decide if it is safe to proceed.
    //

    if (KeTryToAcquireQueuedSpinLockAtRaisedIrql (LockQueueContextSwap) == FALSE) {

        //
        // Unable to acquire the spinlock, queue a DPC to pick it up instead.
        //

        for (Processor = 0; Processor < MiCompressionProcessors; Processor += 1) {

            Queued = KeInsertQueueDpc (MiCompressionDpcArray + Processor,
                                       (PVOID) NumberOfBytes,
                                       NULL);
            if (Queued == TRUE) {
                MiCompressionInfo.CtxswapForcedDpcInsert += 1;
            }
            else {
                MiCompressionInfo.CtxswapFailedDpcInsert += 1;
            }
        }
        return 0;
    }

    KeReleaseQueuedSpinLockFromDpcLevel (LockQueueContextSwap);

    RequestedPages = NumberOfBytes >> PAGE_SHIFT;
    ActualPages = 0;

    MemoryList = FreePageList;

    ListHead = MmPageLocationList[MemoryList];

    LockQueuePfn = &Prcb->LockQueue[LockQueuePfnLock];

    if (KeTryToAcquireQueuedSpinLockAtRaisedIrql (LockQueuePfn) == FALSE) {

        //
        // Unable to acquire the spinlock, queue a DPC to pick it up instead.
        //

        for (Processor = 0; Processor < MiCompressionProcessors; Processor += 1) {

            Queued = KeInsertQueueDpc (MiCompressionDpcArray + Processor,
                                       (PVOID) NumberOfBytes,
                                       NULL);
            if (Queued == TRUE) {
                MiCompressionInfo.PfnForcedDpcInsert += 1;
            }
            else {
                MiCompressionInfo.PfnFailedDpcInsert += 1;
            }
        }
        return 0;
    }

    MiCompressionInfo.IsrRan += 1;

    //
    // Run the free and transition list and zero the pages.
    //

    while (MemoryList <= StandbyPageList) {

        Total = ListHead->Total;

        PageFrameIndex = ListHead->Flink;

        while (Total != 0) {

            //
            // Transition pages may need restoration which requires a
            // hyperspace mapping plus control area deletion actions all of
            // which occur at DISPATCH_LEVEL.  So if we're at device IRQL,
            // only do the minimum and queue the rest.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if ((Pfn1->u3.e1.InPageError == 1) &&
                (Pfn1->u3.e1.ReadInProgress == 1)) {

                //
                // This page is already zeroed so skip it.
                //

                MiCompressionInfo.IsrSkippedZeroedPage += 1;
            }
            else {

                //
                // Zero the page directly now instead of waiting for the low
                // priority zeropage thread to get a slice.  Note that the
                // slower mapping and zeroing routines are used here because
                // the faster ones are for the zeropage thread only.
                // Maybe we should change this someday.
                //

                ZeroBase = MiMapCompressionInHyperSpace (PageFrameIndex);

                RtlZeroMemory (ZeroBase, PAGE_SIZE);

                MiUnmapCompressionInHyperSpace ();

                ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

                //
                // Overload ReadInProgress to signify that collided faults that
                // occur before the PTE is completely restored will know to
                // delay and retry until the page (and PTE) are updated.
                //

                Pfn1->u3.e1.InPageError = 1;
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
                Pfn1->u3.e1.ReadInProgress = 1;

                ActualPages += 1;

                if (ActualPages == RequestedPages) {
                    MemoryList = StandbyPageList;
                    ListHead = MmPageLocationList[MemoryList];
                    break;
                }
            }

            Total -= 1;
            PageFrameIndex = Pfn1->u1.Flink;
        }
        MemoryList += 1;
        ListHead += 1;
    }

    if (ActualPages != 0) {

        //
        // Rearm the interrupt as pages have now been zeroed.
        //

        MiArmCompressionInterrupt ();
    }

    KeReleaseQueuedSpinLockFromDpcLevel (LockQueuePfn);

    if (ActualPages != 0) {

        //
        // Pages were zeroed - queue a DPC to the current processor to
        // move them to the zero list.  Note this is not critical path so
        // don't bother sending a DPC to every processor for this case.
        //

        MiCompressionInfo.IsrPageProcessed += (ULONG)ActualPages;

        Processor = (CCHAR) KeGetCurrentProcessorNumber ();

        //
        // Ensure a hot-added processor scenario just works.
        //

        if (Processor >= MiCompressionProcessors) {
            Processor = MiCompressionProcessors;
        }

        Queued = KeInsertQueueDpc (MiCompressionDpcArray + Processor,
                                   (PVOID) NumberOfBytes,
                                   NULL);
        if (Queued == TRUE) {
            MiCompressionInfo.IsrForcedDpc += 1;
        }
        else {
            MiCompressionInfo.IsrFailedDpc += 1;
        }
    }

    return (ActualPages << PAGE_SHIFT);
#endif
}

SIZE_T
MiMakeCompressibleMemoryAtDispatch (
    IN SIZE_T NumberOfBytes OPTIONAL
    )

/*++

Routine Description:

    This routine attempts to move pages from transition to zero so that
    hardware compression can reclaim the physical memory.

Arguments:

    NumberOfBytes - Supplies the number of bytes to make compressible.
                    Zero indicates as much as possible.

Return Value:

    Returns the number of bytes made compressible.

Environment:

    Kernel mode.  DISPATCH_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageFrameIndex2;
    PVOID ZeroBase;
    PMMPFN Pfn1;
    MMLISTS MemoryList;
    PMMPFNLIST ListHead;
    PFN_NUMBER RequestedPages;
    PFN_NUMBER ActualPages;
    LOGICAL NeedToZero;

    ASSERT (KeGetCurrentIrql () == DISPATCH_LEVEL);

    RequestedPages = NumberOfBytes >> PAGE_SHIFT;
    ActualPages = 0;

    MemoryList = FreePageList;
    ListHead = MmPageLocationList[MemoryList];

    MiCompressionInfo.DpcRan += 1;

    LOCK_PFN2 (OldIrql);

    //
    // Run the free and transition list and zero the pages.
    //

    while (MemoryList <= StandbyPageList) {

        while (ListHead->Total != 0) {

            //
            // Before removing the page from the head of the list (which will
            // zero the flag bits), snap whether it's been zeroed by our ISR
            // or whether we need to zero it here.
            //

            PageFrameIndex = ListHead->Flink;
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            NeedToZero = TRUE;
            if ((Pfn1->u3.e1.InPageError == 1) && (Pfn1->u3.e1.ReadInProgress == 1)) {
                MiCompressionInfo.DpcSkippedZeroedPage += 1;
                NeedToZero = FALSE;
            }

            //
            // Transition pages may need restoration which requires a
            // hyperspace mapping plus control area deletion actions all of
            // which occur at DISPATCH_LEVEL.  Since we're at DISPATCH_LEVEL
            // now, go ahead and do it.
            //

            PageFrameIndex2 = MiRemovePageFromList (ListHead);
            ASSERT (PageFrameIndex == PageFrameIndex2);

            //
            // Zero the page directly now instead of waiting for the low
            // priority zeropage thread to get a slice.  Note that the
            // slower mapping and zeroing routines are used here because
            // the faster ones are for the zeropage thread only.
            // Maybe we should change this someday.
            //

            if (NeedToZero == TRUE) {
                ZeroBase = MiMapCompressionInHyperSpace (PageFrameIndex);

                RtlZeroMemory (ZeroBase, PAGE_SIZE);

                MiUnmapCompressionInHyperSpace ();
            }

            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

            MiInsertPageInList (&MmZeroedPageListHead, PageFrameIndex);

            //
            // We have changed (zeroed) the contents of this page.
            // If memory mirroring is in progress, the bitmap must be updated.
            //

            if (MiMirroringActive == TRUE) {
                RtlSetBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
            }

            MiCompressionInfo.DpcPageProcessed += 1;
            ActualPages += 1;

            if (ActualPages == RequestedPages) {
                MemoryList = StandbyPageList;
                ListHead = MmPageLocationList[MemoryList];
                break;
            }
        }
        MemoryList += 1;
        ListHead += 1;
    }

    //
    // Rearm the interrupt as pages have now been zeroed.
    //

    MiArmCompressionInterrupt ();

    UNLOCK_PFN2 (OldIrql);

    return (ActualPages << PAGE_SHIFT);
}

PVOID
MiMapCompressionInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into the
    PTE within hyper space reserved explicitly for compression page
    mapping.

    The PTE is guaranteed to always be available since the PFN lock is held.

Arguments:

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the virtual address where the specified physical page was
    mapped.

Environment:

    Kernel mode, PFN lock held, any IRQL.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PVOID FlushVaPointer;

    ASSERT (PageFrameIndex != 0);

    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    FlushVaPointer = (PVOID) (ULONG_PTR) COMPRESSION_MAPPING_PTE;

    //
    // Ensure both modified and accessed bits are set so the hardware doesn't
    // ever write this PTE.
    //

    ASSERT (TempPte.u.Hard.Dirty == 1);
    ASSERT (TempPte.u.Hard.Accessed == 1);

    PointerPte = MiGetPteAddress (COMPRESSION_MAPPING_PTE);
    ASSERT (PointerPte->u.Long == 0);

    //
    // Only flush the TB on the current processor as no context switch can
    // occur while using this mapping.
    //

    KeFlushSingleTb (FlushVaPointer,
                     TRUE,
                     FALSE,
                     (PHARDWARE_PTE) PointerPte,
                     TempPte.u.Flush);

    return (PVOID) MiGetVirtualAddressMappedByPte (PointerPte);
}

__forceinline
VOID
MiUnmapCompressionInHyperSpace (
    VOID
    )

/*++

Routine Description:

    This procedure unmaps the PTE reserved for mapping the compression page.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held, any IRQL.

--*/

{
    PMMPTE PointerPte;

    PointerPte = MiGetPteAddress (COMPRESSION_MAPPING_PTE);

    //
    // Capture the number of waiters.
    //

    ASSERT (PointerPte->u.Long != 0);

    PointerPte->u.Long = 0;

    return;
}
#else
NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )
{
    UNREFERENCED_PARAMETER (Context);

    return STATUS_NOT_SUPPORTED;
}

NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )
{
    UNREFERENCED_PARAMETER (Context);

    return STATUS_NOT_SUPPORTED;
}
SIZE_T
MmMakeCompressibleMemory (
    IN SIZE_T NumberOfBytes OPTIONAL
    )
{
    UNREFERENCED_PARAMETER (NumberOfBytes);

    return 0;
}
NTSTATUS
MiArmCompressionInterrupt (
    VOID
    )
{
    return STATUS_NOT_SUPPORTED;
}
#endif

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\crashdmp.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   crashdmp.c

Abstract:

    This module contains routines which provide support for writing out
    a crashdump on system failure.

Author:

    Landy Wang (landyw) 04-Oct-2000

Revision History:

--*/

#include "mi.h"

LOGICAL
MiIsAddressRangeValid (
    IN PVOID VirtualAddress,
    IN SIZE_T Length
    )
{
    PUCHAR Va;
    PUCHAR EndVa;
    ULONG Pages;
    
    Va = PAGE_ALIGN (VirtualAddress);
    Pages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (VirtualAddress, Length);
    EndVa = Va + (Pages << PAGE_SHIFT);
    
    while (Va < EndVa) {

        if (!MmIsAddressValid (Va)) {
            return FALSE;
        }

        Va += PAGE_SIZE;
    }

    return TRUE;
}

VOID
MiRemoveFreePoolMemoryFromDump (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )

/*++

Routine Description:

    Removes all memory from the nonpaged pool free page lists to reduce the size
    of a kernel memory dump.

    Because the entries in these structures are destroyed by errant drivers
    that modify pool after freeing it, the entries are carefully
    validated prior to any dereferences.

Arguments:

    Context - Supplies the dump context pointer that must be passed to
              IoFreeDumpRange.

Return Value:

    None.

Environment:

    Kernel-mode, post-bugcheck.

    For use by crashdump routines ONLY.

--*/
{
    PLIST_ENTRY Entry;
    PLIST_ENTRY List;
    PLIST_ENTRY ListEnd;
    PMMFREE_POOL_ENTRY PoolEntry;
    ULONG LargePageMapped;

    List = &MmNonPagedPoolFreeListHead[0];
    ListEnd = List + MI_MAX_FREE_LIST_HEADS;

    for ( ; List < ListEnd; List += 1) {

        for (Entry = List->Flink; Entry != List; Entry = Entry->Flink) {

            PoolEntry = CONTAINING_RECORD (Entry,
                                           MMFREE_POOL_ENTRY,
                                           List);

            //
            // Check for corrupted values.
            //
            
            if (BYTE_OFFSET(PoolEntry) != 0) {
                break;
            }

            //
            // Check that the entry has not been corrupted.
            //
            
            if (MiIsAddressRangeValid (PoolEntry, sizeof (MMFREE_POOL_ENTRY)) == FALSE) {
                break;
            }

            if (PoolEntry->Size == 0) {
                break;
            }

            //
            // Signature is only maintained in checked builds.
            //
            
            ASSERT (PoolEntry->Signature == MM_FREE_POOL_SIGNATURE);

            //
            // Verify that the element's flinks and blinks are valid.
            //

            if ((!MiIsAddressRangeValid (Entry->Flink, sizeof (LIST_ENTRY))) ||
                (!MiIsAddressRangeValid (Entry->Blink, sizeof (LIST_ENTRY))) ||
                (Entry->Blink->Flink != Entry) ||
                (Entry->Flink->Blink != Entry)) {

                break;
            }

            //
            // The list entry is valid, remove it from the dump.
            //
        
            if (MI_IS_PHYSICAL_ADDRESS (PoolEntry)) {
                LargePageMapped = 1;
            }
            else {
                LargePageMapped = 0;
            }

            Context->FreeDumpRange (Context,
                                    PoolEntry,
                                    PoolEntry->Size,
                                    LargePageMapped);
        }
    }

}


LOGICAL
MiIsPhysicalMemoryAddress (
    IN PFN_NUMBER PageFrameIndex,
    IN OUT PULONG Hint,
    IN LOGICAL PfnLockNeeded
    )

/*++

Routine Description:

    Check if a given address is backed by RAM or IO space.

Arguments:

    PageFrameIndex - Supplies a page frame number to check.

    Hint - Supplies a hint at which memory run we should start
           searching for this pfn.  The hint is updated on success
           and failure.

    PfnLockNeeded - Supplies TRUE if the caller needs this routine to
                    acquire the PFN lock.  FALSE if not (ie: the caller
                    already holds the PFN lock or we are crashing the system
                    and so the PFN lock may already be held by someone else).

Return Value:

    TRUE - If the address is backed by RAM.

    FALSE - If the address is IO mapped memory.

Environment:

    Kernel-mode, post-bugcheck.
    
    For use by crash dump and other Mm internal routines.

--*/
{
    ULONG Index;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RUN Run;
    PPHYSICAL_MEMORY_DESCRIPTOR PhysicalMemoryBlock;
    
    //
    // Initializing OldIrql is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

    if (PfnLockNeeded) {
        LOCK_PFN2 (OldIrql);
    }

    PhysicalMemoryBlock = MmPhysicalMemoryBlock;

    if (PageFrameIndex > MmHighestPhysicalPage) {
        if (PfnLockNeeded) {
            UNLOCK_PFN2 (OldIrql);
        }
        return FALSE;
    }

    if (*Hint < PhysicalMemoryBlock->NumberOfRuns) {

        Run = &PhysicalMemoryBlock->Run[*Hint];

        if ((PageFrameIndex >= Run->BasePage) &&
            (PageFrameIndex < Run->BasePage + Run->PageCount)) {

            if (PfnLockNeeded) {
                UNLOCK_PFN2 (OldIrql);
            }
            return TRUE;
        }
    }
    
    for (Index = 0; Index < PhysicalMemoryBlock->NumberOfRuns; Index += 1) {

        Run = &PhysicalMemoryBlock->Run[Index];

        if ((PageFrameIndex >= Run->BasePage) &&
            (PageFrameIndex < Run->BasePage + Run->PageCount)) {

            *Hint = Index;
            if (PfnLockNeeded) {
                UNLOCK_PFN2 (OldIrql);
            }
            return TRUE;
        }

        //
        // Since the physical memory block is ordered by increasing
        // base page PFN number, if this PFN is smaller, then bail.
        //

        if (Run->BasePage + Run->PageCount > PageFrameIndex) {
            *Hint = Index;
            break;
        }
    }

    if (PfnLockNeeded) {
        UNLOCK_PFN2 (OldIrql);
    }

    return FALSE;
}


VOID
MiAddPagesWithNoMappings (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )
/*++

Routine Description:

    Add pages to a kernel memory crashdump that do not have a
    virtual mapping in this process context.

    This includes entries that are wired directly into the TB.

Arguments:

    Context - Crashdump context pointer.

Return Value:

    None.

Environment:

    Kernel-mode, post-bugcheck.

    For use by crash dump routines ONLY.
    
--*/

{
#if defined (_X86_)

    ULONG LargePageMapped;
    PVOID Va;
    PHYSICAL_ADDRESS DirBase;

    //
    // Add the current page directory table page - don't use the directory
    // table base for the crashing process as we have switched cr3 on
    // stack overflow crashes, etc.
    //

    _asm {
        mov     eax, cr3
        mov     DirBase.LowPart, eax
    }

    //
    // cr3 is always located below 4gb physical.
    //

    DirBase.HighPart = 0;

    Va = MmGetVirtualForPhysical (DirBase);

    if (MI_IS_PHYSICAL_ADDRESS (Va)) {
        LargePageMapped = 1;
    }
    else {
        LargePageMapped = 0;
    }

    Context->SetDumpRange (Context,
                           Va,
                           1,
                           LargePageMapped);

#elif defined(_AMD64_)

    ULONG LargePageMapped;
    PVOID Va;
    PHYSICAL_ADDRESS DirBase;

    //
    // Add the current page directory table page - don't use the directory
    // table base for the crashing process as we have switched cr3 on
    // stack overflow crashes, etc.
    //

    DirBase.QuadPart = ReadCR3 ();

    Va = MmGetVirtualForPhysical (DirBase);

    if (MI_IS_PHYSICAL_ADDRESS (Va)) {
        LargePageMapped = 1;
    }
    else {
        LargePageMapped = 0;
    }

    Context->SetDumpRange (Context,
                           Va,
                           1,
                           LargePageMapped);

#elif defined(_IA64_)

    if (MiKseg0Mapping == TRUE) {
        Context->SetDumpRange (
                        Context,
                        MiKseg0Start,
                        (((ULONG_PTR)MiKseg0End - (ULONG_PTR)MiKseg0Start) >> PAGE_SHIFT) + 1,
                        1);
    }

#endif
}


LOGICAL
MiAddRangeToCrashDump (
    IN PMM_KERNEL_DUMP_CONTEXT Context,
    IN PVOID Va,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Adds the specified range of memory to the crashdump.

Arguments:

    Context - Supplies the crashdump context pointer.

    Va - Supplies the starting virtual address.

    NumberOfBytes - Supplies the number of bytes to dump.  Note that for IA64,
                    this must not cause the range to cross a region boundary.

Return Value:

    TRUE if all valid pages were added to the crashdump, FALSE otherwise.

Environment:

    Kernel mode, post-bugcheck.

    For use by crash dump routines ONLY.

--*/

{
    LOGICAL Status;
    LOGICAL AddThisPage;
    ULONG Hint;
    PVOID EndingAddress;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PFN_NUMBER PageFrameIndex;
#if defined (_X86_) || defined (_AMD64_)
    PFN_NUMBER NumberOfPages;
#endif
    
    Hint = 0;
    Status = TRUE;

    EndingAddress = (PVOID)((ULONG_PTR)Va + NumberOfBytes - 1);

#if defined(_IA64_)

    //
    // IA64 has a separate page directory parent for each region and
    // unimplemented address bits are ignored by the processor (as
    // long as they are canonical), but we must watch for them
    // here so the incrementing PPE walk doesn't go off the end.
    // This is done by truncating any given region request so it does
    // not go past the end of the specified region.  Note this
    // automatically will include the page maps which are sign extended
    // because the PPEs would just wrap anyway.
    //

    if (((ULONG_PTR)EndingAddress & ~VRN_MASK) >= MM_VA_MAPPED_BY_PPE * PDE_PER_PAGE) {
        EndingAddress = (PVOID)(((ULONG_PTR)EndingAddress & VRN_MASK) |
                         ((MM_VA_MAPPED_BY_PPE * PDE_PER_PAGE) - 1));
    }

#endif

    Va = PAGE_ALIGN (Va);

    PointerPxe = MiGetPxeAddress (Va);
    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);

    do {

#if (_MI_PAGING_LEVELS >= 3)
restart:
#endif

        KdCheckForDebugBreak ();

#if (_MI_PAGING_LEVELS >= 4)
        while (PointerPxe->u.Hard.Valid == 0) {

            //
            // This extended page directory parent entry is empty,
            // go to the next one.
            //

            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }
        }
#endif

        ASSERT (MiGetPpeAddress(Va) == PointerPpe);

#if (_MI_PAGING_LEVELS >= 3)
        while (PointerPpe->u.Hard.Valid == 0) {

            //
            // This page directory parent entry is empty, go to the next one.
            //

            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe += 1;
                ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
                goto restart;
            }
#endif

        }
#endif

        while (PointerPde->u.Hard.Valid == 0) {

            //
            // This page directory entry is empty, go to the next one.
            //

            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }

#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe += 1;
                ASSERT (PointerPpe == MiGetPteAddress (PointerPde));
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto restart;
            }
#endif
        }

        //
        // A valid PDE has been located, examine each PTE.
        //

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);
        ASSERT (Va <= EndingAddress);

#if defined (_X86_) || defined (_AMD64_)

        if (PointerPde->u.Hard.LargePage == 1) {

            //
            // Large pages are always backed by RAM, not mapped to
            // I/O space, so always add them to the dump.
            //
                
            NumberOfPages = (((ULONG_PTR)MiGetVirtualAddressMappedByPde (PointerPde + 1) - (ULONG_PTR)Va) / PAGE_SIZE);

            Status = Context->SetDumpRange (Context,
                                            Va,
                                            NumberOfPages,
                                            1);

            if (!NT_SUCCESS (Status)) {
#if DBG
                DbgPrint ("Adding large VA %p to crashdump failed\n", Va);
                DbgBreakPoint ();
#endif
                Status = FALSE;
            }

            PointerPde += 1;
            Va = MiGetVirtualAddressMappedByPde (PointerPde);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            PointerPte = MiGetPteAddress (Va);
            PointerPpe = MiGetPpeAddress (Va);
            PointerPxe = MiGetPxeAddress (Va);

            //
            // March on to the next page directory.
            //

            continue;
        }

#endif

        //
        // Exclude memory that is mapped in the system cache.
        // Note the system cache starts and ends on page directory boundaries
        // and is never mapped with large pages.
        //
        
        if (MI_IS_SYSTEM_CACHE_ADDRESS (Va)) {
            PointerPde += 1;
            Va = MiGetVirtualAddressMappedByPde (PointerPde);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            PointerPte = MiGetPteAddress (Va);
            PointerPpe = MiGetPpeAddress (Va);
            PointerPxe = MiGetPxeAddress (Va);

            //
            // March on to the next page directory.
            //

            continue;
        }

        do {

            AddThisPage = FALSE;
            PageFrameIndex = 0;

            if (PointerPte->u.Hard.Valid == 1) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                AddThisPage = TRUE;
            }
            else if ((PointerPte->u.Soft.Prototype == 0) &&
                     (PointerPte->u.Soft.Transition == 1)) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte);
                AddThisPage = TRUE;
            }

            if (AddThisPage == TRUE) {

                //
                // Include only addresses that are backed by RAM, not mapped to
                // I/O space.
                //
                
                if (MiIsPhysicalMemoryAddress (PageFrameIndex, &Hint, FALSE)) {

                    //
                    // Add this page to the dump.
                    //
        
                    Status = Context->SetDumpRange (Context,
                                                    (PVOID) PageFrameIndex,
                                                    1,
                                                    2);

                    if (!NT_SUCCESS (Status)) {
#if DBG
                        DbgPrint ("Adding VA %p to crashdump failed\n", Va);
                        DbgBreakPoint ();
#endif
                        Status = FALSE;
                    }
                }
            }

            Va = (PVOID)((ULONG_PTR)Va + PAGE_SIZE);
            PointerPte += 1;

            ASSERT64 (PointerPpe->u.Hard.Valid == 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            //
            // If not at the end of a page table and still within the specified
            // range, just march directly on to the next PTE.
            //
            // Otherwise, if the virtual address is on a page directory boundary
            // then attempt to leap forward skipping over empty mappings
            // where possible.
            //

        } while (!MiIsVirtualAddressOnPdeBoundary(Va));

        ASSERT (PointerPte == MiGetPteAddress (Va));
        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);

    } while (TRUE);

    // NEVER REACHED
}


VOID
MiAddActivePageDirectories (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )
{
    UCHAR i;
    PKPRCB Prcb;
    PKPROCESS Process;
    PFN_NUMBER PageFrameIndex;

#if defined (_X86PAE_)
    PMMPTE PointerPte;
    ULONG j;
#endif

    for (i = 0; i < KeNumberProcessors; i += 1) {

        Prcb = KiProcessorBlock[i];

        Process = Prcb->CurrentThread->ApcState.Process;

#if defined (_X86PAE_)

        //
        // Note that on PAE systems, the idle and system process have
        // NULL initialized PaeTop fields.  Thus this field must be
        // explicitly checked for before being referenced here.
        //

        //
        // Add the 4 top level page directory pages to the dump.
        //

        PointerPte = (PMMPTE) ((PEPROCESS)Process)->PaeTop;

        if (PointerPte == NULL) {
            PointerPte = &MiSystemPaeVa.PteEntry[0];
        }

        for (j = 0; j < PD_PER_SYSTEM; j += 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(PointerPte);
            PointerPte += 1;
            Context->SetDumpRange (Context, (PVOID) PageFrameIndex, 1, 2);
        }

        //
        // Add the real cr3 page to the dump, note that the value stored in the
        // directory table base is really a physical address (not a frame).
        //

        PageFrameIndex = Process->DirectoryTableBase[0];
        PageFrameIndex = (PageFrameIndex >> PAGE_SHIFT);

#else

        PageFrameIndex =
            MI_GET_DIRECTORY_FRAME_FROM_PROCESS ((PEPROCESS)(Process));

#endif

        //
        // Add this physical page to the dump.
        //

        Context->SetDumpRange (Context, (PVOID) PageFrameIndex, 1, 2);
    }

#if defined(_IA64_)

    //
    // The first processor's PCR is mapped in region 4 which is not (and cannot)
    // be scanned later, so explicitly add it to the dump here.
    //

    Prcb = KiProcessorBlock[0];

    Context->SetDumpRange (Context, (PVOID) Prcb->PcrPage, 1, 2);
#endif
}


VOID
MmGetKernelDumpRange (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )

/*++

Routine Description:

    Add (and subtract) ranges of system memory to the crashdump.

Arguments:

    Context - Crashdump context pointer.

Return Value:

    None.

Environment:

    Kernel mode, post-bugcheck.

    For use by crash dump routines ONLY.

--*/

{
    PVOID Va;
    SIZE_T NumberOfBytes;
    
    ASSERT ((Context != NULL) &&
            (Context->SetDumpRange != NULL) &&
            (Context->FreeDumpRange != NULL));
            
    MiAddActivePageDirectories (Context);

#if defined(_IA64_)

    //
    // Note each IA64 region must be passed separately to MiAddRange...
    //

    Va = (PVOID) ALT4KB_PERMISSION_TABLE_START;
    NumberOfBytes = PDE_UTBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_SESSION_SPACE_DEFAULT;
    NumberOfBytes = PDE_STBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) KADDRESS_BASE;
    NumberOfBytes = PDE_KTBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#elif defined(_AMD64_)

    Va = (PVOID) MM_SYSTEM_RANGE_START;
    NumberOfBytes = MM_KSEG0_BASE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_KSEG2_BASE;
    NumberOfBytes = MM_SYSTEM_SPACE_START - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_PAGED_POOL_START;
    NumberOfBytes = MM_SYSTEM_SPACE_END - (ULONG_PTR) Va + 1;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#else

    Va = MmSystemRangeStart;
    NumberOfBytes = MM_SYSTEM_SPACE_END - (ULONG_PTR) Va + 1;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#endif

    //
    // Add any memory that is a part of the kernel space, but does not
    // have a virtual mapping (hence was not collected above).
    //
    
    MiAddPagesWithNoMappings (Context);

    //
    // Remove nonpaged pool that is not in use.
    //

    MiRemoveFreePoolMemoryFromDump (Context);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\debugsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   debugsup.c

Abstract:

    This module contains routines which provide support for the
    kernel debugger.

Author:

    Lou Perazzoli (loup) 02-Aug-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#include <kdp.h>

ULONG MmPoisonedTb;


PVOID
MiDbgWriteCheck (
    IN PVOID VirtualAddress,
    IN PHARDWARE_PTE Opaque,
    IN LOGICAL ForceWritableIfPossible
    )

/*++

Routine Description:

    This routine checks the specified virtual address and if it is
    valid and writable, it returns that virtual address, otherwise
    it returns NULL.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

    Opaque - Supplies an opaque pointer.

Return Value:

    Returns NULL if the address is not valid or writable, otherwise
    returns the virtual address.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    MMPTE PteContents;
    PMMPTE InputPte;
    PMMPTE PointerPte;
    ULONG_PTR IsPhysical;

    InputPte = (PMMPTE)Opaque;

    InputPte->u.Long = 0;

    if (!MmIsAddressValid (VirtualAddress)) {
        return NULL;
    }

#if defined(_IA64_)

    //
    // There are regions mapped by TRs (PALcode, PCR, etc) that are
    // not part of the MI_IS_PHYSICAL_ADDRESS macro.
    //

    IsPhysical = MiIsVirtualAddressMappedByTr (VirtualAddress);
    if (IsPhysical == FALSE) {
        IsPhysical = MI_IS_PHYSICAL_ADDRESS (VirtualAddress);
    }
#else
    IsPhysical = MI_IS_PHYSICAL_ADDRESS (VirtualAddress);
#endif

    if (IsPhysical) {

        //
        // All superpage mappings must be read-write and never generate
        // faults so nothing needs to be done for this case.
        //

        return VirtualAddress;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);

    PteContents = *PointerPte;
    
#if defined(NT_UP) || defined(_IA64_)
    if (PteContents.u.Hard.Write == 0)
#else
    if (PteContents.u.Hard.Writable == 0)
#endif
    {
        if (ForceWritableIfPossible == FALSE) {
            return NULL;
        }

        //
        // PTE is not writable, make it so.
        //

        *InputPte = PteContents;
    
        //
        // Carefully modify the PTE to ensure write permissions,
        // preserving the page's cache attributes to keep the TB
        // coherent.
        //
    
#if defined(NT_UP) || defined(_IA64_)
        PteContents.u.Hard.Write = 1;
#else
        PteContents.u.Hard.Writable = 1;
#endif
        MI_SET_PTE_DIRTY (PteContents);
        MI_SET_ACCESSED_IN_PTE (&PteContents, 1);
    
        *PointerPte = PteContents;
    
        //
        // Note KeFillEntryTb does not IPI the other processors. This is
        // required as the other processors are frozen in the debugger
        // and we will deadlock if we try and IPI them.
        // Just flush the current processor instead.
        //

        KeFillEntryTb ((PHARDWARE_PTE)PointerPte, VirtualAddress, TRUE);
    }

    return VirtualAddress;
}

VOID
MiDbgReleaseAddress (
    IN PVOID VirtualAddress,
    IN PHARDWARE_PTE Opaque
    )

/*++

Routine Description:

    This routine resets the specified virtual address access permissions
    to its original state.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

    Opaque - Supplies an opaque pointer.

Return Value:

    None.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE InputPte;

    InputPte = (PMMPTE)Opaque;

    ASSERT (MmIsAddressValid (VirtualAddress));

    if (InputPte->u.Long != 0) {

        PointerPte = MiGetPteAddress (VirtualAddress);

        TempPte = *InputPte;
        TempPte.u.Hard.Dirty = 1;
    
        *PointerPte = TempPte;
    
        KeFillEntryTb ((PHARDWARE_PTE)PointerPte, VirtualAddress, TRUE);
    }

    return;
}

PVOID64
MiDbgTranslatePhysicalAddress (
    IN PHYSICAL_ADDRESS PhysicalAddress,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine maps the specified physical address and returns
    the virtual address which maps the physical address.

    The next call to MiDbgTranslatePhysicalAddress removes the
    previous physical address translation, hence only a single
    physical address can be examined at a time (can't cross page
    boundaries).

Arguments:

    PhysicalAddress - Supplies the physical address to map and translate.

    Flags -

        MMDBG_COPY_WRITE - Ignored.

        MMDBG_COPY_PHYSICAL - Ignored.

        MMDBG_COPY_UNSAFE - Ignored.

        MMDBG_COPY_CACHED - Use a PTE with the cached attribute for the
                            mapping to ensure TB coherence.

        MMDBG_COPY_UNCACHED - Use a PTE with the uncached attribute for the
                              mapping to ensure TB coherence.

        MMDBG_COPY_WRITE_COMBINED - Use a PTE with the writecombined attribute
                                    for the mapping to ensure TB coherence.

        Note the cached/uncached/write combined attribute requested by the
        caller is ignored if Mm can internally determine the proper attribute.

Return Value:

    The virtual address which corresponds to the physical address.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    ULONG Hint;
    MMPTE TempPte;
    PVOID BaseAddress;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    MMPTE OriginalPte;

    //
    // The debugger can call this before Mm has even initialized in Phase 0 !
    // MmDebugPte cannot be referenced before Mm has initialized without
    // causing an infinite loop wedging the machine.
    //

    if (MmPhysicalMemoryBlock == NULL) {
        return NULL;
    }

    Hint = 0;
    BaseAddress = MiGetVirtualAddressMappedByPte (MmDebugPte);

    TempPte = ValidKernelPte;

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    if (MiIsPhysicalMemoryAddress (PageFrameIndex, &Hint, FALSE) == TRUE) {

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        switch (Pfn1->u3.e1.CacheAttribute) {

            case MiCached:
            case MiNotMapped:
            default:
                break;
            case MiNonCached:
                MI_DISABLE_CACHING (TempPte);
                break;
            case MiWriteCombined:
                MI_SET_PTE_WRITE_COMBINE (TempPte);
                break;
        }
    }
    else {

        if (Flags & MMDBG_COPY_CACHED) {
            NOTHING;
        }
        else if (Flags & MMDBG_COPY_UNCACHED) {

            //
            // Just flush the entire TB on this processor but not the others
            // as an IPI may not be safe depending on when/why we broke into
            // the debugger.
            //
            // If IPIs were safe, we would have used
            // MI_PREPARE_FOR_NONCACHED (MiNonCached) instead.
            //

            KeFlushCurrentTb ();

            MI_DISABLE_CACHING (TempPte);
        }
        else if (Flags & MMDBG_COPY_WRITE_COMBINED) {

            //
            // Just flush the entire TB on this processor but not the others
            // as an IPI may not be safe depending on when/why we broke into
            // the debugger.
            //
            // If IPIs were safe, we would have used
            // MI_PREPARE_FOR_NONCACHED (MiWriteCombined) instead.
            //

            KeFlushCurrentTb ();

            MI_SET_PTE_WRITE_COMBINE (TempPte);
        }
        else {

            //
            // This is an access to I/O space and we don't know the correct
            // attribute type.  Only proceed if the caller explicitly specified
            // an attribute and hope he didn't get it wrong.  If no attribute
            // is specified then just return failure.
            //

            return NULL;
        }

        //
        // Since we really don't know if the caller got the attribute right,
        // set the flag below so (assuming the machine doesn't hard hang) we
        // can at least tell in the crash that he may have whacked the TB.
        //

        MmPoisonedTb += 1;
    }

    MI_SET_ACCESSED_IN_PTE (&TempPte, 1);

    OriginalPte.u.Long = 0;

    OriginalPte.u.Long = InterlockedCompareExchangePte (MmDebugPte,
                                                        TempPte.u.Long,
                                                        OriginalPte.u.Long);
                                                         
    if (OriginalPte.u.Long != 0) {

        //
        // Someone else is using the debug PTE.  Inform our caller it is not
        // available.
        //

        return NULL;
    }

    //
    // Just flush (no sweep) the TB entry on this processor as an IPI
    // may not be safe depending on when/why we broke into the debugger.
    // Note that if we are in kd, then all the processors are frozen and
    // this thread can't migrate so the local TB flush is enough.  For
    // the localkd case, our caller has raised to DISPATCH_LEVEL thereby
    // ensuring this thread can't migrate even though the other processors
    // are not frozen.
    //

    KiFlushSingleTb (TRUE, BaseAddress);

    return (PVOID64)((ULONG_PTR)BaseAddress + BYTE_OFFSET(PhysicalAddress.LowPart));
}

VOID
MiDbgUnTranslatePhysicalAddress (
    VOID
    )

/*++

Routine Description:

    This routine unmaps the virtual address currently mapped by the debug PTE.

    This is needed so that stale PTE mappings are not left in the debug PTE
    as if the page attribute subsequently changes, a stale mapping would
    cause TB incoherency.

    This can only be called if the previous MiDbgTranslatePhysicalAddress
    succeeded.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    PVOID BaseAddress;

    BaseAddress = MiGetVirtualAddressMappedByPte (MmDebugPte);

    ASSERT (MmIsAddressValid (BaseAddress));

#if defined (_WIN64)
    InterlockedExchange64 ((PLONG64)MmDebugPte, ZeroPte.u.Long);
#elif defined(_X86PAE_)
    KeInterlockedSwapPte ((PHARDWARE_PTE)MmDebugPte,
                          (PHARDWARE_PTE)&ZeroPte.u.Long);
#else
    InterlockedExchange ((PLONG)MmDebugPte, ZeroPte.u.Long);
#endif

    KiFlushSingleTb (TRUE, BaseAddress);

    return;
}
 
NTSTATUS
MmDbgCopyMemory (
    IN ULONG64 UntrustedAddress,
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Flags
    )

/*++

Routine Description:

    Transfers a single chunk of memory between a buffer and a system
    address.  The transfer can be a read or write with a virtual or
    physical address.

    The chunk size must be 1, 2, 4 or 8 bytes and the address
    must be appropriately aligned for the size.

Arguments:

    UntrustedAddress - Supplies the system address being read from or written
                       into.  The address is translated appropriately and
                       validated before being used.  This address must not
                       cross a page boundary.

    Buffer - Supplies the buffer to read into or write from.  It is the caller's
             responsibility to ensure this buffer address is nonpaged and valid
             (ie: will not generate any faults including access bit faults)
             throughout the duration of this call.  This routine (not the
             caller) will handle copying into this buffer as the buffer
             address may not be aligned properly for the requested transfer.

             Typically this buffer points to a kd circular buffer or an
             ExLockUserBuffer'd address.  Note this buffer can cross page
             boundaries.

    Size - Supplies the size of the transfer.  This may be 1, 2, 4 or 8 bytes.

    Flags -

        MMDBG_COPY_WRITE - Write from the buffer to the address.
                           If this is not set a read is done.

        MMDBG_COPY_PHYSICAL - The address is a physical address and by default
                              a PTE with a cached attribute will be used to
                              map it to retrieve (or set) the specified data.
                              If this is not set the address is virtual.

        MMDBG_COPY_UNSAFE - No locks are taken during operation.  It
                            is the caller's responsibility to ensure
                            stability of the system during the call.

        MMDBG_COPY_CACHED - If MMDBG_COPY_PHYSICAL is specified, then use
                            a PTE with the cached attribute for the mapping
                            to ensure TB coherence.

        MMDBG_COPY_UNCACHED - If MMDBG_COPY_PHYSICAL is specified, then use
                              a PTE with the uncached attribute for the mapping
                              to ensure TB coherence.

        MMDBG_COPY_WRITE_COMBINED - If MMDBG_COPY_PHYSICAL is specified, then
                                    use a PTE with the writecombined attribute
                                    for the mapping to ensure TB coherence.

Return Value:

    NTSTATUS.

--*/

{
    LOGICAL ForceWritableIfPossible;
    ULONG i;
    KIRQL PfnIrql;
    KIRQL OldIrql;
    PVOID VirtualAddress;
    HARDWARE_PTE Opaque;
    CHAR TempBuffer[8];
    PCHAR SourceBuffer;
    PCHAR TargetBuffer;
    PHYSICAL_ADDRESS PhysicalAddress;
    PETHREAD Thread;
    LOGICAL PfnHeld;
    ULONG WsHeld;

    switch (Size) {
        case 1:
            break;
        case 2:
            break;
        case 4:
            break;
        case 8:
            break;
        default:
            return STATUS_INVALID_PARAMETER_3;
    }

    if (UntrustedAddress & (Size - 1)) {

        //
        // The untrusted address is not properly aligned with the requested
        // transfer size.  This is a caller error.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (((ULONG)UntrustedAddress & ~(Size - 1)) !=
        (((ULONG)UntrustedAddress + Size - 1) & ~(Size - 1))) {

        //
        // The range spanned by the untrusted address crosses a page boundary.
        // Straddling pages is not allowed.  This is a caller error.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    PfnHeld = FALSE;
    WsHeld = 0;

    //
    // Initializing OldIrql and PhysicalAddress are not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PfnIrql = PASSIVE_LEVEL;
    OldIrql = PASSIVE_LEVEL;
    PhysicalAddress.LowPart = 0;
    ForceWritableIfPossible = TRUE;

    if ((Flags & MMDBG_COPY_PHYSICAL) == 0) {

        //
        // If the caller has not frozen the machine (ie: this is localkd or the
        // equivalent), then acquire the PFN lock.  This keeps the address
        // valid even after the return from the MmIsAddressValid call.  Note
        // that for system (or session) addresses, the relevant working set
        // mutex is acquired to prevent the page from getting trimmed or the
        // PTE access bit from getting cleared.  For user space addresses,
        // no mutex is needed because the access is performed using the user
        // virtual address inside an exception handler.
        //

        if ((Flags & MMDBG_COPY_UNSAFE) == 0) {

            if (KeGetCurrentIrql () > APC_LEVEL) {
                return STATUS_INVALID_PARAMETER_4;
            }

            //
            // Note that for safe copy mode (ie: the system is live), the
            // address must not be made writable if it is not already because
            // other threads might concurrently access it this way and losing
            // copy-on-write semantics, etc would be very bad.
            //

            ForceWritableIfPossible = FALSE;

            if ((PVOID) (ULONG_PTR) UntrustedAddress >= MmSystemRangeStart) {

                Thread = PsGetCurrentThread ();

                if (MmIsSessionAddress ((PVOID)(ULONG_PTR)UntrustedAddress)) {
                    if (MmGetSessionId (PsGetCurrentProcess ()) == 0) {
                        return STATUS_INVALID_PARAMETER_1;
                    }

                    WsHeld = 1;
                    LOCK_SESSION_SPACE_WS (OldIrql, Thread);
                }
                else {
                    WsHeld = 2;
                    LOCK_SYSTEM_WS (OldIrql, Thread);
                }

                PfnHeld = TRUE;
                LOCK_PFN (PfnIrql);
            }
            else {
                //
                // The caller specified a user address.  Probe and access
                // the address carefully inside an exception handler.
                //

                try {
                    if (Flags & MMDBG_COPY_WRITE) {
                        ProbeForWrite ((PVOID)(ULONG_PTR)UntrustedAddress, Size, Size);
                    }
                    else {
                        ProbeForRead ((PVOID)(ULONG_PTR)UntrustedAddress, Size, Size);
                    }
                } except(EXCEPTION_EXECUTE_HANDLER) {
                    return GetExceptionCode();
                }

                VirtualAddress = (PVOID) (ULONG_PTR) UntrustedAddress;

                if (Flags & MMDBG_COPY_WRITE) {
                    goto WriteData;
                }
                else {
                    goto ReadData;
                }
            }
        }

        if (MmIsAddressValid ((PVOID) (ULONG_PTR) UntrustedAddress) == FALSE) {

            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (WsHeld == 1) {
                UNLOCK_SESSION_SPACE_WS (OldIrql);
            }
            else if (WsHeld == 2) {
                UNLOCK_SYSTEM_WS (OldIrql);
            }

            return STATUS_INVALID_PARAMETER_1;
        }

        VirtualAddress = (PVOID) (ULONG_PTR) UntrustedAddress;
    }
    else {

        PhysicalAddress.QuadPart = UntrustedAddress;

        //
        // If the caller has not frozen the machine (ie: this is localkd or the
        // equivalent), then acquire the PFN lock.  This prevents
        // MmPhysicalMemoryBlock from changing inside the debug PTE routines
        // and also blocks APCs so malicious callers cannot suspend us
        // while we hold the debug PTE.
        //

        if ((Flags & MMDBG_COPY_UNSAFE) == 0) {

            if (KeGetCurrentIrql () > APC_LEVEL) {
                return STATUS_INVALID_PARAMETER_4;
            }

            PfnHeld = TRUE;
            LOCK_PFN (PfnIrql);
        }

        VirtualAddress = (PVOID) (ULONG_PTR) MiDbgTranslatePhysicalAddress (PhysicalAddress, Flags);

        if (VirtualAddress == NULL) {
            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            return STATUS_UNSUCCESSFUL;
        }
    }

    if (Flags & MMDBG_COPY_WRITE) {
        VirtualAddress = MiDbgWriteCheck (VirtualAddress, &Opaque, ForceWritableIfPossible);

        if (VirtualAddress == NULL) {
            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (WsHeld == 1) {
                UNLOCK_SESSION_SPACE_WS (OldIrql);
            }
            else if (WsHeld == 2) {
                UNLOCK_SYSTEM_WS (OldIrql);
            }
            return STATUS_INVALID_PARAMETER_1;
        }

WriteData:

        //
        // Carefully capture the source buffer into a local *aligned* buffer
        // as the write to the target must be done using the desired operation
        // size specified by the caller.  This is because the target may be
        // a memory mapped device which requires specific transfer sizes.
        //

        SourceBuffer = (PCHAR) Buffer;

        try {
            for (i = 0; i < Size; i += 1) {
                TempBuffer[i] = *SourceBuffer;
                SourceBuffer += 1;
            }
        } except(EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (WsHeld == 0);
            ASSERT (PfnHeld == FALSE);
            return GetExceptionCode();
        }

        switch (Size) {
            case 1:
                *(PCHAR) VirtualAddress = *(PCHAR) TempBuffer;
                break;
            case 2:
                *(PSHORT) VirtualAddress = *(PSHORT) TempBuffer;
                break;
            case 4:
                *(PULONG) VirtualAddress = *(PULONG) TempBuffer;
                break;
            case 8:
                *(PULONGLONG) VirtualAddress = *(PULONGLONG) TempBuffer;
                break;
            default:
                break;
        }

        if ((PVOID) (ULONG_PTR) UntrustedAddress >= MmSystemRangeStart) {
            MiDbgReleaseAddress (VirtualAddress, &Opaque);
        }
    }
    else {

ReadData:

        try {
            switch (Size) {
                case 1:
                    *(PCHAR) TempBuffer = *(PCHAR) VirtualAddress;
                    break;
                case 2:
                    *(PSHORT) TempBuffer = *(PSHORT) VirtualAddress;
                    break;
                case 4:
                    *(PULONG) TempBuffer = *(PULONG) VirtualAddress;
                    break;
                case 8:
                    *(PULONGLONG) TempBuffer = *(PULONGLONG) VirtualAddress;
                    break;
                default:
                    break;
            }
        } except(EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (WsHeld == 0);
            ASSERT (PfnHeld == FALSE);
            return GetExceptionCode();
        }

        //
        // The buffer to fill may not be aligned so do it one character at
        // a time.
        //

        TargetBuffer = (PCHAR) Buffer;

        for (i = 0; i < Size; i += 1) {
            *TargetBuffer = TempBuffer[i];
            TargetBuffer += 1;
        }
    }

    if (Flags & MMDBG_COPY_PHYSICAL) {
        MiDbgUnTranslatePhysicalAddress ();
    }

    if (PfnHeld == TRUE) {
        UNLOCK_PFN (PfnIrql);
    }

    if (WsHeld == 1) {
        UNLOCK_SESSION_SPACE_WS (OldIrql);
    }
    else if (WsHeld == 2) {
        UNLOCK_SYSTEM_WS (OldIrql);
    }

    return STATUS_SUCCESS;
}

LOGICAL
MmDbgIsLowMemOk (
    IN PFN_NUMBER PageFrameIndex,
    OUT PPFN_NUMBER NextPageFrameIndex,
    IN OUT PULONG CorruptionOffset
    )

/*++

Routine Description:

    This is a special function called only from the kernel debugger
    to check that the physical memory below 4Gb removed with /NOLOWMEM
    contains the expected fill patterns.  If not, there is a high
    probability that a driver which cannot handle physical addresses greater
    than 32 bits corrupted the memory.

Arguments:

    PageFrameIndex - Supplies the physical page number to check.

    NextPageFrameIndex - Supplies the next physical page number the caller
                         should check or 0 if the search is complete.

    CorruptionOffset - If corruption is found, the byte offset
                       of the corruption start is returned here.

Return Value:

    TRUE if the page was removed and the fill pattern is correct, or
    if the page was never removed.  FALSE if corruption was detected
    in the page.

Environment:

    This routine is for use of the kernel debugger ONLY, specifically
    the !chklowmem command.

    The debugger's PTE will be repointed.

--*/

{
#if defined (_MI_MORE_THAN_4GB_)

    PULONG Va;
    ULONG Index;
    PHYSICAL_ADDRESS Pa;
#if DBG
    PMMPFN Pfn;
#endif

    if (MiNoLowMemory == 0) {
        *NextPageFrameIndex = 0;
        return TRUE;
    }

    if (MiLowMemoryBitMap == NULL) {
        *NextPageFrameIndex = 0;
        return TRUE;
    }

    if (PageFrameIndex >= MiNoLowMemory - 1) {
        *NextPageFrameIndex = 0;
    }
    else {
        *NextPageFrameIndex = PageFrameIndex + 1;
    }

    //
    // Verify that the page to be verified is one of the reclaimed
    // pages.
    //

    if ((PageFrameIndex >= MiLowMemoryBitMap->SizeOfBitMap) ||
        (RtlCheckBit (MiLowMemoryBitMap, PageFrameIndex) == 0)) {

        return TRUE;
    }

    //
    // At this point we have a low page that is not in active use.
    // The fill pattern must match.
    //

#if DBG
    Pfn = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (Pfn->u4.PteFrame == MI_MAGIC_4GB_RECLAIM);
    ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);
#endif

    //
    // Map the physical page using the debug PTE so the
    // fill pattern can be validated.
    //
    // The debugger cannot be using this virtual address on entry or exit.
    //

    Pa.QuadPart = ((ULONGLONG)PageFrameIndex) << PAGE_SHIFT;

    Va = (PULONG) MiDbgTranslatePhysicalAddress (Pa, 0);

    if (Va == NULL) {
        return TRUE;
    }

    for (Index = 0; Index < PAGE_SIZE / sizeof(ULONG); Index += 1) {

        if (*Va != (PageFrameIndex | MI_LOWMEM_MAGIC_BIT)) {

            if (CorruptionOffset != NULL) {
                *CorruptionOffset = Index * sizeof(ULONG);
            }

            MiDbgUnTranslatePhysicalAddress ();
            return FALSE;
        }

        Va += 1;
    }
    MiDbgUnTranslatePhysicalAddress ();
#else
    UNREFERENCED_PARAMETER (PageFrameIndex);
    UNREFERENCED_PARAMETER (CorruptionOffset);

    *NextPageFrameIndex = 0;
#endif

    return TRUE;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\extsect.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

   extsect.c

Abstract:

    This module contains the routines which implement the
    NtExtendSection service.

Author:

    Lou Perazzoli (loup) 8-May-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtExtendSection)
#pragma alloc_text(PAGE,MmExtendSection)
#endif

extern SIZE_T MmAllocationFragment;

ULONG MiExtendedSubsectionsConvertedToDynamic;

#if DBG
VOID
MiSubsectionConsistent(
    IN PSUBSECTION Subsection
    )
/*++

Routine Description:

    This function checks to ensure the subsection is consistent.

Arguments:

    Subsection - Supplies a pointer to the subsection to be checked.

Return Value:

    None.

--*/

{
    ULONG   Sectors;
    ULONG   FullPtes;

    //
    // Compare the disk sectors (4K units) to the PTE allocation
    //

    Sectors = Subsection->NumberOfFullSectors;
    if (Subsection->u.SubsectionFlags.SectorEndOffset) {
        Sectors += 1;
    }

    //
    // Calculate how many PTEs are needed to map this number of sectors.
    //

    FullPtes = Sectors >> (PAGE_SHIFT - MM4K_SHIFT);

    if (Sectors & ((1 << (PAGE_SHIFT - MM4K_SHIFT)) - 1)) {
        FullPtes += 1;
    }

    if (FullPtes != Subsection->PtesInSubsection) {
        DbgPrint("Mm: Subsection inconsistent (%x vs %x)\n",
            FullPtes,
            Subsection->PtesInSubsection);
        DbgBreakPoint();
    }
}
#endif


NTSTATUS
NtExtendSection (
    IN HANDLE SectionHandle,
    IN OUT PLARGE_INTEGER NewSectionSize
    )

/*++

Routine Description:

    This function extends the size of the specified section.  If
    the current size of the section is greater than or equal to the
    specified section size, the size is not updated.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    NewSectionSize - Supplies the new size for the section object.

Return Value:

    NTSTATUS.

--*/

{
    KPROCESSOR_MODE PreviousMode;
    PVOID Section;
    NTSTATUS Status;
    LARGE_INTEGER CapturedNewSectionSize;

    PAGED_CODE();

    //
    // Check to make sure the new section size is accessible.
    //

    PreviousMode = KeGetPreviousMode();

    if (PreviousMode != KernelMode) {

        try {

            ProbeForWriteSmallStructure (NewSectionSize,
                                         sizeof(LARGE_INTEGER),
                                         PROBE_ALIGNMENT (LARGE_INTEGER));

            CapturedNewSectionSize = *NewSectionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }

    }
    else {

        CapturedNewSectionSize = *NewSectionSize;
    }

    //
    // Reference the section object.
    //

    Status = ObReferenceObjectByHandle (SectionHandle,
                                        SECTION_EXTEND_SIZE,
                                        MmSectionObjectType,
                                        PreviousMode,
                                        (PVOID *)&Section,
                                        NULL);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    Status = MmExtendSection (Section, &CapturedNewSectionSize, FALSE);

    ObDereferenceObject (Section);

    //
    // Update the NewSectionSize field.
    //

    try {

        //
        // Return the captured section size.
        //

        *NewSectionSize = CapturedNewSectionSize;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return Status;
}


VOID
MiAppendSubsectionChain (
    IN PMSUBSECTION LastSubsection,
    IN PMSUBSECTION ExtendedSubsectionHead
    )

/*++

Routine Description:

    This nonpagable wrapper function extends the specified subsection chain.

Arguments:

    LastSubsection - Supplies the last subsection in the existing control area.

    ExtendedSubsectionHead - Supplies an anchor pointing to the first
                             subsection in the chain to append.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMSUBSECTION NewSubsection;

    ASSERT (ExtendedSubsectionHead->NextSubsection != NULL);

    ASSERT (ExtendedSubsectionHead->u.SubsectionFlags.SectorEndOffset == 0);

    LOCK_PFN (OldIrql);

    //
    // This subsection may be extending a range that is already
    // mapped by a VAD(s).  There is no way to tell how many VADs
    // already map it so if any do, just leave all the new subsections
    // marked the as not reclaimable until the control area is deleted.
    //
    // If however, there is only the system cache reference and no user
    // references to this control area, then the subsections can be marked
    // as dynamic now.  Note other portions of code (currently only prefetch)
    // that issue "dereference from this subsection to the end of file"
    // are safe because these portions create user sections first and
    // so the first check below will be FALSE.
    //

    if (LastSubsection->ControlArea->NumberOfUserReferences == 0) {

        NewSubsection = (PMSUBSECTION) ExtendedSubsectionHead->NextSubsection;

        do {
            ASSERT (NewSubsection->u.SubsectionFlags.SubsectionStatic == 1);

            MI_SNAP_SUB (NewSubsection, 0x1);

            NewSubsection->u.SubsectionFlags.SubsectionStatic = 0;
            NewSubsection->u2.SubsectionFlags2.SubsectionConverted = 1;
            NewSubsection->NumberOfMappedViews = 1;

            MiRemoveViewsFromSection (NewSubsection, 
                                      NewSubsection->PtesInSubsection);

            MiExtendedSubsectionsConvertedToDynamic += 1;

            MI_SNAP_SUB (NewSubsection, 0x2);
            NewSubsection = (PMSUBSECTION) NewSubsection->NextSubsection;
        } while (NewSubsection != NULL);
    }

    LastSubsection->u.SubsectionFlags.SectorEndOffset = 0;

    LastSubsection->NumberOfFullSectors = ExtendedSubsectionHead->NumberOfFullSectors;

    //
    // A memory barrier is needed to ensure the writes initializing the
    // subsection fields are visible prior to linking the subsection into
    // the chain.  This is because some reads from these fields are done
    // lock free for improved performance.
    //

    KeMemoryBarrier ();

    LastSubsection->NextSubsection = ExtendedSubsectionHead->NextSubsection;

    UNLOCK_PFN (OldIrql);
}


NTSTATUS
MmExtendSection (
    IN PVOID SectionToExtend,
    IN OUT PLARGE_INTEGER NewSectionSize,
    IN ULONG IgnoreFileSizeChecking
    )

/*++

Routine Description:

    This function extends the size of the specified section.  If
    the current size of the section is greater than or equal to the
    specified section size, the size is not updated.

Arguments:

    Section - Supplies a pointer to a referenced section object.

    NewSectionSize - Supplies the new size for the section object.

    IgnoreFileSizeChecking -  Supplies the value TRUE is file size
                              checking should be ignored (i.e., it
                              is being called from a file system which
                              has already done the checks).  FALSE
                              if the checks still need to be made.

Return Value:

    NTSTATUS.

--*/

{
    PMMPTE ProtoPtes;
    MMPTE TempPte;
    PCONTROL_AREA ControlArea;
    PSEGMENT Segment;
    PSECTION Section;
    PSUBSECTION LastSubsection;
    PSUBSECTION Subsection;
    PMSUBSECTION ExtendedSubsection;
    MSUBSECTION ExtendedSubsectionHead;
    PMSUBSECTION LastExtendedSubsection;
    ULONG RequiredPtes;
    ULONG NumberOfPtes;
    ULONG PtesUsed;
    ULONG UnusedPtes;
    ULONG AllocationSize;
    ULONG RunningSize;
    ULONG NewSubsectionCount;
    UINT64 EndOfFile;
    UINT64 NumberOfPtesForEntireFile;
    NTSTATUS Status;
    LARGE_INTEGER NumberOf4KsForEntireFile;
    LARGE_INTEGER Starting4K;
    LARGE_INTEGER NextSubsection4KStart;
    LARGE_INTEGER Last4KChunk;
    ULONG PartialSize;
    SIZE_T AllocationFragment;
    PKTHREAD CurrentThread;

    PAGED_CODE();

    Section = (PSECTION)SectionToExtend;

    //
    // Make sure the section is really extendable - physical and
    // image sections are not.
    //

    Segment = Section->Segment;
    ControlArea = Segment->ControlArea;

    if ((ControlArea->u.Flags.PhysicalMemory || ControlArea->u.Flags.Image) ||
         (ControlArea->FilePointer == NULL)) {
        return STATUS_SECTION_NOT_EXTENDED;
    }

    //
    // Acquire the section extension mutex, this blocks other threads from
    // updating the size at the same time.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceExclusiveLite (&MmSectionExtendResource, TRUE);

    //
    // Calculate the number of prototype PTE chunks to build for this section.
    // A subsection is also allocated for each chunk as all the prototype PTEs
    // in any given chunk are initially encoded to point at the same subsection.
    //
    // The maximum total section size is 16PB (2^54).  This is because the
    // StartingSector4132 field in each subsection, ie: 2^42-1 bits of file
    // offset where the offset is in 4K (not pagesize) units.  Thus, a
    // subsection may describe a *BYTE* file start offset of maximum
    // 2^54 - 4K.
    //
    // Each subsection can span at most 16TB - 64K.
    //
    // This is because the NumberOfFullSectors and various other fields
    // in the subsection are ULONGs.
    //
    // The last item to notice is that the NumberOfSubsections is currently
    // a USHORT in the ControlArea.  Note this does not limit us to 64K-1
    // subsections because this field is only relied on for images not data.
    //

    NumberOfPtesForEntireFile = (NewSectionSize->QuadPart + PAGE_SIZE - 1) >> PAGE_SHIFT;

    NumberOfPtes = (ULONG)NumberOfPtesForEntireFile;

    if (NewSectionSize->QuadPart > MI_MAXIMUM_SECTION_SIZE) {
        Status = STATUS_SECTION_TOO_BIG;
        goto ReleaseAndReturn;
    }

    if (NumberOfPtesForEntireFile > (UINT64)((MAXULONG_PTR / sizeof(MMPTE)) - sizeof (SEGMENT))) {
        Status = STATUS_SECTION_TOO_BIG;
        goto ReleaseAndReturn;
    }

    if (NumberOfPtesForEntireFile > (UINT64)NewSectionSize->QuadPart) {
        Status = STATUS_SECTION_TOO_BIG;
        goto ReleaseAndReturn;
    }

    if (ControlArea->u.Flags.WasPurged == 0) {

        if ((UINT64)NewSectionSize->QuadPart <= (UINT64)Section->SizeOfSection.QuadPart) {
            *NewSectionSize = Section->SizeOfSection;
            goto ReleaseAndReturnSuccess;
        }
    }

    //
    // If a file handle was specified, set the allocation size of the file.
    //

    if (IgnoreFileSizeChecking == FALSE) {

        //
        // Release the resource so we don't deadlock with the file
        // system trying to extend this section at the same time.
        //

        ExReleaseResourceLite (&MmSectionExtendResource);

        //
        // Get a different resource to single thread query/set operations.
        //

        ExAcquireResourceExclusiveLite (&MmSectionExtendSetResource, TRUE);

        //
        // Query the file size to see if this file really needs extending.
        //
        // If the specified size is less than the current size, return
        // the current size.
        //

        Status = FsRtlGetFileSize (ControlArea->FilePointer,
                                   (PLARGE_INTEGER)&EndOfFile);

        if (!NT_SUCCESS (Status)) {
            ExReleaseResourceLite (&MmSectionExtendSetResource);
            KeLeaveCriticalRegionThread (CurrentThread);
            return Status;
        }

        if ((UINT64)NewSectionSize->QuadPart > EndOfFile) {

            //
            // Don't allow section extension unless the section was originally
            // created with write access.  The check couldn't be done at create
            // time without breaking existing binaries, so the caller gets the
            // error at this point instead.
            //

            if (((Section->InitialPageProtection & PAGE_READWRITE) |
                (Section->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {
#if DBG
                    DbgPrint("Section extension failed %x\n", Section);
#endif
                    ExReleaseResourceLite (&MmSectionExtendSetResource);
                    KeLeaveCriticalRegionThread (CurrentThread);
                    return STATUS_SECTION_NOT_EXTENDED;
            }

            //
            // Current file is smaller, attempt to set a new end of file.
            //

            EndOfFile = *(PUINT64)NewSectionSize;

            Status = FsRtlSetFileSize (ControlArea->FilePointer,
                                       (PLARGE_INTEGER)&EndOfFile);

            if (!NT_SUCCESS (Status)) {
                ExReleaseResourceLite (&MmSectionExtendSetResource);
                KeLeaveCriticalRegionThread (CurrentThread);
                return Status;
            }
        }

        if (Segment->ExtendInfo) {
            ExAcquireFastMutex (&MmSectionBasedMutex);
            if (Segment->ExtendInfo) {
                Segment->ExtendInfo->CommittedSize = EndOfFile;
            }
            ExReleaseFastMutex (&MmSectionBasedMutex);
        }

        //
        // Release the query/set resource and reacquire the extend section
        // resource.
        //

        ExReleaseResourceLite (&MmSectionExtendSetResource);
        ExAcquireResourceExclusiveLite (&MmSectionExtendResource, TRUE);
    }

    //
    // Find the last subsection.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint != NULL) {
        LastSubsection = (PSUBSECTION) ((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint;
    }
    else {
        if (ControlArea->u.Flags.Rom == 1) {
            LastSubsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }
        else {
            LastSubsection = (PSUBSECTION)(ControlArea + 1);
        }
    }

    while (LastSubsection->NextSubsection != NULL) {
        ASSERT (LastSubsection->UnusedPtes == 0);
        LastSubsection = LastSubsection->NextSubsection;
    }

    MI_CHECK_SUBSECTION (LastSubsection);

    //
    // Does the structure need extending?
    //

    if (NumberOfPtes <= Segment->TotalNumberOfPtes) {

        //
        // The segment is already large enough, just update
        // the section size and return.
        //

        Section->SizeOfSection = *NewSectionSize;
        if (Segment->SizeOfSegment < (UINT64)NewSectionSize->QuadPart) {

            //
            // Only update if it is really bigger.
            //

            Segment->SizeOfSegment = *(PUINT64)NewSectionSize;

            Mi4KStartFromSubsection(&Starting4K, LastSubsection);

            Last4KChunk.QuadPart = (NewSectionSize->QuadPart >> MM4K_SHIFT) - Starting4K.QuadPart;

            ASSERT (Last4KChunk.HighPart == 0);

            LastSubsection->NumberOfFullSectors = Last4KChunk.LowPart;
            LastSubsection->u.SubsectionFlags.SectorEndOffset =
                                        NewSectionSize->LowPart & MM4K_MASK;
            MI_CHECK_SUBSECTION (LastSubsection);
        }
        goto ReleaseAndReturnSuccess;
    }

    //
    // Add new structures to the section - locate the last subsection
    // and add there.
    //

    RequiredPtes = NumberOfPtes - Segment->TotalNumberOfPtes;
    PtesUsed = 0;

    if (RequiredPtes < LastSubsection->UnusedPtes) {

        //
        // There are ample PTEs to extend the section already allocated.
        //

        PtesUsed = RequiredPtes;
        RequiredPtes = 0;

    }
    else {
        PtesUsed = LastSubsection->UnusedPtes;
        RequiredPtes -= PtesUsed;
    }

    LastSubsection->PtesInSubsection += PtesUsed;
    LastSubsection->UnusedPtes -= PtesUsed;
    Segment->SizeOfSegment += (UINT64)PtesUsed * PAGE_SIZE;
    Segment->TotalNumberOfPtes += PtesUsed;

    if (RequiredPtes == 0) {

        //
        // There is no extension necessary, update the high VBN.
        //

        Mi4KStartFromSubsection(&Starting4K, LastSubsection);

        Last4KChunk.QuadPart = (NewSectionSize->QuadPart >> MM4K_SHIFT) - Starting4K.QuadPart;

        ASSERT (Last4KChunk.HighPart == 0);

        LastSubsection->NumberOfFullSectors = Last4KChunk.LowPart;

        LastSubsection->u.SubsectionFlags.SectorEndOffset =
                                    NewSectionSize->LowPart & MM4K_MASK;
        MI_CHECK_SUBSECTION (LastSubsection);
    }
    else {

        //
        // An extension is required.  Allocate the subsection and prototype
        // PTEs now.
        //

        NewSubsectionCount = 0;

        NumberOf4KsForEntireFile.QuadPart = Segment->SizeOfSegment >> MM4K_SHIFT;
        AllocationSize = (ULONG) ROUND_TO_PAGES (RequiredPtes * sizeof(MMPTE));

        AllocationFragment = MmAllocationFragment;

        RunningSize = 0;

        ExtendedSubsectionHead = *(PMSUBSECTION)LastSubsection;

        LastExtendedSubsection = &ExtendedSubsectionHead;

        ASSERT (LastExtendedSubsection->NextSubsection == NULL);

        //
        // Initializing NextSubsection4KStart is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        NextSubsection4KStart.QuadPart = 0;

        do {

            PartialSize = AllocationSize - RunningSize;

            //
            // Bound the size of each prototype PTE allocation so both :
            //  1. it can succeed even in cases where the pool is fragmented.
            //  2. later on last control area dereference, each subsection
            //     is converted to dynamic and can be pruned/recreated
            //     individually without losing (or requiring) contiguous pool.
            //

            if (PartialSize > AllocationFragment) {
                PartialSize = (ULONG) AllocationFragment;
            }

            //
            // Allocate an extended subsection.
            //

            ExtendedSubsection = (PMSUBSECTION) ExAllocatePoolWithTag (NonPagedPool,
                                                            sizeof(MSUBSECTION),
                                                            'dSmM');
            if (ExtendedSubsection == NULL) {
                goto ExtensionFailed;
            }

            ExtendedSubsection->NextSubsection = NULL;
            LastExtendedSubsection->NextSubsection = (PSUBSECTION) ExtendedSubsection;

            ProtoPtes = (PMMPTE)ExAllocatePoolWithTag (PagedPool,
                                                       PartialSize,
                                                       MMSECT);

            ExtendedSubsection->SubsectionBase = ProtoPtes;

            if (ProtoPtes == NULL) {
                goto ExtensionFailed;
            }

            ASSERT (ControlArea->FilePointer != NULL);

            ExtendedSubsection->u.LongFlags = 0;

            ExtendedSubsection->ControlArea = ControlArea;

            ExtendedSubsection->PtesInSubsection = PartialSize / sizeof(MMPTE);
            ExtendedSubsection->UnusedPtes = 0;

            RunningSize += PartialSize;

            if (RunningSize > (RequiredPtes * sizeof(MMPTE))) {
                UnusedPtes = RunningSize / sizeof(MMPTE) - RequiredPtes;
                ExtendedSubsection->PtesInSubsection -= UnusedPtes;
                ExtendedSubsection->UnusedPtes = UnusedPtes;
            }

            //
            // Fill in the prototype PTEs for this subsection.
            //
            // Set all the PTEs to the initial execute-read-write protection.
            // The section will control access to these and the segment
            // must provide a method to allow other users to map the file
            // for various protections.
            //

            TempPte.u.Long = MiGetSubsectionAddressForPte (ExtendedSubsection);
            TempPte.u.Soft.Prototype = 1;

            TempPte.u.Soft.Protection = Segment->SegmentPteTemplate.u.Soft.Protection;

            MiFillMemoryPte (ProtoPtes, PartialSize, TempPte.u.Long);

            ExtendedSubsection->u.SubsectionFlags.Protection =
                                        (unsigned) TempPte.u.Soft.Protection;

            ExtendedSubsection->DereferenceList.Flink = NULL;
            ExtendedSubsection->DereferenceList.Blink = NULL;
            ExtendedSubsection->NumberOfMappedViews = 0;
            ExtendedSubsection->u2.LongFlags2 = 0;

            //
            // This subsection may be extending a range that is already
            // mapped by a VAD(s).  There is no way to tell how many VADs
            // already map it so just mark the entire subsection as not
            // reclaimable until the control area is deleted.
            //
            // This also saves other portions of code that issue "dereference
            // from this subsection to the end of file" as these subsections are
            // marked as static not dynamic (at least until segment dereference
            // time).
            //
            // When this chain is appended to the control area at the end of
            // this routine an attempt is made to convert the subsection chain
            // to dynamic if no user mapped views are active.
            //

            ExtendedSubsection->u.SubsectionFlags.SubsectionStatic = 1;

            //
            // Adjust the previous subsection to account for the new length.
            // Note that since the next allocation in this loop may fail,
            // the very first previous subsection changes are not rippled
            // to the chained subsection until the loop completes successfully.
            //

            if (LastExtendedSubsection == &ExtendedSubsectionHead) {

                Mi4KStartFromSubsection (&Starting4K, LastExtendedSubsection);

                Last4KChunk.QuadPart = NumberOf4KsForEntireFile.QuadPart -
                                            Starting4K.QuadPart;

                if (LastExtendedSubsection->u.SubsectionFlags.SectorEndOffset) {
                    Last4KChunk.QuadPart += 1;
                }

                ASSERT (Last4KChunk.HighPart == 0);

                LastExtendedSubsection->NumberOfFullSectors = Last4KChunk.LowPart;
                LastExtendedSubsection->u.SubsectionFlags.SectorEndOffset = 0;

                //
                // If the number of sectors doesn't completely fill the PTEs
                // (only possible when the page size is not MM4K), then
                // fill it now.
                //

                if (LastExtendedSubsection->NumberOfFullSectors & ((1 << (PAGE_SHIFT - MM4K_SHIFT)) - 1)) {
                    LastExtendedSubsection->NumberOfFullSectors += 1;
                }

                MI_CHECK_SUBSECTION (LastExtendedSubsection);

                Starting4K.QuadPart += LastExtendedSubsection->NumberOfFullSectors;
                NextSubsection4KStart.QuadPart = Starting4K.QuadPart;
            }
            else {
                NextSubsection4KStart.QuadPart += LastExtendedSubsection->NumberOfFullSectors;
            }

            //
            // Initialize the newly allocated subsection.
            //

            Mi4KStartForSubsection (&NextSubsection4KStart, ExtendedSubsection);

            if (RunningSize < AllocationSize) {

                //
                // Not the final subsection so all quantities are full pages.
                //

                ExtendedSubsection->NumberOfFullSectors =
                        (PartialSize / sizeof (MMPTE)) << (PAGE_SHIFT - MM4K_SHIFT);
                ExtendedSubsection->u.SubsectionFlags.SectorEndOffset = 0;
            }
            else {

                //
                // The final subsection so quantities are not always full pages.
                //

                Last4KChunk.QuadPart =
                    (NewSectionSize->QuadPart >> MM4K_SHIFT) - NextSubsection4KStart.QuadPart;

                ASSERT (Last4KChunk.HighPart == 0);

                ExtendedSubsection->NumberOfFullSectors = Last4KChunk.LowPart;

                ExtendedSubsection->u.SubsectionFlags.SectorEndOffset =
                                    NewSectionSize->LowPart & MM4K_MASK;
            }

            MI_CHECK_SUBSECTION (ExtendedSubsection);

            LastExtendedSubsection = ExtendedSubsection;

            NewSubsectionCount += 1;

        } while (RunningSize < AllocationSize);

        ASSERT (ControlArea->DereferenceList.Flink == NULL);

        //
        // Link the newly created subsection chain into the existing list.
        // Note this any adjustments (NumberOfFullSectors, etc) made to
        // the temp copy of the last subsection in the existing control
        // area must be *CAREFULLY* copied to the real copy in the chain (the
        // entire structure cannot just be block copied) as other fields
        // in the real copy (ie: NumberOfMappedViews may be changed in
        // parallel by another thread).
        //

        if (ControlArea->NumberOfUserReferences == 0) {
            ASSERT (IgnoreFileSizeChecking == TRUE);
        }

        Segment->TotalNumberOfPtes += RequiredPtes;

        MiAppendSubsectionChain ((PMSUBSECTION)LastSubsection,
                                 &ExtendedSubsectionHead);

        ControlArea->NumberOfSubsections =
            (USHORT)(ControlArea->NumberOfSubsections + NewSubsectionCount);

        if (LastExtendedSubsection != &ExtendedSubsectionHead) {
            ((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint =
                    LastExtendedSubsection;
        }
    }

    Segment->SizeOfSegment = *(PUINT64)NewSectionSize;
    Section->SizeOfSection = *NewSectionSize;

ReleaseAndReturnSuccess:

    Status = STATUS_SUCCESS;

ReleaseAndReturn:

    ExReleaseResourceLite (&MmSectionExtendResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    return Status;

ExtensionFailed:

    //
    // Required pool to extend the section could not be allocated.
    // Reset the subsection and control area fields to their
    // original values.
    //

    LastSubsection->PtesInSubsection -= PtesUsed;
    LastSubsection->UnusedPtes += PtesUsed;
    Segment->TotalNumberOfPtes -= PtesUsed;
    Segment->SizeOfSegment -= ((UINT64)PtesUsed * PAGE_SIZE);

    //
    // Free all the previous allocations and return an error.
    //

    LastSubsection = ExtendedSubsectionHead.NextSubsection;

    while (LastSubsection != NULL) {
        Subsection = LastSubsection->NextSubsection;
        if (LastSubsection->SubsectionBase != NULL) {
            ExFreePool (LastSubsection->SubsectionBase);
        }
        ExFreePool (LastSubsection);
        LastSubsection = Subsection;
    }

    Status = STATUS_INSUFFICIENT_RESOURCES;
    goto ReleaseAndReturn;
}

PMMPTE
FASTCALL
MiGetProtoPteAddressExtended (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    )

/*++

Routine Description:

    This function calculates the address of the prototype PTE
    for the corresponding virtual address.

Arguments:

    Vad - Supplies a pointer to the virtual address desciptor which
          encompasses the virtual address.

    Vpn - Supplies the virtual page number to locate a prototype PTE for.

Return Value:

    The corresponding prototype PTE address.

--*/

{
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG PteOffset;

    ControlArea = Vad->ControlArea;

    if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Locate the subsection which contains the First Prototype PTE
    // for this VAD.
    //

    while ((Subsection->SubsectionBase == NULL) ||
           (Vad->FirstPrototypePte < Subsection->SubsectionBase) ||
           (Vad->FirstPrototypePte >=
               &Subsection->SubsectionBase[Subsection->PtesInSubsection])) {

        //
        // Get the next subsection.
        //

        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // How many PTEs beyond this subsection must we go?
    //

    PteOffset = (ULONG) (((Vpn - Vad->StartingVpn) +
                 (ULONG)(Vad->FirstPrototypePte - Subsection->SubsectionBase)) -
                 Subsection->PtesInSubsection);

    ASSERT (PteOffset < 0xF0000000);

    PteOffset += Subsection->PtesInSubsection;

    //
    // Locate the subsection which contains the prototype PTEs.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    //
    // The PTEs are in this subsection.
    //

    ASSERT (Subsection->SubsectionBase != NULL);

    ASSERT (PteOffset < Subsection->PtesInSubsection);

    return &Subsection->SubsectionBase[PteOffset];

}

PSUBSECTION
FASTCALL
MiLocateSubsection (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    )

/*++

Routine Description:

    This function calculates the address of the subsection
    for the corresponding virtual address.

    This function only works for mapped files NOT mapped images.

Arguments:

    Vad - Supplies a pointer to the virtual address desciptor which
          encompasses the virtual address.

    Vpn - Supplies the virtual page number to locate a prototype PTE for.

Return Value:

    The corresponding prototype subsection.

--*/

{
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG PteOffset;

    ControlArea = Vad->ControlArea;

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (ControlArea->u.Flags.Image) {

        if (ControlArea->u.Flags.GlobalOnlyPerSession == 1) {
            Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }

        //
        // There is only one subsection, don't look any further.
        //

        return Subsection;
    }

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    //
    // Locate the subsection which contains the First Prototype PTE
    // for this VAD.  Note all the SubsectionBase values must be non-NULL
    // for the subsection range spanned by the VAD because the VAD still
    // exists.  Carefully skip over preceding subsections not mapped by
    // this VAD because if no other VADs map them either, their base
    // can be NULL.
    //

    while ((Subsection->SubsectionBase == NULL) ||
           (Vad->FirstPrototypePte < Subsection->SubsectionBase) ||
           (Vad->FirstPrototypePte >=
               &Subsection->SubsectionBase[Subsection->PtesInSubsection])) {

        //
        // Get the next subsection.
        //

        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // How many PTEs beyond this subsection must we go?
    //

    PteOffset = (ULONG)((Vpn - Vad->StartingVpn) +
         (ULONG)(Vad->FirstPrototypePte - Subsection->SubsectionBase));

    ASSERT (PteOffset < 0xF0000000);

    //
    // Locate the subsection which contains the prototype PTEs.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
        ASSERT (Subsection->SubsectionBase != NULL);
    }

    //
    // The PTEs are in this subsection.
    //

    ASSERT (Subsection->SubsectionBase != NULL);

    return Subsection;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\flushsec.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

   flushsec.c

Abstract:

    This module contains the routines which implement the
    NtFlushVirtualMemory service.

Author:

    Lou Perazzoli (loup) 8-May-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

PSUBSECTION
MiGetSystemCacheSubsection (
    IN PVOID BaseAddress,
    OUT PMMPTE *ProtoPte
    );

VOID
MiFlushDirtyBitsToPfn (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN PEPROCESS Process,
    IN BOOLEAN SystemCache
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFlushVirtualMemory)
#pragma alloc_text(PAGE,MmFlushVirtualMemory)
#endif

extern POBJECT_TYPE IoFileObjectType;

NTSTATUS
NtFlushVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes a range of virtual address which map
    a data file back into the data file if they have been modified.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
         the base address the flushed region.  The initial value
         of this argument is the base address of the region of the
         pages to flush.

    RegionSize - Supplies a pointer to a variable that will receive
         the actual size in bytes of the flushed region of pages.
         The initial value of this argument is rounded up to the
         next host-page-size boundary.

         If this value is specified as zero, the mapped range from
         the base address to the end of the range is flushed.

    IoStatus - Returns the value of the IoStatus for the last attempted
         I/O operation.

Return Value:

    Returns the status

    TBS


--*/

{
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    IO_STATUS_BLOCK TemporaryIoStatus;

    PAGED_CODE();

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {

        //
        // Establish an exception handler, probe the specified addresses
        // for write access and capture the initial values.
        //

        try {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
            ProbeForWriteIoStatus (IoStatus);

            //
            // Capture the base address.
            //

            CapturedBase = *BaseAddress;

            //
            // Capture the region size.
            //

            CapturedRegionSize = *RegionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }

    }
    else {

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if (((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_2;

    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );
    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    Status = MmFlushVirtualMemory (Process,
                                   &CapturedBase,
                                   &CapturedRegionSize,
                                   &TemporaryIoStatus);

    ObDereferenceObject (Process);

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *RegionSize = CapturedRegionSize;
        *BaseAddress = PAGE_ALIGN (CapturedBase);
        *IoStatus = TemporaryIoStatus;

    } except (EXCEPTION_EXECUTE_HANDLER) {
    }

    return Status;

}


VOID
MiFlushAcquire (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a helper routine to reference count the control area if needed
    during a flush section call to prevent the section object from being
    deleted while the flush is ongoing.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews += 1;

    UNLOCK_PFN (OldIrql);
}


VOID
MiFlushRelease (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a helper routine to release the control area reference needed
    during a flush section call.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}


NTSTATUS
MmFlushVirtualMemory (
    IN PEPROCESS Process,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes a range of virtual address which map
    a data file back into the data file if they have been modified.

    Note that the modification is this process's view of the pages,
    on certain implementations (like the Intel 386), the modify
    bit is captured in the PTE and not forced to the PFN database
    until the page is removed from the working set.  This means
    that pages which have been modified by another process will
    not be flushed to the data file.

Arguments:

    Process - Supplies a pointer to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the flushed region.  The initial value
                  of this argument is the base address of the region of the
                  pages to flush.

    RegionSize - Supplies a pointer to a variable that will receive
                 the actual size in bytes of the flushed region of pages.
                 The initial value of this argument is rounded up to the
                 next host-page-size boundary.

                 If this value is specified as zero, the mapped range from
                 the base address to the end of the range is flushed.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

Return Value:

    NTSTATUS.

--*/

{
    PMMVAD Vad;
    PVOID EndingAddress;
    PVOID Va;
    PEPROCESS CurrentProcess;
    BOOLEAN SystemCache;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    PMMPTE FinalPte;
    PSUBSECTION Subsection;
    PSUBSECTION LastSubsection;
    NTSTATUS Status;
    ULONG ConsecutiveFileLockFailures;
    ULONG Waited;
    LOGICAL EntireRestOfVad;
    LOGICAL Attached;
    KAPC_STATE ApcState;

    PAGED_CODE();

    Attached = FALSE;

    //
    // Determine if the specified base address is within the system
    // cache and if so, don't attach, the working set mutex is still
    // required to "lock" paged pool pages (proto PTEs) into the
    // working set.
    //

    EndingAddress = (PVOID)(((ULONG_PTR)*BaseAddress + *RegionSize - 1) |
                                                            (PAGE_SIZE - 1));
    *BaseAddress = PAGE_ALIGN (*BaseAddress);

    if (MI_IS_SESSION_ADDRESS (*BaseAddress)) {

        //
        // Nothing in session space needs flushing.
        //

        return STATUS_NOT_MAPPED_VIEW;
    }

    CurrentProcess = PsGetCurrentProcess ();

    if (!MI_IS_SYSTEM_CACHE_ADDRESS(*BaseAddress)) {

        SystemCache = FALSE;

        //
        // Attach to the specified process.
        //

        if (CurrentProcess != Process) {
            KeStackAttachProcess (&Process->Pcb, &ApcState);
            Attached = TRUE;
        }

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        Vad = MiLocateAddress (*BaseAddress);

        if (Vad == NULL) {

            //
            // No Virtual Address Descriptor located for Base Address.
            //

            Status = STATUS_NOT_MAPPED_VIEW;
            goto ErrorReturn;
        }

        if (*RegionSize == 0) {
            EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);
            EntireRestOfVad = TRUE;
        }
        else {
            EntireRestOfVad = FALSE;
        }

        if ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (MI_VA_TO_VPN (EndingAddress) > Vad->EndingVpn)) {

            //
            // This virtual address descriptor does not refer to a Segment
            // object.
            //

            Status = STATUS_NOT_MAPPED_VIEW;
            goto ErrorReturn;
        }

        //
        // Make sure this VAD maps a data file (not an image file).
        //

        ControlArea = Vad->ControlArea;

        if ((ControlArea->FilePointer == NULL) ||
             (Vad->u.VadFlags.ImageMap == 1)) {

            //
            // This virtual address descriptor does not refer to a Segment
            // object.
            //

            Status = STATUS_NOT_MAPPED_DATA;
            goto ErrorReturn;
        }

        LOCK_WS_UNSAFE (Process);
    }
    else {

        //
        // Initializing Vad, ControlArea and EntireRestOfVad is not needed for
        // correctness but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        Vad = NULL;
        ControlArea = NULL;
        EntireRestOfVad = FALSE;

        SystemCache = TRUE;
        Process = CurrentProcess;
        LOCK_WS (Process);
    }

    PointerPxe = MiGetPxeAddress (*BaseAddress);
    PointerPpe = MiGetPpeAddress (*BaseAddress);
    PointerPde = MiGetPdeAddress (*BaseAddress);
    PointerPte = MiGetPteAddress (*BaseAddress);
    LastPte = MiGetPteAddress (EndingAddress);
    *RegionSize = (PCHAR)EndingAddress - (PCHAR)*BaseAddress + 1;

retry:

    while (!MiDoesPxeExistAndMakeValid (PointerPxe, Process, FALSE, &Waited)) {

        //
        // This page directory parent entry is empty, go to the next one.
        //

        PointerPxe += 1;
        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        if (PointerPte > LastPte) {
            break;
        }
    }

    while (!MiDoesPpeExistAndMakeValid (PointerPpe, Process, FALSE, &Waited)) {

        //
        // This page directory parent entry is empty, go to the next one.
        //

        PointerPpe += 1;
        PointerPxe = MiGetPteAddress (PointerPpe);
        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        if (PointerPte > LastPte) {
            break;
        }
#if (_MI_PAGING_LEVELS >= 4)
        if (MiIsPteOnPdeBoundary (PointerPpe)) {
            goto retry;
        }
#endif
    }

    Waited = 0;

    if (PointerPte <= LastPte) {
        while (!MiDoesPdeExistAndMakeValid(PointerPde, Process, FALSE, &Waited)) {

            //
            // No page table page exists for this address.
            //

            PointerPde += 1;

            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

            if (PointerPte > LastPte) {
                break;
            }

#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {

                if (MiIsPteOnPpeBoundary (PointerPde)) {
                    PointerPxe = MiGetPdeAddress (PointerPde);
                }
                PointerPpe = MiGetPteAddress (PointerPde);
                goto retry;
            }
#endif

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
        }

        //
        // If the PFN lock (and accordingly the WS mutex) was
        // released and reacquired we must retry the operation.
        //

        if ((PointerPte <= LastPte) && (Waited != 0)) {
            goto retry;
        }
    }

    MiFlushDirtyBitsToPfn (PointerPte, LastPte, Process, SystemCache);

    if (SystemCache) {

        //
        // No VADs exist for the system cache.
        //

        UNLOCK_WS (Process);

        Subsection = MiGetSystemCacheSubsection (*BaseAddress, &PointerPte);

        LastSubsection = MiGetSystemCacheSubsection (EndingAddress, &FinalPte);

        //
        // Flush the PTEs from the specified section.
        //

        Status = MiFlushSectionInternal (PointerPte,
                                         FinalPte,
                                         Subsection,
                                         LastSubsection,
                                         FALSE,
                                         TRUE,
                                         IoStatus);
    }
    else {

        //
        // Protect against the section being prematurely deleted.
        //

        MiFlushAcquire (ControlArea);

        PointerPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (*BaseAddress));
        Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(*BaseAddress));
        LastSubsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(EndingAddress));

        //
        // The last subsection is NULL if the section is not fully 
        // committed.  Only allow the flush if the caller said do the whole
        // thing, otherwise it's an error.
        //

        if (LastSubsection == NULL) {

            if (EntireRestOfVad == FALSE) {

                //
                // Caller can only specify the range that is committed or zero
                // to indicate the entire range.
                //

                UNLOCK_WS_AND_ADDRESS_SPACE (Process);
                if (Attached == TRUE) {
                    KeUnstackDetachProcess (&ApcState);
                }
                MiFlushRelease (ControlArea);
                return STATUS_NOT_MAPPED_VIEW;
            }

            LastSubsection = Subsection;
            while (LastSubsection->NextSubsection) {
                LastSubsection = LastSubsection->NextSubsection;
            }

            //
            // A memory barrier is needed to read the subsection chains
            // in order to ensure the writes to the actual individual
            // subsection data structure fields are visible in correct
            // order.  This avoids the need to acquire any stronger
            // synchronization (ie: PFN lock), thus yielding better
            // performance and pagability.
            //

            KeMemoryBarrier ();

            FinalPte = LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection - 1;
        }
        else {
            FinalPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (EndingAddress));
        }

        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        //
        // Preacquire the file to synchronize the flush.
        //

        ConsecutiveFileLockFailures = 0;

        do {

            Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            if (!NT_SUCCESS(Status)) {
                break;
            }

            //
            // Flush the PTEs from the specified section.
            //

            Status = MiFlushSectionInternal (PointerPte,
                                             FinalPte,
                                             Subsection,
                                             LastSubsection,
                                             TRUE,
                                             TRUE,
                                             IoStatus);

            //
            // Release the file we acquired.
            //

            FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);

            //
            // Only try the request more than once if the filesystem told us
            // it had a deadlock.
            //

            if (Status != STATUS_FILE_LOCK_CONFLICT) {
                break;
            }

            ConsecutiveFileLockFailures += 1;
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

        } while (ConsecutiveFileLockFailures < 5);

        MiFlushRelease (ControlArea);
    }

    return Status;

ErrorReturn:

    ASSERT (SystemCache == FALSE);

    UNLOCK_ADDRESS_SPACE (Process);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    return Status;

}

NTSTATUS
MmFlushSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer,
    IN PLARGE_INTEGER Offset,
    IN SIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus,
    IN ULONG AcquireFile
    )

/*++

Routine Description:

    This function flushes to the backing file any modified pages within
    the specified range of the section.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects.

    Offset - Supplies the offset into the section in which to begin
             flushing pages.  If this argument is not present, then the
             whole section is flushed without regard to the region size
             argument.

    RegionSize - Supplies the size in bytes to flush.  This is rounded
                 to a page multiple.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

    AcquireFile - Nonzero if the callback should be used to acquire the file.

Return Value:

    Returns status of the operation.

--*/

{
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    KIRQL OldIrql;
    ULONG PteOffset;
    ULONG LastPteOffset;
    PSUBSECTION Subsection;
    PSUBSECTION TempSubsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    PMAPPED_FILE_SEGMENT Segment;
    PETHREAD CurrentThread;
    NTSTATUS status;
    BOOLEAN OldClusterState;
    ULONG ConsecutiveFileLockFailures;

    //
    // Initialize IoStatus for success, in case we take an early exit.
    //

    IoStatus->Status = STATUS_SUCCESS;
    IoStatus->Information = RegionSize;

    LOCK_PFN (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    ASSERT ((ControlArea == NULL) || (ControlArea->u.Flags.Image == 0));

    if ((ControlArea == NULL) ||
        (ControlArea->u.Flags.BeingDeleted) ||
        (ControlArea->u.Flags.BeingCreated) ||
        (ControlArea->u.Flags.Rom) ||
        (ControlArea->NumberOfPfnReferences == 0)) {

        //
        // This file no longer has an associated segment or is in the
        // process of coming or going.
        // If the number of PFN references is zero, then this control
        // area does not have any valid or transition pages that need
        // to be flushed.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    //
    // Locate the subsection.
    //

    ASSERT (ControlArea->u.Flags.Image == 0);
    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);
    ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

    if (!ARGUMENT_PRESENT (Offset)) {

        //
        // If the offset is not specified, flush the complete file ignoring
        // the region size.
        //

        ASSERT (ControlArea->FilePointer != NULL);

        PteOffset = 0;

        LastSubsection = Subsection;

        Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

        if (MmIsAddressValid (Segment)) {
            if (Segment->LastSubsectionHint != NULL) {
                LastSubsection = (PSUBSECTION) Segment->LastSubsectionHint;
            }
        }

        while (LastSubsection->NextSubsection != NULL) {
            LastSubsection = LastSubsection->NextSubsection;
        }

        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }
    else {

        PteOffset = (ULONG)(Offset->QuadPart >> PAGE_SHIFT);

        //
        // Make sure the PTEs are not in the extended part of the segment.
        //

        while (PteOffset >= Subsection->PtesInSubsection) {
            PteOffset -= Subsection->PtesInSubsection;
            if (Subsection->NextSubsection == NULL) {

                //
                // Past end of mapping, just return success.
                //

                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }
            Subsection = Subsection->NextSubsection;
        }

        ASSERT (PteOffset < Subsection->PtesInSubsection);

        //
        // Locate the address of the last prototype PTE to be flushed.
        //

        LastPteOffset = PteOffset + (ULONG)(((RegionSize + BYTE_OFFSET(Offset->LowPart)) - 1) >> PAGE_SHIFT);

        LastSubsection = Subsection;

        while (LastPteOffset >= LastSubsection->PtesInSubsection) {
            LastPteOffset -= LastSubsection->PtesInSubsection;
            if (LastSubsection->NextSubsection == NULL) {
                LastPteOffset = LastSubsection->PtesInSubsection - 1;
                break;
            }
            LastSubsection = LastSubsection->NextSubsection;
        }

        ASSERT (LastPteOffset < LastSubsection->PtesInSubsection);
    }

    //
    // Try for the fast reference on the first and last subsection.
    // If that cannot be gotten, then there are no prototype PTEs for this
    // subsection, therefore there is nothing in it to flush so leap forwards.
    //
    // Note that subsections in between do not need referencing as
    // MiFlushSectionInternal is smart enough to skip them if they're
    // nonresident.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
        do {
            //
            // If this increment would put us past the end offset, then nothing
            // to flush, just return success.
            //

            if (Subsection == LastSubsection) {
                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }
            Subsection = Subsection->NextSubsection;

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            if (Subsection == NULL) {
                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }

            if ((PMSUBSECTION)Subsection->SubsectionBase == NULL) {
                continue;
            }

            if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
                continue;
            }

            //
            // Start the flush at this subsection which is now referenced.
            //

            PointerPte = &Subsection->SubsectionBase[0];
            break;

        } while (TRUE);
    }
    else {
        PointerPte = &Subsection->SubsectionBase[PteOffset];
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // The first subsection is referenced, now reference count the last one.
    // If the first is the last, just double reference it anyway as it
    // simplifies cleanup later.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)LastSubsection) == FALSE) {

        ASSERT (Subsection != LastSubsection);

        TempSubsection = Subsection->NextSubsection;
        LastSubsectionWithProtos = NULL;

        while (TempSubsection != LastSubsection) {

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            ASSERT (TempSubsection != NULL);

            if ((PMSUBSECTION)TempSubsection->SubsectionBase != NULL) {
                LastSubsectionWithProtos = TempSubsection;
            }

            TempSubsection = TempSubsection->NextSubsection;
        }

        //
        // End the flush at this subsection and reference it.
        //

        if (LastSubsectionWithProtos == NULL) {
            ASSERT (Subsection != NULL);
            ASSERT (Subsection->SubsectionBase != NULL);
            TempSubsection = Subsection;
        }
        else {
            TempSubsection = LastSubsectionWithProtos;
        }

        if (MiReferenceSubsection ((PMSUBSECTION)TempSubsection) == FALSE) {
            ASSERT (FALSE);
        }

        ASSERT (TempSubsection->SubsectionBase != NULL);

        LastSubsection = TempSubsection;
        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }

    //
    // End the flush at this subsection which is now referenced.
    //

    LastPte = &LastSubsection->SubsectionBase[LastPteOffset];

    //
    // Up the map view count so the control area cannot be deleted
    // out from under the call.
    //

    ControlArea->NumberOfMappedViews += 1;

    UNLOCK_PFN (OldIrql);

    CurrentThread = PsGetCurrentThread();

    //
    // Indicate that disk verify errors should be returned as exceptions.
    //

    OldClusterState = CurrentThread->ForwardClusterOnly;
    CurrentThread->ForwardClusterOnly = TRUE;

    //
    // Preacquire the file if we are going to synchronize the flush.
    //

    if (AcquireFile == 0) {

        //
        // Flush the PTEs from the specified section.
        //

        status = MiFlushSectionInternal (PointerPte,
                                         LastPte,
                                         Subsection,
                                         LastSubsection,
                                         TRUE,
                                         TRUE,
                                         IoStatus);
    }
    else {

        ConsecutiveFileLockFailures = 0;

        do {

            status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            if (!NT_SUCCESS(status)) {
                break;
            }

            //
            // Flush the PTEs from the specified section.
            //

            status = MiFlushSectionInternal (PointerPte,
                                             LastPte,
                                             Subsection,
                                             LastSubsection,
                                             TRUE,
                                             TRUE,
                                             IoStatus);

            //
            // Release the file we acquired.
            //

            FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);

            //
            // Only try the request more than once if the filesystem told us
            // it had a deadlock.
            //

            if (status != STATUS_FILE_LOCK_CONFLICT) {
                break;
            }

            ConsecutiveFileLockFailures += 1;
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

        } while (ConsecutiveFileLockFailures < 5);
    }

    CurrentThread->ForwardClusterOnly = OldClusterState;

    LOCK_PFN (OldIrql);

    MiDecrementSubsections (Subsection, Subsection);
    MiDecrementSubsections (LastSubsection, LastSubsection);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);

    return status;
}


LONGLONG
MiStartingOffset(
    IN PSUBSECTION Subsection,
    IN PMMPTE PteAddress
    )

/*++

Routine Description:

    This function calculates the file offset given a subsection and a PTE
    offset.  Note that images are stored in 512-byte units whereas data is
    stored in 4K units.

    When this is all debugged, this should be made into a macro.

Arguments:

    Subsection - Supplies a subsection to reference for the file address.

    PteAddress - Supplies a PTE within the subsection

Return Value:

    Returns the file offset to obtain the backing data from.

--*/

{
    LONGLONG PteByteOffset;
    LARGE_INTEGER StartAddress;

    if (Subsection->ControlArea->u.Flags.Image == 1) {
            return MI_STARTING_OFFSET ( Subsection,
                                        PteAddress);
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    PteByteOffset = (LONGLONG)((PteAddress - Subsection->SubsectionBase))
                            << PAGE_SHIFT;

    Mi4KStartFromSubsection (&StartAddress, Subsection);

    StartAddress.QuadPart = StartAddress.QuadPart << MM4K_SHIFT;

    PteByteOffset += StartAddress.QuadPart;

    return PteByteOffset;
}

LARGE_INTEGER
MiEndingOffset(
    IN PSUBSECTION Subsection
    )

/*++

Routine Description:

    This function calculates the last valid file offset in a given subsection.
    offset.  Note that images are stored in 512-byte units whereas data is
    stored in 4K units.

    When this is all debugged, this should be made into a macro.

Arguments:

    Subsection - Supplies a subsection to reference for the file address.

    PteAddress - Supplies a PTE within the subsection

Return Value:

    Returns the file offset to obtain the backing data from.

--*/

{
    LARGE_INTEGER FileByteOffset;

    if (Subsection->ControlArea->u.Flags.Image == 1) {
        FileByteOffset.QuadPart =
            (Subsection->StartingSector + Subsection->NumberOfFullSectors) <<
                MMSECTOR_SHIFT;
    }
    else {
        Mi4KStartFromSubsection (&FileByteOffset, Subsection);

        FileByteOffset.QuadPart += Subsection->NumberOfFullSectors;

        FileByteOffset.QuadPart = FileByteOffset.QuadPart << MM4K_SHIFT;
    }

    FileByteOffset.QuadPart += Subsection->u.SubsectionFlags.SectorEndOffset;

    return FileByteOffset;
}


NTSTATUS
MiFlushSectionInternal (
    IN PMMPTE StartingPte,
    IN PMMPTE FinalPte,
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection,
    IN ULONG Synchronize,
    IN LOGICAL WriteInProgressOk,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes to the backing file any modified pages within
    the specified range of the section.  The parameters describe the
    section's prototype PTEs (start and end) and the subsections
    which correspond to the starting and ending PTE.

    Each PTE in the subsection between the specified start and end
    is examined and if the page is either valid or transition AND
    the page has been modified, the modify bit is cleared in the PFN
    database and the page is flushed to its backing file.

Arguments:

    StartingPte - Supplies a pointer to the first prototype PTE to
                  be examined for flushing.

    FinalPte - Supplies a pointer to the last prototype PTE to be
               examined for flushing.

    FirstSubsection - Supplies the subsection that contains the
                      StartingPte.

    LastSubsection - Supplies the subsection that contains the
                     FinalPte.

    Synchronize - Supplies TRUE if synchronization with all threads
                  doing flush operations to this section should occur.

    WriteInProgressOk - Supplies TRUE if the caller can tolerate a write
                        already in progress for any dirty pages.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

Return Value:

    Returns status of the operation.

--*/

{
    LOGICAL DroppedPfnLock;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastWritten;
    PMMPTE FirstWritten;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PMDL Mdl;
    KEVENT IoEvent;
    PSUBSECTION Subsection;
    PMSUBSECTION MappedSubsection;
    PPFN_NUMBER Page;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER LastPage;
    NTSTATUS Status;
    UINT64 StartingOffset;
    UINT64 TempOffset;
    LOGICAL WriteNow;
    LOGICAL Bail;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE) + 1];
    ULONG ReflushCount;
    ULONG MaxClusterSize;
    PFILE_OBJECT FilePointer;
    LOGICAL CurrentThreadIsDereferenceThread;

    //
    // WriteInProgressOk is only FALSE when the segment dereference thread is
    // doing a top-level flush just prior to cleaning the section or subsection.
    // Note that this flag may be TRUE even for the dereference thread because
    // the dereference thread calls filesystems who may then issue a flush.
    //

    if (WriteInProgressOk == FALSE) {
        CurrentThreadIsDereferenceThread = TRUE;
        ASSERT (PsGetCurrentThread()->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread);
    }
    else {
        CurrentThreadIsDereferenceThread = FALSE;

        //
        // This may actually be the dereference thread as the segment deletion
        // dereferences the file object potentially calling the filesystem which
        // may then issue a CcFlushCache/MmFlushSection.  For our purposes,
        // lower level flushes in this context are treated as though they
        // came from a different thread.
        //
    }

    WriteNow = FALSE;
    Bail = FALSE;

    IoStatus->Status = STATUS_SUCCESS;
    IoStatus->Information = 0;
    Mdl = (PMDL)&MdlHack[0];

    KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

    FinalPte += 1;  // Point to 1 past the last one.

    FirstWritten = NULL;
    LastWritten = NULL;
    LastPage = 0;
    Subsection = FirstSubsection;
    PointerPte = StartingPte;
    ControlArea = FirstSubsection->ControlArea;
    FilePointer = ControlArea->FilePointer;

    ASSERT ((ControlArea->u.Flags.Image == 0) &&
            (FilePointer != NULL) &&
            (ControlArea->u.Flags.PhysicalMemory == 0));

    //
    // Initializing these is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    MappedSubsection = NULL;
    StartingOffset = 0;

    //
    // Try to cluster pages as long as the storage stack can handle it.
    //

    MaxClusterSize = MmModifiedWriteClusterSize;

    LOCK_PFN (OldIrql);

    ASSERT (ControlArea->u.Flags.Image == 0);

    if (ControlArea->NumberOfPfnReferences == 0) {

        //
        // No transition or valid prototype PTEs present, hence
        // no need to flush anything.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    while ((Synchronize) && (ControlArea->FlushInProgressCount != 0)) {

        //
        // Another thread is currently performing a flush operation on
        // this file.  Wait for that flush to complete.
        //

        ControlArea->u.Flags.CollidedFlush = 1;

        //
        // Keep APCs blocked so no special APCs can be delivered in KeWait
        // which would cause the dispatcher lock to be released opening a
        // window where this thread could miss a pulse.
        //

        UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

        KeWaitForSingleObject (&MmCollidedFlushEvent,
                               WrPageOut,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)&MmOneSecond);
        KeLowerIrql (OldIrql);
        LOCK_PFN (OldIrql);
    }

    ControlArea->FlushInProgressCount += 1;

    //
    // Clear the deferred entry list as pages from it may get marked modified
    // during the processing.  Note that any transition page which is currently
    // clean but has a nonzero reference count may get marked modified if
    // there is a pending transaction and note well that this transaction may
    // complete at any time !  Thus, this case must be carefully handled.
    //

#if !defined(MI_MULTINODE)
    if (MmPfnDeferredList != NULL) {
        MiDeferredUnlockPages (MI_DEFER_PFN_HELD);
    }
#else
    //
    // Each and every node's deferred list would have to be checked so
    // we might as well go the long way and just call.
    //

    MiDeferredUnlockPages (MI_DEFER_PFN_HELD);
#endif

    for (;;) {

        if (LastSubsection != Subsection) {

            //
            // Flush to the last PTE in this subsection.
            //

            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }
        else {

            //
            // Flush to the end of the range.
            //

            LastPte = FinalPte;
        }

        if (Subsection->SubsectionBase == NULL) {

            //
            // The prototype PTEs for this subsection have either never been
            // created or have been tossed due to memory pressure.  Either
            // way, this range can be skipped as there are obviously no
            // dirty pages in it.  If there are other dirty pages
            // to be written, write them now as we are skipping over PTEs.
            //

            if (LastWritten != NULL) {
                ASSERT (MappedSubsection != NULL);
                WriteNow = TRUE;
                goto CheckForWrite;
            }
            if (LastSubsection == Subsection) {
                break;
            }
            Subsection = Subsection->NextSubsection;
            PointerPte = Subsection->SubsectionBase;
            continue;
        }

        //
        // Up the number of mapped views to prevent other threads
        // from freeing this to the unused subsection list while we're
        // operating on it.
        //

        MappedSubsection = (PMSUBSECTION) Subsection;
        MappedSubsection->NumberOfMappedViews += 1;

        if (MappedSubsection->DereferenceList.Flink != NULL) {

            //
            // Remove this from the list of unused subsections.
            //

            RemoveEntryList (&MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            MappedSubsection->DereferenceList.Flink = NULL;
        }

        if (CurrentThreadIsDereferenceThread == FALSE) {

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
        }

        //
        // If the prototype PTEs are paged out or have a share count
        // of 1, they cannot contain any transition or valid PTEs.
        //

        if (!MiCheckProtoPtePageState(PointerPte, TRUE, &DroppedPfnLock)) {
            PointerPte = (PMMPTE)(((ULONG_PTR)PointerPte | (PAGE_SIZE - 1)) + 1);
        }

        while (PointerPte < LastPte) {

            if (MiIsPteOnPdeBoundary(PointerPte)) {

                //
                // We are on a page boundary, make sure this PTE is resident.
                //

                if (!MiCheckProtoPtePageState(PointerPte, TRUE, &DroppedPfnLock)) {
                    PointerPte = (PMMPTE)((PCHAR)PointerPte + PAGE_SIZE);

                    //
                    // If there are dirty pages to be written, write them
                    // now as we are skipping over PTEs.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                        goto CheckForWrite;
                    }
                    continue;
                }
            }

            PteContents = *PointerPte;

            if ((PteContents.u.Hard.Valid == 1) ||
                   ((PteContents.u.Soft.Prototype == 0) &&
                     (PteContents.u.Soft.Transition == 1))) {

                //
                // Prototype PTE in transition, there are 3 possible cases:
                //  1. The page is part of an image which is sharable and
                //     refers to the paging file - dereference page file
                //     space and free the physical page.
                //  2. The page refers to the segment but is not modified -
                //     free the physical page.
                //  3. The page refers to the segment and is modified -
                //     write the page to the file and free the physical page.
                //

                if (PteContents.u.Hard.Valid == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                }
                else {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                }

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);
                ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                //
                // Note that any transition page which is currently clean but
                // has a nonzero reference count may get marked modified if
                // there is a pending transaction and note well that this
                // transaction may complete at any time !  Thus, this case
                // must be carefully handled since the segment dereference
                // thread must be given a collision error for this one as it
                // requires that no pages be dirtied after a successful return.
                //

                if ((CurrentThreadIsDereferenceThread == TRUE) &&
                    (Pfn1->u3.e2.ReferenceCount != 0)) {

#if DBG
                    if ((PteContents.u.Hard.Valid != 0) &&
                        (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 0) &&
                        (ControlArea->u.Flags.Accessed == 0)) {

                        if (!KdDebuggerNotPresent) {
                            DbgPrint ("MM: flushing valid proto, %p %p\n",
                                            Pfn1, PointerPte);
                            DbgBreakPoint ();
                        }
                    }
#endif

                    PointerPte = LastPte;
                    Bail = TRUE;

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                    goto CheckForWrite;
                }

                //
                // If the page is modified OR a write is in progress
                // flush it.  The write in progress case catches problems
                // where the modified page write continually writes a
                // page and gets errors writing it, by writing pages
                // in this state, the error will be propagated back to
                // the caller.
                //

                if ((Pfn1->u3.e1.Modified == 1) ||
                    (Pfn1->u3.e1.WriteInProgress)) {

                    if ((WriteInProgressOk == FALSE) &&
                        (Pfn1->u3.e1.WriteInProgress)) {

                            PointerPte = LastPte;
                            Bail = TRUE;

                            if (LastWritten != NULL) {
                                WriteNow = TRUE;
                            }
                            goto CheckForWrite;
                    }

                    if (LastWritten == NULL) {

                        //
                        // This is the first page of a cluster, initialize
                        // the MDL, etc.
                        //

                        LastPage = (PPFN_NUMBER)(Mdl + 1);

                        //
                        // Calculate the offset to read into the file.
                        //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
                        //

                        StartingOffset = (UINT64) MiStartingOffset (
                                                             Subsection,
                                                             Pfn1->PteAddress);

                        MI_INITIALIZE_ZERO_MDL (Mdl);

                        Mdl->MdlFlags |= MDL_PAGES_LOCKED;
                        Mdl->StartVa =
                                  (PVOID)ULongToPtr(Pfn1->u3.e1.PageColor << PAGE_SHIFT);
                        Mdl->Size = (CSHORT)(sizeof(MDL) +
                                   (sizeof(PFN_NUMBER) * MaxClusterSize));
                        FirstWritten = PointerPte;
                    }

                    LastWritten = PointerPte;
                    Mdl->ByteCount += PAGE_SIZE;
                    if (Mdl->ByteCount == (PAGE_SIZE * MaxClusterSize)) {
                        WriteNow = TRUE;
                    }

                    if (PteContents.u.Hard.Valid == 0) {

                        //
                        // The page is in transition.
                        //

                        MiUnlinkPageFromList (Pfn1);
                        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE(Pfn1, 18);
                    }
                    else {
                        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 20);
                    }

                    //
                    // Clear the modified bit for this page.
                    //

                    MI_SET_MODIFIED (Pfn1, 0, 0x22);

                    //
                    // Up the reference count for the physical page as there
                    // is I/O in progress.
                    //

                    Pfn1->u3.e2.ReferenceCount += 1;

                    *LastPage = PageFrameIndex;
                    LastPage += 1;
                }
                else {

                    //
                    // This page was not modified and therefore ends the
                    // current write cluster if any.  Set WriteNow to TRUE
                    // if there is a cluster being built.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                }
            }
            else {

                //
                // This page was not modified and therefore ends the
                // current write cluster if any.  Set WriteNow to TRUE
                // if there is a cluster being built.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }

            PointerPte += 1;

CheckForWrite:

            //
            // Write the current cluster if it is complete,
            // full, or the loop is now complete.
            //

            if ((WriteNow) ||
                ((PointerPte == LastPte) && (LastWritten != NULL))) {

                LARGE_INTEGER EndOfFile;

                //
                // Issue the write request.
                //

                UNLOCK_PFN (OldIrql);

                WriteNow = FALSE;

                //
                // Make sure the write does not go past the
                // end of file. (segment size).
                //

                EndOfFile = MiEndingOffset(Subsection);
                TempOffset = (UINT64) EndOfFile.QuadPart;

                if (StartingOffset + Mdl->ByteCount > TempOffset) {

                    ASSERT ((ULONG_PTR)(TempOffset - StartingOffset) >
                             (Mdl->ByteCount - PAGE_SIZE));

                    Mdl->ByteCount = (ULONG)(TempOffset- StartingOffset);
                }

                ReflushCount = 0;
                
                while (TRUE) {

                    KeClearEvent (&IoEvent);

                    Status = IoSynchronousPageWrite (FilePointer,
                                                     Mdl,
                                                     (PLARGE_INTEGER)&StartingOffset,
                                                     &IoEvent,
                                                     IoStatus);

                    if (NT_SUCCESS(Status)) {

                        //
                        // Success was returned, so wait for the i/o event.
                        //

                        KeWaitForSingleObject (&IoEvent,
                                               WrPageOut,
                                               KernelMode,
                                               FALSE,
                                               NULL);
                    }
                    else {

                        //
                        // Copy the error to the IoStatus, for error
                        // handling below.
                        //
    
                        IoStatus->Status = Status;
                    }

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (MmIsRetryIoStatus(IoStatus->Status)) {
                        
                        ReflushCount -= 1;
                        if (ReflushCount & MiIoRetryMask) {
                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
                            continue;
                        }
                    }

                    break;
                }

                Page = (PPFN_NUMBER)(Mdl + 1);

                LOCK_PFN (OldIrql);

                if (MiIsPteOnPdeBoundary(PointerPte) == 0) {

                    //
                    // The next PTE is not in a different page, make
                    // sure the PTE for the prototype PTE page was not
                    // put in transition while the I/O was in progress.
                    // Note the prototype PTE page itself cannot be reused
                    // as each outstanding page has a sharecount on it - but
                    // the PTE mapping it can be put in transition regardless
                    // of sharecount because it is a system page.
                    //

                    MiMakeSystemAddressValidPfn (PointerPte);
                }

                if (NT_SUCCESS(IoStatus->Status)) {

                    //
                    // The I/O completed successfully, unlock the pages.
                    //

                    while (Page < LastPage) {

                        Pfn2 = MI_PFN_ELEMENT (*Page);
                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn2, 19);
                        Page += 1;
                    }
                }
                else {

                    //
                    // Don't count on the file system to convey
                    // anything in the information field on errors.
                    //

                    IoStatus->Information = 0;

                    //
                    // The I/O completed unsuccessfully, unlock the pages
                    // and return an error status.
                    //

                    while (Page < LastPage) {

                        Pfn2 = MI_PFN_ELEMENT (*Page);

                        //
                        // Mark the page dirty again so it can be rewritten.
                        //

                        MI_SET_MODIFIED (Pfn2, 1, 0x1);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn2, 21);

                        Page += 1;
                    }

                    if ((MmIsRetryIoStatus(IoStatus->Status)) &&
                        (MaxClusterSize != 1) &&
                        (Mdl->ByteCount > PAGE_SIZE)) {

                        //
                        // Retries of a cluster have failed, reissue
                        // the cluster one page at a time as the
                        // storage stack should always be able to
                        // make forward progress this way.
                        //

                        ASSERT (FirstWritten != NULL);
                        ASSERT (LastWritten != NULL);
                        ASSERT (FirstWritten != LastWritten);

                        PointerPte = FirstWritten;
                        MiMakeSystemAddressValidPfn (PointerPte);
                        MaxClusterSize = 1;
                    }
                    else {
    
                        //
                        // Calculate how much was written thus far
                        // and add that to the information field
                        // of the IOSB.
                        //
    
                        IoStatus->Information +=
                            (((LastWritten - StartingPte) << PAGE_SHIFT) -
                                                            Mdl->ByteCount);
                        LastWritten = NULL;
    
                        //
                        // Set this to force termination of the outermost loop.
                        //
    
                        Subsection = LastSubsection;
                        break;
                    }

                } // end if error on i/o

                //
                // As the PFN lock has been released and
                // reacquired, do this loop again as the
                // PTE may have changed state.
                //

                LastWritten = NULL;
            } // end if chunk to write

        } //end while

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);
        ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if ((Bail == TRUE) || (Subsection == LastSubsection)) {

            //
            // The last range has been flushed or we have collided with the
            // mapped page writer.  Regardless, exit the top FOR loop
            // and return.
            //

            break;
        }

        Subsection = Subsection->NextSubsection;
        PointerPte = Subsection->SubsectionBase;

    }  //end for

    ASSERT (LastWritten == NULL);

    ControlArea->FlushInProgressCount -= 1;
    if ((ControlArea->u.Flags.CollidedFlush == 1) &&
        (ControlArea->FlushInProgressCount == 0)) {
        ControlArea->u.Flags.CollidedFlush = 0;
        KePulseEvent (&MmCollidedFlushEvent, 0, FALSE);
    }
    UNLOCK_PFN (OldIrql);

    if (Bail == TRUE) {

        //
        // This routine collided with the mapped page writer and the caller
        // expects an error for this.  Give it to him.
        //

        return STATUS_MAPPED_WRITER_COLLISION;
    }

    return IoStatus->Status;
}

BOOLEAN
MmPurgeSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer,
    IN PLARGE_INTEGER Offset,
    IN SIZE_T RegionSize,
    IN ULONG IgnoreCacheViews
    )

/*++

Routine Description:

    This function determines if any views of the specified section
    are mapped, and if not, purges valid pages (even modified ones)
    from the specified section and returns any used pages to the free
    list.  This is accomplished by examining the prototype PTEs
    from the specified offset to the end of the section, and if
    any prototype PTEs are in the transition state, putting the
    prototype PTE back into its original state and putting the
    physical page on the free list.

    NOTE:

    If there is an I/O operation ongoing for one of the pages,
    that page is eliminated from the segment and allowed to "float"
    until the i/o is complete.  Once the share count goes to zero
    the page will be added to the free page list.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects.

    Offset - Supplies the offset into the section in which to begin
             purging pages.  If this argument is not present, then the
             whole section is purged without regard to the region size
             argument.


    RegionSize - Supplies the size of the region to purge.  If this
                 is specified as zero and Offset is specified, the
                 region from Offset to the end of the file is purged.

                 Note: The largest value acceptable for RegionSize is
                 0xFFFF0000;

    IgnoreCacheViews - Supplies FALSE if mapped views in the system
                 cache should cause the function to return FALSE.
                 This is the normal case.
                 Supplies TRUE if mapped views should be ignored
                 and the flush should occur.  NOTE THAT IF TRUE
                 IS SPECIFIED AND ANY DATA PURGED IS CURRENTLY MAPPED
                 AND VALID A BUGCHECK WILL OCCUR!!

Return Value:

    Returns TRUE if either no section exists for the file object or
    the section is not mapped and the purge was done, FALSE otherwise.

    Note that FALSE is returned if during the purge operation, a page
    could not be purged due to a non-zero reference count.

--*/

{
    LOGICAL DroppedPfnLock;
    PCONTROL_AREA ControlArea;
    PMAPPED_FILE_SEGMENT Segment;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE FinalPte;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    ULONG PteOffset;
    ULONG LastPteOffset;
    PMSUBSECTION MappedSubsection;
    PSUBSECTION Subsection;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION TempSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    LARGE_INTEGER LocalOffset;
    LOGICAL LockHeld;
    BOOLEAN ReturnValue;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
#if DBG
    PFN_NUMBER LastLocked = 0;
#endif

    //
    // This is needed in case a page is on the mapped page writer list -
    // the PFN lock will need to be released and APCs disabled.
    //

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    //
    //  Capture caller's file size, since we may modify it.
    //

    if (ARGUMENT_PRESENT(Offset)) {

        LocalOffset = *Offset;
        Offset = &LocalOffset;
    }

    //
    //  See if we can truncate this file to where the caller wants
    //  us to.
    //

    if (!MiCanFileBeTruncatedInternal(SectionObjectPointer, Offset, TRUE, &OldIrql)) {
        return FALSE;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    ControlArea = (PCONTROL_AREA)(SectionObjectPointer->DataSectionObject);
    if ((ControlArea == NULL) || (ControlArea->u.Flags.Rom)) {
        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    //
    //  Even though MiCanFileBeTruncatedInternal returned TRUE, there could
    //  still be a system cache mapped view.  We cannot truncate if
    //  the Cache Manager has a view mapped.
    //

    if ((IgnoreCacheViews == FALSE) &&
        (ControlArea->NumberOfSystemCacheViews != 0)) {

        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

#if 0

    //
    // Prevent races when the control area is being deleted as the clean
    // path releases the PFN lock midway through.  File objects may still have
    // section object pointers and data section objects that point at this
    // control area, hence the purge can be issued.
    //
    // Check for this and fail the purge as the control area (and the section
    // object pointers/data section objects) will be going away momentarily.
    // Note that even though drivers have these data section objects, no one
    // currently has an open section for this control area and no one is
    // allowed to open one until the clean path finishes.
    //

    if (ControlArea->u.Flags.BeingDeleted == 1) {
        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

#else

    //
    // The above check can be removed as MiCanFileBeTruncatedInternal does
    // the same check, so just assert it below.
    //

    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);

#endif

    //
    // Purge the section - locate the subsection which
    // contains the PTEs.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

    if (!ARGUMENT_PRESENT (Offset)) {

        //
        // If the offset is not specified, flush the complete file ignoring
        // the region size.
        //

        PteOffset = 0;
        RegionSize = 0;

    }
    else {

        PteOffset = (ULONG)(Offset->QuadPart >> PAGE_SHIFT);

        //
        // Make sure the PTEs are not in the extended part of the segment.
        //

        while (PteOffset >= Subsection->PtesInSubsection) {
            PteOffset -= Subsection->PtesInSubsection;
            Subsection = Subsection->NextSubsection;
            if (Subsection == NULL) {

                //
                // The offset must be equal to the size of
                // the section, don't purge anything just return.
                //

                UNLOCK_PFN (OldIrql);
                return TRUE;
            }
        }

        ASSERT (PteOffset < Subsection->PtesInSubsection);
    }

    //
    // Locate the address of the last prototype PTE to be flushed.
    //

    if (RegionSize == 0) {

        //
        // Flush to end of section.
        //

        LastSubsection = Subsection;

        Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

        if (MmIsAddressValid (Segment)) {
            if (Segment->LastSubsectionHint != NULL) {
                LastSubsection = (PSUBSECTION) Segment->LastSubsectionHint;
            }
        }

        while (LastSubsection->NextSubsection != NULL) {
            LastSubsection = LastSubsection->NextSubsection;
        }

        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }
    else {

        //
        // Calculate the end of the region.
        //

        LastPteOffset = PteOffset +
            (ULONG) (((RegionSize + BYTE_OFFSET(Offset->LowPart)) - 1) >> PAGE_SHIFT);

        LastSubsection = Subsection;

        while (LastPteOffset >= LastSubsection->PtesInSubsection) {
            LastPteOffset -= LastSubsection->PtesInSubsection;
            if (LastSubsection->NextSubsection == NULL) {
                LastPteOffset = LastSubsection->PtesInSubsection - 1;
                break;
            }
            LastSubsection = LastSubsection->NextSubsection;
        }

        ASSERT (LastPteOffset < LastSubsection->PtesInSubsection);
    }

    //
    // Try for the fast reference on the first and last subsection.
    // If that cannot be gotten, then there are no prototype PTEs for this
    // subsection, therefore there is nothing in it to flush so leap forwards.
    //
    // Note that subsections in between do not need referencing as
    // the purge is smart enough to skip them if they're nonresident.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
        do {
            //
            // If this increment would put us past the end offset, then nothing
            // to flush, just return success.
            //

            if (Subsection == LastSubsection) {
                UNLOCK_PFN (OldIrql);
                return TRUE;
            }
            Subsection = Subsection->NextSubsection;

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            if (Subsection == NULL) {
                UNLOCK_PFN (OldIrql);
                return TRUE;
            }

            if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
                continue;
            }

            //
            // Start the flush at this subsection which is now referenced.
            //

            PointerPte = &Subsection->SubsectionBase[0];
            break;

        } while (TRUE);
    }
    else {
        PointerPte = &Subsection->SubsectionBase[PteOffset];
    }

    FirstSubsection = Subsection;
    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // The first subsection is referenced, now reference count the last one.
    // If the first is the last, just double reference it anyway as it
    // simplifies cleanup later.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)LastSubsection) == FALSE) {

        ASSERT (Subsection != LastSubsection);

        TempSubsection = Subsection->NextSubsection;
        LastSubsectionWithProtos = NULL;

        while (TempSubsection != LastSubsection) {

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            ASSERT (TempSubsection != NULL);

            if ((PMSUBSECTION)TempSubsection->SubsectionBase != NULL) {
                LastSubsectionWithProtos = TempSubsection;
            }

            TempSubsection = TempSubsection->NextSubsection;
        }

        //
        // End the flush at this subsection and reference it.
        //

        if (LastSubsectionWithProtos == NULL) {
            ASSERT (Subsection != NULL);
            ASSERT (Subsection->SubsectionBase != NULL);
            TempSubsection = Subsection;
        }
        else {
            TempSubsection = LastSubsectionWithProtos;
        }

        if (MiReferenceSubsection ((PMSUBSECTION)TempSubsection) == FALSE) {
            ASSERT (FALSE);
        }

        ASSERT (TempSubsection->SubsectionBase != NULL);

        LastSubsection = TempSubsection;
        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }

    //
    // End the flush at this subsection which is now referenced.
    //
    // Point final PTE to 1 beyond the end.
    //

    FinalPte = &LastSubsection->SubsectionBase[LastPteOffset + 1];

    //
    // Increment the number of mapped views to
    // prevent the section from being deleted while the purge is
    // in progress.
    //

    ControlArea->NumberOfMappedViews += 1;

    //
    // Set being purged so no one can map a view
    // while the purge is going on.
    //

    ControlArea->u.Flags.BeingPurged = 1;
    ControlArea->u.Flags.WasPurged = 1;

    LockHeld = TRUE;
    ReturnValue = TRUE;

    for (;;) {

        if (!LockHeld) {
            LockHeld = TRUE;
            LOCK_PFN (OldIrql);
        }

        if (LastSubsection != Subsection) {

            //
            // Flush to the last PTE in this subsection.
            //

            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }
        else {

            //
            // Flush to the end of the range.
            //

            LastPte = FinalPte;
        }

        if (Subsection->SubsectionBase == NULL) {

            //
            // The prototype PTEs for this subsection have either never been
            // created or have been tossed due to memory pressure.  Either
            // way, this range can be skipped as there are obviously no
            // pages to purge in this range.
            //

            ASSERT (LockHeld);
            UNLOCK_PFN (OldIrql);
            LockHeld = FALSE;
            goto nextrange;
        }

        //
        // Up the number of mapped views to prevent other threads
        // from freeing this to the unused subsection list while we're
        // operating on it.
        //

        MappedSubsection = (PMSUBSECTION) Subsection;
        MappedSubsection->NumberOfMappedViews += 1;

        if (MappedSubsection->DereferenceList.Flink != NULL) {

            //
            // Remove this from the list of unused subsections.
            //

            RemoveEntryList (&MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            MappedSubsection->DereferenceList.Flink = NULL;
        }

        //
        // Set the access bit so an already ongoing trim won't blindly
        // delete the prototype PTEs on completion of a mapped write.
        // This can happen if the current thread dirties some pages and
        // then deletes the view before the trim write finishes - this
        // bit informs the trimming thread that a rescan is needed so
        // that writes are not lost.
        //

        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

        //
        // If the page table page containing the PTEs is not
        // resident, then no PTEs can be in the valid or transition
        // state!  Skip over the PTEs.
        //

        if (!MiCheckProtoPtePageState(PointerPte, LockHeld, &DroppedPfnLock)) {
            PointerPte = (PMMPTE)(((ULONG_PTR)PointerPte | (PAGE_SIZE - 1)) + 1);
        }

        while (PointerPte < LastPte) {

            //
            // If the page table page containing the PTEs is not
            // resident, then no PTEs can be in the valid or transition
            // state!  Skip over the PTEs.
            //

            if (MiIsPteOnPdeBoundary(PointerPte)) {
                if (!MiCheckProtoPtePageState(PointerPte, LockHeld, &DroppedPfnLock)) {
                    PointerPte = (PMMPTE)((PCHAR)PointerPte + PAGE_SIZE);
                    continue;
                }
            }

            PteContents = *PointerPte;

            if (PteContents.u.Hard.Valid == 1) {

                //
                // A valid PTE was found, it must be mapped in the
                // system cache.  Just exit the loop and return FALSE
                // and let the caller fix this.
                //

                ReturnValue = FALSE;
                break;
            }

            if ((PteContents.u.Soft.Prototype == 0) &&
                     (PteContents.u.Soft.Transition == 1)) {

                if (!LockHeld) {
                    LockHeld = TRUE;
                    LOCK_PFN (OldIrql);
                    MiMakeSystemAddressValidPfn (PointerPte);
                    continue;
                }

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                if ((Pfn1->OriginalPte.u.Soft.Prototype != 1) ||
                    (Pfn1->OriginalPte.u.Hard.Valid != 0) ||
                    (Pfn1->PteAddress != PointerPte)) {

                    //
                    // The pool containing the prototype PTEs has been
                    // corrupted.  Pool corruption like this is fatal.
                    //

                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x2,
                                  (ULONG_PTR)PointerPte,
                                  (ULONG_PTR)Pfn1->PteAddress,
                                  (ULONG_PTR)PteContents.u.Long);
                }

#if DBG
                if ((Pfn1->u3.e2.ReferenceCount != 0) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {

                    //
                    // There must be an I/O in progress on this page.
                    //

                    if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents) != LastLocked) {
                        UNLOCK_PFN (OldIrql);

                        LastLocked = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                        LOCK_PFN (OldIrql);
                        MiMakeSystemAddressValidPfn (PointerPte);
                        continue;
                    }
                }
#endif //DBG

                //
                // If the modified page writer has page locked for I/O
                // wait for the I/O's to be completed and the pages
                // to be unlocked.  The eliminates a race condition
                // when the modified page writer locks the pages, then
                // a purge occurs and completes before the mapped
                // writer thread runs.
                //

                if (Pfn1->u3.e1.WriteInProgress == 1) {

                    //
                    // A 3 or more thread deadlock can occur where:
                    //
                    // 1.  The mapped page writer thread has issued a write
                    //     and is in the filesystem code waiting for a resource.
                    //
                    // 2.  Thread 2 owns the resource above but is waiting for
                    //     the filesystem's quota mutex.
                    //
                    // 3.  Thread 3 owns the quota mutex and is right here
                    //     doing a purge from the cache manager when he notices
                    //     the page to be purged is either already being written
                    //     or is in the mapped page writer list.  If it is
                    //     already being written everything will unjam.  If it
                    //     is still on the mapped page writer list awaiting
                    //     processing, then it must be cancelled - otherwise
                    //     if this thread were to wait, deadlock can occur.
                    //
                    // The alternative to all this is for the filesystems to
                    // always release the quota mutex before purging but the
                    // filesystem overhead to do this is substantial.
                    //

                    if (MiCancelWriteOfMappedPfn (PageFrameIndex) == TRUE) {

                        //
                        // Stopping any failed writes (even deliberately
                        // cancelled ones) automatically cause a delay.  A
                        // successful stop also results in the PFN lock
                        // being released and reacquired.  So loop back to
                        // the top now as the world may have changed.
                        //

                        MiMakeSystemAddressValidPfn (PointerPte);
                        continue;
                    }

                    ASSERT (ControlArea->ModifiedWriteCount != 0);
                    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

                    ControlArea->u.Flags.SetMappedFileIoComplete = 1;

                    //
                    // Keep APCs blocked so no special APCs can be delivered
                    // in KeWait which would cause the dispatcher lock to be
                    // released opening a window where this thread could miss
                    // a pulse.
                    //

                    UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

                    KeWaitForSingleObject (&MmMappedFileIoComplete,
                                           WrPageOut,
                                           KernelMode,
                                           FALSE,
                                           NULL);
                    KeLowerIrql (OldIrql);
                    LOCK_PFN (OldIrql);
                    MiMakeSystemAddressValidPfn (PointerPte);
                    continue;
                }

                if (Pfn1->u3.e1.ReadInProgress == 1) {

                    //
                    // The page currently is being read in from the
                    // disk.  Treat this just like a valid PTE and
                    // return false.
                    //

                    ReturnValue = FALSE;
                    break;
                }

                ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                    (Pfn1->OriginalPte.u.Soft.Transition == 1)));

                MI_WRITE_INVALID_PTE (PointerPte, Pfn1->OriginalPte);

                ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                ControlArea->NumberOfPfnReferences -= 1;
                ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                MiUnlinkPageFromList (Pfn1);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // If the reference count for the page is zero, insert
                // it into the free page list, otherwise leave it alone
                // and when the reference count is decremented to zero
                // the page will go to the free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                }
            }
            PointerPte += 1;

            if ((MiIsPteOnPdeBoundary(PointerPte)) && (LockHeld)) {

                //
                // Unlock PFN so large requests will not block other
                // threads on MP systems.
                //

                UNLOCK_PFN (OldIrql);
                LockHeld = FALSE;
            }
        }

        if (!LockHeld) {
            LockHeld = TRUE;
            LOCK_PFN (OldIrql);
        }

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);
        ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        ASSERT (LockHeld);

        UNLOCK_PFN (OldIrql);
        LockHeld = FALSE;

nextrange:

        if ((LastSubsection != Subsection) && (ReturnValue)) {

            //
            // Get the next subsection in the list.
            //

            Subsection = Subsection->NextSubsection;
            PointerPte = Subsection->SubsectionBase;

        }
        else {

            //
            // The last range has been flushed, exit the top FOR loop
            // and return.
            //

            break;
        }
    }

    if (!LockHeld) {
        LOCK_PFN (OldIrql);
    }

    MiDecrementSubsections (FirstSubsection, FirstSubsection);
    MiDecrementSubsections (LastSubsection, LastSubsection);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    ControlArea->u.Flags.BeingPurged = 0;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
    return ReturnValue;
}

BOOLEAN
MmFlushImageSection (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN MMFLUSH_TYPE FlushType
    )

/*++

Routine Description:

    This function determines if any views of the specified image section
    are mapped, and if not, flushes valid pages (even modified ones)
    from the specified section and returns any used pages to the free
    list.  This is accomplished by examining the prototype PTEs
    from the specified offset to the end of the section, and if
    any prototype PTEs are in the transition state, putting the
    prototype PTE back into its original state and putting the
    physical page on the free list.

Arguments:

    SectionPointer - Supplies a pointer to a section object pointers
                     within the FCB.

    FlushType - Supplies the type of flush to check for.  One of
                MmFlushForDelete or MmFlushForWrite.

Return Value:

    Returns TRUE if either no section exists for the file object or
    the section is not mapped and the purge was done, FALSE otherwise.

--*/

{
    PLIST_ENTRY Next;
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    KIRQL OldIrql;
    LOGICAL state;

    if (FlushType == MmFlushForDelete) {

        //
        // Do a quick check to see if there are any mapped views for
        // the data section.  If so, just return FALSE.
        //

        LOCK_PFN (OldIrql);
        ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);
        if (ControlArea != NULL) {
            if ((ControlArea->NumberOfUserReferences != 0) ||
                (ControlArea->u.Flags.BeingCreated)) {
                UNLOCK_PFN (OldIrql);
                return FALSE;
            }
        }
        UNLOCK_PFN (OldIrql);
    }

    //
    // Check the status of the control area.  If the control area is in use
    // or the control area is being deleted, this operation cannot continue.
    //

    state = MiCheckControlAreaStatus (CheckImageSection,
                                      SectionPointer,
                                      FALSE,
                                      &ControlArea,
                                      &OldIrql);

    if (ControlArea == NULL) {
        return (BOOLEAN) state;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    //
    // Repeat until there are no more control areas - multiple control areas
    // for the same image section occur to support user global DLLs - these DLLs
    // require data that is shared within a session but not across sessions.
    // Note this can only happen for Hydra.
    //

    do {

        //
        // Set the being deleted flag and up the number of mapped views
        // for the segment.  Upping the number of mapped views prevents
        // the segment from being deleted and passed to the deletion thread
        // while we are forcing a delete.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ControlArea->NumberOfMappedViews = 1;
        LargeControlArea = NULL;

        if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
            NOTHING;
        }
        else if (IsListEmpty(&((PLARGE_CONTROL_AREA)ControlArea)->UserGlobalList)) {
            ASSERT (ControlArea ==
                    (PCONTROL_AREA)SectionPointer->ImageSectionObject);
        }
        else {

            //
            // Check if there's only one image section in this control area, so
            // we don't reference the section object pointers as the
            // MiCleanSection call may result in its deletion.
            //

            //
            // There are multiple control areas, bump the reference count
            // on one of them (doesn't matter which one) so that it can't
            // go away.  This ensures the section object pointers will stick
            // around even after the calls below so we can safely reloop to
            // flush any other remaining control areas.
            //

            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 1);

            Next = ((PLARGE_CONTROL_AREA)ControlArea)->UserGlobalList.Flink;

            LargeControlArea = CONTAINING_RECORD (Next,
                                                  LARGE_CONTROL_AREA,
                                                  UserGlobalList);
        
            ASSERT (LargeControlArea->u.Flags.GlobalOnlyPerSession == 1);

            LargeControlArea->NumberOfSectionReferences += 1;
        }

        //
        // This is a page file backed or image segment.  The segment is being
        // deleted, remove all references to the paging file and physical
        // memory.
        //

        UNLOCK_PFN (OldIrql);

        MiCleanSection (ControlArea, TRUE);

        //
        // Get the next Hydra control area.
        //

        if (LargeControlArea != NULL) {
            state = MiCheckControlAreaStatus (CheckImageSection,
                                              SectionPointer,
                                              FALSE,
                                              &ControlArea,
                                              &OldIrql);
            if (!ControlArea) {
                LOCK_PFN (OldIrql);
                LargeControlArea->NumberOfSectionReferences -= 1;
                MiCheckControlArea ((PCONTROL_AREA)LargeControlArea,
                                    NULL,
                                    OldIrql);
            }
            else {
                LargeControlArea->NumberOfSectionReferences -= 1;
                MiCheckControlArea ((PCONTROL_AREA)LargeControlArea,
                                    NULL,
                                    OldIrql);
                LOCK_PFN (OldIrql);
            }
        }
        else {
            state = TRUE;
            break;
        }

    } while (ControlArea);

    return (BOOLEAN) state;
}

VOID
MiFlushDirtyBitsToPfn (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN PEPROCESS Process,
    IN BOOLEAN SystemCache
    )

{
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PVOID Va;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG Waited;

    Va = MiGetVirtualAddressMappedByPte (PointerPte);
    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {

        PteContents = *PointerPte;

        if ((PteContents.u.Hard.Valid == 1) &&
            (MI_IS_PTE_DIRTY (PteContents))) {

            //
            // Flush the modify bit to the PFN database.
            //

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            MI_SET_MODIFIED (Pfn1, 1, 0x2);

            MI_SET_PTE_CLEAN (PteContents);

            //
            // No need to capture the PTE contents as we are going to
            // write the page anyway and the Modify bit will be cleared
            // before the write is done.
            //

            (VOID)KeFlushSingleTb (Va,
                                   FALSE,
                                   SystemCache,
                                   (PHARDWARE_PTE)PointerPte,
                                   PteContents.u.Flush);
        }

        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
        PointerPte += 1;

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPteAddress (PointerPte);

            while (PointerPte <= LastPte) {

                PointerPxe = MiGetPdeAddress (PointerPde);
                PointerPpe = MiGetPteAddress (PointerPde);

                if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                 Process,
                                                 TRUE,
                                                 &Waited)) {

                    //
                    // No page directory parent page exists for this address.
                    //

                    PointerPxe += 1;
                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                }
                else if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                 Process,
                                                 TRUE,
                                                 &Waited)) {

                    //
                    // No page directory page exists for this address.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                }
                else {

                    Waited = 0;

                    if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                     Process,
                                                     TRUE,
                                                     &Waited)) {

                        //
                        // No page table page exists for this address.
                        //

                        PointerPde += 1;

                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    }
                    else {

                        //
                        // If the PFN lock (and accordingly the WS mutex) was
                        // released and reacquired we must retry the operation.
                        //

                        if (Waited != 0) {
                            continue;
                        }

                        //
                        // The PFN lock has been held since we acquired the
                        // page directory parent, ie: this PTE we can operate on
                        // immediately.
                        //

                        break;
                    }
                }
            }

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
        }
    }

    UNLOCK_PFN (OldIrql);
    return;
}

PSUBSECTION
MiGetSystemCacheSubsection (
    IN PVOID BaseAddress,
    OUT PMMPTE *ProtoPte
    )

{
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PSUBSECTION Subsection;

    PointerPte = MiGetPteAddress (BaseAddress);

    LOCK_PFN (OldIrql);

    Subsection = MiGetSubsectionAndProtoFromPte (PointerPte, ProtoPte);
    UNLOCK_PFN (OldIrql);
    return Subsection;
}


LOGICAL
MiCheckProtoPtePageState (
    IN PMMPTE PrototypePte,
    IN LOGICAL PfnLockHeld,
    OUT PLOGICAL DroppedPfnLock
    )

/*++

Routine Description:

    Checks the state of the page containing the specified
    prototype PTE.

    If the page is valid or transition and has transition or valid prototype
    PTEs contained with it, TRUE is returned and the page is made valid
    (if transition).  Otherwise return FALSE indicating no prototype
    PTEs within this page are of interest.

Arguments:

    PrototypePte - Supplies a pointer to a prototype PTE within the page.

    DroppedPfnLock - Supplies a pointer to a logical this routine sets to
                     TRUE if the PFN lock is released & reacquired.

Return Value:

    TRUE if the page containing the proto PTE was made resident.
    FALSE if otherwise.

--*/

{
    PMMPTE PointerPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;

    *DroppedPfnLock = FALSE;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // First check whether the page directory page is present.  Since there
    // is no lazy loading of PPEs, the validity check alone is sufficient.
    //

    PointerPte = MiGetPdeAddress (PrototypePte);
    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 0) {
        return FALSE;
    }

#endif

    PointerPte = MiGetPteAddress (PrototypePte);

#if (_MI_PAGING_LEVELS < 3)

    if (PointerPte->u.Hard.Valid == 0) {
        MiCheckPdeForPagedPool (PrototypePte);
    }

#endif

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 1) {
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        Pfn = MI_PFN_ELEMENT (PageFrameIndex);
        if (Pfn->u2.ShareCount != 1) {
            return TRUE;
        }
    }
    else if ((PteContents.u.Soft.Prototype == 0) &&
               (PteContents.u.Soft.Transition == 1)) {

        //
        // Transition, if on standby or modified, return FALSE.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        Pfn = MI_PFN_ELEMENT (PageFrameIndex);
        if (Pfn->u3.e1.PageLocation >= ActiveAndValid) {
            if (PfnLockHeld) {
                MiMakeSystemAddressValidPfn (PrototypePte);
                *DroppedPfnLock = TRUE;
            }
            return TRUE;
        }
    }

    //
    // Page is not resident or is on standby / modified list.
    //

    return FALSE;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\flushbuf.c ===
/*++

Copyright (c) 1991  Microsoft Corporation

Module Name:

   flushbuf.c

Abstract:

    This module contains the code to flush the write buffer or otherwise
    synchronize writes on the host processor.  Also, contains code
    to flush the instruction cache of specified process.

Author:

    David N. Cutler 24-Apr-1991

Revision History:

--*/

#include "mi.h"

ULONG
MiFlushRangeFilter (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PVOID *BaseAddress,
    IN PSIZE_T Length,
    IN PLOGICAL Retry
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFlushWriteBuffer)
#pragma alloc_text(PAGE,NtFlushInstructionCache)
#pragma alloc_text(PAGE,MiFlushRangeFilter)
#endif


NTSTATUS
NtFlushWriteBuffer (
   VOID
   )

/*++

Routine Description:

    This function flushes the write buffer on the current processor.

Arguments:

    None.

Return Value:

    STATUS_SUCCESS.

--*/

{
    PAGED_CODE();

    KeFlushWriteBuffer();
    return STATUS_SUCCESS;
}

ULONG
MiFlushRangeFilter (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PVOID *BaseAddress,
    IN PSIZE_T Length,
    IN PLOGICAL Retry
    )

/*++

Routine Description:

    This is the exception handler used by NtFlushInstructionCache to protect
    against bad virtual addresses passed to KeSweepIcacheRange.  If an
    access violation occurs, this routine causes NtFlushInstructionCache to
    restart the sweep at the page following the failing page.

Arguments:

    ExceptionPointers - Supplies exception information.

    BaseAddress - Supplies a pointer to address the base of the region being
                  flushed.  If the failing address is not in the last page of
                  the region, this routine updates BaseAddress to point to the
                  next page of the region.

    Length - Supplies a pointer the length of the region being flushed.
             If the failing address is not in the last page of the region,
             this routine updates Length to reflect restarting the flush at
             the next page of the region.

    Retry - Supplies a pointer to a LOGICAL that the caller has initialized
            to FALSE.  This routine sets this LOGICAL to TRUE if an access
            violation occurs in a page before the last page of the flush region.

Return Value:

    EXCEPTION_EXECUTE_HANDLER.

--*/

{
    PEXCEPTION_RECORD ExceptionRecord;
    ULONG_PTR BadVa;
    ULONG_PTR NextVa;
    ULONG_PTR EndVa;

    ExceptionRecord = ExceptionPointers->ExceptionRecord;

    //
    // If the exception was an access violation, skip the current page of the
    // region and move to the next page.
    //

    if ( ExceptionRecord->ExceptionCode == STATUS_ACCESS_VIOLATION ) {

        //
        // Get the failing address, calculate the base address of the next page,
        // and calculate the address at the end of the region.
        //

        BadVa = ExceptionRecord->ExceptionInformation[1];
        NextVa = ROUND_TO_PAGES( BadVa + 1 );
        EndVa = *(PULONG_PTR)BaseAddress + *Length;

        //
        // If the next page didn't wrap, and the next page is below the end of
        // the region, update Length and BaseAddress appropriately and set Retry
        // to TRUE to indicate to NtFlushInstructionCache that it should call
        // KeSweepIcacheRange again.
        //

        if ( (NextVa > BadVa) && (NextVa < EndVa) ) {
            *Length = (ULONG) (EndVa - NextVa);
            *BaseAddress = (PVOID)NextVa;
            *Retry = TRUE;
        }
    }

    return EXCEPTION_EXECUTE_HANDLER;
}

NTSTATUS
NtFlushInstructionCache (
    IN HANDLE ProcessHandle,
    IN PVOID BaseAddress OPTIONAL,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function flushes the instruction cache for the specified process.

Arguments:

    ProcessHandle - Supplies a handle to the process in which the instruction
                    cache is to be flushed. Must have PROCESS_VM_WRITE access
                    to the specified process.

    BaseAddress - Supplies an optional pointer to base of the region that
                  is flushed.

    Length - Supplies the length of the region that is flushed if the base
             address is specified.

Return Value:

    STATUS_SUCCESS.

--*/

{
    KAPC_STATE ApcState;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    LOGICAL Retry;
    PVOID RangeBase;
    SIZE_T RangeLength;

    PAGED_CODE();

    PreviousMode = KeGetPreviousMode();

    //
    // If the base address is not specified, or the base address is specified
    // and the length is not zero, then flush the specified instruction cache
    // range.
    //

    if ((ARGUMENT_PRESENT(BaseAddress) == FALSE) || (Length != 0)) {

        //
        // If previous mode is user and the range specified falls in kernel
        // address space, return an error.
        //

        if ((ARGUMENT_PRESENT(BaseAddress) != FALSE) &&
            (PreviousMode != KernelMode)) {
            try {
                ProbeForRead(BaseAddress, Length, sizeof(UCHAR));
            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }

        //
        // If the specified process is not the current process, then
        // the process must be attached to during the flush.
        //

        Process = NULL;
        if (ProcessHandle != NtCurrentProcess()) {

            //
            // Reference the specified process checking for PROCESS_VM_WRITE
            // access.
            //

            Status = ObReferenceObjectByHandle(ProcessHandle,
                                               PROCESS_VM_WRITE,
                                               PsProcessType,
                                               PreviousMode,
                                               (PVOID *)&Process,
                                               NULL);

            if (!NT_SUCCESS(Status)) {
                return Status;
            }

            //
            // Attach to the process.
            //

            KeStackAttachProcess (&Process->Pcb, &ApcState);
        }

        //
        // If the base address is not specified, sweep the entire instruction
        // cache.  If the base address is specified, flush the specified range.
        //

        if (ARGUMENT_PRESENT(BaseAddress) == FALSE) {
            KeSweepIcache(FALSE);

        } else {

            //
            // Parts of the specified range may be invalid.  An exception
            // handler is used to skip over those parts.  Before calling
            // KeSweepIcacheRange, we set Retry to FALSE.  If an access
            // violation occurs in KeSweepIcacheRange, the MiFlushRangeFilter
            // exception filter is called.  It updates RangeBase and
            // RangeLength to skip over the failing page, and sets Retry to
            // TRUE.  As long as Retry is TRUE, we continue to call
            // KeSweepIcacheRange.
            //

            RangeBase = BaseAddress;
            RangeLength = Length;

            do {
                Retry = FALSE;
                try {
                    KeSweepIcacheRange(FALSE, RangeBase, RangeLength);
                } except(MiFlushRangeFilter(GetExceptionInformation(),
                                            &RangeBase,
                                            &RangeLength,
                                            &Retry)) {
                    NOTHING;
                }
            } while (Retry != FALSE);
        }

        //
        // If the specified process is not the current process, then
        // detach from it and dereference it.
        //

        if (Process != NULL) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject(Process);
        }
    }

    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\dmpaddr.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    dmpaddr.c

Abstract:

    Routines to examine pages and addresses.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Environment:

    Kernel Mode.

Revision History:

--*/

#include "mi.h"

#if DBG

LOGICAL
MiFlushUnusedSectionInternal (
    IN PCONTROL_AREA ControlArea
    );

#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmPerfSnapShotValidPhysicalMemory)
#endif

extern PFN_NUMBER MiStartOfInitialPoolFrame;
extern PFN_NUMBER MiEndOfInitialPoolFrame;
extern PMMPTE MmSystemPtesEnd[MaximumPtePoolTypes];

#if DBG
PFN_NUMBER MiIdentifyFrame = (PFN_NUMBER)-1;
ULONG MiIdentifyCounters[64];

#define MI_INCREMENT_IDENTIFY_COUNTER(x) {  \
            ASSERT (x < 64);                \
            MiIdentifyCounters[x] += 1;     \
        }
#else
#define MI_INCREMENT_IDENTIFY_COUNTER(x)
#endif


#if DBG
VOID
MiDumpValidAddresses (
    )
{
    ULONG_PTR va;
    ULONG i;
    ULONG j;
    PMMPTE PointerPde;
    PMMPTE PointerPte;

    va = 0;
    PointerPde = MiGetPdeAddress ((PVOID)va);

    for (i = 0; i < PDE_PER_PAGE; i += 1) {
        if (PointerPde->u.Hard.Valid) {
            DbgPrint("  **valid PDE, element %ld  %lx %lx\n",i,i,
                          PointerPde->u.Long);
            PointerPte = MiGetPteAddress ((PVOID)va);

            for (j = 0 ; j < PTE_PER_PAGE; j += 1) {
                if (PointerPte->u.Hard.Valid) {
                    DbgPrint("Valid address at %p PTE %p\n", (ULONG)va,
                          PointerPte->u.Long);
                }
                va += PAGE_SIZE;
                PointerPte += 1;
            }
        }
        else {
            va += (ULONG_PTR)PDE_PER_PAGE * (ULONG_PTR)PAGE_SIZE;
        }

        PointerPde += 1;
    }

    return;

}

VOID
MiFormatPte (
    IN PMMPTE PointerPte
    )
{

    PMMPTE proto_pte;
    PSUBSECTION subsect;

    if (MmIsAddressValid (PointerPte) == FALSE) {
        DbgPrint("   cannot dump PTE %p - it's not valid\n\n",
                 PointerPte);
        return;
    }

    DbgPrint("***DumpPTE at %p contains %p\n",
             PointerPte,
             PointerPte->u.Long);

    proto_pte = MiPteToProto(PointerPte);
    subsect = MiGetSubsectionAddress(PointerPte);

    DbgPrint("   protoaddr %p subsectaddr %p\n\n",
             proto_pte,
             (ULONG_PTR)subsect);

    return;
}

VOID
MiDumpWsl (
    VOID
    )
{
    ULONG i;
    PMMWSLE wsle;
    PEPROCESS CurrentProcess;

    CurrentProcess = PsGetCurrentProcess();

    DbgPrint("***WSLE cursize %lx frstfree %lx  Min %lx  Max %lx\n",
        CurrentProcess->Vm.WorkingSetSize,
        MmWorkingSetList->FirstFree,
        CurrentProcess->Vm.MinimumWorkingSetSize,
        CurrentProcess->Vm.MaximumWorkingSetSize);

    DbgPrint("   quota %lx   firstdyn %lx  last ent %lx  next slot %lx\n",
        MmWorkingSetList->Quota,
        MmWorkingSetList->FirstDynamic,
        MmWorkingSetList->LastEntry,
        MmWorkingSetList->NextSlot);

    wsle = MmWsle;

    for (i = 0; i < MmWorkingSetList->LastEntry; i += 1) {
        DbgPrint(" index %lx  %p\n",i,wsle->u1.Long);
        wsle += 1;
    }
    return;

}

#define ALLOC_SIZE ((ULONG)8*1024)
#define MM_SAVED_CONTROL 64

//
// Note these are deliberately sign-extended so they will always be greater
// than the highest user address.
//

#define MM_NONPAGED_POOL_MARK           ((PUCHAR)(LONG_PTR)0xfffff123)
#define MM_PAGED_POOL_MARK              ((PUCHAR)(LONG_PTR)0xfffff124)
#define MM_KERNEL_STACK_MARK            ((PUCHAR)(LONG_PTR)0xfffff125)
#define MM_PAGEFILE_BACKED_SHMEM_MARK   ((PUCHAR)(LONG_PTR)0xfffff126)

#define MM_DUMP_ONLY_VALID_PAGES    1

typedef struct _KERN_MAP {
    PVOID StartVa;
    PVOID EndVa;
    PKLDR_DATA_TABLE_ENTRY Entry;
} KERN_MAP, *PKERN_MAP;

ULONG
MiBuildKernelMap (
    OUT PKERN_MAP *KernelMapOut
    );

LOGICAL
MiIsAddressRangeValid (
    IN PVOID VirtualAddress,
    IN SIZE_T Length
    );

NTSTATUS
MmMemoryUsage (
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Type,
    OUT PULONG OutLength
    )

/*++

Routine Description:

    This routine (debugging only) dumps the current memory usage by
    walking the PFN database.

Arguments:

    Buffer - Supplies a *USER SPACE* buffer in which to copy the data.

    Size - Supplies the size of the buffer.

    Type - Supplies a value of 0 to dump everything,
           a value of 1 to dump only valid pages.

    OutLength - Returns how much data was written into the buffer.

Return Value:

    NTSTATUS.

--*/

{
    ULONG i;
    MMPFN_IDENTITY PfnId;
    PMMPFN LastPfn;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PSYSTEM_MEMORY_INFORMATION MemInfo;
    PSYSTEM_MEMORY_INFO Info;
    PSYSTEM_MEMORY_INFO InfoStart;
    PSYSTEM_MEMORY_INFO InfoEnd;
    PUCHAR String;
    PUCHAR Master;
    PCONTROL_AREA ControlArea;
    NTSTATUS status;
    ULONG Length;
    PEPROCESS Process;
    PUCHAR End;
    PCONTROL_AREA SavedControl[MM_SAVED_CONTROL];
    PSYSTEM_MEMORY_INFO  SavedInfo[MM_SAVED_CONTROL];
    ULONG j;
    ULONG ControlCount;
    UCHAR PageFileMappedString[] = "PageFile Mapped";
    UCHAR MetaFileString[] =       "Fs Meta File";
    UCHAR NoNameString[] =         "No File Name";
    UCHAR NonPagedPoolString[] =   "NonPagedPool";
    UCHAR PagedPoolString[] =      "PagedPool";
    UCHAR KernelStackString[] =    "Kernel Stack";
    PUCHAR NameString;
    PKERN_MAP KernMap;
    ULONG KernSize;
    PVOID VirtualAddress;
    PSUBSECTION Subsection;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;

    String = NULL;
    ControlCount = 0;
    Master = NULL;
    status = STATUS_SUCCESS;

    KernSize = MiBuildKernelMap (&KernMap);
    if (KernSize == 0) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MemInfo = ExAllocatePoolWithTag (NonPagedPool, (SIZE_T) Size, 'lMmM');

    if (MemInfo == NULL) {
        ExFreePool (KernMap);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    InfoStart = &MemInfo->Memory[0];
    InfoEnd = InfoStart;
    End = (PUCHAR)MemInfo + Size;

    //
    // Walk through the ranges identifying pages.
    //

    LOCK_PFN (OldIrql);

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        Pfn1 = MI_PFN_ELEMENT (MmPhysicalMemoryBlock->Run[i].BasePage);
        LastPfn = Pfn1 + MmPhysicalMemoryBlock->Run[i].PageCount;

        for ( ; Pfn1 < LastPfn; Pfn1 += 1) {

            RtlZeroMemory (&PfnId, sizeof(PfnId));

            MiIdentifyPfn (Pfn1, &PfnId);

            if ((PfnId.u1.e1.ListDescription == FreePageList) ||
                (PfnId.u1.e1.ListDescription == ZeroedPageList) ||
                (PfnId.u1.e1.ListDescription == BadPageList) ||
                (PfnId.u1.e1.ListDescription == TransitionPage)) {
                    continue;
            }

            if (PfnId.u1.e1.ListDescription != ActiveAndValid) {
                if (Type == MM_DUMP_ONLY_VALID_PAGES) {
                    continue;
                }
            }

            if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGEFILEMAPPED) {

                //
                // This page belongs to a pagefile-backed shared memory section.
                //

                Master = MM_PAGEFILE_BACKED_SHMEM_MARK;
            }
            else if ((PfnId.u1.e1.UseDescription == MMPFNUSE_FILE) ||
                     (PfnId.u1.e1.UseDescription == MMPFNUSE_METAFILE)) {

                //
                // This shared page maps a file or file metadata.
                //

                Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
                ControlArea = Subsection->ControlArea;
                Master = (PUCHAR) ControlArea;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_NONPAGEDPOOL) {

                //
                // This is nonpaged pool, put it in the nonpaged pool cell.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGEDPOOL) {

                //
                // This is paged pool, put it in the paged pool cell.
                //

                Master = MM_PAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_SESSIONPRIVATE) {

                //
                // Call this paged pool for now.
                //

                Master = MM_PAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_DRIVERLOCKPAGE) {

                //
                // Call this nonpaged pool for now.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_AWEPAGE) {

                //
                // Call this nonpaged pool for now.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else {

                //
                // See if the page is part of the kernel or a driver image.
                // If not but it's in system PTEs, call it a kernel thread
                // stack.
                //
                // If neither of the above, then see if the page belongs to
                // a user address or a session pagetable page.
                //

                VirtualAddress = PfnId.u2.VirtualAddress;

                for (j = 0; j < KernSize; j += 1) {
                    if ((VirtualAddress >= KernMap[j].StartVa) &&
                        (VirtualAddress < KernMap[j].EndVa)) {
                        Master = (PUCHAR)&KernMap[j];
                        break;
                    }
                }

                if (j == KernSize) {
                    if (PfnId.u1.e1.UseDescription == MMPFNUSE_SYSTEMPTE) {
                        Master = MM_KERNEL_STACK_MARK;
                    }
                    else if (MI_IS_SESSION_PTE (VirtualAddress)) {
                        Master = MM_NONPAGED_POOL_MARK;
                    }
                    else {

                        ASSERT ((PfnId.u1.e1.UseDescription == MMPFNUSE_PROCESSPRIVATE) || (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGETABLE));

                        Master = (PUCHAR) (ULONG_PTR) PfnId.u1.e3.PageDirectoryBase;
                    }
                }
            }

            //
            // The page has been identified.
            // See if there is already a bucket allocated for it.
            //

            for (Info = InfoStart; Info < InfoEnd; Info += 1) {
                if (Info->StringOffset == Master) {
                    break;
                }
            }

            if (Info == InfoEnd) {

                InfoEnd += 1;
                if ((PUCHAR)InfoEnd > End) {
                    status = STATUS_DATA_OVERRUN;
                    goto Done;
                }

                RtlZeroMemory (Info, sizeof(*Info));
                Info->StringOffset = Master;
            }

            if (PfnId.u1.e1.ListDescription == ActiveAndValid) {
                Info->ValidCount += 1;
            }
            else if ((PfnId.u1.e1.ListDescription == StandbyPageList) ||
                     (PfnId.u1.e1.ListDescription == TransitionPage)) {

                Info->TransitionCount += 1;
            }
            else if ((PfnId.u1.e1.ListDescription == ModifiedPageList) ||
                     (PfnId.u1.e1.ListDescription == ModifiedNoWritePageList)) {
                Info->ModifiedCount += 1;
            }

            if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGETABLE) {
                Info->PageTableCount += 1;
            }
        }
    }

    MemInfo->StringStart = (ULONG_PTR)Buffer + (ULONG_PTR)InfoEnd - (ULONG_PTR)MemInfo;
    String = (PUCHAR)InfoEnd;

    //
    // Process the buckets ...
    //

    for (Info = InfoStart; Info < InfoEnd; Info += 1) {

        ControlArea = NULL;

        if (Info->StringOffset == MM_PAGEFILE_BACKED_SHMEM_MARK) {
            Length = 16;
            NameString = PageFileMappedString;
        }
        else if (Info->StringOffset == MM_NONPAGED_POOL_MARK) {
            Length = 14;
            NameString = NonPagedPoolString;
        }
        else if (Info->StringOffset == MM_PAGED_POOL_MARK) {
            Length = 14;
            NameString = PagedPoolString;
        }
        else if (Info->StringOffset == MM_KERNEL_STACK_MARK) {
            Length = 14;
            NameString = KernelStackString;
        }
        else if (((PUCHAR)Info->StringOffset >= (PUCHAR)&KernMap[0]) &&
                   ((PUCHAR)Info->StringOffset <= (PUCHAR)&KernMap[KernSize])) {

            DataTableEntry = ((PKERN_MAP)Info->StringOffset)->Entry;
            NameString = (PUCHAR)DataTableEntry->BaseDllName.Buffer;
            Length = DataTableEntry->BaseDllName.Length;
        }
        else if (Info->StringOffset > (PUCHAR)MM_HIGHEST_USER_ADDRESS) {

            //
            // This points to a control area - get the file name.
            //

            ControlArea = (PCONTROL_AREA)(Info->StringOffset);
            NameString = (PUCHAR)&ControlArea->FilePointer->FileName.Buffer[0];

            Length = ControlArea->FilePointer->FileName.Length;
            if (Length == 0) {
                if (ControlArea->u.Flags.NoModifiedWriting) {
                    NameString = MetaFileString;
                    Length = 14;
                }
                else if (ControlArea->u.Flags.File == 0) {
                    NameString = PageFileMappedString;
                    Length = 16;
                }
                else {
                    NameString = NoNameString;
                    Length = 14;
                }
            }
        }
        else {

            //
            // This is a process (or session) top-level page directory.
            //

            Pfn1 = MI_PFN_ELEMENT (PtrToUlong(Info->StringOffset));
            ASSERT (Pfn1->u4.PteFrame == (ULONG_PTR)(Pfn1 - MmPfnDatabase));

            Process = (PEPROCESS)Pfn1->u1.Event;

            NameString = &Process->ImageFileName[0];
            Length = 16;
        }

        if ((String+Length+2) >= End) {
            status = STATUS_DATA_OVERRUN;
            Info->StringOffset = NULL;
            goto Done;
        }

        if ((ControlArea == NULL) ||
            (MiIsAddressRangeValid (NameString, Length))) {

            RtlCopyMemory (String, NameString, Length);
            Info->StringOffset = (PUCHAR)Buffer + ((PUCHAR)String - (PUCHAR)MemInfo);
            String[Length] = 0;
            String[Length + 1] = 0;
            String += Length + 2;
        }
        else {
            if (!(ControlArea->u.Flags.BeingCreated ||
                  ControlArea->u.Flags.BeingDeleted) &&
                  (ControlCount < MM_SAVED_CONTROL)) {

                SavedControl[ControlCount] = ControlArea;
                SavedInfo[ControlCount] = Info;
                ControlArea->NumberOfSectionReferences += 1;
                ControlCount += 1;
            }
            Info->StringOffset = NULL;
        }
    }

Done:
    UNLOCK_PFN (OldIrql);
    ExFreePool (KernMap);

    while (ControlCount != 0) {

        //
        // Process all the pagable name strings.
        //

        ControlCount -= 1;
        ControlArea = SavedControl[ControlCount];
        Info = SavedInfo[ControlCount];
        NameString = (PUCHAR)&ControlArea->FilePointer->FileName.Buffer[0];
        Length = ControlArea->FilePointer->FileName.Length;
        if (Length == 0) {
            if (ControlArea->u.Flags.NoModifiedWriting) {
                Length = 12;
                NameString = MetaFileString;
            }
            else if (ControlArea->u.Flags.File == 0) {
                NameString = PageFileMappedString;
                Length = 16;

            }
            else {
                NameString = NoNameString;
                Length = 12;
            }
        }
        if ((String+Length+2) >= End) {
            status = STATUS_DATA_OVERRUN;
        }
        if (status != STATUS_DATA_OVERRUN) {
            RtlCopyMemory (String, NameString, Length);
            Info->StringOffset = (PUCHAR)Buffer + ((PUCHAR)String - (PUCHAR)MemInfo);
            String[Length] = 0;
            String[Length + 1] = 0;
            String += Length + 2;
        }

        LOCK_PFN (OldIrql);
        ControlArea->NumberOfSectionReferences -= 1;
        MiCheckForControlAreaDeletion (ControlArea);
        UNLOCK_PFN (OldIrql);
    }
    *OutLength = (ULONG)((PUCHAR)String - (PUCHAR)MemInfo);

    //
    // Carefully copy the results to the user buffer.
    //

    try {
        RtlCopyMemory (Buffer, MemInfo, (ULONG_PTR)String - (ULONG_PTR)MemInfo);
    } except (EXCEPTION_EXECUTE_HANDLER) {
        status = GetExceptionCode();
    }

    ExFreePool (MemInfo);

    return status;
}

ULONG
MiBuildKernelMap (
    OUT PKERN_MAP *KernelMapOut
    )
{
    PKTHREAD CurrentThread;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKERN_MAP KernelMap;
    ULONG i;

    i = 0;
    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceShared (&PsLoadedModuleResource, TRUE);

    //
    // The caller wants us to allocate the return result buffer.  Size it
    // by allocating the maximum possibly needed as this should not be
    // very big (relatively).  It is the caller's responsibility to free
    // this.  Obviously this option can only be requested after pool has
    // been initialized.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {
        i += 1;
        NextEntry = NextEntry->Flink;
    }

    KernelMap = ExAllocatePoolWithTag (NonPagedPool,
                                       i * sizeof(KERN_MAP),
                                       'lMmM');

    if (KernelMap == NULL) {
        return 0;
    }

    *KernelMapOut = KernelMap;

    i = 0;
    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {
        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);
        KernelMap[i].Entry = DataTableEntry;
        KernelMap[i].StartVa = DataTableEntry->DllBase;
        KernelMap[i].EndVa = (PVOID)((ULONG_PTR)KernelMap[i].StartVa +
                                         DataTableEntry->SizeOfImage);
        i += 1;
        NextEntry = NextEntry->Flink;
    }

    ExReleaseResourceLite(&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    return i;
}

VOID
MiDumpReferencedPages (
    VOID
    )

/*++

Routine Description:

    This routine (debugging only) dumps all PFN entries which appear
    to be locked in memory for i/o.

Arguments:

    None.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN PfnLast;

    LOCK_PFN (OldIrql);

    Pfn1 = MI_PFN_ELEMENT (MmLowestPhysicalPage);
    PfnLast = MI_PFN_ELEMENT (MmHighestPhysicalPage);

    while (Pfn1 <= PfnLast) {

        if ((Pfn1->u2.ShareCount == 0) && (Pfn1->u3.e2.ReferenceCount != 0)) {
            MiFormatPfn (Pfn1);
        }

        if (Pfn1->u3.e2.ReferenceCount > 1) {
            MiFormatPfn (Pfn1);
        }

        Pfn1 += 1;
    }

    UNLOCK_PFN (OldIrql);
    return;
}

#else //DBG

NTSTATUS
MmMemoryUsage (
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Type,
    OUT PULONG OutLength
    )
{
    UNREFERENCED_PARAMETER (Buffer);
    UNREFERENCED_PARAMETER (Size);
    UNREFERENCED_PARAMETER (Type);
    UNREFERENCED_PARAMETER (OutLength);

    return STATUS_NOT_IMPLEMENTED;
}

#endif //DBG

//
// One benefit of using run length maximums of less than 4GB is that even
// frame numbers above 4GB are handled properly despite the 32-bit limitations
// of the bitmap routines.
//

#define MI_MAXIMUM_PFNID_RUN    4096


NTSTATUS
MmPerfSnapShotValidPhysicalMemory (
    VOID
    )

/*++

Routine Description:

    This routine logs the PFN numbers of all ActiveAndValid pages.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    ULONG_PTR MemSnapLocal[(sizeof(MMPFN_MEMSNAP_INFORMATION)/sizeof(ULONG_PTR)) + (MI_MAXIMUM_PFNID_RUN / (8*sizeof(ULONG_PTR))) ];
    PMMPFN_MEMSNAP_INFORMATION MemSnap;
    PMMPFN Pfn1;
    PMMPFN FirstPfn;
    PMMPFN LastPfn;
    PMMPFN MaxPfn;
    PMMPFN InitialPfn;
    RTL_BITMAP BitMap;
    PULONG ActualBits;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT ((MI_MAXIMUM_PFNID_RUN % (8 * sizeof(ULONG_PTR))) == 0);

    MemSnap = (PMMPFN_MEMSNAP_INFORMATION)&MemSnapLocal;

    ActualBits = (PULONG)(MemSnap + 1);

    RtlInitializeBitMap (&BitMap, ActualBits, MI_MAXIMUM_PFNID_RUN);

    MemSnap->Count = 0;
    RtlClearAllBits (&BitMap);

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        StartPage = MmPhysicalMemoryBlock->Run[i].BasePage;
        EndPage = StartPage + MmPhysicalMemoryBlock->Run[i].PageCount;
        FirstPfn = MI_PFN_ELEMENT (StartPage);
        LastPfn = MI_PFN_ELEMENT (EndPage);

        //
        // Find the first valid PFN and start the run there.
        //

        for (Pfn1 = FirstPfn; Pfn1 < LastPfn; Pfn1 += 1) {
            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {
                break;
            }
        }

        if (Pfn1 == LastPfn) {

            //
            // No valid PFNs in this block, move on to the next block.
            //

            continue;
        }

        MaxPfn = LastPfn;
        InitialPfn = NULL;

        do {
            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {
                if (InitialPfn == NULL) { 
                    MemSnap->InitialPageFrameIndex = Pfn1 - MmPfnDatabase;
                    InitialPfn = Pfn1;
                    MaxPfn = InitialPfn + MI_MAXIMUM_PFNID_RUN;
                }
                RtlSetBit (&BitMap, (ULONG) (Pfn1 - InitialPfn));
            }

            Pfn1 += 1;

            if ((Pfn1 >= MaxPfn) && (InitialPfn != NULL)) {

                //
                // Log the bitmap as we're at then end of it.
                //

                ASSERT ((Pfn1 - InitialPfn) == MI_MAXIMUM_PFNID_RUN);
                MemSnap->Count = MI_MAXIMUM_PFNID_RUN;
                PerfInfoLogBytes (PERFINFO_LOG_TYPE_MEMORYSNAPLITE,
                                  MemSnap,
                                  sizeof(MemSnapLocal));

                InitialPfn = NULL;
                MaxPfn = LastPfn;
                RtlClearAllBits (&BitMap);
            }
        } while (Pfn1 < LastPfn);

        //
        // Dump any straggling bitmap entries now as this range is finished.
        //

        if (InitialPfn != NULL) {

            ASSERT (Pfn1 == LastPfn);
            ASSERT (Pfn1 < MaxPfn);
            ASSERT (Pfn1 > InitialPfn);

            MemSnap->Count = Pfn1 - InitialPfn;
            PerfInfoLogBytes (PERFINFO_LOG_TYPE_MEMORYSNAPLITE,
                              MemSnap,
                              sizeof(MMPFN_MEMSNAP_INFORMATION) +
                                  (ULONG) ((MemSnap->Count + 8) / 8));

            RtlClearAllBits (&BitMap);
        }
    }

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    return STATUS_SUCCESS;
}

#define PFN_ID_BUFFERS    128


NTSTATUS
MmIdentifyPhysicalMemory (
    VOID
    )

/*++

Routine Description:

    This routine calls the pfn id code for each page.  Because
    the logging can't handle very large amounts of data in a burst
    (limited buffering), the data is broken into page size chunks.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN EndPfn;
    PFN_NUMBER PageFrameIndex;
    MMPFN_IDENTITY PfnIdBuffer[PFN_ID_BUFFERS];
    PMMPFN_IDENTITY BufferPointer;
    PMMPFN_IDENTITY BufferLast;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    BufferPointer = &PfnIdBuffer[0];
    BufferLast = BufferPointer + PFN_ID_BUFFERS;
    RtlZeroMemory (PfnIdBuffer, sizeof(PfnIdBuffer));

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    //
    // Walk through the ranges and identify pages until
    // the buffer is full or we've run out of pages.
    //

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        PageFrameIndex = MmPhysicalMemoryBlock->Run[i].BasePage;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        EndPfn = Pfn1 + MmPhysicalMemoryBlock->Run[i].PageCount;

        LOCK_PFN (OldIrql);

        while (Pfn1 < EndPfn) {

            MiIdentifyPfn (Pfn1, BufferPointer);

            BufferPointer += 1;

            if (BufferPointer == BufferLast) {

                //
                // Release and reacquire the PFN lock so it's not held so long.
                //

                UNLOCK_PFN (OldIrql);

                //
                // Log the buffered entries.
                //

                BufferPointer = &PfnIdBuffer[0];
                do {

                    PerfInfoLogBytes (PERFINFO_LOG_TYPE_PAGEINMEMORY,
                                      BufferPointer,
                                      sizeof(PfnIdBuffer[0]));

                    BufferPointer += 1;

                } while (BufferPointer < BufferLast);

                //
                // Reset the buffer to the beginning and zero it.
                //

                BufferPointer = &PfnIdBuffer[0];
                RtlZeroMemory (PfnIdBuffer, sizeof(PfnIdBuffer));

                LOCK_PFN (OldIrql);
            }
            Pfn1 += 1;
        }

        UNLOCK_PFN (OldIrql);
    }

    //
    // Note that releasing this mutex here means the last entry can be
    // inserted out of order if we are preempted and another thread starts
    // the same operation (or if we're on an MP machine).  The PERF module
    // must handle this properly as any synchronization provided by this
    // routine is purely a side effect not deliberate.
    //

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    if (BufferPointer != &PfnIdBuffer[0]) {

        BufferLast = BufferPointer;
        BufferPointer = &PfnIdBuffer[0];

        do {

            PerfInfoLogBytes (PERFINFO_LOG_TYPE_PAGEINMEMORY,
                              BufferPointer,
                              sizeof(PfnIdBuffer[0]));

            BufferPointer += 1;

        } while (BufferPointer < BufferLast);
    }

    return STATUS_SUCCESS;
}

VOID
FASTCALL
MiIdentifyPfn (
    IN PMMPFN Pfn1,
    OUT PMMPFN_IDENTITY PfnIdentity
    )

/*++

Routine Description:

    This routine captures relevant information for the argument page frame.

Arguments:

    Pfn1 - Supplies the PFN element of the page frame number being queried.

    PfnIdentity - Receives the structure to fill in with the information.

Return Value:

    None.

Environment:

    Kernel mode.  PFN lock held.

--*/

{
    ULONG i;
    PMMPTE PteAddress;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PVOID VirtualAddress;
    PFILE_OBJECT FilePointer;
    PFN_NUMBER PageFrameIndex;

    MI_INCREMENT_IDENTIFY_COUNTER (8);

    ASSERT (PfnIdentity->u2.VirtualAddress == 0);
    ASSERT (PfnIdentity->u1.e1.ListDescription == 0);
    ASSERT (PfnIdentity->u1.e1.UseDescription == 0);
    ASSERT (PfnIdentity->u1.e1.Pinned == 0);
    ASSERT (PfnIdentity->u1.e2.Offset == 0);

    MM_PFN_LOCK_ASSERT();

    PageFrameIndex = Pfn1 - MmPfnDatabase;
    PfnIdentity->PageFrameIndex = PageFrameIndex;
    PfnIdentity->u1.e1.ListDescription = Pfn1->u3.e1.PageLocation;

#if DBG
    if (PageFrameIndex == MiIdentifyFrame) {
        DbgPrint ("MmIdentifyPfn: requested PFN %p\n", PageFrameIndex);
        DbgBreakPoint ();
    }
#endif

    MI_INCREMENT_IDENTIFY_COUNTER (Pfn1->u3.e1.PageLocation);

    switch (Pfn1->u3.e1.PageLocation) {

        case ZeroedPageList:
        case FreePageList:
        case BadPageList:
                return;

        case ActiveAndValid:

                //
                // It's too much work to determine if the page is locked
                // in a working set due to cross-process WSL references, etc.
                // So don't bother for now.
                //

                ASSERT (PfnIdentity->u1.e1.ListDescription == MMPFNLIST_ACTIVE);

                if (Pfn1->u1.WsIndex == 0) {
                    MI_INCREMENT_IDENTIFY_COUNTER (9);
                    PfnIdentity->u1.e1.Pinned = 1;
                }
                else if (Pfn1->u3.e2.ReferenceCount > 1) {

                    //
                    // This page is pinned, presumably for an ongoing I/O.
                    //

                    PfnIdentity->u1.e1.Pinned = 1;
                    MI_INCREMENT_IDENTIFY_COUNTER (10);
                }
                break;

        case StandbyPageList:
        case ModifiedPageList:
        case ModifiedNoWritePageList:
                if (Pfn1->u3.e2.ReferenceCount >= 1) {

                    //
                    // This page is pinned, presumably for an ongoing I/O.
                    //

                    PfnIdentity->u1.e1.Pinned = 1;
                    MI_INCREMENT_IDENTIFY_COUNTER (11);
                }

                if ((Pfn1->u3.e1.PageLocation == ModifiedPageList) &&
                    (MI_IS_PFN_DELETED (Pfn1)) &&
                    (Pfn1->u2.ShareCount == 0)) {

                    //
                    // This page may be a modified write completing in the
                    // context of the modified writer thread.  If the
                    // address space was deleted while the I/O was in
                    // progress, the frame will be released now.  More
                    // importantly, the frame's containing frame is
                    // meaningless as it may have already been freed
                    // and reused.
                    //
                    // We can't tell what this page was being used for
                    // since its address space is gone, so just call it
                    // process private for now.
                    //

                    MI_INCREMENT_IDENTIFY_COUNTER (40);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;

                    return;
                }

                break;

        case TransitionPage:

                //
                // This page is pinned due to a straggling I/O - the virtual
                // address has been deleted but an I/O referencing it has not
                // completed.
                //

                PfnIdentity->u1.e1.Pinned = 1;
                MI_INCREMENT_IDENTIFY_COUNTER (11);
                PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
                return;

        default:
#if DBG
                DbgPrint ("MmIdentifyPfn: unknown PFN %p %x\n",
                            Pfn1, Pfn1->u3.e1.PageLocation);
                DbgBreakPoint ();
#endif
                break;

    }

    //
    // Capture differing information based on the type of page being examined.
    //

    //
    // General purpose stress shows 40% of the pages are prototypes so
    // for speed, check for these first.
    //

    if (Pfn1->u3.e1.PrototypePte == 1) {

        MI_INCREMENT_IDENTIFY_COUNTER (12);

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            //
            // Demand zero or (equivalently) pagefile backed.
            //
            // There are some hard problems here preventing more indepth
            // identification of these pages:
            //
            // 1.  The PFN contains a backpointer to the prototype PTE - but
            //     there is no definitive way to get to the SEGMENT or
            //     CONTROL_AREA from this.
            //
            // 2.  The prototype PTE pointer itself may be paged out and
            //     the PFN lock is held right now.
            //

            MI_INCREMENT_IDENTIFY_COUNTER (13);

#if 0
            PfnIdentity->u2.FileObject = (PVOID) ControlArea->Segment->u1.CreatingProcess;

            PfnIdentity->u1.e2.Offset = (((ULONG_PTR)ControlArea->Segment->u2.FirstMappedVa) >> MMSECTOR_SHIFT);
#endif

            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGEFILEMAPPED;
            return;
        }

        MI_INCREMENT_IDENTIFY_COUNTER (14);

        //
        // Backed by a mapped file.
        //

        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
        ControlArea = Subsection->ControlArea;
        ASSERT (ControlArea->u.Flags.File == 1);
        FilePointer = ControlArea->FilePointer;
        ASSERT (FilePointer != NULL);

        PfnIdentity->u2.FileObject = FilePointer;

        if (Subsection->SubsectionBase != NULL) {
            PfnIdentity->u1.e2.Offset = (MiStartingOffset (Subsection, Pfn1->PteAddress) >> MMSECTOR_SHIFT);
        }
        else {

            //
            // The only time we should be here (a valid PFN with no subsection)
            // is if we are the segment dereference thread putting pages into
            // the freelist.  At this point the PFN lock is held and the
            // control area/subsection/PFN structures are not yet consistent
            // so just treat this as an offset of 0 as it should be rare.
            //

            ASSERT (PsGetCurrentThread()->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread);
        }

        //
        // Check for nomodwrite sections - typically this is filesystem
        // metadata although it could also be registry data (which is named).
        //

        if (ControlArea->u.Flags.NoModifiedWriting) {
            MI_INCREMENT_IDENTIFY_COUNTER (15);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_METAFILE;
            return;
        }

        if (FilePointer->FileName.Length != 0) {

            //
            // This mapped file has a name.
            //

            MI_INCREMENT_IDENTIFY_COUNTER (16);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_FILE;
            return;
        }

        //
        // No name - this file must be in the midst of a purge, but it
        // still *was* a mapped file of some sort.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (17);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_FILE;
        return;
    }

    if ((PageFrameIndex >= MiStartOfInitialPoolFrame) &&
        (PageFrameIndex <= MiEndOfInitialPoolFrame)) {

        //
        // This is initial nonpaged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (18);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
        VirtualAddress = (PVOID)((ULONG_PTR)MmNonPagedPoolStart +
                                ((PageFrameIndex - MiStartOfInitialPoolFrame) << PAGE_SHIFT));
        PfnIdentity->u2.VirtualAddress = PAGE_ALIGN(VirtualAddress);
        return;
    }

    PteAddress = Pfn1->PteAddress;
    VirtualAddress = MiGetVirtualAddressMappedByPte (PteAddress);
    PfnIdentity->u2.VirtualAddress = PAGE_ALIGN(VirtualAddress);

    if (MI_IS_SESSION_ADDRESS(VirtualAddress)) {

        //
        // Note session addresses that map images (or views) that haven't
        // undergone a copy-on-write split were already treated as prototype
        // PTEs above.  This clause handles session pool and copy-on-written
        // pages.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (19);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SESSIONPRIVATE;
        return;
    }

    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress <= MmPagedPoolEnd)) {

        //
        // This is paged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (20);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGEDPOOL;
        return;

    }

    if ((VirtualAddress >= MmNonPagedPoolExpansionStart) &&
        (VirtualAddress < MmNonPagedPoolEnd)) {

        //
        // This is expansion nonpaged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (21);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
        return;
    }

    if ((VirtualAddress >= MmNonPagedSystemStart) &&
        (PteAddress <= MmSystemPtesEnd[SystemPteSpace])) {

        //
        // This is driver space, kernel stack, special pool or other
        // system PTE mappings.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (22);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SYSTEMPTE;
        return;
    }

#if defined (_X86_)

    //
    // 2 other ranges of system PTEs can exist on x86.
    //

    if (((MiNumberOfExtraSystemPdes != 0) &&
         (VirtualAddress >= (PVOID)MiExtraResourceStart) &&
         (VirtualAddress < (PVOID)MiExtraResourceEnd)) ||

        ((MiUseMaximumSystemSpace != 0) &&
         (VirtualAddress >= (PVOID)MiUseMaximumSystemSpace) &&
         (VirtualAddress < (PVOID)MiUseMaximumSystemSpaceEnd)))
    {
        //
        // This is driver space, kernel stack, special pool or other
        // system PTE mappings.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (23);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SYSTEMPTE;
        return;
    }

#endif

    if (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) {

        MI_INCREMENT_IDENTIFY_COUNTER (24);

        //
        // Carefully check here as this could be a legitimate frame as well.
        //

        if ((Pfn1->u3.e1.StartOfAllocation == 1) &&
            (Pfn1->u3.e1.EndOfAllocation == 1) &&
            (Pfn1->u3.e1.PageLocation == ActiveAndValid)) {
                if (MI_IS_PFN_DELETED (Pfn1)) {
                    MI_INCREMENT_IDENTIFY_COUNTER (25);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_DRIVERLOCKPAGE;
                }
                else {
                    MI_INCREMENT_IDENTIFY_COUNTER (26);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_AWEPAGE;
                }
                return;
        }
    }

#if DBG

    //
    // In checked kernels, AWE frames get their containing frame decremented
    // when the AWE frame is freed.
    //

    if (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME - 1) {

        MI_INCREMENT_IDENTIFY_COUNTER (24);

        //
        // Carefully check here as this could be a legitimate frame as well.
        //

        if ((Pfn1->u3.e1.StartOfAllocation == 0) &&
            (Pfn1->u3.e1.EndOfAllocation == 0) &&
            (Pfn1->u3.e1.PageLocation == StandbyPageList)) {

            MI_INCREMENT_IDENTIFY_COUNTER (26);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_AWEPAGE;
            return;
        }
    }

#endif

    //
    // Check the PFN working set index carefully here.  This must be done
    // before walking back through the containing frames because if this page
    // is not in a working set, the containing frame may not be meaningful and
    // dereferencing it can crash the system and/or yield incorrect walks.
    // This is because if a page will never be trimmable there is no need to
    // have a containing frame initialized.  This also covers the case of
    // data pages mapped via large page directory entries as these have no
    // containing page table frame.
    //

    if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

        if (Pfn1->u1.WsIndex == 0) {

            //
            // Default to calling these allocations nonpaged pool because even
            // when they technically are not, from a usage standpoint they are.
            // Note the default is overridden for specific cases where the usage
            // is not in fact nonpaged.
            //

            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
            ASSERT (PfnIdentity->u1.e1.Pinned == 1);
            MI_INCREMENT_IDENTIFY_COUNTER (27);
            return;
        }
    }


    //
    // Must be a process private page
    //
    // OR
    //
    // a page table, page directory, parent or extended parent.
    //

    i = 0;
    while (Pfn1->u4.PteFrame != PageFrameIndex) {

        //
        // The only way the PTE address will go out of bounds is if this is
        // a top level page directory page for a process that has been
        // swapped out but is still waiting for the transition/modified
        // page table pages to be reclaimed.  ie: until that happens, the
        // page directory is marked Active, but the PteAddress & containing
        // page are pointing at the EPROCESS pool page.
        //

#if defined(_IA64_)

        if (((Pfn1->PteAddress >= (PMMPTE) PTE_BASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_TOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PTE_KBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_KTOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PTE_SBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_STOP)))

#else

        if ((Pfn1->PteAddress >= (PMMPTE) PTE_BASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_TOP))

#endif

        {
            PageFrameIndex = Pfn1->u4.PteFrame;
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            i += 1;
        }
        else {
            MI_INCREMENT_IDENTIFY_COUNTER (41);
            break;
        }
    }

    MI_INCREMENT_IDENTIFY_COUNTER (31+i);

    PfnIdentity->u1.e3.PageDirectoryBase = PageFrameIndex;

#if defined(_X86PAE_)

    //
    // PAE is unique because the 3rd level is not defined as only a mini
    // 4 entry 3rd level is in use.  Check for that explicitly, noting that
    // it takes one extra walk to get to the top.  Top level PAE pages (the
    // ones that contain only the 4 PDPTE pointers) are treated above as
    // active pinned pages, not as pagetable pages because each one is shared
    // across 127 processes and resides in the system global space.
    //

    if (i == _MI_PAGING_LEVELS + 1) {

        //
        // Had to walk all the way to the top.  Must be a data page.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (29);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
        return;
    }

#else

    if (i == _MI_PAGING_LEVELS) {

        //
        // Had to walk all the way to the top.  Must be a data page.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (29);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
        return;
    }

#endif

    //
    // Must have been a page in the hierarchy (not a data page) as we arrived
    // at the top early.
    //

    MI_INCREMENT_IDENTIFY_COUNTER (30);
    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGETABLE;

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\deleteva.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   deleteva.c

Abstract:

    This module contains the routines for deleting virtual address space.

Author:

    Lou Perazzoli (loup) 11-May-1989
    Landy Wang (landyw) 02-June-1997

--*/

#include "mi.h"

#if defined (_WIN64_) && defined (DBG_VERBOSE)

typedef struct _MI_TRACK_USE {

    PFN_NUMBER Pfn;
    PVOID Va;
    ULONG Id;
    ULONG PfnUse;
    ULONG PfnUseCounted;
    ULONG TickCount;
    PKTHREAD Thread;
    PEPROCESS Process;
} MI_TRACK_USE, *PMI_TRACK_USE;

ULONG MiTrackUseSize = 8192;
PMI_TRACK_USE MiTrackUse;
LONG MiTrackUseIndex;

VOID
MiInitUseCounts (
    VOID
    )
{
    MiTrackUse = ExAllocatePoolWithTag (NonPagedPool,
                                        MiTrackUseSize * sizeof (MI_TRACK_USE),
                                        'lqrI');
    ASSERT (MiTrackUse != NULL);
}


VOID
MiCheckUseCounts (
    PVOID TempHandle,
    PVOID Va,
    ULONG Id
    )

/*++

Routine Description:

    This routine ensures that all the counters are correct.

Arguments:

    TempHandle - Supplies the handle for used page table counts.

    Va - Supplies the virtual address.

    Id - Supplies the ID.

Return Value:

    None.

Environment:

    Kernel mode, called with APCs disabled, working set mutex and PFN lock
    held.

--*/

{
    LOGICAL LogIt;
    ULONG i;
    ULONG TempHandleCount;
    ULONG TempCounted;
    PMMPTE TempPage;
    KIRQL OldIrql;
    ULONG Index;
    PFN_NUMBER PageFrameIndex;
    PMI_TRACK_USE Information;
    LARGE_INTEGER TimeStamp;
    PMMPFN Pfn1;
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();

    //
    // TempHandle is really the PMMPFN containing the UsedPageTableEntries.
    //

    Pfn1 = (PMMPFN)TempHandle;
    PageFrameIndex = Pfn1 - MmPfnDatabase;

    TempHandleCount = MI_GET_USED_PTES_FROM_HANDLE (TempHandle);

    if (Id & 0x80000000) {
        ASSERT (TempHandleCount != 0);
    }

    TempPage = (PMMPTE) MiMapPageInHyperSpace (Process, PageFrameIndex, &OldIrql);

    TempCounted = 0;

    for (i = 0; i < PTE_PER_PAGE; i += 1) {
        if (TempPage->u.Long != 0) {
            TempCounted += 1;
        }
        TempPage += 1;
    }

#if 0
    if (zz & 0x4) {
        LogIt = FALSE;
        if (Pfn1->PteFrame == PageFrameIndex) {
            // TopLevel parent page, not interesting to us.
        }
        else {
            PMMPFN Pfn2;

            Pfn2 = MI_PFN_ELEMENT (Pfn1->PteFrame);
            if (Pfn2->PteFrame == Pfn1->PteFrame) {
                // our parent is the toplevel, so very interesting.
                LogIt = TRUE;
            }
        }
    }
    else {
        LogIt = TRUE;
    }
#else
    LogIt = TRUE;
#endif

    if (LogIt == TRUE) {

        //
        // Capture information
        //

        Index = InterlockedExchangeAdd(&MiTrackUseIndex, 1);                    
        Index &= (MiTrackUseSize - 1);

        Information = &(MiTrackUse[Index]);

        Information->Thread = KeGetCurrentThread();
        Information->Process = (PEPROCESS)((ULONG_PTR)PsGetCurrentProcess() + KeGetCurrentProcessorNumber());
        Information->Va = Va;
        Information->Id = Id;
        KeQueryTickCount(&TimeStamp);
        Information->TickCount = TimeStamp.LowPart;
        Information->Pfn = PageFrameIndex;
        Information->PfnUse = TempHandleCount;
        Information->PfnUseCounted = TempCounted;

        if (TempCounted != TempHandleCount) {
            DbgPrint ("MiCheckUseCounts %p %x %x %x %x\n", Va, Id, PageFrameIndex, TempHandleCount, TempCounted);
            DbgBreakPoint ();
        }
    }

    MiUnmapPageInHyperSpace (Process, TempPage, OldIrql);
    return;
}
#endif


VOID
MiDeleteVirtualAddresses (
    IN PUCHAR StartingAddress,
    IN PUCHAR EndingAddress,
    IN ULONG AddressSpaceDeletion,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This routine deletes the specified virtual address range within
    the current process.

Arguments:

    StartingAddress - Supplies the first virtual address to delete.

    EndingAddress - Supplies the last address to delete.

    AddressSpaceDeletion - Supplies TRUE if the address space is being
                           deleted, FALSE otherwise.  If TRUE is specified
                           the TB is not flushed and valid addresses are
                           not removed from the working set.

    Vad - Supplies the virtual address descriptor which maps this range
          or NULL if we are not concerned about views.  From the Vad the
          range of prototype PTEs is determined and this information is
          used to uncover if the PTE refers to a prototype PTE or a
          fork PTE.

Return Value:

    None.

Environment:

    Kernel mode, called with APCs disabled, working set mutex and PFN lock
    held.  These mutexes may be released and reacquired to fault pages in.

--*/

{
    PUCHAR Va;
    PVOID TempVa;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE OriginalPointerPte;
    PMMPTE ProtoPte;
    PMMPTE LastProtoPte;
    PEPROCESS CurrentProcess;
    PSUBSECTION Subsection;
    PVOID UsedPageTableHandle;
    KIRQL OldIrql;
    MMPTE_FLUSH_LIST FlushList;
    ULONG Waited;
    LOGICAL Skipped;
#if DBG
    PMMPTE ProtoPte2;
    PMMPTE LastProtoPte2;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID UsedPageDirectoryHandle;
    PVOID TempHandle;
#endif

    OldIrql = APC_LEVEL;
    FlushList.Count = 0;

    MM_PFN_LOCK_ASSERT();
    CurrentProcess = PsGetCurrentProcess();

    Va = StartingAddress;
    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);
    PointerPxe = MiGetPxeAddress (Va);
    OriginalPointerPte = PointerPte;

    do {

#if (_MI_PAGING_LEVELS >= 3)
restart:
#endif

        while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                           CurrentProcess,
                                           TRUE,
                                           &Waited) == FALSE) {

            //
            // This extended page directory parent entry is empty,
            // go to the next one.
            //

            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if (Va > EndingAddress) {

                //
                // All done, return.
                //

                return;
            }
        }
#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                           CurrentProcess,
                                           TRUE,
                                           &Waited) == FALSE) {

            //
            // This page directory parent entry is empty, go to the next one.
            //

            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if (Va > EndingAddress) {

                //
                // All done, return.
                //

                return;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe += 1;
                ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
                goto restart;
            }
#endif
        }

#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
        MI_CHECK_USED_PTES_HANDLE (PointerPte);
        TempHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
        ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (TempHandle) != 0) ||
                (PointerPde->u.Long == 0));
#endif

        while (MiDoesPdeExistAndMakeValid (PointerPde,
                                           CurrentProcess,
                                           TRUE,
                                           &Waited) == FALSE) {

            //
            // This page directory entry is empty, go to the next one.
            //

            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if (Va > EndingAddress) {

                //
                // All done, return.
                //

                return;
            }

#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe += 1;
                ASSERT (PointerPpe == MiGetPteAddress (PointerPde));
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto restart;
            }
#endif
        }

#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
        MI_CHECK_USED_PTES_HANDLE (Va);
        TempHandle = MI_GET_USED_PTES_HANDLE (Va);
        ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (TempHandle) != 0) ||
                (PointerPte->u.Long == 0));
#endif

    } while (Waited != 0);

    //
    // A valid PDE has been located, examine each PTE and delete them.
    //

    ASSERT64 (PointerPpe->u.Hard.Valid == 1);
    ASSERT (PointerPde->u.Hard.Valid == 1);
    ASSERT (Va <= EndingAddress);

    MI_CHECK_USED_PTES_HANDLE (Va);
    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

    if ((Vad == (PMMVAD)NULL) ||
        (Vad->u.VadFlags.PrivateMemory) ||
        (Vad->FirstPrototypePte == (PMMPTE)NULL)) {
        ProtoPte = (PMMPTE)NULL;
        LastProtoPte = (PMMPTE)NULL;
    }
    else {
        ProtoPte = Vad->FirstPrototypePte;
        LastProtoPte = (PMMPTE)4;
    }

    //
    // Initializing Subsection is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    Subsection = NULL;

    //
    // Examine each PTE within the address range and delete it.
    //

    do {

        //
        // The PPE and PDE are now valid, delete the PTE.
        //

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);
        ASSERT (Va <= EndingAddress);

        MI_CHECK_USED_PTES_HANDLE (Va);
        ASSERT (UsedPageTableHandle == MI_GET_USED_PTES_HANDLE (Va));

        if (PointerPte->u.Long != 0) {

            //
            // One less used page table entry in this page table page.
            //

            MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            if (IS_PTE_NOT_DEMAND_ZERO (*PointerPte)) {

                if (LastProtoPte != NULL) {
                    if (ProtoPte >= LastProtoPte) {
                        ProtoPte = MiGetProtoPteAddress(Vad, MI_VA_TO_VPN(Va));
                        Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(Va));
                        LastProtoPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                    }
#if DBG
                    if (Vad->u.VadFlags.ImageMap != 1) {
                        if ((ProtoPte < Subsection->SubsectionBase) ||
                            (ProtoPte >= LastProtoPte)) {
                            DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                                        ProtoPte,Va,Vad,Subsection);
                            DbgBreakPoint();
                        }
                    }
#endif
                }

                Waited = MiDeletePte (PointerPte,
                                      (PVOID)Va,
                                      AddressSpaceDeletion,
                                      CurrentProcess,
                                      ProtoPte,
                                      &FlushList);
        
#if (_MI_PAGING_LEVELS >= 3)

                //
                // This must be recalculated here if MiDeletePte dropped the
                // PFN lock (this can happen when dealing with POSIX forked
                // pages.  Since the used PTE count is kept in the PFN entry
                // which could have been paged out and replaced during this
                // window, recomputation of its address (not the contents)
                // is necessary.
                //

                if (Waited != 0) {
                    MI_CHECK_USED_PTES_HANDLE (Va);
                    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);
                }
#endif

            }
            else {
                MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
            }
        }

        Va += PAGE_SIZE;
        PointerPte += 1;
        ProtoPte += 1;

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);

        //
        // If not at the end of a page table and still within the specified
        // range, then just march directly on to the next PTE.
        //

        if ((!MiIsVirtualAddressOnPdeBoundary(Va)) && (Va <= EndingAddress)) {
            continue;
        }

        //
        // The virtual address is on a page directory boundary:
        //
        // 1. Flush the PTEs for the previous page table page.
        // 2. Delete the previous page directory & page table if appropriate.
        // 3. Attempt to leap forward skipping over empty page directories
        //    and page tables where possible.
        //

        //
        // If all the entries have been eliminated from the previous
        // page table page, delete the page table page itself.
        //
    
        MiFlushPteList (&FlushList, FALSE, ZeroPte);

        //
        // If all the entries have been eliminated from the previous
        // page table page, delete the page table page itself.
        //

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);

#if (_MI_PAGING_LEVELS >= 3)
        MI_CHECK_USED_PTES_HANDLE (PointerPte - 1);
#endif

        if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) &&
            (PointerPde->u.Long != 0)) {

#if (_MI_PAGING_LEVELS >= 3)
            UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
            MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif

            TempVa = MiGetVirtualAddressMappedByPte(PointerPde);
            MiDeletePte (PointerPde,
                         TempVa,
                         AddressSpaceDeletion,
                         CurrentProcess,
                         NULL,
                         NULL);

#if (_MI_PAGING_LEVELS >= 3)
            if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageDirectoryHandle) == 0) &&
                (PointerPpe->u.Long != 0)) {
    
#if (_MI_PAGING_LEVELS >= 4)
                UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                MiDeletePte (PointerPpe,
                             TempVa,
                             AddressSpaceDeletion,
                             CurrentProcess,
                             NULL,
                             NULL);

#if (_MI_PAGING_LEVELS >= 4)
                if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageDirectoryHandle) == 0) &&
                    (PointerPxe->u.Long != 0)) {

                    TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                    MiDeletePte (PointerPxe,
                                 TempVa,
                                 AddressSpaceDeletion,
                                 CurrentProcess,
                                 NULL,
                                 NULL);
                }
#endif
    
            }
#endif
        }

        if (Va > EndingAddress) {
        
            //
            // All done, return.
            //
        
            return;
        }

        //
        // Release the PFN lock.  This prevents a single thread
        // from forcing other high priority threads from being
        // blocked while a large address range is deleted.  There
        // is nothing magic about the instructions within the
        // lock and unlock.
        //

        UNLOCK_PFN (OldIrql);
        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);
        Skipped = FALSE;
        LOCK_PFN (OldIrql);

        //
        // Attempt to leap forward skipping over empty page directories
        // and page tables where possible.
        //

        do {

#if (_MI_PAGING_LEVELS >= 3)
restart2:
#endif

            while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                               CurrentProcess,
                                               TRUE,
                                               &Waited) == FALSE) {

                //
                // This extended page directory parent entry is empty,
                // go to the next one.
                //

                Skipped = TRUE;
                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if (Va > EndingAddress) {

                    //
                    // All done, return.
                    //

                    return;
                }
            }

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif
            while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                               CurrentProcess,
                                               TRUE,
                                               &Waited) == FALSE) {

                //
                // This page directory parent entry is empty,
                // go to the next one.
                //

                Skipped = TRUE;
                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if (Va > EndingAddress) {

                    //
                    // All done, return.
                    //

                    return;
                }
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary (PointerPpe)) {
                    PointerPxe += 1;
                    ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
                    goto restart2;
                }
#endif
            }

#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
            MI_CHECK_USED_PTES_HANDLE (PointerPte);
            TempHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
            ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (TempHandle) != 0) ||
                    (PointerPde->u.Long == 0));
#endif

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            while (MiDoesPdeExistAndMakeValid (PointerPde,
                                               CurrentProcess,
                                               TRUE,
                                               &Waited) == FALSE) {

                //
                // This page directory entry is empty, go to the next one.
                //

                Skipped = TRUE;
                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if (Va > EndingAddress) {

                    //
                    // All done, remove any straggling page directories and
                    // return.
                    //

                    return;
                }

#if (_MI_PAGING_LEVELS >= 3)
                if (MiIsPteOnPdeBoundary (PointerPde)) {
                    PointerPpe += 1;
                    ASSERT (PointerPpe == MiGetPteAddress (PointerPde));
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    goto restart2;
                }
#endif

#if DBG
                if ((LastProtoPte != NULL)  &&
                    (Vad->u2.VadFlags2.ExtendableFile == 0)) {
                    ProtoPte2 = MiGetProtoPteAddress(Vad, MI_VA_TO_VPN (Va));
                    Subsection = MiLocateSubsection (Vad,MI_VA_TO_VPN (Va));
                    LastProtoPte2 = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                    if (Vad->u.VadFlags.ImageMap != 1) {
                        if ((ProtoPte2 < Subsection->SubsectionBase) ||
                            (ProtoPte2 >= LastProtoPte2)) {
                            DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                                ProtoPte2,Va,Vad,Subsection);
                            DbgBreakPoint();
                        }
                    }
                }
#endif
            }
#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
            MI_CHECK_USED_PTES_HANDLE (Va);
            TempHandle = MI_GET_USED_PTES_HANDLE (Va);
            ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (TempHandle) != 0) ||
                    (PointerPte->u.Long == 0));
#endif

        } while (Waited != 0);

        //
        // The PPE and PDE are now valid, get the page table use address
        // as it changes whenever the PDE does.
        //

#if (_MI_PAGING_LEVELS >= 4)
        ASSERT64 (PointerPxe->u.Hard.Valid == 1);
#endif
        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);
        ASSERT (Va <= EndingAddress);

        MI_CHECK_USED_PTES_HANDLE (Va);
        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

        //
        // If we skipped chunks of address space, the prototype PTE pointer
        // must be updated now so VADs that span multiple subsections
        // are handled properly.
        //

        if ((Skipped == TRUE) && (LastProtoPte != NULL)) {

            ProtoPte = MiGetProtoPteAddress(Vad, MI_VA_TO_VPN(Va));
            Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(Va));

            if (Subsection != NULL) {
                LastProtoPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
#if DBG
                if (Vad->u.VadFlags.ImageMap != 1) {
                    if ((ProtoPte < Subsection->SubsectionBase) ||
                        (ProtoPte >= LastProtoPte)) {
                        DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                                    ProtoPte,Va,Vad,Subsection);
                        DbgBreakPoint();
                    }
                }
#endif
            }
            else {

                //
                // The Vad span is larger than the section being mapped.
                // Null the proto PTE local as no more proto PTEs will
                // need to be deleted at this point.
                //

                LastProtoPte = NULL;
            }
        }

        ASSERT (Va <= EndingAddress);

    } while (TRUE);
}


ULONG
MiDeletePte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN ULONG AddressSpaceDeletion,
    IN PEPROCESS CurrentProcess,
    IN PMMPTE PrototypePte,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL
    )

/*++

Routine Description:

    This routine deletes the contents of the specified PTE.  The PTE
    can be in one of the following states:

        - active and valid
        - transition
        - in paging file
        - in prototype PTE format

Arguments:

    PointerPte - Supplies a pointer to the PTE to delete.

    VirtualAddress - Supplies the virtual address which corresponds to
                     the PTE.  This is used to locate the working set entry
                     to eliminate it.

    AddressSpaceDeletion - Supplies TRUE if the address space is being
                           deleted, FALSE otherwise.  If TRUE is specified
                           the TB is not flushed and valid addresses are
                           not removed from the working set.


    CurrentProcess - Supplies a pointer to the current process.

    PrototypePte - Supplies a pointer to the prototype PTE which currently
                   or originally mapped this page.  This is used to determine
                   if the PTE is a fork PTE and should have its reference block
                   decremented.

Return Value:

    Nonzero if this routine released mutexes and locks, FALSE if not.

Environment:

    Kernel mode, APCs disabled, PFN lock and working set mutex held.

--*/

{
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    MMPTE PteContents;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    WSLE_NUMBER WsPfnIndex;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    ULONG Waited;
    ULONG DroppedLocks;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;

    MM_PFN_LOCK_ASSERT();

    DroppedLocks = 0;

#if DBG
    if (MmDebug & MM_DBG_PTE_UPDATE) {
        DbgPrint("deleting PTE\n");
        MiFormatPte(PointerPte);
    }
#endif

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 1) {

#ifdef _X86_
#if DBG
#if !defined(NT_UP)

        if (PteContents.u.Hard.Writable == 1) {
            ASSERT (PteContents.u.Hard.Dirty == 1);
        }
#endif //NTUP
#endif //DBG
#endif //X86

        //
        // PTE is valid.  Check PFN database to see if this is a prototype PTE.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        WsPfnIndex = Pfn1->u1.WsIndex;

#if DBG
        if (MmDebug & MM_DBG_PTE_UPDATE) {
            MiFormatPfn(Pfn1);
        }
#endif

        CloneDescriptor = NULL;

        if (Pfn1->u3.e1.PrototypePte == 1) {

            CloneBlock = (PMMCLONE_BLOCK)Pfn1->PteAddress;

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            PointerPde = MiGetPteAddress (PointerPte);
            if (PointerPde->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                                  0x61940, 
                                  (ULONG_PTR)PointerPte,
                                  (ULONG_PTR)PointerPde->u.Long,
                                  (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
                }
#endif
            }

            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(PointerPde);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

            MiDecrementShareCountInline (Pfn1, PageFrameIndex);

            //
            // Check to see if this is a fork prototype PTE and if so
            // update the clone descriptor address.
            //

            if (PointerPte <= MiHighestUserPte) {

                if (PrototypePte != Pfn1->PteAddress) {

                    //
                    // Locate the clone descriptor within the clone tree.
                    //

                    CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);

#if DBG
                    if (CloneDescriptor == NULL) {
                        DbgPrint("1PrototypePte %p Clone desc %p pfn PTE addr %p\n",
                        PrototypePte, CloneDescriptor, Pfn1->PteAddress);
                        MiFormatPte(PointerPte);
                        ASSERT (FALSE);
                    }
#endif

                }
            }
        }
        else {

            //
            // Initializing CloneBlock & PointerPde is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            CloneBlock = NULL;
            PointerPde = NULL;

            ASSERT (Pfn1->u2.ShareCount == 1);

            //
            // This PTE is a NOT a prototype PTE, delete the physical page.
            //

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            MiDecrementShareCountInline (MI_PFN_ELEMENT(Pfn1->u4.PteFrame),
                                         Pfn1->u4.PteFrame);

            MI_SET_PFN_DELETED (Pfn1);

            //
            // Decrement the share count for the physical page.  As the page
            // is private it will be put on the free list.
            //

            MiDecrementShareCountOnly (PageFrameIndex);

            //
            // Decrement the count for the number of private pages.
            //

            CurrentProcess->NumberOfPrivatePages -= 1;
        }

        //
        // Find the WSLE for this page and eliminate it.
        //

        //
        // If we are deleting the system portion of the address space, do
        // not remove WSLEs or flush translation buffers as there can be
        // no other usage of this address space.
        //

        if (AddressSpaceDeletion == FALSE) {

            WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                            MmWorkingSetList,
                                            WsPfnIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            //
            // Check to see if this entry is locked in the working set
            // or locked in memory.
            //

            Locked = MmWsle[WorkingSetIndex].u1.e1;

            MiRemoveWsle (WorkingSetIndex, MmWorkingSetList);

            //
            // Add this entry to the list of free working set entries
            // and adjust the working set count.
            //

            MiReleaseWsle (WorkingSetIndex, &CurrentProcess->Vm);

            if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

                //
                // This entry is locked.
                //

                ASSERT (WorkingSetIndex < MmWorkingSetList->FirstDynamic);
                MmWorkingSetList->FirstDynamic -= 1;

                if (WorkingSetIndex != MmWorkingSetList->FirstDynamic) {

                    Entry = MmWorkingSetList->FirstDynamic;
                    ASSERT (MmWsle[Entry].u1.e1.Valid);
#if 0
                    SwapVa = MmWsle[Entry].u1.VirtualAddress;
                    SwapVa = PAGE_ALIGN (SwapVa);
                    Pfn2 = MI_PFN_ELEMENT (
                              MiGetPteAddress (SwapVa)->u.Hard.PageFrameNumber);
                    Entry = MiLocateWsleAndParent (SwapVa,
                                                   &Parent,
                                                   MmWorkingSetList,
                                                   Pfn2->u1.WsIndex);

                    //
                    // Swap the removed entry with the last locked entry
                    // which is located at first dynamic.
                    //

                    MiSwapWslEntries (Entry,
                                      Parent,
                                      WorkingSetIndex,
                                      MmWorkingSetList);
#endif //0

                    MiSwapWslEntries (Entry,
                                      WorkingSetIndex,
                                      &CurrentProcess->Vm);
                }
            }
            else {
                ASSERT (WorkingSetIndex >= MmWorkingSetList->FirstDynamic);
            }

            //
            // Flush the entry out of the TB.
            //

            if (!ARGUMENT_PRESENT (PteFlushList)) {
                KeFlushSingleTb (VirtualAddress,
                                 TRUE,
                                 FALSE,
                                 (PHARDWARE_PTE)PointerPte,
                                 ZeroPte.u.Flush);
            }
            else {
                if (PteFlushList->Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList->FlushPte[PteFlushList->Count] = PointerPte;
                    PteFlushList->FlushVa[PteFlushList->Count] = VirtualAddress;
                    PteFlushList->Count += 1;
                }
                MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
            }

            if (CloneDescriptor != NULL) {

                //
                // Flush PTEs as this could release the PFN lock.
                //

                if (ARGUMENT_PRESENT (PteFlushList)) {
                    MiFlushPteList (PteFlushList, FALSE, ZeroPte);
                }

                //
                // Decrement the reference count for the clone block,
                // note that this could release and reacquire
                // the mutexes hence cannot be done until after the
                // working set index has been removed.
                //

                if (MiDecrementCloneBlockReference (CloneDescriptor,
                                                    CloneBlock,
                                                    CurrentProcess)) {

                    //
                    // The working set mutex was released.  This may
                    // have removed the current page directory & table page.
                    //

                    DroppedLocks = 1;

                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);

                    do {

                        MiDoesPxeExistAndMakeValid (PointerPxe,
                                                    CurrentProcess,
                                                    TRUE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS >= 4)
                        Waited = 0;
#endif

                        MiDoesPpeExistAndMakeValid (PointerPpe,
                                                    CurrentProcess,
                                                    TRUE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS < 4)
                        Waited = 0;
#endif

                        //
                        // If the call below results in a PFN release and
                        // reacquire, then we must redo them both.
                        //

                        MiDoesPdeExistAndMakeValid (PointerPde,
                                                    CurrentProcess,
                                                    TRUE,
                                                    &Waited);

                    } while (Waited != 0);
                }
            }
        }

    }
    else if (PteContents.u.Soft.Prototype == 1) {

        //
        // This is a prototype PTE, if it is a fork PTE clean up the
        // fork structures.
        //

        if (PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {

            //
            // Check to see if the prototype PTE is a fork prototype PTE.
            //

            if (PointerPte <= MiHighestUserPte) {

                if (PrototypePte != MiPteToProto (PointerPte)) {

                    CloneBlock = (PMMCLONE_BLOCK)MiPteToProto (PointerPte);
                    CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);


#if DBG
                    if (CloneDescriptor == NULL) {
                        DbgPrint("1PrototypePte %p Clone desc %p \n",
                            PrototypePte, CloneDescriptor);
                        MiFormatPte(PointerPte);
                        ASSERT (FALSE);
                    }
#endif

                    //
                    // Decrement the reference count for the clone block,
                    // note that this could release and reacquire
                    // the mutexes.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

                    if (ARGUMENT_PRESENT (PteFlushList)) {
                        MiFlushPteList (PteFlushList, FALSE, ZeroPte);
                    }

                    if (MiDecrementCloneBlockReference ( CloneDescriptor,
                                                         CloneBlock,
                                                         CurrentProcess )) {

                        //
                        // The working set mutex was released.  This may
                        // have removed the current page directory & table page.
                        //

                        DroppedLocks = 1;

                        PointerPde = MiGetPteAddress (PointerPte);
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPdeAddress (PointerPde);

                        //
                        // If either call below results in a PFN release and
                        // reacquire, then we must redo them both.
                        //

                        do {

                            if (MiDoesPxeExistAndMakeValid (PointerPxe,
                                                        CurrentProcess,
                                                        TRUE,
                                                        &Waited) == FALSE) {

                                //
                                // The PXE has been deleted when the PFN lock
                                // was released.  Just bail as the PPE/PDE/PTE
                                // are gone now anyway.
                                //

                                return DroppedLocks;
                            }

#if (_MI_PAGING_LEVELS >= 4)
                            Waited = 0;
#endif

                            if (MiDoesPpeExistAndMakeValid (PointerPpe,
                                                        CurrentProcess,
                                                        TRUE,
                                                        &Waited) == FALSE) {

                                //
                                // The PPE has been deleted when the PFN lock
                                // was released.  Just bail as the PDE/PTE are
                                // gone now anyway.
                                //

                                return DroppedLocks;
                            }

#if (_MI_PAGING_LEVELS < 4)
                            Waited = 0;
#endif

                            //
                            // If the call below results in a PFN release and
                            // reacquire, then we must redo them both.  If the
                            // PDE was deleted when the PFN lock was released
                            // then we just bail as the PTE is gone anyway.
                            //

                            if (MiDoesPdeExistAndMakeValid (PointerPde,
                                                        CurrentProcess,
                                                        TRUE,
                                                        &Waited) == FALSE) {
                                return DroppedLocks;
                            }

                        } while (Waited != 0);
                    }
                }
            }
        }

    }
    else if (PteContents.u.Soft.Transition == 1) {

        //
        // This is a transition PTE. (Page is private)
        //

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        MI_SET_PFN_DELETED (Pfn1);

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        //
        // Check the reference count for the page, if the reference
        // count is zero, move the page to the free list, if the reference
        // count is not zero, ignore this page.  When the reference count
        // goes to zero, it will be placed on the free list.
        //

        if (Pfn1->u3.e2.ReferenceCount == 0) {
            MiUnlinkPageFromList (Pfn1);
            MiReleasePageFileSpace (Pfn1->OriginalPte);
            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
        }

        //
        // Decrement the count for the number of private pages.
        //

        CurrentProcess->NumberOfPrivatePages -= 1;

    }
    else {

        //
        // Must be page file space.
        //

        if (PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {

            if (MiReleasePageFileSpace (*PointerPte)) {

                //
                // Decrement the count for the number of private pages.
                //

                CurrentProcess->NumberOfPrivatePages -= 1;
            }
        }
    }

    //
    // Zero the PTE contents.
    //

    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

    return DroppedLocks;
}


ULONG
FASTCALL
MiReleasePageFileSpace (
    IN MMPTE PteContents
    )

/*++

Routine Description:

    This routine frees the paging file allocated to the specified PTE.

Arguments:

    PteContents - Supplies the PTE which is in page file format.

Return Value:

    Returns TRUE if any paging file space was deallocated.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    ULONG FreeBit;
    ULONG PageFileNumber;
    PMMPAGING_FILE PageFile;

    MM_PFN_LOCK_ASSERT();

    if (PteContents.u.Soft.Prototype == 1) {

        //
        // Not in page file format.
        //

        return FALSE;
    }

    FreeBit = GET_PAGING_FILE_OFFSET (PteContents);

    if ((FreeBit == 0) || (FreeBit == MI_PTE_LOOKUP_NEEDED)) {

        //
        // Page is not in a paging file, just return.
        //

        return FALSE;
    }

    PageFileNumber = GET_PAGING_FILE_NUMBER (PteContents);

    ASSERT (RtlCheckBit( MmPagingFile[PageFileNumber]->Bitmap, FreeBit) == 1);

#if DBG
    if ((FreeBit < 8192) && (PageFileNumber == 0)) {
        ASSERT ((MmPagingFileDebug[FreeBit] & 1) != 0);
        MmPagingFileDebug[FreeBit] ^= 1;
    }
#endif

    PageFile = MmPagingFile[PageFileNumber];
    MI_CLEAR_BIT (PageFile->Bitmap->Buffer, FreeBit);

    PageFile->FreeSpace += 1;
    PageFile->CurrentUsage -= 1;

    //
    // Check to see if we should move some MDL entries for the
    // modified page writer now that more free space is available.
    //

    if ((MmNumberOfActiveMdlEntries == 0) ||
        (PageFile->FreeSpace == MM_USABLE_PAGES_FREE)) {

        MiUpdateModifiedWriterMdls (PageFileNumber);
    }

    return TRUE;
}


VOID
FASTCALL
MiReleaseConfirmedPageFileSpace (
    IN MMPTE PteContents
    )

/*++

Routine Description:

    This routine frees the paging file allocated to the specified PTE.

Arguments:

    PteContents - Supplies the PTE which is in page file format.

Return Value:

    Returns TRUE if any paging file space was deallocated.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    ULONG FreeBit;
    ULONG PageFileNumber;
    PMMPAGING_FILE PageFile;

    MM_PFN_LOCK_ASSERT();

    ASSERT (PteContents.u.Soft.Prototype == 0);

    FreeBit = GET_PAGING_FILE_OFFSET (PteContents);

    ASSERT ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED));

    PageFileNumber = GET_PAGING_FILE_NUMBER (PteContents);

    PageFile = MmPagingFile[PageFileNumber];

    ASSERT (RtlCheckBit( PageFile->Bitmap, FreeBit) == 1);

#if DBG
    if ((FreeBit < 8192) && (PageFileNumber == 0)) {
        ASSERT ((MmPagingFileDebug[FreeBit] & 1) != 0);
        MmPagingFileDebug[FreeBit] ^= 1;
    }
#endif

    MI_CLEAR_BIT (PageFile->Bitmap->Buffer, FreeBit);

    PageFile->FreeSpace += 1;
    PageFile->CurrentUsage -= 1;

    //
    // Check to see if we should move some MDL entries for the
    // modified page writer now that more free space is available.
    //

    if ((MmNumberOfActiveMdlEntries == 0) ||
        (PageFile->FreeSpace == MM_USABLE_PAGES_FREE)) {

        MiUpdateModifiedWriterMdls (PageFileNumber);
    }
}


VOID
FASTCALL
MiUpdateModifiedWriterMdls (
    IN ULONG PageFileNumber
    )

/*++

Routine Description:

    This routine ensures the MDLs for the specified paging file
    are in the proper state such that paging i/o can continue.

Arguments:

    PageFileNumber - Supplies the page file number to check the MDLs for.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    ULONG i;
    PMMMOD_WRITER_MDL_ENTRY WriterEntry;

    //
    // Put the MDL entries into the active list.
    //

    for (i = 0; i < MM_PAGING_FILE_MDLS; i += 1) {

        if ((MmPagingFile[PageFileNumber]->Entry[i]->Links.Flink !=
                                                    MM_IO_IN_PROGRESS)
                          &&
            (MmPagingFile[PageFileNumber]->Entry[i]->CurrentList ==
                    &MmFreePagingSpaceLow)) {

            //
            // Remove this entry and put it on the active list.
            //

            WriterEntry = MmPagingFile[PageFileNumber]->Entry[i];
            RemoveEntryList (&WriterEntry->Links);
            WriterEntry->CurrentList = &MmPagingFileHeader.ListHead;

            KeSetEvent (&WriterEntry->PagingListHead->Event, 0, FALSE);

            InsertTailList (&WriterEntry->PagingListHead->ListHead,
                            &WriterEntry->Links);
            MmNumberOfActiveMdlEntries += 1;
        }
    }

    return;
}

VOID
MiFlushPteList (
    IN PMMPTE_FLUSH_LIST PteFlushList,
    IN ULONG AllProcessors,
    IN MMPTE FillPte
    )

/*++

Routine Description:

    This routine flushes all the PTEs in the PTE flush list.
    If the list has overflowed, the entire TB is flushed.

Arguments:

    PteFlushList - Supplies an optional pointer to the list to be flushed.

    AllProcessors - Supplies TRUE if the flush occurs on all processors.

    FillPte - Supplies the PTE to fill with.

Return Value:

    None.

Environment:

    Kernel mode, PFN or a pre-process AWE lock may optionally be held.

--*/

{
    ULONG count;

    ASSERT (ARGUMENT_PRESENT (PteFlushList));

    count = PteFlushList->Count;

    if (count != 0) {
        if (count != 1) {
            if (count < MM_MAXIMUM_FLUSH_COUNT) {
                KeFlushMultipleTb (count,
                                   &PteFlushList->FlushVa[0],
                                   TRUE,
                                   (BOOLEAN)AllProcessors,
                                   &((PHARDWARE_PTE)PteFlushList->FlushPte[0]),
                                   FillPte.u.Flush);
            }
            else {

                //
                // Array has overflowed, flush the entire TB.
                //

                KeFlushEntireTb (TRUE, (BOOLEAN)AllProcessors);
            }
        }
        else {
            KeFlushSingleTb (PteFlushList->FlushVa[0],
                             TRUE,
                             (BOOLEAN)AllProcessors,
                             (PHARDWARE_PTE)PteFlushList->FlushPte[0],
                             FillPte.u.Flush);
        }
        PteFlushList->Count = 0;
    }
    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\creasect.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   creasect.c

Abstract:

    This module contains the routines which implement the
    NtCreateSection and NtOpenSection.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

const ULONG MMCONTROL = 'aCmM';
const ULONG MMTEMPORARY = 'xxmM';

#define MM_SIZE_OF_LARGEST_IMAGE ((ULONG)0x77000000)

#define MM_MAXIMUM_IMAGE_HEADER (2 * PAGE_SIZE)

extern SIZE_T MmAllocationFragment;

//
// The maximum number of image object (object table entries) is
// the number which will fit into the MM_MAXIMUM_IMAGE_HEADER with
// the start of the PE image header in the last word of the first
//

#define MM_MAXIMUM_IMAGE_SECTIONS                       \
     ((MM_MAXIMUM_IMAGE_HEADER - (PAGE_SIZE + sizeof(IMAGE_NT_HEADERS))) /  \
            sizeof(IMAGE_SECTION_HEADER))

#if DBG
extern PEPROCESS MmWatchProcess;
ULONG MiMakeImageFloppy[2];
#endif

extern POBJECT_TYPE IoFileObjectType;

CCHAR MmImageProtectionArray[16] = {
                                    MM_NOACCESS,
                                    MM_EXECUTE,
                                    MM_READONLY,
                                    MM_EXECUTE_READ,
                                    MM_WRITECOPY,
                                    MM_EXECUTE_WRITECOPY,
                                    MM_WRITECOPY,
                                    MM_EXECUTE_WRITECOPY,
                                    MM_NOACCESS,
                                    MM_EXECUTE,
                                    MM_READONLY,
                                    MM_EXECUTE_READ,
                                    MM_READWRITE,
                                    MM_EXECUTE_READWRITE,
                                    MM_READWRITE,
                                    MM_EXECUTE_READWRITE };


CCHAR
MiGetImageProtection (
    IN ULONG SectionCharacteristics
    );

NTSTATUS
MiVerifyImageHeader (
    IN PIMAGE_NT_HEADERS NtHeader,
    IN PIMAGE_DOS_HEADER DosHeader,
    IN ULONG NtHeaderSize
    );

LOGICAL
MiCheckDosCalls (
    IN PIMAGE_OS2_HEADER Os2Header,
    IN ULONG HeaderSize
    );

PCONTROL_AREA
MiFindImageSectionObject(
    IN PFILE_OBJECT File,
    IN PLOGICAL GlobalNeeded
    );

VOID
MiInsertImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA ControlArea
    );

LOGICAL
MiFlushDataSection(
    IN PFILE_OBJECT File
    );

PVOID
MiCopyHeaderIfResident (
    IN PFILE_OBJECT File,
    IN PFN_NUMBER ImagePageFrameNumber
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiCreateImageFileMap)
#pragma alloc_text(PAGE,NtCreateSection)
#pragma alloc_text(PAGE,NtOpenSection)
#pragma alloc_text(PAGE,MiGetImageProtection)
#pragma alloc_text(PAGE,MiVerifyImageHeader)
#pragma alloc_text(PAGE,MiCheckDosCalls)
#pragma alloc_text(PAGE,MiCreatePagingFileMap)
#pragma alloc_text(PAGE,MiCreateDataFileMap)

#pragma alloc_text(PAGE,MiGetWritablePagesInSection)
#endif

#pragma pack (1)
typedef struct _PHARLAP_CONFIG {
    UCHAR  uchCopyRight[0x32];
    USHORT usType;
    USHORT usRsv1;
    USHORT usRsv2;
    USHORT usSign;
} CONFIGPHARLAP, *PCONFIGPHARLAP;
#pragma pack ()


NTSTATUS
NtCreateSection (
    OUT PHANDLE SectionHandle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes OPTIONAL,
    IN PLARGE_INTEGER MaximumSize OPTIONAL,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN HANDLE FileHandle OPTIONAL
     )

/*++

Routine Description:

    This function creates a section object and opens a handle to the object
    with the specified desired access.

Arguments:

    SectionHandle - A pointer to a variable that will
        receive the section object handle value.

    DesiredAccess - The desired types of access for the section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is
              desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.


    ObjectAttributes - Supplies a pointer to an object attributes structure.

    MaximumSize - Supplies the maximum size of the section in bytes.
         This value is rounded up to the host page size and
         specifies the size of the section (page file
         backed section) or the maximum size to which a
         file can be extended or mapped (file backed
         section).

    SectionPageProtection - Supplies the protection to place on each page
         in the section.  One of PAGE_READ, PAGE_READWRITE, PAGE_EXECUTE,
         or PAGE_WRITECOPY and, optionally, PAGE_NOCACHE may be specified.

    AllocationAttributes - Supplies a set of flags that describe the
         allocation attributes of the section.

        AllocationAttributes Flags

        SEC_BASED - The section is a based section and will be
            allocated at the same virtual address in each process
            address space that receives the section.  This does not
            imply that addresses are reserved for based sections.
            Rather if the section cannot be mapped at the based address
            an error is returned.


        SEC_RESERVE - All pages of the section are set to the
            reserved state.

        SEC_COMMIT - All pages of the section are set to the commit
            state.

        SEC_IMAGE - The file specified by the file handle is an
                    executable image file.

        SEC_FILE - The file specified by the file handle is a mapped
                   file.  If a file handle is supplied and neither
                   SEC_IMAGE or SEC_FILE is supplied, SEC_FILE is
                   assumed.

        SEC_NO_CHANGE - Once the file is mapped, the protection cannot
                        be changed nor can the view be unmapped.  The
                        view is unmapped when the process is deleted.
                        Cannot be used with SEC_IMAGE.

    FileHandle - Supplies an optional handle of an open file object.
         If the value of this handle is null, then the
         section will be backed by a paging file. Otherwise
         the section is backed by the specified data file.

Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    PVOID Section;
    HANDLE Handle;
    LARGE_INTEGER LargeSize;
    LARGE_INTEGER CapturedSize;
    ULONG RetryCount;
    PCONTROL_AREA ControlArea;

    if ((AllocationAttributes & ~(SEC_COMMIT | SEC_RESERVE | SEC_BASED |
            SEC_IMAGE | SEC_NOCACHE | SEC_NO_CHANGE)) != 0) {
        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & (SEC_COMMIT | SEC_RESERVE | SEC_IMAGE)) == 0) {
        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & SEC_IMAGE) &&
            (AllocationAttributes & (SEC_COMMIT | SEC_RESERVE |
                            SEC_NOCACHE | SEC_NO_CHANGE))) {

        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & SEC_COMMIT) &&
            (AllocationAttributes & SEC_RESERVE)) {
        return STATUS_INVALID_PARAMETER_6;
    }

    //
    // Check the SectionProtection Flag.
    //

    if ((SectionPageProtection & PAGE_NOCACHE) ||
        (SectionPageProtection & PAGE_GUARD) ||
        (SectionPageProtection & PAGE_NOACCESS)) {

        //
        // No cache is only specified through SEC_NOCACHE option in the
        // allocation attributes.
        //

        return STATUS_INVALID_PAGE_PROTECTION;
    }


    if (KeGetPreviousMode() != KernelMode) {
        try {
            ProbeForWriteHandle(SectionHandle);

            if (ARGUMENT_PRESENT (MaximumSize)) {

#if !defined (_WIN64)

                //
                // Note we only probe for byte alignment because prior to 2195,
                // we never probed at all!  We don't want to break user apps
                // that had bad alignment if they worked before.
                //

                ProbeForReadSmallStructure(MaximumSize, sizeof(LARGE_INTEGER), sizeof(UCHAR));
#else
                ProbeForReadSmallStructure(MaximumSize, sizeof(LARGE_INTEGER), sizeof(LARGE_INTEGER));
#endif
                LargeSize = *MaximumSize;
            }
            else {
                ZERO_LARGE (LargeSize);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            return GetExceptionCode();
        }
    }
    else {
        if (ARGUMENT_PRESENT (MaximumSize)) {
            LargeSize = *MaximumSize;
        }
        else {
            ZERO_LARGE (LargeSize);
        }
    }

    RetryCount = 0;

retry:

    CapturedSize = LargeSize;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    Status = MmCreateSection ( &Section,
                               DesiredAccess,
                               ObjectAttributes,
                               &CapturedSize,
                               SectionPageProtection,
                               AllocationAttributes,
                               FileHandle,
                               NULL );


    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    if (!NT_SUCCESS(Status)) {
        if ((Status == STATUS_FILE_LOCK_CONFLICT) &&
            (RetryCount < 3)) {

            //
            // The file system may have prevented this from working
            // due to log file flushing.  Delay and try again.
            //

            RetryCount += 1;

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmHalfSecond);

            goto retry;

        }
        return Status;
    }

    ControlArea = ((PSECTION)Section)->Segment->ControlArea;

#if DBG
    if (MmDebug & MM_DBG_SECTIONS) {
        DbgPrint ("inserting section %p control %p\n", Section, ControlArea);
    }
#endif

    if ((ControlArea != NULL) && (ControlArea->FilePointer != NULL)) {
        CcZeroEndOfLastPage (ControlArea->FilePointer);
    }

    //
    // Note if the insertion fails, Ob will dereference the object for us.
    //

    Status = ObInsertObject (Section,
                             NULL,
                             DesiredAccess,
                             0,
                             (PVOID *)NULL,
                             &Handle);

    if (NT_SUCCESS(Status)) {
        try {
            *SectionHandle = Handle;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If the write attempt fails, then do not report an error.
            // When the caller attempts to access the handle value,
            // an access violation will occur.
            //
        }
    }

    return Status;
}

NTSTATUS
MmCreateSection (
    OUT PVOID *SectionObject,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes OPTIONAL,
    IN PLARGE_INTEGER InputMaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN HANDLE FileHandle OPTIONAL,
    IN PFILE_OBJECT FileObject OPTIONAL
    )

/*++

Routine Description:

    This function creates a section object and opens a handle to the object
    with the specified desired access.

Arguments:

    Section - A pointer to a variable that will
              receive the section object address.

    DesiredAccess - The desired types of access for the section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.

    ObjectAttributes - Supplies a pointer to an object attributes structure.

    InputMaximumSize - Supplies the maximum size of the section in bytes.
                       This value is rounded up to the host page size and
                       specifies the size of the section (page file
                       backed section) or the maximum size to which a
                       file can be extended or mapped (file backed
                       section).

    SectionPageProtection - Supplies the protection to place on each page
                            in the section.  One of PAGE_READ, PAGE_READWRITE,
                            PAGE_EXECUTE, or PAGE_WRITECOPY and, optionally,
                            PAGE_NOCACHE may be specified.

    AllocationAttributes - Supplies a set of flags that describe the
                           allocation attributes of the section.

        AllocationAttributes Flags

        SEC_BASED - The section is a based section and will be
                    allocated at the same virtual address in each process
                    address space that receives the section.  This does not
                    imply that addresses are reserved for based sections.
                    Rather if the section cannot be mapped at the based address
                    an error is returned.

        SEC_RESERVE - All pages of the section are set to the
                      reserved state.

        SEC_COMMIT - All pages of the section are set to the commit state.

        SEC_IMAGE - The file specified by the file handle is an
                    executable image file.

        SEC_FILE - The file specified by the file handle is a mapped
                   file.  If a file handle is supplied and neither
                   SEC_IMAGE or SEC_FILE is supplied, SEC_FILE is
                   assumed.

    FileHandle - Supplies an optional handle of an open file object.
                 If the value of this handle is null, then the
                 section will be backed by a paging file. Otherwise
                 the section is backed by the specified data file.

    FileObject - Supplies an optional pointer to the file object.  If this
                 value is NULL and the FileHandle is NULL, then there is
                 no file to map (image or mapped file).  If this value
                 is specified, then the File is to be mapped as a MAPPED FILE
                 and NO file size checking will be performed.

                 ONLY THE SYSTEM CACHE SHOULD PROVIDE A FILE OBJECT WITH THE
                 CALL!! as this is optimized to not check the size, only do
                 data mapping, no protection check, etc.

    Note - Only one of FileHandle or File should be specified!

Return Value:

    Returns the relevant NTSTATUS code.

--*/

{
    SECTION Section;
    PSECTION NewSection;
    PSEGMENT Segment;
    PSEGMENT NewSegment;
    KPROCESSOR_MODE PreviousMode;
    KIRQL OldIrql;
    NTSTATUS Status;
    NTSTATUS Status2;
    PCONTROL_AREA ControlArea;
    PCONTROL_AREA NewControlArea;
    PCONTROL_AREA SegmentControlArea;
    ACCESS_MASK FileDesiredAccess;
    PFILE_OBJECT File;
    PEVENT_COUNTER Event;
    ULONG IgnoreFileSizing;
    ULONG ProtectionMask;
    ULONG ProtectMaskForAccess;
    ULONG FileAcquired;
    PEVENT_COUNTER SegmentEvent;
    LOGICAL FileSizeChecked;
    LARGE_INTEGER TempSectionSize;
    UINT64 EndOfFile;
    ULONG IncrementedRefCount;
    SIZE_T ControlAreaSize;
    PUINT64 MaximumSize;
    PMMADDRESS_NODE *SectionBasedRoot;
    LOGICAL GlobalNeeded;
    PFILE_OBJECT ChangeFileReference;
    SIZE_T SizeOfSection;
#if DBG
    PVOID PreviousSectionPointer;

    PreviousSectionPointer = (PVOID)-1;
#endif

    NewControlArea = (PCONTROL_AREA)-1;

    UNREFERENCED_PARAMETER (DesiredAccess);

    IgnoreFileSizing = FALSE;
    FileAcquired = FALSE;
    FileSizeChecked = FALSE;
    IncrementedRefCount = FALSE;
    ChangeFileReference = NULL;

    MaximumSize = (PUINT64) InputMaximumSize;

    //
    // Check allocation attributes flags.
    //

    File = (PFILE_OBJECT)NULL;

    ASSERT ((AllocationAttributes & ~(SEC_COMMIT | SEC_RESERVE | SEC_BASED |
            SEC_IMAGE | SEC_NOCACHE | SEC_NO_CHANGE)) == 0);

    ASSERT ((AllocationAttributes & (SEC_COMMIT | SEC_RESERVE | SEC_IMAGE)) != 0);

    ASSERT (!((AllocationAttributes & SEC_IMAGE) &&
            (AllocationAttributes & (SEC_COMMIT | SEC_RESERVE |
                            SEC_NOCACHE | SEC_NO_CHANGE))));

    ASSERT (!((AllocationAttributes & SEC_COMMIT) &&
            (AllocationAttributes & SEC_RESERVE)));

    ASSERT (!((SectionPageProtection & PAGE_NOCACHE) ||
        (SectionPageProtection & PAGE_GUARD) ||
        (SectionPageProtection & PAGE_NOACCESS)));

    if (AllocationAttributes & SEC_NOCACHE) {
        SectionPageProtection |= PAGE_NOCACHE;
    }

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (SectionPageProtection);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ProtectMaskForAccess = ProtectionMask & 0x7;

    FileDesiredAccess = MmMakeFileAccess[ProtectMaskForAccess];

    //
    // Get previous processor mode and probe output arguments if necessary.
    //

    PreviousMode = KeGetPreviousMode();

    Section.InitialPageProtection = SectionPageProtection;
    Section.Segment = (PSEGMENT)NULL;

    //
    // Initializing Segment is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    Segment = (PSEGMENT)-1;

    if (ARGUMENT_PRESENT(FileHandle) || ARGUMENT_PRESENT(FileObject)) {

        //
        // Only one of FileHandle or FileObject should be supplied.
        // If a FileObject is supplied, this must be from the
        // file system and therefore the file's size should not
        // be checked.
        //

        if (ARGUMENT_PRESENT(FileObject)) {
            IgnoreFileSizing = TRUE;
            File = FileObject;

            //
            // Quick check to see if a control area already exists.
            //

            if (File->SectionObjectPointer->DataSectionObject) {

                LOCK_PFN (OldIrql);
                ControlArea =
                    (PCONTROL_AREA)(File->SectionObjectPointer->DataSectionObject);

                if ((ControlArea != NULL) &&
                    (!ControlArea->u.Flags.BeingDeleted) &&
                    (!ControlArea->u.Flags.BeingCreated)) {

                    //
                    // Control area exists and is not being deleted,
                    // reference it.
                    //

                    NewSegment = ControlArea->Segment;
                    if ((ControlArea->NumberOfSectionReferences == 0) &&
                        (ControlArea->NumberOfMappedViews == 0) &&
                        (ControlArea->ModifiedWriteCount == 0)) {

                        //
                        // Dereference the current file object (after releasing
                        // the PFN lock) and reference this one.
                        //

                        ASSERT (ControlArea->FilePointer != NULL);
                        ChangeFileReference = ControlArea->FilePointer;
                        ControlArea->FilePointer = FileObject;
                    }
                    ControlArea->u.Flags.Accessed = 1;
                    ControlArea->NumberOfSectionReferences += 1;
                    if (ControlArea->DereferenceList.Flink != NULL) {

                        //
                        // Remove this from the list of unused segments.
                        //

                        RemoveEntryList (&ControlArea->DereferenceList);

                        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

                        ControlArea->DereferenceList.Flink = NULL;
                        ControlArea->DereferenceList.Blink = NULL;
                    }
                    UNLOCK_PFN (OldIrql);

                    //
                    // Inform the object manager to defer this deletion by
                    // queueing it to another thread to eliminate deadlocks
                    // with the redirector.
                    //

                    if (ChangeFileReference != NULL) {
                        ObDereferenceObjectDeferDelete (ChangeFileReference);
                    }

                    IncrementedRefCount = TRUE;
                    Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;

                    goto ReferenceObject;
                }
                UNLOCK_PFN (OldIrql);
            }

            ObReferenceObject (FileObject);

        }
        else {

            Status = ObReferenceObjectByHandle ( FileHandle,
                                                 FileDesiredAccess,
                                                 IoFileObjectType,
                                                 PreviousMode,
                                                 (PVOID *)&File,
                                                 NULL );
            if (!NT_SUCCESS(Status)) {
                return Status;
            }

            //
            // If this file doesn't have a section object pointer,
            // return an error.
            //

            if (File->SectionObjectPointer == NULL) {
                ObDereferenceObject (File);
                return STATUS_INVALID_FILE_FOR_SECTION;
            }
        }

        //
        // Check to see if the specified file already has a section.
        // If not, indicate in the file object's pointer to an FCB that
        // a section is being built.  This synchronizes segment creation
        // for the file.
        //

        if (AllocationAttributes & SEC_IMAGE) {

            //
            // This control area is always just a place holder - the real one
            // is allocated in MiCreateImageFileMap and will be allocated
            // with the correct size and this one freed in a short while.
            //
            // This place holder must always be allocated as a large control
            // area so that it can be chained for the per-session case.
            //

            ControlAreaSize = sizeof(LARGE_CONTROL_AREA) + sizeof(SUBSECTION);
        }
        else {

            //
            // Data files are mapped with larger subsections than images or
            // pagefile-backed shared memory.  Factor that in here.
            //

            ControlAreaSize = sizeof(CONTROL_AREA) + sizeof(MSUBSECTION);
        }

        NewControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                                ControlAreaSize,
                                                MMCONTROL);

        if (NewControlArea == NULL) {
            ObDereferenceObject (File);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        RtlZeroMemory (NewControlArea, ControlAreaSize);

        NewSegment = NULL;

        //
        // We only need the file resource if this was a user request, i.e. not
        // a call from the cache manager or file system.
        //

        if (ARGUMENT_PRESENT(FileHandle)) {

            Status = FsRtlAcquireToCreateMappedSection (File, SectionPageProtection);

            if (!NT_SUCCESS(Status)) {
                ExFreePool (NewControlArea);
                ObDereferenceObject (File);
                return Status;
            }

            IoSetTopLevelIrp((PIRP)FSRTL_FSP_TOP_LEVEL_IRP);
            FileAcquired = TRUE;
        }

        //
        // Initializing GlobalNeeded is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        GlobalNeeded = FALSE;

        //
        // Allocate an event to wait on in case the segment is in the
        // process of being deleted.  This event cannot be allocated
        // with the PFN database locked as pool expansion would deadlock.
        //

ReallocateandcheckSegment:

        SegmentEvent = MiGetEventCounter();

        if (SegmentEvent == NULL) {
            if (FileAcquired) {
                IoSetTopLevelIrp((PIRP)NULL);
                FsRtlReleaseFile (File);
            }
            ExFreePool (NewControlArea);
            ObDereferenceObject (File);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

RecheckSegment:

        LOCK_PFN (OldIrql);

        if (AllocationAttributes & SEC_IMAGE) {

            ControlArea = MiFindImageSectionObject (File, &GlobalNeeded);

        }
        else {
            ControlArea =
               (PCONTROL_AREA)(File->SectionObjectPointer->DataSectionObject);
        }

        if (ControlArea != NULL) {

            //
            // A segment already exists for this file.  Make sure that it
            // is not in the process of being deleted, or being created.
            //


            if ((ControlArea->u.Flags.BeingDeleted) ||
                (ControlArea->u.Flags.BeingCreated)) {

                //
                // The segment object is in the process of being deleted or
                // created.
                // Check to see if another thread is waiting for the deletion,
                // otherwise create an event object to wait upon.
                //

                if (ControlArea->WaitingForDeletion == NULL) {

                    //
                    // Initialize an event and put its address in the control area.
                    //

                    ControlArea->WaitingForDeletion = SegmentEvent;
                    Event = SegmentEvent;
                    SegmentEvent = NULL;
                }
                else {
                    Event = ControlArea->WaitingForDeletion;

                    //
                    // No interlock is needed for the RefCount increment as
                    // no thread can be decrementing it since it is still
                    // pointed to by the control area.
                    //

                    Event->RefCount += 1;
                }

                //
                // Release the PFN lock, the file lock, and wait for the event.
                //

                UNLOCK_PFN (OldIrql);
                if (FileAcquired) {
                    IoSetTopLevelIrp((PIRP)NULL);
                    FsRtlReleaseFile (File);
                }

                KeWaitForSingleObject(&Event->Event,
                                      WrVirtualMemory,
                                      KernelMode,
                                      FALSE,
                                      (PLARGE_INTEGER)NULL);

                //
                // Before this event can be set, the control area
                // WaitingForDeletion field must be cleared (and may be
                // reinitialized to something else), but cannot be reset
                // to our local event.  This allows us to dereference the
                // event count lock free.
                //

#if 0
                //
                // Note that the control area cannot be referenced at this
                // point because it may have been freed.
                //

                ASSERT (Event != ControlArea->WaitingForDeletion);
#endif

                if (FileAcquired) {
                    Status = FsRtlAcquireToCreateMappedSection (File, SectionPageProtection);

                    if (NT_SUCCESS(Status)) {
                        IoSetTopLevelIrp((PIRP)FSRTL_FSP_TOP_LEVEL_IRP);
                    }
                    else {
                        ExFreePool (NewControlArea);
                        ObDereferenceObject (File);
                        return Status;
                    }
                }

                MiFreeEventCounter (Event);

                if (SegmentEvent == NULL) {

                    //
                    // The event was freed from pool, allocate another
                    // event in case we have to synchronize one more time.
                    //

                    goto ReallocateandcheckSegment;
                }
                goto RecheckSegment;

            }

            //
            // There is already a segment for this file, have
            // this section refer to that segment.
            // No need to reference the file object any more.
            //

            NewSegment = ControlArea->Segment;
            ControlArea->u.Flags.Accessed = 1;
            ControlArea->NumberOfSectionReferences += 1;
            if (ControlArea->DereferenceList.Flink != NULL) {

                //
                // Remove this from the list of unused segments.
                //

                RemoveEntryList (&ControlArea->DereferenceList);

                MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

                ControlArea->DereferenceList.Flink = NULL;
                ControlArea->DereferenceList.Blink = NULL;
            }
            IncrementedRefCount = TRUE;

            //
            // If this reference was not from the cache manager
            // up the count of user references.
            //

            if (IgnoreFileSizing == FALSE) {
                ControlArea->NumberOfUserReferences += 1;
            }
        }
        else {

            //
            // There is no segment associated with this file object.
            // Set the file object to refer to the new control area.
            //

            ControlArea = NewControlArea;
            ControlArea->u.Flags.BeingCreated = 1;

            if (AllocationAttributes & SEC_IMAGE) {
                if (GlobalNeeded == TRUE) {
                    ControlArea->u.Flags.GlobalOnlyPerSession = 1;
                }

                MiInsertImageSectionObject (File, ControlArea);
            }
            else {
#if DBG
                PreviousSectionPointer = File->SectionObjectPointer;
#endif
                File->SectionObjectPointer->DataSectionObject = (PVOID) ControlArea;
            }
        }

        UNLOCK_PFN (OldIrql);

        if (SegmentEvent != NULL) {
            MiFreeEventCounter (SegmentEvent);
        }

        if (NewSegment != (PSEGMENT)NULL) {

            //
            // A segment already exists for this file object.
            // If we're creating an imagemap, flush the data section
            // if there is one.
            //

            if (AllocationAttributes & SEC_IMAGE) {
                MiFlushDataSection (File);
            }

            //
            // Deallocate the new control area as it won't be needed.
            // Dereference the file object later when we're done with it.
            //

            ExFreePool (NewControlArea);

            //
            // The section is in paged pool, this can't be set until
            // the PFN mutex has been released.
            //

            if ((!IgnoreFileSizing) && (ControlArea->u.Flags.Image == 0)) {

                //
                // The file size in the segment may not match the current
                // file size, query the file system and get the file
                // size.
                //

                Status = FsRtlGetFileSize (File, (PLARGE_INTEGER)&EndOfFile );

                if (!NT_SUCCESS (Status)) {

                    if (FileAcquired) {
                        IoSetTopLevelIrp((PIRP)NULL);
                        FsRtlReleaseFile (File);
                        FileAcquired = FALSE;
                    }

                    ObDereferenceObject (File);
                    goto UnrefAndReturn;
                }

                if (EndOfFile == 0 && *MaximumSize == 0) {

                    //
                    // Can't map a zero length without specifying the maximum
                    // size as non-zero.
                    //

                    Status = STATUS_MAPPED_FILE_SIZE_ZERO;

                    if (FileAcquired) {
                        IoSetTopLevelIrp((PIRP)NULL);
                        FsRtlReleaseFile (File);
                        FileAcquired = FALSE;
                    }

                    ObDereferenceObject (File);
                    goto UnrefAndReturn;
                }
            }
            else {

                //
                // The size is okay in the segment.
                //

                EndOfFile = (UINT64) NewSegment->SizeOfSegment;
            }

            if (FileAcquired) {
                IoSetTopLevelIrp((PIRP)NULL);
                FsRtlReleaseFile (File);
                FileAcquired = FALSE;
            }

            ObDereferenceObject (File);

            if (*MaximumSize == 0) {

                Section.SizeOfSection.QuadPart = (LONGLONG)EndOfFile;
                FileSizeChecked = TRUE;
            }
            else if (EndOfFile >= *MaximumSize) {

                //
                // EndOfFile is greater than the MaximumSize,
                // use the specified maximum size.
                //

                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
                FileSizeChecked = TRUE;
            }
            else {

                //
                // Need to extend the section, make sure the file was
                // opened for write access.
                //

                if (((SectionPageProtection & PAGE_READWRITE) |
                    (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

                    Status = STATUS_SECTION_TOO_BIG;
                    goto UnrefAndReturn;
                }
                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
            }
        }
        else {

            //
            // The file does not have an associated segment, create a segment
            // object.
            //

            PERFINFO_SECTION_CREATE1(File);

            if (AllocationAttributes & SEC_IMAGE) {

                Status = MiCreateImageFileMap (File, &Segment);

            }
            else {

                Status = MiCreateDataFileMap (File,
                                              &Segment,
                                              MaximumSize,
                                              SectionPageProtection,
                                              AllocationAttributes,
                                              IgnoreFileSizing );
                ASSERT (PreviousSectionPointer == File->SectionObjectPointer);
            }

            if (!NT_SUCCESS(Status)) {

                //
                // Lock the PFN database and check to see if another thread has
                // tried to create a segment to the file object at the same
                // time.
                //

                LOCK_PFN (OldIrql);

                Event = ControlArea->WaitingForDeletion;
                ControlArea->WaitingForDeletion = NULL;
                ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
                ControlArea->u.Flags.FilePointerNull = 1;

                if (AllocationAttributes & SEC_IMAGE) {
                    MiRemoveImageSectionObject (File, ControlArea);
                }
                else {
                    File->SectionObjectPointer->DataSectionObject = NULL;
                }
                ControlArea->u.Flags.BeingCreated = 0;

                UNLOCK_PFN (OldIrql);

                if (FileAcquired) {
                    IoSetTopLevelIrp((PIRP)NULL);
                    FsRtlReleaseFile (File);
                }

                ExFreePool (NewControlArea);

                ObDereferenceObject (File);

                if (Event != NULL) {

                    //
                    // Signal any waiters that the segment structure exists.
                    //

                    KeSetEvent (&Event->Event, 0, FALSE);
                }

                return Status;
            }

            //
            // If the size was specified as zero, set the section size
            // from the created segment size.  This solves problems with
            // race conditions when multiple sections
            // are created for the same mapped file with varying sizes.
            //

            if (*MaximumSize == 0) {
                Section.SizeOfSection.QuadPart = (LONGLONG)Segment->SizeOfSegment;
            }
            else {
                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
            }
        }

    }
    else {

        //
        // No file handle exists, this is a page file backed section.
        //

        if (AllocationAttributes & SEC_IMAGE) {
            return STATUS_INVALID_FILE_FOR_SECTION;
        }

        Status = MiCreatePagingFileMap (&NewSegment,
                                        MaximumSize,
                                        ProtectionMask,
                                        AllocationAttributes);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // Set the section size from the created segment size.  This
        // solves problems with race conditions when multiple sections
        // are created for the same mapped file with varying sizes.
        //

        Section.SizeOfSection.QuadPart = (LONGLONG)NewSegment->SizeOfSegment;
        ControlArea = NewSegment->ControlArea;

        //
        // Set IncrementedRefCount so any failures from this point before the
        // object is created will result in the control area & segment getting
        // torn down - otherwise these could leak.  This is because pagefile
        // backed sections are not (and should not be) added to the
        // dereference segment cache.
        //

        IncrementedRefCount = 1;
    }

    if (NewSegment == NULL) {

        //
        // A new segment had to be created.  Lock the PFN database and
        // check to see if any other threads also tried to create a new segment
        // for this file object at the same time.
        //

        NewSegment = Segment;

        SegmentControlArea = Segment->ControlArea;

        ASSERT (File != NULL);

        LOCK_PFN (OldIrql);

        Event = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;

        if (AllocationAttributes & SEC_IMAGE) {

            //
            // Change the control area in the file object pointer.
            //

            MiRemoveImageSectionObject (File, NewControlArea);
            MiInsertImageSectionObject (File, SegmentControlArea);

            ControlArea = SegmentControlArea;
        }
        else if (SegmentControlArea->u.Flags.Rom == 1) {
            ASSERT (File->SectionObjectPointer->DataSectionObject == NewControlArea);
            File->SectionObjectPointer->DataSectionObject = SegmentControlArea;

            ControlArea = SegmentControlArea;
        }

        ControlArea->u.Flags.BeingCreated = 0;

        UNLOCK_PFN (OldIrql);

        if ((AllocationAttributes & SEC_IMAGE) ||
            (SegmentControlArea->u.Flags.Rom == 1)) {

            //
            // Deallocate the pool used for the original control area.
            //

            ExFreePool (NewControlArea);
        }

        if (Event != NULL) {

            //
            // Signal any waiters that the segment structure exists.
            //

            KeSetEvent (&Event->Event, 0, FALSE);
        }

        PERFINFO_SECTION_CREATE(ControlArea);
    }

    //
    // Being created has now been cleared allowing other threads
    // to reference the segment.  Release the resource on the file.
    //

    if (FileAcquired) {
        IoSetTopLevelIrp((PIRP)NULL);
        FsRtlReleaseFile (File);
        FileAcquired = FALSE;
    }

ReferenceObject:

    if (ChangeFileReference) {
        ObReferenceObject (FileObject);
    }

    //
    // Now that the segment object is created, make the section object
    // refer to the segment object.
    //

    Section.Segment = NewSegment;
    Section.u.LongFlags = ControlArea->u.LongFlags;

    //
    // Update the count of writable user sections so the transaction semantics
    // can be supported.  Note that no lock synchronization is needed here as
    // the transaction manager must already check for any open writable handles
    // to the file - and no writable sections can be created without a writable
    // file handle.  So all that needs to be provided is a way for the
    // transaction manager to know that there are lingering user views or
    // created sections still open that have write access.
    //
    // This must be done before creating the object so a rogue user program
    // that suspends this thread cannot subvert a transaction.
    //

    if ((FileObject == NULL) &&
        (SectionPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE)) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL)) {

        Section.u.Flags.UserWritable = 1;

        InterlockedIncrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
    }

    //
    // Create the section object now.  The section object is created
    // now so that the error handling when the section object cannot
    // be created is simplified.
    //

    Status = ObCreateObject (PreviousMode,
                             MmSectionObjectType,
                             ObjectAttributes,
                             PreviousMode,
                             NULL,
                             sizeof(SECTION),
                             sizeof(SECTION) +
                                 NewSegment->TotalNumberOfPtes * sizeof(MMPTE),
                             sizeof(CONTROL_AREA) +
                                 NewSegment->ControlArea->NumberOfSubsections *
                                   sizeof(SUBSECTION),
                             (PVOID *)&NewSection);

    if (!NT_SUCCESS(Status)) {

        if ((FileObject == NULL) &&
            (SectionPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE)) &&
            (ControlArea->u.Flags.Image == 0) &&
            (ControlArea->FilePointer != NULL)) {

            ASSERT (Section.u.Flags.UserWritable == 1);

            InterlockedDecrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
        }

        goto UnrefAndReturn;
    }

    RtlCopyMemory (NewSection, &Section, sizeof(SECTION));
    NewSection->Address.StartingVpn = 0;

    if (!IgnoreFileSizing) {

        //
        // Indicate that the cache manager is not the owner of this
        // section.
        //

        NewSection->u.Flags.UserReference = 1;

        if (AllocationAttributes & SEC_NO_CHANGE) {

            //
            // Indicate that once the section is mapped, no protection
            // changes or freeing the mapping is allowed.
            //

            NewSection->u.Flags.NoChange = 1;
        }

        if (((SectionPageProtection & PAGE_READWRITE) |
            (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

            //
            // This section does not support WRITE access, indicate
            // that changing the protection to WRITE results in COPY_ON_WRITE.
            //

            NewSection->u.Flags.CopyOnWrite = 1;
        }

        if (AllocationAttributes & SEC_BASED) {

            NewSection->u.Flags.Based = 1;

            SectionBasedRoot = &MmSectionBasedRoot;

            //
            // This section is based at a unique address system wide.
            // Ensure it does not wrap the virtual address space as the
            // SECTION structure would have to widen to accomodate this and
            // it's not worth the performance penalty for the very few isolated
            // cases that would want this.  Note that sections larger than the
            // address space can easily be created - it's just that beyond a
            // certain point you shouldn't specify SEC_BASED (anything this big
            // couldn't use a SEC_BASED section for anything anyway).
            //

            if ((UINT64)NewSection->SizeOfSection.QuadPart > (UINT64)MmHighSectionBase) {
                ObDereferenceObject (NewSection);
                return STATUS_NO_MEMORY;
            }

#if defined(_WIN64)
            SizeOfSection = NewSection->SizeOfSection.QuadPart;
#else
            SizeOfSection = NewSection->SizeOfSection.LowPart;
#endif

            //
            // Get the allocation base mutex.
            //

            ExAcquireFastMutex (&MmSectionBasedMutex);

            Status2 = MiFindEmptyAddressRangeDownTree (
                                            SizeOfSection,
                                            MmHighSectionBase,
                                            X64K,
                                            MmSectionBasedRoot,
                                            (PVOID *)&NewSection->Address.StartingVpn);
            if (!NT_SUCCESS(Status2)) {
                ExReleaseFastMutex (&MmSectionBasedMutex);
                ObDereferenceObject (NewSection);
                return Status2;
            }

            NewSection->Address.EndingVpn = NewSection->Address.StartingVpn +
                                                SizeOfSection - 1;

            MiInsertBasedSection (NewSection);
            ExReleaseFastMutex (&MmSectionBasedMutex);
        }
    }

    //
    // If the cache manager is creating the section, set the was
    // purged flag as the file size can change.
    //

    ControlArea->u.Flags.WasPurged |= IgnoreFileSizing;

    //
    // Check to see if the section is for a data file and the size
    // of the section is greater than the current size of the
    // segment.
    //

    if (((ControlArea->u.Flags.WasPurged == 1) && (!IgnoreFileSizing)) &&
                      (!FileSizeChecked)
                            ||
        ((UINT64)NewSection->SizeOfSection.QuadPart >
                                NewSection->Segment->SizeOfSegment)) {

        TempSectionSize = NewSection->SizeOfSection;

        NewSection->SizeOfSection.QuadPart = (LONGLONG)NewSection->Segment->SizeOfSegment;

        //
        // Even if the caller didn't specify extension rights, we enable it here
        // temporarily to make the section correct.  Use a temporary section
        // instead of temporarily editing the real section to avoid opening
        // a security window that other concurrent threads could exploit.
        //

        if (((NewSection->InitialPageProtection & PAGE_READWRITE) |
            (NewSection->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {
                SECTION     WritableSection;

                *(PSECTION)&WritableSection = *NewSection;

                Status = MmExtendSection (&WritableSection,
                                          &TempSectionSize,
                                          IgnoreFileSizing);

                NewSection->SizeOfSection = WritableSection.SizeOfSection;
        }
        else {
            Status = MmExtendSection (NewSection,
                                      &TempSectionSize,
                                      IgnoreFileSizing);
        }

        if (!NT_SUCCESS(Status)) {
            ObDereferenceObject (NewSection);
            return Status;
        }
    }

    *SectionObject = (PVOID)NewSection;

    return Status;

UnrefAndReturn:

    //
    // Unreference the control area, if it was referenced and return
    // the error status.
    //

    if (FileAcquired) {
        IoSetTopLevelIrp((PIRP)NULL);
        FsRtlReleaseFile (File);
    }

    if (IncrementedRefCount) {
        LOCK_PFN (OldIrql);
        ControlArea->NumberOfSectionReferences -= 1;
        if (!IgnoreFileSizing) {
            ASSERT ((LONG)ControlArea->NumberOfUserReferences > 0);
            ControlArea->NumberOfUserReferences -= 1;
        }
        MiCheckControlArea (ControlArea, NULL, OldIrql);
    }
    return Status;
}

LOGICAL
MiMakeControlAreaRom (
    IN PFILE_OBJECT File,
    IN PLARGE_CONTROL_AREA ControlArea,
    IN PFN_NUMBER PageFrameNumber
    )

/*++

Routine Description:

    This function marks the control area as ROM-backed.  It can fail if the
    parallel control area (image vs data) is currently active as ROM-backed
    as the same PFNs cannot be used for both control areas simultaneously.

Arguments:

    ControlArea - Supplies the relevant control area.

    PageFrameNumber - Supplies the starting physical page frame number.

Return Value:

    TRUE if the control area was marked as ROM-backed, FALSE if not.

--*/

{
    LOGICAL ControlAreaMarked;
    PCONTROL_AREA OtherControlArea;
    KIRQL OldIrql;

    ControlAreaMarked = FALSE;

    LOCK_PFN (OldIrql);

    if (ControlArea->u.Flags.Image == 1) {
        OtherControlArea = (PCONTROL_AREA) File->SectionObjectPointer->DataSectionObject;
    }
    else {
        OtherControlArea = (PCONTROL_AREA) File->SectionObjectPointer->ImageSectionObject;
    }

    //
    // This could be made smarter (ie: throw away the other control area if it's
    // not in use) but for now, keep it simple.
    //

    if ((OtherControlArea == NULL) || (OtherControlArea->u.Flags.Rom == 0)) {
        ControlArea->u.Flags.Rom = 1;
        ControlArea->StartingFrame = PageFrameNumber;
        ControlAreaMarked = TRUE;
    }

    UNLOCK_PFN (OldIrql);

    return ControlAreaMarked;
}

NTSTATUS
MiCreateImageFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of an image file.

    The image file is opened and verified for correctness, a segment
    object is created and initialized based on data in the image
    header.

Arguments:

    File - Supplies the file object for the image file.

    Segment - Returns the segment object.

Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    ULONG_PTR EndingAddress;
    ULONG NumberOfPtes;
    ULONG SizeOfSegment;
    ULONG SectionVirtualSize;
    ULONG i;
    ULONG j;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE TempPteDemandZero;
    PVOID Base;
    PIMAGE_DOS_HEADER DosHeader;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_FILE_HEADER FileHeader;
    ULONG SizeOfImage;
    ULONG SizeOfHeaders;
#if defined (_WIN64)
    PIMAGE_NT_HEADERS32 NtHeader32;
#endif
    PIMAGE_DATA_DIRECTORY ComPlusDirectoryEntry;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    PSEGMENT NewSegment;
    ULONG SectorOffset;
    ULONG NumberOfSubsections;
    PFN_NUMBER PageFrameNumber;
    PFN_NUMBER XipFrameNumber;
    LOGICAL XipFile;
    LOGICAL GlobalPerSession;
    LARGE_INTEGER StartingOffset;
    PCHAR ExtendedHeader;
    PPFN_NUMBER Page;
    ULONG_PTR PreferredImageBase;
    ULONG_PTR NextVa;
    ULONG_PTR ImageBase;
    PKEVENT InPageEvent;
    PMDL Mdl;
    ULONG ImageFileSize;
    ULONG OffsetToSectionTable;
    ULONG ImageAlignment;
    ULONG RoundingAlignment;
    ULONG FileAlignment;
    LOGICAL ImageCommit;
    LOGICAL SectionCommit;
    IO_STATUS_BLOCK IoStatus;
    LARGE_INTEGER EndOfFile;
    ULONG NtHeaderSize;
    ULONG SubsectionsAllocated;
    PLARGE_CONTROL_AREA LargeControlArea;
    PSUBSECTION NewSubsection;
    ULONG OriginalProtection;
    ULONG LoaderFlags;
    ULONG TempNumberOfSubsections;
    PIMAGE_SECTION_HEADER TempSectionTableEntry;
    ULONG AdditionalSubsections;
    ULONG AdditionalPtes;
    ULONG AdditionalBasePtes;
    ULONG NewSubsectionsAllocated;
    PSEGMENT OldSegment;
    PMMPTE NewPointerPte;
    PMMPTE OldPointerPte;
    ULONG OrigNumberOfPtes;
    PCONTROL_AREA NewControlArea;
    SIZE_T NumberOfCommittedPages;
    PHYSICAL_ADDRESS PhysicalAddress;
    LOGICAL ActiveDataReferences;

#if defined (_IA64_)
    LOGICAL InvalidAlignmentAllowed;

    InvalidAlignmentAllowed = FALSE;
#else
#define InvalidAlignmentAllowed FALSE
#endif

    PAGED_CODE();

    ExtendedHeader = NULL;

    Status = FsRtlGetFileSize (File, &EndOfFile);

    if (Status == STATUS_FILE_IS_A_DIRECTORY) {

        //
        // Can't map a directory as a section. Return error.
        //

        return STATUS_INVALID_FILE_FOR_SECTION;
    }

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    if (EndOfFile.HighPart != 0) {

        //
        // File too big. Return error.
        //

        return STATUS_INVALID_FILE_FOR_SECTION;
    }

    //
    // Create a segment which maps an image file.
    // For now map a COFF image file with the subsections
    // containing the based address of the file.
    //

    //
    // Read in the file header.
    //

    InPageEvent = ExAllocatePoolWithTag (NonPagedPool,
                                  sizeof(KEVENT) + MmSizeOfMdl (
                                                      NULL,
                                                      MM_MAXIMUM_IMAGE_HEADER),
                                                      MMTEMPORARY);
    if (InPageEvent == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initializing IoStatus.Information is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    IoStatus.Information = 0;

    Mdl = (PMDL)(InPageEvent + 1);

    //
    // Create an event for the read operation.
    //

    KeInitializeEvent (InPageEvent, NotificationEvent, FALSE);

    //
    // Build an MDL for the operation.
    //

    MmCreateMdl( Mdl, NULL, PAGE_SIZE);
    Mdl->MdlFlags |= MDL_PAGES_LOCKED;

    PageFrameNumber = MiGetPageForHeader();

    Page = (PPFN_NUMBER)(Mdl + 1);
    *Page = PageFrameNumber;

    ZERO_LARGE (StartingOffset);

    CcZeroEndOfLastPage (File);

    //
    // Flush the data section if there is one.
    //
    // At the same time, capture whether there are any references to the
    // data control area.  If so, flip the pages from the image control
    // area into pagefile backings to prevent anyone else's data writes
    // from changing the file after it is validated (this can happen if the
    // pages from the image control area need to be re-paged in later).
    //

    ActiveDataReferences = MiFlushDataSection (File);

    Base = MiCopyHeaderIfResident (File, PageFrameNumber);

    if (Base == NULL) {
        Mdl->MdlFlags |= MDL_PAGES_LOCKED;
        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject( InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   NULL);

            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            goto BadSection;
        }

        Base = MiMapImageHeaderInHyperSpace (PageFrameNumber);

        if (IoStatus.Information != PAGE_SIZE) {

            //
            // A full page was not read from the file, zero any remaining
            // bytes.
            //

            RtlZeroMemory ((PVOID)((PCHAR)Base + IoStatus.Information),
                           PAGE_SIZE - IoStatus.Information);
        }
    }

    DosHeader = (PIMAGE_DOS_HEADER)Base;

    //
    // Check to determine if this is an NT image (PE format) or
    // a DOS image, Win-16 image, or OS/2 image.  If the image is
    // not NT format, return an error indicating which image it
    // appears to be.
    //

    if (DosHeader->e_magic != IMAGE_DOS_SIGNATURE) {

        Status = STATUS_INVALID_IMAGE_NOT_MZ;
        goto NeImage;
    }

#ifndef i386
    if (((ULONG)DosHeader->e_lfanew & 3) != 0) {

        //
        // The image header is not aligned on a longword boundary.
        // Report this as an invalid protect mode image.
        //

        Status = STATUS_INVALID_IMAGE_PROTECT;
        goto NeImage;
    }
#endif

    if ((ULONG)DosHeader->e_lfanew > EndOfFile.LowPart) {
        Status = STATUS_INVALID_IMAGE_PROTECT;
        goto NeImage;
    }

    if (((ULONG)DosHeader->e_lfanew +
            sizeof(IMAGE_NT_HEADERS) +
            (16 * sizeof(IMAGE_SECTION_HEADER))) <= (ULONG)DosHeader->e_lfanew) {
        Status = STATUS_INVALID_IMAGE_PROTECT;
        goto NeImage;
    }

    if (((ULONG)DosHeader->e_lfanew +
            sizeof(IMAGE_NT_HEADERS) +
            (16 * sizeof(IMAGE_SECTION_HEADER))) > PAGE_SIZE) {

        //
        // The PE header is not within the page already read or the
        // objects are in another page.
        // Build another MDL and read an additional 8k.
        //

        ExtendedHeader = ExAllocatePoolWithTag (NonPagedPool,
                                         MM_MAXIMUM_IMAGE_HEADER,
                                         MMTEMPORARY);
        if (ExtendedHeader == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto NeImage;
        }

        //
        // Build an MDL for the operation.
        //

        MmCreateMdl( Mdl, ExtendedHeader, MM_MAXIMUM_IMAGE_HEADER);

        MmBuildMdlForNonPagedPool (Mdl);

        StartingOffset.LowPart = PtrToUlong(PAGE_ALIGN ((ULONG)DosHeader->e_lfanew));

        KeClearEvent (InPageEvent);

        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {

            KeWaitForSingleObject (InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   NULL);

            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            goto NeImage;
        }

        NtHeader = (PIMAGE_NT_HEADERS)((PCHAR)ExtendedHeader +
                          BYTE_OFFSET((ULONG)DosHeader->e_lfanew));
        NtHeaderSize = MM_MAXIMUM_IMAGE_HEADER -
                            (ULONG)(BYTE_OFFSET((ULONG)DosHeader->e_lfanew));
    }
    else {
        NtHeader = (PIMAGE_NT_HEADERS)((PCHAR)DosHeader +
                                          (ULONG)DosHeader->e_lfanew);
        NtHeaderSize = PAGE_SIZE - (ULONG)DosHeader->e_lfanew;
    }
    FileHeader = &NtHeader->FileHeader;

    //
    // Check to see if this is an NT image or a DOS or OS/2 image.
    //

    Status = MiVerifyImageHeader (NtHeader, DosHeader, NtHeaderSize);
    if (Status != STATUS_SUCCESS) {
        goto NeImage;
    }

#if defined(_WIN64)

    if (NtHeader->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR32_MAGIC) {

        //
        // The image is 32-bit.  All code below this point must check
        // if NtHeader is NULL.  If it is, then the image is PE32 and
        // NtHeader32 must be used.
        //

        NtHeader32 = (PIMAGE_NT_HEADERS32)NtHeader;
        NtHeader = NULL;
    }
    else {
        NtHeader32 = NULL;
    }


    if (NtHeader) {
#endif
        ImageAlignment = NtHeader->OptionalHeader.SectionAlignment;
        FileAlignment = NtHeader->OptionalHeader.FileAlignment - 1;
        SizeOfImage = NtHeader->OptionalHeader.SizeOfImage;
        LoaderFlags = NtHeader->OptionalHeader.LoaderFlags;
        ImageBase = NtHeader->OptionalHeader.ImageBase;
        SizeOfHeaders = NtHeader->OptionalHeader.SizeOfHeaders;

        //
        // Read in the COM+ directory entry.
        //

        ComPlusDirectoryEntry = &NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR];

#if defined (_WIN64)
    }
    else {
        ImageAlignment = NtHeader32->OptionalHeader.SectionAlignment;
        FileAlignment = NtHeader32->OptionalHeader.FileAlignment - 1;
        SizeOfImage = NtHeader32->OptionalHeader.SizeOfImage;
        LoaderFlags = NtHeader32->OptionalHeader.LoaderFlags;
        ImageBase = NtHeader32->OptionalHeader.ImageBase;
        SizeOfHeaders = NtHeader32->OptionalHeader.SizeOfHeaders;

        //
        // Read in the COM+ directory entry.
        //

        ComPlusDirectoryEntry = &NtHeader32->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR];
    }
#endif

    //
    // Set the appropriate bit for .NET images inside the image's section loader flags.
    //
    if ((ComPlusDirectoryEntry->VirtualAddress != 0) && (ComPlusDirectoryEntry->Size != 0)) {
        LoaderFlags |= IMAGE_LOADER_FLAGS_COMPLUS;
    }

    RoundingAlignment = ImageAlignment;
    NumberOfSubsections = FileHeader->NumberOfSections;

    if (ImageAlignment < PAGE_SIZE) {

        //
        // The image alignment is less than the page size,
        // map the image with a single subsection.
        //

        ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                      (ULONG)(sizeof(CONTROL_AREA) + (sizeof(SUBSECTION))),
                      MMCONTROL);
        SubsectionsAllocated = 1;
    }
    else {

        //
        // Allocate a control area and a subsection for each section
        // header plus one for the image header which has no section.
        //

        ControlArea = ExAllocatePoolWithTag(NonPagedPool,
                                            (ULONG)(sizeof(CONTROL_AREA) +
                                                    (sizeof(SUBSECTION) *
                                                (NumberOfSubsections + 1))),
                                            'iCmM');
        SubsectionsAllocated = NumberOfSubsections + 1;
    }

    if (ControlArea == NULL) {

        //
        // The requested pool could not be allocated.
        //

        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto NeImage;
    }

    //
    // Zero the control area and the FIRST subsection.
    //

    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA) + sizeof(SUBSECTION));

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

    NumberOfPtes = BYTES_TO_PAGES (SizeOfImage);

    if (NumberOfPtes == 0) {
        ExFreePool (ControlArea);
        Status = STATUS_INVALID_IMAGE_FORMAT;
        goto NeImage;
    }

#if defined (_WIN64)
    if (NumberOfPtes >= _4gb) {
        ExFreePool (ControlArea);
        Status = STATUS_INVALID_IMAGE_FORMAT;
        goto NeImage;
    }
#endif

#if defined (_IA64_)
    if ((ImageAlignment < PAGE_SIZE) &&
        (KeGetPreviousMode() != KernelMode) &&
        ((FileHeader->Machine < USER_SHARED_DATA->ImageNumberLow) ||
         (FileHeader->Machine > USER_SHARED_DATA->ImageNumberHigh))) {

        InvalidAlignmentAllowed = TRUE;
    }
    OrigNumberOfPtes = NumberOfPtes;
#endif

    SizeOfSegment = sizeof(SEGMENT) + (sizeof(MMPTE) * (NumberOfPtes - 1)) +
                    sizeof(SECTION_IMAGE_INFORMATION);

    NewSegment = ExAllocatePoolWithTag (PagedPool, SizeOfSegment, MMSECT);

    if (NewSegment == NULL) {

        //
        // The requested pool could not be allocated.
        //

        ExFreePool (ControlArea);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto NeImage;
    }
    *Segment = NewSegment;
    RtlZeroMemory (NewSegment, sizeof(SEGMENT));

    //
    // Align the prototype PTEs on the proper boundary.
    //

    PointerPte = &NewSegment->ThePtes[0];
    i = (ULONG) (((ULONG_PTR)PointerPte >> PTE_SHIFT) &
                    ((MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - 1));

    if (i != 0) {
        i = (MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - i;
    }

    NewSegment->PrototypePte = &NewSegment->ThePtes[i];

    NewSegment->ControlArea = ControlArea;
    NewSegment->u2.ImageInformation =
        (PSECTION_IMAGE_INFORMATION)((PCHAR)NewSegment + sizeof(SEGMENT) +
                                       (sizeof(MMPTE) * (NumberOfPtes - 1)));
    NewSegment->TotalNumberOfPtes = NumberOfPtes;
    NewSegment->NonExtendedPtes = NumberOfPtes;
    NewSegment->SizeOfSegment = (ULONG_PTR)NumberOfPtes * PAGE_SIZE;

    RtlZeroMemory (NewSegment->u2.ImageInformation,
                   sizeof (SECTION_IMAGE_INFORMATION));


//
// This code is built twice on the Win64 build - once for PE32+ and once for
// PE32 images.
//
#define INIT_IMAGE_INFORMATION(OptHdr) {                            \
    NewSegment->u2.ImageInformation->TransferAddress =              \
                    (PVOID)((ULONG_PTR)((OptHdr).ImageBase) +       \
                            (OptHdr).AddressOfEntryPoint);          \
    NewSegment->u2.ImageInformation->MaximumStackSize =             \
                            (OptHdr).SizeOfStackReserve;            \
    NewSegment->u2.ImageInformation->CommittedStackSize =           \
                            (OptHdr).SizeOfStackCommit;             \
    NewSegment->u2.ImageInformation->SubSystemType =                \
                            (OptHdr).Subsystem;                     \
    NewSegment->u2.ImageInformation->SubSystemMajorVersion = (USHORT)((OptHdr).MajorSubsystemVersion); \
    NewSegment->u2.ImageInformation->SubSystemMinorVersion = (USHORT)((OptHdr).MinorSubsystemVersion); \
    NewSegment->u2.ImageInformation->DllCharacteristics =           \
                            (OptHdr).DllCharacteristics;            \
    NewSegment->u2.ImageInformation->ImageContainsCode =            \
                            (BOOLEAN)(((OptHdr).SizeOfCode != 0) || \
                                      ((OptHdr).AddressOfEntryPoint != 0)); \
    }

#if defined (_WIN64)
    if (NtHeader) {
#endif
        INIT_IMAGE_INFORMATION(NtHeader->OptionalHeader);
#if defined (_WIN64)
    }
    else {

        //
        // The image is 32-bit so use the 32-bit header
        //

        INIT_IMAGE_INFORMATION(NtHeader32->OptionalHeader);
    }
#endif
    #undef INIT_IMAGE_INFORMATION

    NewSegment->u2.ImageInformation->ImageCharacteristics =
                            FileHeader->Characteristics;
    NewSegment->u2.ImageInformation->Machine =
                            FileHeader->Machine;
    NewSegment->u2.ImageInformation->LoaderFlags =
                            LoaderFlags;

    ControlArea->Segment = NewSegment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->NumberOfUserReferences = 1;
    ControlArea->u.Flags.BeingCreated = 1;

    if (ImageAlignment < PAGE_SIZE) {

        //
        // Image alignment is less than a page, the number
        // of subsections is 1.
        //

        ControlArea->NumberOfSubsections = 1;
    }
    else {
        ControlArea->NumberOfSubsections = (USHORT)NumberOfSubsections;
    }

    ControlArea->u.Flags.Image = 1;
    ControlArea->u.Flags.File = 1;

    if ((ActiveDataReferences == TRUE) ||
        (IoIsDeviceEjectable(File->DeviceObject)) ||
        ((FileHeader->Characteristics &
                IMAGE_FILE_REMOVABLE_RUN_FROM_SWAP) &&
         (FILE_REMOVABLE_MEDIA & File->DeviceObject->Characteristics)) ||
        ((FileHeader->Characteristics &
                IMAGE_FILE_NET_RUN_FROM_SWAP) &&
         (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics))) {

        //
        // This file resides on a floppy disk or a removable media or
        // network with flags set indicating it should be copied
        // to the paging file.
        //

        ControlArea->u.Flags.FloppyMedia = 1;
    }

#if DBG
    if (MiMakeImageFloppy[0] & 0x1) {
        MiMakeImageFloppy[1] += 1;
        ControlArea->u.Flags.FloppyMedia = 1;
    }
#endif

    if (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics) {

        //
        // This file resides on a redirected drive.
        //

        ControlArea->u.Flags.Networked = 1;
    }

    ControlArea->FilePointer = File;

    //
    // Build the subsection and prototype PTEs for the image header.
    //

    Subsection->ControlArea = ControlArea;
    NextVa = ImageBase;

#if defined (_WIN64)

    //
    // Don't let bogus headers cause system alignment faults.
    //

    if (FileHeader->SizeOfOptionalHeader & (sizeof (ULONG_PTR) - 1)) {
        goto BadPeImageSegment;
    }
#endif

    if ((NextVa & (X64K - 1)) != 0) {

        //
        // Image header is not aligned on a 64k boundary.
        //

        goto BadPeImageSegment;
    }

    NewSegment->BasedAddress = (PVOID)NextVa;

    if (SizeOfHeaders >= SizeOfImage) {
        goto BadPeImageSegment;
    }

    Subsection->PtesInSubsection = MI_ROUND_TO_SIZE (
                                       SizeOfHeaders,
                                       ImageAlignment
                                   ) >> PAGE_SHIFT;

    PointerPte = NewSegment->PrototypePte;
    Subsection->SubsectionBase = PointerPte;

    TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
    TempPte.u.Soft.Prototype = 1;

    NewSegment->SegmentPteTemplate = TempPte;
    SectorOffset = 0;

    if (ImageAlignment < PAGE_SIZE) {

        //
        // Aligned on less than a page size boundary.
        // Build a single subsection to refer to the image.
        //

        PointerPte = NewSegment->PrototypePte;

        Subsection->PtesInSubsection = NumberOfPtes;

#if !defined (_WIN64)

        //
        // Note this only needs to be checked for 32-bit systems as NT64
        // does a much more extensive reallocation of images that are built
        // with alignment less than PAGE_SIZE.
        //

        if ((UINT64)SizeOfImage < (UINT64)EndOfFile.QuadPart) {

            //
            // Images that have a size of image (according to the header) that's
            // smaller than the actual file only get that many prototype PTEs.
            // Initialize the subsection properly so no one can read off the
            // end as that would corrupt the PFN database element's original
            // PTE entry.
            //

            Subsection->NumberOfFullSectors = (SizeOfImage >> MMSECTOR_SHIFT);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                  SizeOfImage & MMSECTOR_MASK;
        }
        else {
#endif
            Subsection->NumberOfFullSectors =
                                (ULONG)(EndOfFile.QuadPart >> MMSECTOR_SHIFT);

            ASSERT ((ULONG)(EndOfFile.HighPart & 0xFFFFF000) == 0);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                  EndOfFile.LowPart & MMSECTOR_MASK;
#if !defined (_WIN64)
        }
#endif

        Subsection->u.SubsectionFlags.Protection = MM_EXECUTE_WRITECOPY;

        //
        // Set all the PTEs to the execute-read-write protection.
        // The section will control access to these and the segment
        // must provide a method to allow other users to map the file
        // for various protections.
        //

        TempPte.u.Soft.Protection = MM_EXECUTE_WRITECOPY;

        NewSegment->SegmentPteTemplate = TempPte;

        //
        // Invalid image alignments are supported for cross platform
        // emulation.  Only IA64 requires extra handling because
        // the native page size is larger than x86.
        //

        if (InvalidAlignmentAllowed == TRUE) {

            TempPteDemandZero.u.Long = 0;
            TempPteDemandZero.u.Soft.Protection = MM_EXECUTE_WRITECOPY;
            SectorOffset = 0;

            for (i = 0; i < NumberOfPtes; i += 1) {

                //
                // Set prototype PTEs.
                //

                if (SectorOffset < EndOfFile.LowPart) {

                    //
                    // Data resides on the disk, refer to the control section.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

                }
                else {

                    //
                    // Data does not reside on the disk, use Demand zero pages.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, TempPteDemandZero);
                }

                SectorOffset += PAGE_SIZE;
                PointerPte += 1;
            }

        }
        else {

            for (i = 0; i < NumberOfPtes; i += 1) {

                //
                // Set all the prototype PTEs to refer to the control section.
                //

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                PointerPte += 1;
            }
        }

        NewSegment->u1.ImageCommitment = NumberOfPtes;


        //
        // Indicate alignment is less than a page.
        //

        TempPte.u.Long = 0;

    }
    else {

        //
        // Alignment is PAGE_SIZE or greater.
        //

        if (Subsection->PtesInSubsection > NumberOfPtes) {

            //
            // Inconsistent image, size does not agree with header.
            //

            goto BadPeImageSegment;
        }
        NumberOfPtes -= Subsection->PtesInSubsection;

        Subsection->NumberOfFullSectors =
            SizeOfHeaders >> MMSECTOR_SHIFT;

        Subsection->u.SubsectionFlags.SectorEndOffset =
            SizeOfHeaders & MMSECTOR_MASK;

        Subsection->u.SubsectionFlags.ReadOnly = 1;
        Subsection->u.SubsectionFlags.Protection = MM_READONLY;

        TempPte.u.Soft.Protection = MM_READONLY;
        NewSegment->SegmentPteTemplate = TempPte;

        for (i = 0; i < Subsection->PtesInSubsection; i += 1) {

            //
            // Set all the prototype PTEs to refer to the control section.
            //

            if (SectorOffset < SizeOfHeaders) {
                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
            }
            SectorOffset += PAGE_SIZE;
            PointerPte += 1;
            NextVa += PAGE_SIZE;
        }
    }

    //
    // Build the additional subsections.
    //

    PreferredImageBase = ImageBase;

    //
    // At this point the object table is read in (if it was not
    // already read in) and may displace the image header.
    //

    SectionTableEntry = NULL;
    OffsetToSectionTable = sizeof(ULONG) +
                              sizeof(IMAGE_FILE_HEADER) +
                              FileHeader->SizeOfOptionalHeader;

    if ((BYTE_OFFSET(NtHeader) + OffsetToSectionTable +
#if defined (_WIN64)
                BYTE_OFFSET(NtHeader32) +
#endif
                ((NumberOfSubsections + 1) *
                sizeof (IMAGE_SECTION_HEADER))) <= PAGE_SIZE) {

        //
        // Section tables are within the header which was read.
        //

#if defined(_WIN64)
        if (NtHeader32) {
            SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader32 +
                                    OffsetToSectionTable);
        }
        else
#endif
        {
            SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                    OffsetToSectionTable);
        }

    }
    else {

        //
        // Has an extended header been read in and are the object
        // tables resident?
        //

        if (ExtendedHeader != NULL) {

#if defined(_WIN64)
            if (NtHeader32) {
                SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader32 +
                                        OffsetToSectionTable);
            }
            else
#endif
            {
                SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                        OffsetToSectionTable);
            }

            //
            // Is the whole range of object tables mapped by the
            // extended header?
            //

            if ((((PCHAR)SectionTableEntry +
                 ((NumberOfSubsections + 1) *
                    sizeof (IMAGE_SECTION_HEADER))) -
                         (PCHAR)ExtendedHeader) >
                                            MM_MAXIMUM_IMAGE_HEADER) {
                SectionTableEntry = NULL;

            }
        }
    }

    if (SectionTableEntry == NULL) {

        //
        // The section table entries are not in the same
        // pages as the other data already read in.  Read in
        // the object table entries.
        //

        if (ExtendedHeader == NULL) {
            ExtendedHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                    MM_MAXIMUM_IMAGE_HEADER,
                                                    MMTEMPORARY);
            if (ExtendedHeader == NULL) {
                ExFreePool (NewSegment);
                ExFreePool (ControlArea);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto NeImage;
            }

            //
            // Build an MDL for the operation.
            //

            MmCreateMdl( Mdl, ExtendedHeader, MM_MAXIMUM_IMAGE_HEADER);

            MmBuildMdlForNonPagedPool (Mdl);
        }

        StartingOffset.LowPart = PtrToUlong(PAGE_ALIGN (
                                    (ULONG)DosHeader->e_lfanew +
                                    OffsetToSectionTable));

        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)ExtendedHeader +
                                BYTE_OFFSET((ULONG)DosHeader->e_lfanew +
                                OffsetToSectionTable));

        KeClearEvent (InPageEvent);
        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject( InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)NULL);
            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            ExFreePool (NewSegment);
            ExFreePool (ControlArea);
            goto NeImage;
        }

        //
        // From this point on NtHeader is only valid if it
        // was in the first page of the image, otherwise reading in
        // the object tables wiped it out.
        //
    }

    if ((TempPte.u.Long == 0) && (InvalidAlignmentAllowed == FALSE)) {

        //
        // The image header is no longer valid, TempPte is
        // used to indicate that this image alignment is
        // less than a PAGE_SIZE.
        //
        // Loop through all sections and make sure there is no
        // uninitialized data.
        //

        Status = STATUS_SUCCESS;

        while (NumberOfSubsections > 0) {
            if (SectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = SectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
            }

            //
            // If the raw pointer + raw size overflows a long word,
            // return an error.
            //

            if (SectionTableEntry->PointerToRawData +
                        SectionTableEntry->SizeOfRawData <
                SectionTableEntry->PointerToRawData) {

                KdPrint(("MMCREASECT: invalid section/file size %Z\n",
                    &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                break;
            }

            //
            // If the virtual size and address does not match the rawdata
            // and invalid alignments not allowed return an error.
            //

            if (((SectionTableEntry->PointerToRawData !=
                  SectionTableEntry->VirtualAddress))
                            ||
                   (SectionVirtualSize > SectionTableEntry->SizeOfRawData)) {

                KdPrint(("MMCREASECT: invalid BSS/Trailingzero %Z\n",
                        &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                break;
            }

            SectionTableEntry += 1;
            NumberOfSubsections -= 1;
        }


        if (!NT_SUCCESS(Status)) {
            ExFreePool (NewSegment);
            ExFreePool (ControlArea);
            goto NeImage;
        }

        goto PeReturnSuccess;

    }
    else if ((TempPte.u.Long == 0) && (InvalidAlignmentAllowed == TRUE)) {

        TempNumberOfSubsections = NumberOfSubsections;
        TempSectionTableEntry = SectionTableEntry;

        //
        // The image header is no longer valid, TempPte is
        // used to indicate that this image alignment is
        // less than a PAGE_SIZE.
        //

        //
        // Loop through all sections and make sure there is no
        // uninitialized data.
        //
        // Also determine if there are shared data sections and
        // the number of extra ptes they require.
        //

        AdditionalSubsections = 0;
        AdditionalPtes = 0;
        AdditionalBasePtes = 0;
        RoundingAlignment = PAGE_SIZE;

        while (TempNumberOfSubsections > 0) {
            ULONG EndOfSection;
            ULONG ExtraPages;

            if (TempSectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = TempSectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = TempSectionTableEntry->Misc.VirtualSize;
            }

            EndOfSection = TempSectionTableEntry->PointerToRawData +
                           TempSectionTableEntry->SizeOfRawData;

            //
            // If the raw pointer + raw size overflows a long word, return an error.
            //

            if (EndOfSection < TempSectionTableEntry->PointerToRawData) {

                 KdPrint(("MMCREASECT: invalid section/file size %Z\n",
                      &File->FileName));

                 Status = STATUS_INVALID_IMAGE_FORMAT;

                 ExFreePool (NewSegment);
                 ExFreePool (ControlArea);
                 goto NeImage;
            }

            //
            // If the section goes past SizeOfImage then allocate
            // additional PTEs.  On x86 this is handled by the subsection
            // mapping.  Note the additional data must be in memory so
            // it can be shuffled around later.
            //

            if ((EndOfSection <= EndOfFile.LowPart) &&
                (EndOfSection > SizeOfImage)) {

                //
                // Allocate enough PTEs to cover the end of this section.
                // Maximize with any other sections that extend beyond SizeOfImage
                //

                ExtraPages = MI_ROUND_TO_SIZE (EndOfSection, RoundingAlignment) >> PAGE_SHIFT;
                if ((ExtraPages > OrigNumberOfPtes) &&
                    (ExtraPages - OrigNumberOfPtes > AdditionalBasePtes)) {

                    AdditionalBasePtes = ExtraPages - (ULONG) OrigNumberOfPtes;
                }
            }

            // Count number of shared data sections and additional ptes needed

            if ((TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) &&
                (!(TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_EXECUTE) ||
                 (TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_WRITE))) {
                AdditionalPtes +=
                    MI_ROUND_TO_SIZE (SectionVirtualSize, RoundingAlignment) >>
                                                                PAGE_SHIFT;
                AdditionalSubsections += 1;
            }

            TempSectionTableEntry += 1;
            TempNumberOfSubsections -= 1;
        }

        if (AdditionalBasePtes == 0 && (AdditionalSubsections == 0 || AdditionalPtes == 0)) {
            // no shared data sections
            goto PeReturnSuccess;
        }

        //
        // There are additional Base PTEs or shared data sections.
        // For shared sections, allocate new PTEs for these sections
        // at the end of the image. The WX86 loader will change
        // fixups to point to the new pages.
        //
        // First reallocate the control area.
        //

        NewSubsectionsAllocated = SubsectionsAllocated +
                                                AdditionalSubsections;

        NewControlArea = ExAllocatePoolWithTag(NonPagedPool,
                                    (ULONG) (sizeof(CONTROL_AREA) +
                                            (sizeof(SUBSECTION) *
                                                NewSubsectionsAllocated)),
                                                'iCmM');
        if (NewControlArea == NULL) {
            ExFreePool (NewSegment);
            ExFreePool (ControlArea);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto NeImage;
        }

        //
        // Copy the old control area to the new one, modify some fields.
        //

        RtlCopyMemory (NewControlArea, ControlArea,
                        sizeof(CONTROL_AREA) +
                            sizeof(SUBSECTION) * SubsectionsAllocated);

        NewControlArea->NumberOfSubsections = (USHORT) NewSubsectionsAllocated;

        //
        // Now allocate a new segment that has the newly calculated number
        // of PTEs, initialize it from the previously allocated new segment,
        // and overwrite the fields that should be changed.
        //

        OldSegment = NewSegment;


        OrigNumberOfPtes += AdditionalBasePtes;
        PointerPte += AdditionalBasePtes;

        SizeOfSegment = sizeof(SEGMENT) +
                     (sizeof(MMPTE) * (OrigNumberOfPtes + AdditionalPtes - 1)) +
                        sizeof(SECTION_IMAGE_INFORMATION);

        NewSegment = ExAllocatePoolWithTag (PagedPool,
                                            SizeOfSegment,
                                            MMSECT);

        if (NewSegment == NULL) {

            //
            // The requested pool could not be allocated.
            //

            ExFreePool (ControlArea);
            ExFreePool (NewControlArea);
            ExFreePool (OldSegment);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto NeImage;
        }

        *Segment = NewSegment;
        RtlCopyMemory (NewSegment, OldSegment, sizeof(SEGMENT));

        //
        // Align the prototype PTEs on the proper boundary.
        //

        NewPointerPte = &NewSegment->ThePtes[0];
        i = (ULONG) (((ULONG_PTR)NewPointerPte >> PTE_SHIFT) &
                        ((MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - 1));

        if (i != 0) {
            i = (MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - i;
        }

        NewSegment->PrototypePte = &NewSegment->ThePtes[i];
        if (i != 0) {
            RtlZeroMemory (&NewSegment->ThePtes[0], sizeof(MMPTE) * i);
        }
        PointerPte = NewSegment->PrototypePte +
                                       (PointerPte - OldSegment->PrototypePte);

        NewSegment->ControlArea = NewControlArea;
        NewSegment->u2.ImageInformation =
            (PSECTION_IMAGE_INFORMATION)((PCHAR)NewSegment + sizeof(SEGMENT) +
                     (sizeof(MMPTE) * (OrigNumberOfPtes + AdditionalPtes - 1)));
        NewSegment->TotalNumberOfPtes = OrigNumberOfPtes + AdditionalPtes;
        NewSegment->NonExtendedPtes = OrigNumberOfPtes + AdditionalPtes;
        NewSegment->SizeOfSegment = (ULONG_PTR)(OrigNumberOfPtes + AdditionalPtes) * PAGE_SIZE;

        RtlCopyMemory (NewSegment->u2.ImageInformation,
                       OldSegment->u2.ImageInformation,
                       sizeof (SECTION_IMAGE_INFORMATION));

        //
        // Now change the fields in the subsections to account for the new
        // control area and the new segment. Also change the PTEs in the
        // newly allocated segment to point to the new subsections.
        //

        NewControlArea->Segment = NewSegment;

        Subsection = (PSUBSECTION)(ControlArea + 1);
        NewSubsection = (PSUBSECTION)(NewControlArea + 1);
        NewSubsection->PtesInSubsection += AdditionalBasePtes;

        for (i = 0; i < SubsectionsAllocated; i += 1) {

            //
            // Note: SubsectionsAllocated is always 1 (for wx86), so this loop
            // is executed only once.
            //

            NewSubsection->ControlArea = (PCONTROL_AREA) NewControlArea;

            NewSubsection->SubsectionBase = NewSegment->PrototypePte +
                    (Subsection->SubsectionBase - OldSegment->PrototypePte);

            NewPointerPte = NewSegment->PrototypePte;
            OldPointerPte = OldSegment->PrototypePte;

            TempPte.u.Long = MiGetSubsectionAddressForPte (NewSubsection);
            TempPte.u.Soft.Prototype = 1;

            for (j = 0; j < OldSegment->TotalNumberOfPtes+AdditionalBasePtes; j += 1) {

                if ((OldPointerPte->u.Soft.Prototype == 1) &&
                    (MiGetSubsectionAddress (OldPointerPte) == Subsection)) {
                    OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (OldPointerPte);
                    TempPte.u.Soft.Protection = OriginalProtection;
                    MI_WRITE_INVALID_PTE (NewPointerPte, TempPte);
                }
                else if (i == 0) {

                    //
                    // Since the outer for loop is executed only once, there
                    // is no need for the i == 0 above, but it is safer to
                    // have it. If the code changes later and other sections
                    // are added, their PTEs will get initialized here as
                    // DemandZero and if they are not DemandZero, they will be
                    // overwritten in a later iteration of the outer loop.
                    // For now, this else if clause will be executed only
                    // for DemandZero PTEs.
                    //

                    OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (OldPointerPte);
                    TempPteDemandZero.u.Long = 0;
                    TempPteDemandZero.u.Soft.Protection = OriginalProtection;
                    MI_WRITE_INVALID_PTE (NewPointerPte, TempPteDemandZero);
                }

                NewPointerPte += 1;
                // Stop incrementing the OldPointerPte at the last entry and use it
                // for the additional Base PTEs
                if (j < OldSegment->TotalNumberOfPtes-1) {
                    OldPointerPte += 1;
                }
            }

            Subsection += 1;
            NewSubsection += 1;
        }


        RtlZeroMemory (NewSubsection,
                            sizeof(SUBSECTION) * AdditionalSubsections);

        ExFreePool (OldSegment);
        ExFreePool (ControlArea);
        ControlArea = (PCONTROL_AREA) NewControlArea;

        //
        // Adjust some variables that are used below.
        // PointerPte has already been set above before OldSegment was freed.
        //

        SubsectionsAllocated = NewSubsectionsAllocated;
        Subsection = NewSubsection - 1; // Points to last used subsection.
        NumberOfPtes = AdditionalPtes;  // # PTEs that haven't yet been used in
                                        // previous subsections.

        //
        // Additional Base PTEs have been added. Only continue if there are
        // additional subsections to process.
        //

        if (AdditionalSubsections == 0 || AdditionalPtes == 0) {
            // no shared data sections
            goto PeReturnSuccess;
        }
    }

    ImageFileSize = EndOfFile.LowPart + 1;

    while (NumberOfSubsections > 0) {

        if ((InvalidAlignmentAllowed == FALSE) ||
            ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) &&
             (!(SectionTableEntry->Characteristics & IMAGE_SCN_MEM_EXECUTE) ||
              (SectionTableEntry->Characteristics & IMAGE_SCN_MEM_WRITE)))) {

            //
            // Handle case where virtual size is 0.
            //

            if (SectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = SectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
            }

            //
            // Fix for Borland linker problem.  The SizeOfRawData can
            // be a zero, but the PointerToRawData is not zero.
            // Set it to zero.
            //

            if (SectionTableEntry->SizeOfRawData == 0) {
                SectionTableEntry->PointerToRawData = 0;
            }

            //
            // If the section information wraps return an error.
            //

            if (SectionTableEntry->PointerToRawData +
                        SectionTableEntry->SizeOfRawData <
                SectionTableEntry->PointerToRawData) {

                goto BadPeImageSegment;
            }

            Subsection->NextSubsection = (Subsection + 1);

            Subsection += 1;
            Subsection->ControlArea = ControlArea;
            Subsection->NextSubsection = NULL;
            Subsection->UnusedPtes = 0;

            if (((NextVa != (PreferredImageBase + SectionTableEntry->VirtualAddress)) && (InvalidAlignmentAllowed == FALSE)) ||
                (SectionVirtualSize == 0)) {

                //
                // The specified virtual address does not align
                // with the next prototype PTE.
                //

                goto BadPeImageSegment;
            }

            Subsection->PtesInSubsection =
                MI_ROUND_TO_SIZE (SectionVirtualSize, RoundingAlignment)
                                                                    >> PAGE_SHIFT;

            if (Subsection->PtesInSubsection > NumberOfPtes) {

                //
                // Inconsistent image, size does not agree with object tables.
                //

                goto BadPeImageSegment;
            }
            NumberOfPtes -= Subsection->PtesInSubsection;

            Subsection->u.LongFlags = 0;
            Subsection->StartingSector =
                            SectionTableEntry->PointerToRawData >> MMSECTOR_SHIFT;

            //
            // Align ending sector on file align boundary.
            //

            EndingAddress = (SectionTableEntry->PointerToRawData +
                                         SectionTableEntry->SizeOfRawData +
                                         FileAlignment) & ~FileAlignment;

            Subsection->NumberOfFullSectors = (ULONG)
                             ((EndingAddress >> MMSECTOR_SHIFT) -
                             Subsection->StartingSector);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                        (ULONG) EndingAddress & MMSECTOR_MASK;

            Subsection->SubsectionBase = PointerPte;

            //
            // Build both a demand zero PTE and a PTE pointing to the
            // subsection.
            //

            TempPte.u.Long = 0;
            TempPteDemandZero.u.Long = 0;

            TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
            TempPte.u.Soft.Prototype = 1;
            ImageFileSize = SectionTableEntry->PointerToRawData +
                                        SectionTableEntry->SizeOfRawData;
            TempPte.u.Soft.Protection =
                     MiGetImageProtection (SectionTableEntry->Characteristics);
            TempPteDemandZero.u.Soft.Protection = TempPte.u.Soft.Protection;

            if (SectionTableEntry->PointerToRawData == 0) {
                TempPte = TempPteDemandZero;
            }

            Subsection->u.SubsectionFlags.ReadOnly = 1;
            Subsection->u.SubsectionFlags.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (&TempPte);

            //
            // Assume the subsection will be unwritable and therefore
            // won't be charged for any commitment.
            //

            SectionCommit = FALSE;
            ImageCommit = FALSE;

            if (TempPte.u.Soft.Protection & MM_PROTECTION_WRITE_MASK) {
                if ((TempPte.u.Soft.Protection & MM_COPY_ON_WRITE_MASK)
                                                == MM_COPY_ON_WRITE_MASK) {

                    //
                    // This page is copy on write, charge ImageCommitment
                    // for all pages in this subsection.
                    //

                    ImageCommit = TRUE;
                }
                else {

                    //
                    // This page is write shared, charge commitment when
                    // the mapping completes.
                    //

                    SectionCommit = TRUE;
                    Subsection->u.SubsectionFlags.GlobalMemory = 1;
                    ControlArea->u.Flags.GlobalMemory = 1;
                }
            }

            NewSegment->SegmentPteTemplate = TempPte;
            SectorOffset = 0;

            for (i = 0; i < Subsection->PtesInSubsection; i += 1) {

                //
                // Set all the prototype PTEs to refer to the control section.
                //

                if (SectorOffset < SectionVirtualSize) {

                    //
                    // Make PTE accessible.
                    //

                    if (SectionCommit) {
                        NewSegment->NumberOfCommittedPages += 1;
                    }
                    if (ImageCommit) {
                        NewSegment->u1.ImageCommitment += 1;
                    }

                    if (SectorOffset < SectionTableEntry->SizeOfRawData) {

                        //
                        // Data resides on the disk, use the subsection format PTE.
                        //
                        MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                    }
                    else {

                        //
                        // Demand zero pages.
                        //
                        MI_WRITE_INVALID_PTE (PointerPte, TempPteDemandZero);
                    }
                }
                else {

                    //
                    // No access pages.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
                }
                SectorOffset += PAGE_SIZE;
                PointerPte += 1;
                NextVa += PAGE_SIZE;
            }
        }

        SectionTableEntry += 1;
        NumberOfSubsections -= 1;
    }

    if (InvalidAlignmentAllowed == FALSE) {

        //
        // Account for the number of subsections that really are mapped.
        //

        ASSERT (ImageAlignment >= PAGE_SIZE);
        ControlArea->NumberOfSubsections += 1;

        //
        // If the file size is not as big as the image claimed to be,
        // return an error.
        //

        if (ImageFileSize > EndOfFile.LowPart) {

            //
            // Invalid image size.
            //

            KdPrint(("MMCREASECT: invalid image size - file size %lx - image size %lx\n %Z\n",
                EndOfFile.LowPart, ImageFileSize, &File->FileName));
            goto BadPeImageSegment;
        }

        //
        // The total number of PTEs was decremented as sections were built,
        // make sure that there are less than 64k's worth at this point.
        //

        if (NumberOfPtes >= (ImageAlignment >> PAGE_SHIFT)) {

            //
            // Inconsistent image, size does not agree with object tables.
            //

            KdPrint(("MMCREASECT: invalid image - PTE left %lx\n image name %Z\n",
                NumberOfPtes, &File->FileName));

            goto BadPeImageSegment;
        }

        //
        // Set any remaining PTEs to no access.
        //

        while (NumberOfPtes != 0) {
            MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
            PointerPte += 1;
            NumberOfPtes -= 1;
        }

        //
        // Turn the image header page into a transition page within the
        // prototype PTEs.
        //

        if ((ExtendedHeader == NULL) && (SizeOfHeaders < PAGE_SIZE)) {

            //
            // Zero remaining portion of header.
            //

            RtlZeroMemory ((PVOID)((PCHAR)Base +
                           SizeOfHeaders),
                           PAGE_SIZE - SizeOfHeaders);
        }
    }

    NumberOfCommittedPages = NewSegment->NumberOfCommittedPages;

    if (NumberOfCommittedPages != 0) {

        //
        // Commit the pages for the image section.
        //

        if (MiChargeCommitment (NumberOfCommittedPages, NULL) == FALSE) {
            Status = STATUS_COMMITMENT_LIMIT;
            ExFreePool (NewSegment);
            ExFreePool (ControlArea);
            goto NeImage;
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_IMAGE, NumberOfCommittedPages);
        Status = STATUS_SUCCESS;

        InterlockedExchangeAddSizeT (&MmSharedCommit, NumberOfCommittedPages);
    }

PeReturnSuccess:

    //
    // Only images that are linked with subsections aligned to the native
    // page size can be directly executed from ROM.
    //

    XipFile = FALSE;
    XipFrameNumber = 0;

    if ((FileAlignment == PAGE_SIZE - 1) && (XIPConfigured == TRUE)) {

        Status = XIPLocatePages (File, &PhysicalAddress);

        if (NT_SUCCESS(Status)) {

            XipFrameNumber = (PFN_NUMBER) (PhysicalAddress.QuadPart >> PAGE_SHIFT);
            //
            // The small control area will need to be reallocated as a large
            // one so the starting frame number can be inserted.  Set XipFile
            // to denote this.
            //

            XipFile = TRUE;
        }
    }

    //
    // If this image is global per session (or is going to be executed directly
    // from ROM), then allocate a large control area.  Note this doesn't need
    // to be done for systemwide global control areas or non-global control
    // areas.
    //

    GlobalPerSession = FALSE;
    if ((ControlArea->u.Flags.GlobalMemory) &&
        ((LoaderFlags & IMAGE_LOADER_FLAGS_SYSTEM_GLOBAL) == 0)) {
            GlobalPerSession = TRUE;
    }

    if ((XipFile == TRUE) || (GlobalPerSession == TRUE)) {

        LargeControlArea = ExAllocatePoolWithTag(NonPagedPool,
                                            (ULONG)(sizeof(LARGE_CONTROL_AREA) +
                                                    (sizeof(SUBSECTION) *
                                                    SubsectionsAllocated)),
                                                    'iCmM');
        if (LargeControlArea == NULL) {

            //
            // The requested pool could not be allocated.  If the image is
            // execute-in-place only (ie: not global per session), then just
            // execute it normally instead of inplace (to avoid not executing
            // it at all).
            //

            if ((XipFile == TRUE) && (GlobalPerSession == FALSE)) {
                goto SkipLargeControlArea;
            }

            Status = STATUS_INSUFFICIENT_RESOURCES;

            goto ReturnCommitChargeOnError;
        }

        //
        // Copy the normal control area into our larger one, fix the linkages,
        // Fill in the additional fields in the new one and free the old one.
        //

        RtlCopyMemory (LargeControlArea, ControlArea, sizeof(CONTROL_AREA));

        ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

        if (XipFile == TRUE) {

            //
            // Mark the large control area accordingly.  If we can't, then 
            // throw it away and use the small control area and execute from
            // RAM instead.
            //

            if (MiMakeControlAreaRom (File, LargeControlArea, XipFrameNumber) == FALSE) {
                if (GlobalPerSession == FALSE) {
                    ExFreePool (LargeControlArea);
                    goto SkipLargeControlArea;
                }
            }
        }

        Subsection = (PSUBSECTION)(ControlArea + 1);
        NewSubsection = (PSUBSECTION)(LargeControlArea + 1);

        for (i = 0; i < SubsectionsAllocated; i += 1) {
            RtlCopyMemory (NewSubsection, Subsection, sizeof(SUBSECTION));
            NewSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;
            NewSubsection->NextSubsection = (NewSubsection + 1);

            PointerPte = NewSegment->PrototypePte;

            TempPte.u.Long = MiGetSubsectionAddressForPte (NewSubsection);
            TempPte.u.Soft.Prototype = 1;

            for (j = 0; j < NewSegment->TotalNumberOfPtes; j += 1) {

                if ((PointerPte->u.Soft.Prototype == 1) &&
                    (MiGetSubsectionAddress (PointerPte) == Subsection)) {
                        OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (PointerPte);
                        TempPte.u.Soft.Protection = OriginalProtection;
                        MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                }
                PointerPte += 1;
            }

            Subsection += 1;
            NewSubsection += 1;
        }

        (NewSubsection - 1)->NextSubsection = NULL;

        NewSegment->ControlArea = (PCONTROL_AREA) LargeControlArea;

        if (GlobalPerSession == TRUE) {
            LargeControlArea->u.Flags.GlobalOnlyPerSession = 1;

            LargeControlArea->SessionId = 0;
            InitializeListHead (&LargeControlArea->UserGlobalList);
        }

        ExFreePool (ControlArea);

        ControlArea = (PCONTROL_AREA) LargeControlArea;
    }

SkipLargeControlArea:

    MiUnmapImageHeaderInHyperSpace ();

    //
    // Set the PFN database entry for this page to look like a transition
    // page.
    //

    PointerPte = NewSegment->PrototypePte;

    MiUpdateImageHeaderPage (PointerPte, PageFrameNumber, ControlArea);
    if (ExtendedHeader != NULL) {
        ExFreePool (ExtendedHeader);
    }
    ExFreePool (InPageEvent);

    return STATUS_SUCCESS;


    //
    // Error returns from image verification.
    //

ReturnCommitChargeOnError:

    NumberOfCommittedPages = NewSegment->NumberOfCommittedPages;

    if (NumberOfCommittedPages != 0) {
        MiReturnCommitment (NumberOfCommittedPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_IMAGE_NO_LARGE_CA, NumberOfCommittedPages);
        InterlockedExchangeAddSizeT (&MmSharedCommit, 0-NumberOfCommittedPages);
    }

    ExFreePool (NewSegment);
    ExFreePool (ControlArea);
    goto NeImage;

BadPeImageSegment:

    ExFreePool (NewSegment);
    ExFreePool (ControlArea);

//BadPeImage:
    Status = STATUS_INVALID_IMAGE_FORMAT;

NeImage:
    MiUnmapImageHeaderInHyperSpace ();

BadSection:
    MiRemoveImageHeaderPage(PageFrameNumber);
    if (ExtendedHeader != NULL) {
        ExFreePool (ExtendedHeader);
    }
    ExFreePool (InPageEvent);
    return Status;
}


LOGICAL
MiCheckDosCalls (
    IN PIMAGE_OS2_HEADER Os2Header,
    IN ULONG HeaderSize
    )

/*++

Routine Description:

Arguments:

Return Value:

    Returns the status value.

--*/

{
    PUCHAR ImportTable;
    UCHAR EntrySize;
    USHORT ModuleCount;
    USHORT ModuleSize;
    USHORT i;
    PUSHORT ModuleTable;

    PAGED_CODE();

    //
    // If there are no modules to check return immediately.
    //

    ModuleCount = Os2Header->ne_cmod;

    if (ModuleCount == 0) {
        return FALSE;
    }

    //
    // exe headers are notorious for having junk values for offsets
    // in the import table and module table.  We guard against this
    // via careful checking plus our exception handler.
    //

    try {

        //
        // Find out where the Module ref table is. Mod table has two byte
        // for each entry in import table. These two bytes tell the offset
        // in the import table for that entry.
        //

        ModuleTable = (PUSHORT)((PCHAR)Os2Header + (ULONG)Os2Header->ne_modtab);

        //
        // Make sure that the module table fits within the passed-in header.
        // Note that each module table entry is 2 bytes long.
        //

        if (((ULONG)Os2Header->ne_modtab + (ModuleCount * 2)) > HeaderSize) {
            return FALSE;
        }

        //
        // Now search individual entries for DOSCALLs.
        //

        for (i = 0; i < ModuleCount; i += 1) {

            ModuleSize = *((UNALIGNED USHORT *)ModuleTable);

            //
            // Import table has count byte followed by the string where count
            // is the string length.
            //

            ImportTable = (PUCHAR)((PCHAR)Os2Header +
                          (ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize);

            //
            // Make sure the offset is within the valid range.
            //

            if (((ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize)
                            >= HeaderSize) {
                return FALSE;
            }

            EntrySize = *ImportTable++;

            //
            // 0 is a bad size, bail out.
            //

            if (EntrySize == 0) {
                return FALSE;
            }

            //
            // Make sure the offset is within the valid range.
            // The sizeof(UCHAR) is included in the check because ImportTable
            // was incremented above and is used in the RtlEqualMemory
            // comparison below.
            //

            if (((ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize +
                            (ULONG)EntrySize + sizeof(UCHAR)) > HeaderSize) {
                return FALSE;
            }

            //
            // If size matches compare DOSCALLS.
            //

            if (EntrySize == 8) {
                if (RtlEqualMemory (ImportTable, "DOSCALLS", 8) ) {
                    return TRUE;
                }
            }

            //
            // Move on to the next module table entry.  Each entry is 2 bytes.
            //

            ModuleTable = (PUSHORT)((PCHAR)ModuleTable + 2);
        }
    } except (EXCEPTION_EXECUTE_HANDLER) {
        return FALSE;
    }

    return FALSE;
}


NTSTATUS
MiVerifyImageHeader (
    IN PIMAGE_NT_HEADERS NtHeader,
    IN PIMAGE_DOS_HEADER DosHeader,
    IN ULONG NtHeaderSize
    )

/*++

Routine Description:

    This function checks for various inconsistencies in the image header.

Arguments:

    NtHeader - Supplies a pointer to the NT header of the image.

    DosHeader - Supplies a pointer to the DOS header of the image.

    NtHeaderSize - Supplies the size in bytes of the NT header.

Return Value:

    NTSTATUS.

--*/

{
    PCONFIGPHARLAP PharLapConfigured;
    PUCHAR         pb;
    LONG           pResTableAddress;

    PAGED_CODE();

    if (NtHeader->Signature != IMAGE_NT_SIGNATURE) {
        if ((USHORT)NtHeader->Signature == (USHORT)IMAGE_OS2_SIGNATURE) {

            //
            // Check to see if this is a win-16 image.
            //

            if ((!MiCheckDosCalls ((PIMAGE_OS2_HEADER)NtHeader, NtHeaderSize)) &&
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 2)
                                ||
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 0)  &&
                  (((((PIMAGE_OS2_HEADER)NtHeader)->ne_expver & 0xff00) ==
                        0x200)  ||
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_expver & 0xff00) ==
                        0x300))))) {

                //
                // This is a win-16 image.
                //

                return STATUS_INVALID_IMAGE_WIN_16;
            }

            // The following OS/2 headers types go to NTDOS
            //
            // - exetype == 5 means binary is for Dos 4.0.
            //                e.g Borland Dos extender type
            //
            // - OS/2 apps which have no import table entries
            //   cannot be meant for the OS/2 ss.
            //                e.g. QuickC for dos binaries
            //
            //  - "old" Borland Dosx BC++ 3.x, Paradox 4.x
            //     exe type == 1
            //     DosHeader->e_cs * 16 + DosHeader->e_ip + 0x200 - 10
            //     contains the string " mode EXE$"
            //     but import table is empty, so we don't make special check
            //

            if (((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 5  ||
                ((PIMAGE_OS2_HEADER)NtHeader)->ne_enttab ==
                  ((PIMAGE_OS2_HEADER)NtHeader)->ne_imptab  )
               {
                return STATUS_INVALID_IMAGE_PROTECT;
            }


            //
            // Borland Dosx types: exe type 1
            //
            //  - "new" Borland Dosx BP7.0
            //     exe type == 1
            //     DosHeader + 0x200 contains the string "16STUB"
            //     0x200 happens to be e_parhdr*16
            //

            if (((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 1 &&
                RtlEqualMemory((PUCHAR)DosHeader + 0x200, "16STUB", 6) )
               {
                return STATUS_INVALID_IMAGE_PROTECT;
            }

            //
            // Check for PharLap extended header which we run as a dos app.
            // The PharLap config block is pointed to by the SizeofHeader
            // field in the DosHdr.
            // The following algorithm for detecting a pharlap exe
            // was recommended by PharLap Software Inc.
            //

            PharLapConfigured =(PCONFIGPHARLAP) ((PCHAR)DosHeader +
                                      ((ULONG)DosHeader->e_cparhdr << 4));

            if ((PCHAR)PharLapConfigured <
                       (PCHAR)DosHeader + PAGE_SIZE - sizeof(CONFIGPHARLAP)) {
                if (RtlEqualMemory(&PharLapConfigured->uchCopyRight[0x18],
                                   "Phar Lap Software, Inc.", 24) &&
                    (PharLapConfigured->usSign == 0x4b50 ||  // stub loader type 2
                     PharLapConfigured->usSign == 0x4f50 ||  // bindable 286|DosExtender
                     PharLapConfigured->usSign == 0x5650  )) // bindable 286|DosExtender (Adv)
                  {
                    return STATUS_INVALID_IMAGE_PROTECT;
                }
            }



            //
            // Check for Rational extended header which we run as a dos app.
            // We look for the rational copyright at:
            //     wCopyRight = *(DosHeader->e_cparhdr*16 + 30h)
            //     pCopyRight = wCopyRight + DosHeader->e_cparhdr*16
            //     "Copyright (C) Rational Systems, Inc."
            //

            pb = ((PUCHAR)DosHeader + ((ULONG)DosHeader->e_cparhdr << 4));

            if ((ULONG_PTR)pb < (ULONG_PTR)DosHeader + PAGE_SIZE - 0x30 - sizeof(USHORT)) {
                pb += *(PUSHORT)(pb + 0x30);
                if ( (ULONG_PTR)pb < (ULONG_PTR)DosHeader + PAGE_SIZE - 36 &&
                     RtlEqualMemory(pb,
                                    "Copyright (C) Rational Systems, Inc.",
                                    36) )
                   {
                    return STATUS_INVALID_IMAGE_PROTECT;
                }
            }

            //
            // Check for lotus 123 family of applications. Starting
            // with 123 3.0 (till recently shipped 123 3.4), every
            // exe header is bound but is meant for DOS. This can
            // be checked via, a string signature in the extended
            // header. <len byte>"1-2-3 Preloader" is the string
            // at ne_nrestab offset.
            //

            pResTableAddress = ((PIMAGE_OS2_HEADER)NtHeader)->ne_nrestab;
            if (pResTableAddress > DosHeader->e_lfanew &&
                ((ULONG)((pResTableAddress+16) - DosHeader->e_lfanew) <
                            NtHeaderSize) &&
                RtlEqualMemory(
                    ((PUCHAR)NtHeader + 1 +
                             (ULONG)(pResTableAddress - DosHeader->e_lfanew)),
                    "1-2-3 Preloader",
                    15) ) {
                    return STATUS_INVALID_IMAGE_PROTECT;
            }

            return STATUS_INVALID_IMAGE_NE_FORMAT;
        }

        if ((USHORT)NtHeader->Signature == (USHORT)IMAGE_OS2_SIGNATURE_LE) {

            //
            // This is a LE (OS/2) image. We don't support it, so give it to
            // DOS subsystem. There are cases (Rbase.exe) which have a LE
            // header but actually it is suppose to run under DOS. When we
            // do support LE format, some work needs to be done here to
            // decide whether to give it to VDM or OS/2.
            //

            return STATUS_INVALID_IMAGE_PROTECT;
        }
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if ((NtHeader->FileHeader.Machine == 0) &&
        (NtHeader->FileHeader.SizeOfOptionalHeader == 0)) {

        //
        // This is a bogus DOS app which has a 32-bit portion
        // masquerading as a PE image.
        //

        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if (!(NtHeader->FileHeader.Characteristics & IMAGE_FILE_EXECUTABLE_IMAGE)) {
        return STATUS_INVALID_IMAGE_FORMAT;
    }

#ifdef i386

    //
    // Make sure the image header is aligned on a Long word boundary.
    //

    if (((ULONG_PTR)NtHeader & 3) != 0) {
        return STATUS_INVALID_IMAGE_FORMAT;
    }
#endif

#define VALIDATE_NTHEADER(Hdr) {                                    \
    /* File alignment must be multiple of 512 and power of 2. */    \
    if (((((Hdr)->OptionalHeader).FileAlignment & 511) != 0) &&     \
        (((Hdr)->OptionalHeader).FileAlignment !=                   \
         ((Hdr)->OptionalHeader).SectionAlignment)) {               \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).FileAlignment == 0) {               \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((((Hdr)->OptionalHeader).FileAlignment - 1) &              \
          ((Hdr)->OptionalHeader).FileAlignment) != 0) {            \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).SectionAlignment < ((Hdr)->OptionalHeader).FileAlignment) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).SizeOfImage > MM_SIZE_OF_LARGEST_IMAGE) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if ((Hdr)->FileHeader.NumberOfSections > MM_MAXIMUM_IMAGE_SECTIONS) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR32_MAGIC) && \
        !((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_I386) ) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) && \
        !(((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_IA64) || \
          ((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_AMD64))) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
}

    if (NtHeader->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR_MAGIC) {

        //
        // Image doesn't have the right magic value in its optional header.
        //

#if defined (_WIN64)
        if (NtHeader->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR32_MAGIC) {

            //
            // PE32 image.  Validate it as such.
            //

            PIMAGE_NT_HEADERS32 NtHeader32 = (PIMAGE_NT_HEADERS32)NtHeader;

            VALIDATE_NTHEADER(NtHeader32);
            return STATUS_SUCCESS;
        }
#else /* !defined(_WIN64) */
        if (NtHeader->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) {

            //
            // 64bit image on a 32bit machine.
            //
            return STATUS_INVALID_IMAGE_WIN_64;
        }
#endif
        return STATUS_INVALID_IMAGE_FORMAT;
    }

    VALIDATE_NTHEADER(NtHeader);
    #undef VALIDATE_NTHEADER

    return STATUS_SUCCESS;
}

NTSTATUS
MiCreateDataFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN ULONG IgnoreFileSizing
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of a data file.

    The data file is accessed to verify desired access, a segment
    object is created and initialized.

Arguments:

    File - Supplies the file object for the image file.

    Segment - Returns the segment object.

    MaximumSize - Supplies the maximum size for the mapping.

    SectionPageProtection - Supplies the initial page protection.

    AllocationAttributes - Supplies the allocation attributes for the mapping.

    IgnoreFileSizing - Supplies TRUE if the cache manager is specifying the
                       file size and so it does not need to be validated.

Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    ULONG NumberOfPtes;
    ULONG j;
    ULONG Size;
    ULONG PartialSize;
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    PMAPPED_FILE_SEGMENT NewSegment;
    PMSUBSECTION Subsection;
    PMSUBSECTION ExtendedSubsection;
    PMSUBSECTION LargeExtendedSubsection;
    MMPTE TempPte;
    UINT64 EndOfFile;
    UINT64 LastFileChunk;
    UINT64 FileOffset;
    UINT64 NumberOfPtesForEntireFile;
    ULONG ExtendedSubsections;
    PMSUBSECTION Last;
    ULONG NumberOfNewSubsections;
    SIZE_T AllocationFragment;
    PHYSICAL_ADDRESS PhysicalAddress;
    PFN_NUMBER PageFrameNumber;

    PAGED_CODE();

    ExtendedSubsections = 0;

    // *************************************************************
    // Create mapped file section.
    // *************************************************************

    if (!IgnoreFileSizing) {

        Status = FsRtlGetFileSize (File, (PLARGE_INTEGER)&EndOfFile);

        if (Status == STATUS_FILE_IS_A_DIRECTORY) {

            //
            // Can't map a directory as a section. Return error.
            //

            return STATUS_INVALID_FILE_FOR_SECTION;
        }

        if (!NT_SUCCESS (Status)) {
            return Status;
        }

        if (EndOfFile == 0 && *MaximumSize == 0) {

            //
            // Can't map a zero length without specifying the maximum
            // size as non-zero.
            //

            return STATUS_MAPPED_FILE_SIZE_ZERO;
        }

        //
        // Make sure this file is big enough for the section.
        //

        if (*MaximumSize > EndOfFile) {

            //
            // If the maximum size is greater than the end-of-file,
            // and the user did not request page_write or page_execute_readwrite
            // to the section, reject the request.
            //

            if (((SectionPageProtection & PAGE_READWRITE) |
                (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

                return STATUS_SECTION_TOO_BIG;
            }

            //
            // Check to make sure that the allocation size large enough
            // to contain all the data, if not set a new allocation size.
            //

            EndOfFile = *MaximumSize;

            Status = FsRtlSetFileSize (File, (PLARGE_INTEGER)&EndOfFile);

            if (!NT_SUCCESS (Status)) {
                return Status;
            }
        }
    }
    else {

        //
        // Ignore the file size, this call is from the cache manager.
        //

        EndOfFile = *MaximumSize;
    }

    //
    // Calculate the number of prototype PTE chunks to build for this section.
    // A subsection is also allocated for each chunk as all the prototype PTEs
    // in any given chunk are initially encoded to point at the same subsection.
    //
    // The maximum total section size is 16PB (2^54).  This is because the
    // StartingSector4132 field in each subsection, ie: 2^42-1 bits of file
    // offset where the offset is in 4K (not pagesize) units.  Thus, a
    // subsection may describe a *BYTE* file start offset of maximum
    // 2^54 - 4K.
    //
    // Each subsection can span at most 16TB - 64K.
    //
    // This is because the NumberOfFullSectors and various other fields
    // in the subsection are ULONGs.
    //
    // The last item to notice is that the NumberOfSubsections is currently
    // a USHORT in the ControlArea.  Note this does not limit us to 64K-1
    // subsections because this field is only relied on for images not data.
    //

    NumberOfPtesForEntireFile = (EndOfFile + PAGE_SIZE - 1) >> PAGE_SHIFT;

    NumberOfPtes = (ULONG)NumberOfPtesForEntireFile;

    if (EndOfFile > MI_MAXIMUM_SECTION_SIZE) {
        return STATUS_SECTION_TOO_BIG;
    }

    if (NumberOfPtesForEntireFile > (UINT64)((MAXULONG_PTR / sizeof(MMPTE)) - sizeof (SEGMENT))) {
        return STATUS_SECTION_TOO_BIG;
    }

    if (NumberOfPtesForEntireFile > EndOfFile) {
        return STATUS_SECTION_TOO_BIG;
    }

    NewSegment = ExAllocatePoolWithTag (PagedPool,
                                        sizeof(MAPPED_FILE_SEGMENT),
                                        'mSmM');

    if (NewSegment == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Allocate the subsection memory in smaller sizes so the corresponding
    // prototype PTEs can be trimmed later if paged pool virtual address
    // space becomes scarce.  Note the size is snapped locally so it can
    // be changed dynamically without locking.
    //

    AllocationFragment = MmAllocationFragment;

    ASSERT (MiGetByteOffset (AllocationFragment) == 0);
    ASSERT (AllocationFragment >= PAGE_SIZE);
    ASSERT64 (AllocationFragment < _4gb);

    Size = (ULONG) AllocationFragment;
    PartialSize = NumberOfPtes * sizeof(MMPTE);

    NumberOfNewSubsections = 0;
    ExtendedSubsection = NULL;

    //
    // Initializing Last is not needed for correctness, but without it the
    // compiler cannot compile this code W4 to check for use of uninitialized
    // variables.
    //

    Last = NULL;

    ControlArea = (PCONTROL_AREA)File->SectionObjectPointer->DataSectionObject;

    do {

        if (PartialSize < (ULONG) AllocationFragment) {
            PartialSize = (ULONG) ROUND_TO_PAGES (PartialSize);
            Size = PartialSize;
        }

        if (ExtendedSubsection == NULL) {
            ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);

            //
            // Control area and first subsection were zeroed at allocation time.
            //
        }
        else {

            ExtendedSubsection = ExAllocatePoolWithTag (NonPagedPool,
                                                        sizeof(MSUBSECTION),
                                                        'cSmM');

            if (ExtendedSubsection == NULL) {
                ExFreePool (NewSegment);

                //
                // Free all the previous allocations and return an error.
                //

                ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);
                ExtendedSubsection = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                while (ExtendedSubsection != NULL) {
                    Last = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                    ExFreePool (ExtendedSubsection);
                    ExtendedSubsection = Last;
                }
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            RtlZeroMemory (ExtendedSubsection, sizeof(MSUBSECTION));
            Last->NextSubsection = (PSUBSECTION) ExtendedSubsection;
        }

        NumberOfNewSubsections += 1;

        ExtendedSubsection->PtesInSubsection = Size / sizeof(MMPTE);

        Last = ExtendedSubsection;
        PartialSize -= Size;
    } while (PartialSize != 0);

    *Segment = (PSEGMENT) NewSegment;
    RtlZeroMemory (NewSegment, sizeof(MAPPED_FILE_SEGMENT));

    NewSegment->LastSubsectionHint = ExtendedSubsection;

    //
    // Control area and first subsection were zeroed at allocation time.
    //

    ControlArea->Segment = (PSEGMENT) NewSegment;
    ControlArea->NumberOfSectionReferences = 1;

    if (IgnoreFileSizing == FALSE) {

        //
        // This reference is not from the cache manager.
        //

        ControlArea->NumberOfUserReferences = 1;
    }
    else {

        //
        // Set the was purged flag to indicate that the
        // file size was not explicitly set.
        //

        ControlArea->u.Flags.WasPurged = 1;
    }

    ControlArea->u.Flags.BeingCreated = 1;
    ControlArea->u.Flags.File = 1;

    if (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics) {

        //
        // This file resides on a redirected drive.
        //

        ControlArea->u.Flags.Networked = 1;
    }

    if (AllocationAttributes & SEC_NOCACHE) {
        ControlArea->u.Flags.NoCache = 1;
    }

    //
    // Note this count is not correct if we have 65535 or more subsections
    // for this file, but that's ok because the count is only relied on
    // for images, not data anyway.
    //

    ControlArea->NumberOfSubsections = (USHORT)NumberOfNewSubsections;
    ControlArea->FilePointer = File;

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PMSUBSECTION)(ControlArea + 1);

    //
    // Loop through all the subsections and fill in the PTEs.
    //

    TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
    TempPte.u.Soft.Prototype = 1;

    //
    // Set all the PTEs to the execute-read-write protection.
    // The section will control access to these and the segment
    // must provide a method to allow other users to map the file
    // for various protections.
    //

    TempPte.u.Soft.Protection = MM_EXECUTE_READWRITE;

    NewSegment->ControlArea = ControlArea;
    NewSegment->SizeOfSegment = EndOfFile;
    NewSegment->TotalNumberOfPtes = NumberOfPtes;

    NewSegment->SegmentPteTemplate = TempPte;

    if (Subsection->NextSubsection != NULL) {

        //
        // Multiple segments and subsections.
        // Align first so it is a multiple of the allocation size.
        //

        NewSegment->NonExtendedPtes =
          (Subsection->PtesInSubsection & ~(((ULONG)AllocationFragment >> PAGE_SHIFT) - 1));
    }
    else {
        NewSegment->NonExtendedPtes = NumberOfPtes;
    }

    Subsection->PtesInSubsection = NewSegment->NonExtendedPtes;

    FileOffset = 0;

    do {

        //
        // Loop through all the subsections to initialize them.
        //

        Subsection->ControlArea = ControlArea;

        Mi4KStartForSubsection(&FileOffset, Subsection);

        Subsection->u.SubsectionFlags.Protection = MM_EXECUTE_READWRITE;

        if (Subsection->NextSubsection == NULL) {

            LastFileChunk = (EndOfFile >> MM4K_SHIFT) - FileOffset;

            //
            // Note this next line restricts the number of bytes mapped by
            // a single subsection to 16TB-4K.  Multiple subsections can always
            // be chained together to support an overall file of size 16K TB.
            //

            Subsection->NumberOfFullSectors = (ULONG)LastFileChunk;

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                 (ULONG) EndOfFile & MM4K_MASK;

            j = Subsection->PtesInSubsection;

            Subsection->PtesInSubsection = (ULONG)(
                NumberOfPtesForEntireFile -
                                (FileOffset >> (PAGE_SHIFT - MM4K_SHIFT)));

            MI_CHECK_SUBSECTION (Subsection);

            Subsection->UnusedPtes = j - Subsection->PtesInSubsection;
        }
        else {
            Subsection->NumberOfFullSectors =
                Subsection->PtesInSubsection << (PAGE_SHIFT - MM4K_SHIFT);

            MI_CHECK_SUBSECTION (Subsection);
        }

        FileOffset += Subsection->PtesInSubsection <<
                                        (PAGE_SHIFT - MM4K_SHIFT);
        Subsection = (PMSUBSECTION) Subsection->NextSubsection;
    } while (Subsection != NULL);

    if (XIPConfigured == TRUE) {

        Status = XIPLocatePages (File, &PhysicalAddress);

        if (NT_SUCCESS(Status)) {

            PageFrameNumber = (PFN_NUMBER) (PhysicalAddress.QuadPart >> PAGE_SHIFT);
            //
            // Allocate a large control area (so the starting frame number
            // can be saved) and repoint all the created subsections to it.
            //

            LargeControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                            (ULONG)(sizeof(LARGE_CONTROL_AREA) +
                                                    sizeof(MSUBSECTION)),
                                                    MMCONTROL);

            if (LargeControlArea != NULL) {

                *(PCONTROL_AREA) LargeControlArea = *ControlArea;

                if (MiMakeControlAreaRom (File, LargeControlArea, PageFrameNumber) == TRUE) {

                    LargeExtendedSubsection = (PMSUBSECTION)(LargeControlArea + 1);
                    ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);

                    *LargeExtendedSubsection = *ExtendedSubsection;
                    LargeExtendedSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;

                    //
                    // Only the first subsection needed to be directly modified
                    // as above because it is allocated in a single chunk with
                    // the control area.  Any additional subsections below
                    // just need their control area pointers updated.
                    //

                    ASSERT (NumberOfNewSubsections >= 1);
                    j = NumberOfNewSubsections - 1;

                    while (j != 0) {

                        ExtendedSubsection = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                        ExtendedSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;
                        j -= 1;
                    }

                    NewSegment->ControlArea = (PCONTROL_AREA) LargeControlArea;
                }
                else {
                    ExFreePool (LargeControlArea);
                }
            }
        }
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiCreatePagingFileMap (
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG ProtectionMask,
    IN ULONG AllocationAttributes
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of a paging file.

Arguments:

    Segment - Returns the segment object.

    MaximumSize - Supplies the maximum size for the mapping.

    ProtectionMask - Supplies the initial page protection.

    AllocationAttributes - Supplies the allocation attributes for the
                           mapping.

Return Value:

    NTSTATUS.

--*/


{
    PFN_NUMBER NumberOfPtes;
    ULONG SizeOfSegment;
    ULONG i;
    PCONTROL_AREA ControlArea;
    PSEGMENT NewSegment;
    PMMPTE PointerPte;
    PSUBSECTION Subsection;
    MMPTE TempPte;

    PAGED_CODE();

    //*******************************************************************
    // Create a section backed by the paging file.
    //*******************************************************************

    if (*MaximumSize == 0) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // Limit page file backed sections to the pagefile maximum size.
    //

    if (*MaximumSize > (MI_MAXIMUM_PAGEFILE_SIZE - (1024 * 1024))) {
        return STATUS_SECTION_TOO_BIG;
    }

    //
    // Create the segment object.
    //

    //
    // Calculate the number of prototype PTEs to build for this segment.
    //

    NumberOfPtes = BYTES_TO_PAGES (*MaximumSize);

    if (AllocationAttributes & SEC_COMMIT) {

        //
        // Commit the pages for the section.
        //

        ASSERT (ProtectionMask != 0);

        if (MiChargeCommitment (NumberOfPtes, NULL) == FALSE) {
            return STATUS_COMMITMENT_LIMIT;
        }
    }

    SizeOfSegment = sizeof(SEGMENT) + sizeof(MMPTE) * ((ULONG)NumberOfPtes - 1);

    NewSegment = ExAllocatePoolWithTag (PagedPool, SizeOfSegment, MMSECT);

    if (NewSegment == NULL) {

        //
        // The requested pool could not be allocated.
        //

        if (AllocationAttributes & SEC_COMMIT) {
            MiReturnCommitment (NumberOfPtes);
        }
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    *Segment = NewSegment;

    ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                 (ULONG)sizeof(CONTROL_AREA) +
                                                (ULONG)sizeof(SUBSECTION),
                                         MMCONTROL);

    if (ControlArea == NULL) {

        //
        // The requested pool could not be allocated.
        //

        ExFreePool (NewSegment);

        if (AllocationAttributes & SEC_COMMIT) {
            MiReturnCommitment (NumberOfPtes);
        }
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Zero control area and first subsection.
    //

    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA) + sizeof(SUBSECTION));

    ControlArea->Segment = NewSegment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->NumberOfUserReferences = 1;
    ControlArea->NumberOfSubsections = 1;

    if (AllocationAttributes & SEC_BASED) {
        ControlArea->u.Flags.Based = 1;
    }

    if (AllocationAttributes & SEC_RESERVE) {
        ControlArea->u.Flags.Reserve = 1;
    }

    if (AllocationAttributes & SEC_COMMIT) {
        ControlArea->u.Flags.Commit = 1;
    }

    Subsection = (PSUBSECTION)(ControlArea + 1);

    Subsection->ControlArea = ControlArea;
    Subsection->PtesInSubsection = (ULONG)NumberOfPtes;
    Subsection->u.SubsectionFlags.Protection = ProtectionMask;

    //
    // Align the prototype PTEs on the proper boundary.
    //

    PointerPte = &NewSegment->ThePtes[0];
    i = (ULONG) (((ULONG_PTR)PointerPte >> PTE_SHIFT) &
                    ((MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - 1));

    if (i != 0) {
        i = (MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE) - i;
    }

    //
    // Zero the segment header.
    //

    RtlZeroMemory (NewSegment, sizeof(SEGMENT));

    NewSegment->PrototypePte = &NewSegment->ThePtes[i];

    NewSegment->ControlArea = ControlArea;

    //
    // Record the process that created this segment for the performance
    // analysis tools.
    //

    NewSegment->u1.CreatingProcess = PsGetCurrentProcess ();

    NewSegment->SizeOfSegment = (UINT64)NumberOfPtes * PAGE_SIZE;
    NewSegment->TotalNumberOfPtes = (ULONG)NumberOfPtes;
    NewSegment->NonExtendedPtes = (ULONG)NumberOfPtes;

    PointerPte = NewSegment->PrototypePte;
    Subsection->SubsectionBase = PointerPte;
    TempPte = ZeroPte;

    if (AllocationAttributes & SEC_COMMIT) {
        TempPte.u.Soft.Protection = ProtectionMask;

        //
        // Record commitment charging.
        //

        MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGEFILE_BACKED_SHMEM, NumberOfPtes);

        NewSegment->NumberOfCommittedPages = NumberOfPtes;

        InterlockedExchangeAddSizeT (&MmSharedCommit, NumberOfPtes);
    }

    NewSegment->SegmentPteTemplate.u.Soft.Protection = ProtectionMask;

    //
    // Set all the prototype PTEs to either no access or demand zero
    // depending on the commit flag.
    //

    MiFillMemoryPte (PointerPte, NumberOfPtes * sizeof(MMPTE), TempPte.u.Long);

    return STATUS_SUCCESS;
}


NTSTATUS
NtOpenSection (
    OUT PHANDLE SectionHandle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes
    )

/*++

Routine Description:

    This function opens a handle to a section object with the specified
    desired access.

Arguments:


    Sectionhandle - Supplies a pointer to a variable that will
                    receive the section object handle value.

    DesiredAccess - Supplies the desired types of access  for the
                    section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.

    ObjectAttributes - Supplies a pointer to an object attributes structure.

Return Value:

    NTSTATUS.

--*/

{
    HANDLE Handle;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;

    PAGED_CODE();
    //
    // Get previous processor mode and probe output arguments if necessary.
    //

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {
        try {
            ProbeForWriteHandle(SectionHandle);
        } except (EXCEPTION_EXECUTE_HANDLER) {
            return GetExceptionCode();
        }
    }

    //
    // Open handle to the section object with the specified desired
    // access.
    //

    Status = ObOpenObjectByName (ObjectAttributes,
                                 MmSectionObjectType,
                                 PreviousMode,
                                 NULL,
                                 DesiredAccess,
                                 NULL,
                                 &Handle
                                 );

    try {
        *SectionHandle = Handle;
    } except (EXCEPTION_EXECUTE_HANDLER) {
        return Status;
    }

    return Status;
}

CCHAR
MiGetImageProtection (
    IN ULONG SectionCharacteristics
    )

/*++

Routine Description:

    This function takes a section characteristic mask from the
    image and converts it to an PTE protection mask.

Arguments:

    SectionCharacteristics - Supplies the characteristics mask from the
                             image.

Return Value:

    Returns the protection mask for the PTE.

--*/

{
    ULONG Index;
    PAGED_CODE();

    Index = 0;
    if (SectionCharacteristics & IMAGE_SCN_MEM_EXECUTE) {
        Index |= 1;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_READ) {
        Index |= 2;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_WRITE) {
        Index |= 4;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_SHARED) {
        Index |= 8;
    }

    return MmImageProtectionArray[Index];
}

PFN_NUMBER
MiGetPageForHeader (
    VOID
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, removes
    a page and updates the PFN database as though the page was
    ready to be deleted if the reference count is decremented.

Arguments:

    None.

Return Value:

    Returns the physical page frame number.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameNumber;
    PMMPFN Pfn1;
    PEPROCESS Process;
    ULONG PageColor;

    Process = PsGetCurrentProcess();
    PageColor = MI_PAGE_COLOR_VA_PROCESS ((PVOID)X64K,
                                          &Process->NextPageColor);

    //
    // Lock the PFN database and get a page.
    //

    LOCK_PFN (OldIrql);

    MiEnsureAvailablePageOrWait (NULL, NULL);

    //
    // Remove page for 64k alignment.
    //

    PageFrameNumber = MiRemoveAnyPage (PageColor);

    //
    // Increment the reference count for the page so the
    // paging I/O will work, and so this page cannot be stolen from us.
    //

    Pfn1 = MI_PFN_ELEMENT (PageFrameNumber);
    Pfn1->u3.e2.ReferenceCount += 1;

    //
    // Don't need the PFN lock for the fields below...
    //

    UNLOCK_PFN (OldIrql);

    Pfn1->OriginalPte = ZeroPte;
    Pfn1->PteAddress = (PVOID) (ULONG_PTR)X64K;
    MI_SET_PFN_DELETED (Pfn1);

    return PageFrameNumber;
}

VOID
MiUpdateImageHeaderPage (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER PageFrameNumber,
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, and
    turns the specified prototype PTE into a transition PTE
    referring to the specified physical page.  It then
    decrements the reference count causing the page to
    be placed on the standby or modified list.

Arguments:

    PointerPte - Supplies the PTE to set into the transition state.

    PageFrameNumber - Supplies the physical page.

    ControlArea - Supplies the control area for the prototype PTEs.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    MiMakeSystemAddressValidPfn (PointerPte);

    MiInitializeTransitionPfn (PageFrameNumber, PointerPte);
    ControlArea->NumberOfPfnReferences += 1;

    //
    // Add the page to the standby list.
    //

    MiDecrementReferenceCount (PageFrameNumber);

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiRemoveImageHeaderPage (
    IN PFN_NUMBER PageFrameNumber
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, and decrements
    the reference count thereby causing the physical page to
    be deleted.

Arguments:

    PageFrameNumber - Supplies the PFN to decrement.

Return Value:

    None.

--*/
{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    MiDecrementReferenceCount (PageFrameNumber);
    UNLOCK_PFN (OldIrql);
    return;
}

PCONTROL_AREA
MiFindImageSectionObject(
    IN PFILE_OBJECT File,
    OUT PLOGICAL GlobalNeeded
    )

/*++

Routine Description:

    This function searches the control area chains (if any) for an existing
    cache of the specified image file.  For non-global control areas, there is
    no chain and the control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do the walk.

Arguments:

    File - Supplies the file object for the image file.

    GlobalNeeded - Supplies a pointer to store whether a global control area is
                   required as a placeholder.  This can only be set when there
                   is already some global control area in the list - ie: our
                   caller should only rely on this when this function returns
                   NULL so the caller knows what kind of control area to
                   insert.

Return Value:

    Returns the matching control area or NULL if one does not exist.

Environment:

    Must be holding the PFN lock.

--*/

{
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    PLIST_ENTRY Head, Next;
    ULONG SessionId;

    MM_PFN_LOCK_ASSERT();

    *GlobalNeeded = FALSE;

    //
    // Get first (if any) control area pointer.
    //

    ControlArea = (PCONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    //
    // If no control area, or the control area is not session global,
    // then our job is easy.  Note, however, that they each require different
    // return values as they represent different states.
    //

    if (ControlArea == NULL) {
        return NULL;
    }

    if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        return ControlArea;
    }

    LargeControlArea = (PLARGE_CONTROL_AREA) ControlArea;

    //
    // Get the current session ID and search for a matching control area.
    //

    SessionId = MmGetSessionId (PsGetCurrentProcess());

    if (LargeControlArea->SessionId == SessionId) {
        return (PCONTROL_AREA) LargeControlArea;
    }

    //
    // Must search the control area list for a matching session ID.
    //

    Head = &LargeControlArea->UserGlobalList;

    for (Next = Head->Flink; Next != Head; Next = Next->Flink) {

        LargeControlArea = CONTAINING_RECORD (Next, LARGE_CONTROL_AREA, UserGlobalList);

        ASSERT (LargeControlArea->u.Flags.GlobalOnlyPerSession == 1);

        if (LargeControlArea->SessionId == SessionId) {
            return (PCONTROL_AREA) LargeControlArea;
        }
    }

    //
    // No match, so tell our caller to create a new global control area.
    //

    *GlobalNeeded = TRUE;

    return NULL;
}

VOID
MiInsertImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA InputControlArea
    )

/*++

Routine Description:

    This function inserts the control area into the file's section object
    pointers.  For non-global control areas, there is no chain and the
    control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do a list insertion.

Arguments:

    File - Supplies the file object for the image file.

    InputControlArea - Supplies the control area to insert.

Return Value:

    None.

Environment:

    Must be holding the PFN lock.

--*/

{
    PLIST_ENTRY Head;
    PLARGE_CONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA FirstControlArea;
#if DBG
    PLIST_ENTRY Next;
    PLARGE_CONTROL_AREA NextControlArea;
#endif

    MM_PFN_LOCK_ASSERT();

    ControlArea = (PLARGE_CONTROL_AREA) InputControlArea;

    //
    // If this is not a global-per-session control area or just a placeholder
    // control area (with no chain already in place) then just put it in.
    //

    FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    if (FirstControlArea == NULL) {
        if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
            File->SectionObjectPointer->ImageSectionObject = (PVOID)ControlArea;
            return;
        }
    }

    //
    // A per-session control area needs to be inserted...
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 1);

    ControlArea->SessionId = MmGetSessionId (PsGetCurrentProcess());

    //
    // If the control area list is empty, just initialize links for this entry.
    //

    if (File->SectionObjectPointer->ImageSectionObject == NULL) {
        InitializeListHead (&ControlArea->UserGlobalList);
    }
    else {

        //
        // Insert new entry before the current first entry.  The control area
        // must be in the midst of creation/deletion or have a valid session
        // ID to be inserted.
        //

        ASSERT (ControlArea->u.Flags.BeingDeleted ||
                ControlArea->u.Flags.BeingCreated ||
                ControlArea->SessionId != (ULONG)-1);

        FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

        Head = &FirstControlArea->UserGlobalList;

#if DBG
        //
        // Ensure no duplicate session IDs exist in the list.
        //

        for (Next = Head->Flink; Next != Head; Next = Next->Flink) {
            NextControlArea = CONTAINING_RECORD (Next, LARGE_CONTROL_AREA, UserGlobalList);
            ASSERT (NextControlArea->SessionId != (ULONG)-1 &&
                    NextControlArea->SessionId != ControlArea->SessionId);
        }
#endif

        InsertTailList (Head, &ControlArea->UserGlobalList);
    }

    //
    // Update first control area pointer.
    //

    File->SectionObjectPointer->ImageSectionObject = (PVOID) ControlArea;
}

VOID
MiRemoveImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA InputControlArea
    )

/*++

Routine Description:

    This function searches the control area chains (if any) for an existing
    cache of the specified image file.  For non-global control areas, there is
    no chain and the control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do the walk.

    Upon finding the specified control area, we unlink it.

Arguments:

    File - Supplies the file object for the image file.

    InputControlArea - Supplies the control area to remove.

Return Value:

    None.

Environment:

    Must be holding the PFN lock.

--*/

{
#if DBG
    PLIST_ENTRY Head;
#endif
    PLIST_ENTRY Next;
    PLARGE_CONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA FirstControlArea;
    PLARGE_CONTROL_AREA NextControlArea;

    MM_PFN_LOCK_ASSERT();

    ControlArea = (PLARGE_CONTROL_AREA) InputControlArea;

    FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    //
    // Get a pointer to the first control area.  If this is not a
    // global-per-session control area, then there is no list, so we're done.
    //

    if (FirstControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

        File->SectionObjectPointer->ImageSectionObject = NULL;
        return;
    }

    //
    // A list may exist.  Walk it as necessary and delete the requested entry.
    //

    if (FirstControlArea == ControlArea) {

        //
        // The first entry is the one to remove.  If it is the only entry
        // in the list, then the new first entry pointer will be NULL.
        // Otherwise, get a pointer to the next entry and unlink the current.
        //

        if (IsListEmpty (&FirstControlArea->UserGlobalList)) {
            NextControlArea = NULL;
        }
        else {
            Next = FirstControlArea->UserGlobalList.Flink;
            RemoveEntryList (&FirstControlArea->UserGlobalList);
            NextControlArea = CONTAINING_RECORD (Next,
                                                 LARGE_CONTROL_AREA,
                                                 UserGlobalList);

            ASSERT (NextControlArea->u.Flags.GlobalOnlyPerSession == 1);
        }

        File->SectionObjectPointer->ImageSectionObject = (PVOID)NextControlArea;
        return;
    }

    //
    // Remove the entry, note that the ImageSectionObject need not be updated
    // as the entry is not at the head.
    //

#if DBG
    Head = &FirstControlArea->UserGlobalList;

    for (Next = Head->Flink; Next != Head; Next = Next->Flink) {

        NextControlArea = CONTAINING_RECORD (Next,
                                             LARGE_CONTROL_AREA,
                                             UserGlobalList);

        ASSERT (NextControlArea->u.Flags.GlobalOnlyPerSession == 1);

        if (NextControlArea == ControlArea) {
            break;
        }
    }
    ASSERT (Next != Head);
#endif

    RemoveEntryList (&ControlArea->UserGlobalList);
}


NTSTATUS
MiGetWritablePagesInSection (
    IN PSECTION Section,
    OUT PULONG WritablePages
    )

/*++

Routine Description:

    This routine calculates the number of writable pages in the image.

Arguments:

    Section - Supplies the section for the image.

    WritablePages - Supplies a pointer to fill with the
                    number of writable pages.

Return Value:

    STATUS_SUCCESS if all went well, otherwise various NTSTATUS error codes.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    NTSTATUS Status;
    PVOID ViewBase;
    SIZE_T ViewSize;
    KAPC_STATE ApcState;
    ULONG PagesInSubsection;
    ULONG Protection;
    ULONG NumberOfSubsections;
    ULONG OffsetToSectionTable;
    ULONG SectionVirtualSize;
    PEPROCESS Process;
    LARGE_INTEGER SectionOffset;
    PIMAGE_DOS_HEADER DosHeader;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;

    PAGED_CODE();

    ViewBase = NULL;
    ViewSize = 0;
    SectionOffset.QuadPart = 0;

    *WritablePages = 0;

    KeStackAttachProcess (&PsInitialSystemProcess->Pcb, &ApcState);

    Process = PsGetCurrentProcess();

    Status = MmMapViewOfSection (Section,
                                 Process,
                                 &ViewBase,
                                 0,
                                 0,
                                 &SectionOffset,
                                 &ViewSize,
                                 ViewUnmap,
                                 0,
                                 PAGE_EXECUTE);

    if (!NT_SUCCESS(Status)) {
        KeUnstackDetachProcess (&ApcState);
        return Status;
    }

    //
    // The DLL is mapped as a data file not as an image.  Pull apart the
    // executable header.  The security checks have already been
    // done as part of creating the section in the first place.
    //

    DosHeader = (PIMAGE_DOS_HEADER) ViewBase;

    ASSERT (DosHeader->e_magic == IMAGE_DOS_SIGNATURE);

#ifndef i386
    ASSERT (((ULONG)DosHeader->e_lfanew & 3) == 0);
#endif

    ASSERT ((ULONG)DosHeader->e_lfanew <= ViewSize);

    NtHeader = (PIMAGE_NT_HEADERS)((PCHAR)DosHeader +
                                      (ULONG)DosHeader->e_lfanew);

    OffsetToSectionTable = sizeof(ULONG) +
                              sizeof(IMAGE_FILE_HEADER) +
                              NtHeader->FileHeader.SizeOfOptionalHeader;

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                OffsetToSectionTable);

    NumberOfSubsections = NtHeader->FileHeader.NumberOfSections;

    while (NumberOfSubsections > 0) {

        Protection = MiGetImageProtection (SectionTableEntry->Characteristics);

        if (Protection & MM_PROTECTION_WRITE_MASK) {

            //
            // Handle the case where the virtual size is 0.
            //

            if (SectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = SectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
            }

            PagesInSubsection =
                MI_ROUND_TO_SIZE (SectionVirtualSize, PAGE_SIZE) >> PAGE_SHIFT;

            *WritablePages += PagesInSubsection;
        }

        SectionTableEntry += 1;
        NumberOfSubsections -= 1;
    }

    Status = MiUnmapViewOfSection (Process, ViewBase, FALSE);

    KeUnstackDetachProcess (&ApcState);

    ASSERT (NT_SUCCESS(Status));

    return Status;
}


LOGICAL
MiFlushDataSection(
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    This routine flushes the data section if there is one.

Arguments:

    File - Supplies the file object.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL and below.

--*/

{
    KIRQL OldIrql;
    IO_STATUS_BLOCK IoStatus;
    PCONTROL_AREA ControlArea;
    LOGICAL DataInUse;

    DataInUse = FALSE;

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA) File->SectionObjectPointer->DataSectionObject;

    if (ControlArea) {
        if (ControlArea->NumberOfSystemCacheViews) {
            UNLOCK_PFN (OldIrql);
            CcFlushCache (File->SectionObjectPointer,
                          NULL,
                          0,
                          &IoStatus);

        }
        else {
            UNLOCK_PFN (OldIrql);
            MmFlushSection (File->SectionObjectPointer,
                            NULL,
                            0,
                            &IoStatus,
                            TRUE);
        }

        if ((ControlArea->NumberOfSectionReferences != 0) ||
            (ControlArea->NumberOfMappedViews != 0)) {

            DataInUse = TRUE;
        }
    }
    else {
        UNLOCK_PFN (OldIrql);
    }

    return DataInUse;
}


PVOID
MiCopyHeaderIfResident (
    IN PFILE_OBJECT File,
    IN PFN_NUMBER ImagePageFrameNumber
    )

/*++

Routine Description:

    This routine copies the image header from the data section if there is
    one and the page is already resident or in transition.

Arguments:

    File - Supplies the file object.

    ImagePageFrameNumber - Supplies the image frame to copy the data into.

Return Value:

    Virtual address of the image page frame number if successful, NULL if not.

Environment:

    Kernel mode, APC_LEVEL and below.

--*/

{
    PMMPFN Pfn1;
    PVOID DataPage;
    PVOID ImagePage;
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PEPROCESS Process;
    PSUBSECTION Subsection;
    PSECTION_OBJECT_POINTERS SectionObjectPointer;

    //
    // Take a quick (safely unsynchronized) look to see whether to bother
    // mapping the image header page at all - if there's no data section
    // object, then skip it and just return.
    //

    SectionObjectPointer = File->SectionObjectPointer;
    if (SectionObjectPointer == NULL) {
        return NULL;
    }

    ControlArea = (PCONTROL_AREA) SectionObjectPointer->DataSectionObject;

    if (ControlArea == NULL) {
        return NULL;
    }

    //
    // There's a data section, so map the target page.
    //

    ImagePage = MiMapImageHeaderInHyperSpace (ImagePageFrameNumber);

    LOCK_PFN (OldIrql);

    //
    // Now that we are synchronized via the PFN lock, take a safe look.
    //

    SectionObjectPointer = File->SectionObjectPointer;
    if (SectionObjectPointer == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    ControlArea = (PCONTROL_AREA) SectionObjectPointer->DataSectionObject;

    if (ControlArea == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if ((ControlArea->u.Flags.BeingCreated) ||
        (ControlArea->u.Flags.BeingDeleted)) {

        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION) (ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // If the prototype PTEs have been tossed (or never created) then we
    // don't have any data to copy.
    //

    PointerPte = Subsection->SubsectionBase;

    if (PointerPte == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {

        //
        // We have no reference to the data section so if we can't do this
        // without relinquishing the PFN lock, then don't bother.
        // ie: the entire control area and everything can be freed
        // while a call to MiMakeSystemAddressValidPfn releases the lock.
        //

        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    PteContents = *PointerPte;

    if ((PteContents.u.Hard.Valid == 1) ||
       ((PteContents.u.Soft.Prototype == 0) &&
         (PteContents.u.Soft.Transition == 1))) {

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        }
        else {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if (Pfn1->u3.e1.ReadInProgress != 0) {
                UNLOCK_PFN (OldIrql);
                MiUnmapImageHeaderInHyperSpace ();
                return NULL;
            }
        }

        Process = PsGetCurrentProcess ();

        DataPage = MiMapPageInHyperSpaceAtDpc (Process, PageFrameIndex);

        RtlCopyMemory (ImagePage, DataPage, PAGE_SIZE);

        MiUnmapPageInHyperSpaceFromDpc (Process, DataPage);

        UNLOCK_PFN (OldIrql);

        return ImagePage;
    }

    //
    // The data page is not resident, so return NULL and the caller will take
    // the long way.
    //

    UNLOCK_PFN (OldIrql);
    MiUnmapImageHeaderInHyperSpace ();
    return NULL;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\dynmem.c ===
/*++

Copyright (c) 1999  Microsoft Corporation

Module Name:

    dynmem.c

Abstract:

    This module contains the routines which implement dynamically adding
    and removing physical memory from the system.

Author:

    Landy Wang (landyw) 05-Feb-1999

Revision History:

--*/

#include "mi.h"

FAST_MUTEX MmDynamicMemoryMutex;

LOGICAL MiTrimRemovalPagesOnly = FALSE;

#if DBG
ULONG MiShowStuckPages;
ULONG MiDynmemData[9];
#endif

#if defined (_MI_COMPRESSION)
extern PMM_SET_COMPRESSION_THRESHOLD MiSetCompressionThreshold;
#endif

//
// Leave the low 3 bits clear as this will be inserted into the PFN PteAddress.
//

#define PFN_REMOVED     ((PMMPTE)(INT_PTR)(int)0x99887768)

PFN_COUNT
MiRemovePhysicalPages (
    IN PFN_NUMBER StartPage,
    IN PFN_NUMBER EndPage
    );

NTSTATUS
MiRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN LOGICAL PermanentRemoval,
    IN ULONG Flags
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmRemovePhysicalMemory)
#pragma alloc_text(PAGE,MmMarkPhysicalMemoryAsBad)
#pragma alloc_text(PAGELK,MmAddPhysicalMemory)
#pragma alloc_text(PAGELK,MmAddPhysicalMemoryEx)
#pragma alloc_text(PAGELK,MiRemovePhysicalMemory)
#pragma alloc_text(PAGELK,MmMarkPhysicalMemoryAsGood)
#pragma alloc_text(PAGELK,MmGetPhysicalMemoryRanges)
#pragma alloc_text(PAGELK,MiRemovePhysicalPages)
#endif


NTSTATUS
MmAddPhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    A wrapper for MmAddPhysicalMemoryEx.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being added.
                     If any bytes were added (ie: STATUS_SUCCESS is being
                     returned), the actual amount is returned here.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    return MmAddPhysicalMemoryEx (StartAddress, NumberOfBytes, 0);
}


NTSTATUS
MmAddPhysicalMemoryEx (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine adds the specified physical address range to the system.
    This includes initializing PFN database entries and adding it to the
    freelists.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being added.
                     If any bytes were added (ie: STATUS_SUCCESS is being
                     returned), the actual amount is returned here.

    Flags  - Supplies relevant flags describing the memory range.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    LOGICAL Inserted;
    LOGICAL Updated;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    PFN_NUMBER TotalPagesAllowed;
    PFN_COUNT PagesNeeded;
    PPHYSICAL_MEMORY_DESCRIPTOR OldPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_DESCRIPTOR NewPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_RUN NewRun;
    LOGICAL PfnDatabaseIsPhysical;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

#ifdef _MI_MESSAGE_SERVER
    if (NumberOfBytes->LowPart & 0x1) {
        MI_INSTRUMENT_QUEUE((PVOID)StartAddress);
        return STATUS_NOT_SUPPORTED;
    }
    else if (NumberOfBytes->LowPart & 0x2) {
#if defined(_WIN64)
        StartAddress->QuadPart = (ULONG_PTR)(MI_INSTRUMENTR_QUEUE());
#else
        StartAddress->LowPart = PtrToUlong(MI_INSTRUMENTR_QUEUE());
#endif
        return STATUS_NOT_SUPPORTED;
    }
#endif

    if (BYTE_OFFSET(StartAddress->LowPart) != 0) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (BYTE_OFFSET(NumberOfBytes->LowPart) != 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

#if defined (_MI_COMPRESSION)
    if (Flags & ~MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        return STATUS_INVALID_PARAMETER_3;
    }
#else
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_3;
    }
#endif

    //
    // The system must be configured for dynamic memory addition.  This is
    // critical as only then is the database guaranteed to be non-sparse.
    //
    
    if (MmDynamicPfn == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    if (MI_IS_PHYSICAL_ADDRESS(MmPfnDatabase)) {
        PfnDatabaseIsPhysical = TRUE;
    }
    else {
        PfnDatabaseIsPhysical = FALSE;
    }

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_NUMBER)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    if (EndPage - 1 > MmHighestPossiblePhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPossiblePhysicalPage + 1;
        NumberOfPages = EndPage - StartPage;
    }

    //
    // Ensure that the memory being added does not exceed the license
    // restrictions.
    //

    if (ExVerifySuite(DataCenter) == TRUE) {
        TotalPagesAllowed = MI_DTC_MAX_PAGES;
    }
    else if ((MmProductType != 0x00690057) &&
             (ExVerifySuite(Enterprise) == TRUE)) {

        TotalPagesAllowed = MI_ADS_MAX_PAGES;
    }
    else {
        TotalPagesAllowed = MI_DEFAULT_MAX_PAGES;
    }

    if (MmNumberOfPhysicalPages + NumberOfPages > TotalPagesAllowed) {

        //
        // Truncate the request appropriately.
        //

        NumberOfPages = TotalPagesAllowed - MmNumberOfPhysicalPages;
        EndPage = StartPage + NumberOfPages;
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;

    i = (sizeof(PHYSICAL_MEMORY_DESCRIPTOR) +
         (sizeof(PHYSICAL_MEMORY_RUN) * (MmPhysicalMemoryBlock->NumberOfRuns + 1)));

    NewPhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                    i,
                                                    '  mM');

    if (NewPhysicalMemoryBlock == NULL) {
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // The range cannot overlap any ranges that are already present.
    //

    start = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

#if defined (_MI_COMPRESSION)

    //
    // Adding compression-generated ranges can only be done if the hardware
    // has already successfully announced itself.
    //

    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        if (MiSetCompressionThreshold == NULL) {
            UNLOCK_PFN (OldIrql);
            MmUnlockPagableImageSection(ExPageLockHandle);
            ExReleaseFastMutex (&MmDynamicMemoryMutex);
            ExFreePool (NewPhysicalMemoryBlock);
            return STATUS_NOT_SUPPORTED;
        }
    }
#endif

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {

            LastPage = Page + count;

            if ((StartPage < Page) && (EndPage > Page)) {
                UNLOCK_PFN (OldIrql);
                MmUnlockPagableImageSection(ExPageLockHandle);
                ExReleaseFastMutex (&MmDynamicMemoryMutex);
                ExFreePool (NewPhysicalMemoryBlock);
                return STATUS_CONFLICTING_ADDRESSES;
            }

            if ((StartPage >= Page) && (StartPage < LastPage)) {
                UNLOCK_PFN (OldIrql);
                MmUnlockPagableImageSection(ExPageLockHandle);
                ExReleaseFastMutex (&MmDynamicMemoryMutex);
                ExFreePool (NewPhysicalMemoryBlock);
                return STATUS_CONFLICTING_ADDRESSES;
            }
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // Fill any gaps in the (sparse) PFN database needed for these pages,
    // unless the PFN database was physically allocated and completely
    // committed up front.
    //

    PagesNeeded = 0;

    if (PfnDatabaseIsPhysical == FALSE) {
        PointerPte = MiGetPteAddress (MI_PFN_ELEMENT(StartPage));
        LastPte = MiGetPteAddress ((PCHAR)(MI_PFN_ELEMENT(EndPage)) - 1);
    
        while (PointerPte <= LastPte) {
            if (PointerPte->u.Hard.Valid == 0) {
                PagesNeeded += 1;
            }
            PointerPte += 1;
        }
    
        if (MmAvailablePages < PagesNeeded) {
            UNLOCK_PFN (OldIrql);
            MmUnlockPagableImageSection(ExPageLockHandle);
            ExReleaseFastMutex (&MmDynamicMemoryMutex);
            ExFreePool (NewPhysicalMemoryBlock);
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    
        TempPte = ValidKernelPte;
    
        PointerPte = MiGetPteAddress (MI_PFN_ELEMENT(StartPage));
    
        while (PointerPte <= LastPte) {
            if (PointerPte->u.Hard.Valid == 0) {
    
                PageFrameIndex = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
    
                MiInitializePfn (PageFrameIndex, PointerPte, 0);
    
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                *PointerPte = TempPte;
            }
            PointerPte += 1;
        }
        MmResidentAvailablePages -= PagesNeeded;
    }

    //
    // If the new range is adjacent to an existing range, just merge it into
    // the old block.  Otherwise use the new block as a new entry will have to
    // be used.
    //

    NewPhysicalMemoryBlock->NumberOfRuns = MmPhysicalMemoryBlock->NumberOfRuns + 1;
    NewPhysicalMemoryBlock->NumberOfPages = MmPhysicalMemoryBlock->NumberOfPages + NumberOfPages;

    NewRun = &NewPhysicalMemoryBlock->Run[0];
    start = 0;
    Inserted = FALSE;
    Updated = FALSE;

    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        count = MmPhysicalMemoryBlock->Run[start].PageCount;

        if (Inserted == FALSE) {

            //
            // Note overlaps into adjacent ranges were already checked above.
            //

            if (StartPage == Page + count) {
                MmPhysicalMemoryBlock->Run[start].PageCount += NumberOfPages;
                OldPhysicalMemoryBlock = NewPhysicalMemoryBlock;
                MmPhysicalMemoryBlock->NumberOfPages += NumberOfPages;

                //
                // Coalesce below and above to avoid leaving zero length gaps
                // as these gaps would prevent callers from removing ranges
                // the span them.
                //

                if (start + 1 < MmPhysicalMemoryBlock->NumberOfRuns) {

                    start += 1;
                    Page = MmPhysicalMemoryBlock->Run[start].BasePage;
                    count = MmPhysicalMemoryBlock->Run[start].PageCount;

                    if (StartPage + NumberOfPages == Page) {
                        MmPhysicalMemoryBlock->Run[start - 1].PageCount +=
                            count;
                        MmPhysicalMemoryBlock->NumberOfRuns -= 1;

                        //
                        // Copy any remaining entries.
                        //
    
                        if (start != MmPhysicalMemoryBlock->NumberOfRuns) {
                            RtlMoveMemory (&MmPhysicalMemoryBlock->Run[start],
                                           &MmPhysicalMemoryBlock->Run[start + 1],
                                           (MmPhysicalMemoryBlock->NumberOfRuns - start) * sizeof (PHYSICAL_MEMORY_RUN));
                        }
                    }
                }
                Updated = TRUE;
                break;
            }

            if (StartPage + NumberOfPages == Page) {
                MmPhysicalMemoryBlock->Run[start].BasePage = StartPage;
                MmPhysicalMemoryBlock->Run[start].PageCount += NumberOfPages;
                OldPhysicalMemoryBlock = NewPhysicalMemoryBlock;
                MmPhysicalMemoryBlock->NumberOfPages += NumberOfPages;
                Updated = TRUE;
                break;
            }

            if (StartPage + NumberOfPages <= Page) {

                if (start + 1 < MmPhysicalMemoryBlock->NumberOfRuns) {

                    if (StartPage + NumberOfPages <= MmPhysicalMemoryBlock->Run[start + 1].BasePage) {
                        //
                        // Don't insert here - the new entry really belongs
                        // (at least) one entry further down.
                        //

                        continue;
                    }
                }

                NewRun->BasePage = StartPage;
                NewRun->PageCount = NumberOfPages;
                NewRun += 1;
                Inserted = TRUE;
                Updated = TRUE;
            }
        }

        *NewRun = MmPhysicalMemoryBlock->Run[start];
        NewRun += 1;

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // If the memory block has not been updated, then the new entry must
    // be added at the very end.
    //

    if (Updated == FALSE) {
        ASSERT (Inserted == FALSE);
        NewRun->BasePage = StartPage;
        NewRun->PageCount = NumberOfPages;
        Inserted = TRUE;
    }

    //
    // Repoint the MmPhysicalMemoryBlock at the new chunk, free the old after
    // releasing the PFN lock.
    //

    if (Inserted == TRUE) {
        OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        MmPhysicalMemoryBlock = NewPhysicalMemoryBlock;
    }

    //
    // Note that the page directory (page parent entries on Win64) must be
    // filled in at system boot so that already-created processes do not fault
    // when referencing the new PFNs.
    //

    //
    // Walk through the memory descriptors and add pages to the
    // free list in the PFN database.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (EndPage - 1 > MmHighestPhysicalPage) {
        MmHighestPhysicalPage = EndPage - 1;
    }

    while (PageFrameIndex < EndPage) {

        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ShortFlags == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT64 (Pfn1->UsedPageTableEntries == 0);
        ASSERT (Pfn1->OriginalPte.u.Long == ZeroKernelPte.u.Long);
        ASSERT (Pfn1->u4.PteFrame == 0);
        ASSERT ((Pfn1->PteAddress == PFN_REMOVED) ||
                (Pfn1->PteAddress == (PMMPTE)(UINT_PTR)0));

        //
        // Initialize the color for NUMA purposes.
        //

        MiDetermineNode (PageFrameIndex, Pfn1);

        //
        // Set the PTE address to the physical page for
        // virtual address alignment checking.
        //

        Pfn1->PteAddress = (PMMPTE)(PageFrameIndex << PTE_SHIFT);

        MiInsertPageInFreeList (PageFrameIndex);

        PageFrameIndex += 1;
        
        Pfn1 += 1;
    }

    MmNumberOfPhysicalPages += (PFN_COUNT)NumberOfPages;

    //
    // Only non-compression ranges get to contribute to ResidentAvailable as
    // adding compression ranges to this could crash the system.
    //
    // For the same reason, compression range additions also need to subtract
    // from AvailablePages the amount the above MiInsertPageInFreeList added.
    //

#if defined (_MI_COMPRESSION)
    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        MmAvailablePages -= (PFN_COUNT) NumberOfPages;
        MiNumberOfCompressionPages += NumberOfPages;
    }
    else {
        MmResidentAvailablePages += NumberOfPages;

        //
        // Since real (noncompression-generated) physical memory was added,
        // rearm the interrupt to occur at a higher threshold.
        //

        MiArmCompressionInterrupt ();
    }
#else
    MmResidentAvailablePages += NumberOfPages;
#endif

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                            (LONG) NumberOfPages);

    //
    // Carefully increase all commit limits to reflect the additional memory -
    // notice the current usage must be done first so no one else cuts the
    // line.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommittedPages, PagesNeeded);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    ExFreePool (OldPhysicalMemoryBlock);

    //
    // Indicate number of bytes actually added to our caller.
    //

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;
}


NTSTATUS
MiRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN LOGICAL PermanentRemoval,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine attempts to remove the specified physical address range
    from the system.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

    PermanentRemoval  - Supplies TRUE if the memory is being permanently
                        (ie: physically) removed.  FALSE if not (ie: just a
                        bad page detected via ECC which is being marked
                        "don't-use".

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    ULONG Additional;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    PFN_NUMBER OriginalLastPage;
    PFN_NUMBER start;
    PFN_NUMBER PagesReleased;
    PMMPFN Pfn1;
    PMMPFN StartPfn;
    PMMPFN EndPfn;
    KIRQL OldIrql;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_COUNT NumberOfPages;
    PFN_COUNT ParityPages;
    SPFN_NUMBER MaxPages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER RemovedPages;
    PFN_NUMBER RemovedPagesThisPass;
    LOGICAL Inserted;
    NTSTATUS Status;
    PMMPTE PointerPte;
    PMMPTE EndPte;
    PVOID VirtualAddress;
    PPHYSICAL_MEMORY_DESCRIPTOR OldPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_DESCRIPTOR NewPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_RUN NewRun;
    LOGICAL PfnDatabaseIsPhysical;
    PFN_NUMBER HighestPossiblePhysicalPage;
    PFN_COUNT FluidPages;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (BYTE_OFFSET(NumberOfBytes->LowPart) == 0);
    ASSERT (BYTE_OFFSET(StartAddress->LowPart) == 0);

    if (MI_IS_PHYSICAL_ADDRESS(MmPfnDatabase)) {

        if (PermanentRemoval == TRUE) {

            //
            // The system must be configured for dynamic memory addition.  This
            // is not strictly required to remove the memory, but it's better
            // to check for it now under the assumption that the administrator
            // is probably going to want to add this range of memory back in -
            // better to give the error now and refuse the removal than to
            // refuse the addition later.
            //
        
            if (MmDynamicPfn == 0) {
                return STATUS_NOT_SUPPORTED;
            }
        }
    
        PfnDatabaseIsPhysical = TRUE;
    }
    else {
        PfnDatabaseIsPhysical = FALSE;
    }

    if (PermanentRemoval == TRUE) {
        HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;
        FluidPages = 100;
    }
    else {
        HighestPossiblePhysicalPage = MmHighestPhysicalPage;
        FluidPages = 0;
    }

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_COUNT)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    if (EndPage - 1 > HighestPossiblePhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPossiblePhysicalPage + 1;
        NumberOfPages = (PFN_COUNT)(EndPage - StartPage);
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

#if !defined (_MI_COMPRESSION)
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_4;
    }
#endif

    StartPfn = MI_PFN_ELEMENT (StartPage);
    EndPfn = MI_PFN_ELEMENT (EndPage);

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

#if DBG
    MiDynmemData[0] += 1;
#endif

    //
    // Attempt to decrease all commit limits to reflect the removed memory.
    //

    if (MiChargeTemporaryCommitmentForReduction (NumberOfPages + FluidPages) == FALSE) {
#if DBG
        MiDynmemData[1] += 1;
#endif
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Reduce the systemwide commit limit - note this is carefully done
    // *PRIOR* to returning this commitment so no one else (including a DPC
    // in this very thread) can consume past the limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, 0 - NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, 0 - NumberOfPages);

    //
    // Now that the systemwide commit limit has been lowered, the amount
    // we have removed can be safely returned.
    //

    MiReturnCommitment (NumberOfPages + FluidPages);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Check for outstanding promises that cannot be broken.
    //

    LOCK_PFN (OldIrql);

    if (PermanentRemoval == FALSE) {

        //
        // If it's just the removal of ECC-marked bad pages, then don't
        // allow the caller to remove any pages that have already been
        // ECC-removed.  This is to prevent recursive erroneous charges.
        //

        for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
            if (Pfn1->u3.e1.ParityError == 1) {
                UNLOCK_PFN (OldIrql);
                Status = STATUS_INVALID_PARAMETER_2;
                goto giveup2;
            }
        }
    }

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - FluidPages;

    if ((SPFN_NUMBER)NumberOfPages > MaxPages) {
#if DBG
        MiDynmemData[2] += 1;
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto giveup2;
    }

    //
    // The range must be contained in a single entry.  It is
    // permissible for it to be part of a single entry, but it
    // must not cross multiple entries.
    //

    Additional = (ULONG)-2;

    start = 0;
    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        LastPage = Page + MmPhysicalMemoryBlock->Run[start].PageCount;

        if ((StartPage >= Page) && (EndPage <= LastPage)) {
            if ((StartPage == Page) && (EndPage == LastPage)) {
                Additional = (ULONG)-1;
            }
            else if ((StartPage == Page) || (EndPage == LastPage)) {
                Additional = 0;
            }
            else {
                Additional = 1;
            }
            break;
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (Additional == (ULONG)-2) {
#if DBG
        MiDynmemData[3] += 1;
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto giveup2;
    }

    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
        Pfn1->u3.e1.RemovalRequested = 1;
    }

    if (PermanentRemoval == TRUE) {
        MmNumberOfPhysicalPages -= NumberOfPages;

        InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                                0 - NumberOfPages);
    }

#if defined (_MI_COMPRESSION)

    //
    // Only removal of non-compression ranges decrement ResidentAvailable as
    // only those ranges actually incremented this when they were added.
    //

    if ((Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) == 0) {
        MmResidentAvailablePages -= NumberOfPages;

        //
        // Since real (noncompression-generated) physical memory is being
        // removed, rearm the interrupt to occur at a lower threshold.
        //

        if (PermanentRemoval == TRUE) {
            MiArmCompressionInterrupt ();
        }
    }
#else
    MmResidentAvailablePages -= NumberOfPages;
#endif

    //
    // The free and zero lists must be pruned now before releasing the PFN
    // lock otherwise if another thread allocates the page from these lists,
    // the allocation will clear the RemovalRequested flag forever.
    //

    RemovedPages = MiRemovePhysicalPages (StartPage, EndPage);

#if defined (_MI_COMPRESSION)

    //
    // Compression range removals add back into AvailablePages the same
    // amount that MiUnlinkPageFromList removes (as the original addition
    // of these ranges never bumps this counter).
    //

    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        MmAvailablePages += (PFN_COUNT) RemovedPages;
        MiNumberOfCompressionPages -= RemovedPages;
    }
#endif

    if (RemovedPages != NumberOfPages) {

#if DBG
retry:
#endif
    
        Pfn1 = StartPfn;
    
        InterlockedIncrement (&MiDelayPageFaults);
    
        for (i = 0; i < 5; i += 1) {
    
            UNLOCK_PFN (OldIrql);
    
            //
            // Attempt to move pages to the standby list.  Note that only the
            // pages with RemovalRequested set are moved.
            //
    
            MiTrimRemovalPagesOnly = TRUE;
    
            MmEmptyAllWorkingSets ();
    
            MiTrimRemovalPagesOnly = FALSE;
    
            MiFlushAllPages ();
    
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    
            LOCK_PFN (OldIrql);
    
            RemovedPagesThisPass = MiRemovePhysicalPages (StartPage, EndPage);

            RemovedPages += RemovedPagesThisPass;
    
#if defined (_MI_COMPRESSION)

            //
            // Compression range removals add back into AvailablePages the same
            // amount that MiUnlinkPageFromList removes (as the original
            // addition of these ranges never bumps this counter).
            //

            if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
                MmAvailablePages += (PFN_COUNT) RemovedPagesThisPass;
                MiNumberOfCompressionPages -= RemovedPagesThisPass;
            }

#endif

            if (RemovedPages == NumberOfPages) {
                break;
            }
    
            //
            // RemovedPages doesn't include pages that were freed directly
            // to the bad page list via MiDecrementReferenceCount or by
            // ECC marking.  So use the above check purely as an optimization -
            // and walk here before ever giving up.
            //

            for ( ; Pfn1 < EndPfn; Pfn1 += 1) {
                if (Pfn1->u3.e1.PageLocation != BadPageList) {
                    break;
                }
            }

            if (Pfn1 == EndPfn) {
                RemovedPages = NumberOfPages;
                break;
            }
        }

        InterlockedDecrement (&MiDelayPageFaults);
    }

    if (RemovedPages != NumberOfPages) {
#if DBG
        MiDynmemData[4] += 1;
        if (MiShowStuckPages != 0) {

            RemovedPages = 0;
            for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
                if (Pfn1->u3.e1.PageLocation != BadPageList) {
                    RemovedPages += 1;
                }
            }

            ASSERT (RemovedPages != 0);

            DbgPrint("MiRemovePhysicalMemory : could not get %d of %d pages\n",
                RemovedPages, NumberOfPages);

            if (MiShowStuckPages & 0x2) {

                ULONG PfnsPrinted;
                ULONG EnoughShown;
                PMMPFN FirstPfn;
                PFN_COUNT PfnCount;

                PfnCount = 0;
                PfnsPrinted = 0;
                EnoughShown = 100;
    
                //
                // Initializing FirstPfn is not needed for correctness
                // but without it the compiler cannot compile this code
                // W4 to check for use of uninitialized variables.
                //

                FirstPfn = NULL;

                if (MiShowStuckPages & 0x4) {
                    EnoughShown = (ULONG)-1;
                }
    
                DbgPrint("Stuck PFN list: ");
                for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
                    if (Pfn1->u3.e1.PageLocation != BadPageList) {
                        if (PfnCount == 0) {
                            FirstPfn = Pfn1;
                        }
                        PfnCount += 1;
                    }
                    else {
                        if (PfnCount != 0) {
                            DbgPrint("%x -> %x ; ", FirstPfn - MmPfnDatabase,
                                                    (FirstPfn - MmPfnDatabase) + PfnCount - 1);
                            PfnsPrinted += 1;
                            if (PfnsPrinted == EnoughShown) {
                                break;
                            }
                            PfnCount = 0;
                        }
                    }
                }
                if (PfnCount != 0) {
                    DbgPrint("%x -> %x ; ", FirstPfn - MmPfnDatabase,
                                            (FirstPfn - MmPfnDatabase) + PfnCount - 1);
                }
                DbgPrint("\n");
            }
            if (MiShowStuckPages & 0x8) {
                DbgBreakPoint ();
            }
            if (MiShowStuckPages & 0x10) {
                goto retry;
            }
        }
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_NO_MEMORY;
        goto giveup;
    }

#if DBG
    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
        ASSERT (Pfn1->u3.e1.PageLocation == BadPageList);
    }
#endif

    //
    // All the pages in the range have been removed.
    //

    if (PermanentRemoval == FALSE) {

        //
        // If it's just the removal of ECC-marked bad pages, then no
        // adjustment to the physical memory block ranges or PFN database
        // trimming is needed.  Exit now.
        //

        for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
            ASSERT (Pfn1->u3.e1.ParityError == 0);
            Pfn1->u3.e1.ParityError = 1;
        }

        UNLOCK_PFN (OldIrql);

        MmUnlockPagableImageSection(ExPageLockHandle);
    
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
    
        NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;
    
        return STATUS_SUCCESS;
    }

    //
    // Update the physical memory blocks and other associated housekeeping.
    //

    if (Additional == 0) {

        //
        // The range can be split off from an end of an existing chunk so no
        // pool growth or shrinkage is required.
        //

        NewPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        OldPhysicalMemoryBlock = NULL;
    }
    else {

        //
        // The range cannot be split off from an end of an existing chunk so
        // pool growth or shrinkage is required.
        //

        UNLOCK_PFN (OldIrql);

        i = (sizeof(PHYSICAL_MEMORY_DESCRIPTOR) +
             (sizeof(PHYSICAL_MEMORY_RUN) * (MmPhysicalMemoryBlock->NumberOfRuns + Additional)));

        NewPhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                        i,
                                                        '  mM');

        if (NewPhysicalMemoryBlock == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
#if DBG
            MiDynmemData[5] += 1;
#endif
            goto giveup;
        }

        OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        RtlZeroMemory (NewPhysicalMemoryBlock, i);

        LOCK_PFN (OldIrql);
    }

    //
    // Remove or split the requested range from the existing memory block.
    //

    NewPhysicalMemoryBlock->NumberOfRuns = MmPhysicalMemoryBlock->NumberOfRuns + Additional;
    NewPhysicalMemoryBlock->NumberOfPages = MmPhysicalMemoryBlock->NumberOfPages - NumberOfPages;

    NewRun = &NewPhysicalMemoryBlock->Run[0];
    start = 0;
    Inserted = FALSE;

    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        LastPage = Page + MmPhysicalMemoryBlock->Run[start].PageCount;

        if (Inserted == FALSE) {

            if ((StartPage >= Page) && (EndPage <= LastPage)) {

                if ((StartPage == Page) && (EndPage == LastPage)) {
                    ASSERT (Additional == -1);
                    start += 1;
                    continue;
                }
                else if ((StartPage == Page) || (EndPage == LastPage)) {
                    ASSERT (Additional == 0);
                    if (StartPage == Page) {
                        MmPhysicalMemoryBlock->Run[start].BasePage += NumberOfPages;
                    }
                    MmPhysicalMemoryBlock->Run[start].PageCount -= NumberOfPages;
                }
                else {
                    ASSERT (Additional == 1);

                    OriginalLastPage = LastPage;

                    MmPhysicalMemoryBlock->Run[start].PageCount =
                        StartPage - MmPhysicalMemoryBlock->Run[start].BasePage;

                    *NewRun = MmPhysicalMemoryBlock->Run[start];
                    NewRun += 1;

                    NewRun->BasePage = EndPage;
                    NewRun->PageCount = OriginalLastPage - EndPage;
                    NewRun += 1;

                    start += 1;
                    continue;
                }

                Inserted = TRUE;
            }
        }

        *NewRun = MmPhysicalMemoryBlock->Run[start];
        NewRun += 1;
        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // Repoint the MmPhysicalMemoryBlock at the new chunk.
    // Free the old block after releasing the PFN lock.
    //

    MmPhysicalMemoryBlock = NewPhysicalMemoryBlock;

    if (EndPage - 1 == MmHighestPhysicalPage) {
        MmHighestPhysicalPage = StartPage - 1;
    }

    //
    // Throw away all the removed pages that are currently enqueued.
    //

    ParityPages = 0;
    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {

        ASSERT (Pfn1->u3.e1.PageLocation == BadPageList);
        ASSERT (Pfn1->u3.e1.RemovalRequested == 1);

        //
        // Some pages may have already been ECC-removed.  For these pages,
        // the commit limits and resident available pages have already been
        // adjusted - tally them here so we can undo the extraneous charge
        // just applied.
        //
    
        if (Pfn1->u3.e1.ParityError == 1) {
            ParityPages += 1;
        }

        MiUnlinkPageFromList (Pfn1);

        ASSERT (Pfn1->u1.Flink == 0);
        ASSERT (Pfn1->u2.Blink == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT64 (Pfn1->UsedPageTableEntries == 0);

        Pfn1->PteAddress = PFN_REMOVED;

        //
        // Note this clears ParityError among other flags...
        //

        Pfn1->u3.e2.ShortFlags = 0;
        Pfn1->OriginalPte.u.Long = ZeroKernelPte.u.Long;
        Pfn1->u4.PteFrame = 0;
    }

    //
    // Now that the removed pages have been discarded, eliminate the PFN
    // entries that mapped them.  Straddling entries left over from an
    // adjacent earlier removal are not collapsed at this point.
    //
    //

    PagesReleased = 0;

    if (PfnDatabaseIsPhysical == FALSE) {

        VirtualAddress = (PVOID)ROUND_TO_PAGES(MI_PFN_ELEMENT(StartPage));
        PointerPte = MiGetPteAddress (VirtualAddress);
        EndPte = MiGetPteAddress (PAGE_ALIGN(MI_PFN_ELEMENT(EndPage)));

        while (PointerPte < EndPte) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
            Pfn1->u2.ShareCount = 0;
            MI_SET_PFN_DELETED (Pfn1);
#if DBG
            Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif //DBG
            MiDecrementReferenceCount (PageFrameIndex);
    
            KeFlushSingleTb (VirtualAddress,
                             TRUE,
                             TRUE,
                             (PHARDWARE_PTE)PointerPte,
                             ZeroKernelPte.u.Flush);
    
            PagesReleased += 1;
            PointerPte += 1;
            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
        }

        MmResidentAvailablePages += PagesReleased;
    }

#if DBG
    MiDynmemData[6] += 1;
#endif

    //
    // Give back anything that has been double-charged.
    //

    if (ParityPages != 0) {
        MmResidentAvailablePages += ParityPages;
    }

    UNLOCK_PFN (OldIrql);

    //
    // Give back anything that has been double-charged.
    //

    if (ParityPages != 0) {
        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, ParityPages);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, ParityPages);
    }

    if (PagesReleased != 0) {
        MiReturnCommitment (PagesReleased);
    }

    MmUnlockPagableImageSection(ExPageLockHandle);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    if (OldPhysicalMemoryBlock != NULL) {
        ExFreePool (OldPhysicalMemoryBlock);
    }

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;

giveup:

    //
    // All the pages in the range were not obtained.  Back everything out.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    LOCK_PFN (OldIrql);

    while (PageFrameIndex < EndPage) {

        ASSERT (Pfn1->u3.e1.RemovalRequested == 1);

        Pfn1->u3.e1.RemovalRequested = 0;

        if (Pfn1->u3.e1.PageLocation == BadPageList) {
            MiUnlinkPageFromList (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
        }

        Pfn1 += 1;
        PageFrameIndex += 1;
    }

#if defined (_MI_COMPRESSION)

    //
    // Only removal of non-compression ranges decrement ResidentAvailable as
    // only those ranges actually incremented this when they were added.
    //

    if ((Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) == 0) {
        MmResidentAvailablePages += NumberOfPages;
    }
    else {

        //
        // Compression range removals add back into AvailablePages the same
        // amount that MiUnlinkPageFromList removes (as the original
        // addition of these ranges never bumps this counter).
        //

        MmAvailablePages -= (PFN_COUNT) RemovedPages;
        MiNumberOfCompressionPages += RemovedPages;
    }
#else
    MmResidentAvailablePages += NumberOfPages;
#endif

    if (PermanentRemoval == TRUE) {
        MmNumberOfPhysicalPages += NumberOfPages;

        InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                                NumberOfPages);

#if defined (_MI_COMPRESSION)

        //
        // Rearm the interrupt to occur at the original threshold.
        //

        if ((Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) == 0) {
            MiArmCompressionInterrupt ();
        }
#endif
    }

    UNLOCK_PFN (OldIrql);

giveup2:

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);
    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);
    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    return Status;
}


NTSTATUS
MmRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    A wrapper for MmRemovePhysicalMemoryEx.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    return MmRemovePhysicalMemoryEx (StartAddress, NumberOfBytes, 0);
}

NTSTATUS
MmRemovePhysicalMemoryEx (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine attempts to remove the specified physical address range
    from the system.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

    Flags  - Supplies relevant flags describing the memory range.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    NTSTATUS Status;
#if defined (_X86_)
    BOOLEAN CachesFlushed;
#endif
#if defined(_IA64_)
    PVOID VirtualAddress;
    PVOID SingleVirtualAddress;
    SIZE_T SizeInBytes;
    SIZE_T MapSizeInBytes;
    PFN_COUNT NumberOfPages;
    PFN_COUNT i;
    PFN_NUMBER StartPage;
#endif

    PAGED_CODE();

#if defined (_MI_COMPRESSION_SUPPORTED_)
    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        return STATUS_NOT_SUPPORTED;
    }
#else
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_3;
    }
#endif

#if defined (_X86_)

    //
    // Issue a cache invalidation here just as a test to make sure the
    // machine can support it.  If not, then don't bother trying to remove
    // any memory.
    //

    CachesFlushed = KeInvalidateAllCaches (TRUE);
    if (CachesFlushed == FALSE) {
        return STATUS_NOT_SUPPORTED;
    }
#endif

#if defined(_IA64_)

    //
    // Pick up at least a single PTE mapping now as we do not want to fail this
    // call if no PTEs are available after a successful remove.  Resorting to
    // actually using this PTE should be a very rare case indeed.
    //

    SingleVirtualAddress = (PMMPTE)MiMapSinglePage (NULL,
                                                    0,
                                                    MmCached,
                                                    HighPagePriority);

    if (SingleVirtualAddress == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

#endif

    Status = MiRemovePhysicalMemory (StartAddress, NumberOfBytes, TRUE, Flags);

    if (NT_SUCCESS (Status)) {

#if defined (_X86_)
        CachesFlushed = KeInvalidateAllCaches (TRUE);
        ASSERT (CachesFlushed == TRUE);
#endif

#if defined(_IA64_)
        SizeInBytes = (SIZE_T)NumberOfBytes->QuadPart;

        //
        // Flush the entire TB to remove any KSEG translations that may map the
        // pages being removed.  Otherwise hardware or software speculation
        // can reference the memory speculatively which would crash the machine.
        //

        KeFlushEntireTb (TRUE, TRUE);

        //
        // Establish an uncached mapping to the pages being removed.
        //

        MapSizeInBytes = SizeInBytes;

        //
        // Initializing VirtualAddress is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        VirtualAddress = NULL;

        while (MapSizeInBytes > PAGE_SIZE) {

            VirtualAddress = MmMapIoSpace (*StartAddress,
                                           MapSizeInBytes,
                                           MmNonCached);

            if (VirtualAddress != NULL) {
                break;
            }

            MapSizeInBytes = MapSizeInBytes >> 1;
        }

        if (MapSizeInBytes <= PAGE_SIZE) {

            StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);

            NumberOfPages = (PFN_COUNT)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

            for (i = 0; i < NumberOfPages; i += 1) {

                SingleVirtualAddress = (PMMPTE)MiMapSinglePage (SingleVirtualAddress,
                                                                StartPage,
                                                                MmCached,
                                                                HighPagePriority);

                KeSweepCacheRangeWithDrain (TRUE,
                                            SingleVirtualAddress,
                                            PAGE_SIZE);

                StartPage += 1;
            }
        }
        else {

            //
            // Drain all pending transactions and prefetches and perform cache
            // evictions.  Only drain 4gb max at a time as this API takes a
            // ULONG.
            //

            while (SizeInBytes > _4gb) {
                KeSweepCacheRangeWithDrain (TRUE, VirtualAddress, _4gb - 1);
                SizeInBytes -= (_4gb - 1);
            }

            KeSweepCacheRangeWithDrain (TRUE,
                                        VirtualAddress,
                                        (ULONG)SizeInBytes);

            MmUnmapIoSpace (VirtualAddress, NumberOfBytes->QuadPart);
        }
#endif
    }

#if defined(_IA64_)
    MiUnmapSinglePage (SingleVirtualAddress);
#endif

    return Status;
}

NTSTATUS
MmMarkPhysicalMemoryAsBad (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    This routine attempts to mark the specified physical address range
    as bad so the system will not use it.  This is generally done for pages
    which contain ECC errors.

    Note that this is different from removing pages permanently (ie: physically
    removing the memory board) which should be done via the
    MmRemovePhysicalMemory API.

    The caller is responsible for maintaining a global table so that subsequent
    boots can examine it and remove the ECC pages before loading the kernel.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    PAGED_CODE();

    return MiRemovePhysicalMemory (StartAddress, NumberOfBytes, FALSE, 0);
}

NTSTATUS
MmMarkPhysicalMemoryAsGood (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    This routine attempts to mark the specified physical address range
    as good so the system will use it.  This is generally done for pages
    which used to (but presumably no longer do) contain ECC errors.

    Note that this is different from adding pages permanently (ie: physically
    inserting a new memory board) which should be done via the
    MmAddPhysicalMemory API.

    The caller is responsible for removing these entries from a global table
    so that subsequent boots will use the pages.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    LOGICAL PfnDatabaseIsPhysical;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (BYTE_OFFSET(NumberOfBytes->LowPart) == 0);
    ASSERT (BYTE_OFFSET(StartAddress->LowPart) == 0);

    if (MI_IS_PHYSICAL_ADDRESS(MmPfnDatabase)) {
        PfnDatabaseIsPhysical = TRUE;
    }
    else {
        PfnDatabaseIsPhysical = FALSE;
    }

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_NUMBER)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    if (EndPage - 1 > MmHighestPhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPhysicalPage + 1;
        NumberOfPages = EndPage - StartPage;
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // The request must lie within an already present range.
    //

    start = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {

            LastPage = Page + count;

            if ((StartPage >= Page) && (EndPage <= LastPage)) {
                break;
            }
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (start == MmPhysicalMemoryBlock->NumberOfRuns) {
        UNLOCK_PFN (OldIrql);
        MmUnlockPagableImageSection(ExPageLockHandle);
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return STATUS_CONFLICTING_ADDRESSES;
    }

    //
    // Walk through the range and add only pages previously removed to the
    // free list in the PFN database.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    NumberOfPages = 0;

    while (PageFrameIndex < EndPage) {

        if ((Pfn1->u3.e1.ParityError == 1) &&
            (Pfn1->u3.e1.RemovalRequested == 1) &&
            (Pfn1->u3.e1.PageLocation == BadPageList)) {

            Pfn1->u3.e1.ParityError = 0;
            Pfn1->u3.e1.RemovalRequested = 0;
            MiUnlinkPageFromList (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
            NumberOfPages += 1;
        }

        Pfn1 += 1;
        PageFrameIndex += 1;
    }

    MmResidentAvailablePages += NumberOfPages;

    UNLOCK_PFN (OldIrql);

    //
    // Increase all commit limits to reflect the additional memory.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    //
    // Indicate number of bytes actually added to our caller.
    //

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;
}

PPHYSICAL_MEMORY_RANGE
MmGetPhysicalMemoryRanges (
    VOID
    )

/*++

Routine Description:

    This routine returns the virtual address of a nonpaged pool block which
    contains the physical memory ranges in the system.

    The returned block contains physical address and page count pairs.
    The last entry contains zero for both.

    The caller must understand that this block can change at any point before
    or after this snapshot.

    It is the caller's responsibility to free this block.

Arguments:

    None.

Return Value:

    NULL on failure.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RANGE p;
    PPHYSICAL_MEMORY_RANGE PhysicalMemoryBlock;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    i = sizeof(PHYSICAL_MEMORY_RANGE) * (MmPhysicalMemoryBlock->NumberOfRuns + 1);

    PhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                 i,
                                                 'hPmM');

    if (PhysicalMemoryBlock == NULL) {
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return NULL;
    }

    p = PhysicalMemoryBlock;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    ASSERT (i == (sizeof(PHYSICAL_MEMORY_RANGE) * (MmPhysicalMemoryBlock->NumberOfRuns + 1)));

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {
        p->BaseAddress.QuadPart = (LONGLONG)MmPhysicalMemoryBlock->Run[i].BasePage * PAGE_SIZE;
        p->NumberOfBytes.QuadPart = (LONGLONG)MmPhysicalMemoryBlock->Run[i].PageCount * PAGE_SIZE;
        p += 1;
    }

    p->BaseAddress.QuadPart = 0;
    p->NumberOfBytes.QuadPart = 0;

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection(ExPageLockHandle);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    return PhysicalMemoryBlock;
}

PFN_COUNT
MiRemovePhysicalPages (
    IN PFN_NUMBER StartPage,
    IN PFN_NUMBER EndPage
    )

/*++

Routine Description:

    This routine searches the PFN database for free, zeroed or standby pages
    that are marked for removal.

Arguments:

    StartPage - Supplies the low physical frame number to remove.

    EndPage - Supplies the last physical frame number to remove.

Return Value:

    Returns the number of pages removed from the free, zeroed and standby lists.

Environment:

    Kernel mode, PFN lock held.  Since this routine is PAGELK, the caller is
    responsible for locking it down and unlocking it on return.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPFN PfnNextColored;
    PMMPFN PfnNextFlink;
    PMMPFN PfnLastColored;
    PFN_NUMBER Page;
    LOGICAL RemovePage;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    PFN_NUMBER MovedPage;
    MMLISTS MemoryList;
    PFN_NUMBER PageNextColored;
    PFN_NUMBER PageNextFlink;
    PFN_NUMBER PageLastColored;
    PFN_COUNT NumberOfPages;
    PMMPFNLIST ListHead;
    LOGICAL RescanNeeded;

    MM_PFN_LOCK_ASSERT();

    NumberOfPages = 0;

rescan:

    //
    // Grab all zeroed (and then free) pages first directly from the
    // colored lists to avoid multiple walks down these singly linked lists.
    // Handle transition pages last.
    //

    for (MemoryList = ZeroedPageList; MemoryList <= FreePageList; MemoryList += 1) {

        ListHead = MmPageLocationList[MemoryList];

        for (Color = 0; Color < MmSecondaryColors; Color += 1) {
            ColorHead = &MmFreePagesByColor[MemoryList][Color];

            MovedPage = (PFN_NUMBER) MM_EMPTY_LIST;

            while (ColorHead->Flink != MM_EMPTY_LIST) {

                Page = ColorHead->Flink;
    
                Pfn1 = MI_PFN_ELEMENT(Page);

                ASSERT ((MMLISTS)Pfn1->u3.e1.PageLocation == MemoryList);

                // 
                // The Flink and Blink must be nonzero here for the page
                // to be on the listhead.  Only code that scans the
                // MmPhysicalMemoryBlock has to check for the zero case.
                //

                ASSERT (Pfn1->u1.Flink != 0);
                ASSERT (Pfn1->u2.Blink != 0);

                //
                // See if the page is desired by the caller.
                //
                // Systems utilizing memory compression may have more
                // pages on the zero, free and standby lists than we
                // want to give out.  Explicitly check MmAvailablePages
                // instead (and recheck whenever the PFN lock is
                // released and reacquired).
                //

                if ((Pfn1->u3.e1.RemovalRequested == 1) &&
                    (MmAvailablePages != 0)) {

                    ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                    MiUnlinkFreeOrZeroedPage (Page);
    
                    MiInsertPageInList (&MmBadPageListHead, Page);

                    NumberOfPages += 1;
                }
                else {

                    //
                    // Unwanted so put the page on the end of list.
                    // If first time, save pfn.
                    //

                    if (MovedPage == MM_EMPTY_LIST) {
                        MovedPage = Page;
                    }
                    else if (Page == MovedPage) {

                        //
                        // No more pages available in this colored chain.
                        //

                        break;
                    }

                    //
                    // If the colored chain has more than one entry then
                    // put this page on the end.
                    //

                    PageNextColored = (PFN_NUMBER)Pfn1->OriginalPte.u.Long;

                    if (PageNextColored == MM_EMPTY_LIST) {

                        //
                        // No more pages available in this colored chain.
                        //

                        break;
                    }

                    ASSERT (Pfn1->u1.Flink != 0);
                    ASSERT (Pfn1->u1.Flink != MM_EMPTY_LIST);
                    ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    PfnNextColored = MI_PFN_ELEMENT(PageNextColored);
                    ASSERT ((MMLISTS)PfnNextColored->u3.e1.PageLocation == MemoryList);
                    ASSERT (PfnNextColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    //
                    // Adjust the free page list so Page
                    // follows PageNextFlink.
                    //

                    PageNextFlink = Pfn1->u1.Flink;
                    PfnNextFlink = MI_PFN_ELEMENT(PageNextFlink);

                    ASSERT ((MMLISTS)PfnNextFlink->u3.e1.PageLocation == MemoryList);
                    ASSERT (PfnNextFlink->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    PfnLastColored = ColorHead->Blink;
                    ASSERT (PfnLastColored != (PMMPFN)MM_EMPTY_LIST);
                    ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                    ASSERT (PfnLastColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                    ASSERT (PfnLastColored->u2.Blink != MM_EMPTY_LIST);

                    ASSERT ((MMLISTS)PfnLastColored->u3.e1.PageLocation == MemoryList);
                    PageLastColored = PfnLastColored - MmPfnDatabase;

                    if (ListHead->Flink == Page) {

                        ASSERT (Pfn1->u2.Blink == MM_EMPTY_LIST);
                        ASSERT (ListHead->Blink != Page);

                        ListHead->Flink = PageNextFlink;

                        PfnNextFlink->u2.Blink = MM_EMPTY_LIST;
                    }
                    else {

                        ASSERT (Pfn1->u2.Blink != MM_EMPTY_LIST);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u3.e1.PageLocation == MemoryList);

                        MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink = PageNextFlink;
                        PfnNextFlink->u2.Blink = Pfn1->u2.Blink;
                    }

#if DBG
                    if (PfnLastColored->u1.Flink == MM_EMPTY_LIST) {
                        ASSERT (ListHead->Blink == PageLastColored);
                    }
#endif

                    Pfn1->u1.Flink = PfnLastColored->u1.Flink;
                    Pfn1->u2.Blink = PageLastColored;

                    if (ListHead->Blink == PageLastColored) {
                        ListHead->Blink = Page;
                    }

                    //
                    // Adjust the colored chains.
                    //

                    if (PfnLastColored->u1.Flink != MM_EMPTY_LIST) {
                        ASSERT (MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u3.e1.PageLocation) == MemoryList);
                        MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u2.Blink = Page;
                    }

                    PfnLastColored->u1.Flink = Page;

                    ColorHead->Flink = PageNextColored;
                    Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

                    ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                    PfnLastColored->OriginalPte.u.Long = Page;
                    ColorHead->Blink = Pfn1;
                }
            }
        }
    }

    RescanNeeded = FALSE;
    Pfn1 = MI_PFN_ELEMENT (StartPage);

    do {

        if ((Pfn1->u3.e1.PageLocation == StandbyPageList) &&
            (Pfn1->u1.Flink != 0) &&
            (Pfn1->u2.Blink != 0) &&
            (Pfn1->u3.e2.ReferenceCount == 0) &&
            (MmAvailablePages != 0)) {

            //
            // Systems utilizing memory compression may have more
            // pages on the zero, free and standby lists than we
            // want to give out.  Explicitly check MmAvailablePages
            // above instead (and recheck whenever the PFN lock is
            // released and reacquired).
            //

            ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

            RemovePage = TRUE;

            if (Pfn1->u3.e1.RemovalRequested == 0) {

                //
                // This page is not directly needed for a hot remove - but if
                // it contains a chunk of prototype PTEs (and this chunk is
                // in a page that needs to be removed), then any pages
                // referenced by transition prototype PTEs must also be removed
                // before the desired page can be removed.
                //
                // The same analogy holds for page table, directory, parent
                // and extended parent pages.
                //

                Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
                if (Pfn2->u3.e1.RemovalRequested == 0) {
#if (_MI_PAGING_LEVELS >= 3)
                    Pfn2 = MI_PFN_ELEMENT (Pfn2->u4.PteFrame);
                    if (Pfn2->u3.e1.RemovalRequested == 0) {
                        RemovePage = FALSE;
                    }
                    else if (Pfn2->u2.ShareCount == 1) {
                        RescanNeeded = TRUE;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    Pfn2 = MI_PFN_ELEMENT (Pfn2->u4.PteFrame);
                    if (Pfn2->u3.e1.RemovalRequested == 0) {
                        RemovePage = FALSE;
                    }
                    else if (Pfn2->u2.ShareCount == 1) {
                        RescanNeeded = TRUE;
                    }
#endif
#else
                    RemovePage = FALSE;
#endif
                }
                else if (Pfn2->u2.ShareCount == 1) {
                    RescanNeeded = TRUE;
                }
            }
    
            if (RemovePage == TRUE) {

                //
                // This page is in the desired range - grab it.
                //
    
                MiUnlinkPageFromList (Pfn1);
                MiRestoreTransitionPte (StartPage);
                MiInsertPageInList (&MmBadPageListHead, StartPage);
                NumberOfPages += 1;
            }
        }

        StartPage += 1;
        Pfn1 += 1;

    } while (StartPage < EndPage);

    if (RescanNeeded == TRUE) {

        //
        // A page table, directory or parent was freed by removing a transition
        // page from the cache.  Rescan from the top to pick it up.
        //

#if DBG
        MiDynmemData[7] += 1;
#endif

        goto rescan;
    }
#if DBG
    else {
        MiDynmemData[8] += 1;
    }
#endif

    return NumberOfPages;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\hypermap.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   hypermap.c

Abstract:

    This module contains the routines which map physical pages into
    reserved PTEs within hyper space.

Author:

    Lou Perazzoli (loup) 5-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

PMMPTE MiFirstReservedZeroingPte;

KEVENT MiImageMappingPteEvent;

#pragma alloc_text(PAGE,MiMapImageHeaderInHyperSpace)
#pragma alloc_text(PAGE,MiUnmapImageHeaderInHyperSpace)


PVOID
MiMapPageInHyperSpace (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex,
    IN PKIRQL OldIrql
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and returns the virtual address which maps the page.

    ************************************
    *                                  *
    * Returns with a spin lock held!!! *
    *                                  *
    ************************************

Arguments:

    Process - Supplies the current process.

    PageFrameIndex - Supplies the physical page number to map.

    OldIrql - Supplies a pointer in which to return the entry IRQL.

Return Value:

    Returns the address where the requested page was mapped.

    RETURNS WITH THE HYPERSPACE SPIN LOCK HELD!!!!

    The routine MiUnmapHyperSpaceMap MUST be called to release the lock!!!!

Environment:

    Kernel mode.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PFN_NUMBER offset;

    ASSERT (PageFrameIndex != 0);

    PointerPte = MmFirstReservedMappingPte;

    UNREFERENCED_PARAMETER (Process);       // Workaround compiler W4 bug.

    LOCK_HYPERSPACE (Process, OldIrql);

    //
    // Get offset to first free PTE.
    //

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (offset == 0) {

        //
        // All the reserved PTEs have been used, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_MAPPING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available mapping PTEs.
        //

        offset = NUMBER_OF_MAPPING_PTES;

        //
        // Flush entire TB only on processors executing this process.
        //

        KeFlushEntireTb (TRUE, FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - 1;

    //
    // Point to free entry and make it valid.
    //

    PointerPte += offset;
    ASSERT (PointerPte->u.Hard.Valid == 0);


    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    *PointerPte = TempPte;

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}

PVOID
MiMapPageInHyperSpaceAtDpc (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and returns the virtual address which maps the page.

    ************************************
    *                                  *
    * Returns with a spin lock held!!! *
    *                                  *
    ************************************

Arguments:

    Process - Supplies the current process.

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the address where the requested page was mapped.

    RETURNS WITH THE HYPERSPACE SPIN LOCK HELD!!!!

    The routine MiUnmapHyperSpaceMap MUST be called to release the lock!!!!

Environment:

    Kernel mode, DISPATCH_LEVEL on entry.

--*/

{

    MMPTE TempPte;
    PMMPTE PointerPte;
    PFN_NUMBER offset;

    UNREFERENCED_PARAMETER (Process);       // Workaround compiler W4 bug.

    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);
    ASSERT (PageFrameIndex != 0);

    LOCK_HYPERSPACE_AT_DPC (Process);

    //
    // Get offset to first free PTE.
    //

    PointerPte = MmFirstReservedMappingPte;

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (offset == 0) {

        //
        // All the reserved PTEs have been used, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_MAPPING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available mapping PTEs.
        //

        offset = NUMBER_OF_MAPPING_PTES;

        //
        // Flush entire TB only on processors executing this process.
        //

        KeFlushEntireTb (TRUE, FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - 1;

    //
    // Point to free entry and make it valid.
    //

    PointerPte += offset;
    ASSERT (PointerPte->u.Hard.Valid == 0);


    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    *PointerPte = TempPte;

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}

PVOID
MiMapImageHeaderInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into the
    PTE within hyper space reserved explicitly for image page
    header mapping.  By reserving an explicit PTE for mapping
    the PTE, page faults can occur while the PTE is mapped within
    hyperspace and no other hyperspace maps will affect this PTE.

    Note that if another thread attempts to map an image at the
    same time, it will be forced into a wait state until the
    header is "unmapped".

Arguments:

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the virtual address where the specified physical page was
    mapped.

Environment:

    Kernel mode.

--*/

{
    MMPTE TempPte;
    MMPTE OriginalPte;
    PMMPTE PointerPte;
    PVOID FlushVaPointer;

    ASSERT (PageFrameIndex != 0);

    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    FlushVaPointer = (PVOID) IMAGE_MAPPING_PTE;

    //
    // Ensure both modified and accessed bits are set so the hardware doesn't
    // ever write this PTE.
    //

    ASSERT (TempPte.u.Hard.Dirty == 1);
    ASSERT (TempPte.u.Hard.Accessed == 1);

    PointerPte = MiGetPteAddress (IMAGE_MAPPING_PTE);

    do {
        OriginalPte.u.Long = 0;

        OriginalPte.u.Long = InterlockedCompareExchangePte (
                                PointerPte,
                                TempPte.u.Long,
                                OriginalPte.u.Long);
                                                             
        if (OriginalPte.u.Long == 0) {
            break;
        }

        //
        // Another thread modified the PTE just before us or the PTE was
        // already in use.  This should be very rare - go the long way.
        //

        InterlockedIncrement ((PLONG)&MmWorkingSetList->NumberOfImageWaiters);

        //
        // Deliberately wait with a timeout since the PTE release runs
        // without lock synchronization so there is the extremely rare
        // race window which the timeout saves us from.
        //

        KeWaitForSingleObject (&MiImageMappingPteEvent,
                               Executive,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)&MmOneSecond);

        InterlockedDecrement ((PLONG)&MmWorkingSetList->NumberOfImageWaiters);

    } while (TRUE);

    //
    // Use KeFlushMultiple even though only one PTE is being flushed
    // because this interface provides a way to just flush the
    // specified TB entry without writing a PTE and we always want
    // to do interlocked writes to this PTE.
    //
    // Note the flush must be made across all processors as this thread
    // may migrate.  Also this must be done here instead of in the unmap
    // in order to support lock-free operation.
    //

    KeFlushMultipleTb (1,
                       &FlushVaPointer,
                       TRUE,
                       TRUE,
                       NULL,
                       TempPte.u.Flush);

    return (PVOID) MiGetVirtualAddressMappedByPte (PointerPte);
}

VOID
MiUnmapImageHeaderInHyperSpace (
    VOID
    )

/*++

Routine Description:

    This procedure unmaps the PTE reserved for mapping the image
    header, flushes the TB, and, if the WaitingForImageMapping field
    is not NULL, sets the specified event.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;

    PointerPte = MiGetPteAddress (IMAGE_MAPPING_PTE);

    //
    // Capture the number of waiters.
    //

    ASSERT (PointerPte->u.Long != 0);

#if defined (_WIN64)
    InterlockedExchange64 ((PLONG64)PointerPte, ZeroPte.u.Long);
#elif defined(_X86PAE_)
    KeInterlockedSwapPte ((PHARDWARE_PTE)PointerPte,
                          (PHARDWARE_PTE)&ZeroPte.u.Long);
#else
    InterlockedExchange ((PLONG)PointerPte, ZeroPte.u.Long);
#endif

    if (MmWorkingSetList->NumberOfImageWaiters != 0) {

        //
        // If there are any threads waiting, wake them all now.  Note this
        // will wake threads in other processes as well, but it is very
        // rare that there are any waiters in the entire system period.
        //

        KePulseEvent (&MiImageMappingPteEvent, 0, FALSE);
    }

    return;
}

PVOID
MiMapPageToZeroInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and returns the virtual address which maps the page.

    This is only to be used by THE zeroing page thread.

Arguments:

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the virtual address where the specified physical page was
    mapped.

Environment:

    PASSIVE_LEVEL.

--*/

{
    PFN_NUMBER offset;
    MMPTE TempPte;
    PMMPTE PointerPte;

    ASSERT (PageFrameIndex != 0);

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    PointerPte = MiFirstReservedZeroingPte;

    //
    // Get offset to first free PTE.
    //

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (offset == 0) {

        //
        // All the reserved PTEs have been used, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_ZEROING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available zeroing PTEs.
        //

        offset = NUMBER_OF_ZEROING_PTES;
        PointerPte->u.Hard.PageFrameNumber = offset;

        //
        // Flush entire TB only on processors executing this process as this
        // thread may migrate there at any time.
        //

        KeFlushEntireTb (TRUE, FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - 1;

    //
    // Point to free entry and make it valid.
    //

    PointerPte += offset;
    ASSERT (PointerPte->u.Hard.Valid == 0);

    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    *PointerPte = TempPte;

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\forksup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   forksup.c

Abstract:

    This module contains the routines which support the POSIX fork operation.

Author:

    Lou Perazzoli (loup) 22-Jul-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

VOID
MiUpPfnReferenceCount (
    IN PFN_NUMBER Page,
    IN USHORT Count
    );

VOID
MiDownPfnReferenceCount (
    IN PFN_NUMBER Page,
    IN USHORT Count
    );

VOID
MiUpControlAreaRefs (
    IN PMMVAD Vad
    );

ULONG
MiDoneWithThisPageGetAnother (
    IN PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    );

ULONG
MiLeaveThisPageGetAnother (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    );

VOID
MiUpForkPageShareCount (
    IN PMMPFN PfnForkPtePage
    );

ULONG
MiHandleForkTransitionPte (
    IN PMMPTE PointerPte,
    IN PMMPTE PointerNewPte,
    IN PMMCLONE_BLOCK ForkProtoPte
    );

VOID
MiDownShareCountFlushEntireTb (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiBuildForkPageTable (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PMMPTE PointerNewPde,
    IN PFN_NUMBER PdePhysicalPage,
    IN PMMPFN PfnPdPage,
    IN LOGICAL MakeValid
    );

VOID
MiRetrievePageDirectoryFrames (
    IN PFN_NUMBER RootPhysicalPage,
    OUT PPFN_NUMBER PageDirectoryFrames
    );

#define MM_FORK_SUCCEEDED 0
#define MM_FORK_FAILED 1

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiCloneProcessAddressSpace)
#endif


NTSTATUS
MiCloneProcessAddressSpace (
    IN PEPROCESS ProcessToClone,
    IN PEPROCESS ProcessToInitialize,
    IN PFN_NUMBER RootPhysicalPage,
    IN PFN_NUMBER HyperPhysicalPage
    )

/*++

Routine Description:

    This routine stands on its head to produce a copy of the specified
    process's address space in the process to initialize.  This
    is done by examining each virtual address descriptor's inherit
    attributes.  If the pages described by the VAD should be inherited,
    each PTE is examined and copied into the new address space.

    For private pages, fork prototype PTEs are constructed and the pages
    become shared, copy-on-write, between the two processes.


Arguments:

    ProcessToClone - Supplies the process whose address space should be
                     cloned.

    ProcessToInitialize - Supplies the process whose address space is to
                          be created.

    RootPhysicalPage - Supplies the physical page number of the top level
                       page (parent on 64-bit systems) directory
                       of the process to initialize.

    HyperPhysicalPage - Supplies the physical page number of the page table
                        page which maps hyperspace for the process to
                        initialize.  This is only needed for 32-bit systems.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PFN_NUMBER PdePhysicalPage;
    PEPROCESS CurrentProcess;
    PMMPTE PdeBase;
    PMMCLONE_HEADER CloneHeader;
    PMMCLONE_BLOCK CloneProtos;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    PMMVAD NewVad;
    PMMVAD Vad;
    PMMVAD NextVad;
    PMMVAD *VadList;
    PMMVAD FirstNewVad;
    PMMCLONE_DESCRIPTOR *CloneList;
    PMMCLONE_DESCRIPTOR FirstNewClone;
    PMMCLONE_DESCRIPTOR Clone;
    PMMCLONE_DESCRIPTOR NextClone;
    PMMCLONE_DESCRIPTOR NewClone;
    ULONG Attached;
    ULONG CloneFailed;
    ULONG VadInsertFailed;
    WSLE_NUMBER WorkingSetIndex;
    PVOID VirtualAddress;
    NTSTATUS status;
    PMMPFN Pfn2;
    PMMPFN PfnPdPage;
    MMPTE TempPte;
    MMPTE PteContents;
    KAPC_STATE ApcState;
#if defined (_X86PAE_)
    ULONG i;
    PMDL MdlPageDirectory;
    PPFN_NUMBER MdlPageFrames;
    PFN_NUMBER PageDirectoryFrames[PD_PER_SYSTEM];
    PFN_NUMBER MdlHackPageDirectory[(sizeof(MDL)/sizeof(PFN_NUMBER)) + PD_PER_SYSTEM];
#else
    PFN_NUMBER MdlDirPage;
#endif
    PFN_NUMBER MdlPage;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    PMMPTE PointerNewPte;
    PMMPTE NewPteMappedAddress;
    PMMPTE PointerNewPde;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    PFN_NUMBER PageFrameIndex;
    PMMCLONE_BLOCK ForkProtoPte;
    PMMCLONE_BLOCK CloneProto;
    PMMCLONE_BLOCK LockedForkPte;
    PMMPTE ContainingPte;
    ULONG NumberOfForkPtes;
    PFN_NUMBER NumberOfPrivatePages;
    PFN_NUMBER PageTablePage;
    SIZE_T TotalPagedPoolCharge;
    SIZE_T TotalNonPagedPoolCharge;
    PMMPFN PfnForkPtePage;
    PVOID UsedPageTableEntries;
    ULONG ReleasedWorkingSetMutex;
    ULONG FirstTime;
    ULONG Waited;
    ULONG PpePdeOffset;
#if defined (_MIALT4K_)
    PVOID TempAliasInformation;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpeLast;
    PFN_NUMBER PageDirFrameIndex;
    PVOID UsedPageDirectoryEntries;
    PMMPTE PointerNewPpe;
    PMMPTE PpeBase;
    PMMPFN PfnPpPage;
    PMMPTE PpeInWsle;
#if (_MI_PAGING_LEVELS >= 4)
    PVOID UsedPageDirectoryParentEntries;
    PFN_NUMBER PpePhysicalPage;
    PFN_NUMBER PageParentFrameIndex;
    PMMPTE PointerNewPxe;
    PMMPTE PxeBase;
    PMMPFN PfnPxPage;
    PFN_NUMBER MdlDirParentPage;
#endif

    UNREFERENCED_PARAMETER (HyperPhysicalPage);
#else
    PMMWSL HyperBase;
    PMMWSL HyperWsl;
#endif

    PAGED_CODE();

    PageTablePage = 2;
    NumberOfForkPtes = 0;
    Attached = FALSE;
    PageFrameIndex = (PFN_NUMBER)-1;

#if DBG
    if (MmDebug & MM_DBG_FORK) {
        DbgPrint("beginning clone operation process to clone = %p\n",
            ProcessToClone);
    }
#endif

    if (ProcessToClone != PsGetCurrentProcess()) {
        Attached = TRUE;
        KeStackAttachProcess (&ProcessToClone->Pcb, &ApcState);
    }

#if defined (_X86PAE_)
    MiRetrievePageDirectoryFrames (RootPhysicalPage, PageDirectoryFrames);
#endif

    CurrentProcess = ProcessToClone;

    //
    // Get the working set mutex and the address creation mutex
    // of the process to clone.  This prevents page faults while we
    // are examining the address map and prevents virtual address space
    // from being created or deleted.
    //

    LOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // Write-watch VAD bitmaps are not currently duplicated
    // so fork is not allowed.
    //

    if (CurrentProcess->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) {
        status = STATUS_INVALID_PAGE_PROTECTION;
        goto ErrorReturn1;
    }

    //
    // Check for AWE regions as they are not duplicated so fork is not allowed.
    // Note that since this is a readonly list walk, the address space mutex
    // is sufficient to synchronize properly.
    //

    NextEntry = CurrentProcess->PhysicalVadList.Flink;
    while (NextEntry != &CurrentProcess->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad->u.VadFlags.UserPhysicalPages == 1) {
            status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorReturn1;
        }

        NextEntry = NextEntry->Flink;
    }

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (CurrentProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    //
    // Attempt to acquire the needed pool before starting the
    // clone operation, this allows an easier failure path in
    // the case of insufficient system resources.  The working set mutex
    // must be acquired (and held throughout) to protect against modifications
    // to the NumberOfPrivatePages field in the EPROCESS.
    //

#if defined (_MIALT4K_)
    if (CurrentProcess->Wow64Process != NULL) {
        LOCK_ALTERNATE_TABLE_UNSAFE(CurrentProcess->Wow64Process);
    }
#endif

    LOCK_WS (CurrentProcess);

    ASSERT (CurrentProcess->ForkInProgress == NULL);

    //
    // Indicate to the pager that the current process is being
    // forked.  This blocks other threads in that process from
    // modifying clone block counts and contents as well as alternate PTEs.
    //

    CurrentProcess->ForkInProgress = PsGetCurrentThread ();

#if defined (_MIALT4K_)
    if (CurrentProcess->Wow64Process != NULL) {
        UNLOCK_ALTERNATE_TABLE_UNSAFE(CurrentProcess->Wow64Process);
    }
#endif

    NumberOfPrivatePages = CurrentProcess->NumberOfPrivatePages;

    CloneProtos = ExAllocatePoolWithTag (PagedPool, sizeof(MMCLONE_BLOCK) *
                                                NumberOfPrivatePages,
                                                'lCmM');
    if (CloneProtos == NULL) {
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn1;
    }

    CloneHeader = ExAllocatePoolWithTag (NonPagedPool,
                                         sizeof(MMCLONE_HEADER),
                                         'hCmM');
    if (CloneHeader == NULL) {
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    CloneDescriptor = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MMCLONE_DESCRIPTOR),
                                             'dCmM');
    if (CloneDescriptor == NULL) {
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    Vad = MiGetFirstVad (CurrentProcess);
    VadList = &FirstNewVad;

    while (Vad != NULL) {

        //
        // If the VAD does not go to the child, ignore it.
        //

        if ((Vad->u.VadFlags.UserPhysicalPages == 0) &&

            ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->u2.VadFlags2.Inherit == MM_VIEW_SHARE))) {

            NewVad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');

            if (NewVad == NULL) {

                //
                // Unable to allocate pool for all the VADs.  Deallocate
                // all VADs and other pool obtained so far.
                //

                CurrentProcess->ForkInProgress = NULL;
                UNLOCK_WS (CurrentProcess);
                *VadList = NULL;
                status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn4;
            }

            RtlZeroMemory (NewVad, sizeof(MMVAD_LONG));

#if defined (_MIALT4K_)
            if (((Vad->u.VadFlags.PrivateMemory) && (Vad->u.VadFlags.NoChange == 0)) 
                ||
                (Vad->u2.VadFlags2.LongVad == 0)) {

                NOTHING;
            }
            else if (((PMMVAD_LONG)Vad)->AliasInformation != NULL) {

                //
                // This VAD has aliased VADs which are going to be duplicated
                // into the clone's address space, but the alias list must
                // be explicitly copied.
                //

                ((PMMVAD_LONG)NewVad)->AliasInformation = MiDuplicateAliasVadList (Vad);

                if (((PMMVAD_LONG)NewVad)->AliasInformation == NULL) {
                    CurrentProcess->ForkInProgress = NULL;
                    UNLOCK_WS (CurrentProcess);
                    ExFreePool (NewVad);
                    *VadList = NULL;
                    status = STATUS_INSUFFICIENT_RESOURCES;
                    goto ErrorReturn4;
                }
            }
#endif

            *VadList = NewVad;
            VadList = &NewVad->Parent;
        }
        Vad = MiGetNextVad (Vad);
    }

    //
    // Terminate list of VADs for new process.
    //

    *VadList = NULL;

    //
    // Charge the current process the quota for the paged and nonpaged
    // global structures.  This consists of the array of clone blocks
    // in paged pool and the clone header in non-paged pool.
    //

    status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                            sizeof(MMCLONE_BLOCK) * NumberOfPrivatePages);

    if (!NT_SUCCESS(status)) {

        //
        // Unable to charge quota for the clone blocks.
        //

        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        goto ErrorReturn4;
    }

    PageTablePage = 1;
    status = PsChargeProcessNonPagedPoolQuota (CurrentProcess,
                                               sizeof(MMCLONE_HEADER));

    if (!NT_SUCCESS(status)) {

        //
        // Unable to charge quota for the clone blocks.
        //

        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        goto ErrorReturn4;
    }

    PageTablePage = 0;

    //
    // Initializing UsedPageTableEntries is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    UsedPageTableEntries = NULL;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    PageDirFrameIndex = 0;
    UsedPageDirectoryEntries = NULL;

#if (_MI_PAGING_LEVELS >= 4)
    PageParentFrameIndex = 0;
    UsedPageDirectoryParentEntries = NULL;
#endif

    //
    // Increment the reference count for the pages which are being "locked"
    // in MDLs.  This prevents the page from being reused while it is
    // being double mapped.  Note the refcount below reflects the PXE, PPE,
    // PDE and PTE initial dummy pages that we initialize below.
    //

    MiUpPfnReferenceCount (RootPhysicalPage, _MI_PAGING_LEVELS);

    //
    // Map the (extended) page directory parent page into the system address
    // space.  This is accomplished by building an MDL to describe the
    // page directory (extended) parent page.
    //

    PpeBase = (PMMPTE)MiMapSinglePage (NULL,
                                       RootPhysicalPage,
                                       MmCached,
                                       HighPagePriority);

    if (PpeBase == NULL) {
        MiDownPfnReferenceCount (RootPhysicalPage, _MI_PAGING_LEVELS);
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

    PfnPpPage = MI_PFN_ELEMENT (RootPhysicalPage);

#if (_MI_PAGING_LEVELS >= 4)
    PxeBase = (PMMPTE)MiMapSinglePage (NULL,
                                       RootPhysicalPage,
                                       MmCached,
                                       HighPagePriority);

    if (PxeBase == NULL) {
        MiDownPfnReferenceCount (RootPhysicalPage, _MI_PAGING_LEVELS);
        MiUnmapSinglePage (PpeBase);
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

    PfnPxPage = MI_PFN_ELEMENT (RootPhysicalPage);

    MdlDirParentPage = RootPhysicalPage;

#endif

#elif !defined (_X86PAE_)

    MiUpPfnReferenceCount (RootPhysicalPage, 1);

#endif

    //
    // Initialize a page directory map so it can be
    // unlocked in the loop and the end of the loop without
    // any testing to see if has a valid value the first time through.
    // Note this is a dummy map for 64-bit systems and a real one for 32-bit.
    //

#if !defined (_X86PAE_)

    MdlDirPage = RootPhysicalPage;

    PdePhysicalPage = RootPhysicalPage;

    PdeBase = (PMMPTE)MiMapSinglePage (NULL,
                                       MdlDirPage,
                                       MmCached,
                                       HighPagePriority);

    if (PdeBase == NULL) {
#if (_MI_PAGING_LEVELS >= 3)
        MiDownPfnReferenceCount (RootPhysicalPage, _MI_PAGING_LEVELS);
        MiUnmapSinglePage (PpeBase);
#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapSinglePage (PxeBase);
#endif
#else
        MiDownPfnReferenceCount (RootPhysicalPage, 1);
#endif
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

#else

    //
    // All 4 page directory pages need to be mapped for PAE so the heavyweight
    // mapping must be used.
    //

    MdlPageDirectory = (PMDL)&MdlHackPageDirectory[0];

    MmInitializeMdl (MdlPageDirectory,
                     (PVOID)PDE_BASE,
                     PD_PER_SYSTEM * PAGE_SIZE);

    MdlPageDirectory->MdlFlags |= MDL_PAGES_LOCKED;

    MdlPageFrames = (PPFN_NUMBER)(MdlPageDirectory + 1);

    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        *(MdlPageFrames + i) = PageDirectoryFrames[i];
        MiUpPfnReferenceCount (PageDirectoryFrames[i], 1);
    }

    PdePhysicalPage = RootPhysicalPage;

    PdeBase = (PMMPTE)MmMapLockedPagesSpecifyCache (MdlPageDirectory,
                                                    KernelMode,
                                                    MmCached,
                                                    NULL,
                                                    FALSE,
                                                    HighPagePriority);

    if (PdeBase == NULL) {
        for (i = 0; i < PD_PER_SYSTEM; i += 1) {
            MiDownPfnReferenceCount (PageDirectoryFrames[i], 1);
        }
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

#endif

    PfnPdPage = MI_PFN_ELEMENT (RootPhysicalPage);

#if (_MI_PAGING_LEVELS < 3)

    //
    // Map hyperspace so target UsedPageTable entries can be incremented.
    //

    MiUpPfnReferenceCount (HyperPhysicalPage, 2);

    HyperBase = (PMMWSL)MiMapSinglePage (NULL,
                                         HyperPhysicalPage,
                                         MmCached,
                                         HighPagePriority);

    if (HyperBase == NULL) {
        MiDownPfnReferenceCount (HyperPhysicalPage, 2);
#if !defined (_X86PAE_)
        MiDownPfnReferenceCount (RootPhysicalPage, 1);
        MiUnmapSinglePage (PdeBase);
#else
        for (i = 0; i < PD_PER_SYSTEM; i += 1) {
            MiDownPfnReferenceCount (PageDirectoryFrames[i], 1);
        }
        MmUnmapLockedPages (PdeBase, MdlPageDirectory);
#endif
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

    //
    // MmWorkingSetList is not page aligned when booted /3GB so account
    // for that here when established the used page table entry pointer.
    //

    HyperWsl = (PMMWSL) ((PCHAR)HyperBase + BYTE_OFFSET(MmWorkingSetList));
#endif

    //
    // Initialize a page table MDL to lock and map the hyperspace page so it
    // can be unlocked in the loop and the end of the loop without
    // any testing to see if has a valid value the first time through.
    //

#if (_MI_PAGING_LEVELS >= 3)
    MdlPage = RootPhysicalPage;
#else
    MdlPage = HyperPhysicalPage;
#endif

    NewPteMappedAddress = (PMMPTE)MiMapSinglePage (NULL,
                                                   MdlPage,
                                                   MmCached,
                                                   HighPagePriority);

    if (NewPteMappedAddress == NULL) {

#if (_MI_PAGING_LEVELS >= 3)

        MiDownPfnReferenceCount (RootPhysicalPage, _MI_PAGING_LEVELS);
#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapSinglePage (PxeBase);
#endif
        MiUnmapSinglePage (PpeBase);
        MiUnmapSinglePage (PdeBase);

#else
        MiDownPfnReferenceCount (HyperPhysicalPage, 2);
        MiUnmapSinglePage (HyperBase);
#if !defined (_X86PAE_)
        MiDownPfnReferenceCount (RootPhysicalPage, 1);
        MiUnmapSinglePage (PdeBase);
#else
        for (i = 0; i < PD_PER_SYSTEM; i += 1) {
            MiDownPfnReferenceCount (PageDirectoryFrames[i], 1);
        }
        MmUnmapLockedPages (PdeBase, MdlPageDirectory);
#endif

#endif

        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn4;
    }

    PointerNewPte = NewPteMappedAddress;

    //
    // Build a new clone prototype PTE block and descriptor, note that
    // each prototype PTE has a reference count following it.
    //

    ForkProtoPte = CloneProtos;

    LockedForkPte = ForkProtoPte;
    MiLockPagedAddress (LockedForkPte, FALSE);

    CloneHeader->NumberOfPtes = (ULONG)NumberOfPrivatePages;
    CloneHeader->NumberOfProcessReferences = 1;
    CloneHeader->ClonePtes = CloneProtos;

    CloneDescriptor->StartingVpn = (ULONG_PTR)CloneProtos;
    CloneDescriptor->EndingVpn = (ULONG_PTR)((ULONG_PTR)CloneProtos +
                            NumberOfPrivatePages *
                              sizeof(MMCLONE_BLOCK));
    CloneDescriptor->EndingVpn -= 1;
    CloneDescriptor->NumberOfReferences = 0;
    CloneDescriptor->FinalNumberOfReferences = 0;
    CloneDescriptor->NumberOfPtes = (ULONG)NumberOfPrivatePages;
    CloneDescriptor->CloneHeader = CloneHeader;
    CloneDescriptor->PagedPoolQuotaCharge = sizeof(MMCLONE_BLOCK) *
                                NumberOfPrivatePages;

    //
    // Insert the clone descriptor for this fork operation into the
    // process which was cloned.
    //

    MiInsertClone (CurrentProcess, CloneDescriptor);

    //
    // Examine each virtual address descriptor and create the
    // proper structures for the new process.
    //

    Vad = MiGetFirstVad (CurrentProcess);
    NewVad = FirstNewVad;

    while (Vad != NULL) {

        //
        // Examine the VAD to determine its type and inheritance
        // attribute.
        //

        if ((Vad->u.VadFlags.UserPhysicalPages == 0) &&

            ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->u2.VadFlags2.Inherit == MM_VIEW_SHARE))) {

            //
            // The virtual address descriptor should be shared in the
            // forked process.
            //
            // Make a copy of the VAD for the new process, the new VADs
            // are preallocated and linked together through the parent
            // field.
            //

            NextVad = NewVad->Parent;

            if (Vad->u.VadFlags.PrivateMemory == 1) {
                *(PMMVAD_SHORT)NewVad = *(PMMVAD_SHORT)Vad;
                NewVad->u.VadFlags.NoChange = 0;
            }
            else {
                if (Vad->u2.VadFlags2.LongVad == 0) {
                    *NewVad = *Vad;
                }
                else {

#if defined (_MIALT4K_)

                    //
                    // The VADs duplication earlier in this routine keeps both
                    // the current process' VAD tree and the new process' VAD
                    // list ordered.  ASSERT on this below.
                    //

#if DBG
                    if (((PMMVAD_LONG)Vad)->AliasInformation == NULL) {
                        ASSERT (((PMMVAD_LONG)NewVad)->AliasInformation == NULL);
                    }
                    else {
                        ASSERT (((PMMVAD_LONG)NewVad)->AliasInformation != NULL);
                    }
#endif

                    TempAliasInformation = ((PMMVAD_LONG)NewVad)->AliasInformation;
#endif

                    *(PMMVAD_LONG)NewVad = *(PMMVAD_LONG)Vad;

#if defined (_MIALT4K_)
                    ((PMMVAD_LONG)NewVad)->AliasInformation = TempAliasInformation;
#endif

                    if (Vad->u2.VadFlags2.ExtendableFile == 1) {
                        ExAcquireFastMutexUnsafe (&MmSectionBasedMutex);
                        ASSERT (Vad->ControlArea->Segment->ExtendInfo != NULL);
                        Vad->ControlArea->Segment->ExtendInfo->ReferenceCount += 1;
                        ExReleaseFastMutexUnsafe (&MmSectionBasedMutex);
                    }
                }
            }

            NewVad->u2.VadFlags2.LongVad = 1;

            if (NewVad->u.VadFlags.NoChange) {
                if ((NewVad->u2.VadFlags2.OneSecured) ||
                    (NewVad->u2.VadFlags2.MultipleSecured)) {

                    //
                    // Eliminate these as the memory was secured
                    // only in this process, not in the new one.
                    //

                    NewVad->u2.VadFlags2.OneSecured = 0;
                    NewVad->u2.VadFlags2.MultipleSecured = 0;
                    ((PMMVAD_LONG) NewVad)->u3.List.Flink = NULL;
                    ((PMMVAD_LONG) NewVad)->u3.List.Blink = NULL;
                }
                if (NewVad->u2.VadFlags2.SecNoChange == 0) {
                    NewVad->u.VadFlags.NoChange = 0;
                }
            }
            NewVad->Parent = NextVad;

            //
            // If the VAD refers to a section, up the view count for that
            // section.  This requires the PFN lock to be held.
            //

            if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                (Vad->ControlArea != NULL)) {

                if ((Vad->u.VadFlags.Protection & MM_READWRITE) &&
                    (Vad->ControlArea->FilePointer != NULL) &&
                    (Vad->ControlArea->u.Flags.Image == 0)) {

                    InterlockedIncrement ((PLONG)&Vad->ControlArea->Segment->WritableUserReferences);
                }

                //
                // Increment the count of the number of views for the
                // section object.  This requires the PFN lock to be held.
                //

                MiUpControlAreaRefs (Vad);
            }

            //
            // Examine each PTE and create the appropriate PTE for the
            // new process.
            //

            PointerPde = MiGetPdeAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));
            FirstTime = TRUE;

            while ((PMMPTE)PointerPte <= LastPte) {

                //
                // For each PTE contained in the VAD check the page table
                // page, and if non-zero, make the appropriate modifications
                // to copy the PTE to the new process.
                //

                if ((FirstTime) || MiIsPteOnPdeBoundary (PointerPte)) {

                    PointerPxe = MiGetPpeAddress (PointerPte);
                    PointerPpe = MiGetPdeAddress (PointerPte);
                    PointerPde = MiGetPteAddress (PointerPte);

                    do {

#if (_MI_PAGING_LEVELS >= 4)
                        while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited)) {
    
                            //
                            // Extended page directory parent is empty,
                            // go to the next one.
                            //
    
                            PointerPxe += 1;
                            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if ((PMMPTE)PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
                        }
    
                        Waited = 0;
#endif
                        while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited)) {
    
                            //
                            // Page directory parent is empty, go to the next one.
                            //
    
                            PointerPpe += 1;
                            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                                PointerPxe = MiGetPteAddress (PointerPpe);
                                Waited = 1;
                                break;
                            }
                            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if ((PMMPTE)PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
                        }
    
#if (_MI_PAGING_LEVELS < 4)
                        Waited = 0;
#endif
    
                        while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited)) {
    
                            //
                            // This page directory is empty, go to the next one.
                            //
    
                            PointerPde += 1;
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if ((PMMPTE)PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
#if (_MI_PAGING_LEVELS >= 3)
                            if (MiIsPteOnPdeBoundary (PointerPde)) {
                                PointerPpe = MiGetPteAddress (PointerPde);
                                PointerPxe = MiGetPdeAddress (PointerPde);
                                Waited = 1;
                                break;
                            }
#endif
                        }
    
                    } while (Waited != 0);

                    FirstTime = FALSE;

#if (_MI_PAGING_LEVELS >= 4)
                    //
                    // Calculate the address of the PXE in the new process's
                    // extended page directory parent page.
                    //

                    PointerNewPxe = &PxeBase[MiGetPpeOffset(PointerPte)];

                    if (PointerNewPxe->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a valid page.  This will become
                        // a page directory parent page for the new process.
                        //
                        // Note that unlike page table pages which are left
                        // in transition, page directory parent pages (and page
                        // directory pages) are left valid and hence
                        // no share count decrement is done.
                        //

                        ReleasedWorkingSetMutex =
                                MiLeaveThisPageGetAnother (&PageParentFrameIndex,
                                                           PointerPxe,
                                                           CurrentProcess);

                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageParentFrameIndex));

                        if (ReleasedWorkingSetMutex) {

                            do {

                                MiDoesPxeExistAndMakeValid (PointerPxe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
                                Waited = 0;
    
                                MiDoesPpeExistAndMakeValid (PointerPpe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
                                MiDoesPdeExistAndMakeValid (PointerPde,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
                            } while (Waited != 0);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

                        MiBuildForkPageTable (PageParentFrameIndex,
                                              PointerPxe,
                                              PointerNewPxe,
                                              RootPhysicalPage,
                                              PfnPxPage,
                                              TRUE);

                        //
                        // Map the new page directory page into the system
                        // portion of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        MiDownPfnReferenceCount (MdlDirParentPage, 1);

                        MdlDirParentPage = PageParentFrameIndex;

                        ASSERT (PpeBase != NULL);

                        PpeBase = (PMMPTE)MiMapSinglePage (PpeBase,
                                                           MdlDirParentPage,
                                                           MmCached,
                                                           HighPagePriority);

                        MiUpPfnReferenceCount (MdlDirParentPage, 1);

                        PointerNewPxe = PpeBase;
                        PpePhysicalPage = PageParentFrameIndex;

                        PfnPpPage = MI_PFN_ELEMENT (PpePhysicalPage);
    
                        UsedPageDirectoryParentEntries = (PVOID)PfnPpPage;
                    }
                    else {

                        ASSERT (PointerNewPxe->u.Hard.Valid == 1 ||
                                PointerNewPxe->u.Soft.Transition == 1);

                        if (PointerNewPxe->u.Hard.Valid == 1) {
                            PpePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerNewPxe);
                        }
                        else {
                            PpePhysicalPage = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerNewPxe);
                        }

                        //
                        // If we are switching from one page directory parent
                        // frame to another and the last one is one that we
                        // freshly allocated, the last one's reference count
                        // must be decremented now that we're done with it.
                        //
                        // Note that at least one target PXE has already been
                        // initialized for this codepath to execute.
                        //

                        ASSERT (PageParentFrameIndex == MdlDirParentPage);

                        if (MdlDirParentPage != PpePhysicalPage) {
                            ASSERT (MdlDirParentPage != (PFN_NUMBER)-1);
                            MiDownPfnReferenceCount (MdlDirParentPage, 1);
                            PageParentFrameIndex = PpePhysicalPage;
                            MdlDirParentPage = PageParentFrameIndex;

                            ASSERT (PpeBase != NULL);
    
                            PpeBase = (PMMPTE)MiMapSinglePage (PpeBase,
                                                               MdlDirParentPage,
                                                               MmCached,
                                                               HighPagePriority);
    
                            MiUpPfnReferenceCount (MdlDirParentPage, 1);
    
                            PointerNewPpe = PpeBase;

                            PfnPpPage = MI_PFN_ELEMENT (PpePhysicalPage);
        
                            UsedPageDirectoryParentEntries = (PVOID)PfnPpPage;
                        }
                    }
#endif

#if (_MI_PAGING_LEVELS >= 3)

                    //
                    // Calculate the address of the PPE in the new process's
                    // page directory parent page.
                    //

                    PointerNewPpe = &PpeBase[MiGetPdeOffset(PointerPte)];

                    if (PointerNewPpe->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a valid page.  This will
                        // become a page directory page for the new process.
                        //
                        // Note that unlike page table pages which are left
                        // in transition, page directory pages are left valid
                        // and hence no share count decrement is done.
                        //

                        ReleasedWorkingSetMutex =
                                MiLeaveThisPageGetAnother (&PageDirFrameIndex,
                                                           PointerPpe,
                                                           CurrentProcess);

                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageDirFrameIndex));

                        if (ReleasedWorkingSetMutex) {

                            do {

#if (_MI_PAGING_LEVELS >= 4)
                                MiDoesPxeExistAndMakeValid (PointerPxe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
                                Waited = 0;
#endif

                                MiDoesPpeExistAndMakeValid (PointerPpe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
#if (_MI_PAGING_LEVELS < 4)
                                Waited = 0;
#endif
    
                                MiDoesPdeExistAndMakeValid (PointerPde,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
                            } while (Waited != 0);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

                        MiBuildForkPageTable (PageDirFrameIndex,
                                              PointerPpe,
                                              PointerNewPpe,
#if (_MI_PAGING_LEVELS >= 4)
                                              PpePhysicalPage,
#else
                                              RootPhysicalPage,
#endif
                                              PfnPpPage,
                                              TRUE);

#if (_MI_PAGING_LEVELS >= 4)
                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryParentEntries);
#endif
                        //
                        // Map the new page directory page into the system
                        // portion of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        MiDownPfnReferenceCount (MdlDirPage, 1);

                        MdlDirPage = PageDirFrameIndex;

                        ASSERT (PdeBase != NULL);

                        PdeBase = (PMMPTE)MiMapSinglePage (PdeBase,
                                                           MdlDirPage,
                                                           MmCached,
                                                           HighPagePriority);

                        MiUpPfnReferenceCount (MdlDirPage, 1);

                        PointerNewPde = PdeBase;
                        PdePhysicalPage = PageDirFrameIndex;

                        PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
    
                        UsedPageDirectoryEntries = (PVOID)PfnPdPage;
                    }
                    else {
                        ASSERT (PointerNewPpe->u.Hard.Valid == 1 ||
                                PointerNewPpe->u.Soft.Transition == 1);

                        if (PointerNewPpe->u.Hard.Valid == 1) {
                            PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerNewPpe);
                        }
                        else {
                            PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerNewPpe);
                        }

                        //
                        // If we are switching from one page directory frame to
                        // another and the last one is one that we freshly
                        // allocated, the last one's reference count must be
                        // decremented now that we're done with it.
                        //
                        // Note that at least one target PPE has already been
                        // initialized for this codepath to execute.
                        //

                        ASSERT (PageDirFrameIndex == MdlDirPage);

                        if (MdlDirPage != PdePhysicalPage) {
                            ASSERT (MdlDirPage != (PFN_NUMBER)-1);
                            MiDownPfnReferenceCount (MdlDirPage, 1);
                            PageDirFrameIndex = PdePhysicalPage;
                            MdlDirPage = PageDirFrameIndex;

                            ASSERT (PdeBase != NULL);
    
                            PdeBase = (PMMPTE)MiMapSinglePage (PdeBase,
                                                               MdlDirPage,
                                                               MmCached,
                                                               HighPagePriority);
    
                            MiUpPfnReferenceCount (MdlDirPage, 1);
    
                            PointerNewPde = PdeBase;

                            PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
        
                            UsedPageDirectoryEntries = (PVOID)PfnPdPage;
                        }
                    }
#endif

                    //
                    // Calculate the address of the PDE in the new process's
                    // page directory page.
                    //

#if defined (_X86PAE_)
                    //
                    // All four PAE page directory frames are mapped virtually
                    // contiguous and so the PpePdeOffset can (and must) be
                    // safely used here.
                    //
                    PpePdeOffset = MiGetPdeIndex(MiGetVirtualAddressMappedByPte(PointerPte));
#else
                    PpePdeOffset = MiGetPdeOffset(MiGetVirtualAddressMappedByPte(PointerPte));
#endif

                    PointerNewPde = &PdeBase[PpePdeOffset];

                    if (PointerNewPde->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a transition page.  This will
                        // become a page table page for the new process.
                        //

                        ReleasedWorkingSetMutex =
                                MiDoneWithThisPageGetAnother (&PageFrameIndex,
                                                              PointerPde,
                                                              CurrentProcess);

#if (_MI_PAGING_LEVELS >= 3)
                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageFrameIndex));
#endif
                        if (ReleasedWorkingSetMutex) {

                            do {

#if (_MI_PAGING_LEVELS >= 4)
                                MiDoesPxeExistAndMakeValid (PointerPxe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
                                Waited = 0;
#endif
                                MiDoesPpeExistAndMakeValid (PointerPpe,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
    
#if (_MI_PAGING_LEVELS < 4)
                                Waited = 0;
#endif
    
                                MiDoesPdeExistAndMakeValid (PointerPde,
                                                            CurrentProcess,
                                                            FALSE,
                                                            &Waited);
                            } while (Waited != 0);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

#if defined (_X86PAE_)
                        PdePhysicalPage = PageDirectoryFrames[MiGetPdPteOffset(MiGetVirtualAddressMappedByPte(PointerPte))];
                        PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
#endif

                        MiBuildForkPageTable (PageFrameIndex,
                                              PointerPde,
                                              PointerNewPde,
                                              PdePhysicalPage,
                                              PfnPdPage,
                                              FALSE);

#if (_MI_PAGING_LEVELS >= 3)
                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryEntries);
#endif

                        //
                        // Map the new page table page into the system portion
                        // of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        ASSERT (NewPteMappedAddress != NULL);

                        MiDownPfnReferenceCount (MdlPage, 1);

                        MdlPage = PageFrameIndex;

                        PointerNewPte = (PMMPTE)MiMapSinglePage (NewPteMappedAddress,
                                                                 MdlPage,
                                                                 MmCached,
                                                                 HighPagePriority);
                    
                        ASSERT (PointerNewPte != NULL);

                        MiUpPfnReferenceCount (MdlPage, 1);
                    }

                    //
                    // Calculate the address of the new PTE to build.
                    // Note that FirstTime could be true, yet the page
                    // table page already built.
                    //

                    PointerNewPte = (PMMPTE)((ULONG_PTR)PAGE_ALIGN(PointerNewPte) |
                                            BYTE_OFFSET (PointerPte));

#if (_MI_PAGING_LEVELS >= 3)
                    UsedPageTableEntries = (PVOID)MI_PFN_ELEMENT((PFN_NUMBER)PointerNewPde->u.Hard.PageFrameNumber);
#else
#if !defined (_X86PAE_)
                    UsedPageTableEntries = (PVOID)&HyperWsl->UsedPageTableEntries
                                                [MiGetPteOffset( PointerPte )];
#else
                    UsedPageTableEntries = (PVOID)&HyperWsl->UsedPageTableEntries
                                                [MiGetPdeIndex(MiGetVirtualAddressMappedByPte(PointerPte))];
#endif
#endif

                }

                //
                // Make the fork prototype PTE location resident.
                //

                if (PAGE_ALIGN (ForkProtoPte) != PAGE_ALIGN (LockedForkPte)) {
                    MiUnlockPagedAddress (LockedForkPte, FALSE);
                    LockedForkPte = ForkProtoPte;
                    MiLockPagedAddress (LockedForkPte, FALSE);
                }

                MiMakeSystemAddressValid (PointerPte, CurrentProcess);

                PteContents = *PointerPte;

                //
                // Check each PTE.
                //

                if (PteContents.u.Long == 0) {
                    NOTHING;

                }
                else if (PteContents.u.Hard.Valid == 1) {

                    //
                    // Valid.
                    //

                    Pfn2 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
                    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                    WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                                    MmWorkingSetList,
                                                    Pfn2->u1.WsIndex);

                    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

                    if (Pfn2->u3.e1.PrototypePte == 1) {

                        //
                        // This PTE is already in prototype PTE format.
                        //

                        //
                        // This is a prototype PTE.  The PFN database does
                        // not contain the contents of this PTE it contains
                        // the contents of the prototype PTE.  This PTE must
                        // be reconstructed to contain a pointer to the
                        // prototype PTE.
                        //
                        // The working set list entry contains information about
                        // how to reconstruct the PTE.
                        //

                        if (MmWsle[WorkingSetIndex].u1.e1.SameProtectAsProto
                                                                        == 0) {

                            //
                            // The protection for the prototype PTE is in the
                            // WSLE.
                            //

                            TempPte.u.Long = 0;
                            TempPte.u.Soft.Protection =
                                MI_GET_PROTECTION_FROM_WSLE(&MmWsle[WorkingSetIndex]);
                            TempPte.u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;

                        }
                        else {

                            //
                            // The protection is in the prototype PTE.
                            //

                            TempPte.u.Long = MiProtoAddressForPte (
                                                            Pfn2->PteAddress);
 //                            TempPte.u.Proto.ForkType =
 //                                        MmWsle[WorkingSetIndex].u1.e1.ForkType;
                        }

                        TempPte.u.Proto.Prototype = 1;
                        MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                        //
                        // A PTE is now non-zero, increment the used page
                        // table entries counter.
                        //

                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                        //
                        // Check to see if this is a fork prototype PTE,
                        // and if it is increment the reference count
                        // which is in the longword following the PTE.
                        //

                        if (MiLocateCloneAddress (CurrentProcess, (PVOID)Pfn2->PteAddress) !=
                                    NULL) {

                            //
                            // The reference count field, or the prototype PTE
                            // for that matter may not be in the working set.
                            //

                            CloneProto = (PMMCLONE_BLOCK)Pfn2->PteAddress;

                            ASSERT (CloneProto->CloneRefCount >= 1);
                            InterlockedIncrement (&CloneProto->CloneRefCount);

                            if (PAGE_ALIGN (ForkProtoPte) !=
                                                    PAGE_ALIGN (LockedForkPte)) {
                                MiUnlockPagedAddress (LockedForkPte, FALSE);
                                LockedForkPte = ForkProtoPte;
                                MiLockPagedAddress (LockedForkPte, FALSE);
                            }

                            MiMakeSystemAddressValid (PointerPte,
                                                      CurrentProcess);
                        }

                    }
                    else {

                        //
                        // This is a private page, create a fork prototype PTE
                        // which becomes the "prototype" PTE for this page.
                        // The protection is the same as that in the prototype
                        // PTE so the WSLE does not need to be updated.
                        //

                        MI_MAKE_VALID_PTE_WRITE_COPY (PointerPte);

                        KeFlushSingleTb (VirtualAddress,
                                         TRUE,
                                         FALSE,
                                         (PHARDWARE_PTE)PointerPte,
                                         PointerPte->u.Flush);

                        ForkProtoPte->ProtoPte = *PointerPte;
                        ForkProtoPte->CloneRefCount = 2;

                        //
                        // Transform the PFN element to reference this new fork
                        // prototype PTE.
                        //

                        Pfn2->PteAddress = &ForkProtoPte->ProtoPte;
                        Pfn2->u3.e1.PrototypePte = 1;

                        ContainingPte = MiGetPteAddress(&ForkProtoPte->ProtoPte);
                        if (ContainingPte->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                            if (!NT_SUCCESS(MiCheckPdeForPagedPool (&ForkProtoPte->ProtoPte))) {
#endif
                                KeBugCheckEx (MEMORY_MANAGEMENT,
                                              0x61940, 
                                              (ULONG_PTR)&ForkProtoPte->ProtoPte,
                                              (ULONG_PTR)ContainingPte->u.Long,
                                              (ULONG_PTR)MiGetVirtualAddressMappedByPte(&ForkProtoPte->ProtoPte));
#if (_MI_PAGING_LEVELS < 3)
                            }
#endif
                        }
                        Pfn2->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPte);


                        //
                        // Increment the share count for the page containing the
                        // fork prototype PTEs as we have just placed a valid
                        // PTE into the page.
                        //

                        PfnForkPtePage = MI_PFN_ELEMENT (
                                            ContainingPte->u.Hard.PageFrameNumber );

                        MiUpForkPageShareCount (PfnForkPtePage);

                        //
                        // Change the protection in the PFN database to COPY
                        // on write, if writable.
                        //

                        MI_MAKE_PROTECT_WRITE_COPY (Pfn2->OriginalPte);

                        //
                        // Put the protection into the WSLE and mark the WSLE
                        // to indicate that the protection field for the PTE
                        // is the same as the prototype PTE.
                        //

                        MmWsle[WorkingSetIndex].u1.e1.Protection =
                            MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn2->OriginalPte);

                        MmWsle[WorkingSetIndex].u1.e1.SameProtectAsProto = 1;

                        TempPte.u.Long = MiProtoAddressForPte (Pfn2->PteAddress);
                        TempPte.u.Proto.Prototype = 1;
                        MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                        //
                        // A PTE is now non-zero, increment the used page
                        // table entries counter.
                        //

                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                        //
                        // One less private page (it's now shared).
                        //

                        CurrentProcess->NumberOfPrivatePages -= 1;

                        ForkProtoPte += 1;
                        NumberOfForkPtes += 1;
                    }

                }
                else if (PteContents.u.Soft.Prototype == 1) {

                    //
                    // Prototype PTE, check to see if this is a fork
                    // prototype PTE already.  Note that if COW is set,
                    // the PTE can just be copied (fork compatible format).
                    //

                    MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                    //
                    // Check to see if this is a fork prototype PTE,
                    // and if it is increment the reference count
                    // which is in the longword following the PTE.
                    //

                    CloneProto = (PMMCLONE_BLOCK)(ULONG_PTR)MiPteToProto(PointerPte);

                    if (MiLocateCloneAddress (CurrentProcess, (PVOID)CloneProto) != NULL) {

                        //
                        // The reference count field, or the prototype PTE
                        // for that matter may not be in the working set.
                        //

                        ASSERT (CloneProto->CloneRefCount >= 1);
                        InterlockedIncrement (&CloneProto->CloneRefCount);

                        if (PAGE_ALIGN (ForkProtoPte) !=
                                                PAGE_ALIGN (LockedForkPte)) {
                            MiUnlockPagedAddress (LockedForkPte, FALSE);
                            LockedForkPte = ForkProtoPte;
                            MiLockPagedAddress (LockedForkPte, FALSE);
                        }

                        MiMakeSystemAddressValid (PointerPte, CurrentProcess);
                    }

                }
                else if (PteContents.u.Soft.Transition == 1) {

                    //
                    // Transition.
                    //

                    if (MiHandleForkTransitionPte (PointerPte,
                                                   PointerNewPte,
                                                   ForkProtoPte)) {
                        //
                        // PTE is no longer transition, try again.
                        //

                        continue;
                    }

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                    //
                    // One less private page (it's now shared).
                    //

                    CurrentProcess->NumberOfPrivatePages -= 1;

                    ForkProtoPte += 1;
                    NumberOfForkPtes += 1;

                }
                else {

                    //
                    // Page file format (may be demand zero).
                    //

                    if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {

                        if (PteContents.u.Soft.Protection == MM_DECOMMIT) {

                            //
                            // This is a decommitted PTE, just move it
                            // over to the new process.  Don't increment
                            // the count of private pages.
                            //

                            MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);
                        }
                        else {

                            //
                            // The PTE is not demand zero, move the PTE to
                            // a fork prototype PTE and make this PTE and
                            // the new processes PTE refer to the fork
                            // prototype PTE.
                            //

                            ForkProtoPte->ProtoPte = PteContents;

                            //
                            // Make the protection write-copy if writable.
                            //

                            MI_MAKE_PROTECT_WRITE_COPY (ForkProtoPte->ProtoPte);

                            ForkProtoPte->CloneRefCount = 2;

                            TempPte.u.Long =
                                 MiProtoAddressForPte (&ForkProtoPte->ProtoPte);

                            TempPte.u.Proto.Prototype = 1;

                            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                            MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                            //
                            // One less private page (it's now shared).
                            //

                            CurrentProcess->NumberOfPrivatePages -= 1;

                            ForkProtoPte += 1;
                            NumberOfForkPtes += 1;
                        }
                    }
                    else {

                        //
                        // The page is demand zero, make the new process's
                        // page demand zero.
                        //

                        MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);
                    }

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);
                }

                PointerPte += 1;
                PointerNewPte += 1;

            }  // end while for PTEs
AllDone:
            NewVad = NewVad->Parent;
        }
        Vad = MiGetNextVad (Vad);

    } // end while for VADs

    //
    // Unlock paged pool page.
    //

    MiUnlockPagedAddress (LockedForkPte, FALSE);

    //
    // Unmap the PD Page and hyper space page.
    //

#if (_MI_PAGING_LEVELS >= 4)
    MiUnmapSinglePage (PxeBase);
#endif

#if (_MI_PAGING_LEVELS >= 3)
    MiUnmapSinglePage (PpeBase);
#endif

#if !defined (_X86PAE_)
    MiUnmapSinglePage (PdeBase);
#else
    MmUnmapLockedPages (PdeBase, MdlPageDirectory);
#endif

#if (_MI_PAGING_LEVELS < 3)
    MiUnmapSinglePage (HyperBase);
#endif

    MiUnmapSinglePage (NewPteMappedAddress);

#if (_MI_PAGING_LEVELS >= 3)
    MiDownPfnReferenceCount (RootPhysicalPage, 1);
#endif

#if (_MI_PAGING_LEVELS >= 4)
    MiDownPfnReferenceCount (MdlDirParentPage, 1);
#endif

#if defined (_X86PAE_)
    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        MiDownPfnReferenceCount (PageDirectoryFrames[i], 1);
    }
#else
    MiDownPfnReferenceCount (MdlDirPage, 1);
#endif

#if (_MI_PAGING_LEVELS < 3)
    MiDownPfnReferenceCount (HyperPhysicalPage, 1);
#endif

    MiDownPfnReferenceCount (MdlPage, 1);

    //
    // Make the count of private pages match between the two processes.
    //

    ASSERT ((SPFN_NUMBER)CurrentProcess->NumberOfPrivatePages >= 0);

    ProcessToInitialize->NumberOfPrivatePages =
                                          CurrentProcess->NumberOfPrivatePages;

    ASSERT (NumberOfForkPtes <= CloneDescriptor->NumberOfPtes);

    if (NumberOfForkPtes != 0) {

        //
        // The number of fork PTEs is non-zero, set the values
        // into the structures.
        //

        CloneHeader->NumberOfPtes = NumberOfForkPtes;
        CloneDescriptor->NumberOfReferences = NumberOfForkPtes;
        CloneDescriptor->FinalNumberOfReferences = NumberOfForkPtes;
        CloneDescriptor->NumberOfPtes = NumberOfForkPtes;
    }
    else {

        //
        // There were no fork PTEs created.  Remove the clone descriptor
        // from this process and clean up the related structures.
        //

        MiRemoveClone (CurrentProcess, CloneDescriptor);

        UNLOCK_WS (CurrentProcess);

        ExFreePool (CloneDescriptor->CloneHeader->ClonePtes);

        ExFreePool (CloneDescriptor->CloneHeader);

        //
        // Return the pool for the global structures referenced by the
        // clone descriptor.
        //

        PsReturnProcessPagedPoolQuota (CurrentProcess,
                                       CloneDescriptor->PagedPoolQuotaCharge);

        PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_HEADER));
        ExFreePool (CloneDescriptor);

        LOCK_WS (CurrentProcess);
    }

    //
    // As we have updated many PTEs to clear dirty bits, flush the
    // TB cache.  Note that this was not done every time we changed
    // a valid PTE so other threads could be modifying the address
    // space without causing copy on writes. Too bad, because an app
    // that is not synchronizing itself is going to have coherency problems
    // anyway.  Note that this cannot cause any system page corruption because
    // the address space mutex was (and is) still held throughout and is
    // not released until after we flush the TB now.
    //

    MiDownShareCountFlushEntireTb (PageFrameIndex);

    PageFrameIndex = (PFN_NUMBER)-1;

    //
    // Copy the clone descriptors from this process to the new process.
    //

    Clone = MiGetFirstClone (CurrentProcess);
    CloneList = &FirstNewClone;
    CloneFailed = FALSE;

    while (Clone != NULL) {

        //
        // Increment the count of processes referencing this clone block.
        //

        ASSERT (Clone->CloneHeader->NumberOfProcessReferences >= 1);

        InterlockedIncrement (&Clone->CloneHeader->NumberOfProcessReferences);

        do {
            NewClone = ExAllocatePoolWithTag (NonPagedPool,
                                              sizeof( MMCLONE_DESCRIPTOR),
                                              'dCmM');

            if (NewClone != NULL) {
                break;
            }

            //
            // There are insufficient resources to continue this operation,
            // however, to properly clean up at this point, all the
            // clone headers must be allocated, so when the cloned process
            // is deleted, the clone headers will be found.  if the pool
            // is not readily available, loop periodically trying for it.
            // Force the clone operation to fail so the pool will soon be
            // released.
            //
            // Release the working set mutex so this process can be trimmed
            // and reacquire after the delay.
            //

            UNLOCK_WS (CurrentProcess);

            CloneFailed = TRUE;
            status = STATUS_INSUFFICIENT_RESOURCES;

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmShortTime);

            LOCK_WS (CurrentProcess);

        } while (TRUE);

        *NewClone = *Clone;

        //
        // Carefully update the FinalReferenceCount as this forking thread
        // may have begun while a faulting thread is waiting in
        // MiDecrementCloneBlockReference for the clone PTEs to inpage.
        // In this case, the ReferenceCount has been decremented but the
        // FinalReferenceCount hasn't been yet.  When the faulter awakes, he
        // will automatically take care of this process, but we must fix up
        // the child process now.  Otherwise the clone descriptor, clone header
        // and clone PTE pool allocations will leak and so will the charged
        // quota.
        //

        if (NewClone->FinalNumberOfReferences > NewClone->NumberOfReferences) {
            NewClone->FinalNumberOfReferences = NewClone->NumberOfReferences;
        }

        *CloneList = NewClone;

        CloneList = &NewClone->Parent;
        Clone = MiGetNextClone (Clone);
    }

    *CloneList = NULL;

#if defined (_MIALT4K_)

    if (CurrentProcess->Wow64Process != NULL) {

        //
        // Copy the alternate table entries now.
        //

        MiDuplicateAlternateTable (CurrentProcess, ProcessToInitialize);
    }

#endif

    //
    // Release the working set mutex and the address creation mutex from
    // the current process as all the necessary information is now
    // captured.
    //

    UNLOCK_WS (CurrentProcess);

    ASSERT (CurrentProcess->ForkInProgress == PsGetCurrentThread ());
    CurrentProcess->ForkInProgress = NULL;

    UNLOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // Attach to the process to initialize and insert the vad and clone
    // descriptors into the tree.
    //

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (PsGetCurrentProcess() != ProcessToInitialize) {
        Attached = TRUE;
        KeStackAttachProcess (&ProcessToInitialize->Pcb, &ApcState);
    }

    CurrentProcess = ProcessToInitialize;

    //
    // We are now in the context of the new process, build the
    // VAD list and the clone list.
    //

    Vad = FirstNewVad;
    VadInsertFailed = FALSE;

    LOCK_WS (CurrentProcess);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Update the WSLEs for the page directories that were added.
    //

    PointerPpe = MiGetPpeAddress (0);
    PointerPpeLast = MiGetPpeAddress (MM_HIGHEST_USER_ADDRESS);
    PointerPxe = MiGetPxeAddress (0);
    PpeInWsle = NULL;

    while (PointerPpe <= PointerPpeLast) {

#if (_MI_PAGING_LEVELS >= 4)
        while (PointerPxe->u.Long == 0) {
            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            continue;
        }

        //
        // Update the WSLE for this page directory parent page.
        //

        if (PointerPpe != PpeInWsle) {

            ASSERT (PointerPxe->u.Hard.Valid == 1);

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPxe);

            PfnPdPage = MI_PFN_ELEMENT (PageFrameIndex);
        
            ASSERT (PfnPdPage->u1.Event == 0);
        
            PfnPdPage->u1.Event = (PVOID)PsGetCurrentThread();
        
            MiAddValidPageToWorkingSet (PointerPpe,
                                        PointerPxe,
                                        PfnPdPage,
                                        0);
            PpeInWsle = PointerPpe;
        }
#endif

        if (PointerPpe->u.Long != 0) {

            ASSERT (PointerPpe->u.Hard.Valid == 1);

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPpe);

            PfnPdPage = MI_PFN_ELEMENT (PageFrameIndex);
        
            ASSERT (PfnPdPage->u1.Event == 0);
        
            PfnPdPage->u1.Event = (PVOID)PsGetCurrentThread();
        
            MiAddValidPageToWorkingSet (MiGetVirtualAddressMappedByPte (PointerPpe),
                                        PointerPpe,
                                        PfnPdPage,
                                        0);
        }

        PointerPpe += 1;
#if (_MI_PAGING_LEVELS >= 4)
        if (MiIsPteOnPdeBoundary (PointerPpe)) {
            PointerPxe += 1;
            ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
        }
#endif
    }

#endif

    while (Vad != NULL) {

        NextVad = Vad->Parent;

        if (VadInsertFailed) {
            Vad->u.VadFlags.CommitCharge = MM_MAX_COMMIT;
        }

        status = MiInsertVad (Vad);

        if (!NT_SUCCESS(status)) {

            //
            // Charging quota for the VAD failed, set the
            // remaining quota fields in this VAD and all
            // subsequent VADs to zero so the VADs can be
            // inserted and later deleted.
            //

            VadInsertFailed = TRUE;

            //
            // Do the loop again for this VAD.
            //

            continue;
        }

        //
        // Update the current virtual size.
        //

        CurrentProcess->VirtualSize += PAGE_SIZE +
                            ((Vad->EndingVpn - Vad->StartingVpn) >> PAGE_SHIFT);

        Vad = NextVad;
    }

    UNLOCK_WS (CurrentProcess);

    //
    // Update the peak virtual size.
    //

    CurrentProcess->PeakVirtualSize = CurrentProcess->VirtualSize;

    Clone = FirstNewClone;
    TotalPagedPoolCharge = 0;
    TotalNonPagedPoolCharge = 0;

    while (Clone != NULL) {

        NextClone = Clone->Parent;
        MiInsertClone (CurrentProcess, Clone);

        //
        // Calculate the paged pool and non-paged pool to charge for these
        // operations.
        //

        TotalPagedPoolCharge += Clone->PagedPoolQuotaCharge;
        TotalNonPagedPoolCharge += sizeof(MMCLONE_HEADER);

        Clone = NextClone;
    }

    if (CloneFailed || VadInsertFailed) {

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }

        return status;
    }

    status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                            TotalPagedPoolCharge);

    if (!NT_SUCCESS(status)) {

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
    }

    status = PsChargeProcessNonPagedPoolQuota (CurrentProcess,
                                               TotalNonPagedPoolCharge);

    if (!NT_SUCCESS(status)) {

        PsReturnProcessPagedPoolQuota (CurrentProcess, TotalPagedPoolCharge);

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
    }

    ASSERT ((ProcessToClone->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);
    ASSERT ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

#if DBG
    if (MmDebug & MM_DBG_FORK) {
        DbgPrint("ending clone operation process to clone = %p\n",
            ProcessToClone);
    }
#endif //DBG

    return STATUS_SUCCESS;

    //
    // Error returns.
    //

ErrorReturn4:
        if (PageTablePage == 2) {
            NOTHING;
        }
        else if (PageTablePage == 1) {
            PsReturnProcessPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_BLOCK) *
                                           NumberOfPrivatePages);
        }
        else {
            ASSERT (PageTablePage == 0);
            PsReturnProcessPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_BLOCK) *
                                           NumberOfPrivatePages);
            PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_HEADER));
        }

        NewVad = FirstNewVad;
        while (NewVad != NULL) {
            Vad = NewVad->Parent;
            ExFreePool (NewVad);
            NewVad = Vad;
        }

        ExFreePool (CloneDescriptor);
ErrorReturn3:
        ExFreePool (CloneHeader);
ErrorReturn2:
        ExFreePool (CloneProtos);
ErrorReturn1:
        UNLOCK_ADDRESS_SPACE (CurrentProcess);
        ASSERT ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);
        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
}

ULONG
MiDecrementCloneBlockReference (
    IN PMMCLONE_DESCRIPTOR CloneDescriptor,
    IN PMMCLONE_BLOCK CloneBlock,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine decrements the reference count field of a "fork prototype
    PTE" (clone-block).  If the reference count becomes zero, the reference
    count for the clone-descriptor is decremented and if that becomes zero,
    it is deallocated and the number of processes count for the clone header is
    decremented.  If the number of processes count becomes zero, the clone
    header is deallocated.

Arguments:

    CloneDescriptor - Supplies the clone descriptor which describes the
                      clone block.

    CloneBlock - Supplies the clone block to decrement the reference count of.

    CurrentProcess - Supplies the current process.

Return Value:

    TRUE if the working set mutex was released, FALSE if it was not.

Environment:

    Kernel mode, APCs disabled, address creation mutex, working set mutex
    and PFN lock held.

--*/

{
    PMMCLONE_HEADER CloneHeader;
    ULONG MutexReleased;
    MMPTE CloneContents;
    PMMPFN Pfn3;
    KIRQL OldIrql;
    LONG NewCount;
    LOGICAL WsHeldSafe;

    ASSERT (CurrentProcess == PsGetCurrentProcess ());

    MutexReleased = FALSE;
    OldIrql = APC_LEVEL;

    //
    // Note carefully : the clone descriptor count is decremented *BEFORE*
    // dereferencing the pagable clone PTEs.  This is because the working
    // set mutex is released and reacquired if the clone PTEs need to be made
    // resident for the dereference.  And this opens a window where a fork
    // could begin.  This thread will wait for the fork to finish, but the
    // fork will copy the clone descriptors (including this one) and get a
    // stale descriptor reference count (too high by one) as our decrement
    // will only occur in our descriptor and not the forked one.
    //
    // Decrementing the clone descriptor count *BEFORE* potentially
    // releasing the working set mutex solves this entire problem.
    //
    // Note that after the decrement, the clone descriptor can
    // only be referenced here if the count dropped to exactly zero.  (If it
    // was nonzero, some other thread may drive it to zero and free it in the
    // gap where we release locks to inpage the clone block).
    //

    CloneDescriptor->NumberOfReferences -= 1;

    ASSERT (CloneDescriptor->NumberOfReferences >= 0);

    if (CloneDescriptor->NumberOfReferences == 0) {

        //
        // There are no longer any PTEs in this process which refer
        // to the fork prototype PTEs for this clone descriptor.
        // Remove the CloneDescriptor now so a fork won't see it either.
        //

        MiRemoveClone (CurrentProcess, CloneDescriptor);
    }

    //
    // Now process the clone PTE block and any other descriptor cleanup that
    // may be needed.
    //

    MutexReleased = MiMakeSystemAddressValidPfnWs (CloneBlock, CurrentProcess);

    while (CurrentProcess->ForkInProgress != NULL) {
        MiWaitForForkToComplete (CurrentProcess, TRUE);
        MiMakeSystemAddressValidPfnWs (CloneBlock, CurrentProcess);
        MutexReleased = TRUE;
    }

    NewCount = InterlockedDecrement (&CloneBlock->CloneRefCount);

    ASSERT (NewCount >= 0);

    if (NewCount == 0) {

        CloneContents = CloneBlock->ProtoPte;

        if (CloneContents.u.Long != 0) {

            //
            // The last reference to a fork prototype PTE has been removed.
            // Deallocate any page file space and the transition page, if any.
            //

            ASSERT (CloneContents.u.Hard.Valid == 0);

            //
            // Assert that the PTE is not in subsection format (doesn't point
            // to a file).
            //

            ASSERT (CloneContents.u.Soft.Prototype == 0);

            if (CloneContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, put the page on the free list.
                //

                Pfn3 = MI_PFN_ELEMENT (CloneContents.u.Trans.PageFrameNumber);
                MI_SET_PFN_DELETED (Pfn3);

                MiDecrementShareCount (Pfn3->u4.PteFrame);

                //
                // Check the reference count for the page, if the reference
                // count is zero and the page is not on the freelist,
                // move the page to the free list, if the reference
                // count is not zero, ignore this page.
                // When the reference count goes to zero, it will be placed
                // on the free list.
                //

                if ((Pfn3->u3.e2.ReferenceCount == 0) &&
                    (Pfn3->u3.e1.PageLocation != FreePageList)) {

                    MiUnlinkPageFromList (Pfn3);
                    MiReleasePageFileSpace (Pfn3->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&CloneContents));
                }
            }
            else {

                if (IS_PTE_NOT_DEMAND_ZERO (CloneContents)) {
                    MiReleasePageFileSpace (CloneContents);
                }
            }
        }
    }

    //
    // Decrement the final number of references to the clone descriptor.  The
    // decrement of the NumberOfReferences above serves to decide
    // whether to remove the clone descriptor from the process tree so that
    // a wait on a paged out clone PTE block doesn't let a fork copy the
    // descriptor while it is half-changed.
    //
    // The FinalNumberOfReferences serves as a way to distinguish which
    // thread (multiple threads may have collided waiting for the inpage
    // of the clone PTE block) is the last one to wake up from the wait as
    // only this one (it may not be the same one who drove NumberOfReferences
    // to zero) can finally free the pool safely.
    //

    CloneDescriptor->FinalNumberOfReferences -= 1;

    ASSERT (CloneDescriptor->FinalNumberOfReferences >= 0);

    if (CloneDescriptor->FinalNumberOfReferences == 0) {

        UNLOCK_PFN (OldIrql);

        //
        // There are no longer any PTEs in this process which refer
        // to the fork prototype PTEs for this clone descriptor.
        // Decrement the process reference count in the CloneHeader.
        //

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Handle both cases here and below.
        //

        UNLOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

        MutexReleased = TRUE;

        CloneHeader = CloneDescriptor->CloneHeader;

        NewCount = InterlockedDecrement (&CloneHeader->NumberOfProcessReferences);
        ASSERT (NewCount >= 0);

        //
        // If the count is zero, there are no more processes pointing
        // to this fork header so blow it away.
        //

        if (NewCount == 0) {

#if DBG
            ULONG i;

            CloneBlock = CloneHeader->ClonePtes;
            for (i = 0; i < CloneHeader->NumberOfPtes; i += 1) {
                if (CloneBlock->CloneRefCount != 0) {
                    DbgBreakPoint ();
                }
                CloneBlock += 1;
            }
#endif

            ExFreePool (CloneHeader->ClonePtes);

            ExFreePool (CloneHeader);
        }

        //
        // Return the pool for the global structures referenced by the
        // clone descriptor.
        //

        if ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0) {

            //
            // Fork succeeded so return quota that was taken out earlier.
            //

            PsReturnProcessPagedPoolQuota (CurrentProcess,
                                           CloneDescriptor->PagedPoolQuotaCharge);

            PsReturnProcessNonPagedPoolQuota (CurrentProcess,
                                              sizeof(MMCLONE_HEADER));
        }

        ExFreePool (CloneDescriptor);

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Reacquire it in the same manner our caller did.
        //

        LOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

        LOCK_PFN (OldIrql);
    }

    return MutexReleased;
}

LOGICAL
MiWaitForForkToComplete (
    IN PEPROCESS CurrentProcess,
    IN LOGICAL PfnHeld
    )

/*++

Routine Description:

    This routine waits for the current process to complete a fork operation.

Arguments:

    CurrentProcess - Supplies the current process value.

    PfnHeld - Supplies TRUE if the PFN lock is held on entry.

Return Value:

    TRUE if locks were released and reacquired to wait.  FALSE if not.

Environment:

    Kernel mode, APCs disabled, working set mutex and PFN lock held.

--*/

{
    KIRQL OldIrql;
    LOGICAL WsHeldSafe;

    //
    // A fork operation is in progress and the count of clone-blocks
    // and other structures may not be changed.  Release the mutexes
    // and wait for the address creation mutex which governs the
    // fork operation.
    //

    if (CurrentProcess->ForkInProgress == PsGetCurrentThread()) {
        return FALSE;
    }

    if (PfnHeld == TRUE) {
        UNLOCK_PFN (APC_LEVEL);
    }

    //
    // The working set mutex may have been acquired safely or unsafely
    // by our caller.  Handle both cases here and below, carefully making sure
    // that the OldIrql left in the WS mutex on return is the same as on entry.
    //
    // Note it is ok to drop to PASSIVE or APC level here as long as it is
    // not lower than our caller was at.  Using the WorkingSetMutex whose irql
    // field was initialized by our caller ensures that the proper irql
    // environment is maintained (ie: the caller may be blocking APCs
    // deliberately).
    //

    UNLOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

    //
    // Acquire the address creation mutex as this can only succeed when the
    // forking thread is done in MiCloneProcessAddressSpace.  Thus, acquiring
    // this mutex doesn't stop another thread from starting another fork, but
    // it does serve as a way to know the current fork is done (enough).
    //

    LOCK_ADDRESS_SPACE (CurrentProcess);

    UNLOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // The working set lock may have been acquired safely or unsafely
    // by our caller.  Reacquire it in the same manner our caller did.
    //

    LOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

    //
    // Get the PFN lock again if our caller held it.
    //

    if (PfnHeld == TRUE) {
        LOCK_PFN (OldIrql);
    }

    return TRUE;
}

VOID
MiUpPfnReferenceCount (
    IN PFN_NUMBER Page,
    IN USHORT Count
    )

    // non paged helper routine.

{
    KIRQL OldIrql;
    PMMPFN Pfn1;

    Pfn1 = MI_PFN_ELEMENT (Page);

    LOCK_PFN (OldIrql);
    Pfn1->u3.e2.ReferenceCount = (USHORT)(Pfn1->u3.e2.ReferenceCount + Count);
    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiDownPfnReferenceCount (
    IN PFN_NUMBER Page,
    IN USHORT Count
    )

    // non paged helper routine.

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    while (Count != 0) {
        MiDecrementReferenceCount (Page);
        Count -= 1;
    }
    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiUpControlAreaRefs (
    IN PMMVAD Vad
    )
{
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;

    ControlArea = Vad->ControlArea;

    LOCK_PFN (OldIrql);

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfUserReferences += 1;

    if ((ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL) &&
        (ControlArea->u.Flags.PhysicalMemory == 0)) {

        FirstSubsection = MiLocateSubsection (Vad, Vad->StartingVpn);

        //
        // Note LastSubsection may be NULL for extendable VADs when
        // the EndingVpn is past the end of the section.  In this
        // case, all the subsections can be safely incremented.
        //
        // Note also that the reference must succeed because each
        // subsection's prototype PTEs are guaranteed to already 
        // exist by virtue of the fact that the creating process
        // already has this VAD currently mapping them.
        //

        LastSubsection = MiLocateSubsection (Vad, Vad->EndingVpn);

        while (FirstSubsection != LastSubsection) {
            MiReferenceSubsection ((PMSUBSECTION) FirstSubsection);
            FirstSubsection = FirstSubsection->NextSubsection;
        }

        if (LastSubsection != NULL) {
            MiReferenceSubsection ((PMSUBSECTION) LastSubsection);
        }
    }

    UNLOCK_PFN (OldIrql);
    return;
}


ULONG
MiDoneWithThisPageGetAnother (
    IN PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    )

{
    KIRQL OldIrql;
    ULONG ReleasedMutex;

    UNREFERENCED_PARAMETER (PointerPde);

    LOCK_PFN (OldIrql);

    if (*PageFrameIndex != (PFN_NUMBER)-1) {

        //
        // Decrement the share count of the last page which
        // we operated on.
        //

        MiDecrementShareCountOnly (*PageFrameIndex);
    }

    ReleasedMutex = MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    *PageFrameIndex = MiRemoveZeroPage (
                   MI_PAGE_COLOR_PTE_PROCESS (PointerPde,
                                              &CurrentProcess->NextPageColor));

    UNLOCK_PFN (OldIrql);
    return ReleasedMutex;
}

ULONG
MiLeaveThisPageGetAnother (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    )

{
    KIRQL OldIrql;
    ULONG ReleasedMutex;

    UNREFERENCED_PARAMETER (PointerPde);

    LOCK_PFN (OldIrql);

    ReleasedMutex = MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    *PageFrameIndex = MiRemoveZeroPage (
                   MI_PAGE_COLOR_PTE_PROCESS (PointerPde,
                                              &CurrentProcess->NextPageColor));

    UNLOCK_PFN (OldIrql);
    return ReleasedMutex;
}

ULONG
MiHandleForkTransitionPte (
    IN PMMPTE PointerPte,
    IN PMMPTE PointerNewPte,
    IN PMMCLONE_BLOCK ForkProtoPte
    )

{
    KIRQL OldIrql;
    PMMPFN Pfn2;
    MMPTE PteContents;
    PMMPTE ContainingPte;
    PFN_NUMBER PageTablePage;
    MMPTE TempPte;
    PMMPFN PfnForkPtePage;

    LOCK_PFN (OldIrql);

    //
    // Now that we have the PFN lock which prevents pages from
    // leaving the transition state, examine the PTE again to
    // ensure that it is still transition.
    //

    PteContents = *PointerPte;

    if ((PteContents.u.Soft.Transition == 0) ||
        (PteContents.u.Soft.Prototype == 1)) {

        //
        // The PTE is no longer in transition... do this loop again.
        //

        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    //
    // The PTE is still in transition, handle like a
    // valid PTE.
    //

    Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

    //
    // Assertion that PTE is not in prototype PTE format.
    //

    ASSERT (Pfn2->u3.e1.PrototypePte != 1);

    //
    // This is a private page in transition state,
    // create a fork prototype PTE
    // which becomes the "prototype" PTE for this page.
    //

    ForkProtoPte->ProtoPte = PteContents;

    //
    // Make the protection write-copy if writable.
    //

    MI_MAKE_PROTECT_WRITE_COPY (ForkProtoPte->ProtoPte);

    ForkProtoPte->CloneRefCount = 2;

    //
    // Transform the PFN element to reference this new fork
    // prototype PTE.
    //

    //
    // Decrement the share count for the page table
    // page which contains the PTE as it is no longer
    // valid or in transition.
    //

    Pfn2->PteAddress = &ForkProtoPte->ProtoPte;
    Pfn2->u3.e1.PrototypePte = 1;

    //
    // Make original PTE copy on write.
    //

    MI_MAKE_PROTECT_WRITE_COPY (Pfn2->OriginalPte);

    ContainingPte = MiGetPteAddress(&ForkProtoPte->ProtoPte);

    if (ContainingPte->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (&ForkProtoPte->ProtoPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940, 
                          (ULONG_PTR)&ForkProtoPte->ProtoPte,
                          (ULONG_PTR)ContainingPte->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(&ForkProtoPte->ProtoPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PageTablePage = Pfn2->u4.PteFrame;

    Pfn2->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPte);

    //
    // Increment the share count for the page containing
    // the fork prototype PTEs as we have just placed
    // a transition PTE into the page.
    //

    PfnForkPtePage = MI_PFN_ELEMENT (ContainingPte->u.Hard.PageFrameNumber);

    PfnForkPtePage->u2.ShareCount += 1;

    TempPte.u.Long = MiProtoAddressForPte (Pfn2->PteAddress);
    TempPte.u.Proto.Prototype = 1;
    MI_WRITE_INVALID_PTE (PointerPte, TempPte);
    MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

    //
    // Decrement the share count for the page table
    // page which contains the PTE as it is no longer
    // valid or in transition.
    //

    MiDecrementShareCount (PageTablePage);

    UNLOCK_PFN (OldIrql);
    return FALSE;
}

VOID
MiDownShareCountFlushEntireTb (
    IN PFN_NUMBER PageFrameIndex
    )

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    if (PageFrameIndex != (PFN_NUMBER)-1) {

        //
        // Decrement the share count of the last page which
        // we operated on.
        //

        MiDecrementShareCountOnly (PageFrameIndex);
    }

    KeFlushEntireTb (FALSE, FALSE);

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiUpForkPageShareCount (
    IN PMMPFN PfnForkPtePage
    )
{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    PfnForkPtePage->u2.ShareCount += 1;

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiBuildForkPageTable (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PMMPTE PointerNewPde,
    IN PFN_NUMBER PdePhysicalPage,
    IN PMMPFN PfnPdPage,
    IN LOGICAL MakeValid
    )
{
    KIRQL OldIrql;
    PMMPFN Pfn1;
#if (_MI_PAGING_LEVELS >= 3)
    MMPTE TempPpe;
#endif

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // The PFN lock must be held while initializing the
    // frame to prevent those scanning the database for free
    // frames from taking it after we fill in the u2 field.
    //

    LOCK_PFN (OldIrql);

    Pfn1->OriginalPte = DemandZeroPde;
    Pfn1->u2.ShareCount = 1;
    Pfn1->u3.e2.ReferenceCount = 1;
    Pfn1->PteAddress = PointerPde;
    MI_SET_MODIFIED (Pfn1, 1, 0x10);
    Pfn1->u3.e1.PageLocation = ActiveAndValid;
    Pfn1->u3.e1.CacheAttribute = MiCached;
    Pfn1->u4.PteFrame = PdePhysicalPage;

    //
    // Increment the share count for the page containing
    // this PTE as the PTE is in transition.
    //

    PfnPdPage->u2.ShareCount += 1;

    UNLOCK_PFN (OldIrql);

    if (MakeValid == TRUE) {

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Put the PPE into the valid state as it will point at a page
        // directory page that is valid (not transition) when the fork is
        // complete.  All the page table pages will be in transition, but
        // the page directories cannot be as they contain the PTEs for the
        // page tables.
        //
    
        TempPpe = ValidPdePde;

        MI_MAKE_VALID_PTE (TempPpe,
                           PageFrameIndex,
                           MM_READWRITE,
                           PointerPde);
    
        MI_SET_PTE_DIRTY (TempPpe);

        //
        // Make the PTE owned by user mode.
        //
    
        MI_SET_OWNER_IN_PTE (PointerNewPde, UserMode);

        MI_WRITE_VALID_PTE (PointerNewPde, TempPpe);
#endif
    }
    else {

        //
        // Put the PDE into the transition state as it is not
        // really mapped and decrement share count does not
        // put private pages into transition, only prototypes.
        //
    
        MI_WRITE_INVALID_PTE (PointerNewPde, TransitionPde);

        //
        // Make the PTE owned by user mode.
        //
    
        MI_SET_OWNER_IN_PTE (PointerNewPde, UserMode);

        PointerNewPde->u.Trans.PageFrameNumber = PageFrameIndex;
    }
}

#if defined (_X86PAE_)
VOID
MiRetrievePageDirectoryFrames (
    IN PFN_NUMBER RootPhysicalPage,
    OUT PPFN_NUMBER PageDirectoryFrames
    )
{
    ULONG i;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();

    PointerPte = (PMMPTE)MiMapPageInHyperSpace (Process, RootPhysicalPage, &OldIrql);

    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        PageDirectoryFrames[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        PointerPte += 1;
    }

    MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);

    return;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\lockvm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   lockvm.c

Abstract:

    This module contains the routines which implement the
    NtLockVirtualMemory service.

Author:

    Lou Perazzoli (loup) 20-August-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtLockVirtualMemory)
#pragma alloc_text(PAGE,NtUnlockVirtualMemory)
#endif


NTSTATUS
NtLockVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG MapType
     )

/*++

Routine Description:

    This function locks a region of pages within the working set list
    of a subject process.

    The caller of this function must have PROCESS_VM_OPERATION access
    to the target process.  The caller must also have SeLockMemoryPrivilege.

Arguments:

   ProcessHandle - Supplies an open handle to a process object.

   BaseAddress - The base address of the region of pages
                 to be locked. This value is rounded down to the
                 next host page address boundary.

   RegionSize - A pointer to a variable that will receive
                the actual size in bytes of the locked region of
                pages. The initial value of this argument is
                rounded up to the next host page size boundary.

   MapType - A set of flags that describe the type of locking to
             perform.  One of MAP_PROCESS or MAP_SYSTEM.

Return Value:

    NTSTATUS.

    STATUS_PRIVILEGE_NOT_HELD - The caller did not have sufficient
                                privilege to perform the requested operation.

--*/

{
    PVOID Va;
    PVOID StartingVa;
    PVOID EndingAddress;
    KAPC_STATE ApcState;
    PMMPTE PointerPte;
    PMMPTE PointerPte1;
    PMMPFN Pfn1;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG_PTR CapturedRegionSize;
    PVOID CapturedBase;
    PEPROCESS TargetProcess;
    NTSTATUS Status;
    LOGICAL WasLocked;
    KPROCESSOR_MODE PreviousMode;
    WSLE_NUMBER Entry;
    WSLE_NUMBER SwapEntry;
    SIZE_T NumberOfAlreadyLocked;
    SIZE_T NumberToLock;
    WSLE_NUMBER WorkingSetIndex;
    PMMVAD Vad;
    PVOID LastVa;
    ULONG Waited;
    LOGICAL Attached;
    PETHREAD Thread;
#if defined(_MIALT4K_)
    PVOID Wow64Process;
#endif

    PAGED_CODE();

    WasLocked = FALSE;
    LastVa = NULL;

    //
    // Validate the flags in MapType.
    //

    if ((MapType & ~(MAP_PROCESS | MAP_SYSTEM)) != 0) {
        return STATUS_INVALID_PARAMETER;
    }

    if ((MapType & (MAP_PROCESS | MAP_SYSTEM)) == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    Thread = PsGetCurrentThread ();

    PreviousMode = KeGetPreviousModeByThread(&Thread->Tcb);

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer ((PULONG)BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER;

    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    //
    // Reference the specified process.
    //

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&TargetProcess,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    if ((MapType & MAP_SYSTEM) != 0) {

        //
        // In addition to PROCESS_VM_OPERATION access to the target
        // process, the caller must have SE_LOCK_MEMORY_PRIVILEGE.
        //

        if (!SeSinglePrivilegeCheck(
                           SeLockMemoryPrivilege,
                           PreviousMode
                           )) {

            ObDereferenceObject( TargetProcess );
            return( STATUS_PRIVILEGE_NOT_HELD );
        }
    }

    //
    // Attach to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    //
    // Get address creation mutex, this prevents the
    // address range from being modified while it is examined.  Raise
    // to APC level to prevent an APC routine from acquiring the
    // address creation mutex.  Get the working set mutex so the
    // number of already locked pages in the request can be determined.
    //

#if defined(_MIALT4K_)

    //
    // Changing to 4k aligned should not change the correctness.
    //

    EndingAddress = PAGE_4K_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);
#else
    EndingAddress = PAGE_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);
#endif

    Va = PAGE_ALIGN (CapturedBase);
    NumberOfAlreadyLocked = 0;
    NumberToLock = ((ULONG_PTR)EndingAddress - (ULONG_PTR)Va) >> PAGE_SHIFT;

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    if (NumberToLock + MM_FLUID_WORKING_SET >
                                    TargetProcess->Vm.MinimumWorkingSetSize) {
        Status = STATUS_WORKING_SET_QUOTA;
        goto ErrorReturn1;
    }

    //
    // Note the working set mutex must be held throughout the loop below to
    // prevent other threads from locking or unlocking WSL entries.
    //

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        if (Va > LastVa) {

            //
            // Don't lock physically mapped views.
            //

            Vad = MiLocateAddress (Va);

            if (Vad == NULL) {
                Status = STATUS_ACCESS_VIOLATION;
                goto ErrorReturn;
            }

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Status = STATUS_INCOMPATIBLE_FILE_MAP;
                goto ErrorReturn;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

        if (MmIsAddressValid (Va)) {

            //
            // The page is valid, therefore it is in the working set.
            // Locate the WSLE for the page and see if it is locked.
            //

            PointerPte1 = MiGetPteAddress (Va);
            Pfn1 = MI_PFN_ELEMENT (PointerPte1->u.Hard.PageFrameNumber);

            WorkingSetIndex = MiLocateWsle (Va,
                                            MmWorkingSetList,
                                            Pfn1->u1.WsIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            if (WorkingSetIndex < MmWorkingSetList->FirstDynamic) {

                //
                // This page is locked in the working set.
                //

                NumberOfAlreadyLocked += 1;

                //
                // Check to see if the WAS_LOCKED status should be returned.
                //

                if ((MapType & MAP_PROCESS) &&
                        (MmWsle[WorkingSetIndex].u1.e1.LockedInWs == 1)) {
                    WasLocked = TRUE;
                }

                if ((MapType & MAP_SYSTEM) &&
                        (MmWsle[WorkingSetIndex].u1.e1.LockedInMemory == 1)) {
                    WasLocked = TRUE;
                }
            }
        }
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_UNSAFE (TargetProcess);

    //
    // Check to ensure the working set list is still fluid after
    // the requested number of pages are locked.
    //

    if (TargetProcess->Vm.MinimumWorkingSetSize <
          ((MmWorkingSetList->FirstDynamic + NumberToLock +
                      MM_FLUID_WORKING_SET) - NumberOfAlreadyLocked)) {

        Status = STATUS_WORKING_SET_QUOTA;
        goto ErrorReturn1;
    }

    Va = PAGE_ALIGN (CapturedBase);

#if defined(_MIALT4K_)

    Wow64Process = TargetProcess->Wow64Process;

    if (Wow64Process != NULL) {
        Va = PAGE_4K_ALIGN (CapturedBase);
    }

#endif

    //
    // Set up an exception handler and touch each page in the specified
    // range.  Mark this thread as the address space mutex owner so it cannot
    // sneak its stack in as the argument region and trick us into trying to
    // grow it if the reference faults (as this would cause a deadlock since
    // this thread already owns the address space mutex).  Note this would have
    // the side effect of not allowing this thread to fault on guard pages in
    // other data regions while the accesses below are ongoing - but that could
    // only happen in an APC and those are blocked right now anyway.
    //

    ASSERT (KeGetCurrentIrql () == APC_LEVEL);
    ASSERT (Thread->AddressSpaceOwner == 0);
    Thread->AddressSpaceOwner = 1;

    try {

        while (Va <= EndingAddress) {
            *(volatile ULONG *)Va;
            Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
        ASSERT (KeGetCurrentIrql () == APC_LEVEL);
        ASSERT (Thread->AddressSpaceOwner == 1);
        Thread->AddressSpaceOwner = 0;
        goto ErrorReturn1;
    }

    ASSERT (KeGetCurrentIrql () == APC_LEVEL);
    ASSERT (Thread->AddressSpaceOwner == 1);
    Thread->AddressSpaceOwner = 0;

    //
    // The complete address range is accessible, lock the pages into
    // the working set.
    //

    PointerPte = MiGetPteAddress (CapturedBase);
    Va = PAGE_ALIGN (CapturedBase);

#if defined(_MIALT4K_)

    if (Wow64Process != NULL) {
        Va = PAGE_4K_ALIGN (CapturedBase);
    }

#endif

    StartingVa = Va;

    //
    // Acquire the working set mutex, no page faults are allowed.
    //

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        //
        // Make sure the PDE is valid.
        //

        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);

        do {

            MiDoesPxeExistAndMakeValid (PointerPxe,
                                        TargetProcess,
                                        FALSE,
                                        &Waited);

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            MiDoesPpeExistAndMakeValid (PointerPpe,
                                        TargetProcess,
                                        FALSE,
                                        &Waited);

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            MiDoesPdeExistAndMakeValid (PointerPde,
                                        TargetProcess,
                                        FALSE,
                                        &Waited);

        } while (Waited != 0);

        //
        // Make sure the page is in the working set.
        //

        while (PointerPte->u.Hard.Valid == 0) {

            //
            // Release the working set mutex and fault in the page.
            //

            UNLOCK_WS_UNSAFE (TargetProcess);

            //
            // Page in the PDE and make the PTE valid.
            //

            try {
                *(volatile ULONG *)Va;
            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Since all the pages were accessed above with the address
                // space mutex held and it is still held now, the only way
                // an exception could occur would be due to a device error,
                // ie: hardware malfunction, net cable disconnection, CD
                // being removed, etc.
                //
                // Recompute EndingAddress so the actual number of pages locked
                // is written back to the user now.  If this is the very first
                // page then return a failure status.
                //

                EndingAddress = PAGE_ALIGN (Va);

#if defined(_MIALT4K_)
                if (Wow64Process != NULL) {
                    EndingAddress = PAGE_4K_ALIGN (Va);
                }
#endif

                if (EndingAddress == StartingVa) {
                    Status = GetExceptionCode ();
                    goto ErrorReturn1;
                }

                ASSERT (NT_SUCCESS (Status));
                EndingAddress = (PVOID)((ULONG_PTR)EndingAddress - 1);
#if defined(_MIALT4K_)
                if (Wow64Process != NULL) {
                    CapturedRegionSize = (ULONG_PTR)EndingAddress - (ULONG_PTR)CapturedBase;
                }
#endif
                goto SuccessReturn1;
            }

            //
            // Reacquire the working set mutex.
            //

            LOCK_WS_UNSAFE (TargetProcess);

            //
            // Make sure the page directory & table pages are still valid.
            // Trimming could occur if either of the pages that were just
            // made valid were removed from the working set before the
            // working set lock was acquired.
            //

            do {

                MiDoesPxeExistAndMakeValid (PointerPxe,
                                            TargetProcess,
                                            FALSE,
                                            &Waited);

#if (_MI_PAGING_LEVELS >= 4)
                Waited = 0;
#endif

                MiDoesPpeExistAndMakeValid (PointerPpe,
                                            TargetProcess,
                                            FALSE,
                                            &Waited);

#if (_MI_PAGING_LEVELS < 4)
                Waited = 0;
#endif

                MiDoesPdeExistAndMakeValid (PointerPde,
                                            TargetProcess,
                                            FALSE,
                                            &Waited);
            } while (Waited != 0);
        }

        //
        // The page is now in the working set, lock the page into
        // the working set.
        //

        PointerPte1 = MiGetPteAddress (Va);
        Pfn1 = MI_PFN_ELEMENT (PointerPte1->u.Hard.PageFrameNumber);

        Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);

        if (Entry >= MmWorkingSetList->FirstDynamic) {

            SwapEntry = MmWorkingSetList->FirstDynamic;

            if (Entry != MmWorkingSetList->FirstDynamic) {

                //
                // Swap this entry with the one at first dynamic.
                //

                MiSwapWslEntries (Entry, SwapEntry, &TargetProcess->Vm);
            }

            MmWorkingSetList->FirstDynamic += 1;
        } else {
            SwapEntry = Entry;
        }

        //
        // Indicate that the page is locked.
        //

        if (MapType & MAP_PROCESS) {
            MmWsle[SwapEntry].u1.e1.LockedInWs = 1;
        }

        if (MapType & MAP_SYSTEM) {
            MmWsle[SwapEntry].u1.e1.LockedInMemory = 1;
        }

        //
        // Increment to the next Va and PTE.
        //

        PointerPte += 1;
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_UNSAFE (TargetProcess);

SuccessReturn1:

#if (defined(_MIALT4K_))

    if (Wow64Process != NULL) {
        MiLockFor4kPage (CapturedBase, CapturedRegionSize, TargetProcess);
    }

#endif

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    ObDereferenceObject (TargetProcess);

    //
    // Update return arguments.
    //

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) { 

            *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_4K_ALIGN(CapturedBase)) + PAGE_4K;

            *BaseAddress = PAGE_4K_ALIGN(CapturedBase);


        } else {    

#endif
        *RegionSize = ((PCHAR)EndingAddress - (PCHAR)PAGE_ALIGN(CapturedBase)) +
                                                                    PAGE_SIZE;
        *BaseAddress = PAGE_ALIGN(CapturedBase);

#if defined(_MIALT4K_)
        }
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    if (WasLocked) {
        return STATUS_WAS_LOCKED;
    }

    return STATUS_SUCCESS;

ErrorReturn:
        UNLOCK_WS_UNSAFE (TargetProcess);
ErrorReturn1:
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        ObDereferenceObject (TargetProcess);
        return Status;
}

NTSTATUS
NtUnlockVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG MapType
    )

/*++

Routine Description:

    This function unlocks a region of pages within the working set list
    of a subject process.

    As a side effect, any pages which are not locked and are in the
    process's working set are removed from the process's working set.
    This allows NtUnlockVirtualMemory to remove a range of pages
    from the working set.

    The caller of this function must have PROCESS_VM_OPERATION access
    to the target process.

    The caller must also have SeLockMemoryPrivilege for MAP_SYSTEM.

Arguments:

   ProcessHandle - Supplies an open handle to a process object.

   BaseAddress - The base address of the region of pages
                 to be unlocked. This value is rounded down to the
                 next host page address boundary.

   RegionSize - A pointer to a variable that will receive
                the actual size in bytes of the unlocked region of
                pages. The initial value of this argument is
                rounded up to the next host page size boundary.

   MapType - A set of flags that describe the type of unlocking to
             perform.  One of MAP_PROCESS or MAP_SYSTEM.

Return Value:

    NTSTATUS.

--*/

{
    PVOID Va;
    PVOID EndingAddress;
    SIZE_T CapturedRegionSize;
    PVOID CapturedBase;
    PEPROCESS TargetProcess;
    NTSTATUS Status;
    KPROCESSOR_MODE PreviousMode;
    WSLE_NUMBER Entry;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMVAD Vad;
    PVOID LastVa;
    LOGICAL Attached;
    KAPC_STATE ApcState;
#if defined(_MIALT4K_)
    PVOID Wow64Process;
#endif

    PAGED_CODE();

    LastVa = NULL;

    //
    // Validate the flags in MapType.
    //

    if ((MapType & ~(MAP_PROCESS | MAP_SYSTEM)) != 0) {
        return STATUS_INVALID_PARAMETER;
    }

    if ((MapType & (MAP_PROCESS | MAP_SYSTEM)) == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    PreviousMode = KeGetPreviousMode();

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER;

    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&TargetProcess,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

#if defined(_MIALT4K_)
    Wow64Process = TargetProcess->Wow64Process;
#endif

    if ((MapType & MAP_SYSTEM) != 0) {

        //
        // In addition to PROCESS_VM_OPERATION access to the target
        // process, the caller must have SE_LOCK_MEMORY_PRIVILEGE.
        //

        if (!SeSinglePrivilegeCheck(
                           SeLockMemoryPrivilege,
                           PreviousMode
                           )) {

            ObDereferenceObject( TargetProcess );
            return STATUS_PRIVILEGE_NOT_HELD;
        }
    }

    //
    // Attach to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    EndingAddress = PAGE_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);

    Va = PAGE_ALIGN (CapturedBase);

    //
    // Get address creation mutex, this prevents the
    // address range from being modified while it is examined.
    // Block APCs so an APC routine can't get a page fault and
    // corrupt the working set list, etc.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        //
        // Check to ensure all the specified pages are locked.
        //

        if (Va > LastVa) {
            Vad = MiLocateAddress (Va);
            if (Vad == NULL) {
                Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
                Status = STATUS_NOT_LOCKED;
                break;
            }

            //
            // Don't unlock physically mapped views.
            //

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Va = MI_VPN_TO_VA (Vad->EndingVpn);
                break;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

        if (!MmIsAddressValid (Va)) {

            //
            // This page is not valid, therefore not in working set.
            //

            Status = STATUS_NOT_LOCKED;
        }
        else {

            PointerPte = MiGetPteAddress (Va);
            ASSERT (PointerPte->u.Hard.Valid != 0);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);
            ASSERT (Entry != WSLE_NULL_INDEX);

            if ((MmWsle[Entry].u1.e1.LockedInWs == 0) &&
                (MmWsle[Entry].u1.e1.LockedInMemory == 0)) {

                //
                // Not locked in memory or system, remove from working
                // set.
                //

                PERFINFO_PAGE_INFO_DECL();

                PERFINFO_GET_PAGE_INFO(PointerPte);

                if (MiFreeWsle (Entry, &TargetProcess->Vm, PointerPte)) {
                    PERFINFO_LOG_WS_REMOVAL(PERFINFO_LOG_TYPE_OUTWS_EMPTYQ, &TargetProcess->Vm);
                }

                Status = STATUS_NOT_LOCKED;

            } else if (MapType & MAP_PROCESS) {
                if (MmWsle[Entry].u1.e1.LockedInWs == 0)  {

                    //
                    // This page is not locked.
                    //

                    Status = STATUS_NOT_LOCKED;
                }
            } else {
                if (MmWsle[Entry].u1.e1.LockedInMemory == 0)  {

                    //
                    // This page is not locked.
                    //

                    Status = STATUS_NOT_LOCKED;
                }
            }
        }
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

#if defined(_MIALT4K_)

    if (Wow64Process != NULL) {

        //
        // This call may release and reacquire the working set mutex !!!
        //
        // Therefore the loop following must handle PTEs which have been
        // trimmed during this window.
        //

        Status = MiUnlockFor4kPage (CapturedBase,
                                    CapturedRegionSize,
                                    TargetProcess);
    }

#endif

    if (Status == STATUS_NOT_LOCKED) {
        goto ErrorReturn;
    }

    //
    // The complete address range is locked, unlock them.
    //

    Va = PAGE_ALIGN (CapturedBase);
    LastVa = NULL;

    while (Va <= EndingAddress) {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) {

            //
            // This call may release and reacquire the working set mutex !!!
            //
            // Therefore the code below must handle PTEs which have been
            // trimmed during this window.
            //

            if (!MiShouldBeUnlockedFor4kPage(Va, TargetProcess)) {

                //
                // The other 4k pages in the native page still hold
                // the page lock.  Should skip unlocking.
                //

                Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
                continue;
            }
        }

#endif
        //
        // Don't unlock physically mapped views.
        //

        if (Va > LastVa) {
            Vad = MiLocateAddress (Va);
            ASSERT (Vad != NULL);

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Va = MI_VPN_TO_VA (Vad->EndingVpn);
                break;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

#if defined(_MIALT4K_)
        if (!MmIsAddressValid (Va)) {

            //
            // The page or any mapping table page may have been trimmed when
            // MiUnlockFor4kPage or MiShouldBeUnlockedFor4kPage released the
            // working set mutex.  If this has occurred, then clearly the
            // address is no longer locked so just skip it.
            //

            Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
            continue;
        }
#endif

        PointerPte = MiGetPteAddress (Va);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
        Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);

        if (MapType & MAP_PROCESS) {
            MmWsle[Entry].u1.e1.LockedInWs = 0;
        }

        if (MapType & MAP_SYSTEM) {
            MmWsle[Entry].u1.e1.LockedInMemory = 0;
        }

        if ((MmWsle[Entry].u1.e1.LockedInMemory == 0) &&
             MmWsle[Entry].u1.e1.LockedInWs == 0) {

            //
            // The page is no longer should be locked, move
            // it to the dynamic part of the working set.
            //

            MmWorkingSetList->FirstDynamic -= 1;

            if (Entry != MmWorkingSetList->FirstDynamic) {

                //
                // Swap this element with the last locked page, making
                // this element the new first dynamic entry.
                //

                MiSwapWslEntries (Entry,
                                  MmWorkingSetList->FirstDynamic,
                                  &TargetProcess->Vm);
            }
        }

        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_AND_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    ObDereferenceObject (TargetProcess);

    //
    // Update return arguments.
    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) { 

            *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_4K_ALIGN(CapturedBase)) + PAGE_4K;

            *BaseAddress = PAGE_4K_ALIGN(CapturedBase);


        } else {    

#endif
        *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_ALIGN(CapturedBase)) + PAGE_SIZE;

        *BaseAddress = PAGE_ALIGN(CapturedBase);

#if defined(_MIALT4K_)
        }
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    return STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_WS_UNSAFE (TargetProcess);

ErrorReturn1:

    UNLOCK_ADDRESS_SPACE (TargetProcess);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        ObDereferenceObject (TargetProcess);
        return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mapview.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mapview.c

Abstract:

    This module contains the routines which implement the
    NtMapViewOfSection service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"
#if defined(_WIN64)
#include <wow64t.h>
#endif

#ifdef LARGE_PAGES
const ULONG MMPPTE_NAME = 'tPmM';
#endif
const ULONG MMDB = 'bDmM';
extern const ULONG MMVADKEY;

extern ULONG MmAllocationPreference;

#if DBG
#define MI_BP_BADMAPS() TRUE
#else
ULONG MiStopBadMaps;
#define MI_BP_BADMAPS() (MiStopBadMaps & 0x1)
#endif

NTSTATUS
MiSetPageModified (
    IN PMMVAD Vad,
    IN PVOID Address
    );

extern LIST_ENTRY MmLoadedUserImageList;

ULONG   MiSubsectionsConvertedToDynamic;

#define X256MEG (256*1024*1024)

#if DBG
extern PEPROCESS MmWatchProcess;
#endif // DBG

#define ROUND_TO_PAGES64(Size)  (((UINT64)(Size) + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))

MMSESSION   MmSession;

NTSTATUS
MiMapViewOfImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T ImageCommitment
    );

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    );

VOID
MiRemoveMappedPtes (
    IN PVOID BaseAddress,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea,
    IN PMMSUPPORT WorkingSetInfo
    );

NTSTATUS
MiMapViewInSystemSpace (
    IN PVOID Section,
    IN PMMSESSION Session,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    );

NTSTATUS
MiUnmapViewInSystemSpace (
    IN PMMSESSION Session,
    IN PVOID MappedBase
    );

VOID
MiFillSystemPageDirectory (
    PVOID Base,
    SIZE_T NumberOfBytes
    );

VOID
MiLoadUserSymbols (
    IN PCONTROL_AREA ControlArea,
    IN PVOID StartingAddress,
    IN PEPROCESS Process
    );

#if DBG
VOID
VadTreeWalk (
    VOID
    );

VOID
MiDumpConflictingVad(
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad
    );
#endif //DBG


ULONG
CacheImageSymbols(
    IN PVOID ImageBase
    );

PVOID
MiInsertInSystemSpace (
    IN PMMSESSION Session,
    IN ULONG SizeIn64k,
    IN PCONTROL_AREA ControlArea
    );

ULONG
MiRemoveFromSystemSpace (
    IN PMMSESSION Session,
    IN PVOID Base,
    OUT PCONTROL_AREA *ControlArea
    );

VOID
MiInsertPhysicalViewAndRefControlArea (
    IN PEPROCESS Process,
    IN PCONTROL_AREA ControlArea,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtMapViewOfSection)
#pragma alloc_text(PAGE,MmMapViewOfSection)
#pragma alloc_text(PAGE,MmSecureVirtualMemory)
#pragma alloc_text(PAGE,MiSecureVirtualMemory)
#pragma alloc_text(PAGE,MmUnsecureVirtualMemory)
#pragma alloc_text(PAGE,MiUnsecureVirtualMemory)
#pragma alloc_text(PAGE,CacheImageSymbols)
#pragma alloc_text(PAGE,NtAreMappedFilesTheSame)
#pragma alloc_text(PAGE,MiLoadUserSymbols)
#pragma alloc_text(PAGE,MiMapViewOfImageSection)
#pragma alloc_text(PAGE,MiMapViewOfDataSection)
#pragma alloc_text(PAGE,MiMapViewOfPhysicalSection)
#pragma alloc_text(PAGE,MiInsertInSystemSpace)
#pragma alloc_text(PAGE,MmMapViewInSystemSpace)
#pragma alloc_text(PAGE,MmMapViewInSessionSpace)
#pragma alloc_text(PAGE,MiUnmapViewInSystemSpace)
#pragma alloc_text(PAGE,MmUnmapViewInSystemSpace)
#pragma alloc_text(PAGE,MmUnmapViewInSessionSpace)
#pragma alloc_text(PAGE,MiMapViewInSystemSpace)
#pragma alloc_text(PAGE,MiRemoveFromSystemSpace)
#pragma alloc_text(PAGE,MiInitializeSystemSpaceMap)
#pragma alloc_text(PAGE, MiFreeSessionSpaceMap)

#pragma alloc_text(PAGELK,MiInsertPhysicalViewAndRefControlArea)
#if DBG
#pragma alloc_text(PAGE,MiDumpConflictingVad)
#endif
#endif


NTSTATUS
NtMapViewOfSection (
    IN HANDLE SectionHandle,
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T CommitSize,
    IN OUT PLARGE_INTEGER SectionOffset OPTIONAL,
    IN OUT PSIZE_T ViewSize,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG AllocationType,
    IN ULONG Protect
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not null, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  null, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and
                  tiled).
        
    ZeroBits - Supplies the number of high order address bits that
               must be zero in the base address of the section
               view. The value of this argument must be less than
               or equal to the maximum number of zero bits and is only
               used when memory management determines where to allocate
               the view (i.e. when BaseAddress is null).
        
               If ZeroBits is zero, then no zero bit constraints are applied.

               If ZeroBits is greater than 0 and less than 32, then it is
               the number of leading zero bits from bit 31.  Bits 63:32 are
               also required to be zero.  This retains compatibility
               with 32-bit systems.
                
               If ZeroBits is greater than 32, then it is considered as
               a mask and the number of leading zeroes are counted out
               in the mask.  This then becomes the zero bits argument.

    CommitSize - Supplies the size of the initially committed region
                 of the view in bytes. This value is rounded up to
                 the next host page size boundary.

    SectionOffset - Supplies the offset from the beginning of the
                    section to the view in bytes. This value is
                    rounded down to the next host page size boundary.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view. If the value
               of this argument is zero, then a view of the
               section will be mapped starting at the specified
               section offset and continuing to the end of the
               section. Otherwise the initial value of this
               argument specifies the size of the view in bytes
               and is rounded up to the next host page size
               boundary.
        
    InheritDisposition - Supplies a value that specifies how the
                         view is to be shared by a child process created
                         with a create process operation.

        InheritDisposition Values

         ViewShare - Inherit view and share a single copy
              of the committed pages with a child process
              using the current protection value.

         ViewUnmap - Do not map the view into a child process.

    AllocationType - Supplies the type of allocation.

         MEM_TOP_DOWN
         MEM_DOS_LIM
         MEM_LARGE_PAGES
         MEM_RESERVE - for file mapped sections only.

    Protect - Supplies the protection desired for the region of
              initially committed pages.

        Protect Values


         PAGE_NOACCESS - No access to the committed region
              of pages is allowed. An attempt to read,
              write, or execute the committed region
              results in an access violation (i.e. a GP
              fault).

         PAGE_EXECUTE - Execute access to the committed
              region of pages is allowed. An attempt to
              read or write the committed region results in
              an access violation.

         PAGE_READONLY - Read only and execute access to the
              committed region of pages is allowed. An
              attempt to write the committed region results
              in an access violation.

         PAGE_READWRITE - Read, write, and execute access to
              the region of committed pages is allowed. If
              write access to the underlying section is
              allowed, then a single copy of the pages are
              shared. Otherwise the pages are shared read
              only/copy on write.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PSECTION Section;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    PVOID CapturedBase;
    SIZE_T CapturedViewSize;
    LARGE_INTEGER TempViewSize;
    LARGE_INTEGER CapturedOffset;
    ULONGLONG HighestPhysicalAddressInPfnDatabase;
    ACCESS_MASK DesiredSectionAccess;
    ULONG ProtectMaskForAccess;
    LOGICAL WriteCombined;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check the zero bits argument for correctness.
    //

#if defined (_WIN64)

    if (ZeroBits >= 32) {

        //
        // ZeroBits is a mask instead of a count.  Translate it to a count now.
        //

        ZeroBits = 64 - RtlFindMostSignificantBit (ZeroBits) - 1;
    }
    else if (ZeroBits) {
        ZeroBits += 32;
    }

#endif

    if (ZeroBits > MM_MAXIMUM_ZERO_BITS) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // Check the inherit disposition flags.
    //

    if ((InheritDisposition > ViewUnmap) ||
        (InheritDisposition < ViewShare)) {
        return STATUS_INVALID_PARAMETER_8;
    }

    //
    // Check the allocation type field.
    //

#ifdef i386

    //
    // Only allow DOS_LIM support for i386.  The MEM_DOS_LIM flag allows
    // map views of data sections to be done on 4k boundaries rather
    // than 64k boundaries.
    //

    if ((AllocationType & ~(MEM_TOP_DOWN | MEM_LARGE_PAGES | MEM_DOS_LIM |
           SEC_NO_CHANGE | MEM_RESERVE)) != 0) {
        return STATUS_INVALID_PARAMETER_9;
    }
#else
    if ((AllocationType & ~(MEM_TOP_DOWN | MEM_LARGE_PAGES |
           SEC_NO_CHANGE | MEM_RESERVE)) != 0) {
        return STATUS_INVALID_PARAMETER_9;
    }

#endif //i386

    //
    // Check the protection field.
    //

    if (Protect & PAGE_WRITECOMBINE) {
        Protect &= ~PAGE_WRITECOMBINE;
        WriteCombined = TRUE;
    }
    else {
        WriteCombined = FALSE;
    }

    ProtectMaskForAccess = MiMakeProtectionMask (Protect);
    if (ProtectMaskForAccess == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ProtectMaskForAccess = ProtectMaskForAccess & 0x7;

    DesiredSectionAccess = MmMakeSectionAccess[ProtectMaskForAccess];

    CurrentThread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {
        if (PreviousMode != KernelMode) {
            ProbeForWritePointer ((PULONG)BaseAddress);
            ProbeForWriteUlong_ptr (ViewSize);

        }

        if (ARGUMENT_PRESENT (SectionOffset)) {
            if (PreviousMode != KernelMode) {
                ProbeForWriteSmallStructure (SectionOffset,
                                             sizeof(LARGE_INTEGER),
                                             PROBE_ALIGNMENT (LARGE_INTEGER));
            }
            CapturedOffset = *SectionOffset;
        }
        else {
            ZERO_LARGE (CapturedOffset);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedViewSize = *ViewSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_VAD_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedViewSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;

    }

    if (((ULONG_PTR)CapturedBase + CapturedViewSize) > ((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {

        //
        // Desired Base and zero_bits conflict.
        //

        return STATUS_INVALID_PARAMETER_4;
    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );
    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // Reference the section object, if a view is mapped to the section
    // object, the object is not dereferenced as the virtual address
    // descriptor contains a pointer to the section object.
    //

    Status = ObReferenceObjectByHandle ( SectionHandle,
                                         DesiredSectionAccess,
                                         MmSectionObjectType,
                                         PreviousMode,
                                         (PVOID *)&Section,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        goto ErrorReturn1;
    }

    if (Section->u.Flags.Image == 0) {

        //
        // This is not an image section, make sure the section page
        // protection is compatible with the specified page protection.
        //

        if (!MiIsProtectionCompatible (Section->InitialPageProtection,
                                       Protect)) {
            Status = STATUS_SECTION_PROTECTION;
            goto ErrorReturn;
        }
    }

    //
    // Check to see if this the section backs physical memory, if
    // so DON'T align the offset on a 64K boundary, just a 4k boundary.
    //

    if (Section->Segment->ControlArea->u.Flags.PhysicalMemory) {
        HighestPhysicalAddressInPfnDatabase = (ULONGLONG)MmHighestPhysicalPage << PAGE_SHIFT;
        CapturedOffset.LowPart = CapturedOffset.LowPart & ~(PAGE_SIZE - 1);

        //
        // No usermode mappings past the end of the PFN database are allowed.
        // Address wrap is checked in the common path.
        //

        if (PreviousMode != KernelMode) {

            if ((ULONGLONG)(CapturedOffset.QuadPart + CapturedViewSize) > HighestPhysicalAddressInPfnDatabase) {
                Status = STATUS_INVALID_PARAMETER_6;
                goto ErrorReturn;
            }
        }

    }
    else {

        //
        // Make sure alignments are correct for specified address
        // and offset into the file.
        //

        if ((AllocationType & MEM_DOS_LIM) == 0) {
            if (((ULONG_PTR)CapturedBase & (X64K - 1)) != 0) {
                Status = STATUS_MAPPED_ALIGNMENT;
                goto ErrorReturn;
            }

            if ((ARGUMENT_PRESENT (SectionOffset)) &&
                ((CapturedOffset.LowPart & (X64K - 1)) != 0)) {
                Status = STATUS_MAPPED_ALIGNMENT;
                goto ErrorReturn;
            }
        }
    }

    //
    // Check to make sure the view size plus the offset is less
    // than the size of the section.
    //

    if ((ULONGLONG) (CapturedOffset.QuadPart + CapturedViewSize) <
        (ULONGLONG)CapturedOffset.QuadPart) {

        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    if (((ULONGLONG) (CapturedOffset.QuadPart + CapturedViewSize) >
                 (ULONGLONG)Section->SizeOfSection.QuadPart) &&
        ((AllocationType & MEM_RESERVE) == 0)) {

        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    if (CapturedViewSize == 0) {

        //
        // Set the view size to be size of the section less the offset.
        //

        TempViewSize.QuadPart = Section->SizeOfSection.QuadPart -
                                                CapturedOffset.QuadPart;

        CapturedViewSize = (SIZE_T)TempViewSize.QuadPart;

        if (

#if !defined(_WIN64)

            (TempViewSize.HighPart != 0) ||

#endif

            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedViewSize)) {

            //
            // Invalid region size;
            //

            Status = STATUS_INVALID_VIEW_SIZE;
            goto ErrorReturn;
        }
    }

    //
    // Check commit size.
    //

    if ((CommitSize > CapturedViewSize) &&
        ((AllocationType & MEM_RESERVE) == 0)) {
        Status = STATUS_INVALID_PARAMETER_5;
        goto ErrorReturn;
    }

    if (WriteCombined == TRUE) {
        Protect |= PAGE_WRITECOMBINE;
    }

    Status = MmMapViewOfSection ((PVOID)Section,
                                 Process,
                                 &CapturedBase,
                                 ZeroBits,
                                 CommitSize,
                                 &CapturedOffset,
                                 &CapturedViewSize,
                                 InheritDisposition,
                                 AllocationType,
                                 Protect);

    if (!NT_SUCCESS(Status) ) {

        if ((Status == STATUS_CONFLICTING_ADDRESSES) &&
            (Section->Segment->ControlArea->u.Flags.Image) &&
            (Process == CurrentProcess)) {

            DbgkMapViewOfSection (Section,
                                  CapturedBase,
                                  CapturedOffset.LowPart,
                                  CapturedViewSize);
        }
        goto ErrorReturn;
    }

    //
    // Any time the current process maps an image file,
    // a potential debug event occurs. DbgkMapViewOfSection
    // handles these events.
    //

    if ((Section->Segment->ControlArea->u.Flags.Image) &&
        (Process == CurrentProcess)) {

        if (Status != STATUS_IMAGE_NOT_AT_BASE) {
            DbgkMapViewOfSection (Section,
                                  CapturedBase,
                                  CapturedOffset.LowPart,
                                  CapturedViewSize);
        }
    }

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *ViewSize = CapturedViewSize;
        *BaseAddress = CapturedBase;

        if (ARGUMENT_PRESENT(SectionOffset)) {
            *SectionOffset = CapturedOffset;
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        goto ErrorReturn;
    }

#if 0 // test code...
    if ((Status == STATUS_SUCCESS) &&
        (Section->u.Flags.Image == 0)) {

        PVOID Base;
        ULONG Size = 0;
        NTSTATUS Status;

        Status = MmMapViewInSystemSpace ((PVOID)Section,
                                        &Base,
                                        &Size);
        if (Status == STATUS_SUCCESS) {
            MmUnmapViewInSystemSpace (Base);
        }
    }
#endif //0

    {
ErrorReturn:
        ObDereferenceObject (Section);
ErrorReturn1:
        ObDereferenceObject (Process);
        return Status;
    }
}

NTSTATUS
MmMapViewOfSection(
    IN PVOID SectionToMap,
    IN PEPROCESS Process,
    IN OUT PVOID *CapturedBase,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T CommitSize,
    IN OUT PLARGE_INTEGER SectionOffset,
    IN OUT PSIZE_T CapturedViewSize,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG AllocationType,
    IN ULONG Protect
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.

    This function is a kernel mode interface to allow LPC to map
    a section given the section pointer to map.

    This routine assumes all arguments have been probed and captured.

    ********************************************************************
    ********************************************************************
    ********************************************************************

    NOTE:

    CapturedViewSize, SectionOffset, and CapturedBase must be
    captured in non-paged system space (i.e., kernel stack).

    ********************************************************************
    ********************************************************************
    ********************************************************************

Arguments:

    SectionToMap - Supplies a pointer to the section object.

    Process - Supplies a pointer to the process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not NULL, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  NULL, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and
                  tiled).

    ZeroBits - Supplies the number of high order address bits that
               must be zero in the base address of the section
               view. The value of this argument must be less than
               21 and is only used when the operating system
               determines where to allocate the view (i.e. when
               BaseAddress is NULL).

    CommitSize - Supplies the size of the initially committed region
                 of the view in bytes. This value is rounded up to
                 the next host page size boundary.

    SectionOffset - Supplies the offset from the beginning of the
                    section to the view in bytes. This value is
                    rounded down to the next host page size boundary.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view. If the value
               of this argument is zero, then a view of the
               section will be mapped starting at the specified
               section offset and continuing to the end of the
               section. Otherwise the initial value of this
               argument specifies the size of the view in bytes
               and is rounded up to the next host page size boundary.

    InheritDisposition - Supplies a value that specifies how the
                         view is to be shared by a child process created
                         with a create process operation.

    AllocationType - Supplies the type of allocation.

    Protect - Supplies the protection desired for the region of
              initially committed pages.

Return Value:

    NTSTATUS.

--*/
{
    KAPC_STATE ApcState;
    LOGICAL Attached;
    PSECTION Section;
    PCONTROL_AREA ControlArea;
    ULONG ProtectionMask;
    NTSTATUS status;
    LOGICAL WriteCombined;
    SIZE_T ImageCommitment;
    ULONG ExecutePermission;

    PAGED_CODE();

    Attached = FALSE;

    Section = (PSECTION)SectionToMap;

    //
    // Check to make sure the section is not smaller than the view size.
    //

    if ((LONGLONG)*CapturedViewSize > Section->SizeOfSection.QuadPart) {
        if ((AllocationType & MEM_RESERVE) == 0) {
            return STATUS_INVALID_VIEW_SIZE;
        }
    }

    if (AllocationType & MEM_RESERVE) {
        if (((Section->InitialPageProtection & PAGE_READWRITE) |
            (Section->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

            return STATUS_SECTION_PROTECTION;
        }
    }

    if (Section->u.Flags.NoCache) {
        Protect |= PAGE_NOCACHE;
    }

    //
    // Note that write combining is only relevant to physical memory sections
    // because they are never trimmed - the write combining bits in a PTE entry
    // are not preserved across trims.
    //

    if (Protect & PAGE_WRITECOMBINE) {
        Protect &= ~PAGE_WRITECOMBINE;
        WriteCombined = TRUE;
    }
    else {
        WriteCombined = FALSE;
    }

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (Protect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ControlArea = Section->Segment->ControlArea;

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (PsGetCurrentProcess() != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // Get the address creation mutex to block multiple threads
    // creating or deleting address space at the same time.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }
    
    //
    // Map the view base on the type.
    //

    if (ControlArea->u.Flags.PhysicalMemory) {

        status = MiMapViewOfPhysicalSection (ControlArea,
                                             Process,
                                             CapturedBase,
                                             SectionOffset,
                                             CapturedViewSize,
                                             ProtectionMask,
                                             ZeroBits,
                                             AllocationType,
                                             WriteCombined);

    }
    else if (ControlArea->u.Flags.Image) {
        if (AllocationType & MEM_RESERVE) {
            status = STATUS_INVALID_PARAMETER_9;
        }
        else if (WriteCombined == TRUE) {
            status = STATUS_INVALID_PARAMETER_10;
        }
        else {

            ImageCommitment = Section->Segment->u1.ImageCommitment;

            status = MiMapViewOfImageSection (ControlArea,
                                              Process,
                                              CapturedBase,
                                              SectionOffset,
                                              CapturedViewSize,
                                              Section,
                                              InheritDisposition,
                                              ZeroBits,
                                              ImageCommitment);
        }

    }
    else {

        //
        // Not an image section, therefore it is a data section.
        //

        if (WriteCombined == TRUE) {
            status = STATUS_INVALID_PARAMETER_10;
        }
        else {
            
            //
            // Add execute permission if necessary.
            //

#if defined (_WIN64)
            if (Process->Wow64Process == NULL && Process->Peb != NULL)
#elif defined (_X86PAE_)
            if (Process->Peb != NULL)
#else
            if (FALSE)
#endif
            {

                ExecutePermission = 0;

                try {
                    ExecutePermission = Process->Peb->ExecuteOptions & MEM_EXECUTE_OPTION_DATA;
                } except (EXCEPTION_EXECUTE_HANDLER) {
                    status = GetExceptionCode();
                    goto ErrorReturn;
                }

                if (ExecutePermission != 0) {

                    switch (Protect & 0xF) {
                        case PAGE_READONLY:
                            Protect &= ~PAGE_READONLY;
                            Protect |= PAGE_EXECUTE_READ;
                            break;
                        case PAGE_READWRITE:
                            Protect &= ~PAGE_READWRITE;
                            Protect |= PAGE_EXECUTE_READWRITE;
                            break;
                        case PAGE_WRITECOPY:
                            Protect &= ~PAGE_WRITECOPY;
                            Protect |= PAGE_EXECUTE_WRITECOPY;
                            break;
                        default:
                            break;
                    }

                    //
                    // Recheck protection.
                    //

                    ProtectionMask = MiMakeProtectionMask (Protect);

                    if (ProtectionMask == MM_INVALID_PROTECTION) {
                        status = STATUS_INVALID_PAGE_PROTECTION;
                        goto ErrorReturn;
                    }
                }
            }

            status = MiMapViewOfDataSection (ControlArea,
                                             Process,
                                             CapturedBase,
                                             SectionOffset,
                                             CapturedViewSize,
                                             Section,
                                             InheritDisposition,
                                             ProtectionMask,
                                             CommitSize,
                                             ZeroBits,
                                             AllocationType);
        }
    }

ErrorReturn:
    UNLOCK_ADDRESS_SPACE (Process);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    return status;
}

VOID
MiInsertPhysicalViewAndRefControlArea (
    IN PEPROCESS Process,
    IN PCONTROL_AREA ControlArea,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This is a nonpaged helper routine to insert a physical view and reference
    the control area accordingly.

Arguments:

    Process - Supplies a pointer to the process.

    ControlArea - Supplies a pointer to the control area.

    PhysicalView - Supplies a pointer to the physical view.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    //

    LOCK_PFN (OldIrql);

    ASSERT (PhysicalView->Vad->u.VadFlags.PhysicalMapping == 1);
    PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD);

    InsertHeadList (&Process->PhysicalVadList, &PhysicalView->ListEntry);

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfUserReferences += 1;

    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);
}

//
// Nonpaged wrapper.
//
LOGICAL
MiCheckCacheAttributes (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )
{
    ULONG Hint;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PFN_NUMBER BadFrameStart;
    PFN_NUMBER BadFrameEnd;
    LOGICAL AttemptedMapOfUnownedFrame;
#if DBG
#define BACKTRACE_LENGTH 8
    ULONG i;
    ULONG Hash;
    PVOID StackTrace [BACKTRACE_LENGTH];
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PUNICODE_STRING BadDriverName;
#endif

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    Hint = 0;
    BadFrameStart = 0;
    BadFrameEnd = 0;
    AttemptedMapOfUnownedFrame = FALSE;

    LOCK_PFN (OldIrql);

    do {
        if (MiIsPhysicalMemoryAddress (PageFrameIndex, &Hint, FALSE) == TRUE) {
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            switch (Pfn1->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiNotMapped:
                    if (AttemptedMapOfUnownedFrame == FALSE) {
                        BadFrameStart = PageFrameIndex;
                        AttemptedMapOfUnownedFrame = TRUE;
                    }
                    BadFrameEnd = PageFrameIndex;
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }
        PageFrameIndex += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);

    if (AttemptedMapOfUnownedFrame == TRUE) {

#if DBG

        BadDriverName = NULL;

        RtlZeroMemory (StackTrace, sizeof (StackTrace));

        RtlCaptureStackBackTrace (1, BACKTRACE_LENGTH, StackTrace, &Hash);

        for (i = 0; i < BACKTRACE_LENGTH; i += 1) {

            if (StackTrace[i] <= MM_HIGHEST_USER_ADDRESS) {
                break;
            }

            DataTableEntry = MiLookupDataTableEntry (StackTrace[i], FALSE);

            if ((DataTableEntry != NULL) && ((PVOID)DataTableEntry != (PVOID)PsLoadedModuleList.Flink)) {
                //
                // Found the bad caller.
                //

                BadDriverName = &DataTableEntry->FullDllName;
            }
        }

        if (BadDriverName != NULL) {
            DbgPrint ("*******************************************************************************\n"
                  "*\n"
                  "* %wZ is mapping physical memory %p->%p\n"
                  "* that it does not own.  This can cause internal CPU corruption.\n"
                  "*\n"
                  "*******************************************************************************\n",
                BadDriverName,
                BadFrameStart << PAGE_SHIFT,
                (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
        }
        else {
            DbgPrint ("*******************************************************************************\n"
                  "*\n"
                  "* A driver is mapping physical memory %p->%p\n"
                  "* that it does not own.  This can cause internal CPU corruption.\n"
                  "*\n"
                  "*******************************************************************************\n",
                BadFrameStart << PAGE_SHIFT,
                (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
        }

#else
        DbgPrint ("*******************************************************************************\n"
              "*\n"
              "* A driver is mapping physical memory %p->%p\n"
              "* that it does not own.  This can cause internal CPU corruption.\n"
              "* A checked build will stop in the kernel debugger\n"
              "* so this problem can be fully debugged.\n"
              "*\n"
              "*******************************************************************************\n",
            BadFrameStart << PAGE_SHIFT,
            (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
#endif

        if (MI_BP_BADMAPS()) {
            DbgBreakPoint ();
        }
    }

    return TRUE;
}

NTSTATUS
MiMapViewOfPhysicalSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN ULONG ProtectionMask,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType,
    IN LOGICAL WriteCombined
    )

/*++

Routine Description:

    This routine maps the specified physical section into the
    specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

    ProtectionMask - Supplies the initial page protection-mask.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD_LONG Vad;
    ULONG Hint;
    CSHORT IoMapping;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    PMMPFN Pfn2;
    SIZE_T PhysicalViewSize;
    ULONG_PTR Alignment;
    PVOID UsedPageTableHandle;
    PMI_PHYSICAL_VIEW PhysicalView;
    NTSTATUS Status;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER StartingPageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MEMORY_CACHING_TYPE CacheType;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
#ifdef LARGE_PAGES
    ULONG size;
    PMMPTE protoPte;
    ULONG pageSize;
    PSUBSECTION Subsection;
    ULONG EmPageSize;
#endif //LARGE_PAGES

    //
    // Physical memory section.
    //

    //
    // If running on an R4000 and MEM_LARGE_PAGES is specified,
    // set up the PTEs as a series of pointers to the same
    // prototype PTE.  This prototype PTE will reference a subsection
    // that indicates large pages should be used.
    //
    // The R4000 supports pages of 4k, 16k, 64k, etc (powers of 4).
    // Since the TB supports 2 entries, sizes of 8k, 32k etc can
    // be mapped by 2 LargePages in a single TB entry.  These 2 entries
    // are maintained in the subsection structure pointed to by the
    // prototype PTE.
    //

    if (AllocationType & MEM_RESERVE) {
        return STATUS_INVALID_PARAMETER_9;
    }
    Alignment = X64K;

#ifdef LARGE_PAGES
    if (AllocationType & MEM_LARGE_PAGES) {

        //
        // Determine the page size and the required alignment.
        //

        if ((SectionOffset->LowPart & (X64K - 1)) != 0) {
            return STATUS_INVALID_PARAMETER_9;
        }

        size = (*CapturedViewSize - 1) >> (PAGE_SHIFT + 1);
        pageSize = PAGE_SIZE;

        while (size != 0) {
            size = size >> 2;
            pageSize = pageSize << 2;
        }

        Alignment = pageSize << 1;
        if (Alignment < MM_VA_MAPPED_BY_PDE) {
            Alignment = MM_VA_MAPPED_BY_PDE;
        }

#if defined(_IA64_)

        //
        // Convert pageSize to the EM page-size field format.
        //

        EmPageSize = 0;
        size = pageSize - 1 ;

        while (size) {
            size = size >> 1;
            EmPageSize += 1;
        }

        if (*CapturedViewSize > pageSize) {

            if (MmPageSizeInfo & (pageSize << 1)) {

                //
                // if larger page size is supported in the implementation
                //

                pageSize = pageSize << 1;
                EmPageSize += 1;

            }
            else {

                EmPageSize = EmPageSize | pageSize;

            }
        }

        pageSize = EmPageSize;
#endif

    }
#endif //LARGE_PAGES

    if (*CapturedBase == NULL) {

        //
        // Attempt to locate address space starting on a 64k boundary.
        //

#ifdef i386
        ASSERT (SectionOffset->HighPart == 0);
#endif

#ifdef LARGE_PAGES
        if (AllocationType & MEM_LARGE_PAGES) {
            PhysicalViewSize = Alignment;
        }
        else {
#endif

            PhysicalViewSize = *CapturedViewSize +
                                   (SectionOffset->LowPart & (X64K - 1));
#ifdef LARGE_PAGES
        }
#endif

        Status = MiFindEmptyAddressRange (PhysicalViewSize,
                                          Alignment,
                                          (ULONG)ZeroBits,
                                          &StartingAddress);

        if (!NT_SUCCESS (Status)) {
            return Status;
        }

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                PhysicalViewSize - 1L) | (PAGE_SIZE - 1L));
        StartingAddress = (PVOID)((ULONG_PTR)StartingAddress +
                                     (SectionOffset->LowPart & (X64K - 1)));

        if (ZeroBits > 0) {
            if (EndingAddress > (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {
                return STATUS_NO_MEMORY;
            }
        }

    }
    else {

        //
        // Check to make sure the specified base address to ending address
        // is currently unused.
        //

        StartingAddress = (PVOID)((ULONG_PTR)MI_64K_ALIGN(*CapturedBase) +
                                    (SectionOffset->LowPart & (X64K - 1)));
        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

#ifdef LARGE_PAGES
        if (AllocationType & MEM_LARGE_PAGES) {
            if (((ULONG_PTR)StartingAddress & (Alignment - 1)) != 0) {
                return STATUS_CONFLICTING_ADDRESSES;
            }
            EndingAddress = (PVOID)((PCHAR)StartingAddress + Alignment);
        }
#endif

        if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
            return STATUS_CONFLICTING_ADDRESSES;
        }
    }

    //
    // If a noncachable mapping is requested, none of the physical pages in the
    // range can reside in a large page.  Otherwise we would be creating an
    // incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    StartingPageFrameIndex = (PFN_NUMBER) (SectionOffset->QuadPart >> PAGE_SHIFT);

    CacheType = MmCached;

    if (WriteCombined == TRUE) {
        CacheType = MmWriteCombined;
    }
    else if (ProtectionMask & MM_NOCACHE) {
        CacheType = MmNonCached;
    }

    IoMapping = 1;
    Hint = 0;

    if (MiIsPhysicalMemoryAddress (StartingPageFrameIndex, &Hint, TRUE) == TRUE) {
        IoMapping = 0;
    }

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    NumberOfPages = MiGetPteAddress (EndingAddress) - MiGetPteAddress (StartingAddress) + 1;

    if (CacheAttribute != MiCached) {
        PageFrameIndex = StartingPageFrameIndex;
        while (PageFrameIndex < StartingPageFrameIndex + NumberOfPages) {
            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                MiNonCachedCollisions += 1;
                return STATUS_CONFLICTING_ADDRESSES;
            }
            PageFrameIndex += 1;
        }
    }

    //
    // An unoccupied address range has been found, build the virtual
    // address descriptor to describe this range.
    //

#ifdef LARGE_PAGES
    if (AllocationType & MEM_LARGE_PAGES) {
        //
        // Allocate a subsection and 4 prototype PTEs to hold
        // the information for the large pages.
        //

        Subsection = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(SUBSECTION) + (4 * sizeof(MMPTE)),
                                     MMPPTE_NAME);
        if (Subsection == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }
#endif

    //
    // Attempt to allocate the pool and charge quota during the Vad insertion.
    //

    PhysicalView = (PMI_PHYSICAL_VIEW)ExAllocatePoolWithTag (NonPagedPool,
                                                             sizeof(MI_PHYSICAL_VIEW),
                                                             MI_PHYSICAL_VIEW_KEY);
    if (PhysicalView == NULL) {
#ifdef LARGE_PAGES
        if (AllocationType & MEM_LARGE_PAGES) {
            ExFreePool (Subsection);
        }
#endif
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool,
                                              sizeof(MMVAD_LONG),
                                              'ldaV');
    if (Vad == NULL) {
#ifdef LARGE_PAGES
        if (AllocationType & MEM_LARGE_PAGES) {
            ExFreePool (Subsection);
        }
#endif
        ExFreePool (PhysicalView);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    PhysicalView->Vad = (PMMVAD) Vad;
    PhysicalView->StartVa = StartingAddress;
    PhysicalView->EndVa = EndingAddress;
    PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_PHYS;

    RtlZeroMemory (Vad, sizeof(MMVAD_LONG));
    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->ControlArea = ControlArea;
    Vad->u2.VadFlags2.Inherit = MM_VIEW_UNMAP;
    Vad->u2.VadFlags2.LongVad = 1;
    Vad->u.VadFlags.PhysicalMapping = 1;
    Vad->u.VadFlags.Protection = ProtectionMask;

    //
    // Set the last contiguous PTE field in the Vad to the page frame
    // number of the starting physical page.
    //

    Vad->LastContiguousPte = (PMMPTE) StartingPageFrameIndex;

#ifdef LARGE_PAGES
    if (AllocationType & MEM_LARGE_PAGES) {
        Vad->u.VadFlags.LargePages = 1;
        Vad->FirstPrototypePte = (PMMPTE)Subsection;
    }
    else {
#endif //LARGE_PAGES
        // Vad->u.VadFlags.LargePages = 0;
        Vad->FirstPrototypePte = (PMMPTE) StartingPageFrameIndex;
#ifdef LARGE_PAGES
    }
#endif //LARGE_PAGES

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    //
    // Initialize the PTE templates as the working set mutex is not needed to
    // synchronize this (so avoid adding extra contention).
    //

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    MI_MAKE_VALID_PTE (TempPte,
                       StartingPageFrameIndex,
                       ProtectionMask,
                       PointerPte);

    if (TempPte.u.Hard.Write) {
        MI_SET_PTE_DIRTY (TempPte);
    }

    if (CacheAttribute == MiWriteCombined) {
        MI_SET_PTE_WRITE_COMBINE (TempPte);
    }
    else if (CacheAttribute == MiNonCached) {
        MI_DISABLE_CACHING (TempPte);
    }

    //
    // Ensure no page frame cache attributes conflict.
    //

    if (MiCheckCacheAttributes (StartingPageFrameIndex, NumberOfPages, CacheAttribute) == FALSE) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto Failure1;
    }

    //
    // Insert the VAD.  This could fail due to quota charges.
    //

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad ((PMMVAD) Vad);

    if (!NT_SUCCESS(Status)) {

        UNLOCK_WS_UNSAFE (Process);

Failure1:
        ExFreePool (PhysicalView);

        //
        // The pool allocation succeeded, but the quota charge
        // in InsertVad failed, deallocate the pool and return
        // an error.
        //

        ExFreePool (Vad);
#ifdef LARGE_PAGES
        if (AllocationType & MEM_LARGE_PAGES) {
            ExFreePool (Subsection);
        }
#endif //LARGE_PAGES
        return Status;
    }

    //
    // Build the PTEs in the address space.
    //

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

#ifdef LARGE_PAGES
    if (AllocationType & MEM_LARGE_PAGES) {
        Subsection->StartingSector = pageSize;
        Subsection->EndingSector = (ULONG_PTR)StartingAddress;
        Subsection->u.LongFlags = 0;
        Subsection->u.SubsectionFlags.LargePages = 1;
        protoPte = (PMMPTE)(Subsection + 1);

        //
        // Build the first 2 PTEs as entries for the TLB to
        // map the specified physical address.
        //

        *protoPte = TempPte;
        protoPte += 1;

        if (*CapturedViewSize > pageSize) {
            *protoPte = TempPte;
            protoPte->u.Hard.PageFrameNumber += (pageSize >> PAGE_SHIFT);
        }
        else {
            *protoPte = ZeroPte;
        }
        protoPte += 1;

        //
        // Build the first prototype PTE as a paging file format PTE
        // referring to the subsection.
        //

        protoPte->u.Long = MiGetSubsectionAddressForPte (Subsection);
        protoPte->u.Soft.Prototype = 1;
        protoPte->u.Soft.Protection = ProtectionMask;

        //
        // Set the PTE up for all the user's PTE entries in prototype PTE
        // format pointing to the 3rd prototype PTE.
        //

        TempPte.u.Long = MiProtoAddressForPte (protoPte);
    }

    if (!(AllocationType & MEM_LARGE_PAGES)) {
#endif //LARGE_PAGES

        MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);

        Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);

                MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);

                Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));
            }

            //
            // Increment the count of non-zero page table entries for this
            // page table and the number of private pages for the process.
            //

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            ASSERT (PointerPte->u.Long == 0);

            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            Pfn2->u2.ShareCount += 1;

            PointerPte += 1;
            TempPte.u.Hard.PageFrameNumber += 1;
        }
#ifdef LARGE_PAGES
    }
#endif //LARGE_PAGES

    MI_SWEEP_CACHE (CacheAttribute,
                    StartingAddress,
                    (PCHAR) EndingAddress - (PCHAR) StartingAddress);

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    // At the same time, insert the physical view descriptor now that
    // the page table page hierarchy is in place.  Note probes can find
    // this descriptor immediately.
    //

    MiInsertPhysicalViewAndRefControlArea (Process, ControlArea, PhysicalView);

    UNLOCK_WS_UNSAFE (Process);

    //
    // Update the current virtual size in the process header.
    //

    *CapturedViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    Process->VirtualSize += *CapturedViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

    *CapturedBase = StartingAddress;

    return STATUS_SUCCESS;
}


VOID
MiSetControlAreaSymbolsLoaded (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a nonpaged helper routine to mark the specified control area as
    having its debug symbols loaded.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    ControlArea->u.Flags.DebugSymbolsLoaded = 1;
    UNLOCK_PFN (OldIrql);
}

VOID
MiLoadUserSymbols (
    IN PCONTROL_AREA ControlArea,
    IN PVOID StartingAddress,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine loads symbols for user space executables and DLLs.

Arguments:

    ControlArea - Supplies the control area for the section.

    StartingAddress - Supplies the virtual address the section is mapped at.

    Process - Supplies the process pointer which is receiving the section.

Return Value:

    None.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PLIST_ENTRY Head;
    PLIST_ENTRY Next;
    PKLDR_DATA_TABLE_ENTRY Entry;
    PIMAGE_NT_HEADERS NtHeaders;
    PUNICODE_STRING FileName;
    ANSI_STRING AnsiName;
    NTSTATUS Status;
    PKTHREAD CurrentThread;

    //
    //  TEMP TEMP TEMP rip out ANSI conversion when debugger converted.
    //

    FileName = (PUNICODE_STRING)&ControlArea->FilePointer->FileName;

    if (FileName->Length == 0) {
        return;
    }

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Head = &MmLoadedUserImageList;
    Next = Head->Flink;

    while (Next != Head) {
        Entry = CONTAINING_RECORD (Next,
                                   KLDR_DATA_TABLE_ENTRY,
                                   InLoadOrderLinks);

        if (Entry->DllBase == StartingAddress) {
            Entry->LoadCount += 1;
            break;
        }
        Next = Next->Flink;
    }

    if (Next == Head) {
        Entry = ExAllocatePoolWithTag (NonPagedPool,
                                sizeof( *Entry ) +
                                    FileName->Length +
                                    sizeof( UNICODE_NULL ),
                                    MMDB);
        if (Entry != NULL) {

            RtlZeroMemory (Entry, sizeof(*Entry));

            try {
                NtHeaders = RtlImageNtHeader (StartingAddress);
                if (NtHeaders != NULL) {
#if defined(_WIN64)
                    if (NtHeaders->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) {
                        Entry->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
                        Entry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
                    }
                    else {
                        PIMAGE_NT_HEADERS32 NtHeaders32 = (PIMAGE_NT_HEADERS32)NtHeaders;

                        Entry->SizeOfImage = NtHeaders32->OptionalHeader.SizeOfImage;
                        Entry->CheckSum = NtHeaders32->OptionalHeader.CheckSum;
                    }
#else
                    Entry->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
                    Entry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
#endif
                }
            } except (EXCEPTION_EXECUTE_HANDLER) {
                NOTHING;
            }

            Entry->DllBase = StartingAddress;
            Entry->FullDllName.Buffer = (PWSTR)(Entry+1);
            Entry->FullDllName.Length = FileName->Length;
            Entry->FullDllName.MaximumLength = (USHORT)
                (Entry->FullDllName.Length + sizeof( UNICODE_NULL ));

            RtlCopyMemory (Entry->FullDllName.Buffer,
                           FileName->Buffer,
                           FileName->Length);

            Entry->FullDllName.Buffer[ Entry->FullDllName.Length / sizeof( WCHAR )] = UNICODE_NULL;
            Entry->LoadCount = 1;
            InsertTailList (&MmLoadedUserImageList,
                            &Entry->InLoadOrderLinks);

        }
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    Status = RtlUnicodeStringToAnsiString (&AnsiName,
                                           FileName,
                                           TRUE);

    if (NT_SUCCESS( Status)) {
        DbgLoadImageSymbols (&AnsiName,
                             StartingAddress,
                             (ULONG_PTR)Process);
        RtlFreeAnsiString (&AnsiName);
    }
    return;
}


VOID
MiDereferenceControlArea (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a nonpaged helper routine to dereference the specified control area.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    ControlArea->NumberOfMappedViews -= 1;
    ControlArea->NumberOfUserReferences -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}


NTSTATUS
MiMapViewOfImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T ImageCommitment
    )

/*++

Routine Description:

    This routine maps the specified Image section into the
    specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PVOID StartingAddress;
    PVOID OutputStartingAddress;
    PVOID EndingAddress;
    PVOID HighestUserAddress;
    LOGICAL Attached;
    PSUBSECTION Subsection;
    ULONG PteOffset;
    NTSTATUS Status;
    NTSTATUS ReturnedStatus;
    PMMPTE ProtoPte;
    PVOID BasedAddress;
    SIZE_T NeededViewSize;
    SIZE_T OutputViewSize;
    ULONG AllocationPreference;

    Attached = FALSE;

    //
    // Image file.
    //
    // Locate the first subsection (text) and create a virtual
    // address descriptor to map the entire image here.
    //

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (ControlArea->u.Flags.ImageMappedInSystemSpace) {

        if (KeGetPreviousMode() != KernelMode) {

            //
            // Mapping in system space as a driver, hence copy on write does
            // not work.  Don't allow user processes to map the image.
            //

            return STATUS_CONFLICTING_ADDRESSES;
        }
    }

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    if (MiCheckPurgeAndUpMapCount (ControlArea) == FALSE) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Capture the based address to the stack, to prevent page faults.
    //

    BasedAddress = ControlArea->Segment->BasedAddress;

    if (*CapturedViewSize == 0) {
        *CapturedViewSize =
            (ULONG_PTR)(Section->SizeOfSection.QuadPart - SectionOffset->QuadPart);
    }

    ReturnedStatus = STATUS_SUCCESS;

    //
    // Determine if a specific base was specified.
    //

    if (*CapturedBase != NULL) {

        //
        // Captured base is not NULL.
        //
        // Check to make sure the specified address range is currently unused
        // and within the user address space.
        //

        StartingAddress = MI_64K_ALIGN(*CapturedBase);
        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                       *CapturedViewSize - 1) | (PAGE_SIZE - 1));

        if ((StartingAddress <=  MM_HIGHEST_VAD_ADDRESS) &&
            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                (ULONG_PTR)StartingAddress >= *CapturedViewSize) &&

            (EndingAddress <= MM_HIGHEST_VAD_ADDRESS)) {

            if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
                MiDereferenceControlArea (ControlArea);
                return STATUS_CONFLICTING_ADDRESSES;
            }
        }
        else {
            MiDereferenceControlArea (ControlArea);
            return STATUS_CONFLICTING_ADDRESSES;
        }

        //
        // A conflicting VAD was not found and the specified address range is
        // within the user address space. If the image will not reside at its
        // base address, then set a special return status.
        //

        if (((ULONG_PTR)StartingAddress +
            (ULONG_PTR)MI_64K_ALIGN(SectionOffset->LowPart)) != (ULONG_PTR)BasedAddress) {
            ReturnedStatus = STATUS_IMAGE_NOT_AT_BASE;
        }

    }
    else {

        //
        // Captured base is NULL.
        //
        // If the captured view size is greater than the largest size that
        // can fit in the user address space, then it is not possible to map
        // the image.
        //

        if ((PVOID)*CapturedViewSize > MM_HIGHEST_VAD_ADDRESS) {
            MiDereferenceControlArea (ControlArea);
            return STATUS_NO_MEMORY;
        }

        //
        // Check to make sure the specified address range is currently unused
        // and within the user address space.
        //

        StartingAddress = (PVOID)((ULONG_PTR)BasedAddress +
                                    (ULONG_PTR)MI_64K_ALIGN(SectionOffset->LowPart));

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                    *CapturedViewSize - 1) | (PAGE_SIZE - 1));

        Vad = (PMMVAD) TRUE;
        NeededViewSize = *CapturedViewSize;

        if ((StartingAddress >= MM_LOWEST_USER_ADDRESS) &&
            (StartingAddress <= MM_HIGHEST_VAD_ADDRESS) &&
            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                (ULONG_PTR)StartingAddress >= *CapturedViewSize) &&

            (EndingAddress <= MM_HIGHEST_VAD_ADDRESS)) {

            Vad = (PMMVAD) (ULONG_PTR) MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress);
        }

        //
        // If the VAD address is not NULL, then a conflict was discovered.
        // Attempt to select another address range in which to map the image.
        //

        if (Vad != NULL) {

            //
            // The image could not be mapped at its natural base address
            // try to find another place to map it.
            //
            // If the system has been biased to an alternate base address to
            // allow 3gb of user address space, then make sure the high order
            // address bit is zero.
            //

            if ((MmVirtualBias != 0) && (ZeroBits == 0)) {
                ZeroBits = 1;
            }

            ReturnedStatus = STATUS_IMAGE_NOT_AT_BASE;

            //
            // Check whether the registry indicates that all applications
            // should be given virtual address ranges from the highest
            // address downwards in order to test 3GB-aware apps on 32-bit
            // machines and 64-bit apps on NT64.
            //

            AllocationPreference = MmAllocationPreference;

            ASSERT ((AllocationPreference == 0) ||
                    (AllocationPreference == MEM_TOP_DOWN));

#if defined (_WIN64)
            if (Process->Wow64Process != NULL) {
                AllocationPreference = 0;
            }
#endif

            //
            // Find a starting address on a 64k boundary.
            //

            if (AllocationPreference & MEM_TOP_DOWN) {

                if (ZeroBits != 0) {
                    HighestUserAddress = (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits);
                    if (HighestUserAddress > MM_HIGHEST_VAD_ADDRESS) {
                        HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                    }
                }
                else {
                    HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                }

                Status = MiFindEmptyAddressRangeDown (Process->VadRoot,
                                                      NeededViewSize,
                                                      HighestUserAddress,
                                                      X64K,
                                                      &StartingAddress);
            }
            else {
                Status = MiFindEmptyAddressRange (NeededViewSize,
                                                  X64K,
                                                  (ULONG)ZeroBits,
                                                  &StartingAddress);
            }

            if (!NT_SUCCESS (Status)) {
                MiDereferenceControlArea (ControlArea);
                return Status;
            }

            EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                        *CapturedViewSize - 1) | (PAGE_SIZE - 1));
        }
    }

    //
    // Allocate and initialize a VAD for the specified address range.
    //

    Vad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD), MMVADKEY);

    if (Vad == NULL) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Vad, sizeof(MMVAD));
    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->u2.VadFlags2.Inherit = (InheritDisposition == ViewShare);
    Vad->u.VadFlags.ImageMap = 1;

    //
    // Set the protection in the VAD as EXECUTE_WRITE_COPY.
    //

    Vad->u.VadFlags.Protection = MM_EXECUTE_WRITECOPY;
    Vad->ControlArea = ControlArea;

    //
    // Set the first prototype PTE field in the Vad.
    //

    SectionOffset->LowPart = SectionOffset->LowPart & ~(X64K - 1);
    PteOffset = (ULONG)(SectionOffset->QuadPart >> PAGE_SHIFT);

    Vad->FirstPrototypePte = &Subsection->SubsectionBase[PteOffset];
    Vad->LastContiguousPte = MM_ALLOCATION_FILLS_VAD;

    //
    // NOTE: the full commitment is charged even if a partial map of an
    // image is being done.  This saves from having to run through the
    // entire image (via prototype PTEs) and calculate the charge on
    // a per page basis for the partial map.
    //

    Vad->u.VadFlags.CommitCharge = (SIZE_T)ImageCommitment; // ****** temp

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad (Vad);

    UNLOCK_WS_UNSAFE (Process);

    if (!NT_SUCCESS(Status)) {

        MiDereferenceControlArea (ControlArea);

        //
        // The quota charge in InsertVad failed,
        // deallocate the pool and return an error.
        //

        ExFreePool (Vad);
        return Status;
    }

    OutputStartingAddress = StartingAddress;
    OutputViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;

#if DBG
    if (MmDebug & MM_DBG_WALK_VAD_TREE) {
        DbgPrint("mapped image section vads\n");
        VadTreeWalk ();
    }
#endif

    //
    // Update the current virtual size in the process header.
    //

    Process->VirtualSize += OutputViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

    if (ControlArea->u.Flags.FloppyMedia) {

        //
        // The image resides on a floppy disk, in-page all
        // pages from the floppy and mark them as modified so
        // they migrate to the paging file rather than reread
        // them from the floppy disk which may have been removed.
        //

        ProtoPte = Vad->FirstPrototypePte;

        //
        // This could get an in-page error from the floppy.
        //

        while (StartingAddress < EndingAddress) {

            //
            // If the prototype PTE is valid, transition or
            // in prototype PTE format, bring the page into
            // memory and set the modified bit.
            //

            if ((ProtoPte->u.Hard.Valid == 1) ||

                (((ProtoPte->u.Soft.Prototype == 1) ||
                 (ProtoPte->u.Soft.Transition == 1)) &&
                 (ProtoPte->u.Soft.Protection != MM_NOACCESS))
                
                ) {

                Status = MiSetPageModified (Vad, StartingAddress);

                if (!NT_SUCCESS (Status)) {

                    //
                    // An in page error must have occurred touching the image,
                    // Ignore the error and continue to the next page - unless
                    // it's being run over a network.  If it's being run over
                    // a net and the control area is marked as floppy, then
                    // the image must be marked NET_RUN_FROM_SWAP, so any
                    // inpage error must be treated as terminal now - so the app
                    // doesn't later spontaneously abort when referencing
                    // this page.  This provides app writers with a way to
                    // mark their app in a way which is robust regardless of
                    // network conditions.
                    //

                    if (ControlArea->u.Flags.Networked) {

                        //
                        // N.B.  There are no race conditions with the user
                        // deleting/substituting this mapping from another
                        // thread as the address space mutex is still held.
                        //

                        Process->VirtualSize -= OutputViewSize;

                        PreviousVad = MiGetPreviousVad (Vad);
                        NextVad = MiGetNextVad (Vad);
                    
                        LOCK_WS_UNSAFE (Process);

                        MiRemoveVad (Vad);
                    
                        //
                        // Return commitment for page table pages.
                        //
                    
                        MiReturnPageTablePageCommitment (MI_VPN_TO_VA (Vad->StartingVpn),
                                                         MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                                         Process,
                                                         PreviousVad,
                                                         NextVad);
                    
                        MiRemoveMappedView (Process, Vad);
                    
                        UNLOCK_WS_UNSAFE (Process);

                        ExFreePool (Vad);

                        return Status;
                    }
                }
            }
            ProtoPte += 1;
            StartingAddress = (PVOID)((PCHAR)StartingAddress + PAGE_SIZE);
        }
    }

    *CapturedViewSize = OutputViewSize;
    *CapturedBase = OutputStartingAddress;

    if (NT_SUCCESS(ReturnedStatus)) {


        //
        // Check to see if this image is for the architecture of the current
        // machine.
        //

        if (ControlArea->Segment->u2.ImageInformation->ImageContainsCode &&
            ((ControlArea->Segment->u2.ImageInformation->Machine <
                                          USER_SHARED_DATA->ImageNumberLow) ||
             (ControlArea->Segment->u2.ImageInformation->Machine >
                                          USER_SHARED_DATA->ImageNumberHigh)
            )
           ) {
#if defined (_WIN64)

            //
            // If this is a wow64 process then allow i386 images.
            //

            if (!Process->Wow64Process ||
                ControlArea->Segment->u2.ImageInformation->Machine != IMAGE_FILE_MACHINE_I386) {
                return STATUS_IMAGE_MACHINE_TYPE_MISMATCH;
            }
#else   //!_WIN64
            return STATUS_IMAGE_MACHINE_TYPE_MISMATCH;
#endif
        }

        StartingAddress = MI_VPN_TO_VA (Vad->StartingVpn);

        if (PsImageNotifyEnabled) {

            IMAGE_INFO ImageInfo;

            if ( (StartingAddress < MmHighestUserAddress) &&
                 Process->UniqueProcessId &&
                 Process != PsInitialSystemProcess ) {

                ImageInfo.Properties = 0;
                ImageInfo.ImageAddressingMode = IMAGE_ADDRESSING_MODE_32BIT;
                ImageInfo.ImageBase = StartingAddress;
                ImageInfo.ImageSize = OutputViewSize;
                ImageInfo.ImageSelector = 0;
                ImageInfo.ImageSectionNumber = 0;
                PsCallImageNotifyRoutines(
                            (PUNICODE_STRING) &ControlArea->FilePointer->FileName,
                            Process->UniqueProcessId,
                            &ImageInfo);
            }
        }

        if ((NtGlobalFlag & FLG_ENABLE_KDEBUG_SYMBOL_LOAD) &&
            (ControlArea->u.Flags.Image) &&
            (ReturnedStatus != STATUS_IMAGE_NOT_AT_BASE) &&
            (ControlArea->u.Flags.DebugSymbolsLoaded == 0)) {

            if (CacheImageSymbols (StartingAddress) == TRUE) {

                MiSetControlAreaSymbolsLoaded (ControlArea);

                MiLoadUserSymbols (ControlArea, StartingAddress, Process);
            }

        }
    }

#if defined(_MIALT4K_)

    if (Process->Wow64Process != NULL) {

        MiProtectImageFileFor4kPage(StartingAddress,
                                    OutputViewSize,
                                    Vad->FirstPrototypePte,
                                    Process);
    }

#endif

    return ReturnedStatus;

}

NTSTATUS
MiAddViewsForSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable wrapper routine maps the views into the specified section.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the last PTE offset to end the views at.
                    Supplies zero if views are desired from the supplied
                    subsection to the end of the file.

Return Value:

    NTSTATUS.

Environment:

    Kernel Mode, address creation mutex optionally held if called in process
    context.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    NTSTATUS Status;
    ULONG Waited;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    LOCK_PFN (OldIrql);

    //
    // This routine returns with the PFN lock released !
    //

    Status = MiAddViewsForSection (StartMappedSubsection,
                                   LastPteOffset,
                                   OldIrql,
                                   &Waited);

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    return Status;
}

LOGICAL
MiReferenceSubsection (
    IN PMSUBSECTION MappedSubsection
    )

/*++

Routine Description:

    This nonpagable routine reference counts the specified subsection if
    its prototype PTEs are already setup, otherwise it returns FALSE.

Arguments:

    MappedSubsection - Supplies the mapped subsection to start at.

Return Value:

    NTSTATUS.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    //
    // Note the control area is not necessarily active at this point.
    //

    if (MappedSubsection->SubsectionBase == NULL) {

        //
        // No prototype PTEs exist, caller will have to go the long way.
        //

        return FALSE;
    }

    //
    // The mapping base exists so the number of mapped views can be
    // safely incremented.  This prevents a trim from starting after
    // we release the lock.
    //

    MappedSubsection->NumberOfMappedViews += 1;

    MI_SNAP_SUB (MappedSubsection, 0x4);

    if (MappedSubsection->DereferenceList.Flink != NULL) {

        //
        // Remove this from the list of unused subsections.
        //

        RemoveEntryList (&MappedSubsection->DereferenceList);

        MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

        MappedSubsection->DereferenceList.Flink = NULL;
    }

    //
    // Set the access bit so an already ongoing trim won't blindly
    // delete the prototype PTEs on completion of a mapped write.
    // This can happen if the current thread dirties some pages and
    // then deletes the view before the trim write finishes - this
    // bit informs the trimming thread that a rescan is needed so
    // that writes are not lost.
    //

    MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

    return TRUE;
}

NTSTATUS
MiAddViewsForSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL,
    IN KIRQL OldIrql,
    OUT PULONG Waited
    )

/*++

Routine Description:

    This nonpagable routine maps the views into the specified section.

    N.B. This routine may release and reacquire the PFN lock !

    N.B. This routine always returns with the PFN lock released !

    N.B. Callers may pass in view lengths that exceed the length of
         the section and this must succeed.  Thus MiAddViews must check
         for this and know to stop the references at the end.  More
         importantly, MiRemoveViews must also contain the same logic.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to provide views for.  Supplies zero if
                    views are desired from the supplied subsection to the
                    end of the file.

    Waited - Supplies a pointer to a ULONG to increment if the PFN lock is
             released and reacquired.

Return Value:

    NTSTATUS, PFN lock released.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    MMPTE TempPte;
    ULONG Size;
    PMMPTE ProtoPtes;
    PMSUBSECTION MappedSubsection;

    *Waited = 0;

    MappedSubsection = StartMappedSubsection;

    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {

        //
        // Note the control area must be active at this point.
        //

        ASSERT (MappedSubsection->ControlArea->DereferenceList.Flink == NULL);

        if (MappedSubsection->SubsectionBase != NULL) {

            //
            // The mapping base exists so the number of mapped views can be
            // safely incremented.  This prevents a trim from starting after
            // we release the lock.
            //

            MappedSubsection->NumberOfMappedViews += 1;

            MI_SNAP_SUB (MappedSubsection, 0x5);

            if (MappedSubsection->DereferenceList.Flink != NULL) {

                //
                // Remove this from the list of unused subsections.
                //

                RemoveEntryList (&MappedSubsection->DereferenceList);

                MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

                MappedSubsection->DereferenceList.Flink = NULL;
            }

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
        }
        else {

            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
            ASSERT (MappedSubsection->NumberOfMappedViews == 0);

            MI_SNAP_SUB (MappedSubsection, 0x6);

            //
            // No prototype PTEs currently exist for this subsection.
            // Allocate and populate them properly now.
            //

            UNLOCK_PFN (OldIrql);
            *Waited = 1;

            Size = (MappedSubsection->PtesInSubsection + MappedSubsection->UnusedPtes) * sizeof(MMPTE);

            ASSERT (Size != 0);

            ProtoPtes = (PMMPTE)ExAllocatePoolWithTag (PagedPool, Size, MMSECT);

            if (ProtoPtes == NULL) {
                MI_SNAP_SUB (MappedSubsection, 0x7);
                goto Failed;
            }

            //
            // Fill in the prototype PTEs for this subsection.
            //

            TempPte.u.Long = MiGetSubsectionAddressForPte (MappedSubsection);
            TempPte.u.Soft.Prototype = 1;

            //
            // Set all the PTEs to the initial execute-read-write protection.
            // The section will control access to these and the segment
            // must provide a method to allow other users to map the file
            // for various protections.
            //

            TempPte.u.Soft.Protection = MappedSubsection->ControlArea->Segment->SegmentPteTemplate.u.Soft.Protection;

            MiFillMemoryPte (ProtoPtes, Size, TempPte.u.Long);

            LOCK_PFN (OldIrql);

            //
            // Now that the mapping base is guaranteed to be nonzero (shortly),
            // the number of mapped views can be safely incremented.  This
            // prevents a trim from starting after we release the lock.
            //

            MappedSubsection->NumberOfMappedViews += 1;

            MI_SNAP_SUB (MappedSubsection, 0x8);

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

            //
            // Check to make sure another thread didn't do this already while
            // the lock was released.
            //

            if (MappedSubsection->SubsectionBase == NULL) {

                ASSERT (MappedSubsection->NumberOfMappedViews == 1);

                MappedSubsection->SubsectionBase = ProtoPtes;
            }
            else {
                if (MappedSubsection->DereferenceList.Flink != NULL) {
    
                    //
                    // Remove this from the list of unused subsections.
                    //
    
                    ASSERT (MappedSubsection->NumberOfMappedViews == 1);

                    RemoveEntryList (&MappedSubsection->DereferenceList);
    
                    MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);
    
                    MappedSubsection->DereferenceList.Flink = NULL;
                }
                else {
                    ASSERT (MappedSubsection->NumberOfMappedViews > 1);
                }

                //
                // This unlock and release of pool could be postponed until
                // the end of this routine when the lock is released anyway
                // but this should be a rare case anyway so don't bother.
                //

                UNLOCK_PFN (OldIrql);
                ExFreePool (ProtoPtes);
                LOCK_PFN (OldIrql);
            }
            MI_SNAP_SUB (MappedSubsection, 0x9);
        }

        if (ARGUMENT_PRESENT ((ULONG_PTR)LastPteOffset)) {
            ASSERT ((LONG)MappedSubsection->PtesInSubsection > 0);
            ASSERT ((LONG)LastPteOffset > 0);
            if (LastPteOffset <= MappedSubsection->PtesInSubsection) {
                break;
            }
            LastPteOffset -= MappedSubsection->PtesInSubsection;
        }

        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;

    } while (MappedSubsection != NULL);

    UNLOCK_PFN (OldIrql);

    return STATUS_SUCCESS;

Failed:

    LOCK_PFN (OldIrql);

    //
    // A prototype PTE pool allocation failed.  Carefully undo any allocations
    // and references done so far.
    //

    while (StartMappedSubsection != MappedSubsection) {
        StartMappedSubsection->NumberOfMappedViews -= 1;
        ASSERT (StartMappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
        ASSERT (StartMappedSubsection->DereferenceList.Flink == NULL);
        MI_SNAP_SUB (MappedSubsection, 0xA);
        if (StartMappedSubsection->NumberOfMappedViews == 0) {

            //
            // Insert this subsection into the unused subsection list.
            // Since it's not likely there are any resident protos at this
            // point, enqueue each subsection at the front.
            //

            InsertHeadList (&MmUnusedSubsectionList,
                            &StartMappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }
        StartMappedSubsection = (PMSUBSECTION) StartMappedSubsection->NextSubsection;
    }

    UNLOCK_PFN (OldIrql);

    return STATUS_INSUFFICIENT_RESOURCES;
}


VOID
MiRemoveViewsFromSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable routine removes the views from the specified section if
    the reference count reaches zero.

    N.B. Callers may pass in view lengths that exceed the length of
         the section and this must succeed.  Thus MiAddViews checks
         for this and knows to stop the references at the end.  More
         importantly, MiRemoveViews must also contain the same logic.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to remove.  Supplies zero to remove views
                    from the supplied subsection to the end of the file.

Return Value:

    None.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    PMSUBSECTION MappedSubsection;

    MappedSubsection = StartMappedSubsection;

    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {

        //
        // Note the control area must be active at this point.
        //

        ASSERT (MappedSubsection->ControlArea->DereferenceList.Flink == NULL);
        ASSERT (MappedSubsection->SubsectionBase != NULL);
        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);
        ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        MI_SNAP_SUB (MappedSubsection, 0x3);

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if (ARGUMENT_PRESENT ((ULONG_PTR)LastPteOffset)) {
            if (LastPteOffset <= MappedSubsection->PtesInSubsection) {
                break;
            }
            LastPteOffset -= MappedSubsection->PtesInSubsection;
        }

        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;

    } while (MappedSubsection != NULL);

    return;
}


VOID
MiRemoveViewsFromSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable routine removes the views from the specified section if
    the reference count reaches zero.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to remove.  Supplies zero to remove views
                    from the supplied subsection to the end of the file.

Return Value:

    None.

Environment:

    Kernel Mode, address creation mutex optionally held if called in process
    context.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    LOCK_PFN (OldIrql);

    MiRemoveViewsFromSection (StartMappedSubsection, LastPteOffset);

    UNLOCK_PFN (OldIrql);
}
#if DBG
extern PMSUBSECTION MiActiveSubsection;
#endif

VOID
MiConvertStaticSubsections (
    IN PCONTROL_AREA ControlArea
    )
{
    PMSUBSECTION MappedSubsection;

    ASSERT (ControlArea->u.Flags.Image == 0);
    ASSERT (ControlArea->FilePointer != NULL);
    ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        MappedSubsection = (PMSUBSECTION)(ControlArea + 1);
    }
    else {
        MappedSubsection = (PMSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    do {
        MI_SNAP_SUB (MappedSubsection, 0xB);
        if (MappedSubsection->DereferenceList.Flink != NULL) {

            // 
            // This subsection is already on the dereference subsection list.
            // This is the expected case.
            // 

            NOTHING;
        }
        else if (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1) {

            // 
            // This subsection was not put on the dereference subsection list
            // because it was created as part of an extension or nowrite.
            // Since this is the last segment dereference, convert the
            // subsection and its prototype PTEs to dynamic now (iff nowrite
            // is clear).
            // 

            MappedSubsection->u.SubsectionFlags.SubsectionStatic = 0;
            MappedSubsection->u2.SubsectionFlags2.SubsectionConverted = 1;
            MappedSubsection->NumberOfMappedViews = 1;

            MiRemoveViewsFromSection (MappedSubsection, 
                                      MappedSubsection->PtesInSubsection);

            MiSubsectionsConvertedToDynamic += 1;
        }
        else if (MappedSubsection->SubsectionBase == NULL) {

            //
            // This subsection has already had its prototype PTEs reclaimed
            // (or never allocated), hence it is not on any reclaim lists.
            //

            NOTHING;
        }
        else {

            // 
            // This subsection is being processed by the dereference
            // segment thread right now !  The dereference thread sets the
            // mapped view count to 1 when it starts processing the subsection.
            // The subsequent flush then increases it to 2 while in progress.
            // So the count must be either 1 or 2 at this point.
            // 

            ASSERT (MappedSubsection == MiActiveSubsection);
            ASSERT ((MappedSubsection->NumberOfMappedViews == 1) ||
                    (MappedSubsection->NumberOfMappedViews == 2));
        }
        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;
    } while (MappedSubsection != NULL);
}

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    )

/*++

Routine Description:

    This routine maps the specified mapped file or pagefile-backed section
    into the specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

    ProtectionMask - Supplies the initial page protection-mask.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD Vad;
    SIZE_T VadSize;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PSUBSECTION Subsection;
    ULONG PteOffset;
    ULONG LastPteOffset;
    PVOID HighestUserAddress;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    ULONG_PTR Alignment;
    SIZE_T QuotaCharge;
    SIZE_T QuotaExcess;
    PMMPTE TheFirstPrototypePte;
    PVOID CapturedStartingVa;
    ULONG CapturedCopyOnWrite;
    NTSTATUS Status;
    PSEGMENT Segment;
    SIZE_T SizeOfSection;
#if defined(_MIALT4K_)
    SIZE_T ViewSizeFor4k;
    ULONG AltFlags;
#endif

    QuotaCharge = 0;
    Segment = ControlArea->Segment;

    if ((AllocationType & MEM_RESERVE) && (ControlArea->FilePointer == NULL)) {
        return STATUS_INVALID_PARAMETER_9;
    }

    //
    // Check to see if there is a purge operation ongoing for
    // this segment.
    //

    if ((AllocationType & MEM_DOS_LIM) != 0) {
        if ((*CapturedBase == NULL) ||
            (AllocationType & MEM_RESERVE)) {

            //
            // If MEM_DOS_LIM is specified, the address to map the
            // view MUST be specified as well.
            //

            return STATUS_INVALID_PARAMETER_3;
        }
        Alignment = PAGE_SIZE;
    }
    else {
        Alignment = X64K;
    }

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    if (MiCheckPurgeAndUpMapCount (ControlArea) == FALSE) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (*CapturedViewSize == 0) {

        SectionOffset->LowPart &= ~((ULONG)Alignment - 1);
        *CapturedViewSize = (ULONG_PTR)(Section->SizeOfSection.QuadPart -
                                    SectionOffset->QuadPart);
    }
    else {
        *CapturedViewSize += SectionOffset->LowPart & (Alignment - 1);
        SectionOffset->LowPart &= ~((ULONG)Alignment - 1);
    }

    ASSERT ((SectionOffset->LowPart & ((ULONG)Alignment - 1)) == 0);

    if ((LONG_PTR)*CapturedViewSize <= 0) {

        //
        // Section offset or view size past size of section.
        //

        MiDereferenceControlArea (ControlArea);

        return STATUS_INVALID_VIEW_SIZE;
    }

    //
    // Calculate the first prototype PTE field so it can be stored in the Vad.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PteOffset = (ULONG)(SectionOffset->QuadPart >> PAGE_SHIFT);

    //
    // Make sure the PTEs are not in the extended part of the segment.
    //

    if (PteOffset >= Segment->TotalNumberOfPtes) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    LastPteOffset = (ULONG)((SectionOffset->QuadPart + *CapturedViewSize + PAGE_SIZE - 1) >> PAGE_SHIFT);
    ASSERT (LastPteOffset >= PteOffset);

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        LastPteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        ASSERT (Subsection != NULL);
    }

    if (ControlArea->FilePointer != NULL) {

        //
        // Increment the view count for every subsection spanned by this view.
        //
        // N.B. Callers may pass in view lengths that exceed the length of
        // the section and this must succeed.  Thus MiAddViews must check
        // for this and know to stop the references at the end.  More
        // importantly, MiRemoveViews must also contain the same logic.
        //

        Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION)Subsection,
                                              LastPteOffset);

        if (!NT_SUCCESS (Status)) {
            MiDereferenceControlArea (ControlArea);
            return Status;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);
    TheFirstPrototypePte = &Subsection->SubsectionBase[PteOffset];

    //
    // Calculate the quota for the specified pages.
    //

    if ((ControlArea->FilePointer == NULL) &&
        (CommitSize != 0) &&
        (Segment->NumberOfCommittedPages < Segment->TotalNumberOfPtes)) {

        PointerPte = TheFirstPrototypePte;
        LastPte = PointerPte + BYTES_TO_PAGES(CommitSize);

        //
        // Charge quota for the entire requested range.  If the charge succeeds,
        // excess is returned when the PTEs are actually filled in.
        //

        QuotaCharge = LastPte - PointerPte;
    }

    CapturedStartingVa = (PVOID)Section->Address.StartingVpn;
    CapturedCopyOnWrite = Section->u.Flags.CopyOnWrite;

    if ((*CapturedBase == NULL) && (CapturedStartingVa == NULL)) {

        //
        // The section is not based,
        // find an empty range starting on a 64k boundary.
        //

        if (AllocationType & MEM_TOP_DOWN) {

            if (ZeroBits != 0) {
                HighestUserAddress = (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits);
                if (HighestUserAddress > MM_HIGHEST_VAD_ADDRESS) {
                    HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                }
            }
            else {
                HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
            }

            Status = MiFindEmptyAddressRangeDown (Process->VadRoot,
                                                  *CapturedViewSize,
                                                  HighestUserAddress,
                                                  Alignment,
                                                  &StartingAddress);
        }
        else {
            Status = MiFindEmptyAddressRange (*CapturedViewSize,
                                              Alignment,
                                              (ULONG)ZeroBits,
                                              &StartingAddress);
        }

        if (!NT_SUCCESS (Status)) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }

            MiDereferenceControlArea (ControlArea);
            return Status;
        }

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                    *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

        if (ZeroBits != 0) {
            if (EndingAddress > (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {
                if (ControlArea->FilePointer != NULL) {
                    MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                     LastPteOffset);
                }
                MiDereferenceControlArea (ControlArea);
                return STATUS_NO_MEMORY;
            }
        }
    }
    else {

        if (*CapturedBase == NULL) {

            //
            // The section is based.
            //

#if defined(_WIN64)
            SizeOfSection = SectionOffset->QuadPart;
#else
            SizeOfSection = SectionOffset->LowPart;
#endif

            StartingAddress = (PVOID)((PCHAR)CapturedStartingVa + SizeOfSection);
        }
        else {

            StartingAddress = MI_ALIGN_TO_SIZE (*CapturedBase, Alignment);

        }

        //
        // Check to make sure the specified base address to ending address
        // is currently unused.
        //

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                   *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

        if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }
            MiDereferenceControlArea (ControlArea);

            return STATUS_CONFLICTING_ADDRESSES;
        }
    }

    //
    // An unoccupied address range has been found, build the virtual
    // address descriptor to describe this range.
    //

    if (AllocationType & MEM_RESERVE) {
        VadSize = sizeof (MMVAD_LONG);
        Vad = ExAllocatePoolWithTag (NonPagedPool, VadSize, 'ldaV');
    }
    else {
        VadSize = sizeof (MMVAD);
        Vad = ExAllocatePoolWithTag (NonPagedPool, VadSize, MMVADKEY);
    }

    if (Vad == NULL) {
        if (ControlArea->FilePointer != NULL) {
            MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                             LastPteOffset);
        }
        MiDereferenceControlArea (ControlArea);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Vad, VadSize);

    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->FirstPrototypePte = TheFirstPrototypePte;

    //
    // Set the protection in the PTE template field of the VAD.
    //

    Vad->ControlArea = ControlArea;

    Vad->u2.VadFlags2.Inherit = (InheritDisposition == ViewShare);
    Vad->u.VadFlags.Protection = ProtectionMask;
    Vad->u2.VadFlags2.CopyOnWrite = CapturedCopyOnWrite;

    //
    // Note that MEM_DOS_LIM significance is lost here, but those
    // files are not mapped MEM_RESERVE.
    //

    Vad->u2.VadFlags2.FileOffset = (ULONG)(SectionOffset->QuadPart >> 16);

    if ((AllocationType & SEC_NO_CHANGE) || (Section->u.Flags.NoChange)) {
        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.VadFlags2.SecNoChange = 1;
    }

    if (AllocationType & MEM_RESERVE) {
        PMMEXTEND_INFO ExtendInfo;
        PMMVAD_LONG VadLong;

        Vad->u2.VadFlags2.LongVad = 1;

        ExAcquireFastMutexUnsafe (&MmSectionBasedMutex);
        ExtendInfo = Segment->ExtendInfo;
        if (ExtendInfo) {
            ExtendInfo->ReferenceCount += 1;
        }
        else {

            ExtendInfo = ExAllocatePoolWithTag (NonPagedPool,
                                                sizeof(MMEXTEND_INFO),
                                                'xCmM');
            if (ExtendInfo == NULL) {
                ExReleaseFastMutexUnsafe (&MmSectionBasedMutex);

                if (ControlArea->FilePointer != NULL) {
                    MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                     LastPteOffset);
                }
                MiDereferenceControlArea (ControlArea);
        
                //
                // The pool allocation succeeded, but the quota charge
                // in InsertVad failed, deallocate the pool and return
                // an error.
                //
    
                ExFreePool (Vad);
                return STATUS_INSUFFICIENT_RESOURCES;
            }
            ExtendInfo->ReferenceCount = 1;
            ExtendInfo->CommittedSize = Segment->SizeOfSegment;
            Segment->ExtendInfo = ExtendInfo;
        }
        if (ExtendInfo->CommittedSize < (UINT64)Section->SizeOfSection.QuadPart) {
            ExtendInfo->CommittedSize = (UINT64)Section->SizeOfSection.QuadPart;
        }
        ExReleaseFastMutexUnsafe (&MmSectionBasedMutex);
        Vad->u2.VadFlags2.ExtendableFile = 1;

        VadLong = (PMMVAD_LONG) Vad;

        ASSERT (VadLong->u4.ExtendedInfo == NULL);
        VadLong->u4.ExtendedInfo = ExtendInfo;
    }

    //
    // If the page protection is write-copy or execute-write-copy
    // charge for each page in the view as it may become private.
    //

    if (MI_IS_PTE_PROTECTION_COPY_WRITE(ProtectionMask)) {
        Vad->u.VadFlags.CommitCharge = (BYTES_TO_PAGES ((PCHAR) EndingAddress -
                           (PCHAR) StartingAddress));
    }

    PteOffset += (ULONG)(Vad->EndingVpn - Vad->StartingVpn);

    if (PteOffset < Subsection->PtesInSubsection) {
        Vad->LastContiguousPte = &Subsection->SubsectionBase[PteOffset];

    }
    else {
        Vad->LastContiguousPte = &Subsection->SubsectionBase[
                                    (Subsection->PtesInSubsection - 1) +
                                    Subsection->UnusedPtes];
    }

    if (QuotaCharge != 0) {
        if (MiChargeCommitment (QuotaCharge, NULL) == FALSE) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }
            MiDereferenceControlArea (ControlArea);
    
            ExFreePool (Vad);
            return STATUS_COMMITMENT_LIMIT;
        }
    }

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad (Vad);

    UNLOCK_WS_UNSAFE (Process);

    if (!NT_SUCCESS(Status)) {

        if (ControlArea->FilePointer != NULL) {
            MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                             LastPteOffset);
        }

        MiDereferenceControlArea (ControlArea);

        //
        // The quota charge in InsertVad failed, deallocate the pool and return
        // an error.
        //

        ExFreePool (Vad);
        if (QuotaCharge != 0) {
            MiReturnCommitment (QuotaCharge);
        }
        return Status;
    }

    //
    // Stash the first mapped address for the performance analysis tools.
    // Note this is not synchronized across multiple processes but that's ok.
    //

    if (ControlArea->FilePointer == NULL) {
        if (Segment->u2.FirstMappedVa == NULL) {
            Segment->u2.FirstMappedVa = StartingAddress;
        }
    }

#if DBG
    if (!(AllocationType & MEM_RESERVE)) {
        ASSERT(((ULONG_PTR)EndingAddress - (ULONG_PTR)StartingAddress) <=
                ROUND_TO_PAGES64(Segment->SizeOfSegment));
    }
#endif

    //
    // If a commit size was specified, make sure those pages are committed.
    //

    if (QuotaCharge != 0) {

        PointerPte = Vad->FirstPrototypePte;
        LastPte = PointerPte + BYTES_TO_PAGES(CommitSize);
        TempPte = Segment->SegmentPteTemplate;
        QuotaExcess = 0;

        ExAcquireFastMutexUnsafe (&MmSectionCommitMutex);

        while (PointerPte < LastPte) {

            if (PointerPte->u.Long == 0) {

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                QuotaExcess += 1;
            }
            PointerPte += 1;
        }

        ASSERT (QuotaCharge >= QuotaExcess);
        QuotaCharge -= QuotaExcess;

        MM_TRACK_COMMIT (MM_DBG_COMMIT_MAPVIEW_DATA, QuotaCharge);

        Segment->NumberOfCommittedPages += QuotaCharge;

        ASSERT (Segment->NumberOfCommittedPages <= Segment->TotalNumberOfPtes);

        ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);

        InterlockedExchangeAddSizeT (&MmSharedCommit, QuotaCharge);

        if (QuotaExcess != 0) {
            MiReturnCommitment (QuotaExcess);
        }
    }

#if defined(_MIALT4K_)

    if (Process->Wow64Process != NULL) {


        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                 *CapturedViewSize - 1L) | (PAGE_4K - 1L));

        ViewSizeFor4k = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;

        if (ControlArea->FilePointer != NULL) {

            AltFlags = (AllocationType & MEM_RESERVE) ? 0 : ALT_COMMIT;

            MiProtectFor4kPage (StartingAddress,
                                ViewSizeFor4k,
                                ProtectionMask,
                                (ALT_ALLOCATE | AltFlags),
                                Process);
        }
        else {

            MiProtectMapFileFor4kPage (StartingAddress,
                                       ViewSizeFor4k,
                                       ProtectionMask, 
                                       CommitSize,
                                       Vad->FirstPrototypePte,
                                       Vad->LastContiguousPte,
                                       Process);
        }
    }
#endif

    //
    // Update the current virtual size in the process header.
    //

    *CapturedViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    Process->VirtualSize += *CapturedViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

    *CapturedBase = StartingAddress;

    //
    // Update the count of writable user mappings so the transaction semantics
    // can be supported.  Note that no lock synchronization is needed here as
    // the transaction manager must already check for any open writable handles
    // to the file - and no writable sections can be created without a writable
    // file handle.  So all that needs to be provided is a way for the
    // transaction manager to know that there are lingering views or created
    // sections still open that have write access.
    //

    if ((ProtectionMask & MM_READWRITE) &&
        (ControlArea->FilePointer != NULL)) {

#if 0
        //
        // The section may no longer exist at this point so these ASSERTs
        // cannot be enabled.
        //

        ASSERT (Section->u.Flags.UserWritable == 1);
        ASSERT (Section->InitialPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE));
#endif

        InterlockedIncrement ((PLONG)&Segment->WritableUserReferences);
    }

    return STATUS_SUCCESS;
}

LOGICAL
MiCheckPurgeAndUpMapCount (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine synchronizes with any on going purge operations
    on the same segment (identified via the control area).  If
    another purge operation is occurring, the function blocks until
    it is completed.

    When this function returns the MappedView and the NumberOfUserReferences
    count for the control area will be incremented thereby referencing
    the control area.

Arguments:

    ControlArea - Supplies the control area for the segment to be purged.

Return Value:

    TRUE if the synchronization was successful.
    FALSE if the synchronization did not occur due to low resources, etc.

Environment:

    Kernel Mode.

--*/

{
    KIRQL OldIrql;
    PEVENT_COUNTER PurgedEvent;
    PEVENT_COUNTER WaitEvent;
    ULONG OldRef;
    PKTHREAD CurrentThread;

    PurgedEvent = NULL;
    OldRef = 1;

    LOCK_PFN (OldIrql);

    while (ControlArea->u.Flags.BeingPurged != 0) {

        //
        // A purge operation is in progress.
        //

        if (PurgedEvent == NULL) {

            //
            // Release the locks and allocate pool for the event.
            //

            UNLOCK_PFN (OldIrql);
            PurgedEvent = MiGetEventCounter ();
            if (PurgedEvent == NULL) {
                return FALSE;
            }

            LOCK_PFN (OldIrql);
            continue;
        }

        if (ControlArea->WaitingForDeletion == NULL) {
            ControlArea->WaitingForDeletion = PurgedEvent;
            WaitEvent = PurgedEvent;
            PurgedEvent = NULL;
        }
        else {
            WaitEvent = ControlArea->WaitingForDeletion;

            //
            // No interlock is needed for the RefCount increment as
            // no thread can be decrementing it since it is still
            // pointed to by the control area.
            //

            WaitEvent->RefCount += 1;
        }

        //
        // Release the PFN lock and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_PFN_AND_THEN_WAIT(OldIrql);

        KeWaitForSingleObject(&WaitEvent->Event,
                              WrVirtualMemory,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);

        //
        // Before this event can be set, the control area WaitingForDeletion
        // field must be cleared (and may be reinitialized to something else),
        // but cannot be reset to our local event.  This allows us to
        // dereference the event count lock free.
        //

        ASSERT (WaitEvent != ControlArea->WaitingForDeletion);

        MiFreeEventCounter (WaitEvent);

        LOCK_PFN (OldIrql);
        KeLeaveCriticalRegionThread (CurrentThread);
    }

    //
    // Indicate another file is mapped for the segment.
    //

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfUserReferences += 1;
    ControlArea->u.Flags.HadUserReference = 1;
    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    UNLOCK_PFN (OldIrql);

    if (PurgedEvent != NULL) {
        MiFreeEventCounter (PurgedEvent);
    }

    return TRUE;
}

typedef struct _NTSYM {
    struct _NTSYM *Next;
    PVOID SymbolTable;
    ULONG NumberOfSymbols;
    PVOID StringTable;
    USHORT Flags;
    USHORT EntrySize;
    ULONG MinimumVa;
    ULONG MaximumVa;
    PCHAR MapName;
    ULONG MapNameLen;
} NTSYM, *PNTSYM;

ULONG
CacheImageSymbols(
    IN PVOID ImageBase
    )
{
    PIMAGE_DEBUG_DIRECTORY DebugDirectory;
    ULONG DebugSize;

    PAGED_CODE();

    try {
        DebugDirectory = (PIMAGE_DEBUG_DIRECTORY)
        RtlImageDirectoryEntryToData( ImageBase,
                                      TRUE,
                                      IMAGE_DIRECTORY_ENTRY_DEBUG,
                                      &DebugSize
                                    );
        if (!DebugDirectory) {
            return FALSE;
        }

        //
        // If using remote KD, ImageBase is what it wants to see.
        //

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return FALSE;
    }

    return TRUE;
}


NTSYSAPI
NTSTATUS
NTAPI
NtAreMappedFilesTheSame (
    IN PVOID File1MappedAsAnImage,
    IN PVOID File2MappedAsFile
    )

/*++

Routine Description:

    This routine compares the two files mapped at the specified
    addresses to see if they are both the same file.

Arguments:

    File1MappedAsAnImage - Supplies an address within the first file which
        is mapped as an image file.

    File2MappedAsFile - Supplies an address within the second file which
        is mapped as either an image file or a data file.

Return Value:


    STATUS_SUCCESS is returned if the two files are the same.

    STATUS_NOT_SAME_DEVICE is returned if the files are different.

    Other status values can be returned if the addresses are not mapped as
    files, etc.

Environment:

    User mode callable system service.

--*/

{
    PMMVAD FoundVad1;
    PMMVAD FoundVad2;
    NTSTATUS Status;
    PEPROCESS Process;

    Process = PsGetCurrentProcess();

    LOCK_ADDRESS_SPACE (Process);

    FoundVad1 = MiLocateAddress (File1MappedAsAnImage);
    FoundVad2 = MiLocateAddress (File2MappedAsFile);

    if ((FoundVad1 == NULL) || (FoundVad2 == NULL)) {

        //
        // No virtual address is allocated at the specified base address,
        // return an error.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    //
    // Check file names.
    //

    if ((FoundVad1->u.VadFlags.PrivateMemory == 1) ||
        (FoundVad2->u.VadFlags.PrivateMemory == 1)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    if ((FoundVad1->ControlArea == NULL) ||
        (FoundVad2->ControlArea == NULL)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    if ((FoundVad1->ControlArea->FilePointer == NULL) ||
        (FoundVad2->ControlArea->FilePointer == NULL)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    Status = STATUS_NOT_SAME_DEVICE;

    if ((PVOID)FoundVad1->ControlArea ==
            FoundVad2->ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject) {
        Status = STATUS_SUCCESS;
    }

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}



NTSTATUS
MiSetPageModified (
    IN PMMVAD Vad,
    IN PVOID Address
    )

/*++

Routine Description:

    This routine sets the modified bit in the PFN database for the
    pages that correspond to the specified address range.

    Note that the dirty bit in the PTE is cleared by this operation.

Arguments:

    Vad - Supplies the VAD to charge.

    Address - Supplies the user address to access.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  Address space mutex held.

--*/

{
    PMMPFN Pfn1;
    NTSTATUS Status;
    PEPROCESS CurrentProcess;
    SIZE_T RealCharge;
    MMPTE PteContents;
    KIRQL OldIrql;
    volatile PMMPTE PointerPxe;
    volatile PMMPTE PointerPpe;
    volatile PMMPTE PointerPde;
    volatile PMMPTE PointerPte;
    LOGICAL ReturnCommitment;
    LOGICAL ChargedJobCommit;

    //
    // Charge commitment up front even though we may not use it because
    // failing to get it later would make things messy.
    //

    RealCharge = 1;
    ReturnCommitment = FALSE;
    ChargedJobCommit = FALSE;

    CurrentProcess = PsGetCurrentProcess ();

    Status = PsChargeProcessPageFileQuota (CurrentProcess, RealCharge);
    if (!NT_SUCCESS (Status)) {
        return STATUS_COMMITMENT_LIMIT;
    }

    if (CurrentProcess->CommitChargeLimit) {
        if (CurrentProcess->CommitCharge + RealCharge > CurrentProcess->CommitChargeLimit) {
            if (CurrentProcess->Job) {
                PsReportProcessMemoryLimitViolation ();
            }
            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return STATUS_COMMITMENT_LIMIT;
        }
    }
    if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
        if (PsChangeJobMemoryUsage(RealCharge) == FALSE) {
            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return STATUS_COMMITMENT_LIMIT;
        }
        ChargedJobCommit = TRUE;
    }

    if (MiChargeCommitment (RealCharge, NULL) == FALSE) {
        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
        }
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
        return STATUS_COMMITMENT_LIMIT;
    }

    CurrentProcess->CommitCharge += RealCharge;

    if (CurrentProcess->CommitCharge > CurrentProcess->CommitChargePeak) {
        CurrentProcess->CommitChargePeak = CurrentProcess->CommitCharge;
    }

    MI_INCREMENT_TOTAL_PROCESS_COMMIT (RealCharge);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD, RealCharge);

    //
    // Loop on the copy on write case until the page is only
    // writable.
    //

    PointerPte = MiGetPteAddress (Address);
    PointerPde = MiGetPdeAddress (Address);
    PointerPpe = MiGetPpeAddress (Address);
    PointerPxe = MiGetPxeAddress (Address);

    try {
        *(volatile CCHAR *)Address;
    } except (EXCEPTION_EXECUTE_HANDLER) {
        CurrentProcess->CommitCharge -= RealCharge;
        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
        }
        MiReturnCommitment (RealCharge);
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
        return GetExceptionCode();
    }

    LOCK_PFN (OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
    while ((PointerPxe->u.Hard.Valid == 0) ||
           (PointerPpe->u.Hard.Valid == 0) ||
           (PointerPde->u.Hard.Valid == 0) ||
           (PointerPte->u.Hard.Valid == 0))
#elif (_MI_PAGING_LEVELS >= 3)
    while ((PointerPpe->u.Hard.Valid == 0) ||
           (PointerPde->u.Hard.Valid == 0) ||
           (PointerPte->u.Hard.Valid == 0))
#else
    while ((PointerPde->u.Hard.Valid == 0) ||
           (PointerPte->u.Hard.Valid == 0))
#endif
    {
        //
        // Page is no longer valid.
        //

        UNLOCK_PFN (OldIrql);
        try {
            *(volatile CCHAR *)Address;
        } except (EXCEPTION_EXECUTE_HANDLER) {
            CurrentProcess->CommitCharge -= RealCharge;
            if (ChargedJobCommit == TRUE) {
                PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
            }
            MiReturnCommitment (RealCharge);
            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return GetExceptionCode();
        }
        LOCK_PFN (OldIrql);
    }

    PteContents = *PointerPte;

    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

    MI_SET_MODIFIED (Pfn1, 1, 0x8);

    if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {
        if (Pfn1->u3.e1.WriteInProgress == 0) {
            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        //
        // We didn't need to (and shouldn't have) charged commitment for
        // this page as it was already pagefile backed (ie: someone else
        // already charged commitment for it earlier).
        //

        ReturnCommitment = TRUE;
    }

#ifdef NT_UP
    if (MI_IS_PTE_DIRTY (PteContents)) {
#endif //NT_UP
        MI_SET_PTE_CLEAN (PteContents);

        //
        // Clear the dirty bit in the PTE so new writes can be tracked.
        //

        (VOID)KeFlushSingleTb (Address,
                               FALSE,
                               TRUE,
                               (PHARDWARE_PTE)PointerPte,
                               PteContents.u.Flush);
#ifdef NT_UP
    }
#endif //NT_UP

    UNLOCK_PFN (OldIrql);

    if (ReturnCommitment == TRUE) {
        CurrentProcess->CommitCharge -= RealCharge;
        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
        }
        MiReturnCommitment (RealCharge);
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
    }
    else {

        //
        // Commit has been charged for the copied page, add the charge
        // to the VAD now so it is automatically returned when the VAD is
        // deleted later.
        //

        MM_TRACK_COMMIT (MM_DBG_COMMIT_IMAGE, 1);

        ASSERT (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT);
        Vad->u.VadFlags.CommitCharge += 1;
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MmMapViewInSystemSpace (
    IN PVOID Section,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the system's address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, APC_LEVEL or below.

--*/

{
    PMMSESSION  Session;

    PAGED_CODE();

    Session = &MmSession;

    return MiMapViewInSystemSpace (Section,
                                   Session,
                                   MappedBase,
                                   ViewSize);
}


NTSTATUS
MmMapViewInSessionSpace (
    IN PVOID Section,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the current process's
    session address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, APC_LEVEL or below.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);
    Session = &MmSessionSpace->Session;

    return MiMapViewInSystemSpace (Section,
                                   Session,
                                   MappedBase,
                                   ViewSize);
}

//
// Nonpaged wrapper to set the flag...
//

VOID
MiMarkImageMappedInSystemSpace (
    IN PCONTROL_AREA ControlArea
    )
{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    ControlArea->u.Flags.ImageMappedInSystemSpace = 1;
    UNLOCK_PFN (OldIrql);
}


NTSTATUS
MiMapViewInSystemSpace (
    IN PVOID Section,
    IN PMMSESSION Session,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the system's address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    Session - Supplies the session data structure for this view.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of APC_LEVEL or below.
    
--*/

{
    PVOID Base;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PCONTROL_AREA NewControlArea;
    ULONG StartBit;
    ULONG SizeIn64k;
    ULONG NewSizeIn64k;
    PMMPTE BasePte;
    ULONG NumberOfPtes;
    SIZE_T NumberOfBytes;
    LOGICAL status;
    KIRQL WsIrql;
    SIZE_T SectionSize;
    NTSTATUS Status;

    PAGED_CODE();

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    ControlArea = ((PSECTION)Section)->Segment->ControlArea;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (MiCheckPurgeAndUpMapCount (ControlArea) == FALSE) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

#if defined (_WIN64)
    SectionSize = ((PSECTION)Section)->SizeOfSection.QuadPart;
#else
    SectionSize = ((PSECTION)Section)->SizeOfSection.LowPart;
#endif

    if (*ViewSize == 0) {

        *ViewSize = SectionSize;

    }
    else if (*ViewSize > SectionSize) {

        //
        // Section offset or view size past size of section.
        //

        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    //
    // Calculate the first prototype PTE field in the Vad.
    //

    SizeIn64k = (ULONG)((*ViewSize / X64K) + ((*ViewSize & (X64K - 1)) != 0));

    //
    // 4GB-64K is the maximum individual view size allowed since we encode
    // this into the bottom 16 bits of the MMVIEW entry.
    //

    if (SizeIn64k >= X64K) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    Base = MiInsertInSystemSpace (Session, SizeIn64k, ControlArea);

    if (Base == NULL) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_NO_MEMORY;
    }

    NumberOfBytes = (SIZE_T)SizeIn64k * X64K;

    if (Session == &MmSession) {
        MiFillSystemPageDirectory (Base, NumberOfBytes);
        status = TRUE;

        //
        // Initializing WsIrql is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        WsIrql = 0x99;
    }
    else {
        LOCK_SESSION_SPACE_WS (WsIrql, PsGetCurrentThread ());
        if (NT_SUCCESS(MiSessionCommitPageTables (Base,
                (PVOID)((ULONG_PTR)Base + NumberOfBytes - 1)))) {
                    status = TRUE;
        }
        else {
                    status = FALSE;
        }
    }

    if (status == FALSE) {

        Status = STATUS_NO_MEMORY;
bail:

        if (Session != &MmSession) {
            UNLOCK_SESSION_SPACE_WS (WsIrql);
        }

        MiDereferenceControlArea (ControlArea);

        StartBit = (ULONG) (((ULONG_PTR)Base - (ULONG_PTR)Session->SystemSpaceViewStart) >> 16);

        LOCK_SYSTEM_VIEW_SPACE (Session);

        NewSizeIn64k = MiRemoveFromSystemSpace (Session, Base, &NewControlArea);

        ASSERT (ControlArea == NewControlArea);
        ASSERT (SizeIn64k == NewSizeIn64k);

        RtlClearBits (Session->SystemSpaceBitMap, StartBit, SizeIn64k);

        UNLOCK_SYSTEM_VIEW_SPACE (Session);

        return Status;
    }

    //
    // Setup PTEs to point to prototype PTEs.
    //

    if (((PSECTION)Section)->u.Flags.Image) {

#if DBG
        //
        // The only reason this ASSERT isn't done for Hydra is because
        // the session space working set lock is currently held so faults
        // are not allowed.
        //

        if (Session == &MmSession) {
            ASSERT (((PSECTION)Section)->Segment->ControlArea == ControlArea);
        }
#endif

        MiMarkImageMappedInSystemSpace (ControlArea);
    }

    BasePte = MiGetPteAddress (Base);
    NumberOfPtes = BYTES_TO_PAGES (*ViewSize);

    Status = MiAddMappedPtes (BasePte, NumberOfPtes, ControlArea);

    if (!NT_SUCCESS (Status)) {

        //
        // Regardless of whether the PTEs were mapped, leave the control area
        // marked as mapped in system space so user applications cannot map the
        // file as an image as clearly the intent is to run it as a driver.
        //

        goto bail;
    }

    if (Session != &MmSession) {
        UNLOCK_SESSION_SPACE_WS (WsIrql);
    }

    *MappedBase = Base;

    return STATUS_SUCCESS;
}

VOID
MiFillSystemPageDirectory (
    IN PVOID Base,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This routine allocates page tables and fills the system page directory
    entries for the specified virtual address range.

Arguments:

    Base - Supplies the virtual address of the view.

    NumberOfBytes - Supplies the number of bytes the view spans.

Return Value:

    None.

Environment:

    Kernel Mode, IRQL of dispatch level.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    PMMPTE FirstPde;
    PMMPTE LastPde;
    PMMPTE FirstSystemPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
#if (_MI_PAGING_LEVELS < 3)
    ULONG i;
#endif

    PAGED_CODE();

    //
    // CODE IS ALREADY LOCKED BY CALLER.
    //

    FirstPde = MiGetPdeAddress (Base);
    LastPde = MiGetPdeAddress ((PVOID)(((PCHAR)Base) + NumberOfBytes - 1));

    PointerPpe = MiGetPpeAddress (Base);
    PointerPxe = MiGetPxeAddress (Base);

#if (_MI_PAGING_LEVELS >= 3)
    FirstSystemPde = FirstPde;
#else
    FirstSystemPde = &MmSystemPagePtes[((ULONG_PTR)FirstPde &
                     (PD_PER_SYSTEM * (PDE_PER_PAGE * sizeof(MMPTE)) - 1)) / sizeof(MMPTE) ];
#endif

    do {

#if (_MI_PAGING_LEVELS >= 4)
        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // No page directory page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (PointerPxe->u.Hard.Valid == 0) {

                if (MiEnsureAvailablePageOrWait (NULL, PointerPxe)) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPxe));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPxe, TempPte);

                MiInitializePfn (PageFrameIndex, PointerPxe, 1);

                MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPxe),
                                 PAGE_SIZE,
                                 MM_ZERO_KERNEL_PTE);
            }
            UNLOCK_PFN (OldIrql);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // No page directory page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (PointerPpe->u.Hard.Valid == 0) {

                if (MiEnsureAvailablePageOrWait (NULL, PointerPpe)) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPpe, TempPte);

                MiInitializePfn (PageFrameIndex, PointerPpe, 1);

                MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPpe),
                                 PAGE_SIZE,
                                 MM_ZERO_KERNEL_PTE);
            }
            UNLOCK_PFN (OldIrql);
        }
#endif

        if (FirstSystemPde->u.Hard.Valid == 0) {

            //
            // No page table page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (((volatile MMPTE *)FirstSystemPde)->u.Hard.Valid == 0) {

                if (MiEnsureAvailablePageOrWait (NULL, FirstPde)) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (FirstSystemPde));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (FirstSystemPde, TempPte);

                //
                // The FirstPde and FirstSystemPde may be identical even on
                // 32-bit machines if we are currently running in the
                // system process, so check for the valid bit first so we
                // don't assert on a checked build.
                //

                if (FirstPde->u.Hard.Valid == 0) {
                    MI_WRITE_VALID_PTE (FirstPde, TempPte);
                }

#if (_MI_PAGING_LEVELS >= 3)
                MiInitializePfn (PageFrameIndex, FirstPde, 1);
#else
                i = (FirstPde - MiGetPdeAddress(0)) / PDE_PER_PAGE;
                MiInitializePfnForOtherProcess (PageFrameIndex,
                                                FirstPde,
                                                MmSystemPageDirectory[i]);
#endif

                MiFillMemoryPte (MiGetVirtualAddressMappedByPte (FirstPde),
                                 PAGE_SIZE,
                                 MM_ZERO_KERNEL_PTE);
            }
            UNLOCK_PFN (OldIrql);
        }

        FirstSystemPde += 1;
        FirstPde += 1;
#if (_MI_PAGING_LEVELS >= 3)
        if (MiIsPteOnPdeBoundary (FirstPde)) {
            PointerPpe = MiGetPteAddress (FirstPde);
            if (MiIsPteOnPpeBoundary (FirstPde)) {
                PointerPxe = MiGetPdeAddress (FirstPde);
            }
        }
#endif
    } while (FirstPde <= LastPde);
}

NTSTATUS
MmUnmapViewInSystemSpace (
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    PAGED_CODE();

    return MiUnmapViewInSystemSpace (&MmSession, MappedBase);
}

NTSTATUS
MmUnmapViewInSessionSpace (
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);
    Session = &MmSessionSpace->Session;

    return MiUnmapViewInSystemSpace (Session, MappedBase);
}

NTSTATUS
MiUnmapViewInSystemSpace (
    IN PMMSESSION Session,
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    Session - Supplies the session data structure for this view.

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    ULONG StartBit;
    ULONG Size;
    PCONTROL_AREA ControlArea;
    PMMSUPPORT Ws;
    KIRQL WsIrql;

    PAGED_CODE();

    StartBit =  (ULONG) (((ULONG_PTR)MappedBase - (ULONG_PTR)Session->SystemSpaceViewStart) >> 16);

    LOCK_SYSTEM_VIEW_SPACE (Session);

    Size = MiRemoveFromSystemSpace (Session, MappedBase, &ControlArea);

    RtlClearBits (Session->SystemSpaceBitMap, StartBit, Size);

    //
    // Zero PTEs.
    //

    Size = Size * (X64K >> PAGE_SHIFT);

    if (Session == &MmSession) {
        Ws = &MmSystemCacheWs;
        MiRemoveMappedPtes (MappedBase, Size, ControlArea, Ws);
    }
    else {
        Ws = &MmSessionSpace->Vm;
        LOCK_SESSION_SPACE_WS(WsIrql, PsGetCurrentThread ());
        MiRemoveMappedPtes (MappedBase, Size, ControlArea, Ws);
        UNLOCK_SESSION_SPACE_WS(WsIrql);
    }

    UNLOCK_SYSTEM_VIEW_SPACE (Session);

    return STATUS_SUCCESS;
}


PVOID
MiInsertInSystemSpace (
    IN PMMSESSION Session,
    IN ULONG SizeIn64k,
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine creates a view in system space for the specified control
    area (file mapping).

Arguments:

    SizeIn64k - Supplies the size of the view to be created.

    ControlArea - Supplies a pointer to the control area for this view.

Return Value:

    Base address where the view was mapped, NULL if the view could not be
    mapped.

Environment:

    Kernel Mode.

--*/

{

    PVOID Base;
    ULONG_PTR Entry;
    ULONG Hash;
    ULONG i;
    ULONG AllocSize;
    PMMVIEW OldTable;
    ULONG StartBit;
    ULONG NewHashSize;
    POOL_TYPE PoolType;

    PAGED_CODE();

    ASSERT (SizeIn64k < X64K);

    //
    // CODE IS ALREADY LOCKED BY CALLER.
    //

    LOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceHashEntries + 8 > Session->SystemSpaceHashSize) {

        //
        // Less than 8 free slots, reallocate and rehash.
        //

        NewHashSize = Session->SystemSpaceHashSize << 1;

        AllocSize = sizeof(MMVIEW) * NewHashSize;

        OldTable = Session->SystemSpaceViewTable;

        //
        // The SystemSpaceViewTable for system (not session) space is only
        // allocated from nonpaged pool so it can be safely torn down during
        // clean shutdowns.  Otherwise it could be allocated from paged pool
        // just like the session SystemSpaceViewTable.
        //

        if (Session == &MmSession) {
            PoolType = NonPagedPool;
        }
        else {
            PoolType = PagedPool;
        }

        Session->SystemSpaceViewTable = ExAllocatePoolWithTag (PoolType,
                                                               AllocSize,
                                                               '  mM');

        if (Session->SystemSpaceViewTable == NULL) {
            Session->SystemSpaceViewTable = OldTable;
        }
        else {
            RtlZeroMemory (Session->SystemSpaceViewTable, AllocSize);

            Session->SystemSpaceHashSize = NewHashSize;
            Session->SystemSpaceHashKey = Session->SystemSpaceHashSize - 1;

            for (i = 0; i < (Session->SystemSpaceHashSize / 2); i += 1) {
                if (OldTable[i].Entry != 0) {
                    Hash = (ULONG) ((OldTable[i].Entry >> 16) % Session->SystemSpaceHashKey);

                    while (Session->SystemSpaceViewTable[Hash].Entry != 0) {
                        Hash += 1;
                        if (Hash >= Session->SystemSpaceHashSize) {
                            Hash = 0;
                        }
                    }
                    Session->SystemSpaceViewTable[Hash] = OldTable[i];
                }
            }
            ExFreePool (OldTable);
        }
    }

    if (Session->SystemSpaceHashEntries == Session->SystemSpaceHashSize) {

        //
        // There are no free hash slots to place a new entry into even
        // though there may still be unused virtual address space.
        //

        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return NULL;
    }

    StartBit = RtlFindClearBitsAndSet (Session->SystemSpaceBitMap,
                                       SizeIn64k,
                                       0);

    if (StartBit == NO_BITS_FOUND) {
        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return NULL;
    }

    Base = (PVOID)((PCHAR)Session->SystemSpaceViewStart + ((ULONG_PTR)StartBit * X64K));

    Entry = (ULONG_PTR) MI_64K_ALIGN(Base) + SizeIn64k;

    Hash = (ULONG) ((Entry >> 16) % Session->SystemSpaceHashKey);

    while (Session->SystemSpaceViewTable[Hash].Entry != 0) {
        Hash += 1;
        if (Hash >= Session->SystemSpaceHashSize) {
            Hash = 0;
        }
    }

    Session->SystemSpaceHashEntries += 1;

    Session->SystemSpaceViewTable[Hash].Entry = Entry;
    Session->SystemSpaceViewTable[Hash].ControlArea = ControlArea;

    UNLOCK_SYSTEM_VIEW_SPACE (Session);
    return Base;
}


ULONG
MiRemoveFromSystemSpace (
    IN PMMSESSION Session,
    IN PVOID Base,
    OUT PCONTROL_AREA *ControlArea
    )

/*++

Routine Description:

    This routine looks up the specified view in the system space hash
    table and unmaps the view from system space and the table.

Arguments:

    Session - Supplies the session data structure for this view.

    Base - Supplies the base address for the view.  If this address is
           NOT found in the hash table, the system bugchecks.

    ControlArea - Returns the control area corresponding to the base
                  address.

Return Value:

    Size of the view divided by 64k.

Environment:

    Kernel Mode, system view hash table locked.

--*/

{
    ULONG_PTR Base16;
    ULONG Hash;
    ULONG Size;
    ULONG count;

    PAGED_CODE();

    count = 0;

    Base16 = (ULONG_PTR)Base >> 16;
    Hash = (ULONG)(Base16 % Session->SystemSpaceHashKey);

    while ((Session->SystemSpaceViewTable[Hash].Entry >> 16) != Base16) {
        Hash += 1;
        if (Hash >= Session->SystemSpaceHashSize) {
            Hash = 0;
            count += 1;
            if (count == 2) {
                KeBugCheckEx (DRIVER_UNMAPPING_INVALID_VIEW,
                              (ULONG_PTR)Base,
                              1,
                              0,
                              0);
            }
        }
    }

    Session->SystemSpaceHashEntries -= 1;
    Size = (ULONG) (Session->SystemSpaceViewTable[Hash].Entry & 0xFFFF);
    Session->SystemSpaceViewTable[Hash].Entry = 0;
    *ControlArea = Session->SystemSpaceViewTable[Hash].ControlArea;
    return Size;
}


LOGICAL
MiInitializeSystemSpaceMap (
    PVOID InputSession OPTIONAL
    )

/*++

Routine Description:

    This routine initializes the tables for mapping views into system space.
    Views are kept in a multiple of 64k bytes in a growable hashed table.

Arguments:

    InputSession - Supplies NULL if this is the initial system session
                   (non-Hydra), a valid session pointer (the pointer must
                   be in global space, not session space) for Hydra session
                   initialization.

Return Value:

    TRUE on success, FALSE on failure.

Environment:

    Kernel Mode, initialization.

--*/

{
    SIZE_T AllocSize;
    SIZE_T Size;
    PCHAR ViewStart;
    PMMSESSION Session;
    POOL_TYPE PoolType;

    if (ARGUMENT_PRESENT (InputSession)) {
        Session = (PMMSESSION)InputSession;
        ViewStart = (PCHAR) MiSessionViewStart;
        Size = MmSessionViewSize;
    }
    else {
        Session = &MmSession;
        ViewStart = (PCHAR)MiSystemViewStart;
        Size = MmSystemViewSize;
    }

    //
    // We are passed a system global address for the address of the session.
    // Save a global pointer to the mutex below because multiple sessions will
    // generally give us a session-space (not a global space) pointer to the
    // MMSESSION in subsequent calls.  We need the global pointer for the mutex
    // field for the kernel primitives to work properly.
    //

    Session->SystemSpaceViewLockPointer = &Session->SystemSpaceViewLock;
    ExInitializeFastMutex(Session->SystemSpaceViewLockPointer);

    //
    // If the kernel image has not been biased to allow for 3gb of user space,
    // then the system space view starts at the defined place. Otherwise, it
    // starts 16mb above the kernel image.
    //

    Session->SystemSpaceViewStart = ViewStart;

    MiCreateBitMap (&Session->SystemSpaceBitMap, Size / X64K, NonPagedPool);
    if (Session->SystemSpaceBitMap == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        return FALSE;
    }

    RtlClearAllBits (Session->SystemSpaceBitMap);

    //
    // Build the view table.
    //

    Session->SystemSpaceHashSize = 31;
    Session->SystemSpaceHashKey = Session->SystemSpaceHashSize - 1;
    Session->SystemSpaceHashEntries = 0;

    AllocSize = sizeof(MMVIEW) * Session->SystemSpaceHashSize;
    ASSERT (AllocSize < PAGE_SIZE);

    //
    // The SystemSpaceViewTable for system (not session) space is only
    // allocated from nonpaged pool so it can be safely torn down during
    // clean shutdowns.  Otherwise it could be allocated from paged pool
    // just like the session SystemSpaceViewTable.
    //

    if (Session == &MmSession) {
        PoolType = NonPagedPool;
    }
    else {
        PoolType = PagedPool;
    }

    Session->SystemSpaceViewTable = ExAllocatePoolWithTag (PoolType,
                                                           AllocSize,
                                                           '  mM');

    if (Session->SystemSpaceViewTable == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_SESSION_PAGED_POOL);
        MiRemoveBitMap (&Session->SystemSpaceBitMap);
        return FALSE;
    }

    RtlZeroMemory (Session->SystemSpaceViewTable, AllocSize);

    return TRUE;
}


VOID
MiFreeSessionSpaceMap (
    VOID
    )

/*++

Routine Description:

    This routine frees the tables used for mapping session views.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode.  The caller must be in the correct session context.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    Session = &MmSessionSpace->Session;

    //
    // Check for leaks of objects in the view table.
    //

    LOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceViewTable && Session->SystemSpaceHashEntries) {

        KeBugCheckEx (SESSION_HAS_VALID_VIEWS_ON_EXIT,
                      (ULONG_PTR)MmSessionSpace->SessionId,
                      Session->SystemSpaceHashEntries,
                      (ULONG_PTR)&Session->SystemSpaceViewTable[0],
                      Session->SystemSpaceHashSize);
#if 0
        ULONG Index;

        for (Index = 0; Index < Session->SystemSpaceHashSize; Index += 1) {

            PMMVIEW Table;
            PVOID Base;

            Table = &Session->SystemSpaceViewTable[Index];

            if (Table->Entry) {

#if DBG
                DbgPrint ("MM: MiFreeSessionSpaceMap: view entry %d leak: ControlArea %p, Addr %p, Size %d\n",
                    Index,
                    Table->ControlArea,
                    Table->Entry & ~0xFFFF,
                    Table->Entry & 0x0000FFFF
                );
#endif

                Base = (PVOID)(Table->Entry & ~0xFFFF);

                //
                // MiUnmapViewInSystemSpace locks the ViewLock.
                //

                UNLOCK_SYSTEM_VIEW_SPACE(Session);

                MiUnmapViewInSystemSpace (Session, Base);

                LOCK_SYSTEM_VIEW_SPACE (Session);

                //
                // The view table may have been deleted while we let go of
                // the lock.
                //

                if (Session->SystemSpaceViewTable == NULL) {
                    break;
                }
            }
        }
#endif

    }

    UNLOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceViewTable) {
        ExFreePool (Session->SystemSpaceViewTable);
        Session->SystemSpaceViewTable = NULL;
    }

    if (Session->SystemSpaceBitMap) {
        MiRemoveBitMap (&Session->SystemSpaceBitMap);
    }
}


HANDLE
MmSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode
    )

/*++

Routine Description:

    This routine probes the requested address range and protects
    the specified address range from having its protection made
    more restricted and being deleted.

    MmUnsecureVirtualMemory is used to allow the range to return
    to a normal state.

Arguments:

    Address - Supplies the base address to probe and secure.

    Size - Supplies the size of the range to secure.

    ProbeMode - Supplies one of PAGE_READONLY or PAGE_READWRITE.

Return Value:

    Returns a handle to be used to unsecure the range.
    If the range could not be locked because of protection
    problems or noncommitted memory, the value (HANDLE)0
    is returned.

Environment:

    Kernel Mode.

--*/

{
    return MiSecureVirtualMemory (Address, Size, ProbeMode, FALSE);
}


HANDLE
MiSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode,
    IN LOGICAL AddressSpaceMutexHeld
    )

/*++

Routine Description:

    This routine probes the requested address range and protects
    the specified address range from having its protection made
    more restricted and being deleted.

    MmUnsecureVirtualMemory is used to allow the range to return
    to a normal state.

Arguments:

    Address - Supplies the base address to probe and secure.

    Size - Supplies the size of the range to secure.

    ProbeMode - Supplies one of PAGE_READONLY or PAGE_READWRITE.

    AddressSpaceMutexHeld - Supplies TRUE if the mutex is already held, FALSE
                            if not.

Return Value:

    Returns a handle to be used to unsecure the range.
    If the range could not be locked because of protection
    problems or noncommitted memory, the value (HANDLE)0
    is returned.

Environment:

    Kernel Mode.

--*/

{
    PETHREAD Thread;
    ULONG_PTR EndAddress;
    PVOID StartAddress;
    CHAR Temp;
    ULONG Probe;
    HANDLE Handle;
    PMMVAD Vad;
    PMMVAD_LONG NewVad;
    PMMSECURE_ENTRY Secure;
    PEPROCESS Process;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    ULONG Waited;
#if defined(_WIN64)
    ULONG_PTR PageSize;
#else
    #define PageSize PAGE_SIZE
#endif

    PAGED_CODE();

    if ((ULONG_PTR)Address + Size > (ULONG_PTR)MM_HIGHEST_USER_ADDRESS || (ULONG_PTR)Address + Size <= (ULONG_PTR)Address) {
        return NULL;
    }

    Handle = NULL;

    Probe = (ProbeMode == PAGE_READONLY);

    Thread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (Thread);

    StartAddress = Address;

    if (AddressSpaceMutexHeld == FALSE) {
        LOCK_ADDRESS_SPACE (Process);
    }

    //
    // Check for a private committed VAD first instead of probing to avoid all
    // the page faults and zeroing.  If we find one, then we run the PTEs
    // instead.
    //

    if (Size >= 64 * 1024) {
        EndAddress = (ULONG_PTR)StartAddress + Size - 1;
        Vad = MiLocateAddress (StartAddress);

        if (Vad == NULL) {
            goto Return1;
        }

        if (Vad->u.VadFlags.UserPhysicalPages == 1) {
            goto Return1;
        }

        if (Vad->u.VadFlags.MemCommit == 0) {
            goto LongWay;
        }

        if (Vad->u.VadFlags.PrivateMemory == 0) {
            goto LongWay;
        }

        if (Vad->u.VadFlags.PhysicalMapping == 1) {
            goto LongWay;
        }

        ASSERT (Vad->u.VadFlags.Protection);

        if ((MI_VA_TO_VPN (StartAddress) < Vad->StartingVpn) ||
            (MI_VA_TO_VPN (EndAddress) > Vad->EndingVpn)) {
            goto Return1;
        }

        if (Vad->u.VadFlags.Protection == MM_NOACCESS) {
            goto LongWay;
        }

        if (ProbeMode == PAGE_READONLY) {
            if (Vad->u.VadFlags.Protection > MM_EXECUTE_WRITECOPY) {
                goto LongWay;
            }
        }
        else {
            if (Vad->u.VadFlags.Protection != MM_READWRITE &&
                Vad->u.VadFlags.Protection != MM_EXECUTE_READWRITE) {
                    goto LongWay;
            }
        }

        //
        // Check individual page permissions.
        //

        PointerPde = MiGetPdeAddress (StartAddress);
        PointerPpe = MiGetPteAddress (PointerPde);
        PointerPxe = MiGetPdeAddress (PointerPde);
        PointerPte = MiGetPteAddress (StartAddress);
        LastPte = MiGetPteAddress ((PVOID)EndAddress);

        LOCK_WS_UNSAFE (Process);

        do {

            while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                               Process,
                                               FALSE,
                                               &Waited) == FALSE) {
                //
                // Extended page directory parent entry is empty, go
                // to the next one.
                //

                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
            }

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                               Process,
                                               FALSE,
                                               &Waited) == FALSE) {
                //
                // Page directory parent entry is empty, go to the next one.
                //

                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary (PointerPpe)) {
                    PointerPxe = MiGetPteAddress(PointerPpe);
                    Waited = 1;
                    goto restart;
                }
#endif
            }

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            while (MiDoesPdeExistAndMakeValid (PointerPde,
                                               Process,
                                               FALSE,
                                               &Waited) == FALSE) {
                //
                // This page directory entry is empty, go to the next one.
                //

                PointerPde += 1;
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
#if (_MI_PAGING_LEVELS >= 3)
                if (MiIsPteOnPdeBoundary (PointerPde)) {
                    PointerPxe = MiGetPteAddress(PointerPpe);
                    Waited = 1;
                    break;
                }
#endif
            }

#if (_MI_PAGING_LEVELS >= 4)
restart:
            PointerPxe = PointerPxe;        // satisfy the compiler
#endif

        } while (Waited != 0);

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);

                do {

                    while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                                       Process,
                                                       FALSE,
                                                       &Waited) == FALSE) {
                        //
                        // Page directory parent entry is empty, go to the next one.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
                    }

#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif

                    while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                                       Process,
                                                       FALSE,
                                                       &Waited) == FALSE) {
                        //
                        // Page directory parent entry is empty, go to the next one.
                        //

                        PointerPpe += 1;
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPdeBoundary (PointerPpe)) {
                            PointerPxe = MiGetPteAddress (PointerPpe);
                            Waited = 1;
                            goto restart2;
                        }
#endif
                    }

#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif

                    while (MiDoesPdeExistAndMakeValid (PointerPde,
                                                       Process,
                                                       FALSE,
                                                       &Waited) == FALSE) {
                        //
                        // This page directory entry is empty, go to the next one.
                        //

                        PointerPde += 1;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
#if (_MI_PAGING_LEVELS >= 3)
                        if (MiIsPteOnPdeBoundary (PointerPde)) {
                            PointerPxe = MiGetPteAddress (PointerPpe);
                            Waited = 1;
                            break;
                        }
#endif
                    }

#if (_MI_PAGING_LEVELS >= 4)
restart2:
            PointerPxe = PointerPxe;        // satisfy the compiler
#endif
                } while (Waited != 0);
            }
            if (PointerPte->u.Long) {
                UNLOCK_WS_UNSAFE (Process);
                goto LongWay;
            }
            PointerPte += 1;
        }
        UNLOCK_WS_UNSAFE (Process);
    }
    else {
LongWay:

        //
        // Mark this thread as the address space mutex owner so it cannot
        // sneak its stack in as the argument region and trick us into
        // trying to grow it if the reference faults (as this would cause
        // a deadlock since this thread already owns the address space mutex).
        // Note this would have the side effect of not allowing this thread
        // to fault on guard pages in other data regions while the accesses
        // below are ongoing - but that could only happen in an APC and
        // those are blocked right now anyway.
        //

        ASSERT (KeGetCurrentIrql () == APC_LEVEL);
        ASSERT (Thread->AddressSpaceOwner == 0);
        Thread->AddressSpaceOwner = 1;

#if defined(_WIN64)
        if (Process->Wow64Process != NULL) {
            PageSize = PAGE_SIZE_X86NT;
        } else {
            PageSize = PAGE_SIZE;
        }
#endif

        try {

            if (ProbeMode == PAGE_READONLY) {

                EndAddress = (ULONG_PTR)Address + Size - 1;
                EndAddress = (EndAddress & ~(PageSize - 1)) + PageSize;

                do {
                    Temp = *(volatile CHAR *)Address;
                    Address = (PVOID)(((ULONG_PTR)Address & ~(PageSize - 1)) + PageSize);
                } while ((ULONG_PTR)Address != EndAddress);
            }
            else {
                ProbeForWrite (Address, Size, 1);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (KeGetCurrentIrql () == APC_LEVEL);
            ASSERT (Thread->AddressSpaceOwner == 1);
            Thread->AddressSpaceOwner = 0;
            goto Return1;
        }

        ASSERT (KeGetCurrentIrql () == APC_LEVEL);
        ASSERT (Thread->AddressSpaceOwner == 1);
        Thread->AddressSpaceOwner = 0;

        //
        // Locate VAD and add in secure descriptor.
        //

        EndAddress = (ULONG_PTR)StartAddress + Size - 1;
        Vad = MiLocateAddress (StartAddress);

        if (Vad == NULL) {
            goto Return1;
        }

        if (Vad->u.VadFlags.UserPhysicalPages == 1) {
            goto Return1;
        }

        if ((MI_VA_TO_VPN (StartAddress) < Vad->StartingVpn) ||
            (MI_VA_TO_VPN (EndAddress) > Vad->EndingVpn)) {

            //
            // Not within the section virtual address descriptor,
            // return an error.
            //

            goto Return1;
        }
    }

EditVad:

    //
    // If this is a short or regular VAD, it needs to be reallocated as
    // a large VAD.  Note that a short VAD that was previously converted
    // to a long VAD here will still be marked as private memory, thus to
    // handle this case the NoChange bit must also be tested.
    //

    if (((Vad->u.VadFlags.PrivateMemory) && (Vad->u.VadFlags.NoChange == 0)) 
        ||
        (Vad->u2.VadFlags2.LongVad == 0)) {

        if (Vad->u.VadFlags.PrivateMemory == 0) {
            ASSERT (Vad->u2.VadFlags2.OneSecured == 0);
            ASSERT (Vad->u2.VadFlags2.MultipleSecured == 0);
        }

        NewVad = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(MMVAD_LONG),
                                        'ldaV');
        if (NewVad == NULL) {
            goto Return1;
        }

        RtlZeroMemory (NewVad, sizeof(MMVAD_LONG));

        if (Vad->u.VadFlags.PrivateMemory) {
            RtlCopyMemory (NewVad, Vad, sizeof(MMVAD_SHORT));
        }
        else {
            RtlCopyMemory (NewVad, Vad, sizeof(MMVAD));
        }

        NewVad->u.VadFlags.NoChange = 1;
        NewVad->u2.VadFlags2.OneSecured = 1;

        NewVad->u2.VadFlags2.LongVad = 1;
        NewVad->u2.VadFlags2.ReadOnly = Probe;

        NewVad->u3.Secured.StartVpn = (ULONG_PTR)StartAddress;
        NewVad->u3.Secured.EndVpn = EndAddress;

        //
        // Replace the current VAD with this expanded VAD.
        //

        LOCK_WS_UNSAFE (Process);
        if (Vad->Parent) {
            if (Vad->Parent->RightChild == Vad) {
                Vad->Parent->RightChild = (PMMVAD) NewVad;
            }
            else {
                ASSERT (Vad->Parent->LeftChild == Vad);
                Vad->Parent->LeftChild = (PMMVAD) NewVad;
            }
        }
        else {
            Process->VadRoot = NewVad;
        }
        if (Vad->LeftChild) {
            Vad->LeftChild->Parent = (PMMVAD) NewVad;
        }
        if (Vad->RightChild) {
            Vad->RightChild->Parent = (PMMVAD) NewVad;
        }
        if (Process->VadHint == Vad) {
            Process->VadHint = (PMMVAD) NewVad;
        }
        if (Process->VadFreeHint == Vad) {
            Process->VadFreeHint = (PMMVAD) NewVad;
        }

        if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
            (Vad->u.VadFlags.WriteWatch == 1)) {

            MiPhysicalViewAdjuster (Process, Vad, (PMMVAD) NewVad);
        }

        UNLOCK_WS_UNSAFE (Process);
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }

        ExFreePool (Vad);

        //
        // Or in the low bit to denote the secure entry is in the VAD.
        //

        Handle = (HANDLE)((ULONG_PTR)&NewVad->u2.LongFlags2 | 0x1);

        return Handle;
    }

    //
    // This is already a large VAD, add the secure entry.
    //

    ASSERT (Vad->u2.VadFlags2.LongVad == 1);

    if (Vad->u2.VadFlags2.OneSecured) {

        //
        // This VAD already is secured.  Move the info out of the
        // block into pool.
        //

        Secure = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof (MMSECURE_ENTRY),
                                        'eSmM');
        if (Secure == NULL) {
            goto Return1;
        }

        ASSERT (Vad->u.VadFlags.NoChange == 1);
        Vad->u2.VadFlags2.OneSecured = 0;
        Vad->u2.VadFlags2.MultipleSecured = 1;
        Secure->u2.LongFlags2 = (ULONG) Vad->u.LongFlags;
        Secure->StartVpn = ((PMMVAD_LONG) Vad)->u3.Secured.StartVpn;
        Secure->EndVpn = ((PMMVAD_LONG) Vad)->u3.Secured.EndVpn;

        InitializeListHead (&((PMMVAD_LONG)Vad)->u3.List);
        InsertTailList (&((PMMVAD_LONG)Vad)->u3.List, &Secure->List);
    }

    if (Vad->u2.VadFlags2.MultipleSecured) {

        //
        // This VAD already has a secured element in its list, allocate and
        // add in the new secured element.
        //

        Secure = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof (MMSECURE_ENTRY),
                                        'eSmM');
        if (Secure == NULL) {
            goto Return1;
        }

        Secure->u2.LongFlags2 = 0;
        Secure->u2.VadFlags2.ReadOnly = Probe;
        Secure->StartVpn = (ULONG_PTR)StartAddress;
        Secure->EndVpn = EndAddress;

        InsertTailList (&((PMMVAD_LONG)Vad)->u3.List, &Secure->List);
        Handle = (HANDLE)Secure;

    }
    else {

        //
        // This list does not have a secure element.  Put it in the VAD.
        // The VAD may be either a regular VAD or a long VAD (it cannot be
        // a short VAD) at this point.  If it is a regular VAD, it must be
        // reallocated as a long VAD before any operation can proceed so
        // the secured range can be inserted.
        //

        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.VadFlags2.OneSecured = 1;
        Vad->u2.VadFlags2.ReadOnly = Probe;
        ((PMMVAD_LONG)Vad)->u3.Secured.StartVpn = (ULONG_PTR)StartAddress;
        ((PMMVAD_LONG)Vad)->u3.Secured.EndVpn = EndAddress;

        //
        // Or in the low bit to denote the secure entry is in the VAD.
        //

        Handle = (HANDLE)((ULONG_PTR)&Vad->u2.LongFlags2 | 0x1);
    }

Return1:
    if (AddressSpaceMutexHeld == FALSE) {
        UNLOCK_ADDRESS_SPACE (Process);
    }
    return Handle;
}


VOID
MmUnsecureVirtualMemory (
    IN HANDLE SecureHandle
    )

/*++

Routine Description:

    This routine unsecures memory previously secured via a call to
    MmSecureVirtualMemory.

Arguments:

    SecureHandle - Supplies the handle returned in MmSecureVirtualMemory.

Return Value:

    None.

Environment:

    Kernel Mode.

--*/

{
    MiUnsecureVirtualMemory (SecureHandle, FALSE);
}


VOID
MiUnsecureVirtualMemory (
    IN HANDLE SecureHandle,
    IN LOGICAL AddressSpaceMutexHeld
    )

/*++

Routine Description:

    This routine unsecures memory previously secured via a call to
    MmSecureVirtualMemory.

Arguments:

    SecureHandle - Supplies the handle returned in MmSecureVirtualMemory.

    AddressSpaceMutexHeld - Supplies TRUE if the mutex is already held, FALSE
                            if not.

Return Value:

    None.

Environment:

    Kernel Mode.

--*/

{
    PMMSECURE_ENTRY Secure;
    PEPROCESS Process;
    PMMVAD_LONG Vad;

    PAGED_CODE();

    Secure = (PMMSECURE_ENTRY)SecureHandle;
    Process = PsGetCurrentProcess ();

    if (AddressSpaceMutexHeld == FALSE) {
        LOCK_ADDRESS_SPACE (Process);
    }

    if ((ULONG_PTR)Secure & 0x1) {
        Secure = (PMMSECURE_ENTRY) ((ULONG_PTR)Secure & ~0x1);
        Vad = CONTAINING_RECORD (Secure,
                                 MMVAD_LONG,
                                 u2.LongFlags2);
    }
    else {
        Vad = (PMMVAD_LONG) MiLocateAddress ((PVOID)Secure->StartVpn);
    }

    ASSERT (Vad);
    ASSERT (Vad->u.VadFlags.NoChange == 1);
    ASSERT (Vad->u2.VadFlags2.LongVad == 1);

    if (Vad->u2.VadFlags2.OneSecured) {
        ASSERT (Secure == (PMMSECURE_ENTRY)&Vad->u2.LongFlags2);
        Vad->u2.VadFlags2.OneSecured = 0;
        ASSERT (Vad->u2.VadFlags2.MultipleSecured == 0);
        if (Vad->u2.VadFlags2.SecNoChange == 0) {

            //
            // No more secure entries in this list, remove the state.
            //

            Vad->u.VadFlags.NoChange = 0;
        }
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
    }
    else {
        ASSERT (Vad->u2.VadFlags2.MultipleSecured == 1);

        if (Secure == (PMMSECURE_ENTRY)&Vad->u2.LongFlags2) {

            //
            // This was a single block that got converted into a list.
            // Reset the entry.
            //

            Secure = CONTAINING_RECORD (Vad->u3.List.Flink,
                                        MMSECURE_ENTRY,
                                        List);
        }
        RemoveEntryList (&Secure->List);
        if (IsListEmpty (&Vad->u3.List)) {

            //
            // No more secure entries, reset the state.
            //

            Vad->u2.VadFlags2.MultipleSecured = 0;

            if ((Vad->u2.VadFlags2.SecNoChange == 0) &&
               (Vad->u.VadFlags.PrivateMemory == 0)) {

                //
                // No more secure entries in this list, remove the state
                // if and only if this VAD is not private.  If this VAD
                // is private, removing the state NoChange flag indicates
                // that this is a short VAD which it no longer is.
                //

                Vad->u.VadFlags.NoChange = 0;
            }
        }
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        ExFreePool (Secure);
    }

    return;
}

#if DBG
VOID
MiDumpConflictingVad (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad
    )
{
    if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
        DbgPrint( "MM: [%p ... %p) conflicted with Vad %p\n",
                  StartingAddress, EndingAddress, Vad);
        if ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->ControlArea == NULL)) {
            return;
        }
        if (Vad->ControlArea->u.Flags.Image)
            DbgPrint( "    conflict with %Z image at [%p .. %p)\n",
                      &Vad->ControlArea->FilePointer->FileName,
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
        else
        if (Vad->ControlArea->u.Flags.File)
            DbgPrint( "    conflict with %Z file at [%p .. %p)\n",
                      &Vad->ControlArea->FilePointer->FileName,
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
        else
            DbgPrint( "    conflict with section at [%p .. %p)\n",
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
    }
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mi.h ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    mi.h

Abstract:

    This module contains the private data structures and procedure
    prototypes for the memory management system.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#ifndef _MI_
#define _MI_

#pragma warning(disable:4214)   // bit field types other than int
#pragma warning(disable:4201)   // nameless struct/union
#pragma warning(disable:4324)   // alignment sensitive to declspec
#pragma warning(disable:4127)   // condition expression is constant
#pragma warning(disable:4115)   // named type definition in parentheses
#pragma warning(disable:4232)   // dllimport not static
#pragma warning(disable:4206)   // translation unit empty

#include "ntos.h"
#include "ntimage.h"
#include "ki.h"
#include "fsrtl.h"
#include "zwapi.h"
#include "pool.h"
#include "stdio.h"
#include "string.h"
#include "safeboot.h"
#include "triage.h"
#include "xip.h"

#if defined(_X86_)
#include "..\mm\i386\mi386.h"

#elif defined(_AMD64_)
#include "..\mm\amd64\miamd.h"

#elif defined(_IA64_)
#include "..\mm\ia64\miia64.h"

#else
#error "mm: a target architecture must be defined."
#endif

#if defined (_WIN64)
#define ASSERT32(exp)
#define ASSERT64(exp)   ASSERT(exp)
#else
#define ASSERT32(exp)   ASSERT(exp)
#define ASSERT64(exp)
#endif

//
// Special pool constants
//
#define MI_SPECIAL_POOL_PAGABLE         0x8000
#define MI_SPECIAL_POOL_VERIFIER        0x4000
#define MI_SPECIAL_POOL_IN_SESSION      0x2000
#define MI_SPECIAL_POOL_PTE_PAGABLE     0x0002
#define MI_SPECIAL_POOL_PTE_NONPAGABLE  0x0004


#define _2gb  0x80000000                // 2 gigabytes
#define _4gb 0x100000000                // 4 gigabytes

#define MM_FLUSH_COUNTER_MASK (0xFFFFF)

#define MM_FREE_WSLE_SHIFT 4

#define WSLE_NULL_INDEX ((((WSLE_NUMBER)-1) >> MM_FREE_WSLE_SHIFT))

#define MM_FREE_POOL_SIGNATURE (0x50554F4C)

#define MM_MINIMUM_PAGED_POOL_NTAS ((SIZE_T)(48*1024*1024))

#define MM_ALLOCATION_FILLS_VAD ((PMMPTE)(ULONG_PTR)~3)

#define MM_WORKING_SET_LIST_SEARCH 17

#define MM_FLUID_WORKING_SET 8

#define MM_FLUID_PHYSICAL_PAGES 32  //see MmResidentPages below.

#define MM_USABLE_PAGES_FREE 32

#define X64K (ULONG)65536

#define MM_HIGHEST_VAD_ADDRESS ((PVOID)((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (64 * 1024)))


#define MM_NO_WS_EXPANSION ((PLIST_ENTRY)0)
#define MM_WS_EXPANSION_IN_PROGRESS ((PLIST_ENTRY)35)
#define MM_WS_SWAPPED_OUT ((PLIST_ENTRY)37)
#define MM_IO_IN_PROGRESS ((PLIST_ENTRY)97)  // MUST HAVE THE HIGHEST VALUE

#define MM4K_SHIFT    12  //MUST BE LESS THAN OR EQUAL TO PAGE_SHIFT
#define MM4K_MASK  0xfff

#define MMSECTOR_SHIFT 9  //MUST BE LESS THAN OR EQUAL TO PAGE_SHIFT

#define MMSECTOR_MASK 0x1ff

#define MM_LOCK_BY_REFCOUNT 0

#define MM_LOCK_BY_NONPAGE 1

#define MM_FORCE_TRIM 6

#define MM_GROW_WSLE_HASH 20

#define MM_MAXIMUM_WRITE_CLUSTER (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE)

//
// Number of PTEs to flush singularly before flushing the entire TB.
//

#define MM_MAXIMUM_FLUSH_COUNT (FLUSH_MULTIPLE_MAXIMUM-1)

//
// Page protections
//

#define MM_ZERO_ACCESS         0  // this value is not used.
#define MM_READONLY            1
#define MM_EXECUTE             2
#define MM_EXECUTE_READ        3
#define MM_READWRITE           4  // bit 2 is set if this is writable.
#define MM_WRITECOPY           5
#define MM_EXECUTE_READWRITE   6
#define MM_EXECUTE_WRITECOPY   7

#define MM_NOCACHE            0x8
#define MM_GUARD_PAGE         0x10
#define MM_DECOMMIT           0x10   //NO_ACCESS, Guard page
#define MM_NOACCESS           0x18   //NO_ACCESS, Guard_page, nocache.
#define MM_UNKNOWN_PROTECTION 0x100  //bigger than 5 bits!
#define MM_LARGE_PAGES        0x111

#define MM_INVALID_PROTECTION ((ULONG)-1)  //bigger than 5 bits!

#define MM_KSTACK_OUTSWAPPED  0x1F   // Denotes outswapped kernel stack pages.

#define MM_PROTECTION_WRITE_MASK     4
#define MM_PROTECTION_COPY_MASK      1
#define MM_PROTECTION_OPERATION_MASK 7 // mask off guard page and nocache.
#define MM_PROTECTION_EXECUTE_MASK   2

#define MM_SECURE_DELETE_CHECK 0x55

//
// Debug flags
//

#define MM_DBG_WRITEFAULT       0x1
#define MM_DBG_PTE_UPDATE       0x2
#define MM_DBG_DUMP_WSL         0x4
#define MM_DBG_PAGEFAULT        0x8
#define MM_DBG_WS_EXPANSION     0x10
#define MM_DBG_MOD_WRITE        0x20
#define MM_DBG_CHECK_PTE        0x40
#define MM_DBG_VAD_CONFLICT     0x80
#define MM_DBG_SECTIONS         0x100
#define MM_DBG_SYS_PTES         0x400
#define MM_DBG_CLEAN_PROCESS    0x800
#define MM_DBG_COLLIDED_PAGE    0x1000
#define MM_DBG_DUMP_BOOT_PTES   0x2000
#define MM_DBG_FORK             0x4000
#define MM_DBG_DIR_BASE         0x8000
#define MM_DBG_FLUSH_SECTION    0x10000
#define MM_DBG_PRINTS_MODWRITES 0x20000
#define MM_DBG_PAGE_IN_LIST     0x40000
#define MM_DBG_CHECK_PFN_LOCK   0x80000
#define MM_DBG_PRIVATE_PAGES    0x100000
#define MM_DBG_WALK_VAD_TREE    0x200000
#define MM_DBG_SWAP_PROCESS     0x400000
#define MM_DBG_LOCK_CODE        0x800000
#define MM_DBG_STOP_ON_ACCVIO   0x1000000
#define MM_DBG_PAGE_REF_COUNT   0x2000000
#define MM_DBG_SHOW_FAULTS      0x40000000
#define MM_DBG_SESSIONS         0x80000000

//
// If the PTE.protection & MM_COPY_ON_WRITE_MASK == MM_COPY_ON_WRITE_MASK
// then the PTE is copy on write.
//

#define MM_COPY_ON_WRITE_MASK  5

extern ULONG MmProtectToValue[32];

extern
#if (defined(_WIN64) || defined(_X86PAE_))
ULONGLONG
#else
ULONG
#endif
MmProtectToPteMask[32];
extern ULONG MmMakeProtectNotWriteCopy[32];
extern ACCESS_MASK MmMakeSectionAccess[8];
extern ACCESS_MASK MmMakeFileAccess[8];


//
// Time constants
//

extern const LARGE_INTEGER MmSevenMinutes;
extern LARGE_INTEGER MmWorkingSetProtectionTime;
const extern LARGE_INTEGER MmOneSecond;
const extern LARGE_INTEGER MmTwentySeconds;
const extern LARGE_INTEGER MmShortTime;
const extern LARGE_INTEGER MmHalfSecond;
const extern LARGE_INTEGER Mm30Milliseconds;
extern LARGE_INTEGER MmCriticalSectionTimeout;

//
// A month's worth
//

extern ULONG MmCritsectTimeoutSeconds;

//
// this is the csrss process !
//

extern PEPROCESS ExpDefaultErrorPortProcess;

extern SIZE_T MmExtendedCommit;

extern SIZE_T MmTotalProcessCommit;

//
// The total number of pages needed for the loader to successfully hibernate.
//

extern PFN_NUMBER MmHiberPages;

//
//  The counters and reasons to retry IO to protect against verifier induced
//  failures and temporary conditions.
//

extern ULONG MiIoRetryMask;
extern ULONG MiFaultRetryMask;
extern ULONG MiUserFaultRetryMask;

#define MmIsRetryIoStatus(S) (((S) == STATUS_INSUFFICIENT_RESOURCES) || \
                              ((S) == STATUS_WORKING_SET_QUOTA) ||      \
                              ((S) == STATUS_NO_MEMORY))

#if defined (_MI_MORE_THAN_4GB_)

extern PFN_NUMBER MiNoLowMemory;

#if defined (_WIN64)
#define MI_MAGIC_4GB_RECLAIM     0xffffedf0
#else
#define MI_MAGIC_4GB_RECLAIM     0xffedf0
#endif

#define MI_LOWMEM_MAGIC_BIT     (0x80000000)

extern PRTL_BITMAP MiLowMemoryBitMap;
#endif

//
// This is a version of COMPUTE_PAGES_SPANNED that works for 32 and 64 ranges.
//

#define MI_COMPUTE_PAGES_SPANNED(Va, Size) \
    ((((ULONG_PTR)(Va) & (PAGE_SIZE -1)) + (Size) + (PAGE_SIZE - 1)) >> PAGE_SHIFT)

//++
//
// ULONG
// MI_CONVERT_FROM_PTE_PROTECTION (
//     IN ULONG PROTECTION_MASK
//     )
//
// Routine Description:
//
//  This routine converts a PTE protection into a Protect value.
//
// Arguments:
//
//
// Return Value:
//
//     Returns the
//
//--

#define MI_CONVERT_FROM_PTE_PROTECTION(PROTECTION_MASK)      \
                                     (MmProtectToValue[PROTECTION_MASK])

#define MI_MASK_TO_PTE(PROTECTION_MASK) MmProtectToPteMask[PROTECTION_MASK]


#define MI_IS_PTE_PROTECTION_COPY_WRITE(PROTECTION_MASK)  \
   (((PROTECTION_MASK) & MM_COPY_ON_WRITE_MASK) == MM_COPY_ON_WRITE_MASK)

//++
//
// ULONG
// MI_ROUND_TO_64K (
//     IN ULONG LENGTH
//     )
//
// Routine Description:
//
//
// The ROUND_TO_64k macro takes a LENGTH in bytes and rounds it up to a multiple
// of 64K.
//
// Arguments:
//
//     LENGTH - LENGTH in bytes to round up to 64k.
//
// Return Value:
//
//     Returns the LENGTH rounded up to a multiple of 64k.
//
//--

#define MI_ROUND_TO_64K(LENGTH)  (((LENGTH) + X64K - 1) & ~((ULONG_PTR)X64K - 1))

extern ULONG MiLastVadBit;

//++
//
// ULONG
// MI_ROUND_TO_SIZE (
//     IN ULONG LENGTH,
//     IN ULONG ALIGNMENT
//     )
//
// Routine Description:
//
//
// The ROUND_TO_SIZE macro takes a LENGTH in bytes and rounds it up to a
// multiple of the alignment.
//
// Arguments:
//
//     LENGTH - LENGTH in bytes to round up to.
//
//     ALIGNMENT - alignment to round to, must be a power of 2, e.g, 2**n.
//
// Return Value:
//
//     Returns the LENGTH rounded up to a multiple of the alignment.
//
//--

#define MI_ROUND_TO_SIZE(LENGTH,ALIGNMENT)     \
                    (((LENGTH) + ((ALIGNMENT) - 1)) & ~((ALIGNMENT) - 1))

//++
//
// PVOID
// MI_64K_ALIGN (
//     IN PVOID VA
//     )
//
// Routine Description:
//
//
// The MI_64K_ALIGN macro takes a virtual address and returns a 64k-aligned
// virtual address for that page.
//
// Arguments:
//
//     VA - Virtual address.
//
// Return Value:
//
//     Returns the 64k aligned virtual address.
//
//--

#define MI_64K_ALIGN(VA) ((PVOID)((ULONG_PTR)(VA) & ~((LONG)X64K - 1)))


//++
//
// PVOID
// MI_ALIGN_TO_SIZE (
//     IN PVOID VA
//     IN ULONG ALIGNMENT
//     )
//
// Routine Description:
//
//
// The MI_ALIGN_TO_SIZE macro takes a virtual address and returns a
// virtual address for that page with the specified alignment.
//
// Arguments:
//
//     VA - Virtual address.
//
//     ALIGNMENT - alignment to round to, must be a power of 2, e.g, 2**n.
//
// Return Value:
//
//     Returns the aligned virtual address.
//
//--

#define MI_ALIGN_TO_SIZE(VA,ALIGNMENT) ((PVOID)((ULONG_PTR)(VA) & ~((ULONG_PTR) ALIGNMENT - 1)))

//++
//
// LONGLONG
// MI_STARTING_OFFSET (
//     IN PSUBSECTION SUBSECT
//     IN PMMPTE PTE
//     )
//
// Routine Description:
//
//    This macro takes a pointer to a PTE within a subsection and a pointer
//    to that subsection and calculates the offset for that PTE within the
//    file.
//
// Arguments:
//
//     PTE - PTE within subsection.
//
//     SUBSECT - Subsection
//
// Return Value:
//
//     Offset for issuing I/O from.
//
//--

#define MI_STARTING_OFFSET(SUBSECT,PTE) \
           (((LONGLONG)((ULONG_PTR)((PTE) - ((SUBSECT)->SubsectionBase))) << PAGE_SHIFT) + \
             ((LONGLONG)((SUBSECT)->StartingSector) << MMSECTOR_SHIFT));


// NTSTATUS
// MiFindEmptyAddressRangeDown (
//    IN ULONG_PTR SizeOfRange,
//    IN PVOID HighestAddressToEndAt,
//    IN ULONG_PTR Alignment,
//    OUT PVOID *Base
//    )
//
//
// Routine Description:
//
//    The function examines the virtual address descriptors to locate
//    an unused range of the specified size and returns the starting
//    address of the range.  This routine looks from the top down.
//
// Arguments:
//
//    SizeOfRange - Supplies the size in bytes of the range to locate.
//
//    HighestAddressToEndAt - Supplies the virtual address to begin looking
//                            at.
//
//    Alignment - Supplies the alignment for the address.  Must be
//                 a power of 2 and greater than the page_size.
//
//Return Value:
//
//    Returns the starting address of a suitable range.
//

#define MiFindEmptyAddressRangeDown(Root,SizeOfRange,HighestAddressToEndAt,Alignment,Base) \
               (MiFindEmptyAddressRangeDownTree(                             \
                    (SizeOfRange),                                           \
                    (HighestAddressToEndAt),                                 \
                    (Alignment),                                             \
                    (PMMADDRESS_NODE)(Root),                                 \
                    (Base)))

// PMMVAD
// MiGetPreviousVad (
//     IN PMMVAD Vad
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically precedes the specified virtual
//     address descriptor.
//
// Arguments:
//
//     Vad - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//
//

#define MiGetPreviousVad(VAD) ((PMMVAD)MiGetPreviousNode((PMMADDRESS_NODE)(VAD)))


// PMMVAD
// MiGetNextVad (
//     IN PMMVAD Vad
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically follows the specified address range.
//
// Arguments:
//
//     VAD - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//

#define MiGetNextVad(VAD) ((PMMVAD)MiGetNextNode((PMMADDRESS_NODE)(VAD)))



// PMMVAD
// MiGetFirstVad (
//     Process
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically is first within the address space.
//
// Arguments:
//
//     Process - Specifies the process in which to locate the VAD.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     first address range, NULL if none.

#define MiGetFirstVad(Process) \
    ((PMMVAD)MiGetFirstNode((PMMADDRESS_NODE)(Process->VadRoot)))


LOGICAL
MiCheckForConflictingVadExistence (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

// PMMVAD
// MiCheckForConflictingVad (
//     IN PVOID StartingAddress,
//     IN PVOID EndingAddress
//     )
//
// Routine Description:
//
//     The function determines if any addresses between a given starting and
//     ending address is contained within a virtual address descriptor.
//
// Arguments:
//
//     StartingAddress - Supplies the virtual address to locate a containing
//                       descriptor.
//
//     EndingAddress - Supplies the virtual address to locate a containing
//                       descriptor.
//
// Return Value:
//
//     Returns a pointer to the first conflicting virtual address descriptor
//     if one is found, otherwise a NULL value is returned.
//

#define MiCheckForConflictingVad(CurrentProcess,StartingAddress,EndingAddress) \
    ((PMMVAD)MiCheckForConflictingNode(                                   \
                    MI_VA_TO_VPN(StartingAddress),                        \
                    MI_VA_TO_VPN(EndingAddress),                          \
                    (PMMADDRESS_NODE)(CurrentProcess->VadRoot)))

// PMMCLONE_DESCRIPTOR
// MiGetNextClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically follows the specified address range.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//
//

#define MiGetNextClone(CLONE) \
 ((PMMCLONE_DESCRIPTOR)MiGetNextNode((PMMADDRESS_NODE)(CLONE)))



// PMMCLONE_DESCRIPTOR
// MiGetPreviousClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically precedes the specified virtual
//     address descriptor.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.


#define MiGetPreviousClone(CLONE)  \
             ((PMMCLONE_DESCRIPTOR)MiGetPreviousNode((PMMADDRESS_NODE)(CLONE)))



// PMMCLONE_DESCRIPTOR
// MiGetFirstClone (
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically is first within the address space.
//
// Arguments:
//
//     None.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     first address range, NULL if none.
//


#define MiGetFirstClone(_CurrentProcess) \
    ((PMMCLONE_DESCRIPTOR)MiGetFirstNode((PMMADDRESS_NODE)(_CurrentProcess->CloneRoot)))



// VOID
// MiInsertClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function inserts a virtual address descriptor into the tree and
//     reorders the splay tree as appropriate.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor
//
//
// Return Value:
//
//     None.
//

#define MiInsertClone(_CurrentProcess, CLONE) \
    {                                           \
        ASSERT ((CLONE)->NumberOfPtes != 0);     \
        MiInsertNode(((PMMADDRESS_NODE)(CLONE)),(PMMADDRESS_NODE *)&(_CurrentProcess->CloneRoot)); \
    }




// VOID
// MiRemoveClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function removes a virtual address descriptor from the tree and
//     reorders the splay tree as appropriate.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     None.
//

#define MiRemoveClone(_CurrentProcess, CLONE) \
    MiRemoveNode((PMMADDRESS_NODE)(CLONE),(PMMADDRESS_NODE *)&(_CurrentProcess->CloneRoot));



// PMMCLONE_DESCRIPTOR
// MiLocateCloneAddress (
//     IN PVOID VirtualAddress
//     )
//
// /*++
//
// Routine Description:
//
//     The function locates the virtual address descriptor which describes
//     a given address.
//
// Arguments:
//
//     VirtualAddress - Supplies the virtual address to locate a descriptor
//                      for.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor which contains
//     the supplied virtual address or NULL if none was located.
//

#define MiLocateCloneAddress(_CurrentProcess, VA)                           \
    (_CurrentProcess->CloneRoot ?                                           \
        ((PMMCLONE_DESCRIPTOR)MiLocateAddressInTree(((ULONG_PTR)VA),        \
                   (PMMADDRESS_NODE *)&(_CurrentProcess->CloneRoot))) :     \
        NULL)


#define MI_VA_TO_PAGE(va) ((ULONG_PTR)(va) >> PAGE_SHIFT)

#define MI_VA_TO_VPN(va)  ((ULONG_PTR)(va) >> PAGE_SHIFT)

#define MI_VPN_TO_VA(vpn)  (PVOID)((vpn) << PAGE_SHIFT)

#define MI_VPN_TO_VA_ENDING(vpn)  (PVOID)(((vpn) << PAGE_SHIFT) | (PAGE_SIZE - 1))

#define MiGetByteOffset(va) ((ULONG_PTR)(va) & (PAGE_SIZE - 1))

#define MI_PFN_ELEMENT(index) (&MmPfnDatabase[index])

//
// Make a write-copy PTE, only writable.
//

#define MI_MAKE_PROTECT_NOT_WRITE_COPY(PROTECT) \
            (MmMakeProtectNotWriteCopy[PROTECT])

//
// Define macros to lock and unlock the PFN database.
//

#define MiLockPfnDatabase(OldIrql) \
    OldIrql = KeAcquireQueuedSpinLock(LockQueuePfnLock)

#define MiUnlockPfnDatabase(OldIrql) \
    KeReleaseQueuedSpinLock(LockQueuePfnLock, OldIrql)

#define MiLockPfnDatabaseAtDpcLevel() \
    KeAcquireQueuedSpinLockAtDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiUnlockPfnDatabaseFromDpcLevel() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiTryToLockPfnDatabase(OldIrql) \
    KeTryToAcquireQueuedSpinLock(LockQueuePfnLock, &OldIrql)

#define MiReleasePfnLock() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiLockSystemSpace(OldIrql) \
    OldIrql = KeAcquireQueuedSpinLock(LockQueueSystemSpaceLock)

#define MiUnlockSystemSpace(OldIrql) \
    KeReleaseQueuedSpinLock(LockQueueSystemSpaceLock, OldIrql)

#define MiLockSystemSpaceAtDpcLevel() \
    KeAcquireQueuedSpinLockAtDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueueSystemSpaceLock])

#define MiUnlockSystemSpaceFromDpcLevel() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueueSystemSpaceLock])

// #define _MI_INSTRUMENT_PFN 1

#if defined (_MI_INSTRUMENT_PFN)

#define MI_MAX_PFN_CALLERS  500

typedef struct _MMPFNTIMINGS {
    LARGE_INTEGER HoldTime;     // Low bit is set if another processor waited
    ULONG_PTR AcquiredAddress;
    ULONG_PTR ReleasedAddress;
} MMPFNTIMINGS, *PMMPFNTIMINGS;

extern ULONG MiPfnTimings;
extern ULONG_PTR MiPfnAcquiredAddress;
extern MMPFNTIMINGS MiPfnSorted[];
extern LARGE_INTEGER MiPfnAcquired;
extern LARGE_INTEGER MiPfnThreshold;

ULONG_PTR
MiGetExecutionAddress(
    VOID
    );

#if defined(_X86_)
#define MI_GET_EXECUTION_ADDRESS(varname) varname = MiGetExecutionAddress();
#else
#define MI_GET_EXECUTION_ADDRESS(varname) varname = 0;
#endif

#define LOCK_PFN_TIMESTAMP()                                \
        {                                                   \
            MiPfnAcquired = KeQueryPerformanceCounter (NULL);\
            MI_GET_EXECUTION_ADDRESS(MiPfnAcquiredAddress); \
        }

#define UNLOCK_PFN_TIMESTAMP()                              \
        {                                                   \
            ULONG i;                                        \
            ULONG_PTR ExecutionAddress;                     \
            LARGE_INTEGER PfnReleased;                      \
            LARGE_INTEGER PfnHoldTime;                      \
            PfnReleased = KeQueryPerformanceCounter (NULL); \
            MI_GET_EXECUTION_ADDRESS(ExecutionAddress);     \
            PfnHoldTime.QuadPart = (PfnReleased.QuadPart - MiPfnAcquired.QuadPart) & ~0x1; \
            i = MI_MAX_PFN_CALLERS - 1;                     \
            do {                                            \
                if (PfnHoldTime.QuadPart < MiPfnSorted[i].HoldTime.QuadPart) { \
                    break;                                  \
                }                                           \
                i -= 1;                                     \
            } while (i != (ULONG)-1);                       \
            if (i != MI_MAX_PFN_CALLERS - 1) {              \
                i += 1;                                     \
                if (i != MI_MAX_PFN_CALLERS - 1) {          \
                    RtlMoveMemory (&MiPfnSorted[i+1], &MiPfnSorted[i], (MI_MAX_PFN_CALLERS-(i+1)) * sizeof(MMPFNTIMINGS));                  \
                }                                           \
                MiPfnSorted[i].HoldTime = PfnHoldTime;      \
                if (KeTestForWaitersQueuedSpinLock(LockQueuePfnLock) == TRUE) {\
                    MiPfnSorted[i].HoldTime.LowPart |= 0x1;  \
                } \
                MiPfnSorted[i].AcquiredAddress = MiPfnAcquiredAddress;  \
                MiPfnSorted[i].ReleasedAddress = ExecutionAddress;  \
            }                                               \
            if ((MiPfnTimings & 0x2) && (PfnHoldTime.QuadPart >= MiPfnThreshold.QuadPart)) { \
                DbgBreakPoint ();                           \
            }                                               \
            if (MiPfnTimings & 0x1) {                       \
                MiPfnTimings &= ~0x1;                       \
                RtlZeroMemory (&MiPfnSorted[0], MI_MAX_PFN_CALLERS * sizeof(MMPFNTIMINGS));                                                 \
            }                                               \
        }
#else
#define LOCK_PFN_TIMESTAMP()
#define UNLOCK_PFN_TIMESTAMP()
#endif

#define LOCK_PFN(OLDIRQL) ASSERT (KeGetCurrentIrql() <= APC_LEVEL); \
                          MiLockPfnDatabase(OLDIRQL);               \
                          LOCK_PFN_TIMESTAMP();

#define LOCK_PFN_WITH_TRY(OLDIRQL)                                   \
    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);                        \
    do {                                                             \
    } while (MiTryToLockPfnDatabase(OLDIRQL) == FALSE);              \
    LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN(OLDIRQL)                                        \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabase(OLDIRQL);                                  \
    ASSERT(KeGetCurrentIrql() <= APC_LEVEL);

#define LOCK_PFN2(OLDIRQL) ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL); \
                          MiLockPfnDatabase(OLDIRQL);               \
                          LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN2(OLDIRQL)                                       \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabase(OLDIRQL);                                  \
    ASSERT(KeGetCurrentIrql() <= DISPATCH_LEVEL);

#define LOCK_PFN_AT_DPC() ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL); \
                          MiLockPfnDatabaseAtDpcLevel();                 \
                          LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN_FROM_DPC()                                        \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabaseFromDpcLevel();                             \
    ASSERT(KeGetCurrentIrql() == DISPATCH_LEVEL);

#define UNLOCK_PFN_AND_THEN_WAIT(OLDIRQL)                          \
                {                                                  \
                    KIRQL XXX;                                     \
                    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL); \
                    ASSERT (OLDIRQL <= APC_LEVEL);                 \
                    KiLockDispatcherDatabase (&XXX);               \
                    UNLOCK_PFN_TIMESTAMP();                        \
                    MiReleasePfnLock();                            \
                    (KeGetCurrentThread())->WaitIrql = OLDIRQL;    \
                    (KeGetCurrentThread())->WaitNext = TRUE;       \
                }

extern KMUTANT MmSystemLoadLock;

#if DBG
#define SYSLOAD_LOCK_OWNED_BY_ME()      ((PETHREAD)MmSystemLoadLock.OwnerThread == PsGetCurrentThread())
#else
#define SYSLOAD_LOCK_OWNED_BY_ME()
#endif

#if DBG

#if defined (_MI_COMPRESSION)

extern KIRQL MiCompressionIrql;

#define MM_PFN_LOCK_ASSERT() \
    if (MmDebug & 0x80000) { \
        KIRQL _OldIrql; \
        _OldIrql = KeGetCurrentIrql(); \
        ASSERT ((_OldIrql == DISPATCH_LEVEL) || \
                ((MiCompressionIrql != 0) && (_OldIrql == MiCompressionIrql))); \
    }

#else

#define MM_PFN_LOCK_ASSERT() \
    if (MmDebug & 0x80000) { \
        KIRQL _OldIrql; \
        _OldIrql = KeGetCurrentIrql(); \
        ASSERT (_OldIrql == DISPATCH_LEVEL); \
    }

#endif

extern PETHREAD MiExpansionLockOwner;

#define MM_SET_EXPANSION_OWNER()  ASSERT (MiExpansionLockOwner == NULL); \
                                  MiExpansionLockOwner = PsGetCurrentThread();

#define MM_CLEAR_EXPANSION_OWNER()  ASSERT (MiExpansionLockOwner == PsGetCurrentThread()); \
                                    MiExpansionLockOwner = NULL;

#else
#define MM_PFN_LOCK_ASSERT()
#define MM_SET_EXPANSION_OWNER()
#define MM_CLEAR_EXPANSION_OWNER()
#endif //DBG


#define LOCK_EXPANSION(OLDIRQL)     ASSERT (KeGetCurrentIrql() <= APC_LEVEL); \
                                ExAcquireSpinLock (&MmExpansionLock, &OLDIRQL);\
                                MM_SET_EXPANSION_OWNER ();

#define UNLOCK_EXPANSION(OLDIRQL)    MM_CLEAR_EXPANSION_OWNER (); \
                                ExReleaseSpinLock (&MmExpansionLock, OLDIRQL); \
                                ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

#define UNLOCK_EXPANSION_AND_THEN_WAIT(OLDIRQL)                    \
                {                                                  \
                    KIRQL XXX;                                     \
                    ASSERT (KeGetCurrentIrql() == 2);              \
                    ASSERT (OLDIRQL <= APC_LEVEL);                 \
                    KiLockDispatcherDatabase (&XXX);               \
                    MM_CLEAR_EXPANSION_OWNER ();                   \
                    KiReleaseSpinLock (&MmExpansionLock);          \
                    (KeGetCurrentThread())->WaitIrql = OLDIRQL;    \
                    (KeGetCurrentThread())->WaitNext = TRUE;       \
                }

extern PETHREAD MmSystemLockOwner;

#define LOCK_SYSTEM_WS(OLDIRQL,_Thread)                         \
            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);           \
            KeRaiseIrql(APC_LEVEL,&OLDIRQL);                    \
            ExAcquireResourceExclusiveLite(&MmSystemWsLock,TRUE);   \
            ASSERT (MmSystemLockOwner == NULL);                 \
            MmSystemLockOwner = _Thread;

#define LOCK_SYSTEM_WS_UNSAFE(_Thread)                              \
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);               \
            ExAcquireResourceExclusiveLite(&MmSystemWsLock,TRUE);   \
            ASSERT (MmSystemLockOwner == NULL);                 \
            MmSystemLockOwner = _Thread;

#define UNLOCK_SYSTEM_WS(OLDIRQL)                               \
            ASSERT (MmSystemLockOwner == PsGetCurrentThread()); \
            MmSystemLockOwner = NULL;                           \
            ExReleaseResourceLite (&MmSystemWsLock);            \
            KeLowerIrql (OLDIRQL);                              \
            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

#define UNLOCK_SYSTEM_WS_NO_IRQL()                              \
            ASSERT (MmSystemLockOwner == PsGetCurrentThread()); \
            MmSystemLockOwner = NULL;                           \
            ExReleaseResourceLite (&MmSystemWsLock);

#define MM_SYSTEM_WS_LOCK_ASSERT()                              \
        ASSERT (PsGetCurrentThread() == MmSystemLockOwner);

#define LOCK_HYPERSPACE(_Process, OLDIRQL)                      \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    ExAcquireSpinLock (&_Process->HyperSpaceLock, OLDIRQL);

#define UNLOCK_HYPERSPACE(_Process, VA, OLDIRQL)                \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    MiGetPteAddress(VA)->u.Long = 0;                            \
    ExReleaseSpinLock (&_Process->HyperSpaceLock, OLDIRQL);

#define LOCK_HYPERSPACE_AT_DPC(_Process)                        \
    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);              \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    ExAcquireSpinLockAtDpcLevel (&_Process->HyperSpaceLock);

#define UNLOCK_HYPERSPACE_FROM_DPC(_Process, VA)                \
    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);              \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    MiGetPteAddress(VA)->u.Long = 0;                            \
    ExReleaseSpinLockFromDpcLevel (&_Process->HyperSpaceLock);

#define MiUnmapPageInHyperSpace(_Process, VA, OLDIRQL) UNLOCK_HYPERSPACE(_Process, VA, OLDIRQL)

#define MiUnmapPageInHyperSpaceFromDpc(_Process, VA) UNLOCK_HYPERSPACE_FROM_DPC(_Process, VA)

#if defined (_AXP64_)
#define MI_WS_OWNER(PROCESS) ((PROCESS)->WorkingSetLock.Owner == KeGetCurrentThread())
#define MI_NOT_WS_OWNER(PROCESS) (!MI_WS_OWNER(PROCESS))
#else
#define MI_WS_OWNER(PROCESS)        1
#define MI_NOT_WS_OWNER(PROCESS)    1
#endif

#define MI_MUTEX_ACQUIRED_UNSAFE    0x88

#define MI_IS_WS_UNSAFE(PROCESS) ((PROCESS)->WorkingSetAcquiredUnsafe == MI_MUTEX_ACQUIRED_UNSAFE)

#define LOCK_WS(PROCESS)                                            \
            ASSERT (MI_NOT_WS_OWNER(PROCESS));                      \
            ExAcquireFastMutex( &((PROCESS)->WorkingSetLock));      \
            ASSERT (!MI_IS_WS_UNSAFE(PROCESS));

#define LOCK_WS_UNSAFE(PROCESS)                                     \
            ASSERT (MI_NOT_WS_OWNER(PROCESS));                      \
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);               \
            ExAcquireFastMutexUnsafe( &((PROCESS)->WorkingSetLock));\
            (PROCESS)->WorkingSetAcquiredUnsafe = MI_MUTEX_ACQUIRED_UNSAFE;

#define MI_MUST_BE_UNSAFE(PROCESS)                                  \
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);               \
            ASSERT (MI_WS_OWNER(PROCESS));                          \
            ASSERT (MI_IS_WS_UNSAFE(PROCESS));

#define MI_MUST_BE_SAFE(PROCESS)                                    \
            ASSERT (MI_WS_OWNER(PROCESS));                          \
            ASSERT (!MI_IS_WS_UNSAFE(PROCESS));
#if 0

#define MI_MUST_BE_UNSAFE(PROCESS)                                  \
            if (KeGetCurrentIrql() != APC_LEVEL) {                  \
                KeBugCheckEx(MEMORY_MANAGEMENT, 0x32, (ULONG_PTR)PROCESS, KeGetCurrentIrql(), 0); \
            }                                                       \
            if (!MI_WS_OWNER(PROCESS)) {                            \
                KeBugCheckEx(MEMORY_MANAGEMENT, 0x33, (ULONG_PTR)PROCESS, 0, 0); \
            }                                                       \
            if (!MI_IS_WS_UNSAFE(PROCESS)) {                        \
                KeBugCheckEx(MEMORY_MANAGEMENT, 0x34, (ULONG_PTR)PROCESS, 0, 0); \
            }

#define MI_MUST_BE_SAFE(PROCESS)                                    \
            if (!MI_WS_OWNER(PROCESS)) {                            \
                KeBugCheckEx(MEMORY_MANAGEMENT, 0x42, (ULONG_PTR)PROCESS, 0, 0); \
            }                                                       \
            if (MI_IS_WS_UNSAFE(PROCESS)) {                         \
                KeBugCheckEx(MEMORY_MANAGEMENT, 0x43, (ULONG_PTR)PROCESS, 0, 0); \
            }
#endif


#define UNLOCK_WS(PROCESS)                                          \
            MI_MUST_BE_SAFE(PROCESS);                               \
            ExReleaseFastMutex(&((PROCESS)->WorkingSetLock));

#define UNLOCK_WS_UNSAFE(PROCESS)                                   \
            MI_MUST_BE_UNSAFE(PROCESS);                             \
            (PROCESS)->WorkingSetAcquiredUnsafe = 0;                \
            ExReleaseFastMutexUnsafe(&((PROCESS)->WorkingSetLock)); \
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);

#define LOCK_ADDRESS_SPACE(PROCESS)                                  \
            ExAcquireFastMutex( &((PROCESS)->AddressCreationLock))

#define LOCK_WS_AND_ADDRESS_SPACE(PROCESS)                          \
        LOCK_ADDRESS_SPACE(PROCESS);                                \
        LOCK_WS_UNSAFE(PROCESS);

#define UNLOCK_WS_AND_ADDRESS_SPACE(PROCESS)                        \
        UNLOCK_WS_UNSAFE(PROCESS);                                  \
        UNLOCK_ADDRESS_SPACE(PROCESS);

#define UNLOCK_ADDRESS_SPACE(PROCESS)                            \
            ExReleaseFastMutex( &((PROCESS)->AddressCreationLock))

//
// The working set lock may have been acquired safely or unsafely.
// Release and reacquire it regardless.
//

#define UNLOCK_WS_REGARDLESS(PROCESS, WSHELDSAFE)                   \
            ASSERT (MI_WS_OWNER (PROCESS));                         \
            if (MI_IS_WS_UNSAFE (PROCESS)) {                        \
                UNLOCK_WS_UNSAFE (PROCESS);                         \
                WSHELDSAFE = FALSE;                                 \
            }                                                       \
            else {                                                  \
                UNLOCK_WS (PROCESS);                                \
                WSHELDSAFE = TRUE;                                  \
            }

#define LOCK_WS_REGARDLESS(PROCESS, WSHELDSAFE)                     \
            if (WSHELDSAFE == TRUE) {                               \
                LOCK_WS (PROCESS);                                  \
            }                                                       \
            else {                                                  \
                LOCK_WS_UNSAFE (PROCESS);                           \
            }

#define ZERO_LARGE(LargeInteger)                \
        (LargeInteger).LowPart = 0;             \
        (LargeInteger).HighPart = 0;

#define NO_BITS_FOUND   ((ULONG)-1)

//++
//
// ULONG
// MI_CHECK_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_CHECK_BIT macro checks to see if the specified bit is
//     set within the specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to check.
//
//     BIT - bit number (first bit is 0) to check.
//
// Return Value:
//
//     Returns the value of the bit (0 or 1).
//
//--

#define MI_CHECK_BIT(ARRAY,BIT)  \
        (((ULONG)ARRAY[(BIT) / (sizeof(ULONG)*8)] >> ((BIT) & 0x1F)) & 1)


//++
//
// VOID
// MI_SET_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_SET_BIT macro sets the specified bit within the
//     specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to set.
//
//     BIT - bit number.
//
// Return Value:
//
//     None.
//
//--

#define MI_SET_BIT(ARRAY,BIT)  \
        ARRAY[(BIT) / (sizeof(ULONG)*8)] |= (1 << ((BIT) & 0x1F))


//++
//
// VOID
// MI_CLEAR_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_CLEAR_BIT macro sets the specified bit within the
//     specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to clear.
//
//     BIT - bit number.
//
// Return Value:
//
//     None.
//
//--

#define MI_CLEAR_BIT(ARRAY,BIT)  \
        ARRAY[(BIT) / (sizeof(ULONG)*8)] &= ~(1 << ((BIT) & 0x1F))

//
// These control the mirroring capabilities.
//

extern ULONG MmMirroring;

#define MM_MIRRORING_ENABLED    0x1 // Enable mirroring capabilities.
#define MM_MIRRORING_VERIFYING  0x2 // The HAL wants to verify the copy.

extern PRTL_BITMAP MiMirrorBitMap;
extern PRTL_BITMAP MiMirrorBitMap2;
extern LOGICAL MiMirroringActive;

#if defined (_WIN64)
#define MI_MAGIC_AWE_PTEFRAME   0xffffedcb
#else
#define MI_MAGIC_AWE_PTEFRAME   0xffedcb
#endif

#define MI_PFN_IS_AWE(Pfn1)                                     \
        ((Pfn1->u2.ShareCount <= 3) &&                          \
         (Pfn1->u3.e1.PageLocation == ActiveAndValid) &&        \
         (Pfn1->u4.VerifierAllocation == 0) &&               \
         (Pfn1->u3.e1.LargeSessionAllocation == 0) &&           \
         (Pfn1->u3.e1.StartOfAllocation == 1) &&                \
         (Pfn1->u3.e1.EndOfAllocation == 1) &&                  \
         (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME))


//
// PFN database element.
//

//
// Define pseudo fields for start and end of allocation.
//

#define StartOfAllocation ReadInProgress

#define EndOfAllocation WriteInProgress

#define LargeSessionAllocation PrototypePte

typedef struct _MMPFNENTRY {
    ULONG Modified : 1;
    ULONG ReadInProgress : 1;
    ULONG WriteInProgress : 1;
    ULONG PrototypePte: 1;
    ULONG PageColor : 3;
    ULONG ParityError : 1;
    ULONG PageLocation : 3;
    ULONG RemovalRequested : 1;
    ULONG CacheAttribute : 2;
    ULONG Rom : 1;
    ULONG LockCharged : 1;      // maintained for DBG only
    ULONG DontUse : 16;         // overlays USHORT for reference count field.
} MMPFNENTRY;

//
// The cache type definitions are carefully chosen to line up with the
// MEMORY_CACHING_TYPE definitions to ease conversions.  Any changes here must
// be reflected throughout the code.
//

typedef enum _MI_PFN_CACHE_ATTRIBUTE {
    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiNotMapped
} MI_PFN_CACHE_ATTRIBUTE, *PMI_PFN_CACHE_ATTRIBUTE;

//
// This conversion array is unfortunately needed because not all
// hardware platforms support all possible cache values.  Note that
// the first range is for system RAM, the second range is for I/O space.
//

extern MI_PFN_CACHE_ATTRIBUTE MiPlatformCacheAttributes[2 * MmMaximumCacheType];

//++
//
// MI_PFN_CACHE_ATTRIBUTE
// MI_TRANSLATE_CACHETYPE (
//     IN MEMORY_CACHING_TYPE InputCacheType,
//     IN ULONG IoSpace
//     )
//
// Routine Description:
//
//     Returns the hardware supported cache type for the requested cachetype.
//
// Arguments:
//
//     InputCacheType - Supplies the desired cache type.
//
//     IoSpace - Supplies nonzero (not necessarily 1 though) if this is
//               I/O space, zero if it is main memory.
//
// Return Value:
//
//     The actual cache type.
//
//--
FORCEINLINE
MI_PFN_CACHE_ATTRIBUTE
MI_TRANSLATE_CACHETYPE(
    IN MEMORY_CACHING_TYPE InputCacheType,
    IN ULONG IoSpace
    )
{
    ASSERT (InputCacheType <= MmWriteCombined);

    if (IoSpace != 0) {
        IoSpace = MmMaximumCacheType;
    }
    return MiPlatformCacheAttributes[IoSpace + InputCacheType];
}

//++
//
// VOID
// MI_SET_CACHETYPE_TRANSLATION (
//     IN MEMORY_CACHING_TYPE    InputCacheType,
//     IN ULONG                  IoSpace,
//     IN MI_PFN_CACHE_ATTRIBUTE NewAttribute
//     )
//
// Routine Description:
//
//     This function sets the hardware supported cachetype for the
//     specified cachetype.
//
// Arguments:
//
//     InputCacheType - Supplies the desired cache type.
//
//     IoSpace - Supplies nonzero (not necessarily 1 though) if this is
//               I/O space, zero if it is main memory.
//
//     NewAttribute - Supplies the desired attribute.
//
// Return Value:
//
//     None.
//
//--
FORCEINLINE
VOID
MI_SET_CACHETYPE_TRANSLATION(
    IN MEMORY_CACHING_TYPE    InputCacheType,
    IN ULONG                  IoSpace,
    IN MI_PFN_CACHE_ATTRIBUTE NewAttribute
    )
{
    ASSERT (InputCacheType <= MmWriteCombined);

    if (IoSpace != 0) {
        IoSpace = MmMaximumCacheType;
    }

    MiPlatformCacheAttributes[IoSpace + InputCacheType] = NewAttribute;
}

#if defined (_X86PAE_)
#pragma pack(1)
#endif

typedef struct _MMPFN {
    union {
        PFN_NUMBER Flink;
        WSLE_NUMBER WsIndex;
        PKEVENT Event;
        NTSTATUS ReadStatus;
        SINGLE_LIST_ENTRY NextStackPfn;
    } u1;
    PMMPTE PteAddress;
    union {
        PFN_NUMBER Blink;
        ULONG_PTR ShareCount;
    } u2;
    union {
        MMPFNENTRY e1;
        struct {
            USHORT ShortFlags;
            USHORT ReferenceCount;
        } e2;
    } u3;
#if defined (_WIN64)
    ULONG UsedPageTableEntries;
#endif
    MMPTE OriginalPte;
    union {
        ULONG_PTR EntireFrame;
        struct {
#if defined (_WIN64)
            ULONG_PTR PteFrame: 58;
#else
            ULONG_PTR PteFrame: 26;
#endif
            ULONG_PTR InPageError : 1;
            ULONG_PTR VerifierAllocation : 1;
            ULONG_PTR AweAllocation : 1;
            ULONG_PTR LockCharged : 1;      // maintained for DBG only
            ULONG_PTR KernelStack : 1;      // only for valid (not trans) pages
            ULONG_PTR Reserved : 1;
        };
    } u4;

} MMPFN, *PMMPFN;

#if defined (_X86PAE_)
#pragma pack()
#endif

// #define _MI_DEBUG_DIRTY 1         // Uncomment this for dirty bit logging

#if defined (_MI_DEBUG_DIRTY)

extern ULONG MiTrackDirtys;

#define MI_DIRTY_BACKTRACE_LENGTH 17

typedef struct _MI_DIRTY_TRACES {

    PETHREAD Thread;
    PEPROCESS Process;
    PMMPFN Pfn;
    PMMPTE PointerPte;
    ULONG_PTR CallerId;
    ULONG_PTR ShareCount;
    USHORT ShortFlags;
    USHORT ReferenceCount;
    PVOID StackTrace [MI_DIRTY_BACKTRACE_LENGTH];

} MI_DIRTY_TRACES, *PMI_DIRTY_TRACES;

extern LONG MiDirtyIndex;

extern PMI_DIRTY_TRACES MiDirtyTraces;

VOID
FORCEINLINE
MiSnapDirty (
    IN PMMPFN Pfn,
    IN ULONG NewValue,
    IN ULONG CallerId
    )
{
    PEPROCESS Process;
    PMI_DIRTY_TRACES Information;
    ULONG Index;
    ULONG Hash;

    if (MiDirtyTraces == NULL) {
        return;
    }

    Index = InterlockedIncrement(&MiDirtyIndex);
    Index &= (MiTrackDirtys - 1);
    Information = &MiDirtyTraces[Index];

    Process = PsGetCurrentProcess ();

    Information->Thread = PsGetCurrentThread ();
    Information->Process = Process; 
    Information->Pfn = Pfn;
    Information->PointerPte = Pfn->PteAddress;
    Information->CallerId = CallerId;
    Information->ShareCount = Pfn->u2.ShareCount;
    Information->ShortFlags = Pfn->u3.e2.ShortFlags;
    Information->ReferenceCount = Pfn->u3.e2.ReferenceCount;

    if (NewValue != 0) {
        Information->Process = (PEPROCESS) ((ULONG_PTR)Process | 0x1);
    }

    RtlZeroMemory (&Information->StackTrace[0], MI_DIRTY_BACKTRACE_LENGTH * sizeof(PVOID)); 

    RtlCaptureStackBackTrace (0, MI_DIRTY_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_DIRTY(_Pfn, _NewValue, _Callerid) MiSnapDirty(_Pfn, _NewValue, _Callerid)

#else
#define MI_SNAP_DIRTY(_Pfn, _NewValue, _Callerid)
#endif

#if 0
#define MI_STAMP_MODIFIED(Pfn,id)   (Pfn)->u4.Reserved = (id);
#else
#define MI_STAMP_MODIFIED(Pfn,id)
#endif

//++
//
// VOID
// MI_SET_MODIFIED (
//     IN PMMPFN Pfn,
//     IN ULONG NewValue,
//     IN ULONG CallerId
//     )
//
// Routine Description:
//
//     Set (or clear) the modified bit in the PFN database element.
//     The PFN lock must be held.
//
// Arguments:
//
//     Pfn - Supplies the PFN to operate on.
//
//     NewValue - Supplies 1 to set the modified bit, 0 to clear it.
//
//     CallerId - Supplies a caller ID useful for debugging purposes.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_SET_MODIFIED(_Pfn, _NewValue, _CallerId)             \
            MI_SNAP_DIRTY (_Pfn, _NewValue, _CallerId);         \
            if ((_NewValue) != 0) {                             \
                MI_STAMP_MODIFIED (_Pfn, _CallerId);            \
            }                                                   \
            (_Pfn)->u3.e1.Modified = (_NewValue);

//
// ccNUMA is supported in multiprocessor PAE and WIN64 systems only.
//

#if (defined(_WIN64) || defined(_X86PAE_)) && !defined(NT_UP)
#define MI_MULTINODE

VOID
MiDetermineNode (
    IN PFN_NUMBER Page,
    IN PMMPFN Pfn
    );

#else

#define MiDetermineNode(x,y)    ((y)->u3.e1.PageColor = 0)

#endif

#if defined (_WIN64)

//
// Note there are some places where these portable macros are not currently
// used because we are not in the correct address space required.
//

#define MI_CAPTURE_USED_PAGETABLE_ENTRIES(PFN) \
        ASSERT ((PFN)->UsedPageTableEntries <= PTE_PER_PAGE); \
        (PFN)->OriginalPte.u.Soft.UsedPageTableEntries = (PFN)->UsedPageTableEntries;

#define MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE(RBL, PTE) \
        ASSERT ((PTE)->u.Soft.UsedPageTableEntries <= PTE_PER_PAGE); \
        (RBL)->UsedPageTableEntries = (ULONG)(((PMMPTE)(PTE))->u.Soft.UsedPageTableEntries);

#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(INPAGE_SUPPORT) \
            (INPAGE_SUPPORT)->UsedPageTableEntries = 0;

#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(PFN) (PFN)->UsedPageTableEntries = 0;

#define MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(PFN, INPAGE_SUPPORT) \
        ASSERT ((INPAGE_SUPPORT)->UsedPageTableEntries <= PTE_PER_PAGE); \
        (PFN)->UsedPageTableEntries = (INPAGE_SUPPORT)->UsedPageTableEntries;

#define MI_ZERO_USED_PAGETABLE_ENTRIES(PFN) \
        (PFN)->UsedPageTableEntries = 0;

#define MI_CHECK_USED_PTES_HANDLE(VA) \
        ASSERT (MiGetPdeAddress(VA)->u.Hard.Valid == 1);

#define MI_GET_USED_PTES_HANDLE(VA) \
        ((PVOID)MI_PFN_ELEMENT((PFN_NUMBER)MiGetPdeAddress(VA)->u.Hard.PageFrameNumber))

#define MI_GET_USED_PTES_FROM_HANDLE(PFN) \
        ((ULONG)(((PMMPFN)(PFN))->UsedPageTableEntries))

#define MI_INCREMENT_USED_PTES_BY_HANDLE(PFN) \
        (((PMMPFN)(PFN))->UsedPageTableEntries += 1); \
        ASSERT (((PMMPFN)(PFN))->UsedPageTableEntries <= PTE_PER_PAGE)

#define MI_DECREMENT_USED_PTES_BY_HANDLE(PFN) \
        (((PMMPFN)(PFN))->UsedPageTableEntries -= 1); \
        ASSERT (((PMMPFN)(PFN))->UsedPageTableEntries < PTE_PER_PAGE)

#else

#define MI_CAPTURE_USED_PAGETABLE_ENTRIES(PFN)
#define MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE(RBL, PTE)
#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(INPAGE_SUPPORT)
#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(PFN)

#define MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(PFN, INPAGE_SUPPORT)

#define MI_CHECK_USED_PTES_HANDLE(VA)

#define MI_GET_USED_PTES_HANDLE(VA) ((PVOID)&MmWorkingSetList->UsedPageTableEntries[MiGetPdeIndex(VA)])

#define MI_GET_USED_PTES_FROM_HANDLE(PDSHORT) ((ULONG)(*(PUSHORT)(PDSHORT)))

#define MI_INCREMENT_USED_PTES_BY_HANDLE(PDSHORT) \
    ((*(PUSHORT)(PDSHORT)) += 1); \
    ASSERT (((*(PUSHORT)(PDSHORT)) <= PTE_PER_PAGE))

#define MI_DECREMENT_USED_PTES_BY_HANDLE(PDSHORT) \
    ((*(PUSHORT)(PDSHORT)) -= 1); \
    ASSERT (((*(PUSHORT)(PDSHORT)) < PTE_PER_PAGE))

#endif

extern PFN_NUMBER MmDynamicPfn;

extern FAST_MUTEX MmDynamicMemoryMutex;

extern PFN_NUMBER MmSystemLockPagesCount;

#if DBG

#define MI_LOCK_ID_COUNTER_MAX 64
ULONG MiLockIds[MI_LOCK_ID_COUNTER_MAX];

#define MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)      \
         ASSERT (Pfn->u3.e1.LockCharged == 0);          \
         ASSERT (CallerId < MI_LOCK_ID_COUNTER_MAX);    \
         MiLockIds[CallerId] += 1;                      \
         Pfn->u3.e1.LockCharged = 1;

#define MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)    \
         ASSERT (Pfn->u3.e1.LockCharged == 1);          \
         ASSERT (CallerId < MI_LOCK_ID_COUNTER_MAX);    \
         MiLockIds[CallerId] += 1;                      \
         Pfn->u3.e1.LockCharged = 0;

#else
#define MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)
#define MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)
#endif

//++
//
// VOID
// MI_ADD_LOCKED_PAGE_CHARGE (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Charge the systemwide count of locked pages if this is the initial
//     lock for this page (multiple concurrent locks are only charged once).
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_ADD_LOCKED_PAGE_CHARGE(Pfn, CallerId)                \
    ASSERT (Pfn->u3.e2.ReferenceCount != 0);                    \
    if (Pfn->u3.e2.ReferenceCount == 1) {                       \
        if (Pfn->u2.ShareCount != 0) {                          \
            ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid); \
            MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);         \
            MmSystemLockPagesCount += 1;                        \
        }                                                       \
        else {                                                  \
            ASSERT (Pfn->u3.e1.LockCharged == 1);               \
        }                                                       \
    }

#define MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE(Pfn, CallerId) \
    ASSERT (Pfn->u3.e1.PageLocation != ActiveAndValid);    \
    ASSERT (Pfn->u2.ShareCount == 0);                      \
    if (Pfn->u3.e2.ReferenceCount == 0) {                  \
        MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);        \
        MmSystemLockPagesCount += 1;                       \
    }

#define MI_ADD_LOCKED_PAGE_CHARGE_FOR_TRANSITION_PAGE(Pfn, CallerId) \
    ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);    \
    ASSERT (Pfn->u2.ShareCount == 0);                      \
    ASSERT (Pfn->u3.e2.ReferenceCount != 0);               \
    if (Pfn->u3.e2.ReferenceCount == 1) {                  \
        MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);        \
        MmSystemLockPagesCount += 1;                       \
    }

//++
//
// VOID
// MI_REMOVE_LOCKED_PAGE_CHARGE (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Remove the charge from the systemwide count of locked pages if this
//     is the last lock for this page (multiple concurrent locks are only
//     charged once).
//
//     The PFN reference checks are carefully ordered so the most common case
//     is handled first, the next most common case next, etc.  The case of
//     a reference count of 2 occurs more than 1000x (yes, 3 orders of
//     magnitude) more than a reference count of 1.  And reference counts of >2
//     occur 3 orders of magnitude more frequently than reference counts of
//     exactly 1.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_REMOVE_LOCKED_PAGE_CHARGE(Pfn, CallerId)                         \
        ASSERT (Pfn->u3.e2.ReferenceCount != 0);                            \
        if (Pfn->u3.e2.ReferenceCount == 2) {                               \
            if (Pfn->u2.ShareCount >= 1) {                                  \
                ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);         \
                MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);               \
                MmSystemLockPagesCount -= 1;                                \
            }                                                               \
            else {                                                          \
                /*                                                          \
                 * There are multiple referencers to this page and the      \
                 * page is no longer valid in any process address space.    \
                 * The systemwide lock count can only be decremented        \
                 * by the last dereference.                                 \
                 */                                                         \
                NOTHING;                                                    \
            }                                                               \
        }                                                                   \
        else if (Pfn->u3.e2.ReferenceCount != 1) {                          \
            /*                                                              \
             * There are still multiple referencers to this page (it may    \
             * or may not be resident in any process address space).        \
             * Since the systemwide lock count can only be decremented      \
             * by the last dereference (and this is not it), no action      \
             * is taken here.                                               \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount > 2);                         \
            NOTHING;                                                        \
        }                                                                   \
        else {                                                              \
            /*                                                              \
             * This page has already been deleted from all process address  \
             * spaces.  It is sitting in limbo (not on any list) awaiting   \
             * this last dereference.                                       \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount == 1);                        \
            ASSERT (Pfn->u3.e1.PageLocation != ActiveAndValid);             \
            ASSERT (Pfn->u2.ShareCount == 0);                               \
            MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);                   \
            MmSystemLockPagesCount -= 1;                                    \
        }

//++
//
// VOID
// MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Remove the charge from the systemwide count of locked pages if this
//     is the last lock for this page (multiple concurrent locks are only
//     charged once).
//
//     The PFN reference checks are carefully ordered so the most common case
//     is handled first, the next most common case next, etc.  The case of
//     a reference count of 2 occurs more than 1000x (yes, 3 orders of
//     magnitude) more than a reference count of 1.  And reference counts of >2
//     occur 3 orders of magnitude more frequently than reference counts of
//     exactly 1.
//
//     The PFN reference count is then decremented.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn, CallerId)              \
        ASSERT (Pfn->u3.e2.ReferenceCount != 0);                            \
        if (Pfn->u3.e2.ReferenceCount == 2) {                               \
            if (Pfn->u2.ShareCount >= 1) {                                  \
                ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);         \
                MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);               \
                MmSystemLockPagesCount -= 1;                                \
            }                                                               \
            else {                                                          \
                /*                                                          \
                 * There are multiple referencers to this page and the      \
                 * page is no longer valid in any process address space.    \
                 * The systemwide lock count can only be decremented        \
                 * by the last dereference.                                 \
                 */                                                         \
                NOTHING;                                                    \
            }                                                               \
            Pfn->u3.e2.ReferenceCount -= 1;                                 \
        }                                                                   \
        else if (Pfn->u3.e2.ReferenceCount != 1) {                          \
            /*                                                              \
             * There are still multiple referencers to this page (it may    \
             * or may not be resident in any process address space).        \
             * Since the systemwide lock count can only be decremented      \
             * by the last dereference (and this is not it), no action      \
             * is taken here.                                               \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount > 2);                         \
            Pfn->u3.e2.ReferenceCount -= 1;                                 \
        }                                                                   \
        else {                                                              \
            /*                                                              \
             * This page has already been deleted from all process address  \
             * spaces.  It is sitting in limbo (not on any list) awaiting   \
             * this last dereference.                                       \
             */                                                             \
            PFN_NUMBER _PageFrameIndex;                                     \
            ASSERT (Pfn->u3.e2.ReferenceCount == 1);                        \
            ASSERT (Pfn->u3.e1.PageLocation != ActiveAndValid);             \
            ASSERT (Pfn->u2.ShareCount == 0);                               \
            MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);                   \
            MmSystemLockPagesCount -= 1;                                    \
            _PageFrameIndex = Pfn - MmPfnDatabase;                          \
            MiDecrementReferenceCount (_PageFrameIndex);                    \
        }

//++
//
// VOID
// MI_ZERO_WSINDEX (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Zero the Working Set Index field of the argument PFN entry.
//     There is a subtlety here on systems where the WsIndex ULONG is
//     overlaid with an Event pointer and sizeof(ULONG) != sizeof(PKEVENT).
//     Note this will need to be updated if we ever decide to allocate bodies of
//     thread objects on 4GB boundaries.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_ZERO_WSINDEX(Pfn) \
    Pfn->u1.Event = NULL;

typedef enum _MMSHARE_TYPE {
    Normal,
    ShareCountOnly,
    AndValid
} MMSHARE_TYPE;

typedef struct _MMWSLE_HASH {
    PVOID Key;
    WSLE_NUMBER Index;
} MMWSLE_HASH, *PMMWSLE_HASH;

//++
//
// WSLE_NUMBER
// MI_WSLE_HASH (
//     IN ULONG_PTR VirtualAddress,
//     IN PMMWSL WorkingSetList
//     )
//
// Routine Description:
//
//     Hash the address
//
// Arguments:
//
//     VirtualAddress - the address to hash.
//
//     WorkingSetList - the working set to hash the address into.
//
// Return Value:
//
//     The hash key.
//
//--
//
#define MI_WSLE_HASH(Address, Wsl) \
    ((WSLE_NUMBER)(((ULONG_PTR)PAGE_ALIGN(Address) >> (PAGE_SHIFT - 2)) % \
        ((Wsl)->HashTableSize - 1)))

//
// Working Set List Entry.
//

typedef struct _MMWSLENTRY {
    ULONG_PTR Valid : 1;
    ULONG_PTR LockedInWs : 1;
    ULONG_PTR LockedInMemory : 1;
    ULONG_PTR Protection : 5;
    ULONG_PTR SameProtectAsProto : 1;
    ULONG_PTR Direct : 1;
    ULONG_PTR Age : 2;
#if MM_VIRTUAL_PAGE_FILLER
    ULONG_PTR Filler : MM_VIRTUAL_PAGE_FILLER;
#endif
    ULONG_PTR VirtualPageNumber : MM_VIRTUAL_PAGE_SIZE;
} MMWSLENTRY;

typedef struct _MMWSLE {
    union {
        PVOID VirtualAddress;
        ULONG_PTR Long;
        MMWSLENTRY e1;
    } u1;
} MMWSLE;

#define MI_GET_PROTECTION_FROM_WSLE(Wsl) ((Wsl)->u1.e1.Protection)

typedef MMWSLE *PMMWSLE;

//
// Working Set List.  Must be quadword sized.
//

typedef struct _MMWSL {
    SIZE_T Quota;
    WSLE_NUMBER FirstFree;
    WSLE_NUMBER FirstDynamic;
    WSLE_NUMBER LastEntry;
    WSLE_NUMBER NextSlot;               // The next slot to trim
    PMMWSLE Wsle;
    WSLE_NUMBER LastInitializedWsle;
    WSLE_NUMBER NonDirectCount;
    PMMWSLE_HASH HashTable;
    ULONG HashTableSize;
    ULONG NumberOfCommittedPageTables;
    PVOID HashTableStart;
    PVOID HighestPermittedHashAddress;
    ULONG NumberOfImageWaiters;
    ULONG VadBitMapHint;

#if (_MI_PAGING_LEVELS >= 4)
    PULONG CommittedPageTables;

    ULONG NumberOfCommittedPageDirectories;
    PULONG CommittedPageDirectories;

    ULONG NumberOfCommittedPageDirectoryParents;
    ULONG CommittedPageDirectoryParents[(MM_USER_PAGE_DIRECTORY_PARENT_PAGES + sizeof(ULONG)*8-1)/(sizeof(ULONG)*8)];

#elif (_MI_PAGING_LEVELS >= 3)
    PULONG CommittedPageTables;

    ULONG NumberOfCommittedPageDirectories;
    ULONG CommittedPageDirectories[(MM_USER_PAGE_DIRECTORY_PAGES + sizeof(ULONG)*8-1)/(sizeof(ULONG)*8)];

#else

    //
    // This must be at the end.
    // Not used in system cache or session working set lists.
    //

    USHORT UsedPageTableEntries[MM_USER_PAGE_TABLE_PAGES];

    ULONG CommittedPageTables[MM_USER_PAGE_TABLE_PAGES/(sizeof(ULONG)*8)];
#endif

} MMWSL, *PMMWSL;

#if defined(_X86_)
extern PMMWSL MmWorkingSetList;
#endif

extern PKEVENT MiHighMemoryEvent;
extern PKEVENT MiLowMemoryEvent;

//
// The claim estimate of unused pages in a working set is limited
// to grow by this amount per estimation period.
//

#define MI_CLAIM_INCR 30

//
// The maximum number of different ages a page can be.
//

#define MI_USE_AGE_COUNT 4
#define MI_USE_AGE_MAX (MI_USE_AGE_COUNT - 1)

//
// If more than this "percentage" of the working set is estimated to
// be used then allow it to grow freely.
//

#define MI_REPLACEMENT_FREE_GROWTH_SHIFT 5

//
// If more than this "percentage" of the working set has been claimed
// then force replacement in low memory.
//

#define MI_REPLACEMENT_CLAIM_THRESHOLD_SHIFT 3

//
// If more than this "percentage" of the working set is estimated to
// be available then force replacement in low memory.
//

#define MI_REPLACEMENT_EAVAIL_THRESHOLD_SHIFT 3

//
// If while doing replacement a page is found of this age or older then
// replace it.  Otherwise the oldest is selected.
//

#define MI_IMMEDIATE_REPLACEMENT_AGE 2

//
// When trimming, use these ages for different passes.
//

#define MI_MAX_TRIM_PASSES 4
#define MI_PASS0_TRIM_AGE 2
#define MI_PASS1_TRIM_AGE 1
#define MI_PASS2_TRIM_AGE 1
#define MI_PASS3_TRIM_AGE 1
#define MI_PASS4_TRIM_AGE 0

//
// If not a forced trim, trim pages older than this age.
//

#define MI_TRIM_AGE_THRESHOLD 2

//
// This "percentage" of a claim is up for grabs in a foreground process.
//

#define MI_FOREGROUND_CLAIM_AVAILABLE_SHIFT 3

//
// This "percentage" of a claim is up for grabs in a background process.
//

#define MI_BACKGROUND_CLAIM_AVAILABLE_SHIFT 1

//++
//
// DWORD
// MI_CALC_NEXT_VALID_ESTIMATION_SLOT (
//     DWORD Previous,
//     DWORD Minimum,
//     DWORD Maximum,
//     MI_NEXT_ESTIMATION_SLOT_CONST NextEstimationSlotConst,
//     PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      We iterate through the working set array in a non-sequential
//      manner so that the sample is independent of any aging or trimming.
//
//      This algorithm walks through the working set with a stride of
//      2^MiEstimationShift elements.
//
// Arguments:
//
//      Previous - Last slot used
//
//      Minimum - Minimum acceptable slot (ie. the first dynamic one)
//
//      Maximum - max slot number + 1
//
//      NextEstimationSlotConst - for this algorithm it contains the stride
//
//      Wsle - the working set array
//
// Return Value:
//
//      Next slot.
//
// Environment:
//
//      Kernel mode, APCs disabled, working set lock held and PFN lock held.
//
//--

typedef struct _MI_NEXT_ESTIMATION_SLOT_CONST {
    WSLE_NUMBER Stride;
} MI_NEXT_ESTIMATION_SLOT_CONST;


#define MI_CALC_NEXT_ESTIMATION_SLOT_CONST(NextEstimationSlotConst, WorkingSetList) \
    (NextEstimationSlotConst).Stride = 1 << MiEstimationShift;

#define MI_NEXT_VALID_ESTIMATION_SLOT(Previous, StartEntry, Minimum, Maximum, NextEstimationSlotConst, Wsle) \
    ASSERT(((Previous) >= Minimum) && ((Previous) <= Maximum)); \
    ASSERT(((StartEntry) >= Minimum) && ((StartEntry) <= Maximum)); \
    do { \
        (Previous) += (NextEstimationSlotConst).Stride; \
        if ((Previous) > Maximum) { \
            (Previous) = Minimum + ((Previous + 1) & (NextEstimationSlotConst.Stride - 1)); \
            StartEntry += 1; \
            (Previous) = StartEntry; \
        } \
        if ((Previous) > Maximum || (Previous) < Minimum) { \
            StartEntry = Minimum; \
            (Previous) = StartEntry; \
        } \
    } while (Wsle[Previous].u1.e1.Valid == 0);

//++
//
// WSLE_NUMBER
// MI_NEXT_VALID_AGING_SLOT (
//     DWORD Previous,
//     DWORD Minimum,
//     DWORD Maximum,
//     PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      This finds the next slot to valid slot to age.  It walks
//      through the slots sequentialy.
//
// Arguments:
//
//      Previous - Last slot used
//
//      Minimum - Minimum acceptable slot (ie. the first dynamic one)
//
//      Maximum - Max slot number + 1
//
//      Wsle - the working set array
//
// Return Value:
//
//      None.
//
// Environment:
//
//      Kernel mode, APCs disabled, working set lock held and PFN lock held.
//
//--

#define MI_NEXT_VALID_AGING_SLOT(Previous, Minimum, Maximum, Wsle) \
    ASSERT(((Previous) >= Minimum) && ((Previous) <= Maximum)); \
    do { \
        (Previous) += 1; \
        if ((Previous) > Maximum) { \
            Previous = Minimum; \
        } \
    } while ((Wsle[Previous].u1.e1.Valid == 0));

//++
//
// ULONG
// MI_CALCULATE_USAGE_ESTIMATE (
//     IN PULONG SampledAgeCounts.
//     IN ULONG CounterShift
//     )
//
// Routine Description:
//
//      In Usage Estimation, we count the number of pages of each age in
//      a sample.  The function turns the SampledAgeCounts into an
//      estimate of the unused pages.
//
// Arguments:
//
//      SampledAgeCounts - counts of pages of each different age in the sample
//
//      CounterShift - shift necessary to apply sample to entire WS
//
// Return Value:
//
//      The number of pages to walk in the working set to get a good
//      estimate of the number available.
//
//--

#define MI_CALCULATE_USAGE_ESTIMATE(SampledAgeCounts, CounterShift) \
                (((SampledAgeCounts)[1] + \
                    (SampledAgeCounts)[2] + (SampledAgeCounts)[3]) \
                    << (CounterShift))

//++
//
// VOID
// MI_RESET_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      Clear the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
// Return Value:
//
//      None.
//
//--
#define MI_RESET_WSLE_AGE(PointerPte, Wsle) \
    (Wsle)->u1.e1.Age = 0;

//++
//
// ULONG
// MI_GET_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      Clear the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE
//      Wsle - pointer to the working set list entry
//
// Return Value:
//
//      Age group of the working set entry
//
//--
#define MI_GET_WSLE_AGE(PointerPte, Wsle) \
    ((ULONG)((Wsle)->u1.e1.Age))

//++
//
// VOID
// MI_INC_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle,
//     )
//
// Routine Description:
//
//      Increment the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
// Return Value:
//
//      None
//
//--

#define MI_INC_WSLE_AGE(PointerPte, Wsle) \
    if ((Wsle)->u1.e1.Age < 3) { \
        (Wsle)->u1.e1.Age += 1; \
    }

//++
//
// VOID
// MI_UPDATE_USE_ESTIMATE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle,
//     IN ULONG *SampledAgeCounts
//     )
//
// Routine Description:
//
//      Update the sampled age counts.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
//      SampledAgeCounts - array of age counts to be updated.
//
// Return Value:
//
//      None
//
//--

#define MI_UPDATE_USE_ESTIMATE(PointerPte, Wsle, SampledAgeCounts) \
    (SampledAgeCounts)[(Wsle)->u1.e1.Age] += 1;

//++
//
// BOOLEAN
// MI_WS_GROWING_TOO_FAST (
//     IN PMMSUPPORT VmSupport
//     )
//
// Routine Description:
//
//      Limit the growth rate of processes as the
//      available memory approaches zero.  Note the caller must ensure that
//      MmAvailablePages is low enough so this calculation does not wrap.
//
// Arguments:
//
//      VmSupport - a working set.
//
// Return Value:
//
//      TRUE if the growth rate is too fast, FALSE otherwise.
//
//--

#define MI_WS_GROWING_TOO_FAST(VmSupport) \
    ((VmSupport)->GrowthSinceLastEstimate > \
        (((MI_CLAIM_INCR * (MmAvailablePages*MmAvailablePages)) / (64*64)) + 1))

#define SECTION_BASE_ADDRESS(_NtSection) \
    (*((PVOID *)&(_NtSection)->PointerToRelocations))

#define SECTION_LOCK_COUNT_POINTER(_NtSection) \
    ((PLONG)&(_NtSection)->NumberOfRelocations)

//
// Memory Management Object structures.
//

typedef enum _SECTION_CHECK_TYPE {
    CheckDataSection,
    CheckImageSection,
    CheckUserDataSection,
    CheckBothSection
} SECTION_CHECK_TYPE;

typedef struct _MMEXTEND_INFO {
    UINT64 CommittedSize;
    ULONG ReferenceCount;
} MMEXTEND_INFO, *PMMEXTEND_INFO;

typedef struct _SEGMENT {
    struct _CONTROL_AREA *ControlArea;
    ULONG TotalNumberOfPtes;
    ULONG NonExtendedPtes;
    ULONG WritableUserReferences;

    UINT64 SizeOfSegment;
    MMPTE SegmentPteTemplate;

    SIZE_T NumberOfCommittedPages;
    PMMEXTEND_INFO ExtendInfo;

    PVOID SystemImageBase;   // could replace with single bit in ldrmodlist
    PVOID BasedAddress;

    //
    // The fields below are for image & pagefile-backed sections only.
    // Common fields are above and new common entries must be added to
    // both the SEGMENT and MAPPED_FILE_SEGMENT declarations.
    //

    union {
        SIZE_T ImageCommitment;     // for image-backed sections only
        PEPROCESS CreatingProcess;  // for pagefile-backed sections only
    } u1;

    union {
        PSECTION_IMAGE_INFORMATION ImageInformation;    // for images only
        PVOID FirstMappedVa;        // for pagefile-backed sections only
    } u2;

    PMMPTE PrototypePte;
    MMPTE ThePtes[MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE];

} SEGMENT, *PSEGMENT;

typedef struct _MAPPED_FILE_SEGMENT {
    struct _CONTROL_AREA *ControlArea;
    ULONG TotalNumberOfPtes;
    ULONG NonExtendedPtes;
    ULONG WritableUserReferences;

    UINT64 SizeOfSegment;
    MMPTE SegmentPteTemplate;

    SIZE_T NumberOfCommittedPages;
    PMMEXTEND_INFO ExtendInfo;

    PVOID SystemImageBase;      // could replace with single bit in ldrmodlist
    PVOID BasedAddress;
    struct _MSUBSECTION *LastSubsectionHint;

} MAPPED_FILE_SEGMENT, *PMAPPED_FILE_SEGMENT;

typedef struct _EVENT_COUNTER {
    SINGLE_LIST_ENTRY ListEntry;
    ULONG RefCount;
    KEVENT Event;
} EVENT_COUNTER, *PEVENT_COUNTER;

typedef struct _MMSECTION_FLAGS {
    unsigned BeingDeleted : 1;
    unsigned BeingCreated : 1;
    unsigned BeingPurged : 1;
    unsigned NoModifiedWriting : 1;

    unsigned FailAllIo : 1;
    unsigned Image : 1;
    unsigned Based : 1;
    unsigned File : 1;

    unsigned Networked : 1;
    unsigned NoCache : 1;
    unsigned PhysicalMemory : 1;
    unsigned CopyOnWrite : 1;

    unsigned Reserve : 1;  // not a spare bit!
    unsigned Commit : 1;
    unsigned FloppyMedia : 1;
    unsigned WasPurged : 1;

    unsigned UserReference : 1;
    unsigned GlobalMemory : 1;
    unsigned DeleteOnClose : 1;
    unsigned FilePointerNull : 1;

    unsigned DebugSymbolsLoaded : 1;
    unsigned SetMappedFileIoComplete : 1;
    unsigned CollidedFlush : 1;
    unsigned NoChange : 1;

    unsigned HadUserReference : 1;
    unsigned ImageMappedInSystemSpace : 1;
    unsigned UserWritable : 1;
    unsigned Accessed : 1;

    unsigned GlobalOnlyPerSession : 1;
    unsigned Rom : 1;
    unsigned filler : 2;
} MMSECTION_FLAGS;

typedef struct _CONTROL_AREA {      // must be quadword sized.
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    ULONG NumberOfSectionReferences;
    ULONG NumberOfPfnReferences;
    ULONG NumberOfMappedViews;
    USHORT NumberOfSubsections;
    USHORT FlushInProgressCount;
    ULONG NumberOfUserReferences;
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    PFILE_OBJECT FilePointer;
    PEVENT_COUNTER WaitingForDeletion;
    USHORT ModifiedWriteCount;
    USHORT NumberOfSystemCacheViews;
} CONTROL_AREA, *PCONTROL_AREA;

typedef struct _LARGE_CONTROL_AREA {      // must be quadword sized.
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    ULONG NumberOfSectionReferences;
    ULONG NumberOfPfnReferences;
    ULONG NumberOfMappedViews;
    USHORT NumberOfSubsections;
    USHORT FlushInProgressCount;
    ULONG NumberOfUserReferences;
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    PFILE_OBJECT FilePointer;
    PEVENT_COUNTER WaitingForDeletion;
    USHORT ModifiedWriteCount;
    USHORT NumberOfSystemCacheViews;
    PFN_NUMBER StartingFrame;       // only used if Flags.Rom == 1.
    LIST_ENTRY UserGlobalList;
    ULONG SessionId;
} LARGE_CONTROL_AREA, *PLARGE_CONTROL_AREA;

typedef struct _MMSUBSECTION_FLAGS {
    unsigned ReadOnly : 1;
    unsigned ReadWrite : 1;
    unsigned SubsectionStatic : 1;
    unsigned GlobalMemory: 1;
    unsigned Protection : 5;
    unsigned LargePages : 1;
    unsigned StartingSector4132 : 10;   // 2 ** (42+12) == 4MB*4GB == 16K TB
    unsigned SectorEndOffset : 12;
} MMSUBSECTION_FLAGS;

typedef struct _SUBSECTION { // Must start on quadword boundary and be quad sized
    PCONTROL_AREA ControlArea;
    union {
        ULONG LongFlags;
        MMSUBSECTION_FLAGS SubsectionFlags;
    } u;
    ULONG StartingSector;
    ULONG NumberOfFullSectors;  // (4GB-1) * 4K == 16TB-4K limit per subsection
    PMMPTE SubsectionBase;
    ULONG UnusedPtes;
    ULONG PtesInSubsection;
    struct _SUBSECTION *NextSubsection;
} SUBSECTION, *PSUBSECTION;

extern const ULONG MMSECT;

//
// Accesses to MMSUBSECTION_FLAGS2 are synchronized via the PFN lock
// (unlike MMSUBSECTION_FLAGS access which is not lock protected at all).
//

typedef struct _MMSUBSECTION_FLAGS2 {
    unsigned SubsectionAccessed : 1;
    unsigned SubsectionConverted : 1;       // only needed for debug
    unsigned Reserved : 30;
} MMSUBSECTION_FLAGS2;

//
// Mapped data file subsection structure.  Not used for images
// or pagefile-backed shared memory.
//

typedef struct _MSUBSECTION { // Must start on quadword boundary and be quad sized
    PCONTROL_AREA ControlArea;
    union {
        ULONG LongFlags;
        MMSUBSECTION_FLAGS SubsectionFlags;
    } u;
    ULONG StartingSector;
    ULONG NumberOfFullSectors;  // (4GB-1) * 4K == 16TB-4K limit per subsection
    PMMPTE SubsectionBase;
    ULONG UnusedPtes;
    ULONG PtesInSubsection;
    struct _SUBSECTION *NextSubsection;
    LIST_ENTRY DereferenceList;
    ULONG_PTR NumberOfMappedViews;
    union {
        ULONG LongFlags2;
        MMSUBSECTION_FLAGS2 SubsectionFlags2;
    } u2;
} MSUBSECTION, *PMSUBSECTION;

#define MI_MAXIMUM_SECTION_SIZE ((UINT64)16*1024*1024*1024*1024*1024 - (1<<MM4K_SHIFT))

VOID
MiDecrementSubsections (
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection OPTIONAL
    );

NTSTATUS
MiAddViewsForSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    );

NTSTATUS
MiAddViewsForSection (
    IN PMSUBSECTION MappedSubsection,
    IN ULONG LastPteOffset OPTIONAL,
    IN KIRQL OldIrql,
    OUT PULONG Waited
    );

LOGICAL
MiReferenceSubsection (
    IN PMSUBSECTION MappedSubsection
    );

VOID
MiRemoveViewsFromSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    );

VOID
MiRemoveViewsFromSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN ULONG LastPteOffset OPTIONAL
    );

VOID
MiSubsectionConsistent(
    IN PSUBSECTION Subsection
    );

#if DBG
#define MI_CHECK_SUBSECTION(_subsection) MiSubsectionConsistent((PSUBSECTION)(_subsection))
#else
#define MI_CHECK_SUBSECTION(_subsection)
#endif

//++
//ULONG
//Mi4KStartForSubsection (
//    IN PLARGE_INTEGER address,
//    IN OUT PSUBSECTION subsection
//    );
//
// Routine Description:
//
//    This macro sets into the specified subsection the supplied information
//    indicating the start address (in 4K units) of this portion of the file.
//
// Arguments
//
//    address - Supplies the 64-bit address (in 4K units) of the start of this
//              portion of the file.
//
//    subsection - Supplies the subsection address to store the address in.
//
// Return Value:
//
//    None.
//
//--

#define Mi4KStartForSubsection(address, subsection)  \
   subsection->StartingSector = ((PLARGE_INTEGER)address)->LowPart; \
   subsection->u.SubsectionFlags.StartingSector4132 = \
        (((PLARGE_INTEGER)(address))->HighPart & 0x3ff);

//++
//ULONG
//Mi4KStartFromSubsection (
//    IN OUT PLARGE_INTEGER address,
//    IN PSUBSECTION subsection
//    );
//
// Routine Description:
//
//    This macro gets the start 4K offset from the specified subsection.
//
// Arguments
//
//    address - Supplies the 64-bit address (in 4K units) to place the
//              start of this subsection into.
//
//    subsection - Supplies the subsection address to get the address from.
//
// Return Value:
//
//    None.
//
//--

#define Mi4KStartFromSubsection(address, subsection)  \
   ((PLARGE_INTEGER)address)->LowPart = subsection->StartingSector; \
   ((PLARGE_INTEGER)address)->HighPart = subsection->u.SubsectionFlags.StartingSector4132;

typedef struct _MMDEREFERENCE_SEGMENT_HEADER {
    KSPIN_LOCK Lock;
    KSEMAPHORE Semaphore;
    LIST_ENTRY ListHead;
} MMDEREFERENCE_SEGMENT_HEADER;

//
// This entry is used for calling the segment dereference thread
// to perform page file expansion.  It has a similar structure
// to a control area to allow either a control area or a page file
// expansion entry to be placed on the list.  Note that for a control
// area the segment pointer is valid whereas for page file expansion
// it is null.
//

typedef struct _MMPAGE_FILE_EXPANSION {
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    SIZE_T RequestedExpansionSize;
    SIZE_T ActualExpansion;
    KEVENT Event;
    LONG InProgress;
    ULONG PageFileNumber;
} MMPAGE_FILE_EXPANSION, *PMMPAGE_FILE_EXPANSION;

#define MI_EXTEND_ANY_PAGEFILE      ((ULONG)-1)
#define MI_CONTRACT_PAGEFILES       ((SIZE_T)-1)

typedef struct _MMWORKING_SET_EXPANSION_HEAD {
    LIST_ENTRY ListHead;
} MMWORKING_SET_EXPANSION_HEAD;

#define SUBSECTION_READ_ONLY      1L
#define SUBSECTION_READ_WRITE     2L
#define SUBSECTION_COPY_ON_WRITE  4L
#define SUBSECTION_SHARE_ALLOW    8L

//
// The MMINPAGE_FLAGS relies on the fact that a pool allocation is always
// QUADWORD aligned so the low 3 bits are always available.
//

typedef struct _MMINPAGE_FLAGS {
    ULONG_PTR Completed : 1;
    ULONG_PTR Available1 : 1;
    ULONG_PTR Available2 : 1;
#if defined (_WIN64)
    ULONG_PTR PrefetchMdlHighBits : 61;
#else
    ULONG_PTR PrefetchMdlHighBits : 29;
#endif
} MMINPAGE_FLAGS, *PMMINPAGE_FLAGS;

#define MI_EXTRACT_PREFETCH_MDL(_Support) ((PMDL)((ULONG_PTR)(_Support->u1.PrefetchMdl) & ~(sizeof(QUAD) - 1)))

typedef struct _MMINPAGE_SUPPORT {
    KEVENT Event;
    IO_STATUS_BLOCK IoStatus;
    LARGE_INTEGER ReadOffset;
    LONG WaitCount;
#if defined (_WIN64)
    ULONG UsedPageTableEntries;
#endif
    PETHREAD Thread;
    PFILE_OBJECT FilePointer;
    PMMPTE BasePte;
    PMMPFN Pfn;
    union {
        MMINPAGE_FLAGS e1;
        ULONG_PTR LongFlags;
        PMDL PrefetchMdl;       // Only used under _PREFETCH_
    } u1;
    MDL Mdl;
    PFN_NUMBER Page[MM_MAXIMUM_READ_CLUSTER_SIZE + 1];
    SINGLE_LIST_ENTRY ListEntry;
} MMINPAGE_SUPPORT, *PMMINPAGE_SUPPORT;

#define MI_PF_DUMMY_PAGE_PTE ((PMMPTE)0x23452345)   // Only used by _PREFETCH_

//
// Address Node.
//

typedef struct _MMADDRESS_NODE {
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    struct _MMADDRESS_NODE *Parent;
    struct _MMADDRESS_NODE *LeftChild;
    struct _MMADDRESS_NODE *RightChild;
} MMADDRESS_NODE, *PMMADDRESS_NODE;

typedef struct _SECTION {
    MMADDRESS_NODE Address;
    PSEGMENT Segment;
    LARGE_INTEGER SizeOfSection;
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    ULONG InitialPageProtection;
} SECTION, *PSECTION;

//
// Banked memory descriptor.  Pointed to by VAD which has
// the PhysicalMemory flags set and the Banked pointer field as
// non-NULL.
//

typedef struct _MMBANKED_SECTION {
    PFN_NUMBER BasePhysicalPage;
    PMMPTE BasedPte;
    ULONG BankSize;
    ULONG BankShift; //shift for PTEs to calculate bank number
    PBANKED_SECTION_ROUTINE BankedRoutine;
    PVOID Context;
    PMMPTE CurrentMappedPte;
    MMPTE BankTemplate[1];
} MMBANKED_SECTION, *PMMBANKED_SECTION;


//
// Virtual address descriptor
//
// ***** NOTE **********
//  The first part of a virtual address descriptor is a MMADDRESS_NODE!!!
//

#if defined (_WIN64)

#define COMMIT_SIZE 51

#if ((COMMIT_SIZE + PAGE_SHIFT) < 63)
#error COMMIT_SIZE too small
#endif

#else
#define COMMIT_SIZE 19

#if ((COMMIT_SIZE + PAGE_SHIFT) < 31)
#error COMMIT_SIZE too small
#endif
#endif

#define MM_MAX_COMMIT (((ULONG_PTR) 1 << COMMIT_SIZE) - 1)

#define MM_VIEW_UNMAP 0
#define MM_VIEW_SHARE 1

typedef struct _MMVAD_FLAGS {
    ULONG_PTR CommitCharge : COMMIT_SIZE; //limits system to 4k pages or bigger!
    ULONG_PTR PhysicalMapping : 1;
    ULONG_PTR ImageMap : 1;
    ULONG_PTR UserPhysicalPages : 1;
    ULONG_PTR NoChange : 1;
    ULONG_PTR WriteWatch : 1;
    ULONG_PTR Protection : 5;
    ULONG_PTR LargePages : 1;
    ULONG_PTR MemCommit: 1;
    ULONG_PTR PrivateMemory : 1;    //used to tell VAD from VAD_SHORT
} MMVAD_FLAGS;

typedef struct _MMVAD_FLAGS2 {
    unsigned FileOffset : 24;       // number of 64k units into file
    unsigned SecNoChange : 1;       // set if SEC_NOCHANGE specified
    unsigned OneSecured : 1;        // set if u3 field is a range
    unsigned MultipleSecured : 1;   // set if u3 field is a list head
    unsigned ReadOnly : 1;          // protected as ReadOnly
    unsigned LongVad : 1;           // set if VAD is a long VAD
    unsigned ExtendableFile : 1;
    unsigned Inherit : 1;           //1 = ViewShare, 0 = ViewUnmap
    unsigned CopyOnWrite : 1;
} MMVAD_FLAGS2;

typedef struct _MMADDRESS_LIST {
    ULONG_PTR StartVpn;
    ULONG_PTR EndVpn;
} MMADDRESS_LIST, *PMMADDRESS_LIST;

typedef struct _MMSECURE_ENTRY {
    union {
        ULONG_PTR LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
    ULONG_PTR StartVpn;
    ULONG_PTR EndVpn;
    LIST_ENTRY List;
} MMSECURE_ENTRY, *PMMSECURE_ENTRY;

typedef struct _ALIAS_VAD_INFO {
    KAPC Apc;
    ULONG NumberOfEntries;
    ULONG MaximumEntries;
} ALIAS_VAD_INFO, *PALIAS_VAD_INFO;

typedef struct _ALIAS_VAD_INFO2 {
    ULONG BaseAddress;
    HANDLE SecureHandle;
} ALIAS_VAD_INFO2, *PALIAS_VAD_INFO2;

typedef struct _MMVAD {
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    struct _MMVAD *Parent;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
    PCONTROL_AREA ControlArea;
    PMMPTE FirstPrototypePte;
    PMMPTE LastContiguousPte;
    union {
        ULONG LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
} MMVAD, *PMMVAD;

typedef struct _MMVAD_LONG {
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    struct _MMVAD *Parent;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
    PCONTROL_AREA ControlArea;
    PMMPTE FirstPrototypePte;
    PMMPTE LastContiguousPte;
    union {
        ULONG LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
    union {
        LIST_ENTRY List;
        MMADDRESS_LIST Secured;
    } u3;
    union {
        PMMBANKED_SECTION Banked;
        PMMEXTEND_INFO ExtendedInfo;
    } u4;
#if defined(_MIALT4K_)
    PALIAS_VAD_INFO AliasInformation;
#endif
} MMVAD_LONG, *PMMVAD_LONG;

typedef struct _MMVAD_SHORT {
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    struct _MMVAD *Parent;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
} MMVAD_SHORT, *PMMVAD_SHORT;

#define MI_GET_PROTECTION_FROM_VAD(_Vad) ((ULONG)(_Vad)->u.VadFlags.Protection)

#define MI_PHYSICAL_VIEW_AWE    0x1     // AWE region
#define MI_PHYSICAL_VIEW_PHYS   0x2     // Device\PhysicalMemory region

typedef struct _MI_PHYSICAL_VIEW {
    LIST_ENTRY ListEntry;
    PMMVAD Vad;
    PCHAR StartVa;
    PCHAR EndVa;
    union {
        ULONG_PTR LongFlags;    // physical or AWE Vad identification
        PRTL_BITMAP BitMap;     // only if Vad->u.VadFlags.WriteWatch == 1
    } u;
} MI_PHYSICAL_VIEW, *PMI_PHYSICAL_VIEW;

#define MI_PHYSICAL_VIEW_KEY 'vpmM'
#define MI_WRITEWATCH_VIEW_KEY 'wWmM'

//
// Stuff for support of Write Watch.
//

extern ULONG_PTR MiActiveWriteWatch;

VOID
MiCaptureWriteWatchDirtyBit (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    );

//
// Stuff for support of AWE (Address Windowing Extensions).
//

typedef struct _AWEINFO {
    PRTL_BITMAP VadPhysicalPagesBitMap;
    ULONG_PTR VadPhysicalPages;

    //
    // The PushLock is used to allow most of the NtMapUserPhysicalPages{Scatter}
    // to execute in parallel as this is acquired shared for these calls.
    // Exclusive acquisitions are used to protect maps against frees of the
    // pages as well as to protect updates to the AweVadList.  Collisions
    // should be rare because the exclusive acquisitions should be rare.
    //

    PEX_PUSH_LOCK_CACHE_AWARE PushLock;

    LIST_ENTRY AweVadList;
} AWEINFO, *PAWEINFO;

VOID
MiAweViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

VOID
MiAweViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

//
// Stuff for support of POSIX Fork.
//

typedef struct _MMCLONE_BLOCK {
    MMPTE ProtoPte;
    LONG CloneRefCount;
} MMCLONE_BLOCK, *PMMCLONE_BLOCK;

typedef struct _MMCLONE_HEADER {
    ULONG NumberOfPtes;
    LONG NumberOfProcessReferences;
    PMMCLONE_BLOCK ClonePtes;
} MMCLONE_HEADER, *PMMCLONE_HEADER;

typedef struct _MMCLONE_DESCRIPTOR {
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    struct _MMCLONE_DESCRIPTOR *Parent;
    struct _MMCLONE_DESCRIPTOR *LeftChild;
    struct _MMCLONE_DESCRIPTOR *RightChild;
    ULONG NumberOfPtes;
    PMMCLONE_HEADER CloneHeader;
    LONG NumberOfReferences;
    LONG FinalNumberOfReferences;
    SIZE_T PagedPoolQuotaCharge;
} MMCLONE_DESCRIPTOR, *PMMCLONE_DESCRIPTOR;

//
// The following macro allocates and initializes a bitmap from the
// specified pool of the specified size.
//
//      VOID
//      MiCreateBitMap (
//          OUT PRTL_BITMAP *BitMapHeader,
//          IN SIZE_T SizeOfBitMap,
//          IN POOL_TYPE PoolType
//          );
//

#define MiCreateBitMap(BMH,S,P) {                          \
    ULONG _S;                                              \
    ASSERT ((ULONG64)(S) < _4gb);                          \
    _S = sizeof(RTL_BITMAP) + (ULONG)((((S) + 31) / 32) * 4);         \
    *(BMH) = (PRTL_BITMAP)ExAllocatePoolWithTag( (P), _S, '  mM');       \
    if (*(BMH)) { \
        RtlInitializeBitMap( *(BMH), (PULONG)((*(BMH))+1), (ULONG)(S)); \
    }                                                          \
}

#define MiRemoveBitMap(BMH)     {                          \
    ExFreePool(*(BMH));                                    \
    *(BMH) = NULL;                                         \
}

#define MI_INITIALIZE_ZERO_MDL(MDL) { \
    MDL->Next = (PMDL) NULL; \
    MDL->MdlFlags = 0; \
    MDL->StartVa = NULL; \
    MDL->ByteOffset = 0; \
    MDL->ByteCount = 0; \
    }

//
// Page File structures.
//

typedef struct _MMMOD_WRITER_LISTHEAD {
    LIST_ENTRY ListHead;
    KEVENT Event;
} MMMOD_WRITER_LISTHEAD, *PMMMOD_WRITER_LISTHEAD;

typedef struct _MMMOD_WRITER_MDL_ENTRY {
    LIST_ENTRY Links;
    LARGE_INTEGER WriteOffset;
    union {
        IO_STATUS_BLOCK IoStatus;
        LARGE_INTEGER LastByte;
    } u;
    PIRP Irp;
    ULONG_PTR LastPageToWrite;
    PMMMOD_WRITER_LISTHEAD PagingListHead;
    PLIST_ENTRY CurrentList;
    struct _MMPAGING_FILE *PagingFile;
    PFILE_OBJECT File;
    PCONTROL_AREA ControlArea;
    PERESOURCE FileResource;
    MDL Mdl;
    PFN_NUMBER Page[1];
} MMMOD_WRITER_MDL_ENTRY, *PMMMOD_WRITER_MDL_ENTRY;


#define MM_PAGING_FILE_MDLS 2

typedef struct _MMPAGING_FILE {
    PFN_NUMBER Size;
    PFN_NUMBER MaximumSize;
    PFN_NUMBER MinimumSize;
    PFN_NUMBER FreeSpace;
    PFN_NUMBER CurrentUsage;
    PFN_NUMBER PeakUsage;
    PFN_NUMBER Hint;
    PFN_NUMBER HighestPage;
    PMMMOD_WRITER_MDL_ENTRY Entry[MM_PAGING_FILE_MDLS];
    PRTL_BITMAP Bitmap;
    PFILE_OBJECT File;
    UNICODE_STRING PageFileName;
    ULONG PageFileNumber;
    BOOLEAN Extended;
    BOOLEAN HintSetToZero;
    BOOLEAN BootPartition;
    HANDLE FileHandle;
} MMPAGING_FILE, *PMMPAGING_FILE;

//
// System PTE structures.
//

typedef enum _MMSYSTEM_PTE_POOL_TYPE {
    SystemPteSpace,
    NonPagedPoolExpansion,
    MaximumPtePoolTypes
} MMSYSTEM_PTE_POOL_TYPE;

typedef struct _MMFREE_POOL_ENTRY {
    LIST_ENTRY List;        // maintained free&chk, 1st entry only
    PFN_NUMBER Size;        // maintained free&chk, 1st entry only
    ULONG Signature;        // maintained chk only, all entries
    struct _MMFREE_POOL_ENTRY *Owner; // maintained free&chk, all entries
} MMFREE_POOL_ENTRY, *PMMFREE_POOL_ENTRY;


typedef struct _MMLOCK_CONFLICT {
    LIST_ENTRY List;
    PETHREAD Thread;
} MMLOCK_CONFLICT, *PMMLOCK_CONFLICT;

//
// System view structures
//

typedef struct _MMVIEW {
    ULONG_PTR Entry;
    PCONTROL_AREA ControlArea;
} MMVIEW, *PMMVIEW;

//
// The MMSESSION structure represents kernel memory that is only valid on a
// per-session basis, thus the calling thread must be in the proper session
// to access this structure.
//

typedef struct _MMSESSION {

    //
    // Never refer to the SystemSpaceViewLock directly - always use the pointer
    // following it or you will break support for multiple concurrent sessions.
    //

    FAST_MUTEX SystemSpaceViewLock;

    //
    // This points to the mutex above and is needed because the MMSESSION
    // is mapped in session space on Hydra and the mutex needs to be globally
    // visible for proper KeWaitForSingleObject & KeSetEvent operation.
    //

    PFAST_MUTEX SystemSpaceViewLockPointer;
    PCHAR SystemSpaceViewStart;
    PMMVIEW SystemSpaceViewTable;
    ULONG SystemSpaceHashSize;
    ULONG SystemSpaceHashEntries;
    ULONG SystemSpaceHashKey;
    PRTL_BITMAP SystemSpaceBitMap;

} MMSESSION, *PMMSESSION;

extern MMSESSION   MmSession;

#define LOCK_SYSTEM_VIEW_SPACE(_Session) \
            ExAcquireFastMutex (_Session->SystemSpaceViewLockPointer)

#define UNLOCK_SYSTEM_VIEW_SPACE(_Session) \
            ExReleaseFastMutex (_Session->SystemSpaceViewLockPointer)

//
// List for flushing TBs singularly.
//

typedef struct _MMPTE_FLUSH_LIST {
    ULONG Count;
    PMMPTE FlushPte[MM_MAXIMUM_FLUSH_COUNT];
    PVOID FlushVa[MM_MAXIMUM_FLUSH_COUNT];
} MMPTE_FLUSH_LIST, *PMMPTE_FLUSH_LIST;

typedef struct _LOCK_TRACKER {
    LIST_ENTRY ListEntry;
    PMDL Mdl;
    PVOID StartVa;
    PFN_NUMBER Count;
    ULONG Offset;
    ULONG Length;
    PFN_NUMBER Page;
    PVOID CallingAddress;
    PVOID CallersCaller;
    LIST_ENTRY GlobalListEntry;
    ULONG Who;
    PEPROCESS Process;
} LOCK_TRACKER, *PLOCK_TRACKER;

extern LOGICAL  MmTrackLockedPages;
extern BOOLEAN  MiTrackingAborted;
extern KSPIN_LOCK MiTrackLockedPagesLock;

typedef struct _LOCK_HEADER {
    LIST_ENTRY ListHead;
    PFN_NUMBER Count;
} LOCK_HEADER, *PLOCK_HEADER;

extern LOGICAL MmSnapUnloads;

#define MI_UNLOADED_DRIVERS 50

extern ULONG MmLastUnloadedDriver;
extern PUNLOADED_DRIVERS MmUnloadedDrivers;


VOID
MiInitMachineDependent (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiReportPhysicalMemory (
    VOID
    );

extern PFN_NUMBER MiNumberOfCompressionPages;

NTSTATUS
MiArmCompressionInterrupt (
    VOID
    );

VOID
MiBuildPagedPool (
    VOID
    );

VOID
MiInitializeNonPagedPool (
    VOID
    );

LOGICAL
MiInitializeSystemSpaceMap (
    PVOID Session OPTIONAL
    );

VOID
MiFindInitializationCode (
    OUT PVOID *StartVa,
    OUT PVOID *EndVa
    );

VOID
MiFreeInitializationCode (
    IN PVOID StartVa,
    IN PVOID EndVa
    );

extern ULONG MiNonCachedCollisions;

//
// If /NOLOWMEM is used, this is set to the boundary PFN (pages below this
// value are not used whenever possible).
//

extern PFN_NUMBER MiNoLowMemory;

PVOID
MiAllocateLowMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PVOID CallingAddress,
    IN MEMORY_CACHING_TYPE CacheType,
    IN ULONG Tag
    );

LOGICAL
MiFreeLowMemory (
    IN PVOID BaseAddress,
    IN ULONG Tag
    );

//
// Move drivers out of the low 16mb that ntldr placed them at - this makes more
// memory below 16mb available for ISA-type drivers that cannot run without it.
//

extern LOGICAL MmMakeLowMemory;

VOID
MiRemoveLowPages (
    IN ULONG RemovePhase
    );

ULONG
MiSectionInitialization (
    VOID
    );

#define MI_MAX_DEREFERENCE_CHUNK (64 * 1024 / PAGE_SIZE)

typedef struct _MI_PFN_DEREFERENCE_CHUNK {
    SINGLE_LIST_ENTRY ListEntry;
    CSHORT Flags;
    USHORT NumberOfPages;
    PFN_NUMBER Pfns[MI_MAX_DEREFERENCE_CHUNK];
} MI_PFN_DEREFERENCE_CHUNK, *PMI_PFN_DEREFERENCE_CHUNK;

extern SLIST_HEADER MmPfnDereferenceSListHead;
extern PSINGLE_LIST_ENTRY MmPfnDeferredList;

#define MI_DEFER_PFN_HELD               0x1
#define MI_DEFER_DRAIN_LOCAL_ONLY       0x2

VOID
MiDeferredUnlockPages (
     ULONG Flags
     );

LOGICAL
MiFreeAllExpansionNonPagedPool (
    IN LOGICAL PoolLockHeld
    );

VOID
FASTCALL
MiDecrementReferenceCount (
    IN PFN_NUMBER PageFrameIndex
    );

//++
//VOID
//MiDecrementReferenceCountInline (
//    IN PMMPFN PFN
//    IN PFN_NUMBER FRAME
//    );
//
// Routine Description:
//
//    MiDecrementReferenceCountInline decrements the reference count inline,
//    only calling MiDecrementReferenceCount if the count would go to zero
//    which would cause the page to be released.
//
// Arguments:
//
//    PFN - Supplies the PFN to decrement.
//
//    FRAME - Supplies the frame matching the above PFN.
//
// Return Value:
//
//    None.
//
// Environment:
//
//    PFN lock held.
//
//--

#define MiDecrementReferenceCountInline(PFN, FRAME)                     \
            MM_PFN_LOCK_ASSERT();                                       \
            ASSERT (MI_PFN_ELEMENT(FRAME) == (PFN));                    \
            ASSERT ((FRAME) <= MmHighestPhysicalPage);                  \
            ASSERT ((PFN)->u3.e2.ReferenceCount != 0);                  \
            if ((PFN)->u3.e2.ReferenceCount != 1) {                     \
                (PFN)->u3.e2.ReferenceCount -= 1;                       \
            }                                                           \
            else {                                                      \
                MiDecrementReferenceCount (FRAME);                      \
            }

VOID
FASTCALL
MiDecrementShareCount (
    IN PFN_NUMBER PageFrameIndex
    );

#define MiDecrementShareCountOnly(P) MiDecrementShareCount(P)

#define MiDecrementShareAndValidCount(P) MiDecrementShareCount(P)

//++
//VOID
//MiDecrementShareCountInline (
//    IN PMMPFN PFN,
//    IN PFN_NUMBER FRAME
//    );
//
// Routine Description:
//
//    MiDecrementShareCountInline decrements the share count inline,
//    only calling MiDecrementShareCount if the count would go to zero
//    which would cause the page to be released.
//
// Arguments:
//
//    PFN - Supplies the PFN to decrement.
//
//    FRAME - Supplies the frame matching the above PFN.
//
// Return Value:
//
//    None.
//
// Environment:
//
//    PFN lock held.
//
//--

#define MiDecrementShareCountInline(PFN, FRAME)                         \
            MM_PFN_LOCK_ASSERT();                                       \
            ASSERT (((FRAME) <= MmHighestPhysicalPage) && ((FRAME) > 0));   \
            ASSERT (MI_PFN_ELEMENT(FRAME) == (PFN));                    \
            ASSERT ((PFN)->u2.ShareCount != 0);                         \
            if ((PFN)->u3.e1.PageLocation != ActiveAndValid && (PFN)->u3.e1.PageLocation != StandbyPageList) {                                            \
                KeBugCheckEx (PFN_LIST_CORRUPT, 0x99, FRAME, (PFN)->u3.e1.PageLocation, 0);                                                             \
            }                                                           \
            if ((PFN)->u2.ShareCount != 1) {                            \
                (PFN)->u2.ShareCount -= 1;                              \
                PERFINFO_DECREFCNT((PFN), PERF_SOFT_TRIM, PERFINFO_LOG_TYPE_DECSHARCNT); \
                ASSERT ((PFN)->u2.ShareCount < 0xF000000);              \
            }                                                           \
            else {                                                      \
                MiDecrementShareCount (FRAME);                          \
            }

//
// Routines which operate on the Page Frame Database Lists
//

VOID
FASTCALL
MiInsertPageInList (
    IN PMMPFNLIST ListHead,
    IN PFN_NUMBER PageFrameIndex
    );

VOID
FASTCALL
MiInsertPageInFreeList (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
FASTCALL
MiInsertStandbyListAtFront (
    IN PFN_NUMBER PageFrameIndex
    );

PFN_NUMBER  //PageFrameIndex
FASTCALL
MiRemovePageFromList (
    IN PMMPFNLIST ListHead
    );

VOID
FASTCALL
MiUnlinkPageFromList (
    IN PMMPFN Pfn
    );

VOID
MiUnlinkFreeOrZeroedPage (
    IN PFN_NUMBER Page
    );

VOID
FASTCALL
MiInsertFrontModifiedNoWrite (
    IN PFN_NUMBER PageFrameIndex
    );

#define MM_MEDIUM_LIMIT 32

#define MM_HIGH_LIMIT 128

ULONG
FASTCALL
MiEnsureAvailablePageOrWait (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    );

PFN_NUMBER
MiAllocatePfn (
    IN PMMPTE PointerPte,
    IN ULONG Protection
    );

PFN_NUMBER
FASTCALL
MiRemoveAnyPage (
    IN ULONG PageColor
    );

PFN_NUMBER
FASTCALL
MiRemoveZeroPage (
    IN ULONG PageColor
    );

VOID
MiPurgeTransitionList (
    VOID
    );

PVOID
MiFindContiguousMemory (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN MEMORY_CACHING_TYPE CacheType,
    IN PVOID CallingAddress
    );

PVOID
MiCheckForContiguousMemory (
    IN PVOID BaseAddress,
    IN PFN_NUMBER BaseAddressPages,
    IN PFN_NUMBER SizeInPages,
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    );

//
// Routines which operate on the page frame database entry.
//

VOID
MiInitializePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN ULONG ModifiedState
    );

VOID
MiInitializePfnForOtherProcess (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN PFN_NUMBER ContainingPageFrame
    );

VOID
MiInitializeCopyOnWritePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN WSLE_NUMBER WorkingSetIndex,
    IN PVOID SessionSpace
    );

VOID
MiInitializeTransitionPfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte
    );

extern SLIST_HEADER MmInPageSupportSListHead;

VOID
MiFreeInPageSupportBlock (
    IN PMMINPAGE_SUPPORT Support
    );

PMMINPAGE_SUPPORT
MiGetInPageSupportBlock (
    IN LOGICAL PfnHeld,
    IN PEPROCESS Process
    );

//
// Routines which require a physical page to be mapped into hyperspace
// within the current process.
//

VOID
FASTCALL
MiZeroPhysicalPage (
    IN PFN_NUMBER PageFrameIndex,
    IN ULONG Color
    );

VOID
FASTCALL
MiRestoreTransitionPte (
    IN PFN_NUMBER PageFrameIndex
    );

PSUBSECTION
MiGetSubsectionAndProtoFromPte (
    IN PMMPTE PointerPte,
    IN PMMPTE *ProtoPte
    );

PVOID
MiMapPageInHyperSpace (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex,
    OUT PKIRQL OldIrql
    );

PVOID
MiMapPageInHyperSpaceAtDpc (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex
    );

#define MiUnmapPageInZeroSpace(VA) \
    MiGetPteAddress(VA)->u.Long = 0;

PVOID
MiMapImageHeaderInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiUnmapImageHeaderInHyperSpace (
    VOID
    );

VOID
MiUpdateImageHeaderPage (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER PageFrameNumber,
    IN PCONTROL_AREA ControlArea
    );

PFN_NUMBER
MiGetPageForHeader (
    VOID
    );

VOID
MiRemoveImageHeaderPage (
    IN PFN_NUMBER PageFrameNumber
    );

PVOID
MiMapPageToZeroInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    );


NTSTATUS
MiGetWritablePagesInSection(
    IN PSECTION Section,
    OUT PULONG WritablePages
    );


//
// Routines to obtain and release system PTEs.
//

PMMPTE
MiReserveSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

PMMPTE
MiReserveAlignedSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType,
    IN ULONG Alignment
    );

VOID
MiReleaseSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

VOID
MiReleaseSplitSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

VOID
MiIncrementSystemPtes (
    IN ULONG  NumberOfPtes
    );

LOGICAL
MiGetSystemPteAvailability (
    IN ULONG NumberOfPtes,
    IN MM_PAGE_PRIORITY Priority
    );

VOID
MiIssueNoPtesBugcheck (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

VOID
MiInitializeSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

NTSTATUS
MiAddMappedPtes (
    IN PMMPTE FirstPte,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea
    );

VOID
MiInitializeIoTrackers (
    VOID
    );

PVOID
MiMapSinglePage (
     IN PVOID VirtualAddress OPTIONAL,
     IN PFN_NUMBER PageFrameIndex,
     IN MEMORY_CACHING_TYPE CacheType,
     IN MM_PAGE_PRIORITY Priority
     );

VOID
MiUnmapSinglePage (
     IN PVOID BaseAddress
     );

typedef struct _MM_PTE_MAPPING {
    LIST_ENTRY ListEntry;
    PVOID SystemVa;
    PVOID SystemEndVa;
    ULONG Protection;
} MM_PTE_MAPPING, *PMM_PTE_MAPPING;

extern LIST_ENTRY MmProtectedPteList;

extern KSPIN_LOCK MmProtectedPteLock;

LOGICAL
MiCheckSystemPteProtection (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress
    );

//
// Access Fault routines.
//

#define STATUS_ISSUE_PAGING_IO (0xC0033333)

NTSTATUS
MiDispatchFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAdress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN PEPROCESS Process,
    OUT PLOGICAL ApcNeeded
    );

NTSTATUS
MiResolveDemandZeroFault (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN ULONG PrototypePte
    );

NTSTATUS
MiResolveTransitionFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN ULONG PfnLockHeld,
    OUT PLOGICAL ApcNeeded,
    OUT PMMINPAGE_SUPPORT *InPageBlock
    );

NTSTATUS
MiResolvePageFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process
    );

NTSTATUS
MiResolveProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    OUT PLOGICAL ApcNeeded
    );


NTSTATUS
MiResolveMappedFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process
    );

VOID
MiAddValidPageToWorkingSet (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG WsleMask
    );

NTSTATUS
MiWaitForInPageComplete (
    IN PMMPFN Pfn,
    IN PMMPTE PointerPte,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPteContents,
    IN PMMINPAGE_SUPPORT InPageSupport,
    IN PEPROCESS CurrentProcess
    );

LOGICAL
FASTCALL
MiCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    );

VOID
MiSetDirtyBit (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN ULONG PfnHeld
    );

VOID
MiSetModifyBit (
    IN PMMPFN Pfn
    );

PMMPTE
MiFindActualFaultingPte (
    IN PVOID FaultingAddress
    );

VOID
MiInitializeReadInProgressSinglePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    );

VOID
MiInitializeReadInProgressPfn (
    IN PMDL Mdl,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    );

NTSTATUS
MiAccessCheck (
    IN PMMPTE PointerPte,
    IN ULONG_PTR WriteOperation,
    IN KPROCESSOR_MODE PreviousMode,
    IN ULONG Protection,
    IN BOOLEAN CallerHoldsPfnLock
    );

NTSTATUS
FASTCALL
MiCheckForUserStackOverflow (
    IN PVOID FaultingAddress
    );

PMMPTE
MiCheckVirtualAddress (
    IN PVOID VirtualAddress,
    OUT PULONG ProtectCode
    );

NTSTATUS
FASTCALL
MiCheckPdeForPagedPool (
    IN PVOID VirtualAddress
    );

//
// Routines which operate on an address tree.
//

PMMADDRESS_NODE
FASTCALL
MiGetNextNode (
    IN PMMADDRESS_NODE Node
    );

PMMADDRESS_NODE
FASTCALL
MiGetPreviousNode (
    IN PMMADDRESS_NODE Node
    );


PMMADDRESS_NODE
FASTCALL
MiGetFirstNode (
    IN PMMADDRESS_NODE Root
    );

PMMADDRESS_NODE
MiGetLastNode (
    IN PMMADDRESS_NODE Root
    );

VOID
FASTCALL
MiInsertNode (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    );

VOID
FASTCALL
MiRemoveNode (
    IN PMMADDRESS_NODE Node,
    IN OUT PMMADDRESS_NODE *Root
    );

PMMADDRESS_NODE
FASTCALL
MiLocateAddressInTree (
    IN ULONG_PTR Vpn,
    IN PMMADDRESS_NODE *Root
    );

PMMADDRESS_NODE
MiCheckForConflictingNode (
    IN ULONG_PTR StartVpn,
    IN ULONG_PTR EndVpn,
    IN PMMADDRESS_NODE Root
    );

NTSTATUS
MiFindEmptyAddressRangeInTree (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN PMMADDRESS_NODE Root,
    OUT PMMADDRESS_NODE *PreviousVad,
    OUT PVOID *Base
    );

NTSTATUS
MiFindEmptyAddressRangeDownTree (
    IN SIZE_T SizeOfRange,
    IN PVOID HighestAddressToEndAt,
    IN ULONG_PTR Alignment,
    IN PMMADDRESS_NODE Root,
    OUT PVOID *Base
    );

VOID
NodeTreeWalk (
    PMMADDRESS_NODE Start
    );

//
// Routines which operate on the tree of virtual address descriptors.
//

NTSTATUS
MiInsertVad (
    IN PMMVAD Vad
    );

VOID
MiRemoveVad (
    IN PMMVAD Vad
    );

PMMVAD
FASTCALL
MiLocateAddress (
    IN PVOID Vad
    );

NTSTATUS
MiFindEmptyAddressRange (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN ULONG QuickCheck,
    IN PVOID *Base
    );

//
// Routines which operate on the clone tree structure.
//


NTSTATUS
MiCloneProcessAddressSpace (
    IN PEPROCESS ProcessToClone,
    IN PEPROCESS ProcessToInitialize,
    IN PFN_NUMBER PdePhysicalPage,
    IN PFN_NUMBER HyperPhysicalPage
    );


ULONG
MiDecrementCloneBlockReference (
    IN PMMCLONE_DESCRIPTOR CloneDescriptor,
    IN PMMCLONE_BLOCK CloneBlock,
    IN PEPROCESS CurrentProcess
    );

LOGICAL
MiWaitForForkToComplete (
    IN PEPROCESS CurrentProcess,
    IN LOGICAL PfnHeld
    );

//
// Routines which operate on the working set list.
//

WSLE_NUMBER
MiLocateAndReserveWsle (
    IN PMMSUPPORT WsInfo
    );

VOID
MiReleaseWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo
    );

VOID
MiUpdateWsle (
    IN PWSLE_NUMBER DesiredIndex,
    IN PVOID VirtualAddress,
    IN PMMWSL WorkingSetList,
    IN PMMPFN Pfn
    );

VOID
MiInitializeWorkingSetList (
    IN PEPROCESS CurrentProcess
    );

VOID
MiGrowWsleHash (
    IN PMMSUPPORT WsInfo
    );

WSLE_NUMBER
MiTrimWorkingSet (
    IN WSLE_NUMBER Reduction,
    IN PMMSUPPORT WsInfo,
    IN ULONG TrimAge
    );

LOGICAL
MmTrimProcessMemory (
    IN LOGICAL PurgeTransition
    );

LOGICAL
MmTrimSessionMemory (
    IN LOGICAL PurgeTransition
    );

VOID
MiRemoveWorkingSetPages (
    IN PMMWSL WorkingSetList,
    IN PMMSUPPORT WsInfo
    );

VOID
MiAgeAndEstimateAvailableInWorkingSet (
    IN PMMSUPPORT VmSupport,
    IN LOGICAL DoAging,
    IN PWSLE_NUMBER WslesScanned,
    IN OUT PPFN_NUMBER TotalClaim,
    IN OUT PPFN_NUMBER TotalEstimatedAvailable
    );

VOID
FASTCALL
MiInsertWsleHash (
    IN WSLE_NUMBER Entry,
    IN PMMWSL WorkingSetList
    );

VOID
FASTCALL
MiRemoveWsle (
    IN WSLE_NUMBER Entry,
    IN PMMWSL WorkingSetList
    );

WSLE_NUMBER
FASTCALL
MiLocateWsle (
    IN PVOID VirtualAddress,
    IN PMMWSL WorkingSetList,
    IN WSLE_NUMBER WsPfnIndex
    );

ULONG
MiFreeWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte
    );

VOID
MiSwapWslEntries (
    IN WSLE_NUMBER SwapEntry,
    IN WSLE_NUMBER Entry,
    IN PMMSUPPORT WsInfo
    );

VOID
MiRemoveWsleFromFreeList (
    IN WSLE_NUMBER Entry,
    IN PMMWSLE Wsle,
    IN PMMWSL WorkingSetList
    );

ULONG
MiRemovePageFromWorkingSet (
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN PMMSUPPORT WsInfo
    );

PFN_NUMBER
MiDeleteSystemPagableVm (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER NumberOfPtes,
    IN MMPTE NewPteValue,
    IN LOGICAL SessionAllocation,
    OUT PPFN_NUMBER ResidentPages OPTIONAL
    );

VOID
MiLockCode (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG LockType
    );

PKLDR_DATA_TABLE_ENTRY
MiLookupDataTableEntry (
    IN PVOID AddressWithinSection,
    IN ULONG ResourceHeld
    );

//
// Routines which perform working set management.
//

VOID
MiObtainFreePages (
    VOID
    );

VOID
MiModifiedPageWriter (
    IN PVOID StartContext
    );

VOID
MiMappedPageWriter (
    IN PVOID StartContext
    );

LOGICAL
MiIssuePageExtendRequest (
    IN PMMPAGE_FILE_EXPANSION PageExtend
    );

VOID
MiIssuePageExtendRequestNoWait (
    IN PFN_NUMBER SizeInPages
    );

SIZE_T
MiExtendPagingFiles (
    IN PMMPAGE_FILE_EXPANSION PageExpand
    );

VOID
MiContractPagingFiles (
    VOID
    );

VOID
MiAttemptPageFileReduction (
    VOID
    );

LOGICAL
MiCancelWriteOfMappedPfn (
    IN PFN_NUMBER PageToStop
    );

//
// Routines to delete address space.
//

VOID
MiDeletePteRange (
    IN PEPROCESS Process,
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL AddressSpaceDeletion
    );

VOID
MiDeleteVirtualAddresses (
    IN PUCHAR StartingAddress,
    IN PUCHAR EndingAddress,
    IN ULONG AddressSpaceDeletion,
    IN PMMVAD Vad
    );

ULONG
MiDeletePte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN ULONG AddressSpaceDeletion,
    IN PEPROCESS CurrentProcess,
    IN PMMPTE PrototypePte,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL
    );

VOID
MiDeletePageTablesForPhysicalRange (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

VOID
MiFlushPteList (
    IN PMMPTE_FLUSH_LIST PteFlushList,
    IN ULONG AllProcessors,
    IN MMPTE FillPte
    );

ULONG
FASTCALL
MiReleasePageFileSpace (
    IN MMPTE PteContents
    );

VOID
FASTCALL
MiReleaseConfirmedPageFileSpace (
    IN MMPTE PteContents
    );

VOID
FASTCALL
MiUpdateModifiedWriterMdls (
    IN ULONG PageFileNumber
    );

PVOID
MiAllocateAweInfo (
    VOID
    );

VOID
MiRemoveUserPhysicalPagesVad (
    IN PMMVAD_SHORT FoundVad
    );

VOID
MiCleanPhysicalProcessPages (
    IN PEPROCESS Process
    );

VOID
MiPhysicalViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

VOID
MiPhysicalViewAdjuster (
    IN PEPROCESS Process,
    IN PMMVAD OldVad,
    IN PMMVAD NewVad
    );

LOGICAL
MiIsPhysicalMemoryAddress (
    IN PFN_NUMBER PageFrameIndex,
    IN OUT PULONG Hint,
    IN LOGICAL PfnLockNeeded
    );

//
// MM_SYSTEM_PAGE_COLOR - MmSystemPageColor
//
// This variable is updated frequently, on MP systems we keep
// a separate system color per processor to avoid cache line
// thrashing.
//

#if defined(NT_UP)

#define MI_SYSTEM_PAGE_COLOR    MmSystemPageColor

#else

#define MI_SYSTEM_PAGE_COLOR    (KeGetCurrentPrcb()->PageColor)

#endif

#if defined(MI_MULTINODE)

extern PKNODE KeNodeBlock[];

#define MI_NODE_FROM_COLOR(c)                                               \
        (KeNodeBlock[(c) >> MmSecondaryColorNodeShift])

#define MI_GET_COLOR_FROM_LIST_ENTRY(index,pfn)                             \
    ((ULONG)(((pfn)->u3.e1.PageColor << MmSecondaryColorNodeShift) |        \
         MI_GET_SECONDARY_COLOR((index),(pfn))))

#define MI_ADJUST_COLOR_FOR_NODE(c,n)   ((c) | (n)->Color)
#define MI_CURRENT_NODE_COLOR           (KeGetCurrentNode()->MmShiftedColor)

#define MiRemoveZeroPageIfAny(c)                                            \
        (KeGetCurrentNode()->FreeCount[ZeroedPageList] ? MiRemoveZeroPage(c) : 0)

#define MI_GET_PAGE_COLOR_NODE(n)                                           \
        (((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask) |                \
         KeNodeBlock[n]->MmShiftedColor)

#else

#define MI_NODE_FROM_COLOR(c)

#define MI_GET_COLOR_FROM_LIST_ENTRY(index,pfn)                             \
         ((ULONG)MI_GET_SECONDARY_COLOR((index),(pfn)))

#define MI_ADJUST_COLOR_FOR_NODE(c,n)   (c)
#define MI_CURRENT_NODE_COLOR           0

#define MiRemoveZeroPageIfAny(COLOR)   \
    (MmFreePagesByColor[ZeroedPageList][COLOR].Flink != MM_EMPTY_LIST) ? \
                       MiRemoveZeroPage(COLOR) : 0

#define MI_GET_PAGE_COLOR_NODE(n)                                           \
        ((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask)

#endif

FORCEINLINE
PFN_NUMBER
MiRemoveZeroPageMayReleaseLocks (
    IN ULONG Color,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine returns a zeroed page.
    
    It may release and reacquire the PFN lock to do so, as well as mapping
    the page in hyperspace to perform the actual zeroing if necessary.

Environment:

    Kernel mode.  PFN lock held, hyperspace lock NOT held.

--*/

{
    PFN_NUMBER PageFrameIndex;

    PageFrameIndex = MiRemoveZeroPageIfAny (Color);

    if (PageFrameIndex == 0) {
        PageFrameIndex = MiRemoveAnyPage (Color);
        UNLOCK_PFN (OldIrql);
        MiZeroPhysicalPage (PageFrameIndex, Color);
        LOCK_PFN (OldIrql);
    }

    return PageFrameIndex;
}

//
// General support routines.
//

#if (_MI_PAGING_LEVELS <= 3)

//++
//PMMPTE
//MiGetPxeAddress (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPxeAddress returns the address of the extended page directory parent
//    entry which maps the given virtual address.  This is one level above the
//    page parent directory.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PXE for.
//
// Return Value:
//
//    The address of the PXE.
//
//--

#define MiGetPxeAddress(va)   ((PMMPTE)0)

//++
//LOGICAL
//MiIsPteOnPxeBoundary (
//    IN PVOID PTE
//    );
//
// Routine Description:
//
//    MiIsPteOnPxeBoundary returns TRUE if the PTE is
//    on an extended page directory parent entry boundary.
//
// Arguments
//
//    PTE - Supplies the PTE to check.
//
// Return Value:
//
//    TRUE if on a boundary, FALSE if not.
//
//--

#define MiIsPteOnPxeBoundary(PTE) (FALSE)

#endif

#if (_MI_PAGING_LEVELS <= 2)

//++
//PMMPTE
//MiGetPpeAddress (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPpeAddress returns the address of the page directory parent entry
//    which maps the given virtual address.  This is one level above the
//    page directory.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PPE for.
//
// Return Value:
//
//    The address of the PPE.
//
//--

#define MiGetPpeAddress(va)  ((PMMPTE)0)

//++
//LOGICAL
//MiIsPteOnPpeBoundary (
//    IN PVOID VA
//    );
//
// Routine Description:
//
//    MiIsPteOnPpeBoundary returns TRUE if the PTE is
//    on a page directory parent entry boundary.
//
// Arguments
//
//    VA - Supplies the virtual address to check.
//
// Return Value:
//
//    TRUE if on a boundary, FALSE if not.
//
//--

#define MiIsPteOnPpeBoundary(PTE) (FALSE)

#endif

ULONG
MiDoesPdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld,
    OUT PULONG Waited
    );

#if (_MI_PAGING_LEVELS >= 3)
#define MiDoesPpeExistAndMakeValid(PPE, PROCESS, PFNLOCKHELD, WAITED) \
            MiDoesPdeExistAndMakeValid(PPE, PROCESS, PFNLOCKHELD, WAITED)
#else
#define MiDoesPpeExistAndMakeValid(PPE, PROCESS, PFNLOCKHELD, WAITED) 1
#endif

#if (_MI_PAGING_LEVELS >= 4)
#define MiDoesPxeExistAndMakeValid(PXE, PROCESS, PFNLOCKHELD, WAITED) \
            MiDoesPdeExistAndMakeValid(PXE, PROCESS, PFNLOCKHELD, WAITED)
#else
#define MiDoesPxeExistAndMakeValid(PXE, PROCESS, PFNLOCKHELD, WAITED) 1
#endif

VOID
MiMakePdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    );

#if (_MI_PAGING_LEVELS >= 4)
VOID
MiMakePxeExistAndMakeValid (
    IN PMMPTE PointerPpe,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    );
#else
#define MiMakePxeExistAndMakeValid(PDE, PROCESS, PFNLOCKHELD)
#endif

#if (_MI_PAGING_LEVELS >= 3)
VOID
MiMakePpeExistAndMakeValid (
    IN PMMPTE PointerPpe,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    );
#else
#define MiMakePpeExistAndMakeValid(PDE, PROCESS, PFNLOCKHELD)
#endif

ULONG
FASTCALL
MiMakeSystemAddressValid (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfnWs (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess OPTIONAL
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfnSystemWs (
    IN PVOID VirtualAddress
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfn (
    IN PVOID VirtualAddress
    );

ULONG
FASTCALL
MiLockPagedAddress (
    IN PVOID VirtualAddress,
    IN ULONG PfnLockHeld
    );

VOID
FASTCALL
MiUnlockPagedAddress (
    IN PVOID VirtualAddress,
    IN ULONG PfnLockHeld
    );

ULONG
FASTCALL
MiIsPteDecommittedPage (
    IN PMMPTE PointerPte
    );

ULONG
FASTCALL
MiIsProtectionCompatible (
    IN ULONG OldProtect,
    IN ULONG NewProtect
    );

ULONG
FASTCALL
MiIsPteProtectionCompatible (
    IN ULONG OldPteProtection,
    IN ULONG NewProtect
    );

ULONG
FASTCALL
MiMakeProtectionMask (
    IN ULONG Protect
    );

ULONG
MiIsEntireRangeCommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

ULONG
MiIsEntireRangeDecommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

LOGICAL
MiCheckProtoPtePageState (
    IN PMMPTE PrototypePte,
    IN LOGICAL PfnLockHeld,
    OUT PLOGICAL DroppedPfnLock
    );

//++
//PMMPTE
//MiGetProtoPteAddress (
//    IN PMMPTE VAD,
//    IN PVOID VA
//    );
//
// Routine Description:
//
//    MiGetProtoPteAddress returns a pointer to the prototype PTE which
//    is mapped by the given virtual address descriptor and address within
//    the virtual address descriptor.
//
// Arguments
//
//    VAD - Supplies a pointer to the virtual address descriptor that contains
//          the VA.
//
//    VPN - Supplies the virtual page number.
//
// Return Value:
//
//    A pointer to the proto PTE which corresponds to the VA.
//
//--


#define MiGetProtoPteAddress(VAD,VPN)                                        \
    ((((((VPN) - (VAD)->StartingVpn) << PTE_SHIFT) +                         \
      (ULONG_PTR)(VAD)->FirstPrototypePte) <= (ULONG_PTR)(VAD)->LastContiguousPte) ? \
    ((PMMPTE)(((((VPN) - (VAD)->StartingVpn) << PTE_SHIFT) +                 \
        (ULONG_PTR)(VAD)->FirstPrototypePte))) :                                  \
        MiGetProtoPteAddressExtended ((VAD),(VPN)))

PMMPTE
FASTCALL
MiGetProtoPteAddressExtended (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    );

PSUBSECTION
FASTCALL
MiLocateSubsection (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    );

VOID
MiInitializeSystemCache (
    IN ULONG MinimumWorkingSet,
    IN ULONG MaximumWorkingSet
    );

VOID
MiAdjustWorkingSetManagerParameters(
    IN LOGICAL WorkStation
    );

VOID
MiNotifyMemoryEvents (
    VOID
    );

extern PFN_NUMBER MmLowMemoryThreshold;
extern PFN_NUMBER MmHighMemoryThreshold;

//
// Section support
//

VOID
FASTCALL
MiInsertBasedSection (
    IN PSECTION Section
    );

NTSTATUS
MiMapViewOfPhysicalSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN ULONG ProtectionMask,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType,
    IN LOGICAL WriteCombined
    );

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    );

NTSTATUS
MiUnmapViewOfSection (
    IN PEPROCESS Process,
    IN PVOID BaseAddress,
    IN LOGICAL AddressSpaceMutexHeld
    );

VOID
MiRemoveImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA ControlArea
    );

VOID
MiAddSystemPtes(
    IN PMMPTE StartingPte,
    IN ULONG  NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

VOID
MiRemoveMappedView (
    IN PEPROCESS CurrentProcess,
    IN PMMVAD Vad
    );

VOID
MiSegmentDelete (
    PSEGMENT Segment
    );

VOID
MiSectionDelete (
    IN PVOID Object
    );

VOID
MiDereferenceSegmentThread (
    IN PVOID StartContext
    );

NTSTATUS
MiCreateImageFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment
    );

NTSTATUS
MiCreateDataFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN ULONG IgnoreFileSizing
    );

NTSTATUS
MiCreatePagingFileMap (
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG ProtectionMask,
    IN ULONG AllocationAttributes
    );

VOID
MiPurgeSubsectionInternal (
    IN PSUBSECTION Subsection,
    IN ULONG PteOffset
    );

VOID
MiPurgeImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process
    );

VOID
MiCleanSection (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL DirtyDataPagesOk
    );

VOID
MiDereferenceControlArea (
    IN PCONTROL_AREA ControlArea
    );

VOID
MiCheckControlArea (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS CurrentProcess,
    IN KIRQL PreviousIrql
    );

LOGICAL
MiCheckPurgeAndUpMapCount (
    IN PCONTROL_AREA ControlArea
    );

VOID
MiCheckForControlAreaDeletion (
    IN PCONTROL_AREA ControlArea
    );

LOGICAL
MiCheckControlAreaStatus (
    IN SECTION_CHECK_TYPE SectionCheckType,
    IN PSECTION_OBJECT_POINTERS SectionObjectPointers,
    IN ULONG DelayClose,
    OUT PCONTROL_AREA *ControlArea,
    OUT PKIRQL OldIrql
    );

extern SLIST_HEADER MmEventCountSListHead;

PEVENT_COUNTER
MiGetEventCounter (
    VOID
    );

VOID
MiFreeEventCounter (
    IN PEVENT_COUNTER Support
    );

ULONG
MiCanFileBeTruncatedInternal (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN PLARGE_INTEGER NewFileSize OPTIONAL,
    IN LOGICAL BlockNewViews,
    OUT PKIRQL PreviousIrql
    );

#define STATUS_MAPPED_WRITER_COLLISION (0xC0033333)

NTSTATUS
MiFlushSectionInternal (
    IN PMMPTE StartingPte,
    IN PMMPTE FinalPte,
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection,
    IN ULONG Synchronize,
    IN LOGICAL WriteInProgressOk,
    OUT PIO_STATUS_BLOCK IoStatus
    );

//
// protection stuff...
//

NTSTATUS
MiProtectVirtualMemory (
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PSIZE_T CapturedRegionSize,
    IN ULONG Protect,
    IN PULONG LastProtect
    );

ULONG
MiGetPageProtection (
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN LOGICAL PteCapturedToLocalStack
    );

NTSTATUS
MiSetProtectionOnSection (
    IN PEPROCESS Process,
    IN PMMVAD Vad,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN ULONG NewProtect,
    OUT PULONG CapturedOldProtect,
    IN ULONG DontCharge,
    OUT PULONG Locked
    );

NTSTATUS
MiCheckSecuredVad (
    IN PMMVAD Vad,
    IN PVOID Base,
    IN ULONG_PTR Size,
    IN ULONG ProtectionMask
    );

HANDLE
MiSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode,
    IN LOGICAL AddressSpaceMutexHeld
    );

VOID
MiUnsecureVirtualMemory (
    IN HANDLE SecureHandle,
    IN LOGICAL AddressSpaceMutexHeld
    );

ULONG
MiChangeNoAccessForkPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

VOID
MiSetImageProtect (
    IN PSEGMENT Segment,
    IN ULONG Protection
    );

//
// Routines for charging quota and commitment.
//

VOID
MiTrimSegmentCache (
    VOID
    );

VOID
MiInitializeCommitment (
    VOID
    );

LOGICAL
FASTCALL
MiChargeCommitment (
    IN SIZE_T QuotaCharge,
    IN PEPROCESS Process OPTIONAL
    );

LOGICAL
FASTCALL
MiChargeCommitmentCantExpand (
    IN SIZE_T QuotaCharge,
    IN ULONG MustSucceed
    );

LOGICAL
FASTCALL
MiChargeTemporaryCommitmentForReduction (
    IN SIZE_T QuotaCharge
    );

#if defined (_MI_DEBUG_COMMIT_LEAKS)

VOID
FASTCALL
MiReturnCommitment (
    IN SIZE_T QuotaCharge
    );

#else

#define MiReturnCommitment(_QuotaCharge)                                \
            ASSERT ((SSIZE_T)(_QuotaCharge) >= 0);                      \
            ASSERT (MmTotalCommittedPages >= (_QuotaCharge));           \
            InterlockedExchangeAddSizeT (&MmTotalCommittedPages, 0-((SIZE_T)(_QuotaCharge))); \
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NORMAL, (_QuotaCharge));

#endif

VOID
MiCauseOverCommitPopup (
    VOID
    );

extern SIZE_T MmPeakCommitment;

extern SIZE_T MmTotalCommitLimitMaximum;

SIZE_T
MiCalculatePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

VOID
MiReturnPageTablePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PEPROCESS CurrentProcess,
    IN PMMVAD PreviousVad,
    IN PMMVAD NextVad
    );

VOID
MiFlushAllPages (
    VOID
    );

VOID
MiModifiedPageWriterTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    );

LONGLONG
MiStartingOffset(
    IN PSUBSECTION Subsection,
    IN PMMPTE PteAddress
    );

LARGE_INTEGER
MiEndingOffset(
    IN PSUBSECTION Subsection
    );

VOID
MiReloadBootLoadedDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiInitializeLoadedModuleList (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

extern ULONG MmSpecialPoolTag;
extern PVOID MmSpecialPoolStart;
extern PVOID MmSpecialPoolEnd;
extern PVOID MmSessionSpecialPoolStart;
extern PVOID MmSessionSpecialPoolEnd;

LOGICAL
MiInitializeSpecialPool (
    IN POOL_TYPE PoolType
    );

LOGICAL
MiIsSpecialPoolAddressNonPaged (
    IN PVOID VirtualAddress
    );

#if defined (_WIN64)
LOGICAL
MiInitializeSessionSpecialPool (
    VOID
    );

VOID
MiDeleteSessionSpecialPool (
    VOID
    );
#endif

#if defined (_X86_)
LOGICAL
MiRecoverSpecialPtes (
    IN ULONG NumberOfPtes
    );
#endif

VOID
MiEnableRandomSpecialPool (
    IN LOGICAL Enable
    );

LOGICAL
MiTriageSystem (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiTriageAddDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiTriageVerifyDriver (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

extern ULONG MmTriageActionTaken;

#if defined (_WIN64)
#define MM_SPECIAL_POOL_PTES ((1024 * 1024) / sizeof (MMPTE))
#else
#define MM_SPECIAL_POOL_PTES (24 * PTE_PER_PAGE)
#endif

#define MI_SUSPECT_DRIVER_BUFFER_LENGTH 512

extern WCHAR MmVerifyDriverBuffer[];
extern ULONG MmVerifyDriverBufferLength;
extern ULONG MmVerifyDriverLevel;

extern LOGICAL MmDontVerifyRandomDrivers;
extern LOGICAL MmSnapUnloads;
extern LOGICAL MmProtectFreedNonPagedPool;
extern ULONG MmEnforceWriteProtection;
extern LOGICAL MmTrackLockedPages;
extern ULONG MmTrackPtes;

#define VI_POOL_FREELIST_END  ((ULONG_PTR)-1)

typedef struct _VI_POOL_ENTRY_INUSE {
    PVOID VirtualAddress;
    PVOID CallingAddress;
    SIZE_T NumberOfBytes;
    ULONG_PTR Tag;
} VI_POOL_ENTRY_INUSE, *PVI_POOL_ENTRY_INUSE;

typedef struct _VI_POOL_ENTRY {
    union {
        VI_POOL_ENTRY_INUSE InUse;
        ULONG_PTR FreeListNext;
    };
} VI_POOL_ENTRY, *PVI_POOL_ENTRY;

#define MI_VERIFIER_ENTRY_SIGNATURE            0x98761940

typedef struct _MI_VERIFIER_DRIVER_ENTRY {
    LIST_ENTRY Links;
    ULONG Loads;
    ULONG Unloads;

    UNICODE_STRING BaseName;
    PVOID StartAddress;
    PVOID EndAddress;

#define VI_VERIFYING_DIRECTLY   0x1
#define VI_VERIFYING_INVERSELY  0x2
#define VI_DISABLE_VERIFICATION 0x4

    ULONG Flags;
    ULONG_PTR Signature;
    ULONG_PTR Reserved;
    KSPIN_LOCK VerifierPoolLock;

    PVI_POOL_ENTRY PoolHash;
    ULONG_PTR PoolHashSize;
    ULONG_PTR PoolHashFree;
    ULONG_PTR PoolHashReserved;

    ULONG CurrentPagedPoolAllocations;
    ULONG CurrentNonPagedPoolAllocations;
    ULONG PeakPagedPoolAllocations;
    ULONG PeakNonPagedPoolAllocations;

    SIZE_T PagedBytes;
    SIZE_T NonPagedBytes;
    SIZE_T PeakPagedBytes;
    SIZE_T PeakNonPagedBytes;

} MI_VERIFIER_DRIVER_ENTRY, *PMI_VERIFIER_DRIVER_ENTRY;

typedef struct _MI_VERIFIER_POOL_HEADER {
    ULONG_PTR ListIndex;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
} MI_VERIFIER_POOL_HEADER, *PMI_VERIFIER_POOL_HEADER;

typedef struct _MM_DRIVER_VERIFIER_DATA {
    ULONG Level;
    ULONG RaiseIrqls;
    ULONG AcquireSpinLocks;
    ULONG SynchronizeExecutions;

    ULONG AllocationsAttempted;
    ULONG AllocationsSucceeded;
    ULONG AllocationsSucceededSpecialPool;
    ULONG AllocationsWithNoTag;

    ULONG TrimRequests;
    ULONG Trims;
    ULONG AllocationsFailed;
    ULONG AllocationsFailedDeliberately;

    ULONG Loads;
    ULONG Unloads;
    ULONG UnTrackedPool;
    ULONG UserTrims;

    ULONG CurrentPagedPoolAllocations;
    ULONG CurrentNonPagedPoolAllocations;
    ULONG PeakPagedPoolAllocations;
    ULONG PeakNonPagedPoolAllocations;

    SIZE_T PagedBytes;
    SIZE_T NonPagedBytes;
    SIZE_T PeakPagedBytes;
    SIZE_T PeakNonPagedBytes;

    ULONG BurstAllocationsFailedDeliberately;
    ULONG SessionTrims;
    ULONG Reserved[2];

} MM_DRIVER_VERIFIER_DATA, *PMM_DRIVER_VERIFIER_DATA;

LOGICAL
MiInitializeDriverVerifierList (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiInitializeVerifyingComponents (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiApplyDriverVerifier (
    IN PKLDR_DATA_TABLE_ENTRY,
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    );

VOID
MiReApplyVerifierToLoadedModules(
    IN PLIST_ENTRY ModuleListHead
    );

VOID
MiVerifyingDriverUnloading (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
MiVerifierCheckThunks (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

extern ULONG MiActiveVerifierThunks;
extern LIST_ENTRY MiSuspectDriverList;

extern ULONG MiVerifierThunksAdded;

VOID
MiEnableKernelVerifier (
    VOID
    );

extern LOGICAL KernelVerifier;

extern MM_DRIVER_VERIFIER_DATA MmVerifierData;

#define MI_FREED_SPECIAL_POOL_SIGNATURE 0x98764321

#define MI_STACK_BYTES 1024

typedef struct _MI_FREED_SPECIAL_POOL {
    POOL_HEADER OverlaidPoolHeader;
    MI_VERIFIER_POOL_HEADER OverlaidVerifierPoolHeader;

    ULONG Signature;
    ULONG TickCount;
    ULONG NumberOfBytesRequested;
    ULONG Pagable;

    PVOID VirtualAddress;
    PVOID StackPointer;
    ULONG StackBytes;
    PETHREAD Thread;

    UCHAR StackData[MI_STACK_BYTES];
} MI_FREED_SPECIAL_POOL, *PMI_FREED_SPECIAL_POOL;

#define MM_DBG_COMMIT_NONPAGED_POOL_EXPANSION           0
#define MM_DBG_COMMIT_PAGED_POOL_PAGETABLE              1
#define MM_DBG_COMMIT_PAGED_POOL_PAGES                  2
#define MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES          3
#define MM_DBG_COMMIT_ALLOCVM1                          4
#define MM_DBG_COMMIT_ALLOCVM_SEGMENT                   5
#define MM_DBG_COMMIT_IMAGE                             6
#define MM_DBG_COMMIT_PAGEFILE_BACKED_SHMEM             7
#define MM_DBG_COMMIT_INDEPENDENT_PAGES                 8
#define MM_DBG_COMMIT_CONTIGUOUS_PAGES                  9
#define MM_DBG_COMMIT_MDL_PAGES                         0xA
#define MM_DBG_COMMIT_NONCACHED_PAGES                   0xB
#define MM_DBG_COMMIT_MAPVIEW_DATA                      0xC
#define MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY             0xD
#define MM_DBG_COMMIT_EXTRA_SYSTEM_PTES                 0xE
#define MM_DBG_COMMIT_DRIVER_PAGING_AT_INIT             0xF
#define MM_DBG_COMMIT_PAGEFILE_FULL                     0x10
#define MM_DBG_COMMIT_PROCESS_CREATE                    0x11
#define MM_DBG_COMMIT_KERNEL_STACK_CREATE               0x12
#define MM_DBG_COMMIT_SET_PROTECTION                    0x13
#define MM_DBG_COMMIT_SESSION_CREATE                    0x14
#define MM_DBG_COMMIT_SESSION_IMAGE_PAGES               0x15
#define MM_DBG_COMMIT_SESSION_PAGETABLE_PAGES           0x16
#define MM_DBG_COMMIT_SESSION_SHARED_IMAGE              0x17
#define MM_DBG_COMMIT_DRIVER_PAGES                      0x18
#define MM_DBG_COMMIT_INSERT_VAD                        0x19
#define MM_DBG_COMMIT_SESSION_WS_INIT                   0x1A
#define MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES       0x1B
#define MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_HASHPAGES   0x1C
#define MM_DBG_COMMIT_SPECIAL_POOL_PAGES                0x1D
#define MM_DBG_COMMIT_SPECIAL_POOL_MAPPING_PAGES        0x1E
#define MM_DBG_COMMIT_SMALL                             0x1F
#define MM_DBG_COMMIT_EXTRA_WS_PAGES                    0x20
#define MM_DBG_COMMIT_EXTRA_INITIAL_SESSION_WS_PAGES    0x21
#define MM_DBG_COMMIT_ALLOCVM_PROCESS                   0x22
#define MM_DBG_COMMIT_INSERT_VAD_PT                     0x23
#define MM_DBG_COMMIT_ALLOCVM_PROCESS2                  0x24
#define MM_DBG_COMMIT_CHARGE_NORMAL                     0x25
#define MM_DBG_COMMIT_CHARGE_CAUSE_POPUP                0x26
#define MM_DBG_COMMIT_CHARGE_CANT_EXPAND                0x27

#define MM_DBG_COMMIT_RETURN_NONPAGED_POOL_EXPANSION    0x40
#define MM_DBG_COMMIT_RETURN_PAGED_POOL_PAGES           0x41
#define MM_DBG_COMMIT_RETURN_SESSION_DATAPAGE           0x42
#define MM_DBG_COMMIT_RETURN_ALLOCVM_SEGMENT            0x43
#define MM_DBG_COMMIT_RETURN_ALLOCVM2                   0x44

#define MM_DBG_COMMIT_RETURN_IMAGE_NO_LARGE_CA          0x46
#define MM_DBG_COMMIT_RETURN_PTE_RANGE                  0x47
#define MM_DBG_COMMIT_RETURN_NTFREEVM1                  0x48
#define MM_DBG_COMMIT_RETURN_NTFREEVM2                  0x49
#define MM_DBG_COMMIT_RETURN_INDEPENDENT_PAGES          0x4A
#define MM_DBG_COMMIT_RETURN_AWE_EXCESS                 0x4B
#define MM_DBG_COMMIT_RETURN_MDL_PAGES                  0x4C
#define MM_DBG_COMMIT_RETURN_NONCACHED_PAGES            0x4D
#define MM_DBG_COMMIT_RETURN_SESSION_CREATE_FAILURE     0x4E
#define MM_DBG_COMMIT_RETURN_PAGETABLES                 0x4F
#define MM_DBG_COMMIT_RETURN_PROTECTION                 0x50
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE1            0x51
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE2            0x52
#define MM_DBG_COMMIT_RETURN_PAGEFILE_FULL              0x53
#define MM_DBG_COMMIT_RETURN_SESSION_DEREFERENCE        0x54
#define MM_DBG_COMMIT_RETURN_VAD                        0x55
#define MM_DBG_COMMIT_RETURN_PROCESS_CREATE_FAILURE1    0x56
#define MM_DBG_COMMIT_RETURN_PROCESS_DELETE             0x57
#define MM_DBG_COMMIT_RETURN_PROCESS_CLEAN_PAGETABLES   0x58
#define MM_DBG_COMMIT_RETURN_KERNEL_STACK_DELETE        0x59
#define MM_DBG_COMMIT_RETURN_SESSION_DRIVER_LOAD_FAILURE1 0x5A
#define MM_DBG_COMMIT_RETURN_DRIVER_INIT_CODE           0x5B
#define MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD              0x5C
#define MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1             0x5D
#define MM_DBG_COMMIT_RETURN_NORMAL                     0x5E
#define MM_DBG_COMMIT_RETURN_PF_FULL_EXTEND             0x5F
#define MM_DBG_COMMIT_RETURN_EXTENDED                   0x60
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE3            0x61

#if 0

#define MM_COMMIT_COUNTER_MAX 0x80

#define MM_TRACK_COMMIT(_index, bump) \
    if (_index >= MM_COMMIT_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid commit counter %d %d\n", _index, MM_COMMIT_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        InterlockedExchangeAddSizeT (&MmTrackCommit[_index], bump); \
    }

#define MM_TRACK_COMMIT_REDUCTION(_index, bump) \
    if (_index >= MM_COMMIT_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid commit counter %d %d\n", _index, MM_COMMIT_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        InterlockedExchangeAddSizeT (&MmTrackCommit[_index], 0 - (bump)); \
    }

extern SIZE_T MmTrackCommit[MM_COMMIT_COUNTER_MAX];

#define MI_INCREMENT_TOTAL_PROCESS_COMMIT(_charge) InterlockedExchangeAddSizeT (&MmTotalProcessCommit, (_charge));

#else

#define MM_TRACK_COMMIT(_index, bump)
#define MM_TRACK_COMMIT_REDUCTION(_index, bump)
#define MI_INCREMENT_TOTAL_PROCESS_COMMIT(_charge)

#endif


#define MM_BUMP_COUNTER_MAX 60

extern SIZE_T MmResTrack[MM_BUMP_COUNTER_MAX];

#define MM_BUMP_COUNTER(_index, bump) \
    ASSERT (_index < MM_BUMP_COUNTER_MAX); \
    InterlockedExchangeAddSizeT (&MmResTrack[_index], (SIZE_T)(bump));

extern ULONG MiSpecialPagesNonPaged;
extern ULONG MiSpecialPagesNonPagedMaximum;

//++
//PFN_NUMBER
//MI_NONPAGABLE_MEMORY_AVAILABLE(
//    VOID
//    );
//
// Routine Description:
//
//    This routine lets callers know how many pages can be charged against
//    the resident available, factoring in earlier Mm promises that
//    may not have been redeemed at this point (ie: nonpaged pool expansion,
//    etc, that must be honored at a later point if requested).
//
// Arguments
//
//    None.
//
// Return Value:
//
//    The number of currently available pages in the resident available.
//
//    N.B.  This is a signed quantity and can be negative.
//
//--
#define MI_NONPAGABLE_MEMORY_AVAILABLE()                                    \
        ((SPFN_NUMBER)                                                      \
            (MmResidentAvailablePages -                                     \
             MmSystemLockPagesCount))

extern ULONG MmLargePageMinimum;

//
// hack stuff for testing.
//

VOID
MiDumpValidAddresses (
    VOID
    );

VOID
MiDumpPfn ( VOID );

VOID
MiDumpWsl ( VOID );


VOID
MiFormatPte (
    IN PMMPTE PointerPte
    );

VOID
MiCheckPfn ( VOID );

VOID
MiCheckPte ( VOID );

VOID
MiFormatPfn (
    IN PMMPFN PointerPfn
    );




extern const MMPTE ZeroPte;

extern const MMPTE ZeroKernelPte;

extern const MMPTE ValidKernelPteLocal;

extern MMPTE ValidKernelPte;

extern MMPTE ValidKernelPde;

extern const MMPTE ValidKernelPdeLocal;

extern const MMPTE ValidUserPte;

extern const MMPTE ValidPtePte;

extern const MMPTE ValidPdePde;

extern MMPTE DemandZeroPde;

extern const MMPTE DemandZeroPte;

extern MMPTE KernelPrototypePte;

extern const MMPTE TransitionPde;

extern MMPTE PrototypePte;

extern const MMPTE NoAccessPte;

extern ULONG_PTR MmSubsectionBase;

extern ULONG_PTR MmSubsectionTopPage;

extern ULONG ExpMultiUserTS;

//
// Virtual alignment for PTEs (machine specific) minimum value is
// 4k maximum value is 64k.  The maximum value can be raised by
// changing the MM_PROTO_PTE_ALIGNMENT constant and adding more
// reserved mapping PTEs in hyperspace.
//

//
// Total number of physical pages on the system.
//

extern PFN_COUNT MmNumberOfPhysicalPages;

//
// Lowest physical page number on the system.
//

extern PFN_NUMBER MmLowestPhysicalPage;

//
// Highest physical page number on the system.
//

extern PFN_NUMBER MmHighestPhysicalPage;

//
// Highest possible physical page number in the system.
//

extern PFN_NUMBER MmHighestPossiblePhysicalPage;

#if defined (_WIN64)

#define MI_DTC_MAX_PAGES ((PFN_NUMBER)(((ULONG64)128 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DTC_BOOTED_3GB_MAX_PAGES     MI_DTC_MAX_PAGES

#define MI_ADS_MAX_PAGES ((PFN_NUMBER)(((ULONG64)64 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DEFAULT_MAX_PAGES ((PFN_NUMBER)(((ULONG64)16 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#else

#define MI_DTC_MAX_PAGES ((PFN_NUMBER)(((ULONG64)64 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DTC_BOOTED_3GB_MAX_PAGES ((PFN_NUMBER)(((ULONG64)16 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_ADS_MAX_PAGES ((PFN_NUMBER)(((ULONG64)32 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DEFAULT_MAX_PAGES ((PFN_NUMBER)(((ULONG64)4 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#endif

#define MI_BLADE_MAX_PAGES ((PFN_NUMBER)(((ULONG64)2 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

//
// Total number of available pages on the system.  This
// is the sum of the pages on the zeroed, free and standby lists.
//

extern PFN_COUNT MmAvailablePages;

//
// Total number of free pages to base working set trimming on.
//

extern PFN_NUMBER MmMoreThanEnoughFreePages;

//
// Total number physical pages which would be usable if every process
// was at it's minimum working set size.  This value is initialized
// at system initialization to MmAvailablePages - MM_FLUID_PHYSICAL_PAGES.
// Everytime a thread is created, the kernel stack is subtracted from
// this and every time a process is created, the minimum working set
// is subtracted from this.  If the value would become negative, the
// operation (create process/kernel stack/ adjust working set) fails.
// The PFN LOCK must be owned to manipulate this value.
//

extern SPFN_NUMBER MmResidentAvailablePages;

//
// The total number of pages which would be removed from working sets
// if every working set was at its minimum.
//

extern PFN_NUMBER MmPagesAboveWsMinimum;

//
// The total number of pages which would be removed from working sets
// if every working set above its maximum was at its maximum.
//

extern PFN_NUMBER MmPagesAboveWsMaximum;

//
// If memory is becoming short and MmPagesAboveWsMinimum is
// greater than MmPagesAboveWsThreshold, trim working sets.
//

extern PFN_NUMBER MmPagesAboveWsThreshold;

//
// The number of pages to add to a working set if there are ample
// available pages and the working set is below its maximum.
//

extern PFN_NUMBER MmWorkingSetSizeIncrement;

//
// The number of pages to extend the maximum working set size by
// if the working set at its maximum and there are ample available pages.

extern PFN_NUMBER MmWorkingSetSizeExpansion;

extern ULONG MmPlentyFreePages;

//
// The number of pages required to be freed by working set reduction
// before working set reduction is attempted.
//

extern PFN_NUMBER MmWsAdjustThreshold;

//
// The number of pages available to allow the working set to be
// expanded above its maximum.
//

extern PFN_NUMBER MmWsExpandThreshold;

//
// The total number of pages to reduce by working set trimming.
//

extern PFN_NUMBER MmWsTrimReductionGoal;

extern LONG MiDelayPageFaults;

extern PMMPFN MmPfnDatabase;

extern MMPFNLIST MmZeroedPageListHead;

extern MMPFNLIST MmFreePageListHead;

extern MMPFNLIST MmStandbyPageListHead;

extern MMPFNLIST MmRomPageListHead;

extern MMPFNLIST MmModifiedPageListHead;

extern MMPFNLIST MmModifiedNoWritePageListHead;

extern MMPFNLIST MmBadPageListHead;

extern PMMPFNLIST MmPageLocationList[NUMBER_OF_PAGE_LISTS];

extern MMPFNLIST MmModifiedPageListByColor[MM_MAXIMUM_NUMBER_OF_COLORS];

//
// Mask for isolating secondary color from physical page number.
//

extern ULONG MmSecondaryColorMask;

//
// Mask for isolating node color from combined node and secondary
// color.
//

extern ULONG MmSecondaryColorNodeMask;

//
// Width of MmSecondaryColorMask in bits.   In multi node systems,
// the node number is combined with the secondary color to make up
// the page color.
//

extern UCHAR MmSecondaryColorNodeShift;

//
// Event for available pages, set means pages are available.
//

extern KEVENT MmAvailablePagesEvent;

extern KEVENT MmAvailablePagesEventMedium;

extern KEVENT MmAvailablePagesEventHigh;

//
// Event for the zeroing page thread.
//

extern KEVENT MmZeroingPageEvent;

//
// Boolean to indicate if the zeroing page thread is currently
// active.  This is set to true when the zeroing page event is
// set and set to false when the zeroing page thread is done
// zeroing all the pages on the free list.
//

extern BOOLEAN MmZeroingPageThreadActive;

//
// Minimum number of free pages before zeroing page thread starts.
//

extern PFN_NUMBER MmMinimumFreePagesToZero;

//
// Global event to synchronize mapped writing with cleaning segments.
//

extern KEVENT MmMappedFileIoComplete;

//
// Hyper space items.
//

extern PMMPTE MmFirstReservedMappingPte;

extern PMMPTE MmLastReservedMappingPte;

//
// System space sizes - MmNonPagedSystemStart to MM_NON_PAGED_SYSTEM_END
// defines the ranges of PDEs which must be copied into a new process's
// address space.
//

extern PVOID MmNonPagedSystemStart;

extern PCHAR MmSystemSpaceViewStart;

extern LOGICAL MmProtectFreedNonPagedPool;

//
// Pool sizes.
//

extern SIZE_T MmSizeOfNonPagedPoolInBytes;

extern SIZE_T MmMinimumNonPagedPoolSize;

extern SIZE_T MmDefaultMaximumNonPagedPool;

extern ULONG MmMaximumNonPagedPoolPercent;

extern ULONG MmMinAdditionNonPagedPoolPerMb;

extern ULONG MmMaxAdditionNonPagedPoolPerMb;

extern SIZE_T MmSizeOfPagedPoolInBytes;

extern SIZE_T MmMaximumNonPagedPoolInBytes;

extern PFN_NUMBER MmAllocatedNonPagedPool;

extern PVOID MmNonPagedPoolExpansionStart;

extern ULONG MmExpandedPoolBitPosition;

extern PFN_NUMBER MmNumberOfFreeNonPagedPool;

extern ULONG MmNumberOfSystemPtes;

extern ULONG MiRequestedSystemPtes;

extern ULONG MmTotalFreeSystemPtes[MaximumPtePoolTypes];

extern PMMPTE MmSystemPagePtes;

extern ULONG MmSystemPageDirectory[];

extern SIZE_T MmHeapSegmentReserve;

extern SIZE_T MmHeapSegmentCommit;

extern SIZE_T MmHeapDeCommitTotalFreeThreshold;

extern SIZE_T MmHeapDeCommitFreeBlockThreshold;

#define MI_MAX_FREE_LIST_HEADS  4

extern LIST_ENTRY MmNonPagedPoolFreeListHead[MI_MAX_FREE_LIST_HEADS];

//
// Counter for flushes of the entire TB.
//

extern ULONG MmFlushCounter;

//
// Pool start and end.
//

extern PVOID MmNonPagedPoolStart;

extern PVOID MmNonPagedPoolEnd;

extern PVOID MmPagedPoolStart;

extern PVOID MmPagedPoolEnd;

//
// Pool bit maps and other related structures.
//

typedef struct _MM_PAGED_POOL_INFO {

    PRTL_BITMAP PagedPoolAllocationMap;
    PRTL_BITMAP EndOfPagedPoolBitmap;
    PRTL_BITMAP PagedPoolLargeSessionAllocationMap;     // HYDRA only
    PMMPTE FirstPteForPagedPool;
    PMMPTE LastPteForPagedPool;
    PMMPTE NextPdeForPagedPoolExpansion;
    ULONG PagedPoolHint;
    SIZE_T PagedPoolCommit;
    SIZE_T AllocatedPagedPool;

} MM_PAGED_POOL_INFO, *PMM_PAGED_POOL_INFO;

extern MM_PAGED_POOL_INFO MmPagedPoolInfo;

extern PVOID MmPageAlignedPoolBase[2];

extern PRTL_BITMAP VerifierLargePagedPoolMap;

//
// MmFirstFreeSystemPte contains the offset from the
// Nonpaged system base to the first free system PTE.
// Note, that an offset of zero indicates an empty list.
//

extern MMPTE MmFirstFreeSystemPte[MaximumPtePoolTypes];

extern ULONG_PTR MiSystemViewStart;

//
// System cache sizes.
//

//extern MMSUPPORT MmSystemCacheWs;

extern PMMWSL MmSystemCacheWorkingSetList;

extern PMMWSLE MmSystemCacheWsle;

extern PVOID MmSystemCacheStart;

extern PVOID MmSystemCacheEnd;

extern PFN_NUMBER MmSystemCacheWsMinimum;

extern PFN_NUMBER MmSystemCacheWsMaximum;

//
// Virtual alignment for PTEs (machine specific) minimum value is
// 0 (no alignment) maximum value is 64k.  The maximum value can be raised by
// changing the MM_PROTO_PTE_ALIGNMENT constant and adding more
// reserved mapping PTEs in hyperspace.
//

extern ULONG MmAliasAlignment;

//
// Mask to AND with virtual address to get an offset to go
// with the alignment.  This value is page aligned.
//

extern ULONG MmAliasAlignmentOffset;

//
// Mask to and with PTEs to determine if the alias mapping is compatible.
// This value is usually (MmAliasAlignment - 1)
//

extern ULONG MmAliasAlignmentMask;

//
// Cells to track unused thread kernel stacks to avoid TB flushes
// every time a thread terminates.
//

extern ULONG MmMaximumDeadKernelStacks;
extern SLIST_HEADER MmDeadStackSListHead;

//
// MmSystemPteBase contains the address of 1 PTE before
// the first free system PTE (zero indicates an empty list).
// The value of this field does not change once set.
//

extern PMMPTE MmSystemPteBase;

//
// Root of system space virtual address descriptors.  These define
// the pagable portion of the system.
//

extern PMMVAD MmVirtualAddressDescriptorRoot;

extern PMMADDRESS_NODE MmSectionBasedRoot;

extern PVOID MmHighSectionBase;

//
// Section commit mutex.
//

extern FAST_MUTEX MmSectionCommitMutex;

//
// Section base address mutex.
//

extern FAST_MUTEX MmSectionBasedMutex;

//
// Resource for section extension.
//

extern ERESOURCE MmSectionExtendResource;
extern ERESOURCE MmSectionExtendSetResource;

//
// Inpage cluster sizes for executable pages (set based on memory size).
//

extern ULONG MmDataClusterSize;

extern ULONG MmCodeClusterSize;

//
// Pagefile creation mutex.
//

extern FAST_MUTEX MmPageFileCreationLock;

//
// Event to set when first paging file is created.
//

extern PKEVENT MmPagingFileCreated;

//
// Paging file debug information.
//

extern ULONG_PTR MmPagingFileDebug[];

//
// Spinlock which guards PFN database.  This spinlock is used by
// memory management for accessing the PFN database.  The I/O
// system makes use of it for unlocking pages during I/O complete.
//

// extern KSPIN_LOCK MmPfnLock;

//
// Spinlock which guards the working set list for the system shared
// address space (paged pool, system cache, pagable drivers).
//

extern ERESOURCE MmSystemWsLock;

//
// Spin lock for allowing working set expansion.
//

extern KSPIN_LOCK MmExpansionLock;

//
// To prevent optimizations.
//

extern MMPTE GlobalPte;

//
// Page color for system working set.
//

extern ULONG MmSystemPageColor;

extern ULONG MmSecondaryColors;

extern ULONG MmProcessColorSeed;

//
// Set from ntos\config\CMDAT3.C  Used by customers to disable paging
// of executive on machines with lots of memory.  Worth a few TPS on a
// data base server.
//

#define MM_SYSTEM_CODE_LOCKED_DOWN 0x1
#define MM_PAGED_POOL_LOCKED_DOWN  0x2

extern ULONG MmDisablePagingExecutive;


//
// For debugging.


#if DBG
extern ULONG MmDebug;
#endif

//
// Unused segment management
//

extern MMDEREFERENCE_SEGMENT_HEADER MmDereferenceSegmentHeader;

extern LIST_ENTRY MmUnusedSegmentList;

extern LIST_ENTRY MmUnusedSubsectionList;

extern KEVENT MmUnusedSegmentCleanup;

extern ULONG MmConsumedPoolPercentage;

extern ULONG MmUnusedSegmentCount;

extern ULONG MmUnusedSubsectionCount;

extern ULONG MmUnusedSubsectionCountPeak;

extern SIZE_T MiUnusedSubsectionPagedPool;

extern SIZE_T MiUnusedSubsectionPagedPoolPeak;

#define MI_UNUSED_SUBSECTIONS_COUNT_INSERT(_MappedSubsection) \
        MmUnusedSubsectionCount += 1; \
        if (MmUnusedSubsectionCount > MmUnusedSubsectionCountPeak) { \
            MmUnusedSubsectionCountPeak = MmUnusedSubsectionCount; \
        } \
        MiUnusedSubsectionPagedPool += EX_REAL_POOL_USAGE((_MappedSubsection->PtesInSubsection + _MappedSubsection->UnusedPtes) * sizeof (MMPTE)); \
        if (MiUnusedSubsectionPagedPool > MiUnusedSubsectionPagedPoolPeak) { \
            MiUnusedSubsectionPagedPoolPeak = MiUnusedSubsectionPagedPool; \
        } \

#define MI_UNUSED_SUBSECTIONS_COUNT_REMOVE(_MappedSubsection) \
        MmUnusedSubsectionCount -= 1; \
        MiUnusedSubsectionPagedPool -= EX_REAL_POOL_USAGE((_MappedSubsection->PtesInSubsection + _MappedSubsection->UnusedPtes) * sizeof (MMPTE));

#define MI_FILESYSTEM_NONPAGED_POOL_CHARGE 150

#define MI_FILESYSTEM_PAGED_POOL_CHARGE 1024

//++
//LOGICAL
//MI_UNUSED_SEGMENTS_SURPLUS (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    This routine determines whether a surplus of unused
//    segments exist.  If so, the caller can initiate a trim to free pool.
//
// Arguments
//
//    None.
//
// Return Value:
//
//    TRUE if unused segment trimming should be initiated, FALSE if not.
//
//--
#define MI_UNUSED_SEGMENTS_SURPLUS()                                    \
        (((ULONG)((MmPagedPoolInfo.AllocatedPagedPool * 100) / (MmSizeOfPagedPoolInBytes >> PAGE_SHIFT)) > MmConsumedPoolPercentage) || \
        ((ULONG)((MmAllocatedNonPagedPool * 100) / (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT)) > MmConsumedPoolPercentage))

VOID
MiConvertStaticSubsections (
    IN PCONTROL_AREA ControlArea
    );

//++
//VOID
//MI_INSERT_UNUSED_SEGMENT (
//    IN PCONTROL_AREA _ControlArea
//    );
//
// Routine Description:
//
//    This routine inserts a control area into the unused segment list,
//    also managing the associated pool charges.
//
// Arguments
//
//    _ControlArea - Supplies the control area to obtain the pool charges from.
//
// Return Value:
//
//    None.
//
//--
#define MI_INSERT_UNUSED_SEGMENT(_ControlArea)                               \
        {                                                                    \
           MM_PFN_LOCK_ASSERT();                                             \
           if ((_ControlArea->u.Flags.Image == 0) &&                         \
               (_ControlArea->FilePointer != NULL) &&                        \
               (_ControlArea->u.Flags.PhysicalMemory == 0)) {                \
               MiConvertStaticSubsections(_ControlArea);                     \
           }                                                                 \
           InsertTailList (&MmUnusedSegmentList, &_ControlArea->DereferenceList); \
           MmUnusedSegmentCount += 1; \
        }

//++
//VOID
//MI_UNUSED_SEGMENTS_REMOVE_CHARGE (
//    IN PCONTROL_AREA _ControlArea
//    );
//
// Routine Description:
//
//    This routine manages pool charges during removals of segments from
//    the unused segment list.
//
// Arguments
//
//    _ControlArea - Supplies the control area to obtain the pool charges from.
//
// Return Value:
//
//    None.
//
//--
#define MI_UNUSED_SEGMENTS_REMOVE_CHARGE(_ControlArea)                       \
        {                                                                    \
           MM_PFN_LOCK_ASSERT();                                             \
           MmUnusedSegmentCount -= 1; \
        }

//
// List heads
//

extern MMWORKING_SET_EXPANSION_HEAD MmWorkingSetExpansionHead;

extern MMPAGE_FILE_EXPANSION MmAttemptForCantExtend;

//
// Paging files
//

extern MMMOD_WRITER_LISTHEAD MmPagingFileHeader;

extern MMMOD_WRITER_LISTHEAD MmMappedFileHeader;

extern PMMPAGING_FILE MmPagingFile[MAX_PAGE_FILES];

extern LIST_ENTRY MmFreePagingSpaceLow;

extern ULONG MmNumberOfActiveMdlEntries;

extern ULONG MmNumberOfPagingFiles;

extern KEVENT MmModifiedPageWriterEvent;

extern KEVENT MmCollidedFlushEvent;

extern KEVENT MmCollidedLockEvent;

//
// Total number of committed pages.
//

extern SIZE_T MmTotalCommittedPages;

extern SIZE_T MmTotalCommitLimit;

extern SIZE_T MmOverCommit;

extern SIZE_T MmSharedCommit;

// #define _MI_DEBUG_DATA 1         // Uncomment this for data logging

#if defined (_MI_DEBUG_DATA)

#define MI_DATA_BACKTRACE_LENGTH 8

typedef struct _MI_DATA_TRACES {

    PETHREAD Thread;
    PMMPFN Pfn;
    PMMPTE PointerPte;
    MMPFN PfnData;
    ULONG CallerId;
    ULONG DataInThePage[2];
    PVOID StackTrace [MI_DATA_BACKTRACE_LENGTH];

} MI_DATA_TRACES, *PMI_DATA_TRACES;

extern LONG MiDataIndex;

extern ULONG MiTrackData;

extern PMI_DATA_TRACES MiDataTraces;

VOID
FORCEINLINE
MiSnapData (
    IN PMMPFN Pfn,
    IN PMMPTE PointerPte,
    IN ULONG CallerId
    )
{
    KIRQL OldIrql;
    PVOID Va;
    PMI_DATA_TRACES Information;
    ULONG Index;
    ULONG Hash;
    PEPROCESS CurrentProcess;

    if (MiDataTraces == NULL) {
        return;
    }

    Index = InterlockedIncrement(&MiDataIndex);
    Index &= (MiTrackData - 1);
    Information = &MiDataTraces[Index];

    Information->Thread = PsGetCurrentThread ();
    Information->Pfn = Pfn;
    Information->PointerPte = PointerPte;
    Information->PfnData = *Pfn;
    Information->CallerId = CallerId;

    CurrentProcess = PsGetCurrentProcess ();
    Va = MiMapPageInHyperSpace (CurrentProcess, Pfn - MmPfnDatabase, &OldIrql);

    RtlCopyMemory (&Information->DataInThePage[0],
                   Va,
                   sizeof (Information->DataInThePage));

    MiUnmapPageInHyperSpace (CurrentProcess, Va, OldIrql);

    RtlZeroMemory (&Information->StackTrace[0], MI_DATA_BACKTRACE_LENGTH * sizeof(PVOID));                                                 \

    RtlCaptureStackBackTrace (0, MI_DATA_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_DATA(_Pfn, _Pte, _CallerId) MiSnapData(_Pfn, _Pte, _CallerId)

#else
#define MI_SNAP_DATA(_Pfn, _Pte, _CallerId)
#endif


//
// Modified page writer.
//

extern PFN_NUMBER MmMinimumFreePages;

extern PFN_NUMBER MmFreeGoal;

extern PFN_NUMBER MmModifiedPageMaximum;

extern PFN_NUMBER MmModifiedPageMinimum;

extern ULONG MmModifiedWriteClusterSize;

extern ULONG MmMinimumFreeDiskSpace;

extern ULONG MmPageFileExtension;

extern ULONG MmMinimumPageFileReduction;

extern LARGE_INTEGER MiModifiedPageLife;

extern BOOLEAN MiTimerPending;

extern KEVENT MiMappedPagesTooOldEvent;

extern KDPC MiModifiedPageWriterTimerDpc;

extern KTIMER MiModifiedPageWriterTimer;

//
// System process working set sizes.
//

extern PFN_NUMBER MmSystemProcessWorkingSetMin;

extern PFN_NUMBER MmSystemProcessWorkingSetMax;

extern PFN_NUMBER MmMinimumWorkingSetSize;

//
// Support for debugger's mapping physical memory.
//

extern PMMPTE MmDebugPte;

extern PMMPTE MmCrashDumpPte;

extern ULONG MiOverCommitCallCount;

//
// Event tracing routines
//

extern PPAGE_FAULT_NOTIFY_ROUTINE MmPageFaultNotifyRoutine;

extern SIZE_T MmSystemViewSize;

VOID
FASTCALL
MiIdentifyPfn (
    IN PMMPFN Pfn1,
    OUT PMMPFN_IDENTITY PfnIdentity
    );

#if defined (_WIN64)
#define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd64((PLONGLONG)a, b)
#else
#define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd((PLONG)(a), b)
#endif

//
// This is a special value loaded into an EPROCESS pointer to indicate that
// the action underway is for a Hydra session, not really the current process.
// (Any value could be used here that is not a valid system pointer or NULL).
//

#define HYDRA_PROCESS   ((PEPROCESS)1)

#define PREFETCH_PROCESS   ((PEPROCESS)2)

#define MI_SESSION_SPACE_STRUCT_SIZE MM_ALLOCATION_GRANULARITY

#if defined (_WIN64)

/*++

  Virtual memory layout of session space when loaded down from
  0x2000.0002.0000.0000 (IA64) or FFFF.F980.0000.0000 (AMD64) :

  Note that the sizes of mapped views, paged pool & images are registry tunable.

                        +------------------------------------+
    2000.0002.0000.0000 |                                    |
                        |   win32k.sys & video drivers       |
                        |             (16MB)                 |
                        |                                    |
                        +------------------------------------+
    2000.0001.FF00.0000 |                                    |
                        |   MM_SESSION_SPACE & Session WSLs  |
                        |              (16MB)                |
                        |                                    |
    2000.0001.FEFF.0000 +------------------------------------+
                        |                                    |
                        |              ...                   |
                        |                                    |
                        +------------------------------------+
    2000.0001.FE80.0000 |                                    |
                        |   Mapped Views for this session    |
                        |              (104MB)               |
                        |                                    |
                        +------------------------------------+
    2000.0001.F800.0000 |                                    |
                        |   Paged Pool for this session      |
                        |              (64MB)                |
                        |                                    |
    2000.0001.F400.0000 +------------------------------------+
                        |   Special Pool for this session    |
                        |              (64MB)                |
                        |                                    |
    2000.0000.0000.0000 +------------------------------------+

--*/

#define MI_SESSION_SPACE_WS_SIZE  ((ULONG_PTR)(16*1024*1024) - MI_SESSION_SPACE_STRUCT_SIZE)

#define MI_SESSION_DEFAULT_IMAGE_SIZE     ((ULONG_PTR)(16*1024*1024))

#define MI_SESSION_DEFAULT_VIEW_SIZE      ((ULONG_PTR)(104*1024*1024))

#define MI_SESSION_DEFAULT_POOL_SIZE      ((ULONG_PTR)(64*1024*1024))

#define MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE (MM_VA_MAPPED_BY_PPE)

#else

/*++

  Virtual memory layout of session space when loaded down from 0xC0000000.

  Note that the sizes of mapped views, paged pool and images are registry
  tunable on 32-bit systems (if NOT booted /3GB, as 3GB has very limited
  address space).

                 +------------------------------------+
        C0000000 |                                    |
                 | win32k.sys, video drivers and any  |
                 | rebased NT4 printer drivers.       |
                 |                                    |
                 |             (8MB)                  |
                 |                                    |
                 +------------------------------------+
        BF800000 |                                    |
                 |   MM_SESSION_SPACE & Session WSLs  |
                 |              (4MB)                 |
                 |                                    |
                 +------------------------------------+
        BF400000 |                                    |
                 |   Mapped views for this session    |
                 |     (20MB by default, but is       |
                 |      registry configurable)        |
                 |                                    |
                 +------------------------------------+
        BE000000 |                                    |
                 |   Paged pool for this session      |
                 |     (16MB by default, but is       |
                 |      registry configurable)        |
                 |                                    |
        BD000000 +------------------------------------+

--*/

#define MI_SESSION_SPACE_WS_SIZE  (4*1024*1024 - MI_SESSION_SPACE_STRUCT_SIZE)

#define MI_SESSION_DEFAULT_IMAGE_SIZE      (8*1024*1024)

#define MI_SESSION_DEFAULT_VIEW_SIZE      (20*1024*1024)

#define MI_SESSION_DEFAULT_POOL_SIZE      (16*1024*1024)

#define MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE \
            (MM_SYSTEM_CACHE_END_EXTRA - MM_KSEG2_BASE)

#endif



#define MI_SESSION_SPACE_DEFAULT_TOTAL_SIZE \
            (MI_SESSION_DEFAULT_IMAGE_SIZE + \
             MI_SESSION_SPACE_STRUCT_SIZE + \
             MI_SESSION_SPACE_WS_SIZE + \
             MI_SESSION_DEFAULT_VIEW_SIZE + \
             MI_SESSION_DEFAULT_POOL_SIZE)

extern ULONG_PTR MmSessionBase;
extern PMMPTE MiSessionBasePte;
extern PMMPTE MiSessionLastPte;

extern ULONG_PTR MiSessionSpaceWs;

extern ULONG_PTR MiSessionViewStart;
extern SIZE_T MmSessionViewSize;

extern ULONG_PTR MiSessionImageStart;
extern ULONG_PTR MiSessionImageEnd;
extern SIZE_T MmSessionImageSize;

extern PMMPTE MiSessionImagePteStart;
extern PMMPTE MiSessionImagePteEnd;

extern ULONG_PTR MiSessionPoolStart;
extern ULONG_PTR MiSessionPoolEnd;
extern SIZE_T MmSessionPoolSize;

extern ULONG_PTR MiSessionSpaceEnd;

extern ULONG MiSessionSpacePageTables;

//
// The number of page table pages required to map all of session space.
//

#define MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES \
            (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE / MM_VA_MAPPED_BY_PDE)

extern SIZE_T MmSessionSize;        // size of the entire session space.

//
// Macros to determine if a given address lies in the specified session range.
//

#define MI_IS_SESSION_IMAGE_ADDRESS(VirtualAddress) \
        ((PVOID)(VirtualAddress) >= (PVOID)MiSessionImageStart && (PVOID)(VirtualAddress) < (PVOID)(MiSessionImageEnd))

#define MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) \
        ((PVOID)(VirtualAddress) >= (PVOID)MiSessionPoolStart && (PVOID)(VirtualAddress) < (PVOID)MiSessionPoolEnd)

#define MI_IS_SESSION_ADDRESS(_VirtualAddress) \
        ((PVOID)(_VirtualAddress) >= (PVOID)MmSessionBase && (PVOID)(_VirtualAddress) < (PVOID)(MiSessionSpaceEnd))

#define MI_IS_SESSION_PTE(_Pte) \
        ((PMMPTE)(_Pte) >= MiSessionBasePte && (PMMPTE)(_Pte) < MiSessionLastPte)

#define MI_IS_SESSION_IMAGE_PTE(_Pte) \
        ((PMMPTE)(_Pte) >= MiSessionImagePteStart && (PMMPTE)(_Pte) < MiSessionImagePteEnd)

#define SESSION_GLOBAL(_Session)    (_Session->GlobalVirtualAddress)

#define MM_DBG_SESSION_INITIAL_PAGETABLE_ALLOC          0
#define MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_RACE      1
#define MM_DBG_SESSION_INITIAL_PAGE_ALLOC               2
#define MM_DBG_SESSION_INITIAL_PAGE_FREE_FAIL1          3
#define MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_FAIL1     4
#define MM_DBG_SESSION_WS_PAGE_FREE                     5
#define MM_DBG_SESSION_PAGETABLE_ALLOC                  6
#define MM_DBG_SESSION_SYSMAPPED_PAGES_ALLOC            7
#define MM_DBG_SESSION_WS_PAGETABLE_ALLOC               8
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC        9
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_FREE_FAIL1   10
#define MM_DBG_SESSION_WS_PAGE_ALLOC                    11
#define MM_DBG_SESSION_WS_PAGE_ALLOC_GROWTH             12
#define MM_DBG_SESSION_INITIAL_PAGE_FREE                13
#define MM_DBG_SESSION_PAGETABLE_FREE                   14
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC1       15
#define MM_DBG_SESSION_DRIVER_PAGES_LOCKED              16
#define MM_DBG_SESSION_DRIVER_PAGES_UNLOCKED            17
#define MM_DBG_SESSION_WS_HASHPAGE_ALLOC                18
#define MM_DBG_SESSION_SYSMAPPED_PAGES_COMMITTED        19

#define MM_DBG_SESSION_COMMIT_PAGEDPOOL_PAGES           30
#define MM_DBG_SESSION_COMMIT_DELETE_VM_RETURN          31
#define MM_DBG_SESSION_COMMIT_POOL_FREED                32
#define MM_DBG_SESSION_COMMIT_IMAGE_UNLOAD              33
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED1         34
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED2         35
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_NOACCESS        36

#define MM_DBG_SESSION_NP_LOCK_CODE1                    38
#define MM_DBG_SESSION_NP_LOCK_CODE2                    39
#define MM_DBG_SESSION_NP_SESSION_CREATE                40
#define MM_DBG_SESSION_NP_PAGETABLE_ALLOC               41
#define MM_DBG_SESSION_NP_POOL_CREATE                   42
#define MM_DBG_SESSION_NP_COMMIT_IMAGE                  43
#define MM_DBG_SESSION_NP_COMMIT_IMAGE_PT               44
#define MM_DBG_SESSION_NP_INIT_WS                       45
#define MM_DBG_SESSION_NP_WS_GROW                       46
#define MM_DBG_SESSION_NP_HASH_GROW                     47

#define MM_DBG_SESSION_NP_PAGE_DRIVER                   48
#define MM_DBG_SESSION_NP_POOL_CREATE_FAILED            49
#define MM_DBG_SESSION_NP_WS_PAGE_FREE                  50
#define MM_DBG_SESSION_NP_SESSION_DESTROY               51
#define MM_DBG_SESSION_NP_SESSION_PTDESTROY             52
#define MM_DBG_SESSION_NP_DELVA                         53

#if DBG
#define MM_SESS_COUNTER_MAX 54

#define MM_BUMP_SESS_COUNTER(_index, bump) \
    if (_index >= MM_SESS_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid bump counter %d %d\n", _index, MM_SESS_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    MmSessionSpace->Debug[_index] += (bump);

typedef struct _MM_SESSION_MEMORY_COUNTERS {
    SIZE_T NonPagablePages;
    SIZE_T CommittedPages;
} MM_SESSION_MEMORY_COUNTERS, *PMM_SESSION_MEMORY_COUNTERS;

#define MM_SESS_MEMORY_COUNTER_MAX  8

#define MM_SNAP_SESS_MEMORY_COUNTERS(_index) \
    if (_index >= MM_SESS_MEMORY_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid session mem counter %d %d\n", _index, MM_SESS_MEMORY_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        MmSessionSpace->Debug2[_index].NonPagablePages = MmSessionSpace->NonPagablePages; \
        MmSessionSpace->Debug2[_index].CommittedPages = MmSessionSpace->CommittedPages; \
    }

#else
#define MM_BUMP_SESS_COUNTER(_index, bump)
#define MM_SNAP_SESS_MEMORY_COUNTERS(_index)
#endif


#define MM_SESSION_FAILURE_NO_IDS                   0
#define MM_SESSION_FAILURE_NO_COMMIT                1
#define MM_SESSION_FAILURE_NO_RESIDENT              2
#define MM_SESSION_FAILURE_RACE_DETECTED            3
#define MM_SESSION_FAILURE_NO_SYSPTES               4
#define MM_SESSION_FAILURE_NO_PAGED_POOL            5
#define MM_SESSION_FAILURE_NO_NONPAGED_POOL         6
#define MM_SESSION_FAILURE_NO_IMAGE_VA_SPACE        7
#define MM_SESSION_FAILURE_NO_SESSION_PAGED_POOL    8
#define MM_SESSION_FAILURE_NO_AVAILABLE             9

#define MM_SESSION_FAILURE_CAUSES                   10

ULONG MmSessionFailureCauses[MM_SESSION_FAILURE_CAUSES];

#define MM_BUMP_SESSION_FAILURES(_index) MmSessionFailureCauses[_index] += 1;

typedef struct _MM_SESSION_SPACE_FLAGS {
    ULONG Initialized : 1;
    ULONG Filler0 : 1;
    ULONG WorkingSetInserted : 1;
    ULONG SessionListInserted : 1;
    ULONG HasWsLock : 1;
    ULONG DeletePending : 1;
    ULONG Filler : 26;
} MM_SESSION_SPACE_FLAGS;

//
// The session space data structure - allocated per session and only visible at
// MM_SESSION_SPACE_BASE when in the context of a process from the session.
// This virtual address space is rotated at context switch time when switching
// from a process in session A to a process in session B.  This rotation is
// useful for things like providing paged pool per session so many sessions
// won't exhaust the VA space which backs the system global pool.
//
// A kernel PTE is also allocated to double map this page so that global
// pointers can be maintained to provide system access from any process context.
// This is needed for things like mutexes and WSL chains.
//

typedef struct _MM_SESSION_SPACE {

    ULONG ReferenceCount;
    union {
        ULONG LongFlags;
        MM_SESSION_SPACE_FLAGS Flags;
    } u;
    ULONG SessionId;

    //
    // All the page tables for session space use this as their parent.
    // Note that it's not really a page directory - it's really a page
    // table page itself (the one used to map this very structure).
    //
    // This provides a reference to something that won't go away and
    // is relevant regardless of which process within the session is current.
    //

    PFN_NUMBER SessionPageDirectoryIndex;

    //
    // This is a pointer in global system address space, used to make various
    // fields that can be referenced from any process visible from any process
    // context.  This is for things like mutexes, WSL chains, etc.
    //

    struct _MM_SESSION_SPACE *GlobalVirtualAddress;

    //
    // This is the list of the processes in this group that have
    // session space entries.
    //

    LIST_ENTRY ProcessList;

    //
    // Pool allocation counts - these are always valid.
    //

    SIZE_T NonPagedPoolBytes;
    SIZE_T PagedPoolBytes;
    ULONG NonPagedPoolAllocations;
    ULONG PagedPoolAllocations;

    //
    // This is the count of non paged allocations to support this session
    // space.  This includes the session structure page table and data pages,
    // WSL page table and data pages, session pool page table pages and session
    // image page table pages.  These are all charged against
    // MmResidentAvailable.
    //

    SIZE_T NonPagablePages;

    //
    // This is the count of pages in this session that have been charged against
    // the systemwide commit.  This includes all the NonPagablePages plus the
    // data pages they typically map.
    //

    SIZE_T CommittedPages;

    LARGE_INTEGER LastProcessSwappedOutTime;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // The page directory that maps session space is saved here so
    // trimmers can attach.
    //

    MMPTE PageDirectory;

#else

    //
    // The second level page tables that map session space are shared
    // by all processes in the session.
    //

    PMMPTE PageTables;

#endif

    //
    // Session space paged pool support.
    //

    FAST_MUTEX PagedPoolMutex;

    //
    // Start of session paged pool virtual space.
    //

    PVOID PagedPoolStart;

    //
    // Current end of pool virtual space. Can be extended to the
    // end of the session space.
    //

    PVOID PagedPoolEnd;

    //
    // PTE pointers for pool.
    //

    PMMPTE PagedPoolBasePde;

    MM_PAGED_POOL_INFO PagedPoolInfo;

    ULONG Color;

    ULONG ProcessOutSwapCount;

    //
    // This is the list of system images currently valid in
    // this session space.  This information is in addition
    // to the module global information in PsLoadedModuleList.
    //

    LIST_ENTRY ImageList;

    //
    // The system PTE self-map entry.
    //

    PMMPTE GlobalPteEntry;

    ULONG CopyOnWriteCount;

    ULONG SessionPoolAllocationFailures[4];

    //
    // The count of "known attachers and the associated event.
    //

    ULONG AttachCount;

    KEVENT AttachEvent;

    PEPROCESS LastProcess;

    //
    // Working set information.
    //

    MMSUPPORT  Vm;
    PMMWSLE    Wsle;

    ERESOURCE  WsLock;         // owned by WorkingSetLockOwner

    //
    // This chain is in global system addresses (not session VAs) and can
    // be walked from any system context, ie: for WSL trimming.
    //

    LIST_ENTRY WsListEntry;

    //
    // Support for mapping system views into session space.  Each desktop
    // allocates a 3MB heap and the global system view space is only 48M
    // total.  This would limit us to only 20-30 users - rotating the
    // system view space with each session removes this limitation.
    //

    MMSESSION Session;

    //
    // This is the driver object entry for WIN32K.SYS
    //
    // It is not a real driver object, but contained here
    // for information such as the DriverUnload routine.
    //

    DRIVER_OBJECT Win32KDriverObject;

    PETHREAD WorkingSetLockOwner;

    //
    // Pool descriptor for less than 1 page allocations.
    //

    POOL_DESCRIPTOR PagedPool;

    //
    // This is generally decremented in process delete (not clean) so that
    // the session data page and mapping PTE can finally be freed when this
    // reaches zero.  smss is the only process that decrements it in other
    // places as smss never exits.
    //

    LONG ProcessReferenceToSession;

    LCID LocaleId;

#if defined (_WIN64)

    //
    // NT64 has enough virtual address space to support per-session special
    // pool.
    //

    PMMPTE SpecialPoolFirstPte;
    PMMPTE SpecialPoolLastPte;
    PMMPTE NextPdeForSpecialPoolExpansion;
    PMMPTE LastPdeForSpecialPoolExpansion;
    PFN_NUMBER SpecialPagesInUse;
#endif

#if defined(_IA64_)
    REGION_MAP_INFO SessionMapInfo;
    PFN_NUMBER PageDirectoryParentPage;
#endif

#if DBG
    ULONG Debug[MM_SESS_COUNTER_MAX];

    MM_SESSION_MEMORY_COUNTERS Debug2[MM_SESS_MEMORY_COUNTER_MAX];
#endif

} MM_SESSION_SPACE, *PMM_SESSION_SPACE;

extern PMM_SESSION_SPACE MmSessionSpace;

extern ULONG MiSessionCount;

//
// This could be improved to just flush the non-global TB entries.
//

#define MI_FLUSH_SESSION_TB() KeFlushEntireTb (TRUE, TRUE);

//
// The default number of pages for the session working set minimum & maximum.
//

#define MI_SESSION_SPACE_WORKING_SET_MINIMUM 20

#define MI_SESSION_SPACE_WORKING_SET_MAXIMUM 384

NTSTATUS
MiSessionCommitPageTables (
    PVOID StartVa,
    PVOID EndVa
    );

NTSTATUS
MiInitializeAndChargePfn (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PFN_NUMBER ContainingPageFrame,
    IN LOGICAL SessionAllocation
    );

VOID
MiSessionPageTableRelease (
    IN PFN_NUMBER PageFrameIndex
    );

NTSTATUS
MiInitializeSessionPool (
    VOID
    );

VOID
MiCheckSessionPoolAllocations (
    VOID
    );

VOID
MiFreeSessionPoolBitMaps (
    VOID
    );

VOID
MiDetachSession (
    VOID
    );

VOID
MiAttachSession (
    IN PMM_SESSION_SPACE SessionGlobal
    );

PVOID
MiAttachToSecureProcessInSession (
    IN PRKAPC_STATE ApcState
    );

VOID
MiDetachFromSecureProcessInSession (
    IN PVOID OpaqueSession,
    IN PRKAPC_STATE ApcState
    );

VOID
MiReleaseProcessReferenceToSessionDataPage (
    PMM_SESSION_SPACE SessionGlobal
    );

#define MM_SET_SESSION_RESOURCE_OWNER(_Thread)                          \
        ASSERT (MmSessionSpace->WorkingSetLockOwner == NULL);           \
        MmSessionSpace->WorkingSetLockOwner = _Thread;

#define MM_CLEAR_SESSION_RESOURCE_OWNER()                               \
        ASSERT (MmSessionSpace->WorkingSetLockOwner == PsGetCurrentThread()); \
        MmSessionSpace->WorkingSetLockOwner = NULL;

#define MM_SESSION_SPACE_WS_LOCK_ASSERT()                               \
        ASSERT (MmSessionSpace->WorkingSetLockOwner == PsGetCurrentThread())

#define LOCK_SESSION_SPACE_WS(OLDIRQL,_Thread)                          \
            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);                   \
            KeRaiseIrql(APC_LEVEL,&OLDIRQL);                            \
            ExAcquireResourceExclusiveLite(&MmSessionSpace->WsLock, TRUE);  \
            MM_SET_SESSION_RESOURCE_OWNER (_Thread);

#define UNLOCK_SESSION_SPACE_WS(OLDIRQL)                                 \
                            MM_CLEAR_SESSION_RESOURCE_OWNER ();          \
                            ExReleaseResourceLite (&MmSessionSpace->WsLock); \
                            KeLowerIrql (OLDIRQL);                       \
                            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

extern PMMPTE MiHighestUserPte;
extern PMMPTE MiHighestUserPde;
#if (_MI_PAGING_LEVELS >= 4)
extern PMMPTE MiHighestUserPpe;
extern PMMPTE MiHighestUserPxe;
#endif

NTSTATUS
MiEmptyWorkingSet (
    IN PMMSUPPORT WsInfo,
    IN LOGICAL WaitOk
    );

//++
//ULONG
//MiGetPdeSessionIndex (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPdeSessionIndex returns the session structure index for the PDE
//    will (or does) map the given virtual address.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PDE index for.
//
// Return Value:
//
//    The index of the PDE entry.
//
//--

#define MiGetPdeSessionIndex(va)  ((ULONG)(((ULONG_PTR)(va) - (ULONG_PTR)MmSessionBase) >> PDI_SHIFT))

//
// Session space contains the image loader and tracker, virtual
// address allocator, paged pool allocator, system view image mappings,
// and working set for kernel mode virtual addresses that are instanced
// for groups of processes in a Session process group. This
// process group is identified by a SessionId.
//
// Each Session process group's loaded kernel modules, paged pool
// allocations, working set, and mapped system views are separate from
// other Session process groups, even though they have the same
// virtual addresses.
//
// This is to support the Hydra multi-user Windows NT system by
// replicating WIN32K.SYS, and its complement of video and printer drivers,
// desktop heaps, memory allocations, etc.
//

//
// Structure linked into a session space structure to describe
// which system images in PsLoadedModuleTable and
// SESSION_DRIVER_GLOBAL_LOAD_ADDRESS's
// have been allocated for the current session space.
//
// The reference count tracks the number of loads of this image within
// this session.
//

typedef struct _IMAGE_ENTRY_IN_SESSION {
    LIST_ENTRY Link;
    PVOID Address;
    PVOID LastAddress;
    ULONG ImageCountInThisSession;
    PMMPTE PrototypePtes;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
} IMAGE_ENTRY_IN_SESSION, *PIMAGE_ENTRY_IN_SESSION;

extern LIST_ENTRY MiSessionWsList;

NTSTATUS
FASTCALL
MiCheckPdeForSessionSpace(
    IN PVOID VirtualAddress
    );

NTSTATUS
MiShareSessionImage (
    IN PSECTION Section,
    IN OUT PSIZE_T ViewSize
    );

VOID
MiSessionWideInitializeAddresses (
    VOID
    );

NTSTATUS
MiSessionWideReserveImageAddress (
    IN PUNICODE_STRING pImageName,
    IN PSECTION Section,
    IN ULONG_PTR Alignment,
    OUT PVOID *ppAddr,
    OUT PLOGICAL pAlreadyLoaded
    );

VOID
MiInitializeSessionIds (
    VOID
    );

VOID
MiInitializeSessionWsSupport(
    VOID
    );

VOID
MiSessionAddProcess (
    IN PEPROCESS NewProcess
    );

VOID
MiSessionRemoveProcess (
    VOID
    );

NTSTATUS
MiRemovePsLoadedModule(
    PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

NTSTATUS
MiRemoveImageSessionWide(
    IN PVOID BaseAddr
    );

NTSTATUS
MiDeleteSessionVirtualAddresses(
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    );

NTSTATUS
MiUnloadSessionImageByForce (
    IN SIZE_T NumberOfPtes,
    IN PVOID ImageBase
    );

NTSTATUS
MiSessionWideGetImageSize(
    IN PVOID BaseAddress,
    OUT PSIZE_T NumberOfBytes OPTIONAL,
    OUT PSIZE_T CommitPages OPTIONAL
    );

PIMAGE_ENTRY_IN_SESSION
MiSessionLookupImage (
    IN PVOID BaseAddress
    );

NTSTATUS
MiSessionCommitImagePages(
    PVOID BaseAddr,
    SIZE_T Size
    );

VOID
MiSessionUnloadAllImages (
    VOID
    );

VOID
MiFreeSessionSpaceMap (
    VOID
    );

NTSTATUS
MiSessionInitializeWorkingSetList (
    VOID
    );

VOID
MiSessionUnlinkWorkingSet (
    VOID
    );

NTSTATUS
MiSessionCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    );

VOID
MiSessionOutSwapProcess (
    IN PEPROCESS Process
    );

VOID
MiSessionInSwapProcess (
    IN PEPROCESS Process
    );

#if !defined (_X86PAE_)

#define MI_GET_DIRECTORY_FRAME_FROM_PROCESS(_Process) \
        MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&((_Process)->Pcb.DirectoryTableBase[0])))

#define MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS(_Process) \
        MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&((_Process)->Pcb.DirectoryTableBase[1])))

#else

#define MI_GET_DIRECTORY_FRAME_FROM_PROCESS(_Process) \
        (MI_GET_PAGE_FRAME_FROM_PTE(((PMMPTE)((_Process)->PaeTop)) + PD_PER_SYSTEM - 1))

#define MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS(_Process) \
        ((PFN_NUMBER)((_Process)->Pcb.DirectoryTableBase[1]))

#endif

#if defined(_MIALT4K_)
NTSTATUS
MiSetCopyPagesFor4kPage (
    IN PEPROCESS Process,
    IN PMMVAD Vad,
    IN OUT PVOID StartingAddress,
    IN OUT PVOID EndingAddress,
    IN ULONG ProtectionMask,
    OUT PMMVAD *CallerNewVad
    );

VOID
MiRemoveAliasedVads (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

PVOID
MiDuplicateAliasVadList (
    IN PMMVAD Vad
    );
#endif

//
// The LDR_DATA_TABLE_ENTRY->LoadedImports is used as a list of imported DLLs.
//
// This field is zero if the module was loaded at boot time and the
// import information was never filled in.
//
// This field is -1 if no imports are defined by the module.
//
// This field contains a valid paged pool PLDR_DATA_TABLE_ENTRY pointer
// with a low-order (bit 0) tag of 1 if there is only 1 usable import needed
// by this driver.
//
// This field will contain a valid paged pool PLOAD_IMPORTS pointer in all
// other cases (ie: where at least 2 imports exist).
//

typedef struct _LOAD_IMPORTS {
    SIZE_T                  Count;
    PKLDR_DATA_TABLE_ENTRY   Entry[1];
} LOAD_IMPORTS, *PLOAD_IMPORTS;

#define LOADED_AT_BOOT  ((PLOAD_IMPORTS)0)
#define NO_IMPORTS_USED ((PLOAD_IMPORTS)-2)

#define SINGLE_ENTRY(ImportVoid)    ((ULONG)((ULONG_PTR)(ImportVoid) & 0x1))

#define SINGLE_ENTRY_TO_POINTER(ImportVoid)    ((PKLDR_DATA_TABLE_ENTRY)((ULONG_PTR)(ImportVoid) & ~0x1))

#define POINTER_TO_SINGLE_ENTRY(Pointer)    ((PKLDR_DATA_TABLE_ENTRY)((ULONG_PTR)(Pointer) | 0x1))

//
// This tracks allocated group virtual addresses.  The term SESSIONWIDE is used
// to denote data that is the same across all sessions (as opposed to
// per-session data which can vary from session to session).
//
// Since each driver loaded into a session space is linked and fixed up
// against the system image, it must remain at the same virtual address
// across the system regardless of the session.
//
// A list is maintained by the group allocator of which virtual
// addresses are in use and by which DLL.
//
// The reference count tracks the number of sessions that have loaded
// this image.
//
// Access to this structure is guarded by the MmSystemLoadLock.
//

typedef struct _SESSIONWIDE_DRIVER_ADDRESS {
    LIST_ENTRY Link;
    ULONG ReferenceCount;
    PVOID Address;
    ULONG_PTR Size;
    ULONG_PTR WritablePages;
    UNICODE_STRING FullDllName;
} SESSIONWIDE_DRIVER_ADDRESS, *PSESSIONWIDE_DRIVER_ADDRESS;

// #define _MI_DEBUG_RONLY 1     // Uncomment this for session readonly tracking

#if _MI_DEBUG_RONLY

VOID
MiAssertNotSessionData (
    IN PMMPTE PointerPte
    );

VOID
MiLogSessionDataStart (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

#define MI_ASSERT_NOT_SESSION_DATA(PTE) MiAssertNotSessionData(PTE)
#define MI_LOG_SESSION_DATA_START(DataTableEntry) MiLogSessionDataStart(DataTableEntry)
#else
#define MI_ASSERT_NOT_SESSION_DATA(PTE)
#define MI_LOG_SESSION_DATA_START(DataTableEntry)
#endif

//
// This tracks driver-specified individual verifier thunks.
//

typedef struct _DRIVER_SPECIFIED_VERIFIER_THUNKS {
    LIST_ENTRY ListEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG NumberOfThunks;
} DRIVER_SPECIFIED_VERIFIER_THUNKS, *PDRIVER_SPECIFIED_VERIFIER_THUNKS;

// #define _MI_DEBUG_SUB 1         // Uncomment this for subsection logging

#if defined (_MI_DEBUG_SUB)

extern ULONG MiTrackSubs;

#define MI_SUB_BACKTRACE_LENGTH 8

typedef struct _MI_SUB_TRACES {

    PETHREAD Thread;
    PMSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG_PTR CallerId;
    PVOID StackTrace [MI_SUB_BACKTRACE_LENGTH];

    MSUBSECTION SubsectionContents;
    CONTROL_AREA ControlAreaContents;

} MI_SUB_TRACES, *PMI_SUB_TRACES;

extern LONG MiSubsectionIndex;

extern PMI_SUB_TRACES MiSubsectionTraces;

VOID
FORCEINLINE
MiSnapSubsection (
    IN PMSUBSECTION Subsection,
    IN ULONG CallerId
    )
{
    PMI_SUB_TRACES Information;
    PCONTROL_AREA ControlArea;
    ULONG Index;
    ULONG Hash;

    if (MiSubsectionTraces == NULL) {
        return;
    }

    ControlArea = Subsection->ControlArea;

#if 0
    if (ControlArea->u.Flags.Mft == 0) {
        return;
    }
#endif

    Index = InterlockedIncrement(&MiSubsectionIndex);
    Index &= (MiTrackSubs - 1);
    Information = &MiSubsectionTraces[Index];

    Information->Subsection = Subsection;
    Information->ControlArea = ControlArea;
    *(PMSUBSECTION)&Information->SubsectionContents = *Subsection;
    *(PCONTROL_AREA)&Information->ControlAreaContents = *ControlArea;
    Information->Thread = PsGetCurrentThread();
    Information->CallerId = CallerId;
    RtlCaptureStackBackTrace (0, MI_SUB_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_SUB(_Sub, callerid) MiSnapSubsection(_Sub, callerid)

#else
#define MI_SNAP_SUB(_Sub, callerid)
#endif

#ifdef _MI_MESSAGE_SERVER
LOGICAL
MiQueueMessage (
    IN PVOID Message
    );

PVOID
MiRemoveMessage (
    VOID
    );

#define MI_INSTRUMENT_QUEUE(Message)    MiQueueMessage (Message)
#define MI_INSTRUMENTR_QUEUE()          MiRemoveMessage ()

#else

#define MI_INSTRUMENT_QUEUE(Message)
#define MI_INSTRUMENTR_QUEUE()

#endif

#endif  // MI
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\freevm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   freevm.c

Abstract:

    This module contains the routines which implement the
    NtFreeVirtualMemory service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#define MEM_CHECK_COMMIT_STATE 0x400000

#define MM_VALID_PTE_SIZE (256)


MMPTE MmDecommittedPte = {MM_DECOMMIT << MM_PROTECT_FIELD_SHIFT};

#if DBG
extern PEPROCESS MmWatchProcess;
VOID MmFooBar(VOID);
#endif // DBG
// #include "ntos.h"


#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFreeVirtualMemory)
#pragma alloc_text(PAGE,MiIsEntireRangeCommitted)
#endif

VOID
MiProcessValidPteList (
    IN PMMPTE *PteList,
    IN ULONG Count
    );

ULONG
MiDecommitPages (
    IN PVOID StartingAddress,
    IN PMMPTE EndingPte,
    IN PEPROCESS Process,
    IN PMMVAD_SHORT Vad
    );

VOID
MiDeleteFreeVm (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );


NTSTATUS
NtFreeVirtualMemory(
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG FreeType
     )

/*++

Routine Description:

    This function deletes a region of pages within the virtual address
    space of a subject process.

Arguments:

    ProcessHandle - An open handle to a process object.

    BaseAddress - The base address of the region of pages
                  to be freed. This value is rounded down to the
                  next host page address boundary.

    RegionSize - A pointer to a variable that will receive
                 the actual size in bytes of the freed region of
                 pages. The initial value of this argument is
                 rounded up to the next host page size boundary.

    FreeType - A set of flags that describe the type of
               free that is to be performed for the specified
               region of pages.

       FreeType Flags

        MEM_DECOMMIT - The specified region of pages is to be decommitted.

        MEM_RELEASE - The specified region of pages is to be released.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    PMMVAD_SHORT Vad;
    PMMVAD_SHORT NewVad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    PVOID StartingAddress;
    PVOID EndingAddress;
    NTSTATUS Status;
    LOGICAL Attached;
    SIZE_T CapturedRegionSize;
    PVOID CapturedBase;
    PMMPTE StartingPte;
    PMMPTE EndingPte;
    SIZE_T OldQuota;
    SIZE_T QuotaCharge;
    SIZE_T CommitReduction;
    ULONG_PTR OldEnd;
    LOGICAL UserPhysicalPages;
#if defined(_MIALT4K_)
    PVOID StartingAddress4k;
    PVOID EndingAddress4k;
    PVOID Wow64Process;
#endif
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check to make sure FreeType is good.
    //

    if ((FreeType & ~(MEM_DECOMMIT | MEM_RELEASE)) != 0) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // One of MEM_DECOMMIT or MEM_RELEASE must be specified, but not both.
    //

    if (((FreeType & (MEM_DECOMMIT | MEM_RELEASE)) == 0) ||
        ((FreeType & (MEM_DECOMMIT | MEM_RELEASE)) ==
                            (MEM_DECOMMIT | MEM_RELEASE))) {
        return STATUS_INVALID_PARAMETER_4;
    }
    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;

    }

    EndingAddress = (PVOID)(((LONG_PTR)CapturedBase + CapturedRegionSize - 1) |
                        (PAGE_SIZE - 1));

    StartingAddress = PAGE_ALIGN(CapturedBase);

    Attached = FALSE;

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {

        //
        // Reference the specified process handle for VM_OPERATION access.
        //

        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // If the specified process is not the current process, attach
        // to the specified process.
        //

        if (CurrentProcess != Process) {
            KeStackAttachProcess (&Process->Pcb, &ApcState);
            Attached = TRUE;
        }
    }

    CommitReduction = 0;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs to prevent page faults while
    // we own the working set mutex.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

#if defined(_MIALT4K_)

    Wow64Process = Process->Wow64Process;

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    StartingAddress4k = NULL;
    EndingAddress4k = NULL;

    if (CapturedRegionSize != 0) {
        
        if (Wow64Process != NULL) {

            //
            // Adjust Starting/EndingAddress for the native page size.
            //
            // StartingAddress: if this happened to be 4k aligned, but not
            // native aligned, then look at the previous 4k page and if it's
            // allocated then align the starting page to the next native
            // page, otherwise align it to the current one.
            //
            // EndingAddress: if this happened to be 4k aligned but not
            // native aligned, then look at the next 4k page and if it's
            // allocated, then make the ending address the previous
            // native page, otherwise make it the current.
            //
            // This is to ensure VADs are not leaked inside
            // the process when releasing partial allocations.
            //
            
            ASSERT (StartingAddress == PAGE_ALIGN(StartingAddress));

            StartingAddress4k = (PVOID)PAGE_4K_ALIGN(CapturedBase);

            if (StartingAddress4k >= (PVOID)MM_MAX_WOW64_ADDRESS) {

                //
                // The caller's address is not in the WOW64 area, pass it
                // through as a native request.
                //

                Wow64Process = NULL;
                goto NativeRequest;
            }

            EndingAddress4k = (PVOID)(((LONG_PTR)CapturedBase + CapturedRegionSize - 1) |
                                (PAGE_4K - 1));
    
            if (BYTE_OFFSET (StartingAddress4k) != 0) {

                if (MiArePreceding4kPagesAllocated (StartingAddress4k) == TRUE) {
                    StartingAddress = PAGE_NEXT_ALIGN (StartingAddress4k);
                }
            }

            if (EndingAddress4k >= (PVOID)MM_MAX_WOW64_ADDRESS) {

                //
                // The caller's address is not in the WOW64 area, pass it
                // through as a native request.
                //

                Wow64Process = NULL;
                goto NativeRequest;
            }

            if (BYTE_OFFSET (EndingAddress4k) != PAGE_SIZE - 1) {

                if (MiAreFollowing4kPagesAllocated (EndingAddress4k) == TRUE) {
                    EndingAddress = (PVOID)((ULONG_PTR)PAGE_ALIGN(EndingAddress4k) - 1);
                }
            }

            if (StartingAddress > EndingAddress) {

                //
                // There is no need to free native pages.
                //

                Vad = NULL;
                goto FreeAltPages;
            }
        }
    }
        
NativeRequest:

#endif

    Vad = (PMMVAD_SHORT)MiLocateAddress (StartingAddress);

    if (Vad == NULL) {

        //
        // No Virtual Address Descriptor located for Base Address.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    //
    // Found the associated Virtual Address Descriptor.
    //

    if (Vad->EndingVpn < MI_VA_TO_VPN (EndingAddress)) {

        //
        // The entire range to delete is not contained within a single
        // virtual address descriptor.  Return an error.
        //

        Status = STATUS_UNABLE_TO_FREE_VM;
        goto ErrorReturn;
    }

    //
    // Check to ensure this Vad is deletable.  Delete is required
    // for both decommit and release.
    //

    if ((Vad->u.VadFlags.PrivateMemory == 0) ||
        (Vad->u.VadFlags.PhysicalMapping == 1)) {
        Status = STATUS_UNABLE_TO_DELETE_SECTION;
        goto ErrorReturn;
    }

    if (Vad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is being made to delete a secured VAD, check
        // to see if this deletion is allowed.
        //

        if (FreeType & MEM_RELEASE) {

            //
            // Specify the whole range, this solves the problem with
            // splitting the VAD and trying to decide where the various
            // secure ranges need to go.
            //

            Status = MiCheckSecuredVad ((PMMVAD)Vad,
                                        MI_VPN_TO_VA (Vad->StartingVpn),
                        ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT) +
                                (PAGE_SIZE - 1),
                                        MM_SECURE_DELETE_CHECK);

        }
        else {
            Status = MiCheckSecuredVad ((PMMVAD)Vad,
                                        CapturedBase,
                                        CapturedRegionSize,
                                        MM_SECURE_DELETE_CHECK);
        }
        if (!NT_SUCCESS (Status)) {
            goto ErrorReturn;
        }
    }

    UserPhysicalPages = FALSE;

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);
    if (FreeType & MEM_RELEASE) {

        //
        // *****************************************************************
        // MEM_RELEASE was specified.
        // *****************************************************************
        //

        //
        // The descriptor for the address range is deletable.  Remove or split
        // the descriptor.
        //

        //
        // If the region size is zero, remove the whole VAD.
        //

        if (CapturedRegionSize == 0) {

            //
            // If the region size is specified as 0, the base address
            // must be the starting address for the region.
            //

            if (MI_VA_TO_VPN (CapturedBase) != Vad->StartingVpn) {
                Status = STATUS_FREE_VM_NOT_AT_BASE;
                goto ErrorReturn;
            }

            //
            // This Virtual Address Descriptor has been deleted.
            //

            StartingAddress = MI_VPN_TO_VA (Vad->StartingVpn);
            EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

#if defined(_MIALT4K_)
            StartingAddress4k = StartingAddress;
            EndingAddress4k  = EndingAddress;
#endif

            //
            // Free all the physical pages that this VAD might be mapping.
            // Since only the AWE lock synchronizes the remap API, carefully
            // remove this VAD from the list first.
            //

            LOCK_WS_UNSAFE (Process);

            if (Vad->u.VadFlags.UserPhysicalPages == 1) {
                MiAweViewRemover (Process, (PMMVAD)Vad);
                MiRemoveUserPhysicalPagesVad (Vad);
                UserPhysicalPages = TRUE;
            }
            else if (Vad->u.VadFlags.WriteWatch == 1) {
                MiPhysicalViewRemover (Process, (PMMVAD)Vad);
            }

            MiRemoveVad ((PMMVAD)Vad);

            //
            // Free the VAD pool after releasing our mutexes
            // to reduce contention.
            //

        }
        else {

            //
            // Region's size was not specified as zero, delete the
            // whole VAD or split the VAD.
            //

            if (MI_VA_TO_VPN (StartingAddress) == Vad->StartingVpn) {
                if (MI_VA_TO_VPN (EndingAddress) == Vad->EndingVpn) {

                    //
                    // This Virtual Address Descriptor has been deleted.
                    //

                    //
                    // Free all the physical pages that this VAD might be
                    // mapping.  Since only the AWE lock synchronizes the
                    // remap API, carefully remove this VAD from the list first.
                    //
        
                    LOCK_WS_UNSAFE (Process);

                    if (Vad->u.VadFlags.UserPhysicalPages == 1) {
                        MiAweViewRemover (Process, (PMMVAD)Vad);
                        MiRemoveUserPhysicalPagesVad (Vad);
                        UserPhysicalPages = TRUE;
                    }
                    else if (Vad->u.VadFlags.WriteWatch == 1) {
                        MiPhysicalViewRemover (Process, (PMMVAD)Vad);
                    }

                    MiRemoveVad ((PMMVAD)Vad);

                    //
                    // Free the VAD pool after releasing our mutexes
                    // to reduce contention.
                    //

                }
                else {

                    if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
                        (Vad->u.VadFlags.WriteWatch == 1)) {

                        //
                        // Splitting or chopping a physical VAD or a write-watch
                        // VAD is not allowed.
                        //

                        Status = STATUS_FREE_VM_NOT_AT_BASE;
                        goto ErrorReturn;
                    }

                    LOCK_WS_UNSAFE (Process);

                    //
                    // This Virtual Address Descriptor has a new starting
                    // address.
                    //

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    Vad->StartingVpn = MI_VA_TO_VPN ((PCHAR)EndingAddress + 1);
                    Vad->u.VadFlags.CommitCharge -= CommitReduction;
                    ASSERT ((SSIZE_T)Vad->u.VadFlags.CommitCharge >= 0);
                    NextVad = (PMMVAD)Vad;
                    Vad = NULL;
                }
            }
            else {

                if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
                    (Vad->u.VadFlags.WriteWatch == 1)) {

                    //
                    // Splitting or chopping a physical VAD or a write-watch
                    // VAD is not allowed.
                    //

                    Status = STATUS_FREE_VM_NOT_AT_BASE;
                    goto ErrorReturn;
                }

                //
                // Starting address is greater than start of VAD.
                //

                if (MI_VA_TO_VPN (EndingAddress) == Vad->EndingVpn) {

                    //
                    // Change the ending address of the VAD.
                    //

                    LOCK_WS_UNSAFE (Process);

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    Vad->u.VadFlags.CommitCharge -= CommitReduction;

                    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)StartingAddress - 1);
                    PreviousVad = (PMMVAD)Vad;
                }
                else {

                    //
                    // Split this VAD as the address range is within the VAD.
                    //

                    NewVad = ExAllocatePoolWithTag (NonPagedPool,
                                                    sizeof(MMVAD_SHORT),
                                                    'FdaV');

                    if (NewVad == NULL) {
                        Status = STATUS_INSUFFICIENT_RESOURCES;
                        goto ErrorReturn;
                    }

                    *NewVad = *Vad;

                    NewVad->StartingVpn = MI_VA_TO_VPN ((PCHAR)EndingAddress + 1);
                    //
                    // Set the commit charge to zero so MiInsertVad will
                    // not charge commitment for splitting the VAD.
                    //

                    NewVad->u.VadFlags.CommitCharge = 0;

                    OldEnd = Vad->EndingVpn;

                    LOCK_WS_UNSAFE (Process);

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    OldQuota = Vad->u.VadFlags.CommitCharge - CommitReduction;

                    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)StartingAddress - 1);

                    //
                    // Insert the VAD, this could fail due to quota charges.
                    //

                    Status = MiInsertVad ((PMMVAD)NewVad);

                    if (!NT_SUCCESS(Status)) {

                        //
                        // Inserting the Vad failed, reset the original
                        // Vad, free the new Vad and return an error.
                        //

                        Vad->EndingVpn = OldEnd;
                        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
                        ExFreePool (NewVad);
                        goto ErrorReturn2;
                    }

                    //
                    // As we have split the original VAD into 2 separate VADs
                    // there is no way of knowing what the commit charge
                    // is for each VAD.  Calculate the charge and reset
                    // each VAD.  Note that we also use the previous value
                    // to make sure the books stay balanced.
                    //

                    QuotaCharge = MiCalculatePageCommitment (MI_VPN_TO_VA (Vad->StartingVpn),
                                                             (PCHAR)StartingAddress - 1,
                                                             (PMMVAD)Vad,
                                                             Process);

                    Vad->u.VadFlags.CommitCharge = QuotaCharge;

                    //
                    // Give the remaining charge to the new VAD.
                    //

                    NewVad->u.VadFlags.CommitCharge = OldQuota - QuotaCharge;
                    PreviousVad = (PMMVAD)Vad;
                    NextVad = (PMMVAD)NewVad;
                }
                Vad = NULL;
            }
        }

        //
        // Return commitment for page table pages if possible.
        //

        MiReturnPageTablePageCommitment (StartingAddress,
                                         EndingAddress,
                                         Process,
                                         PreviousVad,
                                         NextVad);

        if (UserPhysicalPages == TRUE) {
            MiDeletePageTablesForPhysicalRange (StartingAddress, EndingAddress);
        }
        else {

            //
            // Get the PFN lock so MiDeleteVirtualAddresses can be called.
            //

            MiDeleteFreeVm (StartingAddress, EndingAddress);
        }

        UNLOCK_WS_UNSAFE (Process);

        CapturedRegionSize = 1 + (PCHAR)EndingAddress - (PCHAR)StartingAddress;

        //
        // Update the virtual size in the process header.
        //

        Process->VirtualSize -= CapturedRegionSize;

#if defined(_MIALT4K_)
        if (Wow64Process != NULL) {
            goto FreeAltPages;
        }
#endif

        Process->CommitCharge -= CommitReduction;

        UNLOCK_ADDRESS_SPACE (Process);

        if (CommitReduction != 0) {

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - CommitReduction);

            ASSERT (Vad == NULL);
            PsReturnProcessPageFileQuota (Process, CommitReduction);
            MiReturnCommitment (CommitReduction);

            if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                PsChangeJobMemoryUsage (-(SSIZE_T)CommitReduction);
            }

            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NTFREEVM1, CommitReduction);
        }
        else if (Vad != NULL) {
            ExFreePool (Vad);
        }

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        //
        // Establish an exception handler and write the size and base
        // address.
        //

        try {

            *RegionSize = CapturedRegionSize;
            *BaseAddress = StartingAddress;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // An exception occurred, don't take any action (just handle
            // the exception and return success.

        }

        return STATUS_SUCCESS;
    }

    if (Vad->u.VadFlags.UserPhysicalPages == 1) {

        //
        // Pages from a physical VAD must be released via
        // NtFreeUserPhysicalPages, not this routine.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    //
    // **************************************************************
    //
    // MEM_DECOMMIT was specified.
    //
    // **************************************************************
    //

    //
    // Check to ensure the complete range of pages is already committed.
    //

    if (CapturedRegionSize == 0) {

        if (MI_VA_TO_VPN (CapturedBase) != Vad->StartingVpn) {
            Status = STATUS_FREE_VM_NOT_AT_BASE;
            goto ErrorReturn;
        }
        EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

#if defined(_MIALT4K_)
        StartingAddress4k = StartingAddress;
        EndingAddress4k  = EndingAddress;
#endif
    }

#if 0
    if (FreeType & MEM_CHECK_COMMIT_STATE) {
        if ( !MiIsEntireRangeCommitted(StartingAddress,
                                       EndingAddress,
                                       Vad,
                                       Process)) {

            //
            // The entire range to be decommitted is not committed,
            // return an error.
            //

            Status = STATUS_UNABLE_TO_DECOMMIT_VM;
            goto ErrorReturn;
        }
    }
#endif //0

    //
    // The address range is entirely committed, decommit it now.
    //

    //
    // Calculate the initial quotas and commit charges for this VAD.
    //

    StartingPte = MiGetPteAddress (StartingAddress);
    EndingPte = MiGetPteAddress (EndingAddress);

    CommitReduction = 1 + EndingPte - StartingPte;

    LOCK_WS_UNSAFE (Process);

    //
    // Check to see if the entire range can be decommitted by
    // just updating the virtual address descriptor.
    //

    CommitReduction -= MiDecommitPages (StartingAddress,
                                        EndingPte,
                                        Process,
                                        Vad);

    UNLOCK_WS_UNSAFE (Process);

    //
    // Adjust the quota charges.
    //

    ASSERT ((LONG)CommitReduction >= 0);

    Vad->u.VadFlags.CommitCharge -= CommitReduction;
    ASSERT ((LONG)Vad->u.VadFlags.CommitCharge >= 0);
    Vad = NULL;

#if defined(_MIALT4K_)

FreeAltPages:

    if (Wow64Process != NULL) {

        if (FreeType & MEM_RELEASE) {
            MiReleaseFor4kPage (StartingAddress4k, 
                                EndingAddress4k, 
                                Process);
        }
        else {
            MiDecommitFor4kPage (StartingAddress4k, 
                                 EndingAddress4k, 
                                 Process);
        }

        StartingAddress = StartingAddress4k;
        EndingAddress = EndingAddress4k;
    }

#endif

    Process->CommitCharge -= CommitReduction;

    UNLOCK_ADDRESS_SPACE (Process);

    if (CommitReduction != 0) {

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - CommitReduction);

        PsReturnProcessPageFileQuota (Process, CommitReduction);
        MiReturnCommitment (CommitReduction);

        if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
            PsChangeJobMemoryUsage (-(SSIZE_T)CommitReduction);
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NTFREEVM2, CommitReduction);
    }
    else if (Vad != NULL) {
        ExFreePool (Vad);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    //
    // Establish an exception handler and write the size and base address.
    //

    try {

        *RegionSize = 1 + (PCHAR)EndingAddress - (PCHAR)StartingAddress;
        *BaseAddress = StartingAddress;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return STATUS_SUCCESS;

ErrorReturn:
       UNLOCK_ADDRESS_SPACE (Process);

ErrorReturn2:
       if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
       }

       if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
       }
       return Status;
}

ULONG
MiIsEntireRangeCommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine examines the range of pages from the starting address
    up to and including the ending address and returns TRUE if every
    page in the range is committed, FALSE otherwise.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the virtual address descriptor which describes the range.

    Process - Supplies the current process.

Return Value:

    TRUE if the entire range is committed.
    FALSE if any page within the range is not committed.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG FirstTime;
    ULONG Waited;
    PVOID Va;

    PAGED_CODE();

    FirstTime = TRUE;

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Set the Va to the starting address + 8, this solves problems
    // associated with address 0 (NULL) being used as a valid virtual
    // address and NULL in the VAD commitment field indicating no pages
    // are committed.
    //

    Va = (PVOID)((PCHAR)StartingAddress + 8);

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary(PointerPte) || (FirstTime)) {

            //
            // This may be a PXE/PPE/PDE boundary, check to see if all the
            // PXE/PPE/PDE pages exist.
            //

            FirstTime = FALSE;
            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPteAddress (PointerPde);
            PointerPxe = MiGetPteAddress (PointerPpe);

            do {

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif

                while (!MiDoesPxeExistAndMakeValid(PointerPxe, Process, FALSE, &Waited)) {

                    //
                    // No PPE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPxe += 1;

                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not
                            // committed, return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
                }

                while (!MiDoesPpeExistAndMakeValid(PointerPpe, Process, FALSE, &Waited)) {

                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not
                            // committed, return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    if (MiIsPteOnPdeBoundary (PointerPpe)) {
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        goto retry;
                    }
#endif
                }

                Waited = 0;

                while (!MiDoesPdeExistAndMakeValid(PointerPde, Process, FALSE, &Waited)) {

                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPde += 1;
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not committed,
                            // return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
#if (_MI_PAGING_LEVELS >= 3)
                    if (MiIsPteOnPdeBoundary (PointerPde)) {
                        PointerPpe = MiGetPteAddress (PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPpeBoundary (PointerPde)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            Waited = 1;
                            break;
                        }
#endif
                        Waited = 1;
                        break;
                    }
#endif
                }
            } while (Waited != 0);
        }

        //
        // The page table page exists, check each PTE for commitment.
        //

        if (PointerPte->u.Long == 0) {

            //
            // This page has not been committed, check the VAD.
            //

            if (Vad->u.VadFlags.MemCommit == 0) {

                //
                // The entire range to be decommitted is not committed,
                // return an error.
                //

                return FALSE;
            }
        }
        else {

            //
            // Has this page been explicitly decommitted?
            //

            if (MiIsPteDecommittedPage (PointerPte)) {

                //
                // This page has been explicitly decommitted, return an error.
                //

                return FALSE;
            }
        }
        PointerPte += 1;
        Va = (PVOID)((PCHAR)(Va) + PAGE_SIZE);
    }
    return TRUE;
}

ULONG
MiDecommitPages (
    IN PVOID StartingAddress,
    IN PMMPTE EndingPte,
    IN PEPROCESS Process,
    IN PMMVAD_SHORT Vad
    )

/*++

Routine Description:

    This routine decommits the specified range of pages.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingPte - Supplies the ending PTE of the range.

    Process - Supplies the current process.

    Vad - Supplies the virtual address descriptor which describes the range.

Return Value:

    Value to reduce commitment by for the VAD.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PVOID Va;
    ULONG CommitReduction;
    PMMPTE CommitLimitPte;
    KIRQL OldIrql;
    PMMPTE ValidPteList[MM_VALID_PTE_SIZE];
    ULONG count;
    WSLE_NUMBER WorkingSetIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PVOID SwapVa;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    MMPTE PteContents;
    PFN_NUMBER PageTableFrameIndex;
    PVOID UsedPageTableHandle;

    count = 0;
    CommitReduction = 0;

    if (Vad->u.VadFlags.MemCommit) {
        CommitLimitPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));
    }
    else {
        CommitLimitPte = NULL;
    }

    //
    // Decommit each page by setting the PTE to be explicitly
    // decommitted.  The PTEs cannot be deleted all at once as
    // this would set the PTEs to zero which would auto-evaluate
    // as committed if referenced by another thread when a page
    // table page is being in-paged.
    //

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    Va = StartingAddress;

    //
    // Loop through all the PDEs which map this region and ensure that
    // they exist.  If they don't exist create them by touching a
    // PTE mapped by the PDE.
    //

    MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);

    while (PointerPte <= EndingPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPdeAddress (Va);
            if (count != 0) {
                MiProcessValidPteList (&ValidPteList[0], count);
                count = 0;
            }

            MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);
        }

        //
        // The working set lock is held.  No PTEs can go from
        // invalid to valid or valid to invalid.  Transition
        // PTEs can go from transition to pagefile.
        //

        PteContents = *PointerPte;

        if (PteContents.u.Long != 0) {

            if (PointerPte->u.Long == MmDecommittedPte.u.Long) {

                //
                // This PTE is already decommitted.
                //

                CommitReduction += 1;
            }
            else {

                Process->NumberOfPrivatePages -= 1;

                if (PteContents.u.Hard.Valid == 1) {

                    //
                    // Make sure this is not a forked PTE.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                    if (Pfn1->u3.e1.PrototypePte) {

                        LOCK_PFN (OldIrql);
                        MiDeletePte (PointerPte,
                                     Va,
                                     FALSE,
                                     Process,
                                     NULL,
                                     NULL);
                        UNLOCK_PFN (OldIrql);
                        Process->NumberOfPrivatePages += 1;
                        MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                    }
                    else {

                        //
                        // Pte is valid, process later when PFN lock is held.
                        //

                        if (count == MM_VALID_PTE_SIZE) {
                            MiProcessValidPteList (&ValidPteList[0], count);
                            count = 0;
                        }
                        ValidPteList[count] = PointerPte;
                        count += 1;

                        //
                        // Remove address from working set list.
                        //


                        WorkingSetIndex = Pfn1->u1.WsIndex;

                        ASSERT (PAGE_ALIGN(MmWsle[WorkingSetIndex].u1.Long) ==
                                                                           Va);
                        //
                        // Check to see if this entry is locked in the working set
                        // or locked in memory.
                        //

                        Locked = MmWsle[WorkingSetIndex].u1.e1;

                        MiRemoveWsle (WorkingSetIndex, MmWorkingSetList);

                        //
                        // Add this entry to the list of free working set entries
                        // and adjust the working set count.
                        //

                        MiReleaseWsle (WorkingSetIndex, &Process->Vm);

                        if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

                            //
                            // This entry is locked.
                            //

                            MmWorkingSetList->FirstDynamic -= 1;

                            if (WorkingSetIndex != MmWorkingSetList->FirstDynamic) {

                                SwapVa = MmWsle[MmWorkingSetList->FirstDynamic].u1.VirtualAddress;
                                SwapVa = PAGE_ALIGN (SwapVa);
                                Pfn2 = MI_PFN_ELEMENT (
                                          MiGetPteAddress (SwapVa)->u.Hard.PageFrameNumber);

                                Entry = MiLocateWsle (SwapVa,
                                                      MmWorkingSetList,
                                                      Pfn2->u1.WsIndex);

                                MiSwapWslEntries (Entry,
                                                  WorkingSetIndex,
                                                  &Process->Vm);
                            }
                        }
                        MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);
                    }
                }
                else if (PteContents.u.Soft.Prototype) {

                    //
                    // This is a forked PTE, just delete it.
                    //

                    LOCK_PFN (OldIrql);
                    MiDeletePte (PointerPte,
                                 Va,
                                 FALSE,
                                 Process,
                                 NULL,
                                 NULL);
                    UNLOCK_PFN (OldIrql);
                    Process->NumberOfPrivatePages += 1;
                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                }
                else if (PteContents.u.Soft.Transition == 1) {

                    //
                    // Transition PTE, get the PFN database lock
                    // and reprocess this one.
                    //

                    LOCK_PFN (OldIrql);
                    PteContents = *PointerPte;

                    if (PteContents.u.Soft.Transition == 1) {

                        //
                        // PTE is still in transition, delete it.
                        //

                        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                        MI_SET_PFN_DELETED (Pfn1);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // Check the reference count for the page, if the
                        // reference count is zero, move the page to the
                        // free list, if the reference count is not zero,
                        // ignore this page.  When the reference count
                        // goes to zero, it will be placed on the free list.
                        //

                        if (Pfn1->u3.e2.ReferenceCount == 0) {
                            MiUnlinkPageFromList (Pfn1);
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
                        }

                    }
                    else {

                        //
                        // Page MUST be in page file format!
                        //

                        ASSERT (PteContents.u.Soft.Valid == 0);
                        ASSERT (PteContents.u.Soft.Prototype == 0);
                        ASSERT (PteContents.u.Soft.PageFileHigh != 0);
                        MiReleasePageFileSpace (PteContents);
                    }
                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                    UNLOCK_PFN (OldIrql);
                }
                else {

                    //
                    // Must be demand zero or paging file format.
                    //

                    if (PteContents.u.Soft.PageFileHigh != 0) {
                        LOCK_PFN (OldIrql);
                        MiReleasePageFileSpace (PteContents);
                        UNLOCK_PFN (OldIrql);
                    }
                    else {

                        //
                        // Don't subtract out the private page count for
                        // a demand zero page.
                        //

                        Process->NumberOfPrivatePages += 1;
                    }

                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                }
            }
        }
        else {

            //
            // The PTE is already zero.
            //

            //
            // Increment the count of non-zero page table entries for this
            // page table and the number of private pages for the process.
            //

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            if (PointerPte > CommitLimitPte) {

                //
                // Pte is not committed.
                //

                CommitReduction += 1;
            }
            MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
        }

        PointerPte += 1;
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }
    if (count != 0) {
        MiProcessValidPteList (&ValidPteList[0], count);
    }

    return CommitReduction;
}


VOID
MiProcessValidPteList (
    IN PMMPTE *ValidPteList,
    IN ULONG Count
    )

/*++

Routine Description:

    This routine flushes the specified range of valid PTEs.

Arguments:

    ValidPteList - Supplies a pointer to an array of PTEs to flush.

    Count - Supplies the count of the number of elements in the array.

Return Value:

    none.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    ULONG i;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    KIRQL OldIrql;

    i = 0;
    PteFlushList.Count = Count;

    LOCK_PFN (OldIrql);

    do {
        PteContents = *ValidPteList[i];
        ASSERT (PteContents.u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Decrement the share and valid counts of the page table
        // page which maps this PTE.
        //

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        //
        // Decrement the share count for the physical page.  As the page
        // is private it will be put on the free list.
        //

        MiDecrementShareCountOnly (PageFrameIndex);

        if (Count < MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushPte[i] = ValidPteList[i];
            PteFlushList.FlushVa[i] =
                        MiGetVirtualAddressMappedByPte (ValidPteList[i]);
        }
        *ValidPteList[i] = MmDecommittedPte;
        i += 1;
    } while (i != Count);

    MiFlushPteList (&PteFlushList, FALSE, MmDecommittedPte);
    UNLOCK_PFN (OldIrql);
    return;
}


VOID
MiDeleteFreeVm (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    Nonpagable routine to call acquire PFN lock and call
    MiDeleteVirtualAddresses.

Arguments:


Return Value:

    none.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    //
    // Delete the address range.
    //

    MiDeleteVirtualAddresses (StartingAddress,
                              EndingAddress,
                              FALSE,
                              (PMMVAD)NULL);

    UNLOCK_PFN (OldIrql);

}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\iosup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   iosup.c

Abstract:

    This module contains routines which provide support for the I/O system.

Author:

    Lou Perazzoli (loup) 25-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#undef MmIsRecursiveIoFault

ULONG MiCacheOverride[3];

extern ULONG MmTotalSystemDriverPages;

BOOLEAN
MmIsRecursiveIoFault (
    VOID
    );

PVOID
MiAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MEMORY_CACHING_TYPE CacheType,
    PVOID CallingAddress
    );

PVOID
MiMapLockedPagesInUserSpace (
     IN PMDL MemoryDescriptorList,
     IN PVOID StartingVa,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID BaseVa
     );

VOID
MiUnmapLockedPagesInUserSpace (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     );

VOID
MiAddMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller,
    IN PFN_NUMBER NumberOfPagesToLock,
    IN ULONG Who
    );

typedef struct _PTE_TRACKER {
    LIST_ENTRY ListEntry;
    PMDL Mdl;
    PFN_NUMBER Count;
    PVOID SystemVa;
    PVOID StartVa;
    ULONG Offset;
    ULONG Length;
    ULONG_PTR Page;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PVOID PteAddress;
} PTE_TRACKER, *PPTE_TRACKER;

typedef struct _SYSPTES_HEADER {
    LIST_ENTRY ListHead;
    PFN_NUMBER Count;
} SYSPTES_HEADER, *PSYSPTES_HEADER;

ULONG MmTrackPtes = 0;
BOOLEAN MiTrackPtesAborted = FALSE;
SYSPTES_HEADER MiPteHeader;
SLIST_HEADER MiDeadPteTrackerSListHead;
KSPIN_LOCK MiPteTrackerLock;

LOCK_HEADER MmLockedPagesHead;
BOOLEAN MiTrackingAborted = FALSE;

ULONG MiNonCachedCollisions;

#if DBG
PFN_NUMBER MiCurrentAdvancedPages;
PFN_NUMBER MiAdvancesGiven;
PFN_NUMBER MiAdvancesFreed;
#endif

VOID
MiInsertPteTracker (
     IN PPTE_TRACKER PteTracker,
     IN PMDL MemoryDescriptorList,
     IN PFN_NUMBER NumberOfPtes,
     IN PVOID MyCaller,
     IN PVOID MyCallersCaller
     );

VOID
MiRemovePteTracker (
     IN PMDL MemoryDescriptorList OPTIONAL,
     IN PVOID PteAddress,
     IN PFN_NUMBER NumberOfPtes
     );

PPTE_TRACKER
MiReleaseDeadPteTrackers (
    VOID
    );

VOID
MiProtectFreeNonPagedPool (
    IN PVOID VirtualAddress,
    IN ULONG SizeInPages
    );

LOGICAL
MiUnProtectFreeNonPagedPool (
    IN PVOID VirtualAddress,
    IN ULONG SizeInPages
    );

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

#if DBG
ULONG MiPrintLockedPages;

VOID
MiVerifyLockedPageCharges (
    VOID
    );
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MmSetPageProtection)
#pragma alloc_text(INIT, MmFreeIndependentPages)
#pragma alloc_text(INIT, MiInitializeIoTrackers)

#pragma alloc_text(PAGE, MmAllocateIndependentPages)
#pragma alloc_text(PAGE, MmLockPagableDataSection)
#pragma alloc_text(PAGE, MiLookupDataTableEntry)
#pragma alloc_text(PAGE, MmSetBankedSection)
#pragma alloc_text(PAGE, MmProbeAndLockProcessPages)
#pragma alloc_text(PAGE, MmProbeAndLockSelectedPages)
#pragma alloc_text(PAGE, MmMapVideoDisplay)
#pragma alloc_text(PAGE, MmUnmapVideoDisplay)
#pragma alloc_text(PAGE, MmGetSectionRange)
#pragma alloc_text(PAGE, MiMapSinglePage)
#pragma alloc_text(PAGE, MiUnmapSinglePage)
#pragma alloc_text(PAGE, MmAllocateMappingAddress)
#pragma alloc_text(PAGE, MmFreeMappingAddress)
#pragma alloc_text(PAGE, MmAllocateNonCachedMemory)
#pragma alloc_text(PAGE, MmFreeNonCachedMemory)
#pragma alloc_text(PAGE, MmLockPagedPool)
#pragma alloc_text(PAGE, MmLockPagableSectionByHandle)
#pragma alloc_text(PAGE, MiMapLockedPagesInUserSpace)

#pragma alloc_text(PAGELK, MmEnablePAT)
#pragma alloc_text(PAGELK, MiUnmapLockedPagesInUserSpace)
#pragma alloc_text(PAGELK, MmAllocatePagesForMdl)
#pragma alloc_text(PAGELK, MmFreePagesFromMdl)
#pragma alloc_text(PAGELK, MmUnlockPagedPool)
#pragma alloc_text(PAGELK, MmGatherMemoryForHibernate)
#pragma alloc_text(PAGELK, MmReturnMemoryForHibernate)
#pragma alloc_text(PAGELK, MmReleaseDumpAddresses)
#pragma alloc_text(PAGELK, MmMapUserAddressesToPage)
#pragma alloc_text(PAGELK, MiPhysicalViewInserter)
#pragma alloc_text(PAGELK, MiPhysicalViewAdjuster)

#pragma alloc_text(PAGEVRFY, MmIsSystemAddressLocked)
#pragma alloc_text(PAGEVRFY, MmAreMdlPagesLocked)
#endif

extern POOL_DESCRIPTOR NonPagedPoolDescriptor;

PFN_NUMBER MmMdlPagesAllocated;

KEVENT MmCollidedLockEvent;
LONG MmCollidedLockWait;

SIZE_T MmLockedCode;

BOOLEAN MiWriteCombiningPtes = FALSE;

#ifdef LARGE_PAGES
ULONG MmLargeVideoMapped;
#endif

#if DBG
ULONG MiPrintAwe;
ULONG MmStopOnBadProbe = 1;
#endif

#define MI_PROBE_RAISE_SIZE 10

ULONG MiProbeRaises[MI_PROBE_RAISE_SIZE];

#define MI_INSTRUMENT_PROBE_RAISES(i)       \
        ASSERT (i < MI_PROBE_RAISE_SIZE);   \
        MiProbeRaises[i] += 1;

//
//  Note: this should be > 2041 to account for the cache manager's
//  aggressive zeroing logic.
//

ULONG MmReferenceCountCheck = 2500;

ULONG MiMdlsAdjusted = FALSE;


VOID
MmProbeAndLockPages (
     IN OUT PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode,
     IN LOCK_OPERATION Operation
     )

/*++

Routine Description:

    This routine probes the specified pages, makes the pages resident and
    locks the physical pages mapped by the virtual pages in memory.  The
    Memory descriptor list is updated to describe the physical pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                            (MDL). The supplied MDL must supply a virtual
                            address, byte offset and length field.  The
                            physical page portion of the MDL is updated when
                            the pages are locked in memory.

    AccessMode - Supplies the access mode in which to probe the arguments.
                 One of KernelMode or UserMode.

    Operation - Supplies the operation type.  One of IoReadAccess, IoWriteAccess
                or IoModifyAccess.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below for pagable addresses,
                  DISPATCH_LEVEL and below for non-pagable addresses.

--*/

{
    PPFN_NUMBER Page;
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PVOID Va;
    PVOID EndVa;
    PVOID AlignedVa;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPageFrameIndex;
    PEPROCESS CurrentProcess;
    KIRQL OldIrql;
    PFN_NUMBER NumberOfPagesToLock;
    PFN_NUMBER NumberOfPagesSpanned;
    NTSTATUS status;
    NTSTATUS ProbeStatus;
    PETHREAD Thread;
    ULONG SavedState;
    LOGICAL AddressIsPhysical;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    PCHAR StartVa;
    PVOID CallingAddress;
    PVOID CallersCaller;
#if defined (_MIALT4K_)
    MMPTE AltPteContents;
    PMMPTE PointerAltPte;
    PMMPTE LastPointerAltPte;
    PMMPTE AltPointerPte;
    PMMPTE AltPointerPde;
    PMMPTE AltPointerPpe;
    PMMPTE AltPointerPxe;
#endif

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT (((ULONG)MemoryDescriptorList->ByteOffset & ~(PAGE_SIZE - 1)) == 0);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT (((ULONG_PTR)MemoryDescriptorList->StartVa & (PAGE_SIZE - 1)) == 0);
    AlignedVa = (PVOID)MemoryDescriptorList->StartVa;

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL |
                    MDL_IO_SPACE)) == 0);

    Va = (PCHAR)AlignedVa + MemoryDescriptorList->ByteOffset;
    StartVa = Va;

    //
    // Endva is one byte past the end of the buffer, if ACCESS_MODE is not
    // kernel, make sure the EndVa is in user space AND the byte count
    // does not cause it to wrap.
    //

    EndVa = (PVOID)((PCHAR)Va + MemoryDescriptorList->ByteCount);

    if ((AccessMode != KernelMode) &&
        ((EndVa > (PVOID)MM_USER_PROBE_ADDRESS) || (Va >= EndVa))) {
        *Page = MM_EMPTY_LIST;
        MI_INSTRUMENT_PROBE_RAISES(0);
        ExRaiseStatus (STATUS_ACCESS_VIOLATION);
        return;
    }

    //
    // You would think there is an optimization which could be performed here:
    // if the operation is for WriteAccess and the complete page is
    // being modified, we can remove the current page, if it is not
    // resident, and substitute a demand zero page.
    // Note, that after analysis by marking the thread and then
    // noting if a page read was done, this rarely occurs.
    //

    Thread = PsGetCurrentThread ();

    if (!MI_IS_PHYSICAL_ADDRESS(Va)) {

        AddressIsPhysical = FALSE;
        ProbeStatus = STATUS_SUCCESS;

        NumberOfPagesToLock = ADDRESS_AND_SIZE_TO_SPAN_PAGES (Va,
                                       MemoryDescriptorList->ByteCount);

        ASSERT (NumberOfPagesToLock != 0);

        NumberOfPagesSpanned = NumberOfPagesToLock;

        PointerPxe = MiGetPxeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPde = MiGetPdeAddress (Va);
        PointerPte = MiGetPteAddress (Va);

        MmSavePageFaultReadAhead (Thread, &SavedState);
        MmSetPageFaultReadAhead (Thread, (ULONG)(NumberOfPagesToLock - 1));

        try {

            do {

                *Page = MM_EMPTY_LIST;

                //
                // Make sure the page is resident.
                //

                *(volatile CHAR *)Va;

                if ((Operation != IoReadAccess) &&
                    (Va <= MM_HIGHEST_USER_ADDRESS)) {

                    //
                    // Probe for write access as well.
                    //

                    ProbeForWriteChar ((PCHAR)Va);
                }

                NumberOfPagesToLock -= 1;

                MmSetPageFaultReadAhead (Thread, (ULONG)(NumberOfPagesToLock - 1));
                Va = (PVOID) (((ULONG_PTR)Va + PAGE_SIZE) & ~(PAGE_SIZE - 1));
                Page += 1;
            } while (Va < EndVa);

            ASSERT (NumberOfPagesToLock == 0);

        } except (EXCEPTION_EXECUTE_HANDLER) {
            ProbeStatus = GetExceptionCode();
        }

        //
        // We may still fault again below but it's generally rare.
        // Restore this thread's normal fault behavior now.
        //

        MmResetPageFaultReadAhead (Thread, SavedState);

        if (ProbeStatus != STATUS_SUCCESS) {
            MI_INSTRUMENT_PROBE_RAISES(1);
            MemoryDescriptorList->Process = NULL;
            ExRaiseStatus (ProbeStatus);
            return;
        }
    }
    else {
        AddressIsPhysical = TRUE;
        *Page = MM_EMPTY_LIST;

        //
        // Initializing these is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        NumberOfPagesSpanned = 0;
        PointerPxe = NULL;
        PointerPpe = NULL;
        PointerPde = NULL;
        PointerPte = NULL;
        SavedState = 0;
    }

    Va = AlignedVa;
    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    //
    // Indicate that this is a write operation.
    //

    if (Operation != IoReadAccess) {
        MemoryDescriptorList->MdlFlags |= MDL_WRITE_OPERATION;
    }
    else {
        MemoryDescriptorList->MdlFlags &= ~(MDL_WRITE_OPERATION);
    }

    //
    // Initialize MdlFlags (assume the probe will succeed).
    //

    MemoryDescriptorList->MdlFlags |= MDL_PAGES_LOCKED;

    if (Va <= MM_HIGHEST_USER_ADDRESS) {

        //
        // These are user space addresses, check to see if the
        // working set size will allow these pages to be locked.
        //

        ASSERT (AddressIsPhysical == FALSE);
        ASSERT (NumberOfPagesSpanned != 0);

        CurrentProcess = PsGetCurrentProcess ();

        //
        // Initialize the MDL process field (assume the probe will succeed).
        //

        MemoryDescriptorList->Process = CurrentProcess;

        LastPte = MiGetPteAddress ((PCHAR)EndVa - 1);

        //
        // Acquire the PFN database lock.
        //
    
        LOCK_PFN2 (OldIrql);
    
        //
        // Check for a transfer to/from a physical VAD - no reference counts
        // may be modified for these pages.
        //

        if (CurrentProcess->Flags & PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD) {

            //
            // This process has a physical VAD which maps directly to RAM
            // not necessarily present in the PFN database.  See if the
            // MDL request intersects this physical VAD.
            //

            NextEntry = CurrentProcess->PhysicalVadList.Flink;
            while (NextEntry != &CurrentProcess->PhysicalVadList) {

                PhysicalView = CONTAINING_RECORD(NextEntry,
                                                 MI_PHYSICAL_VIEW,
                                                 ListEntry);

                if (PhysicalView->Vad->u.VadFlags.PhysicalMapping == 0) {
                    NextEntry = NextEntry->Flink;
                    continue;
                }

                if (StartVa < PhysicalView->StartVa) {
    
                    if ((PCHAR)EndVa - 1 >= PhysicalView->StartVa) {
    
                        //
                        // The range encompasses a physical VAD.  This is not
                        // allowed.
                        //
    
                        UNLOCK_PFN2 (OldIrql);
                        MI_INSTRUMENT_PROBE_RAISES(2);
                        MemoryDescriptorList->Process = NULL;
                        MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                        ExRaiseStatus (STATUS_ACCESS_VIOLATION);
                        return;
                    }
    
                    NextEntry = NextEntry->Flink;
                    continue;
                }

                if (StartVa <= PhysicalView->EndVa) {
    
                    //
                    // Ensure that the entire range lies within the VAD.
                    //
    
                    if ((PCHAR)EndVa - 1 > PhysicalView->EndVa) {
    
                        //
                        // The range goes past the end of the VAD - not allowed.
                        //
    
                        UNLOCK_PFN2 (OldIrql);
                        MI_INSTRUMENT_PROBE_RAISES(3);
                        MemoryDescriptorList->Process = NULL;
                        MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                        ExRaiseStatus (STATUS_ACCESS_VIOLATION);
                        return;
                    }
    
                    //
                    // The range lies within a physical VAD.
                    //
    
                    if (Operation != IoReadAccess) {
    
                        //
                        // Ensure the VAD is writable.  Changing individual PTE
                        // protections in a physical VAD is not allowed.
                        //
    
                        if ((PhysicalView->Vad->u.VadFlags.Protection & MM_READWRITE) == 0) {
                            UNLOCK_PFN2 (OldIrql);
                            MI_INSTRUMENT_PROBE_RAISES(4);
                            MemoryDescriptorList->Process = NULL;
                            MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                            ExRaiseStatus (STATUS_ACCESS_VIOLATION);
                            return;
                        }
                    }
    
                    //
                    // Don't charge page locking for this transfer as it is all
                    // physical, just initialize the MDL.  Note the pages do not
                    // have to be physically contiguous, so the frames must be
                    // extracted from the PTEs.
                    //
                    // Treat this as an I/O space address and don't allow
                    // operations on addresses not in the PFN database.
                    //

                    LastPte = PointerPte + NumberOfPagesSpanned;

                    do {
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                        *Page = PageFrameIndex;
                        Page += 1;
                        PointerPte += 1;
                    } while (PointerPte < LastPte);
    
                    UNLOCK_PFN2 (OldIrql);

                    MemoryDescriptorList->MdlFlags |= (MDL_IO_SPACE | MDL_PAGES_LOCKED);
                    return;
                }
                NextEntry = NextEntry->Flink;
            }
        }

        InterlockedExchangeAddSizeT (&CurrentProcess->NumberOfLockedPages,
                                     NumberOfPagesSpanned);
    }
    else {

        MemoryDescriptorList->Process = NULL;

        if (AddressIsPhysical == TRUE) {

            //
            // On certain architectures, virtual addresses
            // may be physical and hence have no corresponding PTE.
            //

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (Va);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            Va = (PCHAR)Va + MemoryDescriptorList->ByteOffset;
            NumberOfPagesToLock = ADDRESS_AND_SIZE_TO_SPAN_PAGES (Va,
                                        MemoryDescriptorList->ByteCount);

            LastPageFrameIndex = PageFrameIndex + NumberOfPagesToLock;

            //
            // Acquire the PFN database lock.
            //
    
            LOCK_PFN2 (OldIrql);

            ASSERT (PageFrameIndex <= MmHighestPhysicalPage);
            ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

            //
            // Ensure the systemwide locked pages count remains fluid.
            //
    
            if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER) NumberOfPagesToLock) {
    
                //
                // If this page is for paged pool or privileged code/data,
                // then force it in regardless.
                //
    
                if ((Va < MM_HIGHEST_USER_ADDRESS) ||
                    (MI_IS_SYSTEM_CACHE_ADDRESS(Va))) {

                    UNLOCK_PFN2 (OldIrql);
                    MI_INSTRUMENT_PROBE_RAISES(5);
                    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                    ExRaiseStatus (STATUS_WORKING_SET_QUOTA);
                    return;
                }

                MI_INSTRUMENT_PROBE_RAISES(8);
            }
    
            do {
    
                //
                // Check to make sure each page is not locked down an unusually
                // high number of times.
                //
    
                if (Pfn1->u3.e2.ReferenceCount >= MmReferenceCountCheck) {
                    UNLOCK_PFN2 (OldIrql);
                    ASSERT (FALSE);
                    status = STATUS_WORKING_SET_QUOTA;
                    goto failure;
                }
    
                if (MemoryDescriptorList->MdlFlags & MDL_WRITE_OPERATION) {
                    MI_SNAP_DIRTY (Pfn1, 1, 0x99);
                }

                MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 0);

                Pfn1->u3.e2.ReferenceCount += 1;

                *Page = PageFrameIndex;

                Page += 1;
                PageFrameIndex += 1;
                Pfn1 += 1;

            } while (PageFrameIndex < LastPageFrameIndex);

            UNLOCK_PFN2 (OldIrql);
            return;
        }

        //
        // Since this operation is to a system address, no need to check for
        // PTE write access below so mark the access as a read so only the
        // operation type (and not where the Va is) needs to be checked in the
        // subsequent loop.
        //

        ASSERT (Va > MM_HIGHEST_USER_ADDRESS);
        Operation = IoReadAccess;

        LastPte = MiGetPteAddress ((PCHAR)EndVa - 1);
        LOCK_PFN2 (OldIrql);
    }

    do {

#if (_MI_PAGING_LEVELS==4)
        while ((PointerPxe->u.Hard.Valid == 0) ||
               (PointerPpe->u.Hard.Valid == 0) ||
               (PointerPde->u.Hard.Valid == 0) ||
               (PointerPte->u.Hard.Valid == 0))
#elif (_MI_PAGING_LEVELS==3)
        while ((PointerPpe->u.Hard.Valid == 0) ||
               (PointerPde->u.Hard.Valid == 0) ||
               (PointerPte->u.Hard.Valid == 0))
#else
        while ((PointerPde->u.Hard.Valid == 0) ||
               (PointerPte->u.Hard.Valid == 0))
#endif
        {

            //
            // PDE is not resident, release the PFN lock and access the page
            // to make it appear.
            //

            UNLOCK_PFN2 (OldIrql);

            MmSetPageFaultReadAhead (Thread, 0);

            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            status = MmAccessFault (FALSE, Va, KernelMode, NULL);

            MmResetPageFaultReadAhead (Thread, SavedState);

            if (!NT_SUCCESS(status)) {
                goto failure;
            }

            LOCK_PFN2 (OldIrql);
        }

        PteContents = *PointerPte;

        //
        // There is a subtle race here where the PTE contents can get zeroed
        // by a thread running on another processor.  This can only happen
        // for an AWE address space because these ranges (deliberately for
        // performance reasons) do not acquire the PFN lock during remap
        // operations.  In this case, one of 2 scenarios is possible -
        // either the old PTE is read or the new.  The new may be a zero
        // PTE if the map request was to invalidate *or* non-zero (and
        // valid) if the map request was inserting a new entry.  For the
        // latter, we don't care if we lock the old or new frame here as
        // it's an application bug to provoke this behavior - and
        // regardless of which is used, no corruption can occur because
        // the PFN lock is acquired during an NtFreeUserPhysicalPages.
        // But the former must be checked for explicitly here.  As a
        // separate note, the PXE/PPE/PDE accesses above are always safe
        // even for the AWE deletion race because these tables
        // are never lazy-allocated for AWE ranges.
        //

        if (PteContents.u.Hard.Valid == 0) {
            ASSERT (PteContents.u.Long == 0);
            ASSERT (PsGetCurrentProcess ()->AweInfo != NULL);
            UNLOCK_PFN2 (OldIrql);
            status = STATUS_ACCESS_VIOLATION;
            goto failure;
        }

#if defined (_MIALT4K_)
        
            if (PteContents.u.Hard.Cache == MM_PTE_CACHE_RESERVED) {
                           
                //
                // This is a wow64 split page - ie: the individual 4k
                // pages have different permissions, so each 4k page within
                // this native page must be probed individually.
                //
                // Note split pages are generally rare.
                //
    
                ASSERT (PsGetCurrentProcess()->Wow64Process != NULL);
                ASSERT (EndVa < (PVOID)MM_MAX_WOW64_ADDRESS);
    
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                PointerAltPte = MiGetAltPteAddress (Va);
                LastPointerAltPte = PointerAltPte + (PAGE_SIZE / PAGE_4K) - 1;
    
                AltPointerPxe = MiGetPxeAddress (PointerAltPte);
                AltPointerPpe = MiGetPpeAddress (PointerAltPte);
                AltPointerPde = MiGetPdeAddress (PointerAltPte);
                AltPointerPte = MiGetPteAddress (PointerAltPte);
    
#if (_MI_PAGING_LEVELS==4)
                while ((AltPointerPxe->u.Hard.Valid == 0) ||
                       (AltPointerPpe->u.Hard.Valid == 0) ||
                       (AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#elif (_MI_PAGING_LEVELS==3)
                while ((AltPointerPpe->u.Hard.Valid == 0) ||
                       (AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#else
                while ((AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#endif
                {
    
                    //
                    // The ALTPTEs are not resident, release the PFN lock and
                    // access it to make it appear.  Then restart the entire
                    // operation as the PFN lock was released so anything
                    // could have happened to the address space.
                    //
    
                    UNLOCK_PFN2 (OldIrql);
    
                    MmSetPageFaultReadAhead (Thread, 0);
    
                    status = MmAccessFault (FALSE, PointerAltPte, KernelMode, NULL);
    
                    MmResetPageFaultReadAhead (Thread, SavedState);
    
                    if (!NT_SUCCESS(status)) {
                        goto failure;
                    }
    
                    LOCK_PFN2 (OldIrql);
    
                    continue;
                }

                //
                // The ALTPTEs are now present and the PFN lock is held again.  
                // Examine the individual 4k page states in the ALTPTEs.
                //
                // Note that only the relevant 4k pages can be examined - ie:
                // if the transfer starts in the 2nd 4k of a native page,
                // then don't examine the 1st 4k.  If the transfer ends in
                // the first half of a native page, then don't examine the
                // 2nd 4k.
                //
                
                ASSERT (PAGE_SIZE == 2 * PAGE_4K);

                if (PAGE_ALIGN (StartVa) == PAGE_ALIGN (Va)) {

                    //
                    // We are in the first page, see if we need to round up.
                    //

                    if (BYTE_OFFSET (StartVa) >= PAGE_4K) {
                        PointerAltPte += 1;
                        Va = (PVOID)((ULONG_PTR)Va + PAGE_4K);
                    }
                }

                if (PAGE_ALIGN ((PCHAR)EndVa - 1) == PAGE_ALIGN (Va)) {

                    //
                    // We are in the last page, see if we need to round down.
                    //

                    if (BYTE_OFFSET ((PCHAR)EndVa - 1) < PAGE_4K) {
                        LastPointerAltPte -= 1;
                    }
                }

                //
                // We better not have rounded up and down in the same page !
                //

                ASSERT (PointerAltPte <= LastPointerAltPte);
    
                ASSERT (PointerAltPte != NULL);
    
                do {
    
                    //
                    //  If the sub 4k page is :
                    //
                    //  1 - No access or
                    //  2 - This is a private not-committed page or
                    //  3 - This is write operation and the page is read only
                    //
                    // then return an access violation.
                    //
    
                    AltPteContents = *PointerAltPte;

                    if (AltPteContents.u.Alt.NoAccess != 0) {
                        status = STATUS_ACCESS_VIOLATION;
                        UNLOCK_PFN2 (OldIrql);
                        goto failure;
                    }

                    if ((AltPteContents.u.Alt.Commit == 0) && (AltPteContents.u.Alt.Private != 0)) {
                        status = STATUS_ACCESS_VIOLATION;
                        UNLOCK_PFN2 (OldIrql);
                        goto failure;
                    }

                    if (Operation != IoReadAccess) {

                        //
                        // If the caller is writing and the ALTPTE indicates
                        // it's not writable or copy on write, then AV.
                        //
                        // If it's copy on write, then fall through for further
                        // interrogation.
                        //

                        if ((AltPteContents.u.Alt.Write == 0) &&
                            (AltPteContents.u.Alt.CopyOnWrite == 0)) {
    
                            status = STATUS_ACCESS_VIOLATION;
                            UNLOCK_PFN2 (OldIrql);
                            goto failure;
                        }
                    }
    
                    //
                    //  If the sub 4k page is :
                    //
                    //  1 - has not been accessed yet or
                    //  2 - demand-fill zero or
                    //  3 - copy-on-write, and this is a write operation
                    //
                    //  then go the long way and see if it can be paged in.
                    //
    
                    if ((AltPteContents.u.Alt.Accessed == 0) ||
                        (AltPteContents.u.Alt.FillZero != 0) ||
                        ((Operation != IoReadAccess) && (AltPteContents.u.Alt.CopyOnWrite == 1))) {
    
                        UNLOCK_PFN2 (OldIrql);
    
                        MmSetPageFaultReadAhead (Thread, 0);
    
                        status = MmX86Fault (FALSE, Va, KernelMode, NULL);
    
                        MmResetPageFaultReadAhead (Thread, SavedState);
    
                        if (!NT_SUCCESS(status)) {
                            goto failure;
                        }
    
                        //
                        // Clear PointerAltPte to signify a restart is needed
                        // (because the PFN lock was released so the address
                        // space may have changed).
                        //

                        PointerAltPte = NULL;

                        LOCK_PFN2 (OldIrql);
                        
                        break;
                    } 
                    
                    PointerAltPte += 1;
                    Va = (PVOID)((ULONG_PTR)Va + PAGE_4K);
    
                } while (PointerAltPte <= LastPointerAltPte);
    
                if (PointerAltPte == NULL) {
                    continue;
                }
            }
#endif

        if (Operation != IoReadAccess) {

            if ((PteContents.u.Long & MM_PTE_WRITE_MASK) == 0) {

                if (PteContents.u.Long & MM_PTE_COPY_ON_WRITE_MASK) {

                    //
                    // The protection has changed from writable to copy on
                    // write.  This can happen if a fork is in progress for
                    // example.  Restart the operation at the top.
                    //

                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (Va <= MM_HIGHEST_USER_ADDRESS) {
                        UNLOCK_PFN2 (OldIrql);

                        MmSetPageFaultReadAhead (Thread, 0);

                        status = MmAccessFault (FALSE, Va, KernelMode, NULL);

                        MmResetPageFaultReadAhead (Thread, SavedState);

                        if (!NT_SUCCESS(status)) {
                            goto failure;
                        }

                        LOCK_PFN2 (OldIrql);

                        continue;
                    }
                }

                //
                // The caller has made the page protection more
                // restrictive, this should never be done once the
                // request has been issued !  Rather than wading
                // through the PFN database entry to see if it
                // could possibly work out, give the caller an
                // access violation.
                //

#if DBG
                DbgPrint ("MmProbeAndLockPages: PTE %p %p changed\n",
                    PointerPte,
                    PteContents.u.Long);

                if (MmStopOnBadProbe) {
                    DbgBreakPoint ();
                }
#endif

                UNLOCK_PFN2 (OldIrql);
                status = STATUS_ACCESS_VIOLATION;
                goto failure;
            }
        }

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

        if (PageFrameIndex <= MmHighestPhysicalPage) {

            ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            //
            // Check to make sure this page is not locked down an unusually
            // high number of times.
            //
    
            if (Pfn1->u3.e2.ReferenceCount >= MmReferenceCountCheck) {
                UNLOCK_PFN2 (OldIrql);
                ASSERT (FALSE);
                status = STATUS_WORKING_SET_QUOTA;
                goto failure;
            }

            //
            // Ensure the systemwide locked pages count is fluid.
            //
    
            if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= 0) {

                //
                // If this page is for paged pool or privileged code/data,
                // then force it in regardless.
                //

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if ((Va < MM_HIGHEST_USER_ADDRESS) ||
                    (MI_IS_SYSTEM_CACHE_ADDRESS(Va))) {

                    MI_INSTRUMENT_PROBE_RAISES(5);
                    UNLOCK_PFN2 (OldIrql);
                    status = STATUS_WORKING_SET_QUOTA;
                    goto failure;
                }

                MI_INSTRUMENT_PROBE_RAISES(8);
            }
    
            if (MemoryDescriptorList->MdlFlags & MDL_WRITE_OPERATION) {
                MI_SNAP_DIRTY (Pfn1, 1, 0x98);
            }

            MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 0);

            Pfn1->u3.e2.ReferenceCount += 1;
        }
        else {

            //
            // This is an I/O space address - don't allow operations
            // on addresses not in the PFN database.
            //

            MemoryDescriptorList->MdlFlags |= MDL_IO_SPACE;
        }

        *Page = PageFrameIndex;

        Page += 1;

        PointerPte += 1;
        if (MiIsPteOnPdeBoundary(PointerPte)) {
            PointerPde += 1;
            if (MiIsPteOnPpeBoundary(PointerPte)) {
                PointerPpe += 1;
                if (MiIsPteOnPxeBoundary(PointerPte)) {
                    PointerPxe += 1;
                }
            }
        }

    } while (PointerPte <= LastPte);

    UNLOCK_PFN2 (OldIrql);

    if ((MmTrackLockedPages == TRUE) && (AlignedVa <= MM_HIGHEST_USER_ADDRESS)) {
        ASSERT (NumberOfPagesSpanned != 0);

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        MiAddMdlTracker (MemoryDescriptorList,
                         CallingAddress,
                         CallersCaller,
                         NumberOfPagesSpanned,
                         1);
    }

    return;

failure:

    //
    // An exception occurred.  Unlock the pages locked so far.
    //

    if (MmTrackLockedPages == TRUE) {

        //
        // Adjust the MDL length so that MmUnlockPages only
        // processes the part that was completed.
        //

        ULONG PagesLocked;

        PagesLocked = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartVa,
                              MemoryDescriptorList->ByteCount);

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        MiAddMdlTracker (MemoryDescriptorList,
                         CallingAddress,
                         CallersCaller,
                         PagesLocked,
                         0);
    }

    MmUnlockPages (MemoryDescriptorList);

    //
    // Raise an exception of access violation to the caller.
    //

    MI_INSTRUMENT_PROBE_RAISES(7);
    ExRaiseStatus (status);
    return;
}

NTKERNELAPI
VOID
MmProbeAndLockProcessPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PEPROCESS Process,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )

/*++

Routine Description:

    This routine probes and locks the address range specified by
    the MemoryDescriptorList in the specified Process for the AccessMode
    and Operation.

Arguments:

    MemoryDescriptorList - Supplies a pre-initialized MDL that describes the
                           address range to be probed and locked.

    Process - Specifies the address of the process whose address range is
              to be locked.

    AccessMode - The mode for which the probe should check access to the range.

    Operation - Supplies the type of access which for which to check the range.

Return Value:

    None.

--*/

{
    KAPC_STATE ApcState;
    LOGICAL Attached;
    NTSTATUS Status;

    Attached = FALSE;
    Status = STATUS_SUCCESS;

    if (Process != PsGetCurrentProcess ()) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    try {

        MmProbeAndLockPages (MemoryDescriptorList,
                             AccessMode,
                             Operation);

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
    }

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (Status != STATUS_SUCCESS) {
        ExRaiseStatus (Status);
    }
    return;
}

VOID
MiAddMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller,
    IN PFN_NUMBER NumberOfPagesToLock,
    IN ULONG Who
    )

/*++

Routine Description:

    This routine adds an MDL to the specified process' chain.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length. The
                           physical page portion of the MDL is updated when
                           the pages are locked in memory.

    CallingAddress - Supplies the address of the caller of our caller.

    CallersCaller - Supplies the address of the caller of CallingAddress.

    NumberOfPagesToLock - Specifies the number of pages to lock.

    Who - Specifies which routine is adding the entry.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/

{
    KIRQL OldIrql;
    PEPROCESS Process;
    PLOCK_HEADER LockedPagesHeader;
    PLOCK_TRACKER Tracker;
    PLOCK_TRACKER P;
    PLIST_ENTRY NextEntry;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return;
    }

    LockedPagesHeader = Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return;
    }

    //
    // It's ok to check unsynchronized for aborted tracking as the worst case
    // is just that one more entry gets added which will be freed later anyway.
    // The main purpose behind aborted tracking is that frees and exits don't
    // mistakenly bugcheck when an entry cannot be found.
    //

    if (MiTrackingAborted == TRUE) {
        return;
    }

    Tracker = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof (LOCK_TRACKER),
                                     'kLmM');

    if (Tracker == NULL) {

        //
        // It's ok to set this without synchronization as the worst case
        // is just that a few more entries gets added which will be freed
        // later anyway.  The main purpose behind aborted tracking is that
        // frees and exits don't mistakenly bugcheck when an entry cannot
        // be found.
        //
    
        MiTrackingAborted = TRUE;

        return;
    }

    Tracker->Mdl = MemoryDescriptorList;
    Tracker->Count = NumberOfPagesToLock;
    Tracker->StartVa = MemoryDescriptorList->StartVa;
    Tracker->Offset = MemoryDescriptorList->ByteOffset;
    Tracker->Length = MemoryDescriptorList->ByteCount;
    Tracker->Page = *(PPFN_NUMBER)(MemoryDescriptorList + 1);

    Tracker->CallingAddress = CallingAddress;
    Tracker->CallersCaller = CallersCaller;

    Tracker->Who = Who;
    Tracker->Process = Process;

    ExAcquireSpinLock (&MiTrackLockedPagesLock, &OldIrql);

    //
    // Update the list for this process.  First make sure it's not already
    // inserted.
    //

    NextEntry = LockedPagesHeader->ListHead.Flink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        P = CONTAINING_RECORD (NextEntry,
                               LOCK_TRACKER,
                               ListEntry);

        if (P->Mdl == MemoryDescriptorList) {
            KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                          0x1,
                          (ULONG_PTR)P,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)MmLockedPagesHead.Count);
        }
        NextEntry = NextEntry->Flink;
    }

    InsertTailList (&LockedPagesHeader->ListHead, &Tracker->ListEntry);
    LockedPagesHeader->Count += NumberOfPagesToLock;

    //
    // Update the systemwide global list.  First make sure it's not
    // already inserted.
    //

    NextEntry = MmLockedPagesHead.ListHead.Flink;
    while (NextEntry != &MmLockedPagesHead.ListHead) {

        P = CONTAINING_RECORD(NextEntry,
                              LOCK_TRACKER,
                              GlobalListEntry);

        if (P->Mdl == MemoryDescriptorList) {
            KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                          0x2,
                          (ULONG_PTR)P,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)MmLockedPagesHead.Count);
        }

        NextEntry = NextEntry->Flink;
    }

    InsertTailList (&MmLockedPagesHead.ListHead, &Tracker->GlobalListEntry);

    MmLockedPagesHead.Count += NumberOfPagesToLock;

    ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);
}

LOGICAL
MiFreeMdlTracker (
    IN OUT PMDL MemoryDescriptorList,
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    This deletes an MDL from the specified process' chain.  Used specifically
    by MmProbeAndLockSelectedPages () because it builds an MDL in its local
    stack and then copies the requested pages into the real MDL.  this lets
    us track these pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length.

    NumberOfPages - Supplies the number of pages to be freed.

Return Value:

    TRUE.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/
{
    KIRQL OldIrql;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PPFN_NUMBER Page;
    PLOCK_TRACKER Found;
    PVOID PoolToFree;

    ASSERT (MemoryDescriptorList->Process != NULL);

    LockedPagesHeader = (PLOCK_HEADER)MemoryDescriptorList->Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return TRUE;
    }

    //
    // Initializing PoolToFree is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PoolToFree = NULL;

    Found = NULL;
    Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

    ExAcquireSpinLock (&MiTrackLockedPagesLock, &OldIrql);

    NextEntry = LockedPagesHeader->ListHead.Flink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {

            if (Found != NULL) {
                KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                              0x3,
                              (ULONG_PTR)Found,
                              (ULONG_PTR)Tracker,
                              (ULONG_PTR)MemoryDescriptorList);
            }

            ASSERT (Tracker->Page == *Page);
            ASSERT (NumberOfPages == Tracker->Count);
            Tracker->Count = (PFN_NUMBER)-1;
            RemoveEntryList (NextEntry);
            LockedPagesHeader->Count -= NumberOfPages;

            RemoveEntryList (&Tracker->GlobalListEntry);
            MmLockedPagesHead.Count -= NumberOfPages;

            Found = Tracker;
            PoolToFree = (PVOID)NextEntry;
        }
        NextEntry = Tracker->ListEntry.Flink;
    }

    ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);

    if (Found == NULL) {

        //
        // A driver is trying to unlock pages that aren't locked.
        //

        if (MiTrackingAborted == TRUE) {
            return TRUE;
        }

        KeBugCheckEx (PROCESS_HAS_LOCKED_PAGES,
                      1,
                      (ULONG_PTR)MemoryDescriptorList,
                      MemoryDescriptorList->Process->NumberOfLockedPages,
                      (ULONG_PTR)MemoryDescriptorList->Process->LockedPagesList);
    }

    ExFreePool (PoolToFree);

    return TRUE;
}


LOGICAL
MmUpdateMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller
    )

/*++

Routine Description:

    This updates an MDL in the specified process' chain.  Used by the I/O
    system so that proper driver identification can be done even when I/O
    is actually locking the pages on their behalf.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List.

    CallingAddress - Supplies the address of the caller of our caller.

    CallersCaller - Supplies the address of the caller of CallingAddress.

Return Value:

    TRUE if the MDL was found, FALSE if not.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/
{
    KIRQL OldIrql;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PEPROCESS Process;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return FALSE;
    }

    LockedPagesHeader = (PLOCK_HEADER) Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return FALSE;
    }

    ExAcquireSpinLock (&MiTrackLockedPagesLock, &OldIrql);

    //
    // Walk the list backwards as it's likely the MDL was
    // just recently inserted.
    //

    NextEntry = LockedPagesHeader->ListHead.Blink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {
            ASSERT (Tracker->Page == *(PPFN_NUMBER) (MemoryDescriptorList + 1));
            Tracker->CallingAddress = CallingAddress;
            Tracker->CallersCaller = CallersCaller;
            ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);
            return TRUE;
        }
        NextEntry = Tracker->ListEntry.Blink;
    }

    ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);

    //
    // The caller is trying to update an MDL that is no longer locked.
    //

    return FALSE;
}


LOGICAL
MiUpdateMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN ULONG AdvancePages
    )

/*++

Routine Description:

    This updates an MDL in the specified process' chain.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List.

    AdvancePages - Supplies the number of pages being advanced.

Return Value:

    TRUE if the MDL was found, FALSE if not.

Environment:

    Kernel mode.  DISPATCH_LEVEL and below.

--*/
{
    PPFN_NUMBER Page;
    KIRQL OldIrql;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PEPROCESS Process;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return FALSE;
    }

    LockedPagesHeader = (PLOCK_HEADER) Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return FALSE;
    }

    ExAcquireSpinLock (&MiTrackLockedPagesLock, &OldIrql);

    //
    // Walk the list backwards as it's likely the MDL was
    // just recently inserted.
    //

    NextEntry = LockedPagesHeader->ListHead.Blink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {

            Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

            ASSERT (Tracker->Page == *Page);
            ASSERT (Tracker->Count > AdvancePages);

            Tracker->Page = *(Page + AdvancePages);
            Tracker->Count -= AdvancePages;

            MmLockedPagesHead.Count -= AdvancePages;

            ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);
            return TRUE;
        }
        NextEntry = Tracker->ListEntry.Blink;
    }

    ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);

    //
    // The caller is trying to update an MDL that is no longer locked.
    //

    return FALSE;
}


NTKERNELAPI
VOID
MmProbeAndLockSelectedPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PFILE_SEGMENT_ELEMENT SegmentArray,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )

/*++

Routine Description:

    This routine probes the specified pages, makes the pages resident and
    locks the physical pages mapped by the virtual pages in memory.  The
    Memory descriptor list is updated to describe the physical pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length. The
                           physical page portion of the MDL is updated when
                           the pages are locked in memory.

    SegmentArray - Supplies a pointer to a list of buffer segments to be
                   probed and locked.

    AccessMode - Supplies the access mode in which to probe the arguments.
                 One of KernelMode or UserMode.

    Operation - Supplies the operation type.  One of IoReadAccess, IoWriteAccess
                or IoModifyAccess.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/

{
    PMDL TempMdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PPFN_NUMBER Page;
    PFILE_SEGMENT_ELEMENT LastSegment;
    PVOID CallingAddress;
    PVOID CallersCaller;
    ULONG NumberOfPagesToLock;

    PAGED_CODE();

    NumberOfPagesToLock = 0;

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT (((ULONG_PTR)MemoryDescriptorList->ByteOffset & ~(PAGE_SIZE - 1)) == 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL |
                    MDL_IO_SPACE)) == 0);

    //
    // Initialize TempMdl.
    //

    TempMdl = (PMDL) &MdlHack;

    MmInitializeMdl( TempMdl, SegmentArray->Buffer, PAGE_SIZE );

    Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

    //
    // Calculate the end of the segment list.
    //

    LastSegment = SegmentArray +
                  BYTES_TO_PAGES(MemoryDescriptorList->ByteCount);

    ASSERT(SegmentArray < LastSegment);

    //
    // Build a small Mdl for each segment and call probe and lock pages.
    // Then copy the PFNs to the real mdl.  The first page is processed
    // outside of the try/finally to ensure that the flags and process
    // field are correctly set in case MmUnlockPages needs to be called.
    //

    //
    // Even systems without 64 bit pointers are required to zero the
    // upper 32 bits of the segment address so use alignment rather
    // than the buffer pointer.
    //

    SegmentArray += 1;
    MmProbeAndLockPages( TempMdl, AccessMode, Operation );

    if (MmTrackLockedPages == TRUE) {

        //
        // Since we move the page from the temp MDL to the real one below
        // and never free the temp one, fixup our accounting now.
        //

        if (MiFreeMdlTracker (TempMdl, 1) == TRUE) {
            NumberOfPagesToLock += 1;
        }
    }

    *Page++ = *((PPFN_NUMBER) (TempMdl + 1));

    //
    // Copy the flags and process fields.
    //

    MemoryDescriptorList->MdlFlags |= TempMdl->MdlFlags;
    MemoryDescriptorList->Process = TempMdl->Process;

    try {

        while (SegmentArray < LastSegment) {

            //
            // Even systems without 64 bit pointers are required to zero the
            // upper 32 bits of the segment address so use alignment rather
            // than the buffer pointer.
            //

            TempMdl->StartVa = (PVOID)(ULONG_PTR)SegmentArray->Buffer;
            TempMdl->MdlFlags = 0;

            SegmentArray += 1;
            MmProbeAndLockPages( TempMdl, AccessMode, Operation );


            if (MmTrackLockedPages == TRUE) {

                //
                // Since we move the page from the temp MDL to the real one
                // below and never free the temp one, fixup our accounting now.
                //

                if (MiFreeMdlTracker (TempMdl, 1) == TRUE) {
                    NumberOfPagesToLock += 1;
                }
            }

            *Page++ = *((PPFN_NUMBER) (TempMdl + 1));
        }
    } finally {

        if (abnormal_termination()) {

            //
            // Adjust the MDL length so that MmUnlockPages only processes
            // the part that was completed.
            //

            MemoryDescriptorList->ByteCount =
                (ULONG) (Page - (PPFN_NUMBER) (MemoryDescriptorList + 1)) << PAGE_SHIFT;

            if (MmTrackLockedPages == TRUE) {

                RtlGetCallersAddress (&CallingAddress, &CallersCaller);

                MiAddMdlTracker (MemoryDescriptorList,
                                 CallingAddress,
                                 CallersCaller,
                                 NumberOfPagesToLock,
                                 2);
            }

            MmUnlockPages (MemoryDescriptorList);
        }
        else if (MmTrackLockedPages == TRUE) {

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            MiAddMdlTracker (MemoryDescriptorList,
                             CallingAddress,
                             CallersCaller,
                             NumberOfPagesToLock,
                             3);
        }
    }
}

VOID
MmUnlockPages (
     IN OUT PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unlocks physical pages which are described by a Memory
    Descriptor List.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a memory descriptor list
                           (MDL). The supplied MDL must have been supplied
                           to MmLockPages to lock the pages down.  As the
                           pages are unlocked, the MDL is updated.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PVOID OldValue;
    PEPROCESS Process;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PVOID StartingVa;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PSINGLE_LIST_ENTRY SingleListEntry;
    PMI_PFN_DEREFERENCE_CHUNK DerefMdl;
    PSLIST_HEADER PfnDereferenceSListHead;
    PSINGLE_LIST_ENTRY *PfnDeferredList;

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PAGES_LOCKED) != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) == 0);
    ASSERT (MemoryDescriptorList->ByteCount != 0);

    Process = MemoryDescriptorList->Process;

    //
    // Carefully snap a copy of the MDL flags - realize that bits in it may
    // change due to some of the subroutines called below.  Only bits that
    // we know can't change are examined in this local copy.  This is done
    // to reduce the amount of processing while the PFN lock is held.
    //

    MdlFlags = MemoryDescriptorList->MdlFlags;

    if (MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

        //
        // This MDL has been mapped into system space, unmap now.
        //

        MmUnmapLockedPages (MemoryDescriptorList->MappedSystemVa,
                            MemoryDescriptorList);
    }

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    if (MmTrackLockedPages == TRUE) {
        if ((Process != NULL) &&
            ((MdlFlags & MDL_IO_SPACE) == 0)) {
                MiFreeMdlTracker (MemoryDescriptorList, NumberOfPages);
        }

        if (MmLockedPagesHead.ListHead.Flink != 0) {
    
            PLOCK_TRACKER P;
            PLIST_ENTRY NextEntry;
    
            ExAcquireSpinLock (&MiTrackLockedPagesLock, &OldIrql);

            NextEntry = MmLockedPagesHead.ListHead.Flink;
            while (NextEntry != &MmLockedPagesHead.ListHead) {
    
                P = CONTAINING_RECORD(NextEntry,
                                      LOCK_TRACKER,
                                      GlobalListEntry);
    
                if (P->Mdl == MemoryDescriptorList) {
                    KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                                  0x4,
                                  (ULONG_PTR)P,
                                  (ULONG_PTR)MemoryDescriptorList,
                                  0);
                }
    
                NextEntry = NextEntry->Flink;
            }
            ExReleaseSpinLock (&MiTrackLockedPagesLock, OldIrql);
        }
    }

    //
    // Only unlock if not I/O space.
    //

    if ((MdlFlags & MDL_IO_SPACE) == 0) {

        if (Process != NULL) {
            ASSERT ((SPFN_NUMBER)Process->NumberOfLockedPages >= 0);
            InterlockedExchangeAddSizeT (&Process->NumberOfLockedPages,
                                         0 - NumberOfPages);
        }

        LastPage = Page + NumberOfPages;

        //
        // Calculate PFN addresses and termination without the PFN lock
        // (it's not needed for this) to reduce PFN lock contention.
        //

        ASSERT (sizeof(PFN_NUMBER) == sizeof(PMMPFN));

        do {

            if (*Page == MM_EMPTY_LIST) {

                //
                // There are no more locked pages - if there were none at all
                // then we're done.
                //

                if (Page == (PPFN_NUMBER)(MemoryDescriptorList + 1)) {
                    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                    return;
                }

                LastPage = Page;
                break;
            }
            ASSERT (*Page <= MmHighestPhysicalPage);

            Pfn1 = MI_PFN_ELEMENT (*Page);
            *Page = (PFN_NUMBER) Pfn1;
            Page += 1;
        } while (Page < LastPage);

        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        //
        // If the MDL can be queued so the PFN acquisition/release can be
        // amortized then do so.
        //

        if (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK) {

#if defined(MI_MULTINODE)

            PKNODE Node = KeGetCurrentNode ();

            //
            // The node may change beneath us but that should be fairly
            // infrequent and not worth checking for.  Just make sure the
            // same node that gives us a free entry gets the deferred entry
            // back.
            //

            PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
#else
            PfnDereferenceSListHead = &MmPfnDereferenceSListHead;
#endif

            //
            // Pop an entry from the freelist.
            //

            SingleListEntry = InterlockedPopEntrySList (PfnDereferenceSListHead);

            if (SingleListEntry != NULL) {
                DerefMdl = CONTAINING_RECORD (SingleListEntry,
                                              MI_PFN_DEREFERENCE_CHUNK,
                                              ListEntry);

                DerefMdl->Flags = MdlFlags;
                DerefMdl->NumberOfPages = (USHORT) (LastPage - Page);

                RtlCopyMemory ((PVOID)(&DerefMdl->Pfns[0]),
                               (PVOID)Page,
                               (LastPage - Page) * sizeof (PFN_NUMBER));

                MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;

                //
                // Push this entry on the deferred list.
                //

#if defined(MI_MULTINODE)
                PfnDeferredList = &Node->PfnDeferredList;
#else
                PfnDeferredList = &MmPfnDeferredList;
#endif

                do {

                    OldValue = *PfnDeferredList;
                    SingleListEntry->Next = OldValue;

                } while (InterlockedCompareExchangePointer (
                                PfnDeferredList,
                                SingleListEntry,
                                OldValue) != OldValue);
                return;
            }
        }

        SingleListEntry = NULL;

        if (MdlFlags & MDL_WRITE_OPERATION) {

            LOCK_PFN2 (OldIrql);

            do {

                //
                // If this was a write operation set the modified bit in the
                // PFN database.
                //

                Pfn1 = (PMMPFN) (*Page);

                MI_SET_MODIFIED (Pfn1, 1, 0x3);

                if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                             (Pfn1->u3.e1.WriteInProgress == 0)) {

                    ULONG FreeBit;
                    FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                    if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                        MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                        Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                    }
                }

                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                Page += 1;
            } while (Page < LastPage);
        }
        else {

            LOCK_PFN2 (OldIrql);

            do {

                Pfn1 = (PMMPFN) (*Page);

                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                Page += 1;
            } while (Page < LastPage);
        }

        if (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK) {

            //
            // The only reason this code path is being reached is because
            // a deferred entry was not available so clear the list now.
            //

            MiDeferredUnlockPages (MI_DEFER_PFN_HELD | MI_DEFER_DRAIN_LOCAL_ONLY);
        }

        UNLOCK_PFN2 (OldIrql);
    }

    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;

    return;
}


VOID
MiDeferredUnlockPages (
     ULONG Flags
     )

/*++

Routine Description:

    This routine unlocks physical pages which were previously described by
    a Memory Descriptor List.

Arguments:

    Flags - Supplies a bitfield of the caller's needs :

        MI_DEFER_PFN_HELD - Indicates the caller holds the PFN lock on entry.

        MI_DEFER_DRAIN_LOCAL_ONLY - Indicates the caller only wishes to drain
                                    the current processor's queue.  This only
                                    has meaning in NUMA systems.

Return Value:

    None.

Environment:

    Kernel mode, PFN database lock *MAY* be held on entry (see Flags).

--*/

{
    KIRQL OldIrql = 0;
    ULONG FreeBit;
    ULONG i;
    ULONG ListCount;
    ULONG TotalNodes;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PSINGLE_LIST_ENTRY SingleListEntry;
    PSINGLE_LIST_ENTRY LastEntry;
    PSINGLE_LIST_ENTRY FirstEntry;
    PSINGLE_LIST_ENTRY NextEntry;
    PSINGLE_LIST_ENTRY VeryLastEntry;
    PMI_PFN_DEREFERENCE_CHUNK DerefMdl;
    PSLIST_HEADER PfnDereferenceSListHead;
    PSINGLE_LIST_ENTRY *PfnDeferredList;
#if defined(MI_MULTINODE)
    PKNODE Node;
#endif

    i = 0;
    ListCount = 0;
    TotalNodes = 1;

    if ((Flags & MI_DEFER_PFN_HELD) == 0) {
        LOCK_PFN2 (OldIrql);
    }

    MM_PFN_LOCK_ASSERT();

#if defined(MI_MULTINODE)
    if (Flags & MI_DEFER_DRAIN_LOCAL_ONLY) {
        Node = KeGetCurrentNode();
        PfnDeferredList = &Node->PfnDeferredList;
        PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
    }
    else {
        TotalNodes = KeNumberNodes;
        Node = KeNodeBlock[0];
        PfnDeferredList = &Node->PfnDeferredList;
        PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
    }
#else
    PfnDeferredList = &MmPfnDeferredList;
    PfnDereferenceSListHead = &MmPfnDereferenceSListHead;
#endif

    do {

        if (*PfnDeferredList == NULL) {

#if !defined(MI_MULTINODE)
            if ((Flags & MI_DEFER_PFN_HELD) == 0) {
                UNLOCK_PFN2 (OldIrql);
            }
            return;
#else
            i += 1;
            if (i < TotalNodes) {
                Node = KeNodeBlock[i];
                PfnDeferredList = &Node->PfnDeferredList;
                PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
                continue;
            }
            break;
#endif
        }

        //
        // Process each deferred unlock entry until they're all done.
        //

        LastEntry = NULL;
        VeryLastEntry = NULL;

        do {

            SingleListEntry = *PfnDeferredList;

            FirstEntry = SingleListEntry;

            do {

                NextEntry = SingleListEntry->Next;

                //
                // Process the deferred entry.
                //

                DerefMdl = CONTAINING_RECORD (SingleListEntry,
                                              MI_PFN_DEREFERENCE_CHUNK,
                                              ListEntry);

                MdlFlags = DerefMdl->Flags;
                NumberOfPages = (PFN_NUMBER) DerefMdl->NumberOfPages;
                ASSERT (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK);
                Page = &DerefMdl->Pfns[0];
                LastPage = Page + NumberOfPages;

                if (MdlFlags & MDL_WRITE_OPERATION) {

                    do {

                        //
                        // If this was a write operation set the modified bit
                        // in the PFN database.
                        //

                        Pfn1 = (PMMPFN) (*Page);

                        MI_SET_MODIFIED (Pfn1, 1, 0x4);

                        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                            (Pfn1->u3.e1.WriteInProgress == 0)) {

                            FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                            if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                                MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                            }
                        }

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                        Page += 1;
                    } while (Page < LastPage);
                }
                else {

                    do {

                        Pfn1 = (PMMPFN) (*Page);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                        Page += 1;
                    } while (Page < LastPage);
                }

                ListCount += 1;

                //
                // March on to the next entry if there is one.
                //

                if (NextEntry == LastEntry) {
                    break;
                }

                SingleListEntry = NextEntry;

            } while (TRUE);

            if (VeryLastEntry == NULL) {
                VeryLastEntry = SingleListEntry;
            }

            if ((*PfnDeferredList == FirstEntry) &&
                (InterlockedCompareExchangePointer (PfnDeferredList,
                                                    NULL,
                                                    FirstEntry) == FirstEntry)) {
                break;
            }
            LastEntry = FirstEntry;

        } while (TRUE);

        //
        // Push the processed list chain on the freelist.
        //

        ASSERT (ListCount != 0);
        ASSERT (FirstEntry != NULL);
        ASSERT (VeryLastEntry != NULL);

#if defined(MI_MULTINODE)
        InterlockedPushListSList (PfnDereferenceSListHead,
                                  FirstEntry,
                                  VeryLastEntry,
                                  ListCount);

        i += 1;
        if (i < TotalNodes) {
            Node = KeNodeBlock[i];
            PfnDeferredList = &Node->PfnDeferredList;
            PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
            ListCount = 0;
        }
        else {
            break;
        }
    } while (TRUE);
#else
    } while (FALSE);
#endif

    if ((Flags & MI_DEFER_PFN_HELD) == 0) {
        UNLOCK_PFN2 (OldIrql);
    }

#if !defined(MI_MULTINODE)

    //
    // If possible, push the processed chain after releasing the PFN lock.
    //

    InterlockedPushListSList (PfnDereferenceSListHead,
                              FirstEntry,
                              VeryLastEntry,
                              ListCount);
#endif
}

VOID
MmBuildMdlForNonPagedPool (
    IN OUT PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine fills in the "pages" portion of the MDL using the PFN
    numbers corresponding to the buffers which resides in non-paged pool.

    Unlike MmProbeAndLockPages, there is no corresponding unlock as no
    reference counts are incremented as the buffers being in nonpaged
    pool are always resident.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                            (MDL). The supplied MDL must supply a virtual
                            address, byte offset and length field.  The
                            physical page portion of the MDL is updated when
                            the pages are locked in memory.  The virtual
                            address must be within the non-paged portion
                            of the system space.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PPFN_NUMBER Page;
    PPFN_NUMBER EndPage;
    PMMPTE PointerPte;
    PVOID VirtualAddress;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL)) == 0);

    MemoryDescriptorList->Process = NULL;

    //
    // Endva is last byte of the buffer.
    //

    MemoryDescriptorList->MdlFlags |= MDL_SOURCE_IS_NONPAGED_POOL;

    ASSERT (MmIsNonPagedSystemAddressValid (MemoryDescriptorList->StartVa));

    VirtualAddress = MemoryDescriptorList->StartVa;

    MemoryDescriptorList->MappedSystemVa =
            (PVOID)((PCHAR)VirtualAddress + MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MemoryDescriptorList->MappedSystemVa,
                                           MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    EndPage = Page + NumberOfPages;

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {

        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (VirtualAddress);

        do {
            *Page = PageFrameIndex;
            Page += 1;
            PageFrameIndex += 1;
        } while (Page < EndPage);
    }
    else {

        PointerPte = MiGetPteAddress (VirtualAddress);

        do {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            *Page = PageFrameIndex;
            Page += 1;
            PointerPte += 1;
        } while (Page < EndPage);
    }

    //
    // Assume either all the frames are in the PFN database (ie: the MDL maps
    // pool) or none of them (the MDL maps dualport RAM) are.  Avoid picking
    // up a spinlock for the determination as this is a hotpath and assume
    // that dualport RAM spaces will be after physical memory - this may need
    // revisiting for sparse physical spaces.
    //

    if (PageFrameIndex > MmHighestPhysicalPage) {
        MemoryDescriptorList->MdlFlags |= MDL_IO_SPACE;
    }

    return;
}

VOID
MiInitializeIoTrackers (
    VOID
    )
{
    if (MmTrackPtes != 0) {
        InitializeSListHead (&MiDeadPteTrackerSListHead);
        KeInitializeSpinLock (&MiPteTrackerLock);
        InitializeListHead (&MiPteHeader.ListHead);
    }

    if (MmTrackLockedPages == TRUE) {
        KeInitializeSpinLock (&MiTrackLockedPagesLock);
        InitializeListHead (&MmLockedPagesHead.ListHead);
    }
}

VOID
MiInsertPteTracker (
     IN PPTE_TRACKER Tracker,
     IN PMDL MemoryDescriptorList,
     IN PFN_NUMBER NumberOfPtes,
     IN PVOID MyCaller,
     IN PVOID MyCallersCaller
     )
/*++

Routine Description:

    This function inserts a PTE tracking block as the caller has just
    consumed system PTEs.

Arguments:

    Tracker - Supplies a tracker pool block.  This is supplied by the caller
              since the MmSystemSpaceLock is held on entry hence pool
              allocations may not be done here.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List.

    NumberOfPtes - Supplies the number of system PTEs allocated.

    MyCaller - Supplies the return address of the caller who consumed the
               system PTEs to map this MDL.

    MyCallersCaller - Supplies the return address of the caller of the caller
                      who consumed the system PTEs to map this MDL.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;

    Tracker->Mdl = MemoryDescriptorList;
    Tracker->SystemVa = MemoryDescriptorList->MappedSystemVa;
    Tracker->Count = NumberOfPtes;

    Tracker->StartVa = MemoryDescriptorList->StartVa;
    Tracker->Offset = MemoryDescriptorList->ByteOffset;
    Tracker->Length = MemoryDescriptorList->ByteCount;
    Tracker->Page = *(PPFN_NUMBER)(MemoryDescriptorList + 1);

    Tracker->CallingAddress = MyCaller;
    Tracker->CallersCaller = MyCallersCaller;
    Tracker->PteAddress = MiGetPteAddress (Tracker->SystemVa);

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    MiPteHeader.Count += NumberOfPtes;

    InsertHeadList (&MiPteHeader.ListHead, &Tracker->ListEntry);

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}

VOID
MiRemovePteTracker (
     IN PMDL MemoryDescriptorList OPTIONAL,
     IN PVOID PteAddress,
     IN PFN_NUMBER NumberOfPtes
     )

/*++

Routine Description:

    This function removes a PTE tracking block from the lists as the PTEs
    are being freed.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List.

    PteAddress - Supplies the address the system PTEs were mapped to.

    NumberOfPtes - Supplies the number of system PTEs allocated.

Return Value:

    The pool block that held the tracking info that must be freed by our
    caller _AFTER_ our caller releases MmSystemSpaceLock (to prevent deadlock).

Environment:

    Kernel mode, DISPATCH_LEVEL or below. Locks (including the PFN) may be held.

--*/

{
    KIRQL OldIrql;
    PPTE_TRACKER Tracker;
    PFN_NUMBER Page;
    PVOID BaseAddress;
    PLIST_ENTRY LastFound;
    PLIST_ENTRY NextEntry;

    //
    // Initializing Page is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Page = 0;

    BaseAddress = MiGetVirtualAddressMappedByPte (PteAddress);

    if (ARGUMENT_PRESENT (MemoryDescriptorList)) {
        Page = *(PPFN_NUMBER)(MemoryDescriptorList + 1);
    }

    LastFound = NULL;

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    NextEntry = MiPteHeader.ListHead.Flink;
    while (NextEntry != &MiPteHeader.ListHead) {

        Tracker = (PPTE_TRACKER) CONTAINING_RECORD (NextEntry,
                                                    PTE_TRACKER,
                                                    ListEntry.Flink);

        if (PteAddress == Tracker->PteAddress) {

            if (LastFound != NULL) {

                //
                // Duplicate map entry.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x1,
                              (ULONG_PTR)Tracker,
                              (ULONG_PTR)MemoryDescriptorList,
                              (ULONG_PTR)LastFound);
            }

            if (Tracker->Count != NumberOfPtes) {

                //
                // Not unmapping the same of number of PTEs that were mapped.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x2,
                              (ULONG_PTR)Tracker,
                              Tracker->Count,
                              NumberOfPtes);
            }

            if ((ARGUMENT_PRESENT (MemoryDescriptorList)) &&
                ((MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) == 0) &&
                (MiMdlsAdjusted == FALSE)) {

                if (Tracker->SystemVa != MemoryDescriptorList->MappedSystemVa) {

                    //
                    // Not unmapping the same address that was mapped.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x3,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->SystemVa,
                                  (ULONG_PTR)MemoryDescriptorList->MappedSystemVa);
                }

                if (Tracker->Page != Page) {

                    //
                    // The first page in the MDL has changed since it was mapped.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x4,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->Page,
                                  (ULONG_PTR)Page);
                }

                if (Tracker->StartVa != MemoryDescriptorList->StartVa) {

                    //
                    // Map and unmap don't match up.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x5,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->StartVa,
                                  (ULONG_PTR)MemoryDescriptorList->StartVa);
                }
            }

            RemoveEntryList (NextEntry);
            LastFound = NextEntry;
        }
        NextEntry = Tracker->ListEntry.Flink;
    }

    if ((LastFound == NULL) && (MiTrackPtesAborted == FALSE)) {

        //
        // Can't unmap something that was never (or isn't currently) mapped.
        //

        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x6,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)BaseAddress,
                      (ULONG_PTR)NumberOfPtes);
    }

    MiPteHeader.Count -= NumberOfPtes;

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);

    //
    // Insert the tracking block into the dead PTE list for later
    // release.  Locks (including the PFN lock) may be held on entry, thus the
    // block cannot be directly freed to pool at this time.
    //

    if (LastFound != NULL) {
        InterlockedPushEntrySList (&MiDeadPteTrackerSListHead,
                                   (PSINGLE_LIST_ENTRY)LastFound);
    }

    return;
}

PPTE_TRACKER
MiReleaseDeadPteTrackers (
    VOID
    )
/*++

Routine Description:

    This routine removes tracking blocks from the dead PTE list and frees
    them to pool.  One entry is returned (if possible) to the caller to use
    for the next allocation.

Arguments:

    None.

Return Value:

    A PTE tracking block or NULL.

Environment:

    Kernel mode.  No locks held.

--*/
{
    LOGICAL ListToProcess;
    PPTE_TRACKER Tracker;
    PSINGLE_LIST_ENTRY SingleListEntry;
    PSINGLE_LIST_ENTRY NextSingleListEntry;

    ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL);

    if (ExQueryDepthSList (&MiDeadPteTrackerSListHead) < 10) {
        SingleListEntry = InterlockedPopEntrySList (&MiDeadPteTrackerSListHead);
        ListToProcess = FALSE;
    }
    else {
        SingleListEntry = ExInterlockedFlushSList (&MiDeadPteTrackerSListHead);
        ListToProcess = TRUE;
    }

    if (SingleListEntry == NULL) {

        Tracker = ExAllocatePoolWithTag (NonPagedPool,
                                         sizeof (PTE_TRACKER),
                                         'ySmM');

        if (Tracker == NULL) {
            MiTrackPtesAborted = TRUE;
        }

        return Tracker;
    }

    Tracker = (PPTE_TRACKER) SingleListEntry;

    if (ListToProcess == TRUE) {

        SingleListEntry = SingleListEntry->Next;

        while (SingleListEntry != NULL) {

            NextSingleListEntry = SingleListEntry->Next;

            ExFreePool (SingleListEntry);

            SingleListEntry = NextSingleListEntry;
        }
    }

    return Tracker;
}

PVOID
MiGetHighestPteConsumer (
    OUT PULONG_PTR NumberOfPtes
    )

/*++

Routine Description:

    This function examines the PTE tracking blocks and returns the biggest
    consumer.

Arguments:

    None.

Return Value:

    The loaded module entry of the biggest consumer.

Environment:

    Kernel mode, called during bugcheck only.  Many locks may be held.

--*/

{
    PPTE_TRACKER Tracker;
    PVOID BaseAddress;
    PFN_NUMBER NumberOfPages;
    PLIST_ENTRY NextEntry;
    PLIST_ENTRY NextEntry2;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG_PTR Highest;
    ULONG_PTR PagesByThisModule;
    PKLDR_DATA_TABLE_ENTRY HighDataTableEntry;

    *NumberOfPtes = 0;

    //
    // No locks are acquired as this is only called during a bugcheck.
    //

    if ((MmTrackPtes & 0x1) == 0) {
        return NULL;
    }

    if (MiTrackPtesAborted == TRUE) {
        return NULL;
    }

    if (IsListEmpty(&MiPteHeader.ListHead)) {
        return NULL;
    }

    if (PsLoadedModuleList.Flink == NULL) {
        return NULL;
    }

    Highest = 0;
    HighDataTableEntry = NULL;

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        PagesByThisModule = 0;

        //
        // Walk the PTE mapping list and update each driver's counts.
        //
    
        NextEntry2 = MiPteHeader.ListHead.Flink;
        while (NextEntry2 != &MiPteHeader.ListHead) {
    
            Tracker = (PPTE_TRACKER) CONTAINING_RECORD (NextEntry2,
                                                        PTE_TRACKER,
                                                        ListEntry.Flink);
    
            BaseAddress = Tracker->CallingAddress;
            NumberOfPages = Tracker->Count;
    
            if ((BaseAddress >= DataTableEntry->DllBase) &&
                (BaseAddress < (PVOID)((ULONG_PTR)(DataTableEntry->DllBase) + DataTableEntry->SizeOfImage))) {

                PagesByThisModule += NumberOfPages;
            }
        
            NextEntry2 = NextEntry2->Flink;
    
        }
    
        if (PagesByThisModule > Highest) {
            Highest = PagesByThisModule;
            HighDataTableEntry = DataTableEntry;
        }

        NextEntry = NextEntry->Flink;
    }

    *NumberOfPtes = Highest;

    return (PVOID)HighDataTableEntry;
}

PVOID
MmMapLockedPagesSpecifyCache (
     IN PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID RequestedAddress,
     IN ULONG BugCheckOnFailure,
     IN MM_PAGE_PRIORITY Priority
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space or the user portion of
    the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    AccessMode - Supplies an indicator of where to map the pages;
                 KernelMode indicates that the pages should be mapped in the
                 system part of the address space, UserMode indicates the
                 pages should be mapped in the user part of the address space.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

    RequestedAddress - Supplies the base user address of the view.

                       This is only treated as an address if the AccessMode
                       is UserMode.  If the initial value of this argument
                       is not NULL, then the view will be allocated starting
                       at the specified virtual address rounded down to the
                       next 64kb address boundary. If the initial value of
                       this argument is NULL, then the operating system
                       will determine where to allocate the view.

                       If the AccessMode is KernelMode, then this argument is
                       treated as a bit field of attributes.

    BugCheckOnFailure - Supplies whether to bugcheck if the mapping cannot be
                        obtained.  This flag is only checked if the MDL's
                        MDL_MAPPING_CAN_FAIL is zero, which implies that the
                        default MDL behavior is to bugcheck.  This flag then
                        provides an additional avenue to avoid the bugcheck.
                        Done this way in order to provide WDM compatibility.

    Priority - Supplies an indication as to how important it is that this
               request succeed under low available PTE conditions.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if the processor mode is USER_MODE
    and quota limits or VM limits are exceeded.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if access mode is KernelMode,
                  APC_LEVEL or below if access mode is UserMode.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER SavedPageCount;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PMMPTE PointerPte;
    PVOID BaseVa;
    MMPTE TempPte;
    PVOID StartingVa;
    PFN_NUMBER NumberOfPtes;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PVOID Tracker;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn2;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    //
    // If this assert fires, the MiPlatformCacheAttributes array
    // initialization needs to be checked.
    //

    ASSERT (MmMaximumCacheType == 6);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);

    if (AccessMode == KernelMode) {

        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               MemoryDescriptorList->ByteCount);
        SavedPageCount = NumberOfPages;

        //
        // Map the pages into the system part of the address space as
        // kernel read/write.
        //

        ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_MAPPED_TO_SYSTEM_VA |
                        MDL_SOURCE_IS_NONPAGED_POOL |
                        MDL_PARTIAL_HAS_BEEN_MAPPED)) == 0);

        ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_PAGES_LOCKED |
                        MDL_PARTIAL)) != 0);

        //
        // Make sure there are enough PTEs of the requested size.
        // Try to ensure available PTEs inline when we're rich.
        // Otherwise go the long way.
        //

        if ((Priority != HighPagePriority) &&
            ((LONG)(NumberOfPages) > (LONG)MmTotalFreeSystemPtes[SystemPteSpace] - 2048) &&
            (MiGetSystemPteAvailability ((ULONG)NumberOfPages, Priority) == FALSE)) {
            return NULL;
        }

        IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

        CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

        //
        // If a noncachable mapping is requested, none of the pages in the
        // requested MDL can reside in a large page.  Otherwise we would be
        // creating an incoherent overlapping TB entry as the same physical
        // page would be mapped by 2 different TB entries with different
        // cache attributes.
        //

        if (CacheAttribute != MiCached) {

            LastPage = Page + NumberOfPages;
            do {

                if (*Page == MM_EMPTY_LIST) {
                    break;
                }

                PageFrameIndex = *Page;

                if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {

                    MiNonCachedCollisions += 1;

                    if (((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) == 0) && (BugCheckOnFailure)) {

                        KeBugCheckEx (MEMORY_MANAGEMENT,
                                      0x1000,
                                      (ULONG_PTR)MemoryDescriptorList,
                                      (ULONG_PTR)PageFrameIndex,
                                      (ULONG_PTR)CacheAttribute);
                    }
                    return NULL;
                }

                Page += 1;
            } while (Page < LastPage);

            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
        }

        PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages,
                                          SystemPteSpace);

        if (PointerPte == NULL) {

            if (((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) == 0) &&
                (BugCheckOnFailure)) {

                MiIssueNoPtesBugcheck ((ULONG)NumberOfPages, SystemPteSpace);
            }

            //
            // Not enough system PTES are available.
            //

            return NULL;
        }
        BaseVa = (PVOID)((PCHAR)MiGetVirtualAddressMappedByPte (PointerPte) +
                                MemoryDescriptorList->ByteOffset);

        NumberOfPtes = NumberOfPages;

        TempPte = ValidKernelPte;

        switch (CacheAttribute) {

            case MiNonCached:
                MI_DISABLE_CACHING (TempPte);
                break;

            case MiCached:
                break;

            case MiWriteCombined:
                MI_SET_PTE_WRITE_COMBINE (TempPte);
                break;

            default:
                ASSERT (FALSE);
                break;
        }

        OldIrql = HIGH_LEVEL;

        LastPage = Page + NumberOfPages;

        MI_PREPARE_FOR_NONCACHED (CacheAttribute);

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }
            ASSERT (PointerPte->u.Hard.Valid == 0);

            if (IoMapping == 0) {

                Pfn2 = MI_PFN_ELEMENT (*Page);
                ASSERT (Pfn2->u3.e2.ReferenceCount != 0);
                TempPte = ValidKernelPte;

                switch (Pfn2->u3.e1.CacheAttribute) {

                    case MiCached:

                        if (CacheAttribute != MiCached) {

                            //
                            // The caller asked for a noncached or writecombined
                            // mapping, but the page is already mapped cached by
                            // someone else.  Override the caller's request in
                            // order to keep the TB page attribute coherent.
                            //

                            MiCacheOverride[0] += 1;
                        }
                        break;

                    case MiNonCached:

                        if (CacheAttribute != MiNonCached) {

                            //
                            // The caller asked for a cached or writecombined
                            // mapping, but the page is already mapped noncached
                            // by someone else.  Override the caller's request
                            // in order to keep the TB page attribute coherent.
                            //

                            MiCacheOverride[1] += 1;
                        }
                        MI_DISABLE_CACHING (TempPte);
                        break;

                    case MiWriteCombined:

                        if (CacheAttribute != MiWriteCombined) {

                            //
                            // The caller asked for a cached or noncached
                            // mapping, but the page is already mapped
                            // writecombined by someone else.  Override the
                            // caller's request in order to keep the TB page
                            // attribute coherent.
                            //

                            MiCacheOverride[2] += 1;
                        }
                        MI_SET_PTE_WRITE_COMBINE (TempPte);
                        break;

                    case MiNotMapped:

                        //
                        // This better be for a page allocated with
                        // MmAllocatePagesForMdl.  Otherwise it might be a
                        // page on the freelist which could subsequently be
                        // given out with a different attribute !
                        //

                        ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                                (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));

                        if (OldIrql == HIGH_LEVEL) {
                            LOCK_PFN2 (OldIrql);
                        }

                        switch (CacheAttribute) {

                            case MiCached:
                                Pfn2->u3.e1.CacheAttribute = MiCached;
                                break;

                            case MiNonCached:
                                Pfn2->u3.e1.CacheAttribute = MiNonCached;
                                MI_DISABLE_CACHING (TempPte);
                                break;

                            case MiWriteCombined:
                                Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                                MI_SET_PTE_WRITE_COMBINE (TempPte);
                                break;

                            default:
                                ASSERT (FALSE);
                                break;
                        }
                        break;

                    default:
                        ASSERT (FALSE);
                        break;
                }
            }

            TempPte.u.Hard.PageFrameNumber = *Page;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);
            Page += 1;
            PointerPte += 1;
        } while (Page < LastPage);

        if (OldIrql != HIGH_LEVEL) {
            UNLOCK_PFN2 (OldIrql);
        }

        MI_SWEEP_CACHE (CacheAttribute, BaseVa, SavedPageCount * PAGE_SIZE);

        ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
        MemoryDescriptorList->MappedSystemVa = BaseVa;

        MemoryDescriptorList->MdlFlags |= MDL_MAPPED_TO_SYSTEM_VA;

        if (MmTrackPtes & 0x1) {

            //
            // First free any zombie blocks as no locks are being held.
            //

            Tracker = MiReleaseDeadPteTrackers ();

            if (Tracker != NULL) {

                RtlGetCallersAddress (&CallingAddress, &CallersCaller);

                MiInsertPteTracker (Tracker,
                                    MemoryDescriptorList,
                                    NumberOfPtes,
                                    CallingAddress,
                                    CallersCaller);
            }
        }

        if ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) != 0) {
            MemoryDescriptorList->MdlFlags |= MDL_PARTIAL_HAS_BEEN_MAPPED;
        }

        return BaseVa;

    }

    return MiMapLockedPagesInUserSpace (MemoryDescriptorList,
                                        StartingVa,
                                        CacheType,
                                        RequestedAddress);
}

PVOID
MiMapSinglePage (
     IN PVOID VirtualAddress OPTIONAL,
     IN PFN_NUMBER PageFrameIndex,
     IN MEMORY_CACHING_TYPE CacheType,
     IN MM_PAGE_PRIORITY Priority
     )

/*++

Routine Description:

    This function (re)maps a single system PTE to the specified physical page.

Arguments:

    VirtualAddress - Supplies the virtual address to map the page frame at.
                     NULL indicates a system PTE is needed.  Non-NULL supplies
                     the virtual address returned by an earlier
                     MiMapSinglePage call.

    PageFrameIndex - Supplies the page frame index to map.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

    Priority - Supplies an indication as to how important it is that this
               request succeed under low available PTE conditions.

Return Value:

    Returns the base address where the page is mapped, or NULL if the
    mapping failed.

Environment:

    Kernel mode.  APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    MMPTE TempPte;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    PAGED_CODE ();

    UNREFERENCED_PARAMETER (Priority);

    //
    // If this routine is ever changed to allow other than fully cachable
    // requests then checks must be added for large page TB overlaps which
    // can result in this function failing where it cannot today.
    //

    ASSERT (CacheType == MmCached);

    if (VirtualAddress == NULL) {

        PointerPte = MiReserveSystemPtes (1, SystemPteSpace);

        if (PointerPte == NULL) {
    
            //
            // Not enough system PTES are available.
            //
    
            return NULL;
        }

        ASSERT (PointerPte->u.Hard.Valid == 0);
        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
    }
    else {
        ASSERT (MI_IS_PHYSICAL_ADDRESS (VirtualAddress) == 0);
        ASSERT (VirtualAddress >= MM_SYSTEM_RANGE_START);

        PointerPte = MiGetPteAddress (VirtualAddress);
        ASSERT (PointerPte->u.Hard.Valid == 1);

        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

        KeFlushSingleTb (VirtualAddress,
                         TRUE,
                         TRUE,
                         (PHARDWARE_PTE)PointerPte,
                         ZeroPte.u.Flush);
    }

    TempPte = ValidKernelPte;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    MI_SWEEP_CACHE (CacheAttribute, VirtualAddress, PAGE_SIZE);

    return VirtualAddress;
}

PVOID
MmMapLockedPages (
     IN PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space or the user portion of
    the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                            been updated by MmProbeAndLockPages.


    AccessMode - Supplies an indicator of where to map the pages;
                 KernelMode indicates that the pages should be mapped in the
                 system part of the address space, UserMode indicates the
                 pages should be mapped in the user part of the address space.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if the processor mode is USER_MODE
    and quota limits or VM limits are exceeded.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if access mode is KernelMode,
                  APC_LEVEL or below if access mode is UserMode.

--*/

{
    return MmMapLockedPagesSpecifyCache (MemoryDescriptorList,
                                         AccessMode,
                                         MmCached,
                                         NULL,
                                         TRUE,
                                         HighPagePriority);
}

VOID
MiUnmapSinglePage (
     IN PVOID VirtualAddress
     )

/*++

Routine Description:

    This routine unmaps a single locked page which was previously mapped via
    an MiMapSinglePage call.

Arguments:

    VirtualAddress - Supplies the virtual address used to map the page.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL or below, base address is within system space.

--*/

{
    PMMPTE PointerPte;

    PAGED_CODE ();

    ASSERT (MI_IS_PHYSICAL_ADDRESS (VirtualAddress) == 0);
    ASSERT (VirtualAddress >= MM_SYSTEM_RANGE_START);

    PointerPte = MiGetPteAddress (VirtualAddress);

    MiReleaseSystemPtes (PointerPte, 1, SystemPteSpace);
    return;
}

PVOID
MmAllocateMappingAddress (
     IN SIZE_T NumberOfBytes,
     IN ULONG PoolTag
     )

/*++

Routine Description:

    This function allocates a system PTE mapping of the requested length
    that can be used later to map arbitrary addresses.

Arguments:

    NumberOfBytes - Supplies the maximum number of bytes the mapping can span.

    PoolTag - Supplies a pool tag to associate this mapping to the caller.

Return Value:

    Returns a virtual address where to use for later mappings.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PVOID BaseVa;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PVOID Tracker;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PMDL MemoryDescriptorList;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    //
    // Make sure there are enough PTEs of the requested size.
    // Try to ensure available PTEs inline when we're rich.
    // Otherwise go the long way.
    //

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (0, NumberOfBytes);

    if (NumberOfPages == 0) {

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x100,
                      NumberOfPages,
                      PoolTag,
                      (ULONG_PTR) CallingAddress);
    }

    //
    // Callers must identify themselves.
    //

    if (PoolTag == 0) {
        return NULL;
    }

    //
    // Leave space to stash the length and tag.
    //

    NumberOfPages += 2;

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {

        //
        // Not enough system PTES are available.
        //

        return NULL;
    }

    //
    // Make sure the valid bit is always zero in the stash PTEs.
    //

    *(PULONG_PTR)PointerPte = (NumberOfPages << 1);
    PointerPte += 1;

    *(PULONG_PTR)PointerPte = (PoolTag & ~0x1);
    PointerPte += 1;

    BaseVa = MiGetVirtualAddressMappedByPte (PointerPte);

    if (MmTrackPtes & 0x1) {

        //
        // First free any zombie blocks as no locks are being held.
        //

        Tracker = MiReleaseDeadPteTrackers ();

        if (Tracker != NULL) {

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            MemoryDescriptorList = (PMDL) &MdlHack;

            MemoryDescriptorList->MappedSystemVa = BaseVa;
            MemoryDescriptorList->StartVa = (PVOID)(ULONG_PTR)PoolTag;
            MemoryDescriptorList->ByteOffset = 0;
            MemoryDescriptorList->ByteCount = (ULONG)((NumberOfPages - 2) * PAGE_SIZE);

            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            *Page = 0;

            MiInsertPteTracker (Tracker,
                                MemoryDescriptorList,
                                NumberOfPages - 2,
                                CallingAddress,
                                CallersCaller);
        }
    }

    return BaseVa;
}

VOID
MmFreeMappingAddress (
     IN PVOID BaseAddress,
     IN ULONG PoolTag
     )

/*++

Routine Description:

    This routine unmaps a virtual address range previously reserved with
    MmAllocateMappingAddress.

Arguments:

    BaseAddress - Supplies the base address previously reserved.

    PoolTag - Supplies the caller's identifying tag.

Return Value:

    None.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    ULONG OriginalPoolTag;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerBase;
    PMMPTE PointerPte;
    PMMPTE LastPte;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));
    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x101,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    NumberOfPages = *(PULONG_PTR)PointerBase;
    ASSERT ((NumberOfPages & 0x1) == 0);
    NumberOfPages >>= 1;

    if (NumberOfPages <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x102,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      NumberOfPages);
    }

    NumberOfPages -= 2;
    LastPte = PointerPte + NumberOfPages;

    while (PointerPte < LastPte) {
        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x103,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }
        PointerPte += 1;
    }

    if (MmTrackPtes & 0x1) {
        MiRemovePteTracker (NULL,
                            PointerBase + 2,
                            NumberOfPages);
    }

    //
    // Note the tag and size are nulled out when the PTEs are released below
    // so any drivers that try to use their mapping after freeing it get
    // caught immediately.
    //

    MiReleaseSystemPtes (PointerBase, (ULONG)NumberOfPages + 2, SystemPteSpace);
    return;
}

PVOID
MmMapLockedPagesWithReservedMapping (
    IN PVOID MappingAddress,
    IN ULONG PoolTag,
    IN PMDL MemoryDescriptorList,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space.

Arguments:

    MappingAddress - Supplies a valid mapping address obtained earlier via
                     MmAllocateMappingAddress.

    PoolTag - Supplies the caller's identifying tag.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will return NULL if the cache type requested is incompatible
    with the pages being mapped or if the caller tries to map an MDL that is
    larger than the virtual address range originally reserved.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.  The caller must synchronize usage
    of the argument virtual address space.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER VaPageSpan;
    PFN_NUMBER SavedPageCount;
    PPFN_NUMBER Page;
    PMMPTE PointerBase;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    PVOID StartingVa;
    PFN_NUMBER NumberOfPtes;
    PFN_NUMBER PageFrameIndex;
    ULONG OriginalPoolTag;
    PMMPFN Pfn2;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    PointerPte = MiGetPteAddress (MappingAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x104,
                      (ULONG_PTR)MappingAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    VaPageSpan = *(PULONG_PTR)PointerBase;
    ASSERT ((VaPageSpan & 0x1) == 0);
    VaPageSpan >>= 1;

    if (VaPageSpan <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x105,
                      (ULONG_PTR)MappingAddress,
                      PoolTag,
                      VaPageSpan);
    }

    if (NumberOfPages > VaPageSpan - 2) {

        //
        // The caller is trying to map an MDL that spans a range larger than
        // the reserving mapping !  This is a driver bug.
        //

        ASSERT (FALSE);
        return NULL;
    }

    //
    // All the mapping PTEs must be zero.
    //

    LastPte = PointerPte + VaPageSpan - 2;

    while (PointerPte < LastPte) {

        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x107,
                          (ULONG_PTR)MappingAddress,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)LastPte);
        }

        PointerPte += 1;
    }

    PointerPte = PointerBase + 2;
    SavedPageCount = NumberOfPages;

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_MAPPED_TO_SYSTEM_VA |
                        MDL_SOURCE_IS_NONPAGED_POOL |
                        MDL_PARTIAL_HAS_BEEN_MAPPED)) == 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_PAGES_LOCKED |
                        MDL_PARTIAL)) != 0);

    //
    // If a noncachable mapping is requested, none of the pages in the
    // requested MDL can reside in a large page.  Otherwise we would be
    // creating an incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    if (CacheAttribute != MiCached) {

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }

            PageFrameIndex = *Page;

            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                MiNonCachedCollisions += 1;
                return NULL;
            }

            Page += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);

        NumberOfPages = SavedPageCount;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        MI_PREPARE_FOR_NONCACHED (CacheAttribute);
    }

    NumberOfPtes = NumberOfPages;

    TempPte = ValidKernelPte;

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    OldIrql = HIGH_LEVEL;

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        ASSERT (PointerPte->u.Hard.Valid == 0);

        if (IoMapping == 0) {

            Pfn2 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn2->u3.e2.ReferenceCount != 0);
            TempPte = ValidKernelPte;

            switch (Pfn2->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {

                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

                    ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
                    if (OldIrql == HIGH_LEVEL) {
                        LOCK_PFN2 (OldIrql);
                    }

                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn2->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn2->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }

        TempPte.u.Hard.PageFrameNumber = *Page;
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    if (OldIrql != HIGH_LEVEL) {
        UNLOCK_PFN2 (OldIrql);
    }

    MI_SWEEP_CACHE (CacheAttribute, MappingAddress, SavedPageCount * PAGE_SIZE);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
    MemoryDescriptorList->MappedSystemVa = MappingAddress;

    MemoryDescriptorList->MdlFlags |= MDL_MAPPED_TO_SYSTEM_VA;

    if ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) != 0) {
        MemoryDescriptorList->MdlFlags |= MDL_PARTIAL_HAS_BEEN_MAPPED;
    }

    MappingAddress = (PVOID)((PCHAR)MappingAddress + MemoryDescriptorList->ByteOffset);

    return MappingAddress;
}

VOID
MmUnmapReservedMapping (
     IN PVOID BaseAddress,
     IN ULONG PoolTag,
     IN PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    a MmMapLockedPagesWithReservedMapping call.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    PoolTag - Supplies the caller's identifying tag.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.  The caller must synchronize usage
    of the argument virtual address space.

--*/

{
    ULONG OriginalPoolTag;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER ExtraPages;
    PFN_NUMBER VaPageSpan;
    PMMPTE PointerBase;
    PMMPTE LastPte;
    PMMPTE LastMdlPte;
    PVOID StartingVa;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    PMMPTE PointerPte;
    PFN_NUMBER i;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastCurrentPage;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) != 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARENT_MAPPED_SYSTEM_VA) == 0);
    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));
    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x108,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    VaPageSpan = *(PULONG_PTR)PointerBase;
    ASSERT ((VaPageSpan & 0x1) == 0);
    VaPageSpan >>= 1;

    if (VaPageSpan <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x109,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      VaPageSpan);
    }

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    if (NumberOfPages > VaPageSpan - 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x10A,
                      (ULONG_PTR)BaseAddress,
                      VaPageSpan,
                      NumberOfPages);
    }

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    LastCurrentPage = Page + NumberOfPages;

    if (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) {

        ExtraPages = *(Page + NumberOfPages);
        ASSERT (ExtraPages <= MiCurrentAdvancedPages);
        ASSERT (NumberOfPages + ExtraPages <= VaPageSpan - 2);
        NumberOfPages += ExtraPages;
#if DBG
        InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, 0 - ExtraPages);
        MiAdvancesFreed += ExtraPages;
#endif
    }

    LastMdlPte = PointerPte + NumberOfPages;
    LastPte = PointerPte + VaPageSpan - 2;

    //
    // The range described by the argument MDL must be mapped.
    //

    while (PointerPte < LastMdlPte) {
        if (PointerPte->u.Hard.Valid == 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x10B,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }

#if DBG
        ASSERT ((*Page == MI_GET_PAGE_FRAME_FROM_PTE (PointerPte)) ||
                (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES));

        if (((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) &&
            (Page < LastCurrentPage)) {

            PMMPFN Pfn3;
            Pfn3 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn3->u3.e2.ReferenceCount != 0);
        }

        Page += 1;
#endif

        PointerPte += 1;
    }

    //
    // The range past the argument MDL must be unmapped.
    //

    while (PointerPte < LastPte) {
        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x10C,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }
        PointerPte += 1;
    }

    MiFillMemoryPte (PointerBase + 2,
                     NumberOfPages * sizeof (MMPTE),
                     ZeroPte.u.Long);

    if (NumberOfPages == 1) {
        KeFlushSingleTb (BaseAddress,
                         TRUE,
                         TRUE,
                         (PHARDWARE_PTE)(PointerBase + 2),
                         ZeroPte.u.Flush);
    }
    else if (NumberOfPages < MM_MAXIMUM_FLUSH_COUNT) {

        for (i = 0; i < NumberOfPages; i += 1) {
            VaFlushList[i] = BaseAddress;
            BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        }

        KeFlushMultipleTb ((ULONG)NumberOfPages,
                           &VaFlushList[0],
                           TRUE,
                           TRUE,
                           NULL,
                           *(PHARDWARE_PTE)&ZeroPte.u.Flush);
    }
    else {
        KeFlushEntireTb (TRUE, TRUE);
    }

    MemoryDescriptorList->MdlFlags &= ~(MDL_MAPPED_TO_SYSTEM_VA |
                                        MDL_PARTIAL_HAS_BEEN_MAPPED);

    return;
}
NTKERNELAPI
NTSTATUS
MmAdvanceMdl (
    IN PMDL Mdl,
    IN ULONG NumberOfBytes
    )

/*++

Routine Description:

    This routine takes the specified MDL and "advances" it forward
    by the specified number of bytes.  If this causes the MDL to advance
    past the initial page, the pages that are advanced over are immediately
    unlocked and the system VA that maps the MDL is also adjusted (along
    with the user address).
    
    WARNING !  WARNING !  WARNING !

    This means the caller MUST BE AWARE that the "advanced" pages are
    immediately reused and therefore MUST NOT BE REFERENCED by the caller
    once this routine has been called.  Likewise the virtual address as
    that is also being adjusted here.

    Even if the caller has statically allocated this MDL on his local stack,
    he cannot use more than the space currently described by the MDL on return
    from this routine unless he first unmaps the MDL (if it was mapped).
    Otherwise the system PTE lists will be corrupted.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    NumberOfBytes - The number of bytes to advance the MDL by.

Return Value:

    NTSTATUS.

--*/

{
    ULONG i;
    ULONG PageCount;
    ULONG FreeBit;
    ULONG Slush;
    KIRQL OldIrql;
    PPFN_NUMBER Page;
    PPFN_NUMBER NewPage;
    ULONG OffsetPages;
    PEPROCESS Process;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PVOID StartingVa;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT (Mdl->MdlFlags & (MDL_PAGES_LOCKED | MDL_SOURCE_IS_NONPAGED_POOL));
    ASSERT (BYTE_OFFSET (Mdl->StartVa) == 0);

    //
    // Disallow advancement past the end of the MDL.
    //

    if (NumberOfBytes >= Mdl->ByteCount) {
        return STATUS_INVALID_PARAMETER_2;
    }

    PageCount = 0;

    MiMdlsAdjusted = TRUE;

    StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa, Mdl->ByteCount);

    if (Mdl->ByteOffset != 0) {
        Slush = PAGE_SIZE - Mdl->ByteOffset;

        if (NumberOfBytes < Slush) {

            Mdl->ByteCount -= NumberOfBytes;
            Mdl->ByteOffset += NumberOfBytes;

            //
            // StartVa never includes the byte offset (it's always page-aligned)
            // so don't adjust it here.  MappedSystemVa does include byte
            // offsets so do adjust that.
            //

            if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa + NumberOfBytes);
            }

            return STATUS_SUCCESS;
        }

        NumberOfBytes -= Slush;

        Mdl->StartVa = (PVOID) ((PCHAR)Mdl->StartVa + PAGE_SIZE);
        Mdl->ByteOffset = 0;
        Mdl->ByteCount -= Slush;

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa + Slush);
        }

        //
        // Up the number of pages (and addresses) that need to slide.
        //

        PageCount += 1;
    }

    //
    // The MDL start is now nicely page aligned.  Make sure there's still
    // data left in it (we may have finished it off above), then operate on it.
    //

    if (NumberOfBytes != 0) {

        Mdl->ByteCount -= NumberOfBytes;

        Mdl->ByteOffset = BYTE_OFFSET (NumberOfBytes);

        OffsetPages = NumberOfBytes >> PAGE_SHIFT;

        Mdl->StartVa = (PVOID) ((PCHAR)Mdl->StartVa + (OffsetPages << PAGE_SHIFT));
        PageCount += OffsetPages;

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

            Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa +
                                           (OffsetPages << PAGE_SHIFT) +
                                           Mdl->ByteOffset);
        }
    }

    ASSERT (PageCount <= NumberOfPages);

    if (PageCount != 0) {

        //
        // Slide the page frame numbers forward decrementing reference counts
        // on the ones that are released.  Then adjust the mapped system VA
        // (if there is one) to reflect the current frame.  Note that the TB
        // does not need to be flushed due to the careful sliding and when
        // the MDL is finally completely unmapped, the extra information
        // added to the MDL here is used to free the entire original PTE
        // mapping range in one chunk so as not to fragment the PTE space.
        //

        Page = (PPFN_NUMBER)(Mdl + 1);
        NewPage = Page;

        Process = Mdl->Process;

        MdlFlags = Mdl->MdlFlags;

        if (Process != NULL) {

            if ((MdlFlags & MDL_PAGES_LOCKED) &&
                ((MdlFlags & MDL_IO_SPACE) == 0)) {

                ASSERT ((MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
                ASSERT ((SPFN_NUMBER)Process->NumberOfLockedPages >= 0);

                InterlockedExchangeAddSizeT (&Process->NumberOfLockedPages,
                                             0 - PageCount);
            }

            if (MmTrackLockedPages == TRUE) {
                MiUpdateMdlTracker (Mdl, PageCount);
            }
        }

        LOCK_PFN2 (OldIrql);

        for (i = 0; i < PageCount; i += 1) {

            //
            // Decrement the stale page frames now, this will unlock them
            // resulting in them being immediately reused if necessary.
            //

            if ((MdlFlags & MDL_PAGES_LOCKED) &&
                ((MdlFlags & MDL_IO_SPACE) == 0)) {

                ASSERT ((MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);

                Pfn1 = MI_PFN_ELEMENT (*Page);

                if (MdlFlags & MDL_WRITE_OPERATION) {

                    //
                    // If this was a write operation set the modified bit
                    // in the PFN database.
                    //

                    MI_SET_MODIFIED (Pfn1, 1, 0x3);

                    if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                                 (Pfn1->u3.e1.WriteInProgress == 0)) {

                        FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                        if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                            MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                        }
                    }
                }
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);
            }
            Page += 1;
        }

        UNLOCK_PFN2 (OldIrql);

        //
        // Now ripple the remaining pages to the front of the MDL, effectively
        // purging the old ones which have just been released.
        //

        ASSERT (i < NumberOfPages);

        for ( ; i < NumberOfPages; i += 1) {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }

            *NewPage = *Page;
            NewPage += 1;
            Page += 1;
        }

        //
        // If the MDL has been mapped, stash the number of pages advanced
        // at the end of the frame list inside the MDL and mark the MDL as
        // containing extra PTEs to free.  Thus when the MDL is finally
        // completely unmapped, this can be used so the entire original PTE
        // mapping range can be freed in one chunk so as not to fragment the
        // PTE space.
        //

        if (MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

#if DBG
            InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, PageCount);
            MiAdvancesGiven += PageCount;
#endif

            if (MdlFlags & MDL_FREE_EXTRA_PTES) {

                //
                // This MDL has already been advanced at least once.  Any
                // PTEs from those advancements need to be preserved now.
                //

                ASSERT (*Page <= MiCurrentAdvancedPages - PageCount);
                PageCount += *(PULONG)Page;
            }
            else {
                Mdl->MdlFlags |= MDL_FREE_EXTRA_PTES;
            }

            *NewPage = PageCount;
        }
    }

    return STATUS_SUCCESS;
}

NTKERNELAPI
NTSTATUS
MmProtectMdlSystemAddress (
    IN PMDL MemoryDescriptorList,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function protects the system address range specified
    by the argument Memory Descriptor List.

    Note the caller must make this MDL mapping readwrite before finally
    freeing (or reusing) it.

Arguments:

    MemoryDescriptorList - Supplies the MDL describing the virtual range.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, IRQL DISPATCH_LEVEL or below.  The caller is responsible for
    synchronizing access to this routine.

--*/

{
    KIRQL OldIrql;
    PVOID BaseAddress;
    PVOID SystemVa;
    MMPTE PteContents;
    MMPTE JunkPte;
    PMMPTE PointerPte;
    ULONG ProtectionMask;
#if DBG
    PMMPFN Pfn1;
    PPFN_NUMBER Page;
#endif
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE OriginalPte;
    LOGICAL WasValid;
    PMM_PTE_MAPPING Map;
    PMM_PTE_MAPPING MapEntry;
    PMM_PTE_MAPPING FoundMap;
    PLIST_ENTRY NextEntry;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PAGES_LOCKED) != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) == 0);
    ASSERT (MemoryDescriptorList->ByteCount != 0);

    if ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    BaseAddress = MemoryDescriptorList->MappedSystemVa;

    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));

    ProtectionMask = MiMakeProtectionMask (NewProtect);

    //
    // No bogus or copy-on-write protections allowed for these.
    //

    if ((ProtectionMask == MM_INVALID_PROTECTION) ||
        (ProtectionMask == MM_GUARD_PAGE) ||
        (ProtectionMask == MM_DECOMMIT) ||
        (ProtectionMask == MM_NOCACHE) ||
        (ProtectionMask == MM_WRITECOPY) ||
        (ProtectionMask == MM_EXECUTE_WRITECOPY)) {

        return STATUS_INVALID_PAGE_PROTECTION;
    }

    PointerPte = MiGetPteAddress (BaseAddress);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress,
                                           MemoryDescriptorList->ByteCount);

    SystemVa = PAGE_ALIGN (BaseAddress);

    //
    // Initializing Map is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Map = NULL;

    if (ProtectionMask != MM_READWRITE) {

        Map = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(MM_PTE_MAPPING),
                                     'mPmM');

        if (Map == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        Map->SystemVa = SystemVa;
        Map->SystemEndVa = (PVOID)((ULONG_PTR)SystemVa + (NumberOfPages << PAGE_SHIFT));
        Map->Protection = ProtectionMask;
    }

#if DBG
    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
#endif

    PteFlushList.Count = 0;

    while (NumberOfPages != 0) {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {
            WasValid = TRUE;
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            OriginalPte = PteContents;
        }
        else if ((PteContents.u.Soft.Transition == 1) &&
                 (PteContents.u.Soft.Protection == MM_NOACCESS)) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            WasValid = FALSE;
#if defined(_IA64_)
            OriginalPte.u.Hard.Cache = PteContents.u.Trans.Rsvd0;
#else
            OriginalPte.u.Hard.WriteThrough = PteContents.u.Soft.PageFileLow;
            OriginalPte.u.Hard.CacheDisable = (PteContents.u.Soft.PageFileLow >> 1);
#endif

        }
        else {
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x1235,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteContents.u.Long);
        }

#if DBG
        ASSERT (*Page == PageFrameIndex);

        if ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) {
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
        }

        Page += 1;
#endif

        if (ProtectionMask == MM_NOACCESS) {

            //
            // To generate a bugcheck on bogus access: Prototype must stay
            // clear, transition must stay set, protection must stay NO_ACCESS.
            //

            MI_MAKE_VALID_PTE_TRANSITION (PteContents, MM_NOACCESS);

            //
            // Stash the cache attributes into the software PTE so they can
            // be restored later.
            //

#if defined(_IA64_)
            PteContents.u.Trans.Rsvd0 = OriginalPte.u.Hard.Cache;
#else
            PteContents.u.Soft.PageFileLow = OriginalPte.u.Hard.WriteThrough;
            PteContents.u.Soft.PageFileLow |= (OriginalPte.u.Hard.CacheDisable << 1);
#endif
        }
        else {
            MI_MAKE_VALID_PTE (PteContents,
                               PageFrameIndex,
                               ProtectionMask,
                               PointerPte);

            if (ProtectionMask & MM_READWRITE) {
                MI_SET_PTE_DIRTY (PteContents);
            }

            //
            // Extract cache type from the original PTE so it can be preserved.
            // Note that since we only allow protection changes (not caching
            // attribute changes), there is no need to flush or sweep TBs on
            // insertion below.
            //

#if defined(_IA64_)
            PteContents.u.Hard.Cache = OriginalPte.u.Hard.Cache;
#else
            PteContents.u.Hard.WriteThrough = OriginalPte.u.Hard.WriteThrough;
            PteContents.u.Hard.CacheDisable = OriginalPte.u.Hard.CacheDisable;
#endif
        }

        *PointerPte = PteContents;

        if ((WasValid == TRUE) &&
            (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT)) {

            PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
            PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
            PteFlushList.Count += 1;
        }

        BaseAddress = (PVOID)((ULONG_PTR)BaseAddress + PAGE_SIZE);
        PointerPte += 1;
        NumberOfPages -= 1;
    }

    //
    // Flush the TB entries for any relevant pages.  Note the ZeroPte is
    // not written to the actual PTEs as they have already been set above.
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, FALSE, ZeroPte);
    }

    if (ProtectionMask != MM_READWRITE) {

        //
        // Insert (or update) the list entry describing this range.
        // Don't bother sorting the list as there will never be many entries.
        //

        FoundMap = NULL;

        OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

        NextEntry = MmProtectedPteList.Flink;

        while (NextEntry != &MmProtectedPteList) {

            MapEntry = CONTAINING_RECORD (NextEntry,
                                          MM_PTE_MAPPING,
                                          ListEntry);

            if (MapEntry->SystemVa == SystemVa) {
                ASSERT (MapEntry->SystemEndVa == Map->SystemEndVa);
                MapEntry->Protection = Map->Protection;
                FoundMap = MapEntry;
                break;
            }
            NextEntry = NextEntry->Flink;
        }

        if (FoundMap == NULL) {
            InsertHeadList (&MmProtectedPteList, &Map->ListEntry);
        }

        KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

        if (FoundMap != NULL) {
            ExFreePool (Map);
        }
    }
    else {

        //
        // If there is an existing list entry describing this range, remove it.
        //

        if (!IsListEmpty (&MmProtectedPteList)) {

            FoundMap = NULL;

            OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

            NextEntry = MmProtectedPteList.Flink;

            while (NextEntry != &MmProtectedPteList) {

                MapEntry = CONTAINING_RECORD (NextEntry,
                                              MM_PTE_MAPPING,
                                              ListEntry);

                if (MapEntry->SystemVa == SystemVa) {
                    RemoveEntryList (NextEntry);
                    FoundMap = MapEntry;
                    break;
                }
                NextEntry = NextEntry->Flink;
            }

            KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

            if (FoundMap != NULL) {
                ExFreePool (FoundMap);
            }
        }
    }

    ASSERT (MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA);

    return STATUS_SUCCESS;
}

LOGICAL
MiCheckSystemPteProtection (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function determines whether the faulting virtual address lies
    within the non-writable alternate system PTE mappings.

Arguments:

    StoreInstruction - Supplies nonzero if the operation causes a write into
                       memory, zero if not.

    VirtualAddress - Supplies the virtual address which caused the fault.

Return Value:

    TRUE if the fault was handled by this code (and PTE updated), FALSE if not.

Environment:

    Kernel mode.  Called from the fault handler at any IRQL.

--*/

{
    KIRQL OldIrql;
    PMMPTE PointerPte;
    ULONG ProtectionCode;
    PLIST_ENTRY NextEntry;
    PMM_PTE_MAPPING MapEntry;

    //
    // If PTE mappings with various protections are active and the faulting
    // address lies within these mappings, resolve the fault with
    // the appropriate protections.
    //

    if (IsListEmpty (&MmProtectedPteList)) {
        return FALSE;
    }

    OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

    NextEntry = MmProtectedPteList.Flink;

    while (NextEntry != &MmProtectedPteList) {

        MapEntry = CONTAINING_RECORD (NextEntry,
                                      MM_PTE_MAPPING,
                                      ListEntry);

        if ((VirtualAddress >= MapEntry->SystemVa) &&
            (VirtualAddress < MapEntry->SystemEndVa)) {

            ProtectionCode = MapEntry->Protection;
            KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

            PointerPte = MiGetPteAddress (VirtualAddress);

            if (StoreInstruction != 0) {
                if ((ProtectionCode & MM_READWRITE) == 0) {

                    KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                  (ULONG_PTR)VirtualAddress,
                                  (ULONG_PTR)PointerPte->u.Long,
                                  0,
                                  16);
                }
            }

            MI_NO_FAULT_FOUND (StoreInstruction,
                               PointerPte,
                               VirtualAddress,
                               FALSE);

            //
            // Fault was handled directly here, no need for the caller to
            // do anything.
            //

            return TRUE;
        }
        NextEntry = NextEntry->Flink;
    }

    KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

    return FALSE;
}

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to insert
    a physical VAD into the process chain.

Arguments:

    Process - Supplies the process to add the physical VAD to.

    PhysicalView - Supplies the physical view data to link in.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL, working set and address space mutexes held.

--*/
{
    KIRQL OldIrql;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    InsertTailList (&Process->PhysicalVadList, &PhysicalView->ListEntry);

    if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {
        MiActiveWriteWatch += 1;
    }

    if (PhysicalView->Vad->u.VadFlags.PhysicalMapping == 1) {
        PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD);
    }

    UNLOCK_PFN (OldIrql);

    if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {

        //
        // Mark this process as forever containing write-watch
        // address space(s).
        //

        if ((Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) == 0) {
            PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_USING_WRITE_WATCH);
        }
    }
    MmUnlockPagableImageSection (ExPageLockHandle);
}

VOID
MiPhysicalViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to remove
    a physical VAD from the process chain.

Arguments:

    Process - Supplies the process to remove the physical VAD from.

    Vad - Supplies the Vad to remove.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, working set and address space mutexes held.

--*/
{
    KIRQL OldIrql;
    PRTL_BITMAP BitMap;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    ULONG BitMapSize;
    ULONG PhysicalVadCount;

    BitMap = NULL;
    PhysicalVadCount = 0;

    LOCK_PFN (OldIrql);

    NextEntry = Process->PhysicalVadList.Flink;
    while (NextEntry != &Process->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad == Vad) {
            RemoveEntryList (NextEntry);

            if (Vad->u.VadFlags.WriteWatch == 1) {
                MiActiveWriteWatch -= 1;
                BitMap = PhysicalView->u.BitMap;
                ASSERT (BitMap != NULL);
            }
            else if (Vad->u.VadFlags.PhysicalMapping == 1) {
                ASSERT (Process->Flags & PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD);

                //
                // If this might be the last physical VAD, scan the rest to
                // see.  If so, then mark the process as no longer having
                // any so probe and locks can execute faster.
                //

                if (PhysicalVadCount == 0) {
                    NextEntry = NextEntry->Flink;
                    while (NextEntry != &Process->PhysicalVadList) {
                        if (Vad->u.VadFlags.PhysicalMapping == 1) {
                            PhysicalVadCount += 1;
                            break;
                        }
                        NextEntry = NextEntry->Flink;
                    }
                    if (PhysicalVadCount == 0) {
                        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD);
                    }
                }
            }

            UNLOCK_PFN (OldIrql);
            ExFreePool (PhysicalView);

            if (BitMap != NULL) {
                BitMapSize = sizeof(RTL_BITMAP) + (ULONG)(((BitMap->SizeOfBitMap + 31) / 32) * 4);
                PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
                ExFreePool (BitMap);
            }

            return;
        }

        if (Vad->u.VadFlags.PhysicalMapping == 1) {
            PhysicalVadCount += 1;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (FALSE);

    UNLOCK_PFN (OldIrql);
}

VOID
MiPhysicalViewAdjuster (
    IN PEPROCESS Process,
    IN PMMVAD OldVad,
    IN PMMVAD NewVad
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to repoint
    a physical VAD in the process chain.

Arguments:

    Process - Supplies the process in which to adjust the physical VAD.

    Vad - Supplies the old Vad to replace.

    NewVad - Supplies the newVad to substitute.

Return Value:

    None.

Environment:

    Kernel mode, called with APCs disabled, working set mutex held.

--*/
{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    NextEntry = Process->PhysicalVadList.Flink;
    while (NextEntry != &Process->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad == OldVad) {
            PhysicalView->Vad = NewVad;
            UNLOCK_PFN (OldIrql);
            MmUnlockPagableImageSection (ExPageLockHandle);
            return;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (FALSE);

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);
}

PVOID
MiMapLockedPagesInUserSpace (
     IN PMDL MemoryDescriptorList,
     IN PVOID StartingVa,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID BaseVa
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the user portion of the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.


    StartingVa - Supplies the starting address.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" user mappings.

    BaseVa - Supplies the base address of the view. If the initial
             value of this argument is not null, then the view will
             be allocated starting at the specified virtual
             address rounded down to the next 64kb address
             boundary. If the initial value of this argument is
             null, then the operating system will determine
             where to allocate the view.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if quota limits or VM limits are
    exceeded.

Environment:

    Kernel mode.  APC_LEVEL or below.

--*/

{
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER SavedPageCount;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PCHAR Va;
    MMPTE TempPte;
    PVOID EndingAddress;
    PMMVAD_LONG Vad;
    PEPROCESS Process;
    PMMPFN Pfn2;
    PVOID UsedPageTableHandle;
    PMI_PHYSICAL_VIEW PhysicalView;
    NTSTATUS Status;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    PAGED_CODE ();
    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    //
    // If a noncachable mapping is requested, none of the pages in the
    // requested MDL can reside in a large page.  Otherwise we would be
    // creating an incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    if (CacheAttribute != MiCached) {

        SavedPageCount = NumberOfPages;

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }
            PageFrameIndex = *Page;
            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                MiNonCachedCollisions += 1;
                ExRaiseStatus (STATUS_INVALID_ADDRESS);
                return NULL;
            }

            Page += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);

        NumberOfPages = SavedPageCount;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    }

    //
    // Map the pages into the user part of the address as user
    // read/write no-delete.
    //

    Vad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');

    if (Vad == NULL) {
        ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        return NULL;
    }

    PhysicalView = (PMI_PHYSICAL_VIEW)ExAllocatePoolWithTag (NonPagedPool,
                                                             sizeof(MI_PHYSICAL_VIEW),
                                                             MI_PHYSICAL_VIEW_KEY);
    if (PhysicalView == NULL) {
        ExFreePool (Vad);
        ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        return NULL;
    }

    RtlZeroMemory (Vad, sizeof (MMVAD_LONG));

    ASSERT (Vad->ControlArea == NULL);
    ASSERT (Vad->FirstPrototypePte == NULL);
    ASSERT (Vad->u.LongFlags == 0);
    Vad->u.VadFlags.Protection = MM_READWRITE;
    Vad->u.VadFlags.PhysicalMapping = 1;
    Vad->u.VadFlags.PrivateMemory = 1;

    Vad->u2.VadFlags2.LongVad = 1;

    PhysicalView->Vad = (PMMVAD) Vad;
    PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_PHYS;

    Process = PsGetCurrentProcess ();

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (BaseVa != NULL) {

        if (BYTE_OFFSET (BaseVa) != 0) {

            //
            // Invalid base address.
            //

            Status = STATUS_INVALID_ADDRESS;
            goto ErrorReturn;
        }

        EndingAddress = (PVOID)((PCHAR)BaseVa + ((ULONG_PTR)NumberOfPages * PAGE_SIZE) - 1);

        if ((EndingAddress <= BaseVa) || (EndingAddress > MM_HIGHEST_VAD_ADDRESS)) {
            //
            // Invalid region size.
            //

            Status = STATUS_INVALID_ADDRESS;
            goto ErrorReturn;
        }

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        //
        // Make sure the address space is not already in use.
        //

        if (MiCheckForConflictingVadExistence (Process, BaseVa, EndingAddress) == TRUE) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_CONFLICTING_ADDRESSES;
            goto ErrorReturn;
        }
    }
    else {

        //
        // Get the address creation mutex.
        //

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        Status = MiFindEmptyAddressRange ((ULONG_PTR)NumberOfPages * PAGE_SIZE,
                                          X64K,
                                          0,
                                          &BaseVa);

        if (!NT_SUCCESS (Status)) {
            UNLOCK_ADDRESS_SPACE (Process);
            goto ErrorReturn;
        }

        EndingAddress = (PVOID)((PCHAR)BaseVa + ((ULONG_PTR)NumberOfPages * PAGE_SIZE) - 1);
    }

    PhysicalView->StartVa = BaseVa;
    PhysicalView->EndVa = EndingAddress;

    Vad->StartingVpn = MI_VA_TO_VPN (BaseVa);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad ((PMMVAD) Vad);

    if (!NT_SUCCESS(Status)) {
        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
        goto ErrorReturn;
    }

    //
    // The VAD has been inserted, but the physical view descriptor cannot
    // be until the page table page hierarchy is in place.  This is to
    // prevent races with probes.
    //

    //
    // Create a page table and fill in the mappings for the Vad.
    //

    Va = BaseVa;
    PointerPte = MiGetPteAddress (BaseVa);

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        PointerPde = MiGetPteAddress (PointerPte);

        MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);

        ASSERT (PointerPte->u.Hard.Valid == 0);

        //
        // Another zeroed PTE is being made non-zero.
        //

        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        TempPte = ValidUserPte;
        TempPte.u.Hard.PageFrameNumber = *Page;

        if (IoMapping == 0) {

            Pfn2 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

            switch (Pfn2->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {
                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

                    ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn2->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn2->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }
        else {
            switch (CacheAttribute) {

                case MiCached:
                    break;

                case MiNonCached:
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        //
        // A PTE just went from not present, not transition to
        // present.  The share count and valid count must be
        // updated in the page table page which contains this PTE.
        //

        Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
        Pfn2->u2.ShareCount += 1;

        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
        Va += PAGE_SIZE;
    } while (NumberOfPages != 0);

    MI_SWEEP_CACHE (CacheAttribute, BaseVa, MemoryDescriptorList->ByteCount);

    //
    // Insert the physical view descriptor now that the page table page
    // hierarchy is in place.  Note probes can find this descriptor immediately.
    //

    MiPhysicalViewInserter (Process, PhysicalView);

    UNLOCK_WS_AND_ADDRESS_SPACE (Process);

    ASSERT (BaseVa != NULL);

    BaseVa = (PVOID)((PCHAR)BaseVa + MemoryDescriptorList->ByteOffset);

    return BaseVa;

ErrorReturn:

    ExFreePool (Vad);
    ExFreePool (PhysicalView);
    ExRaiseStatus (Status);
    return NULL;
}

VOID
MmUnmapLockedPages (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    a MmMapLockedPages call.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if base address is within
    system space; APC_LEVEL or below if base address is user space.

    Note that in some instances the PFN lock is held by the caller.

--*/

{
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerBase;
    PVOID StartingVa;
    PPFN_NUMBER Page;
#if DBG
    PMMPTE PointerPte;
    PFN_NUMBER i;
#endif

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARENT_MAPPED_SYSTEM_VA) == 0);

    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {

        StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                        MemoryDescriptorList->ByteOffset);

        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               MemoryDescriptorList->ByteCount);

        PointerBase = MiGetPteAddress (BaseAddress);


        ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) != 0);


#if DBG
        PointerPte = PointerBase;
        i = NumberOfPages;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        while (i != 0) {
            ASSERT (PointerPte->u.Hard.Valid == 1);
            ASSERT (*Page == MI_GET_PAGE_FRAME_FROM_PTE (PointerPte));
            if ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) {
                PMMPFN Pfn3;
                Pfn3 = MI_PFN_ELEMENT (*Page);
                ASSERT (Pfn3->u3.e2.ReferenceCount != 0);
            }

            Page += 1;
            PointerPte += 1;
            i -= 1;
        }

#endif

        if (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) {
            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            Page += NumberOfPages;
            ASSERT (*Page <= MiCurrentAdvancedPages);
            NumberOfPages += *Page;
            PointerBase -= *Page;
#if DBG
            InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, 0 - *Page);
            MiAdvancesFreed += *Page;
#endif
        }

        if (MmTrackPtes & 0x1) {
            MiRemovePteTracker (MemoryDescriptorList,
                                PointerBase,
                                NumberOfPages);
        }

        MiReleaseSystemPtes (PointerBase, (ULONG)NumberOfPages, SystemPteSpace);

        MemoryDescriptorList->MdlFlags &= ~(MDL_MAPPED_TO_SYSTEM_VA |
                                            MDL_PARTIAL_HAS_BEEN_MAPPED |
                                            MDL_FREE_EXTRA_PTES);

        return;
    }

    MiUnmapLockedPagesInUserSpace (BaseAddress,
                                   MemoryDescriptorList);
}

VOID
MiUnmapLockedPagesInUserSpace (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    a MmMapLockedPages function.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if base address is within system
    space, APC_LEVEL or below if base address is in user space.

--*/

{
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif
    PVOID StartingVa;
    KIRQL OldIrql;
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PVOID TempVa;
    PEPROCESS Process;
    PMMPFN PageTablePfn;
    PFN_NUMBER PageTablePage;
    PVOID UsedPageTableHandle;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerPde = MiGetPdeAddress (BaseAddress);

    //
    // This was mapped into the user portion of the address space and
    // the corresponding virtual address descriptor must be deleted.
    //

    //
    // Get the working set mutex and address creation mutex.
    //

    Process = PsGetCurrentProcess ();

    LOCK_ADDRESS_SPACE (Process);

    Vad = MiLocateAddress (BaseAddress);

    if ((Vad == NULL) || (Vad->u.VadFlags.PhysicalMapping == 0)) {
        UNLOCK_ADDRESS_SPACE (Process);
        MmUnlockPagableImageSection(ExPageLockHandle);
        return;
    }

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);

    LOCK_WS_UNSAFE (Process);

    MiPhysicalViewRemover (Process, Vad);

    MiRemoveVad (Vad);

    //
    // Return commitment for page table pages if possible.
    //

    MiReturnPageTablePageCommitment (MI_VPN_TO_VA (Vad->StartingVpn),
                                     MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                     Process,
                                     PreviousVad,
                                     NextVad);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (BaseAddress);
    PageTablePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
    PageTablePfn = MI_PFN_ELEMENT (PageTablePage);

    //
    // Get the PFN lock so we can safely decrement share and valid
    // counts on page table pages.
    //

    LOCK_PFN (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        ASSERT64 (MiGetPdeAddress(PointerPte)->u.Hard.Valid == 1);
        ASSERT (MiGetPteAddress(PointerPte)->u.Hard.Valid == 1);
        ASSERT (PointerPte->u.Hard.Valid == 1);

        //
        // Another PTE is being zeroed.
        //

        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        KeFlushSingleTb (BaseAddress,
                         TRUE,
                         FALSE,
                         (PHARDWARE_PTE)PointerPte,
                         ZeroPte.u.Flush);

        MiDecrementShareCountInline (PageTablePfn, PageTablePage);

        PointerPte += 1;
        NumberOfPages -= 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        Page += 1;

        if ((MiIsPteOnPdeBoundary(PointerPte)) || (NumberOfPages == 0)) {

            PointerPde = MiGetPteAddress(PointerPte - 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.  Likewise
            // with the page directory and parent pages.
            //

            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                ASSERT (PointerPde->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 3)
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte (PointerPde);
                MiDeletePte (PointerPde,
                             TempVa,
                             FALSE,
                             Process,
                             NULL,
                             NULL);

#if (_MI_PAGING_LEVELS >= 3)
                if ((MiIsPteOnPpeBoundary(PointerPte)) || (NumberOfPages == 0)) {
    
                    PointerPpe = MiGetPteAddress (PointerPde);
                    ASSERT (PointerPpe->u.Hard.Valid == 1);
    
                    //
                    // If all the entries have been eliminated from the previous
                    // page directory page, delete the page directory page too.
                    //
    
                    if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                        ASSERT (PointerPpe->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                        TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     Process,
                                     NULL,
                                     NULL);

#if (_MI_PAGING_LEVELS >= 4)
                        if ((MiIsPteOnPxeBoundary(PointerPte)) || (NumberOfPages == 0)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            ASSERT (PointerPxe->u.Long != 0);
                            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                                TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                                MiDeletePte (PointerPxe,
                                             TempVa,
                                             FALSE,
                                             Process,
                                             NULL,
                                             NULL);
                            }
                        }
#endif    
                    }
                }
#endif
            }

            if (NumberOfPages == 0) {
                break;
            }

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (BaseAddress);
            PointerPde += 1;
            PageTablePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
            PageTablePfn = MI_PFN_ELEMENT (PageTablePage);
        }

    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);
    UNLOCK_WS_AND_ADDRESS_SPACE (Process);
    ExFreePool (Vad);
    MmUnlockPagableImageSection(ExPageLockHandle);
    return;
}


PVOID
MmMapIoSpace (
     IN PHYSICAL_ADDRESS PhysicalAddress,
     IN SIZE_T NumberOfBytes,
     IN MEMORY_CACHING_TYPE CacheType
     )

/*++

Routine Description:

    This function maps the specified physical address into the non-pagable
    portion of the system address space.

Arguments:

    PhysicalAddress - Supplies the starting physical address to map.

    NumberOfBytes - Supplies the number of bytes to map.

    CacheType - Supplies MmNonCached if the physical address is to be mapped
                as non-cached, MmCached if the address should be cached, and
                MmWriteCombined if the address should be cached and
                write-combined as a frame buffer which is to be used only by
                the video port driver.  All other callers should use
                MmUSWCCached.  MmUSWCCached is available only if the PAT
                feature is present and available.

                For I/O device registers, this is usually specified
                as MmNonCached.

Return Value:

    Returns the virtual address which maps the specified physical addresses.
    The value NULL is returned if sufficient virtual address space for
    the mapping could not be found.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    ULONG Hint;
    PMMPFN Pfn1;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPageFrameIndex;
    PMMPTE PointerPte;
    PVOID BaseVa;
    MMPTE TempPte;
    PMDL TempMdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PPTE_TRACKER Tracker;
    PVOID CallingAddress;
    PVOID CallersCaller;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    //
    // For compatibility for when CacheType used to be passed as a BOOLEAN
    // mask off the upper bits (TRUE == MmCached, FALSE == MmNonCached).
    //

    CacheType &= 0xFF;

    if (CacheType >= MmMaximumCacheType) {
        return NULL;
    }

    //
    // See if the first frame is in the PFN database and if so, they all must
    // be.
    //

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
    Pfn1 = NULL;
    IoMapping = 1;
    Hint = 0;

    if (MiIsPhysicalMemoryAddress (PageFrameIndex, &Hint, TRUE) == TRUE) {
        IoMapping = 0;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    }

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

#if !defined (_MI_MORE_THAN_4GB_)
    ASSERT (PhysicalAddress.HighPart == 0);
#endif

    ASSERT (NumberOfBytes != 0);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (PhysicalAddress.LowPart,
                                                    NumberOfBytes);

    if (CacheAttribute != MiCached) { 

        //
        // If a noncachable mapping is requested, none of the pages in the
        // requested MDL can reside in a large page.  Otherwise we would be
        // creating an incoherent overlapping TB entry as the same physical
        // page would be mapped by 2 different TB entries with different
        // cache attributes.
        //

        PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
        LastPageFrameIndex = PageFrameIndex + NumberOfPages;

        do {

            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                MiNonCachedCollisions += 1;
                return NULL;
            }

            PageFrameIndex += 1;

        } while (PageFrameIndex < LastPageFrameIndex);
    }

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {
        return NULL;
    }

    BaseVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);
    BaseVa = (PVOID)((PCHAR)BaseVa + BYTE_OFFSET(PhysicalAddress.LowPart));

    TempPte = ValidKernelPte;

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

#if defined(_X86_)

    //
    // Set the physical range to the proper caching type.  If the PAT feature
    // is supported, then we just use the caching type in the PTE.  Otherwise
    // modify the MTRRs if applicable.
    //
    // Note if the cache request is for cached or noncached, don't waste
    // an MTRR on this range because the PTEs can be encoded to provide
    // equivalent functionality.
    //

    if ((MiWriteCombiningPtes == FALSE) && (CacheAttribute == MiWriteCombined)) {

        //
        // If the address is an I/O space address, use MTRRs if possible.
        //

        NTSTATUS Status;

        //
        // If the address is a memory address, don't risk using MTRRs because
        // other pages in the range are likely mapped with differing attributes
        // in the TB and we must not add a conflicting range.
        //

        if (Pfn1 != NULL) {
            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            return NULL;
        }

        //
        // Since the attribute may have been overridden (due to a collision
        // with a prior exiting mapping), make sure the CacheType is also
        // consistent before editing the MTRRs.
        //

        CacheType = MmWriteCombined;

        Status = KeSetPhysicalCacheTypeRange (PhysicalAddress,
                                              NumberOfBytes,
                                              CacheType);

        if (!NT_SUCCESS(Status)) {

            //
            // There's still a problem, fail the request.
            //

            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            return NULL;
        }

        //
        // Override the write combine (weak UC) bits in the PTE and
        // instead use a cached attribute.  This is because the processor
        // will use the least cachable (ie: functionally safer) attribute
        // of the PTE & MTRR to use - so specifying fully cached for the PTE
        // ensures that the MTRR value will win out.
        //

        TempPte = ValidKernelPte;
    }
#endif

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
    ASSERT ((Pfn1 == MI_PFN_ELEMENT (PageFrameIndex)) || (Pfn1 == NULL));

    Hint = 0;
    OldIrql = HIGH_LEVEL;

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);
        if (Pfn1 != NULL) {

            ASSERT ((Pfn1->u3.e2.ReferenceCount != 0) ||
                    ((Pfn1->u3.e1.Rom == 1) && (CacheType == MmCached)));

            TempPte = ValidKernelPte;

            switch (Pfn1->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {

                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

#if defined (_MI_MORE_THAN_4GB_)
                    ASSERT ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)) ||
                            (Pfn1->u4.PteFrame == MI_MAGIC_4GB_RECLAIM));
#else
                    ASSERT ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
#endif
                    if (OldIrql == HIGH_LEVEL) {
                        LOCK_PFN2 (OldIrql);
                    }

                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn1->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn1->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn1->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
            Pfn1 += 1;
        }
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
        PageFrameIndex += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    if (OldIrql != HIGH_LEVEL) {
        UNLOCK_PFN2 (OldIrql);
    }

    MI_SWEEP_CACHE (CacheAttribute, BaseVa, NumberOfBytes);

    if (MmTrackPtes & 0x1) {

        //
        // First free any zombie blocks as no locks are being held.
        //

        Tracker = MiReleaseDeadPteTrackers ();

        if (Tracker != NULL) {

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            TempMdl = (PMDL) &MdlHack;
            TempMdl->MappedSystemVa = BaseVa;
            TempMdl->StartVa = (PVOID)(ULONG_PTR)PhysicalAddress.QuadPart;
            TempMdl->ByteOffset = BYTE_OFFSET(PhysicalAddress.LowPart);
            TempMdl->ByteCount = (ULONG)NumberOfBytes;
    
            MiInsertPteTracker (Tracker,
                                TempMdl,
                                ADDRESS_AND_SIZE_TO_SPAN_PAGES (PhysicalAddress.LowPart,
                                               NumberOfBytes),
                                CallingAddress,
                                CallersCaller);
        }
    }
    
    return BaseVa;
}

VOID
MmUnmapIoSpace (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     )

/*++

Routine Description:

    This function unmaps a range of physical address which were previously
    mapped via an MmMapIoSpace function call.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    NumberOfBytes - Supplies the number of bytes which were mapped.

Return Value:

    None.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    PFN_NUMBER NumberOfPages;
    PMMPTE FirstPte;

    ASSERT (NumberOfBytes != 0);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress, NumberOfBytes);
    FirstPte = MiGetPteAddress (BaseAddress);

    MiReleaseSystemPtes (FirstPte, (ULONG)NumberOfPages, SystemPteSpace);

    if (MmTrackPtes & 0x1) {
        MiRemovePteTracker (NULL, FirstPte, NumberOfPages);
    }

    return;
}

PVOID
MiAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MEMORY_CACHING_TYPE CacheType,
    PVOID CallingAddress
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-paged
    pool.  It relies on the fact that non-paged pool is built at
    system initialization time from a contiguous range of physical
    memory.  It allocates the specified size of non-paged pool and
    then checks to ensure it is contiguous as pool expansion does
    not maintain the contiguous nature of non-paged pool.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of physical memory for
    issuing DMA requests from.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    LowestAcceptablePfn - Supplies the lowest page frame number
                          which is valid for the allocation.

    HighestAcceptablePfn - Supplies the highest page frame number
                           which is valid for the allocation.

    BoundaryPfn - Supplies the page frame number multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    CacheType - Supplies the type of cache mapping that will be used for the
                memory.

    CallingAddress - Supplies the calling address of the allocator.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PVOID BaseAddress;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER LowestPfn;
    PFN_NUMBER HighestPfn;
    PFN_NUMBER i;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (NumberOfBytes != 0);

    LowestPfn = LowestAcceptablePfn;

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {
        if (HighestAcceptablePfn < MiNoLowMemory) {

            return MiAllocateLowMemory (NumberOfBytes,
                                        LowestAcceptablePfn,
                                        HighestAcceptablePfn,
                                        BoundaryPfn,
                                        CallingAddress,
                                        CacheType,
                                        'tnoC');
        }
        LowestPfn = MiNoLowMemory;
    }
#endif

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    //
    // N.B. This setting of SizeInPages to exactly the request size
    // means the non-NULL return value from MiCheckForContiguousMemory
    // is guaranteed to be the BaseAddress.  If this size is ever
    // changed, then the non-NULL return value must be checked and
    // split/returned accordingly.
    //

    SizeInPages = BYTES_TO_PAGES (NumberOfBytes);
    HighestPfn = HighestAcceptablePfn;

    if (CacheAttribute == MiCached) {

        BaseAddress = ExAllocatePoolWithTag (NonPagedPoolCacheAligned,
                                             NumberOfBytes,
                                             'mCmM');

        if (BaseAddress != NULL) {

            if (MiCheckForContiguousMemory (BaseAddress,
                                            SizeInPages,
                                            SizeInPages,
                                            LowestPfn,
                                            HighestPfn,
                                            BoundaryPfn,
                                            CacheAttribute)) {

                return BaseAddress;
            }

            //
            // The allocation from pool does not meet the contiguous
            // requirements.  Free the allocation and see if any of
            // the free pool pages do.
            //

            ExFreePool (BaseAddress);
        }
    }

    if (KeGetCurrentIrql() > APC_LEVEL) {
        return NULL;
    }

    BaseAddress = NULL;

    i = 3;

    InterlockedIncrement (&MiDelayPageFaults);

    do {

        BaseAddress = MiFindContiguousMemory (LowestPfn,
                                              HighestPfn,
                                              BoundaryPfn,
                                              SizeInPages,
                                              CacheType,
                                              CallingAddress);

        if ((BaseAddress != NULL) || (i == 0)) {
            break;
        }

        //
        // Attempt to move pages to the standby list.  This is done with
        // gradually increasing aggresiveness so as not to prematurely
        // drain modified writes unless it's truly needed.  This is because
        // the writing can be an expensive cost performance wise if drivers
        // are calling this routine every few seconds (and some really do).
        //

        switch (i) {

            case 3:
                MmEmptyAllWorkingSets ();
                break;

            case 2:
                MiFlushAllPages ();
                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&MmHalfSecond);
                break;

            default:
                MmEmptyAllWorkingSets ();
                MiFlushAllPages ();
                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&MmOneSecond);
                break;
        }

        i -= 1;

    } while (TRUE);

    InterlockedDecrement (&MiDelayPageFaults);

    return BaseAddress;
}


PVOID
MmAllocateContiguousMemorySpecifyCache (
    IN SIZE_T NumberOfBytes,
    IN PHYSICAL_ADDRESS LowestAcceptableAddress,
    IN PHYSICAL_ADDRESS HighestAcceptableAddress,
    IN PHYSICAL_ADDRESS BoundaryAddressMultiple OPTIONAL,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-cached,
    non-paged memory.  This is accomplished by using MmAllocateContiguousMemory
    which uses nonpaged pool virtual addresses to map the found memory chunk.

    Then this function establishes another map to the same physical addresses,
    but this alternate map is initialized as non-cached.  All references by
    our caller will be done through this alternate map.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of noncached physical memory for
    things like the AGP GART.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    LowestAcceptableAddress - Supplies the lowest physical address
                              which is valid for the allocation.  For
                              example, if the device can only reference
                              physical memory in the 8M to 16MB range, this
                              value would be set to 0x800000 (8Mb).

    HighestAcceptableAddress - Supplies the highest physical address
                               which is valid for the allocation.  For
                               example, if the device can only reference
                               physical memory below 16MB, this
                               value would be set to 0xFFFFFF (16Mb - 1).

    BoundaryAddressMultiple - Supplies the physical address multiple this
                              allocation must not cross.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PVOID BaseAddress;
    PFN_NUMBER LowestPfn;
    PFN_NUMBER HighestPfn;
    PFN_NUMBER BoundaryPfn;
    PVOID CallingAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);

    ASSERT (NumberOfBytes != 0);

    LowestPfn = (PFN_NUMBER)(LowestAcceptableAddress.QuadPart >> PAGE_SHIFT);
    if (BYTE_OFFSET(LowestAcceptableAddress.LowPart)) {
        LowestPfn += 1;
    }

    if (BYTE_OFFSET(BoundaryAddressMultiple.LowPart)) {
        return NULL;
    }

    BoundaryPfn = (PFN_NUMBER)(BoundaryAddressMultiple.QuadPart >> PAGE_SHIFT);

    HighestPfn = (PFN_NUMBER)(HighestAcceptableAddress.QuadPart >> PAGE_SHIFT);

    if (HighestPfn > MmHighestPossiblePhysicalPage) {
        HighestPfn = MmHighestPossiblePhysicalPage;
    }

    if (LowestPfn > HighestPfn) {

        //
        // The caller's range is beyond what physically exists, it cannot
        // succeed.  Bail now to avoid an expensive fruitless search.
        //

        return NULL;
    }

    BaseAddress = MiAllocateContiguousMemory (NumberOfBytes,
                                              LowestPfn,
                                              HighestPfn,
                                              BoundaryPfn,
                                              CacheType,
                                              CallingAddress);

    return BaseAddress;
}

PVOID
MmAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PHYSICAL_ADDRESS HighestAcceptableAddress
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-paged pool.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of physical memory for
    issuing DMA requests from.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    HighestAcceptableAddress - Supplies the highest physical address
                               which is valid for the allocation.  For
                               example, if the device can only reference
                               physical memory in the lower 16MB this
                               value would be set to 0xFFFFFF (16Mb - 1).

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PFN_NUMBER HighestPfn;
    PVOID CallingAddress;
    PVOID VirtualAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);

    HighestPfn = (PFN_NUMBER)(HighestAcceptableAddress.QuadPart >> PAGE_SHIFT);

    if (HighestPfn > MmHighestPossiblePhysicalPage) {
        HighestPfn = MmHighestPossiblePhysicalPage;
    }

    VirtualAddress = MiAllocateContiguousMemory (NumberOfBytes,
                                                 0,
                                                 HighestPfn,
                                                 0,
                                                 MmCached,
                                                 CallingAddress);
            
    return VirtualAddress;
}

#if defined (_WIN64)
#define SPECIAL_POOL_ADDRESS(p) \
        ((((p) >= MmSpecialPoolStart) && ((p) < MmSpecialPoolEnd)) || \
        (((p) >= MmSessionSpecialPoolStart) && ((p) < MmSessionSpecialPoolEnd)))
#else
#define SPECIAL_POOL_ADDRESS(p) \
        (((p) >= MmSpecialPoolStart) && ((p) < MmSpecialPoolEnd))
#endif


VOID
MmFreeContiguousMemory (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This function deallocates a range of physically contiguous non-paged
    pool which was allocated with the MmAllocateContiguousMemory function.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    ULONG SizeInPages;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPage;
    PMMPFN Pfn1;
    PMMPFN StartPfn;

    PAGED_CODE();

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {
        if (MiFreeLowMemory (BaseAddress, 'tnoC') == TRUE) {
            return;
        }
    }
#endif

    if (((BaseAddress >= MmNonPagedPoolStart) &&
        (BaseAddress < (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes))) ||

        ((BaseAddress >= MmNonPagedPoolExpansionStart) &&
        (BaseAddress < MmNonPagedPoolEnd)) ||

        (SPECIAL_POOL_ADDRESS(BaseAddress))) {

        ExFreePool (BaseAddress);
    }
    else {

        //
        // The contiguous memory being freed may be the target of a delayed
        // unlock.  Since these pages may be immediately released, force
        // any pending delayed actions to occur now.
        //

        MiDeferredUnlockPages (0);

        PointerPte = MiGetPteAddress (BaseAddress);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if (Pfn1->u3.e1.StartOfAllocation == 0) {
            KeBugCheckEx (BAD_POOL_CALLER,
                          0x60,
                          (ULONG_PTR)BaseAddress,
                          0,
                          0);
        }

        StartPfn = Pfn1;
        Pfn1->u3.e1.StartOfAllocation = 0;
        Pfn1 -= 1;

        do {
            Pfn1 += 1;
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->PteAddress == PointerPte);
            ASSERT (Pfn1->OriginalPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE);
            ASSERT (Pfn1->u4.PteFrame == MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPte)));
            ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);
            ASSERT (Pfn1->u4.VerifierAllocation == 0);
            ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
            ASSERT (Pfn1->u3.e1.PrototypePte == 0);
            MI_SET_PFN_DELETED(Pfn1);
            PointerPte += 1;

        } while (Pfn1->u3.e1.EndOfAllocation == 0);

        Pfn1->u3.e1.EndOfAllocation = 0;

        SizeInPages = (ULONG)(Pfn1 - StartPfn + 1);

        //
        // Notify deadlock verifier that a region that can contain locks
        // will become invalid.
        //

        if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
            VerifierDeadlockFreePool (BaseAddress, SizeInPages << PAGE_SHIFT);
        }

        //
        // Release the mapping.
        //

        MmUnmapIoSpace (BaseAddress, SizeInPages << PAGE_SHIFT);

        //
        // Release the actual pages.
        //

        LastPage = PageFrameIndex + SizeInPages;

        LOCK_PFN (OldIrql);

        do {
            MiDecrementShareCount (PageFrameIndex);
            PageFrameIndex += 1;
        } while (PageFrameIndex < LastPage);

        MmResidentAvailablePages += SizeInPages;
        MM_BUMP_COUNTER(20, SizeInPages);

        UNLOCK_PFN (OldIrql);

        MiReturnCommitment (SizeInPages);
    }
}


VOID
MmFreeContiguousMemorySpecifyCache (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function deallocates a range of noncached memory in
    the non-paged portion of the system address space.

Arguments:

    BaseAddress - Supplies the base virtual address where the noncached

    NumberOfBytes - Supplies the number of bytes allocated to the request.
                    This must be the same number that was obtained with
                    the MmAllocateContiguousMemorySpecifyCache call.

    CacheType - Supplies the cachetype used when the caller made the
                MmAllocateContiguousMemorySpecifyCache call.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    UNREFERENCED_PARAMETER (NumberOfBytes);
    UNREFERENCED_PARAMETER (CacheType);

    MmFreeContiguousMemory (BaseAddress);
}



PVOID
MmAllocateIndependentPages (
    IN SIZE_T NumberOfBytes,
    IN ULONG Node
    )

/*++

Routine Description:

    This function allocates a range of virtually contiguous nonpaged pages
    without using superpages.  This allows the caller to apply independent
    page protections to each page.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    Node - Supplies the preferred node number for the backing physical pages.
           If pages on the preferred node are not available, any page will
           be used.  -1 indicates no preferred node.

Return Value:

    The virtual address of the memory or NULL if none could be allocated.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    ULONG PageColor;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PVOID BaseAddress;
    KIRQL OldIrql;

    ASSERT ((Node == (ULONG)-1) || (Node < KeNumberNodes));

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {
        return NULL;
    }

    if (MiChargeCommitment (NumberOfPages, NULL) == FALSE) {
        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
        return NULL;
    }

    BaseAddress = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)NumberOfPages > MI_NONPAGABLE_MEMORY_AVAILABLE()) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (NumberOfPages);
        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_INDEPENDENT_PAGES, NumberOfPages);

    MmResidentAvailablePages -= NumberOfPages;
    MM_BUMP_COUNTER(28, NumberOfPages);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);
        MiEnsureAvailablePageOrWait (NULL, NULL);

        if (Node == (ULONG)-1) {
            PageColor = MI_GET_PAGE_COLOR_FROM_PTE (PointerPte);
        }
        else {
            PageColor = (((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask) |
                           (Node << MmSecondaryColorNodeShift));
        }

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           PointerPte);

        MI_SET_PTE_DIRTY (TempPte);
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        MiInitializePfn (PageFrameIndex, PointerPte, 1);

        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    return BaseAddress;
}

BOOLEAN
MmSetPageProtection (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function sets the specified virtual address range to the desired
    protection.  This assumes that the virtual addresses are backed by PTEs
    which can be set (ie: not in kseg0 or large pages).

Arguments:

    VirtualAddress - Supplies the start address to protect.

    NumberOfBytes - Supplies the number of bytes to set.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    TRUE if the protection was applied, FALSE if not.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PFN_NUMBER i;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE NewPteContents;
    KIRQL OldIrql;
    ULONG ProtectionMask;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {
        return FALSE;
    }

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return FALSE;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);
    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    LOCK_PFN (OldIrql);

    for (i = 0; i < NumberOfPages; i += 1) {
        TempPte.u.Long = PointerPte->u.Long;

        MI_MAKE_VALID_PTE (NewPteContents,
                           TempPte.u.Hard.PageFrameNumber,
                           ProtectionMask,
                           PointerPte);

        NewPteContents.u.Hard.Dirty = TempPte.u.Hard.Dirty;

        KeFlushSingleTb ((PVOID)((PUCHAR)VirtualAddress + (i << PAGE_SHIFT)),
                         TRUE,
                         TRUE,
                         (PHARDWARE_PTE)PointerPte,
                         NewPteContents.u.Flush);

        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MmFreeIndependentPages (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Returns pages previously allocated with MmAllocateIndependentPages.

Arguments:

    VirtualAddress - Supplies the virtual address to free.

    NumberOfBytes - Supplies the number of bytes to free.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE BasePte;
    PMMPFN Pfn1;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    PointerPte = MiGetPteAddress (VirtualAddress);
    BasePte = PointerPte;

    LOCK_PFN (OldIrql);

    for (i = 0; i < NumberOfPages; i += 1) {

        PteContents = *PointerPte;

        ASSERT (PteContents.u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (MI_GET_PAGE_FRAME_FROM_PTE (&PteContents));

        PointerPte += 1;
    }

    //
    // Update the count of resident available pages.
    //

    MmResidentAvailablePages += NumberOfPages;
    MM_BUMP_COUNTER(30, NumberOfPages);

    UNLOCK_PFN (OldIrql);

    //
    // Return PTEs and commitment.
    //

    MiReleaseSystemPtes (BasePte, (ULONG)NumberOfPages, SystemPteSpace);

    MiReturnCommitment (NumberOfPages);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_INDEPENDENT_PAGES, NumberOfPages);
}

PFN_NUMBER MiLastCallLowPage;
PFN_NUMBER MiLastCallHighPage;
ULONG MiLastCallColor;


PMDL
MmAllocatePagesForMdl (
    IN PHYSICAL_ADDRESS LowAddress,
    IN PHYSICAL_ADDRESS HighAddress,
    IN PHYSICAL_ADDRESS SkipBytes,
    IN SIZE_T TotalBytes
    )

/*++

Routine Description:

    This routine searches the PFN database for free, zeroed or standby pages
    to satisfy the request.  This does not map the pages - it just allocates
    them and puts them into an MDL.  It is expected that our caller will
    map the MDL as needed.

    NOTE: this routine may return an MDL mapping a smaller number of bytes
    than the amount requested.  It is the caller's responsibility to check the
    MDL upon return for the size actually allocated.

    These pages comprise physical non-paged memory and are zero-filled.

    This routine is designed to be used by an AGP driver to obtain physical
    memory in a specified range since hardware may provide substantial
    performance wins depending on where the backing memory is allocated.

    Because the caller may use these pages for a noncached mapping, care is
    taken to never allocate any pages that reside in a large page (in order
    to prevent TB incoherency of the same page being mapped by multiple
    translations with different attributes).

Arguments:

    LowAddress - Supplies the low physical address of the first range that
                 the allocated pages can come from.

    HighAddress - Supplies the high physical address of the first range that
                  the allocated pages can come from.

    SkipBytes - Number of bytes to skip (from the Low Address) to get to the
                next physical address range that allocated pages can come from.

    TotalBytes - Supplies the number of bytes to allocate.

Return Value:

    MDL - An MDL mapping a range of pages in the specified range.
          This may map less memory than the caller requested if the full amount
          is not currently available.

    NULL - No pages in the specified range OR not enough virtually contiguous
           nonpaged pool for the MDL is available at this time.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMDL MemoryDescriptorList;
    PMDL MemoryDescriptorList2;
    PMMPFN Pfn1;
    PMMPFN PfnNextColored;
    PMMPFN PfnNextFlink;
    PMMPFN PfnLastColored;
    KIRQL OldIrql;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER Page;
    PFN_NUMBER NextPage;
    PFN_NUMBER found;
    PFN_NUMBER BasePage;
    PFN_NUMBER LowPage;
    PFN_NUMBER HighPage;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER MdlPageSpan;
    PFN_NUMBER SkipPages;
    PFN_NUMBER MaxPages;
    PPFN_NUMBER MdlPage;
    PPFN_NUMBER LastMdlPage;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    MMLISTS MemoryList;
    PPFN_NUMBER ZeroRunStart[2];
    PPFN_NUMBER ZeroRunEnd[2];
    PFN_NUMBER LowPage1;
    PFN_NUMBER HighPage1;
    LOGICAL PagePlacementOk;
    PFN_NUMBER PageNextColored;
    PFN_NUMBER PageNextFlink;
    PFN_NUMBER PageLastColored;
    PMMPFNLIST ListHead;
    PPFN_NUMBER ColoredPagesLeftToScanBase;
    PPFN_NUMBER ColoredPagesLeftToScan;
    ULONG ColorHeadsDrained;
    ULONG RunsToZero;
    LOGICAL MoreNodePasses;
    ULONG ColorCount;
    ULONG BaseColor;
#if DBG
    ULONG FinishedCount;
#endif

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    //
    // The skip increment must be a page-size multiple.
    //

    if (BYTE_OFFSET(SkipBytes.LowPart)) {
        return NULL;
    }

    LowPage = (PFN_NUMBER)(LowAddress.QuadPart >> PAGE_SHIFT);
    HighPage = (PFN_NUMBER)(HighAddress.QuadPart >> PAGE_SHIFT);

    if (HighPage > MmHighestPossiblePhysicalPage) {
        HighPage = MmHighestPossiblePhysicalPage;
    }

    //
    // Maximum allocation size is constrained by the MDL ByteCount field.
    //

    if (TotalBytes > (SIZE_T)((ULONG)(MAXULONG - PAGE_SIZE))) {
        TotalBytes = (SIZE_T)((ULONG)(MAXULONG - PAGE_SIZE));
    }

    SizeInPages = (PFN_NUMBER)ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes);

    SkipPages = (PFN_NUMBER)(SkipBytes.QuadPart >> PAGE_SHIFT);

    BasePage = LowPage;

    //
    // Check without the PFN lock as the actual number of pages to get will
    // be recalculated later while holding the lock.
    //

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - 1024;

    if ((SPFN_NUMBER)MaxPages <= 0) {
        SizeInPages = 0;
    }
    else if (SizeInPages > MaxPages) {
        SizeInPages = MaxPages;
    }

    if (SizeInPages == 0) {
        return NULL;
    }

#if DBG
    if (SizeInPages < (PFN_NUMBER)ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes)) {
        if (MiPrintAwe != 0) {
            DbgPrint("MmAllocatePagesForMdl1: unable to get %p pages, trying for %p instead\n",
                ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes),
                SizeInPages);
        }
    }
#endif

    //
    // Allocate an MDL to return the pages in.
    //

    do {
        MemoryDescriptorList = MmCreateMdl (NULL,
                                            NULL,
                                            SizeInPages << PAGE_SHIFT);
    
        if (MemoryDescriptorList != NULL) {
            break;
        }
        SizeInPages -= (SizeInPages >> 4);
    } while (SizeInPages != 0);

    if (MemoryDescriptorList == NULL) {
        return NULL;
    }

    //
    // Ensure there is enough commit prior to allocating the pages.
    //

    if (MiChargeCommitment (SizeInPages, NULL) == FALSE) {
        ExFreePool (MemoryDescriptorList);
        return NULL;
    }

    //
    // Allocate a list of colored anchors.
    //

    ColoredPagesLeftToScanBase = (PPFN_NUMBER) ExAllocatePoolWithTag (NonPagedPool,
                                                            MmSecondaryColors * sizeof (PFN_NUMBER),
                                                            'ldmM');

    if (ColoredPagesLeftToScanBase == NULL) {
        ExFreePool (MemoryDescriptorList);
        MiReturnCommitment (SizeInPages);
        return NULL;
    }

    MdlPageSpan = SizeInPages;

    //
    // Recalculate the total size while holding the PFN lock.
    //

    start = 0;
    found = 0;

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    RunsToZero = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    LOCK_PFN (OldIrql);

    MiDeferredUnlockPages (MI_DEFER_PFN_HELD);

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - 1024;

    if ((SPFN_NUMBER)MaxPages <= 0) {
        SizeInPages = 0;
    }
    else if (SizeInPages > MaxPages) {
        SizeInPages = MaxPages;
    }

    //
    // Systems utilizing memory compression may have more pages on the zero,
    // free and standby lists than we want to give out.  Explicitly check
    // MmAvailablePages instead (and recheck whenever the PFN lock is released
    // and reacquired).
    //

    if (SizeInPages > MmAvailablePages) {
        SizeInPages = MmAvailablePages;
    }

    if (SizeInPages == 0) {
        UNLOCK_PFN (OldIrql);
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        MmUnlockPagableImageSection (ExPageLockHandle);
        ExFreePool (MemoryDescriptorList);
        MiReturnCommitment (MdlPageSpan);
        ExFreePool (ColoredPagesLeftToScanBase);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_MDL_PAGES, SizeInPages);

    if ((MiLastCallLowPage != LowPage) || (MiLastCallHighPage != HighPage)) {
        MiLastCallColor = 0;
    }

    MiLastCallLowPage = LowPage;
    MiLastCallHighPage = HighPage;

    //
    // Charge resident available pages now for all the pages so the PFN lock
    // can be released between the loops below.  Excess charging is returned
    // at the conclusion of the loops.
    //

    MmMdlPagesAllocated += SizeInPages;
    MmResidentAvailablePages -= SizeInPages;
    MM_BUMP_COUNTER(34, SizeInPages);

    do {
        //
        // Grab all zeroed (and then free) pages first directly from the
        // colored lists to avoid multiple walks down these singly linked lists.
        // Then snatch transition pages as needed.  In addition to optimizing
        // the speed of the removals this also avoids cannibalizing the page
        // cache unless it's absolutely needed.
        //

        MoreNodePasses = FALSE;
        ColorCount = MmSecondaryColors;
        BaseColor = 0;

#if defined(MI_MULTINODE) 

        if (KeNumberNodes > 1) {

            PKNODE Node;

            Node = KeGetCurrentNode();

            if ((Node->FreeCount[ZeroedPageList]) ||
                (Node->FreeCount[FreePageList])) {

                //
                // There are available pages on this node.  Restrict search.
                //

                MoreNodePasses = TRUE;
                ColorCount = MmSecondaryColorMask + 1;
                BaseColor = Node->MmShiftedColor;
                ASSERT(ColorCount == MmSecondaryColors / KeNumberNodes);
            }
        }

        do {

            //
            // Loop: 1st pass restricted to node, 2nd pass unrestricted.
            //

#endif

            MemoryList = ZeroedPageList;

            do {

                //
                // Scan the zero list and then the free list.
                //

                ASSERT (MemoryList <= FreePageList);

                ListHead = MmPageLocationList[MemoryList];

                //
                // Initialize the loop iteration controls.  Clearly pages
                // can be added or removed from the colored lists when we
                // deliberately drop the PFN lock below (just to be a good
                // citizen), but even if we never released the lock, we wouldn't
                // have scanned more than the colorhead count anyway, so
                // this is a much better way to go.
                //

                ColorHeadsDrained = 0;

                ColorHead = &MmFreePagesByColor[MemoryList][BaseColor];
                ColoredPagesLeftToScan = &ColoredPagesLeftToScanBase[BaseColor];
                for (Color = 0; Color < ColorCount; Color += 1) {
                    ASSERT (ColorHead->Count <= MmNumberOfPhysicalPages);
                    *ColoredPagesLeftToScan = ColorHead->Count;
                    if (ColorHead->Count == 0) {
                        ColorHeadsDrained += 1;
                    }
                    ColorHead += 1;
                    ColoredPagesLeftToScan += 1;
                }

                Color = MiLastCallColor;

#if defined(MI_MULTINODE)

                Color = (Color & MmSecondaryColorMask) | BaseColor;

#endif

                ASSERT (Color < MmSecondaryColors);
                do {

                    //
                    // Scan the current list by color.
                    //

                    ColorHead = &MmFreePagesByColor[MemoryList][Color];
                    ColoredPagesLeftToScan = &ColoredPagesLeftToScanBase[Color];
    
                    if (MoreNodePasses == FALSE) {

                        //
                        // Unrestricted search across all colors.
                        //

                        Color += 1;
                        if (Color >= MmSecondaryColors) {
                            Color = 0;
                        }
                    }

#if defined(MI_MULTINODE) 

                    else {

                        //
                        // Restrict first pass searches to current node.
                        //

                        Color = BaseColor |
                                ((Color + 1) & MmSecondaryColorMask);
                    }

#endif

                    if (*ColoredPagesLeftToScan == 0) {

                        //
                        // This colored list has already been completely
                        // searched.
                        //

                        continue;
                    }

                    if (ColorHead->Flink == MM_EMPTY_LIST) {

                        //
                        // This colored list is empty.
                        //

                        ColorHeadsDrained += 1;
                        *ColoredPagesLeftToScan = 0;
                        continue;
                    }

                    while (ColorHead->Flink != MM_EMPTY_LIST) {
    
                        ASSERT (*ColoredPagesLeftToScan != 0);
                        *ColoredPagesLeftToScan = *ColoredPagesLeftToScan - 1;
                        if (*ColoredPagesLeftToScan == 0) {
                            ColorHeadsDrained += 1;
                        }

                        Page = ColorHead->Flink;
        
                        Pfn1 = MI_PFN_ELEMENT(Page);

                        ASSERT ((MMLISTS)Pfn1->u3.e1.PageLocation == MemoryList);
    
                        //
                        // See if the page is within the caller's page constraints.
                        //

                        PagePlacementOk = FALSE;

                        LowPage1 = LowPage;
                        HighPage1 = HighPage;

                        do {
                            if (((Page >= LowPage1) && (Page <= HighPage1)) &&
                                (!MI_PAGE_FRAME_INDEX_MUST_BE_CACHED(Page))) {
                                PagePlacementOk = TRUE;
                                break;
                            }

                            if (SkipPages == 0) {
                                break;
                            }

                            LowPage1 += SkipPages;
                            HighPage1 += SkipPages;

                            if (LowPage1 > MmHighestPhysicalPage) {
                                break;
                            }
                            if (HighPage1 > MmHighestPhysicalPage) {
                                HighPage1 = MmHighestPhysicalPage;
                            }
                        } while (TRUE);
                
                        // 
                        // The Flink and Blink must be nonzero here for the page
                        // to be on the listhead.  Only code that scans the
                        // MmPhysicalMemoryBlock has to check for the zero case.
                        //

                        ASSERT (Pfn1->u1.Flink != 0);
                        ASSERT (Pfn1->u2.Blink != 0);

                        if (PagePlacementOk == FALSE) {

                            if (*ColoredPagesLeftToScan == 0) {

                                //
                                // No more pages to scan in this colored chain.
                                //

                                break;
                            }

                            //
                            // If the colored list has more than one entry then
                            // move this page to the end of this colored list.
                            //

                            PageNextColored = (PFN_NUMBER)Pfn1->OriginalPte.u.Long;

                            if (PageNextColored == MM_EMPTY_LIST) {

                                //
                                // No more pages in this colored chain.
                                //

                                *ColoredPagesLeftToScan = 0;
                                ColorHeadsDrained += 1;
                                break;
                            }

                            ASSERT (Pfn1->u1.Flink != 0);
                            ASSERT (Pfn1->u1.Flink != MM_EMPTY_LIST);
                            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                            PfnNextColored = MI_PFN_ELEMENT(PageNextColored);
                            ASSERT ((MMLISTS)PfnNextColored->u3.e1.PageLocation == MemoryList);
                            ASSERT (PfnNextColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                            //
                            // Adjust the free page list so Page
                            // follows PageNextFlink.
                            //

                            PageNextFlink = Pfn1->u1.Flink;
                            PfnNextFlink = MI_PFN_ELEMENT(PageNextFlink);

                            ASSERT ((MMLISTS)PfnNextFlink->u3.e1.PageLocation == MemoryList);
                            ASSERT (PfnNextFlink->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                            PfnLastColored = ColorHead->Blink;
                            ASSERT (PfnLastColored != (PMMPFN)MM_EMPTY_LIST);
                            ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                            ASSERT (PfnLastColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                            ASSERT (PfnLastColored->u2.Blink != MM_EMPTY_LIST);

                            ASSERT ((MMLISTS)PfnLastColored->u3.e1.PageLocation == MemoryList);
                            PageLastColored = PfnLastColored - MmPfnDatabase;

                            if (ListHead->Flink == Page) {

                                ASSERT (Pfn1->u2.Blink == MM_EMPTY_LIST);
                                ASSERT (ListHead->Blink != Page);

                                ListHead->Flink = PageNextFlink;

                                PfnNextFlink->u2.Blink = MM_EMPTY_LIST;
                            }
                            else {

                                ASSERT (Pfn1->u2.Blink != MM_EMPTY_LIST);
                                ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                                ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u3.e1.PageLocation == MemoryList);

                                MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink = PageNextFlink;
                                PfnNextFlink->u2.Blink = Pfn1->u2.Blink;
                            }

#if DBG
                            if (PfnLastColored->u1.Flink == MM_EMPTY_LIST) {
                                ASSERT (ListHead->Blink == PageLastColored);
                            }
#endif

                            Pfn1->u1.Flink = PfnLastColored->u1.Flink;
                            Pfn1->u2.Blink = PageLastColored;

                            if (ListHead->Blink == PageLastColored) {
                                ListHead->Blink = Page;
                            }

                            //
                            // Adjust the colored chains.
                            //

                            if (PfnLastColored->u1.Flink != MM_EMPTY_LIST) {
                                ASSERT (MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                                ASSERT ((MMLISTS)(MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u3.e1.PageLocation) == MemoryList);
                                MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u2.Blink = Page;
                            }

                            PfnLastColored->u1.Flink = Page;

                            ColorHead->Flink = PageNextColored;
                            Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

                            ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                            PfnLastColored->OriginalPte.u.Long = (ULONG)Page;
                            ColorHead->Blink = Pfn1;

                            continue;
                        }
    
                        found += 1;
                        ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
                        MiUnlinkFreeOrZeroedPage (Page);
    
                        Pfn1->u3.e2.ReferenceCount = 1;
                        Pfn1->u2.ShareCount = 1;
                        MI_SET_PFN_DELETED(Pfn1);
                        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                        Pfn1->u4.PteFrame = MI_MAGIC_AWE_PTEFRAME;
                        Pfn1->u3.e1.PageLocation = ActiveAndValid;
                        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
    
                        Pfn1->u3.e1.StartOfAllocation = 1;
                        Pfn1->u3.e1.EndOfAllocation = 1;
                        Pfn1->u4.VerifierAllocation = 0;
                        Pfn1->u3.e1.LargeSessionAllocation = 0;
    
                        *MdlPage = Page;
                        MdlPage += 1;
    
                        if (found == SizeInPages) {
    
                            //
                            // All the pages requested are available.
                            //

                            if (MemoryList == ZeroedPageList) {
                                MiLastCallColor = Color;
                            }
    
#if DBG
                            FinishedCount = 0;
                            for (Color = 0; Color < ColorCount; Color += 1) {
                                if (ColoredPagesLeftToScanBase[Color + BaseColor] == 0) {
                                    FinishedCount += 1;
                                }
                            }
                            ASSERT (FinishedCount == ColorHeadsDrained);
#endif

                            goto pass2_done;
                        }
    
                        //
                        // March on to the next colored chain so the overall
                        // allocation round-robins the page colors.
                        //

                        break;
                    }

                    //
                    // Release the PFN lock to give DPCs and other processors
                    // a chance to run.
                    //

                    UNLOCK_PFN (OldIrql);

                    LOCK_PFN (OldIrql);

                    //
                    // Systems utilizing memory compression may have more
                    // pages on the zero, free and standby lists than we
                    // want to give out.  Explicitly check MmAvailablePages
                    // instead (and recheck whenever the PFN lock is released
                    // and reacquired).
                    //

                    if (MmAvailablePages == 0) {
                        goto pass2_done;
                    }

                } while (ColorHeadsDrained != ColorCount);

                //
                // Release the PFN lock to give DPCs and other processors
                // a chance to run.  Nothing magic about the instructions
                // between the unlock and the relock.
                //

                UNLOCK_PFN (OldIrql);

#if DBG
                FinishedCount = 0;
                for (Color = 0; Color < ColorCount; Color += 1) {
                    if (ColoredPagesLeftToScanBase[Color + BaseColor] == 0) {
                        FinishedCount += 1;
                    }
                }
                ASSERT (FinishedCount == ColorHeadsDrained);
#endif

                if (MemoryList == ZeroedPageList) {
                    ZeroRunStart[RunsToZero] = MdlPage;
                    RunsToZero += 1;
                }
                else {
                    ZeroRunEnd[RunsToZero - 1] = MdlPage;
                }

                MemoryList += 1;
                if (MemoryList > FreePageList) {
                    break;
                }

                LOCK_PFN (OldIrql);

                //
                // Systems utilizing memory compression may have more
                // pages on the zero, free and standby lists than we
                // want to give out.  Explicitly check MmAvailablePages
                // instead (and recheck whenever the PFN lock is released
                // and reacquired).
                //

                if (MmAvailablePages == 0) {
                    goto pass2_done;
                }

                MiLastCallColor = 0;

            } while (TRUE);

#if defined(MI_MULTINODE)

            if (MoreNodePasses == FALSE) {
                break;
            }

            //
            // Expand range to all colors for next pass.
            //

            ColorCount = MmSecondaryColors;
            BaseColor = 0;
            MoreNodePasses = FALSE;

            LOCK_PFN (OldIrql);

            //
            // Systems utilizing memory compression may have more
            // pages on the zero, free and standby lists than we
            // want to give out.  Explicitly check MmAvailablePages
            // instead (and recheck whenever the PFN lock is released
            // and reacquired).
            //

            if (MmAvailablePages == 0) {
                goto pass2_done;
            }

        } while (TRUE);

#endif

        //
        // Walk the transition list looking for pages satisfying the
        // constraints as walking the physical memory block can be draining.
        //

        LOCK_PFN (OldIrql);

        count = MmStandbyPageListHead.Total;
        Page = MmStandbyPageListHead.Flink;

        while (count != 0) {

            LowPage1 = LowPage;
            HighPage1 = HighPage;

            PagePlacementOk = FALSE;
            Pfn1 = MI_PFN_ELEMENT (Page);

            do {
                if (((Page >= LowPage1) && (Page <= HighPage1)) &&
                    (!MI_PAGE_FRAME_INDEX_MUST_BE_CACHED(Page))) {

                    ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

                    //
                    // Systems utilizing memory compression may have more
                    // pages on the zero, free and standby lists than we
                    // want to give out.  Explicitly check MmAvailablePages
                    // instead (and recheck whenever the PFN lock is released
                    // and reacquired).
                    //

                    if (MmAvailablePages == 0) {
                        goto pass2_done;
                    }

                    found += 1;

                    //
                    // This page is in the desired range - grab it.
                    //

                    NextPage = Pfn1->u1.Flink;
                    MiUnlinkPageFromList (Pfn1);
                    MiRestoreTransitionPte (Page);

                    Pfn1->u3.e2.ReferenceCount = 1;
                    Pfn1->u2.ShareCount = 1;
                    MI_SET_PFN_DELETED(Pfn1);
                    Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                    Pfn1->u4.PteFrame = MI_MAGIC_AWE_PTEFRAME;
                    Pfn1->u3.e1.PageLocation = ActiveAndValid;
                    ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
                    Pfn1->u3.e1.StartOfAllocation = 1;
                    Pfn1->u3.e1.EndOfAllocation = 1;
                    Pfn1->u4.VerifierAllocation = 0;
                    Pfn1->u3.e1.LargeSessionAllocation = 0;

                    *MdlPage = Page;
                    MdlPage += 1;

                    if (found == SizeInPages) {

                        //
                        // All the pages requested are available.
                        //

                        goto pass2_done;
                    }
                    PagePlacementOk = TRUE;
                    Page = NextPage;
                    break;
                }

                if (SkipPages == 0) {
                    break;
                }

                LowPage1 += SkipPages;
                HighPage1 += SkipPages;

                if (LowPage1 > MmHighestPhysicalPage) {
                    break;
                }
                if (HighPage1 > MmHighestPhysicalPage) {
                    HighPage1 = MmHighestPhysicalPage;
                }
            } while (TRUE);
            
            if (PagePlacementOk == FALSE) {
                Page = Pfn1->u1.Flink;
            }
            count -= 1;
        }

        UNLOCK_PFN (OldIrql);

        if (SkipPages == 0) {
            LOCK_PFN (OldIrql);
            break;
        }
        LowPage += SkipPages;
        HighPage += SkipPages;
        if (LowPage > MmHighestPhysicalPage) {
            LOCK_PFN (OldIrql);
            break;
        }
        if (HighPage > MmHighestPhysicalPage) {
            HighPage = MmHighestPhysicalPage;
        }

        //
        // Reinitialize the zeroed list variable in preparation
        // for another loop.
        //

        MemoryList = ZeroedPageList;
    
        LOCK_PFN (OldIrql);

        //
        // Systems utilizing memory compression may have more
        // pages on the zero, free and standby lists than we
        // want to give out.  Explicitly check MmAvailablePages
        // instead (and recheck whenever the PFN lock is released
        // and reacquired).
        //

    } while (MmAvailablePages != 0);

pass2_done:

    //
    // The full amount was charged up front - remove any excess now.
    //

    MmMdlPagesAllocated -= (SizeInPages - found);
    MmResidentAvailablePages += (SizeInPages - found);
    MM_BUMP_COUNTER(38, SizeInPages - found);

    UNLOCK_PFN (OldIrql);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);
    MmUnlockPagableImageSection (ExPageLockHandle);

    ExFreePool (ColoredPagesLeftToScanBase);

    if (found != MdlPageSpan) {
        ASSERT (found < MdlPageSpan);
        MiReturnCommitment (MdlPageSpan - found);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_AWE_EXCESS, MdlPageSpan - found);
    }

    if (found == 0) {
        ExFreePool (MemoryDescriptorList);
        return NULL;
    }

    MemoryDescriptorList->ByteCount = (ULONG)(found << PAGE_SHIFT);

    if (found != SizeInPages) {
        *MdlPage = MM_EMPTY_LIST;
    }

    //
    // If the number of pages allocated was substantially less than the
    // initial request amount, attempt to allocate a smaller MDL to save
    // pool.
    //

    if ((MdlPageSpan - found) > ((4 * PAGE_SIZE) / sizeof (PFN_NUMBER))) {
        MemoryDescriptorList2 = MmCreateMdl ((PMDL)0,
                                             (PVOID)0,
                                             found << PAGE_SHIFT);
    
        if (MemoryDescriptorList2 != (PMDL)0) {

            ULONG n;
            PFN_NUMBER Diff;

            RtlCopyMemory ((PVOID)(MemoryDescriptorList2 + 1),
                           (PVOID)(MemoryDescriptorList + 1),
                           found * sizeof (PFN_NUMBER));

            Diff = (PPFN_NUMBER)(MemoryDescriptorList2 + 1) -
                   (PPFN_NUMBER)(MemoryDescriptorList + 1);

            for (n = 0; n < RunsToZero; n += 1) {
                ZeroRunStart[n] += Diff;
                ZeroRunEnd[n] += Diff;
            }
            ExFreePool (MemoryDescriptorList);
            MemoryDescriptorList = MemoryDescriptorList2;
        }
    }

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    LastMdlPage = MdlPage + found;

#if DBG
    //
    // Ensure all pages are within the caller's page constraints.
    //

    LowPage = (PFN_NUMBER)(LowAddress.QuadPart >> PAGE_SHIFT);
    HighPage = (PFN_NUMBER)(HighAddress.QuadPart >> PAGE_SHIFT);

    while (MdlPage < LastMdlPage) {
        Page = *MdlPage;
        PagePlacementOk = FALSE;
        LowPage1 = LowPage;
        HighPage1 = HighPage;

        do {
            if ((Page >= LowPage1) && (Page <= HighPage1)) {
                PagePlacementOk = TRUE;
                break;
            }

            if (SkipPages == 0) {
                break;
            }

            LowPage1 += SkipPages;
            HighPage1 += SkipPages;

            if (LowPage1 > MmHighestPhysicalPage) {
                break;
            }
            if (HighPage1 > MmHighestPhysicalPage) {
                HighPage1 = MmHighestPhysicalPage;
            }
        } while (TRUE);

        ASSERT (PagePlacementOk == TRUE);
        Pfn1 = MI_PFN_ELEMENT(*MdlPage);
        ASSERT (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME);
        MdlPage += 1;
    }
    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT(RunsToZero <= 2);

#endif

    //
    // Zero any pages that were allocated from the free or standby lists.
    //

    if (RunsToZero) {

        //
        // Lengthen the last run to include the standby pages.
        //

        ZeroRunEnd[RunsToZero - 1] = LastMdlPage;

        while (RunsToZero != 0) {
            RunsToZero -= 1;
            for (MdlPage = ZeroRunStart[RunsToZero];
                 MdlPage < ZeroRunEnd[RunsToZero];
                 MdlPage += 1) {

                MiZeroPhysicalPage (*MdlPage, 0);
            }
        }
    }

    //
    // Mark the MDL's pages as locked so the the kernelmode caller can
    // map the MDL using MmMapLockedPages* without asserting.
    //

    MemoryDescriptorList->MdlFlags |= MDL_PAGES_LOCKED;

    return MemoryDescriptorList;
}


VOID
MmFreePagesFromMdl (
    IN PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine walks the argument MDL freeing each physical page back to
    the PFN database.  This is designed to free pages acquired via
    MmAllocatePagesForMdl only.

Arguments:

    MemoryDescriptorList - Supplies an MDL which contains the pages to be freed.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PVOID StartingAddress;
    PVOID AlignedVa;
    PPFN_NUMBER Page;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER TotalPages;
    PFN_NUMBER DeltaPages;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

    ASSERT (((ULONG_PTR)MemoryDescriptorList->StartVa & (PAGE_SIZE - 1)) == 0);
    AlignedVa = (PVOID)MemoryDescriptorList->StartVa;

    StartingAddress = (PVOID)((PCHAR)AlignedVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingAddress,
                                              MemoryDescriptorList->ByteCount);

    TotalPages = NumberOfPages;

    //
    // Notify deadlock verifier that a region that can contain locks
    // will become invalid.
    //

    if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
        VerifierDeadlockFreePool (StartingAddress, TotalPages << PAGE_SHIFT);
    }

    MI_MAKING_MULTIPLE_PTES_INVALID (TRUE);

    LOCK_PFN (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {

            //
            // There are no more locked pages.
            //

            break;
        }

        ASSERT (*Page <= MmHighestPhysicalPage);

        Pfn1 = MI_PFN_ELEMENT (*Page);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (MI_IS_PFN_DELETED (Pfn1) == TRUE);
        ASSERT (MI_PFN_IS_AWE (Pfn1) == TRUE);
        ASSERT (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME);

        Pfn1->u3.e1.StartOfAllocation = 0;
        Pfn1->u3.e1.EndOfAllocation = 0;
        Pfn1->u2.ShareCount = 0;
#if DBG
        Pfn1->u4.PteFrame -= 1;
        Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif

        MiDecrementReferenceCountInline (Pfn1, *Page);

        *Page++ = MM_EMPTY_LIST;

        //
        // Nothing magic about the divisor here - just releasing the PFN lock
        // periodically to allow other processors and DPCs a chance to execute.
        //

        if ((NumberOfPages & 0xFF) == 0) {

            DeltaPages = TotalPages - NumberOfPages;
            MmMdlPagesAllocated -= DeltaPages;
            MmResidentAvailablePages += DeltaPages;
            MM_BUMP_COUNTER(35, DeltaPages);

            UNLOCK_PFN (OldIrql);

            MiReturnCommitment (DeltaPages);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_MDL_PAGES, DeltaPages);

            TotalPages -= DeltaPages;

            LOCK_PFN (OldIrql);
        }

        NumberOfPages -= 1;

    } while (NumberOfPages != 0);

    MmMdlPagesAllocated -= TotalPages;
    MmResidentAvailablePages += TotalPages;
    MM_BUMP_COUNTER(35, TotalPages);

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);

    MiReturnCommitment (TotalPages);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_MDL_PAGES, TotalPages);

    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
}


NTSTATUS
MmMapUserAddressesToPage (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes,
    IN PVOID PageAddress
    )

/*++

Routine Description:

    This function maps a range of addresses in a physical memory VAD to the
    specified page address.  This is typically used by a driver to nicely
    remove an application's access to things like video memory when the
    application is not responding to requests to relinquish it.

    Note the entire range must be currently mapped (ie, all the PTEs must
    be valid) by the caller.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address is mapped.

    NumberOfBytes - Supplies the number of bytes to remap to the new address.

    PageAddress - Supplies the virtual address of the page this is remapped to.
                  This must be nonpaged memory.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMVAD Vad;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PMMPTE LastPte;
    PEPROCESS Process;
    NTSTATUS Status;
    PVOID EndingAddress;
    PFN_NUMBER PageFrameNumber;
    SIZE_T NumberOfPtes;
    PHYSICAL_ADDRESS PhysicalAddress;
    KIRQL OldIrql;

    PAGED_CODE();

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if ((ULONG_PTR)BaseAddress + NumberOfBytes > (ULONG64)MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER_2;
    }

    Process = PsGetCurrentProcess();

    EndingAddress = (PVOID)((PCHAR)BaseAddress + NumberOfBytes - 1);

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    Vad = (PMMVAD)MiLocateAddress (BaseAddress);

    if (Vad == NULL) {

        //
        // No virtual address descriptor located.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    if (NumberOfBytes == 0) {

        //
        // If the region size is specified as 0, the base address
        // must be the starting address for the region.  The entire VAD
        // will then be repointed.
        //

        if (MI_VA_TO_VPN (BaseAddress) != Vad->StartingVpn) {
            Status = STATUS_FREE_VM_NOT_AT_BASE;
            goto ErrorReturn;
        }

        BaseAddress = MI_VPN_TO_VA (Vad->StartingVpn);
        EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);
        NumberOfBytes = (PCHAR)EndingAddress - (PCHAR)BaseAddress + 1;
    }

    //
    // Found the associated virtual address descriptor.
    //

    if (Vad->EndingVpn < MI_VA_TO_VPN (EndingAddress)) {

        //
        // The entire range to remap is not contained within a single
        // virtual address descriptor.  Return an error.
        //

        Status = STATUS_INVALID_PARAMETER_2;
        goto ErrorReturn;
    }

    if (Vad->u.VadFlags.PhysicalMapping == 0) {

        //
        // The virtual address descriptor is not a physical mapping.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    PointerPte = MiGetPteAddress (BaseAddress);
    LastPte = MiGetPteAddress (EndingAddress);
    NumberOfPtes = LastPte - PointerPte + 1;

    //
    // Lock down because the PFN lock is going to be acquired shortly.
    //

    MmLockPagableSectionByHandle(ExPageLockHandle);

    LOCK_WS_UNSAFE (Process);

    PhysicalAddress = MmGetPhysicalAddress (PageAddress);
    PageFrameNumber = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    PteContents = *PointerPte;
    PteContents.u.Hard.PageFrameNumber = PageFrameNumber;

#if DBG

    //
    // All the PTEs must be valid or the filling will corrupt the
    // UsedPageTableCounts.
    //

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PointerPte += 1;
    } while (PointerPte < LastPte);
    PointerPte = MiGetPteAddress (BaseAddress);
#endif

    //
    // Fill the PTEs and flush at the end - no race here because it doesn't
    // matter whether the user app sees the old or the new data until we
    // return (writes going to either page is acceptable prior to return
    // from this function).  There is no race with I/O and ProbeAndLockPages
    // because the PFN lock is acquired here.
    //

    LOCK_PFN (OldIrql);

#if !defined (_X86PAE_)
    MiFillMemoryPte (PointerPte,
                     NumberOfPtes * sizeof (MMPTE),
                     PteContents.u.Long);
#else

    //
    // Note that the PAE architecture must very carefully fill these PTEs.
    //

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PointerPte += 1;
        (VOID)KeInterlockedSwapPte ((PHARDWARE_PTE)PointerPte,
                                    (PHARDWARE_PTE)&PteContents);
    } while (PointerPte < LastPte);
    PointerPte = MiGetPteAddress (BaseAddress);

#endif

    if (NumberOfPtes == 1) {

        (VOID)KeFlushSingleTb (BaseAddress,
                               TRUE,
                               TRUE,
                               (PHARDWARE_PTE)PointerPte,
                               PteContents.u.Flush);
    }
    else {
        KeFlushEntireTb (TRUE, TRUE);
    }

    UNLOCK_PFN (OldIrql);

    UNLOCK_WS_UNSAFE (Process);

    MmUnlockPagableImageSection (ExPageLockHandle);

    Status = STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);

    return Status;
}


PHYSICAL_ADDRESS
MmGetPhysicalAddress (
     IN PVOID BaseAddress
     )

/*++

Routine Description:

    This function returns the corresponding physical address for a
    valid virtual address.

Arguments:

    BaseAddress - Supplies the virtual address for which to return the
                  physical address.

Return Value:

    Returns the corresponding physical address.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    PMMPTE PointerPte;
    PHYSICAL_ADDRESS PhysicalAddress;

    if (MI_IS_PHYSICAL_ADDRESS(BaseAddress)) {
        PhysicalAddress.QuadPart = MI_CONVERT_PHYSICAL_TO_PFN (BaseAddress);
    }
    else {

        PointerPte = MiGetPdeAddress (BaseAddress);
        if (PointerPte->u.Hard.Valid == 0) {
            KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                      BaseAddress));
            ZERO_LARGE (PhysicalAddress);
            return PhysicalAddress;
        }

        PointerPte = MiGetPteAddress(BaseAddress);

        if (PointerPte->u.Hard.Valid == 0) {
            KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                      BaseAddress));
            ZERO_LARGE (PhysicalAddress);
            return PhysicalAddress;
        }
        PhysicalAddress.QuadPart = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    }

    PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    PhysicalAddress.LowPart += BYTE_OFFSET(BaseAddress);

    return PhysicalAddress;
}

PVOID
MmGetVirtualForPhysical (
    IN PHYSICAL_ADDRESS PhysicalAddress
     )

/*++

Routine Description:

    This function returns the corresponding virtual address for a physical
    address whose primary virtual address is in system space.

Arguments:

    PhysicalAddress - Supplies the physical address for which to return the
                  virtual address.

Return Value:

    Returns the corresponding virtual address.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    Pfn = MI_PFN_ELEMENT (PageFrameIndex);

    return (PVOID)((PCHAR)MiGetVirtualAddressMappedByPte (Pfn->PteAddress) +
                    BYTE_OFFSET (PhysicalAddress.LowPart));
}

//
// Nonpaged helper routine.
//

VOID
MiMarkMdlPageAttributes (
    IN PMDL Mdl,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )
{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER Page;

    Page = (PPFN_NUMBER)(Mdl + 1);

    do {
        PageFrameIndex = *Page;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        Pfn1->u3.e1.CacheAttribute = CacheAttribute;

        Page += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);
}


PVOID
MmAllocateNonCachedMemory (
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This function allocates a range of noncached memory in
    the non-paged portion of the system address space.

    This routine is designed to be used by a driver's initialization
    routine to allocate a noncached block of virtual memory for
    various device specific buffers.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

Return Value:

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

    NULL - The specified request could not be satisfied.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    MMPTE TempPte;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMDL Mdl;
    PVOID BaseAddress;
    PHYSICAL_ADDRESS LowAddress;
    PHYSICAL_ADDRESS HighAddress;
    PHYSICAL_ADDRESS SkipBytes;
    PFN_NUMBER NumberOfPagesAllocated;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (NumberOfBytes != 0);

#if defined (_WIN64)

    //
    // Maximum allocation size is constrained by the MDL ByteCount field.
    //

    if (NumberOfBytes >= _4gb) {
        return NULL;
    }

#endif

    NumberOfPages = BYTES_TO_PAGES(NumberOfBytes);

    //
    // Even though an MDL is not needed per se, it is much more convenient
    // to use the routine below because it checks for things like appropriate
    // cachability of the pages, etc.  Note that the MDL returned may map
    // fewer pages than requested - check for this and if so, return NULL.
    //

    LowAddress.QuadPart = 0;
    HighAddress.QuadPart = (ULONGLONG)-1;
    SkipBytes.QuadPart = 0;

    Mdl = MmAllocatePagesForMdl (LowAddress,
                                 HighAddress,
                                 SkipBytes,
                                 NumberOfBytes);
    if (Mdl == NULL) {
        return NULL;
    }

    BaseAddress = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    NumberOfPagesAllocated = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress, Mdl->ByteCount);

    if (NumberOfPages != NumberOfPagesAllocated) {
        ASSERT (NumberOfPages > NumberOfPagesAllocated);
        MmFreePagesFromMdl (Mdl);
        ExFreePool (Mdl);
        return NULL;
    }

    //
    // Obtain enough virtual space to map the pages.  Add an extra PTE so the
    // MDL can be stashed now and retrieved on release.
    //

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages + 1, SystemPteSpace);

    if (PointerPte == NULL) {
        MmFreePagesFromMdl (Mdl);
        ExFreePool (Mdl);
        return NULL;
    }

    *(PMDL *)PointerPte = Mdl;
    PointerPte += 1;

    BaseAddress = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);

    Page = (PPFN_NUMBER)(Mdl + 1);

    MI_MAKE_VALID_PTE (TempPte,
                       0,
                       MM_READWRITE,
                       PointerPte);

    MI_SET_PTE_DIRTY (TempPte);

    CacheAttribute = MI_TRANSLATE_CACHETYPE (MmNonCached, FALSE);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);
        PageFrameIndex = *Page;

        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    MI_SWEEP_CACHE (CacheAttribute, BaseAddress, NumberOfBytes);

    MiMarkMdlPageAttributes (Mdl, NumberOfPagesAllocated, CacheAttribute);

    return BaseAddress;
}

VOID
MmFreeNonCachedMemory (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This function deallocates a range of noncached memory in
    the non-paged portion of the system address space.

Arguments:

    BaseAddress - Supplies the base virtual address where the noncached
                  memory resides.

    NumberOfBytes - Supplies the number of bytes allocated to the request.
                    This must be the same number that was obtained with
                    the MmAllocateNonCachedMemory call.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMDL Mdl;
    PMMPTE PointerPte;
    PFN_NUMBER NumberOfPages;
#if DBG
    PFN_NUMBER i;
    PVOID StartingAddress;
#endif

    ASSERT (NumberOfBytes != 0);
    ASSERT (PAGE_ALIGN (BaseAddress) == BaseAddress);

    MI_MAKING_MULTIPLE_PTES_INVALID (TRUE);

    NumberOfPages = BYTES_TO_PAGES(NumberOfBytes);

    PointerPte = MiGetPteAddress (BaseAddress);

    Mdl = *(PMDL *)(PointerPte - 1);

#if DBG
    StartingAddress = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    i = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingAddress, Mdl->ByteCount);

    ASSERT (NumberOfPages == i);
#endif

    MmFreePagesFromMdl (Mdl);

    ExFreePool (Mdl);

    MiReleaseSystemPtes (PointerPte - 1,
                         (ULONG)NumberOfPages + 1,
                         SystemPteSpace);

    return;
}

SIZE_T
MmSizeOfMdl (
    IN PVOID Base,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function returns the number of bytes required for an MDL for a
    given buffer and size.

Arguments:

    Base - Supplies the base virtual address for the buffer.

    Length - Supplies the size of the buffer in bytes.

Return Value:

    Returns the number of bytes required to contain the MDL.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    return( sizeof( MDL ) +
                (ADDRESS_AND_SIZE_TO_SPAN_PAGES( Base, Length ) *
                 sizeof( PFN_NUMBER ))
          );
}


PMDL
MmCreateMdl (
    IN PMDL MemoryDescriptorList OPTIONAL,
    IN PVOID Base,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function optionally allocates and initializes an MDL.

Arguments:

    MemoryDescriptorList - Optionally supplies the address of the MDL
                           to initialize.  If this address is supplied as NULL
                           an MDL is allocated from non-paged pool and
                           initialized.

    Base - Supplies the base virtual address for the buffer.

    Length - Supplies the size of the buffer in bytes.

Return Value:

    Returns the address of the initialized MDL.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    SIZE_T MdlSize;

#if defined (_WIN64)
    //
    // Since the Length has to fit into the MDL's ByteCount field, ensure it
    // doesn't wrap on 64-bit systems.
    //

    if (Length >= _4gb) {
        return NULL;
    }
#endif

    MdlSize = MmSizeOfMdl (Base, Length);

    if (!ARGUMENT_PRESENT(MemoryDescriptorList)) {

        MemoryDescriptorList = (PMDL)ExAllocatePoolWithTag (NonPagedPool,
                                                            MdlSize,
                                                            'ldmM');
        if (MemoryDescriptorList == (PMDL)0) {
            return NULL;
        }
    }

    MmInitializeMdl (MemoryDescriptorList, Base, Length);
    return MemoryDescriptorList;
}

BOOLEAN
MmSetAddressRangeModified (
    IN PVOID Address,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This routine sets the modified bit in the PFN database for the
    pages that correspond to the specified address range.

    Note that the dirty bit in the PTE is cleared by this operation.

Arguments:

    Address - Supplies the address of the start of the range.  This
              range must reside within the system cache.

    Length - Supplies the length of the range.

Return Value:

    TRUE if at least one PTE was dirty in the range, FALSE otherwise.

Environment:

    Kernel mode.  APC_LEVEL and below for pagable addresses,
                  DISPATCH_LEVEL and below for non-pagable addresses.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    PMMPTE FlushPte;
    MMPTE PteContents;
    MMPTE FlushContents;
    KIRQL OldIrql;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    ULONG Count;
    BOOLEAN Result;

    Count = 0;
    Result = FALSE;

    //
    // Initializing Flush* is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    FlushPte = NULL;
    FlushContents = ZeroPte;

    //
    // Loop on the copy on write case until the page is only
    // writable.
    //

    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + Length - 1));

    LOCK_PFN2 (OldIrql);

    do {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            MI_SET_MODIFIED (Pfn1, 1, 0x5);

            if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                         (Pfn1->u3.e1.WriteInProgress == 0)) {
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
            }

#ifdef NT_UP
            //
            // On uniprocessor systems no need to flush if this processor
            // doesn't think the PTE is dirty.
            //

            if (MI_IS_PTE_DIRTY (PteContents)) {
                Result = TRUE;
#else  //NT_UP
                Result |= (BOOLEAN)(MI_IS_PTE_DIRTY (PteContents));
#endif //NT_UP
                MI_SET_PTE_CLEAN (PteContents);
                MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);
                FlushContents = PteContents;
                FlushPte = PointerPte;

                //
                // Clear the write bit in the PTE so new writes can be tracked.
                //

                if (Count != MM_MAXIMUM_FLUSH_COUNT) {
                    VaFlushList[Count] = Address;
                    Count += 1;
                }
#ifdef NT_UP
            }
#endif //NT_UP
        }
        PointerPte += 1;
        Address = (PVOID)((PCHAR)Address + PAGE_SIZE);
    } while (PointerPte <= LastPte);

    if (Count != 0) {
        if (Count == 1) {

            (VOID)KeFlushSingleTb (VaFlushList[0],
                                   FALSE,
                                   TRUE,
                                   (PHARDWARE_PTE)FlushPte,
                                   FlushContents.u.Flush);

        }
        else if (Count != MM_MAXIMUM_FLUSH_COUNT) {

            KeFlushMultipleTb (Count,
                               &VaFlushList[0],
                               FALSE,
                               TRUE,
                               NULL,
                               *(PHARDWARE_PTE)&ZeroPte.u.Flush);

        }
        else {
            KeFlushEntireTb (FALSE, TRUE);
        }
    }
    UNLOCK_PFN2 (OldIrql);
    return Result;
}


PVOID
MiCheckForContiguousMemory (
    IN PVOID BaseAddress,
    IN PFN_NUMBER BaseAddressPages,
    IN PFN_NUMBER SizeInPages,
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )

/*++

Routine Description:

    This routine checks to see if the physical memory mapped
    by the specified BaseAddress for the specified size is
    contiguous and that the first page is greater than or equal to
    the specified LowestPfn and that the last page of the physical memory is
    less than or equal to the specified HighestPfn.

Arguments:

    BaseAddress - Supplies the base address to start checking at.

    BaseAddressPages - Supplies the number of pages to scan from the
                       BaseAddress.

    SizeInPages - Supplies the number of pages in the range.

    LowestPfn - Supplies lowest PFN acceptable as a physical page.

    HighestPfn - Supplies the highest PFN acceptable as a physical page.

    BoundaryPfn - Supplies the PFN multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    CacheAttribute - Supplies the type of cache mapping that will be used
                     for the memory.

Return Value:

    Returns the usable virtual address within the argument range that the
    caller should return to his caller.  NULL if there is no usable address.

Environment:

    Kernel mode, memory management internal.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER PreviousPage;
    PFN_NUMBER Page;
    PFN_NUMBER HighestStartPage;
    PFN_NUMBER LastPage;
    PFN_NUMBER OriginalPage;
    PFN_NUMBER OriginalLastPage;
    PVOID BoundaryAllocation;
    PFN_NUMBER BoundaryMask;
    ULONG PageCount;
    MMPTE PteContents;

    BoundaryMask = ~(BoundaryPfn - 1);

    if (LowestPfn > HighestPfn) {
        return NULL;
    }

    if (LowestPfn + SizeInPages <= LowestPfn) {
        return NULL;
    }

    if (LowestPfn + SizeInPages - 1 > HighestPfn) {
        return NULL;
    }

    if (BaseAddressPages < SizeInPages) {
        return NULL;
    }

    if (MI_IS_PHYSICAL_ADDRESS (BaseAddress)) {

        //
        // All physical addresses are by definition cached and therefore do
        // not qualify for our caller.
        //

        if (CacheAttribute != MiCached) {
            return NULL;
        }

        OriginalPage = MI_CONVERT_PHYSICAL_TO_PFN(BaseAddress);
        OriginalLastPage = OriginalPage + BaseAddressPages;

        Page = OriginalPage;
        LastPage = OriginalLastPage;

        //
        // Close the gaps, then examine the range for a fit.
        //

        if (Page < LowestPfn) {
            Page = LowestPfn;
        }

        if (LastPage > HighestPfn + 1) {
            LastPage = HighestPfn + 1;
        }

        HighestStartPage = LastPage - SizeInPages;

        if (Page > HighestStartPage) {
            return NULL;
        }

        if (BoundaryPfn != 0) {
            do {
                if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) == 0) {

                    //
                    // This portion of the range meets the alignment
                    // requirements.
                    //

                    break;
                }
                Page |= (BoundaryPfn - 1);
                Page += 1;
            } while (Page <= HighestStartPage);

            if (Page > HighestStartPage) {
                return NULL;
            }
            BoundaryAllocation = (PVOID)((PCHAR)BaseAddress + ((Page - OriginalPage) << PAGE_SHIFT));

            //
            // The request can be satisfied.  Since specific alignment was
            // requested, return the fit now without getting fancy.
            //

            return BoundaryAllocation;
        }

        //
        // If possible return a chunk on the end to reduce fragmentation.
        //
    
        if (LastPage == OriginalLastPage) {
            return (PVOID)((PCHAR)BaseAddress + ((BaseAddressPages - SizeInPages) << PAGE_SHIFT));
        }
    
        //
        // The end chunk did not satisfy the requirements.  The next best option
        // is to return a chunk from the beginning.  Since that's where the search
        // began, just return the current chunk.
        //

        return (PVOID)((PCHAR)BaseAddress + ((Page - OriginalPage) << PAGE_SHIFT));
    }

    //
    // Check the virtual addresses for physical contiguity.
    //

    PointerPte = MiGetPteAddress (BaseAddress);
    LastPte = PointerPte + BaseAddressPages;

    HighestStartPage = HighestPfn + 1 - SizeInPages;
    PageCount = 0;

    //
    // Initializing PreviousPage is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PreviousPage = 0;

    while (PointerPte < LastPte) {

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Hard.Valid == 1);
        Page = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

        //
        // Before starting a new run, ensure that it
        // can satisfy the location & boundary requirements (if any).
        //

        if (PageCount == 0) {

            if ((Page >= LowestPfn) &&
                (Page <= HighestStartPage) &&
                ((CacheAttribute == MiCached) || (!MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (Page)))) {

                if (BoundaryPfn == 0) {
                    PageCount += 1;
                }
                else if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) == 0) {
                    //
                    // This run's physical address meets the alignment
                    // requirement.
                    //

                    PageCount += 1;
                }
            }

            if (PageCount == SizeInPages) {

                //
                // Success - found a single page satifying the requirements.
                //

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                return BaseAddress;
            }

            PreviousPage = Page;
            PointerPte += 1;
            continue;
        }

        if (Page != PreviousPage + 1) {

            //
            // This page is not physically contiguous.  Start over.
            //

            PageCount = 0;
            continue;
        }

        PageCount += 1;

        if (PageCount == SizeInPages) {

            //
            // Success - found a page range satifying the requirements.
            //

            BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte - PageCount + 1);
            return BaseAddress;
        }

        PointerPte += 1;
    }

    return NULL;
}


VOID
MmLockPagableSectionByHandle (
    IN PVOID ImageSectionHandle
    )


/*++

Routine Description:

    This routine checks to see if the specified pages are resident in
    the process's working set and if so the reference count for the
    page is incremented.  The allows the virtual address to be accessed
    without getting a hard page fault (have to go to the disk... except
    for extremely rare case when the page table page is removed from the
    working set and migrates to the disk.

    If the virtual address is that of the system wide global "cache" the
    virtual address of the "locked" pages is always guaranteed to
    be valid.

    NOTE: This routine is not to be used for general locking of user
    addresses - use MmProbeAndLockPages.  This routine is intended for
    well behaved system code like the file system caches which allocates
    virtual addresses for mapping files AND guarantees that the mapping
    will not be modified (deleted or changed) while the pages are locked.

Arguments:

    ImageSectionHandle - Supplies the value returned by a previous call
                         to MmLockPagableDataSection.  This is a pointer to
                         the section header for the image.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    ULONG EntryCount;
    ULONG OriginalCount;
    PETHREAD CurrentThread;
    PIMAGE_SECTION_HEADER NtSection;
    PVOID BaseAddress;
    ULONG SizeToLock;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PLONG SectionLockCountPointer;

    if (MI_IS_PHYSICAL_ADDRESS(ImageSectionHandle)) {

        //
        // No need to lock physical addresses.
        //

        return;
    }

    NtSection = (PIMAGE_SECTION_HEADER)ImageSectionHandle;

    BaseAddress = SECTION_BASE_ADDRESS(NtSection);
    SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);

    ASSERT (!MI_IS_SYSTEM_CACHE_ADDRESS(BaseAddress));

    //
    // The address must be within the system space.
    //

    ASSERT (BaseAddress >= MmSystemRangeStart);

    SizeToLock = NtSection->SizeOfRawData;
    PointerPte = MiGetPteAddress(BaseAddress);
    LastPte = MiGetPteAddress((PCHAR)BaseAddress + SizeToLock - 1);

    ASSERT (SizeToLock != 0);

    CurrentThread = PsGetCurrentThread ();

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    //
    //  The lock count values have the following meanings :
    //
    //  Value of 0 means unlocked.
    //  Value of 1 means lock in progress by another thread.
    //  Value of 2 or more means locked.
    //
    //  If the value is 1, this thread must block until the other thread's
    //  lock operation is complete.
    //

    do {
        EntryCount = *SectionLockCountPointer;

        if (EntryCount != 1) {

            OriginalCount = InterlockedCompareExchange (SectionLockCountPointer,
                                                        EntryCount + 1,
                                                        EntryCount);

            if (OriginalCount == EntryCount) {

                //
                // Success - this is the first thread to update.
                //

                ASSERT (OriginalCount != 1);
                break;
            }

            //
            // Another thread updated the count before this thread's attempt
            // so it's time to start over.
            //
        }
        else {

            //
            // A lock is in progress, wait for it to finish.  This should be
            // generally rare, and even in this case, the pulse will usually
            // wake us.  A timeout is used so that the wait and the pulse do
            // not need to be interlocked.
            //

            InterlockedIncrement (&MmCollidedLockWait);

            KeWaitForSingleObject (&MmCollidedLockEvent,
                                   WrVirtualMemory,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)&MmShortTime);

            InterlockedDecrement (&MmCollidedLockWait);
        }

    } while (TRUE);

    if (OriginalCount >= 2) {

        //
        // Already locked, just return.
        //

        KeLeaveCriticalRegionThread (&CurrentThread->Tcb);
        return;
    }

    ASSERT (OriginalCount == 0);
    ASSERT (*SectionLockCountPointer == 1);

    //
    // Value was 0 when the lock was obtained.  It is now 1 indicating
    // a lock is in progress.
    //

    MiLockCode (PointerPte, LastPte, MM_LOCK_BY_REFCOUNT);

    //
    // Set lock count to 2 (it was 1 when this started) and check
    // to see if any other threads tried to lock while this was happening.
    //

    ASSERT (*SectionLockCountPointer == 1);
    OriginalCount = InterlockedIncrement (SectionLockCountPointer);
    ASSERT (OriginalCount >= 2);

    if (MmCollidedLockWait != 0) {
        KePulseEvent (&MmCollidedLockEvent, 0, FALSE);
    }

    //
    // Enable user APCs now that the pulse has occurred.  They had to be
    // blocked to prevent any suspensions of this thread as that would
    // stop all waiters indefinitely.
    //

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    return;
}


VOID
MiLockCode (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG LockType
    )

/*++

Routine Description:

    This routine checks to see if the specified pages are resident in
    the process's working set and if so the reference count for the
    page is incremented.  This allows the virtual address to be accessed
    without getting a hard page fault (have to go to the disk...) except
    for the extremely rare case when the page table page is removed from the
    working set and migrates to the disk.

    If the virtual address is that of the system wide global "cache", the
    virtual address of the "locked" pages is always guaranteed to
    be valid.

    NOTE: This routine is not to be used for general locking of user
    addresses - use MmProbeAndLockPages.  This routine is intended for
    well behaved system code like the file system caches which allocates
    virtual addresses for mapping files AND guarantees that the mapping
    will not be modified (deleted or changed) while the pages are locked.

Arguments:

    FirstPte - Supplies the base address to begin locking.

    LastPte - The last PTE to lock.

    LockType - Supplies either MM_LOCK_BY_REFCOUNT or MM_LOCK_NONPAGE.
               LOCK_BY_REFCOUNT increments the reference count to keep
               the page in memory, LOCK_NONPAGE removes the page from
               the working set so it's locked just like nonpaged pool.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE PteContents;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER SwapEntry;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    KIRQL DontCareIrql;
    LOGICAL SessionSpace;
    LOGICAL NewlyLocked;
    PMMWSL WorkingSetList;
    PMMSUPPORT Vm;
    PETHREAD CurrentThread;

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(FirstPte)));
    PointerPte = FirstPte;

    CurrentThread = PsGetCurrentThread ();

    SessionSpace = MI_IS_SESSION_IMAGE_ADDRESS (MiGetVirtualAddressMappedByPte(FirstPte));

    if (SessionSpace == TRUE) {
        Vm = &MmSessionSpace->Vm;
        WorkingSetList = MmSessionSpace->Vm.VmWorkingSetList;

        //
        // Session space is never locked by refcount.
        //

        ASSERT (LockType != MM_LOCK_BY_REFCOUNT);

        LOCK_SESSION_SPACE_WS (OldIrqlWs, CurrentThread);
    }
    else {

        //
        // Initializing these is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        WorkingSetList = NULL;
        Vm = NULL;

        LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
    }

    LOCK_PFN (OldIrql);

    MmLockedCode += 1 + LastPte - FirstPte;

    do {

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Long != ZeroKernelPte.u.Long);
        if (PteContents.u.Hard.Valid == 1) {

            //
            // This address is already in the system (or session) working set.
            //

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            //
            // Up the reference count so the page cannot be released.
            //

            MI_ADD_LOCKED_PAGE_CHARGE (Pfn1, 36);
            Pfn1->u3.e2.ReferenceCount += 1;

            if (LockType != MM_LOCK_BY_REFCOUNT) {

                //
                // If the page is in the system working set, remove it.
                // The system working set lock MUST be owned to check to
                // see if this page is in the working set or not.  This
                // is because the pager may have just released the PFN lock,
                // acquired the system lock and is now trying to add the
                // page to the system working set.
                //
                // If the page is in the SESSION working set, it cannot be
                // removed as all these pages are carefully accounted for.
                // Instead move it to the locked portion of the working set
                // if it is not there already.
                //

                if (Pfn1->u1.WsIndex != 0) {

                    UNLOCK_PFN (APC_LEVEL);

                    if (SessionSpace == TRUE) {

                        WorkingSetIndex = MiLocateWsle (
                                    MiGetVirtualAddressMappedByPte(PointerPte),
                                    WorkingSetList,
                                    Pfn1->u1.WsIndex);

                        if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {
                
                            SwapEntry = WorkingSetList->FirstDynamic;
                
                            if (WorkingSetIndex != WorkingSetList->FirstDynamic) {
                
                                //
                                // Swap this entry with the one at first
                                // dynamic.  Note that the working set index
                                // in the PTE is updated here as well.
                                //
                
                                MiSwapWslEntries (WorkingSetIndex,
                                                  SwapEntry,
                                                  Vm);
                            }
                
                            WorkingSetList->FirstDynamic += 1;
                            NewlyLocked = TRUE;

                            //
                            // Indicate that the page is now locked.
                            //
            
                            MmSessionSpace->Wsle[SwapEntry].u1.e1.LockedInWs = 1;
                            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_LOCK_CODE2, 1);
                            MmSessionSpace->NonPagablePages += 1;
                            LOCK_PFN (DontCareIrql);
                            MM_BUMP_COUNTER(25, 1);
                            Pfn1->u1.WsIndex = SwapEntry;
                        }
                        else {
                            NewlyLocked = FALSE;
                            ASSERT (MmSessionSpace->Wsle[WorkingSetIndex].u1.e1.LockedInWs == 1);
                            LOCK_PFN (DontCareIrql);
                        }
                    }
                    else {
                        NewlyLocked = TRUE;
                        MiRemoveWsle (Pfn1->u1.WsIndex, MmSystemCacheWorkingSetList);
                        MiReleaseWsle (Pfn1->u1.WsIndex, &MmSystemCacheWs);

                        MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);
                        LOCK_PFN (DontCareIrql);
                        MM_BUMP_COUNTER(29, 1);
                        MI_ZERO_WSINDEX (Pfn1);
                    }

                    //
                    // Adjust available pages as this page is now not in any
                    // working set, just like a non-paged pool page.
                    //
    
                    if (NewlyLocked == TRUE) {
                        MmResidentAvailablePages -= 1;
                        if (Pfn1->u3.e1.PrototypePte == 0) {
                            MmTotalSystemDriverPages -= 1;
                        }
                    }
                }
                ASSERT (Pfn1->u3.e2.ReferenceCount > 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 37);
            }
        }
        else if (PteContents.u.Soft.Prototype == 1) {

            //
            // Page is not in memory and it is a prototype.
            //

            MiMakeSystemAddressValidPfnSystemWs (
                    MiGetVirtualAddressMappedByPte(PointerPte));

            continue;
        }
        else if (PteContents.u.Soft.Transition == 1) {

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            if ((Pfn1->u3.e1.ReadInProgress) ||
                (Pfn1->u4.InPageError)) {

                //
                // Page read is ongoing, force a collided fault.
                //

                MiMakeSystemAddressValidPfnSystemWs (
                        MiGetVirtualAddressMappedByPte(PointerPte));

                continue;
            }

            //
            // Paged pool is trimmed without regard to sharecounts.
            // This means a paged pool PTE can be in transition while
            // the page is still marked active.
            //

            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

                ASSERT (((Pfn1->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                        ((Pfn1->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

                //
                // Don't increment the valid PTE count for the
                // paged pool page.
                //

                ASSERT (Pfn1->u2.ShareCount != 0);
                ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
                Pfn1->u2.ShareCount += 1;
            }
            else {

                if (MmAvailablePages == 0) {

                    //
                    // This can only happen if the system is utilizing
                    // a hardware compression cache.  This ensures that
                    // only a safe amount of the compressed virtual cache
                    // is directly mapped so that if the hardware gets
                    // into trouble, we can bail it out.
                    //
                    // Just unlock everything here to give the compression
                    // reaper a chance to ravage pages and then retry.
                    //

                    UNLOCK_PFN (APC_LEVEL);

                    if (SessionSpace == TRUE) {
                        UNLOCK_SESSION_SPACE_WS (OldIrqlWs);

                        LOCK_SESSION_SPACE_WS (OldIrqlWs, CurrentThread);
                    }
                    else {
                        UNLOCK_SYSTEM_WS (OldIrqlWs);

                        LOCK_SYSTEM_WS (OldIrql, CurrentThread);
                    }
                    LOCK_PFN (DontCareIrql);

                    continue;
                }

                MiUnlinkPageFromList (Pfn1);

                //
                // Set the reference count and share counts to 1.  Note the
                // reference count may be 1 already if a modified page
                // write is underway.  The systemwide locked page charges
                // are correct in either case and nothing needs to be done
                // just yet.
                //

                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->u2.ShareCount = 1;
            }

            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiCached;

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               Pfn1->OriginalPte.u.Soft.Protection,
                               PointerPte);

            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            //
            // Increment the reference count one for putting it the
            // working set list and one for locking it for I/O.
            //

            if (LockType == MM_LOCK_BY_REFCOUNT) {

                //
                // Lock the page in the working set by upping the
                // reference count.
                //

                MI_ADD_LOCKED_PAGE_CHARGE (Pfn1, 34);
                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->u1.Event = (PVOID) CurrentThread;

                UNLOCK_PFN (APC_LEVEL);
                WorkingSetIndex = MiLocateAndReserveWsle (&MmSystemCacheWs);

                MiUpdateWsle (&WorkingSetIndex,
                              MiGetVirtualAddressMappedByPte (PointerPte),
                              MmSystemCacheWorkingSetList,
                              Pfn1);

                MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

                LOCK_PFN (DontCareIrql);

            }
            else {

                //
                // The wsindex field must be zero because the
                // page is not in the system (or session) working set.
                //

                ASSERT (Pfn1->u1.WsIndex == 0);

                //
                // Adjust available pages as this page is now not in any
                // working set, just like a non-paged pool page.  On entry
                // this page was in transition so it was part of the
                // available pages by definition.
                //

                MmResidentAvailablePages -= 1;
                if (Pfn1->u3.e1.PrototypePte == 0) {
                    MmTotalSystemDriverPages -= 1;
                }
                if (SessionSpace == TRUE) {
                    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_LOCK_CODE1, 1);
                    MmSessionSpace->NonPagablePages += 1;
                }
                MM_BUMP_COUNTER(26, 1);
            }
        }
        else {

            //
            // Page is not in memory.
            //

            MiMakeSystemAddressValidPfnSystemWs (
                    MiGetVirtualAddressMappedByPte(PointerPte));

            continue;
        }

        PointerPte += 1;
    } while (PointerPte <= LastPte);

    UNLOCK_PFN (OldIrql);

    if (SessionSpace == TRUE) {
        UNLOCK_SESSION_SPACE_WS (OldIrqlWs);
    }
    else {
        UNLOCK_SYSTEM_WS (OldIrqlWs);
    }
    return;
}


NTSTATUS
MmGetSectionRange (
    IN PVOID AddressWithinSection,
    OUT PVOID *StartingSectionAddress,
    OUT PULONG SizeofSection
    )
{
    ULONG Span;
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    NTSTATUS Status;
    ULONG_PTR Rva;

    PAGED_CODE();

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    Status = STATUS_NOT_FOUND;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, TRUE);
    if (DataTableEntry) {

        Rva = (ULONG_PTR)((PUCHAR)AddressWithinSection - (ULONG_PTR)DataTableEntry->DllBase);

        NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(DataTableEntry->DllBase);

        NtSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                            sizeof(ULONG) +
                            sizeof(IMAGE_FILE_HEADER) +
                            NtHeaders->FileHeader.SizeOfOptionalHeader
                            );

        for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {

            //
            // Generally, SizeOfRawData is larger than VirtualSize for each
            // section because it includes the padding to get to the subsection
            // alignment boundary.  However, on MP kernels where we link with
            // subsection alignment == native page alignment, the linker will
            // have VirtualSize be much larger than SizeOfRawData because it
            // will account for all the bss.
            //

            Span = NtSection->SizeOfRawData;

            if (Span < NtSection->Misc.VirtualSize) {
                Span = NtSection->Misc.VirtualSize;
            }

            if ((Rva >= NtSection->VirtualAddress) &&
                (Rva < NtSection->VirtualAddress + Span)) {

                //
                // Found it.
                //

                *StartingSectionAddress = (PVOID)
                    ((PCHAR) DataTableEntry->DllBase + NtSection->VirtualAddress);
                *SizeofSection = NtSection->SizeOfRawData;
                Status = STATUS_SUCCESS;
                break;
            }

            NtSection += 1;
        }
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);
    return Status;
}


PVOID
MmLockPagableDataSection (
    IN PVOID AddressWithinSection
    )

/*++

Routine Description:

    This functions locks the entire section that contains the specified
    section in memory.  This allows pagable code to be brought into
    memory and to be used as if the code was not really pagable.  This
    should not be done with a high degree of frequency.

Arguments:

    AddressWithinSection - Supplies the address of a function
        contained within a section that should be brought in and locked
        in memory.

Return Value:

    This function returns a value to be used in a subsequent call to
    MmUnlockPagableImageSection.

--*/

{
    PLONG SectionLockCountPointer;
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    PIMAGE_SECTION_HEADER FoundSection;
    ULONG_PTR Rva;

    PAGED_CODE();

    if (MI_IS_PHYSICAL_ADDRESS(AddressWithinSection)) {

        //
        // Physical address, just return that as the handle.
        //

        return AddressWithinSection;
    }

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    FoundSection = NULL;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, TRUE);

    Rva = (ULONG_PTR)((PUCHAR)AddressWithinSection - (ULONG_PTR)DataTableEntry->DllBase);

    NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(DataTableEntry->DllBase);

    NtSection = (PIMAGE_SECTION_HEADER)((ULONG_PTR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {

        if ( Rva >= NtSection->VirtualAddress &&
             Rva < NtSection->VirtualAddress + NtSection->SizeOfRawData ) {
            FoundSection = NtSection;

            if (SECTION_BASE_ADDRESS(NtSection) != ((PUCHAR)DataTableEntry->DllBase +
                            NtSection->VirtualAddress)) {

                //
                // Overwrite the PointerToRelocations field (and on Win64, the
                // PointerToLinenumbers field also) so that it contains
                // the Va of this section.
                //
                // NumberOfRelocations & NumberOfLinenumbers contains
                // the Lock Count for the section.
                //

                SECTION_BASE_ADDRESS(NtSection) = ((PUCHAR)DataTableEntry->DllBase +
                                        NtSection->VirtualAddress);

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
                *SectionLockCountPointer = 0;
            }

            //
            // Now lock in the code
            //

#if DBG
            if (MmDebug & MM_DBG_LOCK_CODE) {
                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
                DbgPrint("MM Lock %wZ %8s %p -> %p : %p %3ld.\n",
                        &DataTableEntry->BaseDllName,
                        NtSection->Name,
                        AddressWithinSection,
                        NtSection,
                        SECTION_BASE_ADDRESS(NtSection),
                        *SectionLockCountPointer);
            }
#endif //DBG

            MmLockPagableSectionByHandle ((PVOID)NtSection);

            break;
        }
        NtSection += 1;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);
    if (!FoundSection) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x1234,
                      (ULONG_PTR)AddressWithinSection,
                      0,
                      0);
    }
    return (PVOID)FoundSection;
}


PKLDR_DATA_TABLE_ENTRY
MiLookupDataTableEntry (
    IN PVOID AddressWithinSection,
    IN ULONG ResourceHeld
    )

/*++

Routine Description:

    This functions locates the data table entry that maps the specified address.

Arguments:

    AddressWithinSection - Supplies the address of a function contained
                           within the desired module.

    ResourceHeld - Supplies TRUE if the loaded module resource is already held,
                   FALSE if not.

Return Value:

    The address of the loaded module list data table entry that maps the
    argument address.

--*/

{
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY FoundEntry;
    PLIST_ENTRY NextEntry;

    PAGED_CODE();

    FoundEntry = NULL;

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    if (!ResourceHeld) {
        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);
    }
    else {
        CurrentThread = NULL;
    }

    NextEntry = PsLoadedModuleList.Flink;
    do {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        //
        // Locate the loaded module that contains this address.
        //

        if ( AddressWithinSection >= DataTableEntry->DllBase &&
             AddressWithinSection < (PVOID)((PUCHAR)DataTableEntry->DllBase+DataTableEntry->SizeOfImage) ) {

            FoundEntry = DataTableEntry;
            break;
        }

        NextEntry = NextEntry->Flink;
    } while (NextEntry != &PsLoadedModuleList);

    if (CurrentThread != NULL) {
        ExReleaseResourceLite (&PsLoadedModuleResource);
        KeLeaveCriticalRegionThread (CurrentThread);
    }
    return FoundEntry;
}

VOID
MmUnlockPagableImageSection (
    IN PVOID ImageSectionHandle
    )

/*++

Routine Description:

    This function unlocks from memory, the pages locked by a preceding call to
    MmLockPagableDataSection.

Arguments:

    ImageSectionHandle - Supplies the value returned by a previous call
                         to MmLockPagableDataSection.

Return Value:

    None.

--*/

{
    PKTHREAD CurrentThread;
    PIMAGE_SECTION_HEADER NtSection;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PVOID BaseAddress;
    ULONG SizeToUnlock;
    ULONG Count;
    PLONG SectionLockCountPointer;

    if (MI_IS_PHYSICAL_ADDRESS(ImageSectionHandle)) {

        //
        // No need to unlock physical addresses.
        //

        return;
    }

    NtSection = (PIMAGE_SECTION_HEADER)ImageSectionHandle;

    //
    // Address must be in the system working set.
    //

    BaseAddress = SECTION_BASE_ADDRESS(NtSection);
    SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
    SizeToUnlock = NtSection->SizeOfRawData;

    PointerPte = MiGetPteAddress(BaseAddress);
    LastPte = MiGetPteAddress((PCHAR)BaseAddress + SizeToUnlock - 1);

    CurrentThread = KeGetCurrentThread ();

    //
    // Block user APCs as the initial decrement below could push the count to 1.
    // This puts this thread into the critical path that must finish as all
    // other threads trying to lock the section will be waiting for this thread.
    // Entering a critical region here ensures that a suspend cannot stop us.
    //

    KeEnterCriticalRegionThread (CurrentThread);

    Count = InterlockedDecrement (SectionLockCountPointer);
    
    if (Count < 1) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x1010,
                      (ULONG_PTR)BaseAddress,
                      (ULONG_PTR)NtSection,
                      *SectionLockCountPointer);
    }

    if (Count != 1) {
        KeLeaveCriticalRegionThread (CurrentThread);
        return;
    }

    LOCK_PFN2 (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 37);

        PointerPte += 1;

    } while (PointerPte <= LastPte);

    UNLOCK_PFN2 (OldIrql);

    ASSERT (*SectionLockCountPointer == 1);
    Count = InterlockedDecrement (SectionLockCountPointer);
    ASSERT (Count == 0);

    if (MmCollidedLockWait != 0) {
        KePulseEvent (&MmCollidedLockEvent, 0, FALSE);
    }

    //
    // Enable user APCs now that the pulse has occurred.  They had to be
    // blocked to prevent any suspensions of this thread as that would
    // stop all waiters indefinitely.
    //

    KeLeaveCriticalRegionThread (CurrentThread);

    return;
}


BOOLEAN
MmIsRecursiveIoFault (
    VOID
    )

/*++

Routine Description:

    This function examines the thread's page fault clustering information
    and determines if the current page fault is occurring during an I/O
    operation.

Arguments:

    None.

Return Value:

    Returns TRUE if the fault is occurring during an I/O operation,
    FALSE otherwise.

--*/

{
    PETHREAD Thread;

    Thread = PsGetCurrentThread ();

    return (BOOLEAN)(Thread->DisablePageFaultClustering |
                     Thread->ForwardClusterOnly);
}


VOID
MmMapMemoryDumpMdl (
    IN OUT PMDL MemoryDumpMdl
    )

/*++

Routine Description:

    For use by crash dump routine ONLY.  Maps an MDL into a fixed
    portion of the address space.  Only 1 MDL can be mapped at a
    time.

Arguments:

    MemoryDumpMdl - Supplies the MDL to map.

Return Value:

    None, fields in MDL updated.

--*/

{
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    PCHAR BaseVa;
    MMPTE TempPte;
    PPFN_NUMBER Page;

    NumberOfPages = BYTES_TO_PAGES (MemoryDumpMdl->ByteCount + MemoryDumpMdl->ByteOffset);

    ASSERT (NumberOfPages <= 16);

    PointerPte = MmCrashDumpPte;
    BaseVa = (PCHAR)MiGetVirtualAddressMappedByPte(PointerPte);
    MemoryDumpMdl->MappedSystemVa = (PCHAR)BaseVa + MemoryDumpMdl->ByteOffset;
    TempPte = ValidKernelPte;
    Page = (PPFN_NUMBER)(MemoryDumpMdl + 1);

    //
    // If the pages don't span the entire dump virtual address range,
    // build a barrier.  Otherwise use the default barrier provided at the
    // end of the dump virtual address range.
    //

    if (NumberOfPages < 16) {
        (PointerPte + NumberOfPages)->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
        KiFlushSingleTb (TRUE, BaseVa + (NumberOfPages << PAGE_SHIFT));
    }

    do {

        TempPte.u.Hard.PageFrameNumber = *Page;

        //
        // Note this PTE may be valid or invalid prior to the overwriting here.
        //

        *PointerPte = TempPte;

        KiFlushSingleTb (TRUE, BaseVa);

        Page += 1;
        PointerPte += 1;
        BaseVa += PAGE_SIZE;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    return;
}


VOID
MmReleaseDumpAddresses (
    IN PFN_NUMBER Pages
    )

/*++

Routine Description:

    For use by hibernate routine ONLY.  Puts zeros back into the
    used dump PTEs.

Arguments:

    None

Return Value:

    None

--*/

{
    PMMPTE PointerPte;
    PCHAR BaseVa;

    PointerPte = MmCrashDumpPte;
    BaseVa = (PCHAR)MiGetVirtualAddressMappedByPte(PointerPte);

    while (Pages) {

        PointerPte->u.Long = MM_ZERO_PTE;

        KiFlushSingleTb (TRUE, BaseVa);

        PointerPte += 1;
        BaseVa += PAGE_SIZE;
        Pages -= 1;
    }
}


NTSTATUS
MmSetBankedSection (
    IN HANDLE ProcessHandle,
    IN PVOID VirtualAddress,
    IN ULONG BankLength,
    IN BOOLEAN ReadWriteBank,
    IN PBANKED_SECTION_ROUTINE BankRoutine,
    IN PVOID Context
    )

/*++

Routine Description:

    This function declares a mapped video buffer as a banked
    section.  This allows banked video devices (i.e., even
    though the video controller has a megabyte or so of memory,
    only a small bank (like 64k) can be mapped at any one time.

    In order to overcome this problem, the pager handles faults
    to this memory, unmaps the current bank, calls off to the
    video driver and then maps in the new bank.

    This function creates the necessary structures to allow the
    video driver to be called from the pager.

 ********************* NOTE NOTE NOTE *************************
    At this time only read/write banks are supported!

Arguments:

    ProcessHandle - Supplies a handle to the process in which to
                    support the banked video function.

    VirtualAddress - Supplies the virtual address where the video
                     buffer is mapped in the specified process.

    BankLength - Supplies the size of the bank.

    ReadWriteBank - Supplies TRUE if the bank is read and write.

    BankRoutine - Supplies a pointer to the routine that should be
                  called by the pager.

    Context - Supplies a context to be passed by the pager to the
              BankRoutine.

Return Value:

    Returns the status of the function.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    KAPC_STATE ApcState;
    NTSTATUS Status;
    PEPROCESS Process;
    PMMVAD Vad;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    ULONG_PTR size;
    LONG count;
    ULONG NumberOfPtes;
    PMMBANKED_SECTION Bank;

    PAGED_CODE ();

    UNREFERENCED_PARAMETER (ReadWriteBank);

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         KernelMode,
                                         (PVOID *)&Process,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    KeStackAttachProcess (&Process->Pcb, &ApcState);

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    Vad = MiLocateAddress (VirtualAddress);

    if ((Vad == NULL) ||
        (Vad->StartingVpn != MI_VA_TO_VPN (VirtualAddress)) ||
        (Vad->u.VadFlags.PhysicalMapping == 0)) {
        Status = STATUS_NOT_MAPPED_DATA;
        goto ErrorReturn;
    }

    size = PAGE_SIZE + ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT);
    if ((size % BankLength) != 0) {
        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    count = -1;
    NumberOfPtes = BankLength;

    do {
        NumberOfPtes = NumberOfPtes >> 1;
        count += 1;
    } while (NumberOfPtes != 0);

    //
    // Turn VAD into Banked VAD
    //

    NumberOfPtes = BankLength >> PAGE_SHIFT;

    Bank = ExAllocatePoolWithTag (NonPagedPool,
                                    sizeof (MMBANKED_SECTION) +
                                       (NumberOfPtes - 1) * sizeof(MMPTE),
                                    'kBmM');
    if (Bank == NULL) {
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn;
    }

    Bank->BankShift = PTE_SHIFT + count - PAGE_SHIFT;

    PointerPte = MiGetPteAddress(MI_VPN_TO_VA (Vad->StartingVpn));
    ASSERT (PointerPte->u.Hard.Valid == 1);

    Bank->BasePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    Bank->BasedPte = PointerPte;
    Bank->BankSize = BankLength;
    Bank->BankedRoutine = BankRoutine;
    Bank->Context = Context;
    Bank->CurrentMappedPte = PointerPte;

    //
    // Build the template PTEs structure.
    //

    count = 0;
    TempPte = ZeroPte;

    MI_MAKE_VALID_PTE (TempPte,
                       Bank->BasePhysicalPage,
                       MM_READWRITE,
                       PointerPte);

    if (TempPte.u.Hard.Write) {
        MI_SET_PTE_DIRTY (TempPte);
    }

    do {
        Bank->BankTemplate[count] = TempPte;
        TempPte.u.Hard.PageFrameNumber += 1;
        count += 1;
    } while ((ULONG)count < NumberOfPtes );

    LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));

    //
    // Set all PTEs within this range to zero.  Any faults within
    // this range will call the banked routine before making the
    // page valid.
    //

    LOCK_WS_UNSAFE (Process);

    ((PMMVAD_LONG) Vad)->u4.Banked = Bank;

    RtlFillMemory (PointerPte,
                   (size >> (PAGE_SHIFT - PTE_SHIFT)),
                   (UCHAR)ZeroPte.u.Long);

    KeFlushEntireTb (TRUE, TRUE);

    UNLOCK_WS_UNSAFE (Process);

    Status = STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    KeUnstackDetachProcess (&ApcState);
    ObDereferenceObject (Process);
    return Status;
}

PVOID
MmMapVideoDisplay (
     IN PHYSICAL_ADDRESS PhysicalAddress,
     IN SIZE_T NumberOfBytes,
     IN MEMORY_CACHING_TYPE CacheType
     )

/*++

Routine Description:

    This function maps the specified physical address into the non-pagable
    portion of the system address space.

Arguments:

    PhysicalAddress - Supplies the starting physical address to map.

    NumberOfBytes - Supplies the number of bytes to map.

    CacheType - Supplies MmNonCached if the physical address is to be mapped
                as non-cached, MmCached if the address should be cached, and
                MmWriteCombined if the address should be cached and
                write-combined as a frame buffer. For I/O device registers,
                this is usually specified as MmNonCached.

Return Value:

    Returns the virtual address which maps the specified physical addresses.
    The value NULL is returned if sufficient virtual address space for
    the mapping could not be found.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PVOID BaseVa;
#ifdef LARGE_PAGES
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    ULONG size;
    PMMPTE protoPte;
    PMMPTE largePte;
    ULONG pageSize;
    PSUBSECTION Subsection;
    ULONG Alignment;
    ULONG EmPageSize;
#endif LARGE_PAGES
    ULONG LargePages;

    LargePages = FALSE;
    PointerPte = NULL;

#if !defined (_MI_MORE_THAN_4GB_)
    ASSERT (PhysicalAddress.HighPart == 0);
#endif

    PAGED_CODE();

    ASSERT (NumberOfBytes != 0);

#ifdef LARGE_PAGES

    If this is ever enabled, care must be taken not to insert overlapping
    TB entries with different cache attributes.

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (PhysicalAddress.LowPart,
                                           NumberOfBytes);

    TempPte = ValidKernelPte;
    MI_DISABLE_CACHING (TempPte);
    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    if ((NumberOfBytes > X64K) && (!MmLargeVideoMapped)) {
        size = (NumberOfBytes - 1) >> (PAGE_SHIFT + 1);
        pageSize = PAGE_SIZE;

        while (size != 0) {
            size = size >> 2;
            pageSize = pageSize << 2;
        }

        Alignment = pageSize << 1;
        if (Alignment < MM_VA_MAPPED_BY_PDE) {
            Alignment = MM_VA_MAPPED_BY_PDE;
        }

#if defined(_IA64_)

        //
        // Convert pageSize to the EM specific page-size field format
        //

        EmPageSize = 0;
        size = pageSize - 1 ;

        while (size) {
            size = size >> 1;
            EmPageSize += 1;
        }

        if (NumberOfBytes > pageSize) {

            if (MmPageSizeInfo & (pageSize << 1)) {

                //
                // if larger page size is supported in the implementation
                //

                pageSize = pageSize << 1;
                EmPageSize += 1;

            }
            else {

                EmPageSize = EmPageSize | pageSize;

            }
        }

        pageSize = EmPageSize;
#endif

        NumberOfPages = Alignment >> PAGE_SHIFT;

        PointerPte = MiReserveAlignedSystemPtes (NumberOfPages,
                                                 SystemPteSpace,
                                                 Alignment);

        if (PointerPte == NULL) {
            goto MapWithSmallPages;
        }

        protoPte = ExAllocatePoolWithTag (PagedPool,
                                           sizeof (MMPTE),
                                           'bSmM');

        if (protoPte == NULL) {
            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            goto MapWithSmallPages;
        }

        Subsection = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(SUBSECTION) + (4 * sizeof(MMPTE)),
                                     'bSmM');

        if (Subsection == NULL) {
            ExFreePool (protoPte);
            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            goto MapWithSmallPages;
        }

        MiFillMemoryPte (PointerPte,
                         Alignment >> (PAGE_SHIFT - PTE_SHIFT),
                         MM_ZERO_KERNEL_PTE);

        //
        // Build large page descriptor and fill in all the PTEs.
        //

        Subsection->StartingSector = pageSize;
        Subsection->EndingSector = (ULONG)NumberOfPages;
        Subsection->u.LongFlags = 0;
        Subsection->u.SubsectionFlags.LargePages = 1;
        Subsection->u.SubsectionFlags.Protection = MM_READWRITE | MM_NOCACHE;
        Subsection->PtesInSubsection = Alignment;
        Subsection->SubsectionBase = PointerPte;

        largePte = (PMMPTE)(Subsection + 1);

        //
        // Build the first 2 PTEs as entries for the TLB to
        // map the specified physical address.
        //

        *largePte = TempPte;
        largePte += 1;

        if (NumberOfBytes > pageSize) {
            *largePte = TempPte;
            largePte->u.Hard.PageFrameNumber += (pageSize >> PAGE_SHIFT);
        }
        else {
            *largePte = ZeroKernelPte;
        }

        //
        // Build the first prototype PTE as a paging file format PTE
        // referring to the subsection.
        //

        protoPte->u.Long = MiGetSubsectionAddressForPte(Subsection);
        protoPte->u.Soft.Prototype = 1;
        protoPte->u.Soft.Protection = MM_READWRITE | MM_NOCACHE;

        //
        // Set the PTE up for all the user's PTE entries in prototype PTE
        // format pointing to the 3rd prototype PTE.
        //

        TempPte.u.Long = MiProtoAddressForPte (protoPte);
        MI_SET_GLOBAL_STATE (TempPte, 1);
        LargePages = TRUE;
        MmLargeVideoMapped = TRUE;
    }

    if (PointerPte != NULL) {
        BaseVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);
        BaseVa = (PVOID)((PCHAR)BaseVa + BYTE_OFFSET(PhysicalAddress.LowPart));

        do {
            ASSERT (PointerPte->u.Hard.Valid == 0);
            MI_WRITE_VALID_PTE (PointerPte, TempPte);
            PointerPte += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);
    }
    else {

MapWithSmallPages:

#endif //LARGE_PAGES

        BaseVa = MmMapIoSpace (PhysicalAddress,
                               NumberOfBytes,
                               CacheType);
#ifdef LARGE_PAGES
    }
#endif //LARGE_PAGES

    return BaseVa;
}

VOID
MmUnmapVideoDisplay (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     )

/*++

Routine Description:

    This function unmaps a range of physical address which were previously
    mapped via an MmMapVideoDisplay function call.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    NumberOfBytes - Supplies the number of bytes which were mapped.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{

#ifdef LARGE_PAGES
    PFN_NUMBER NumberOfPages;
    ULONG i;
    PMMPTE FirstPte;
    KIRQL OldIrql;
    PMMPTE LargePte;
    PSUBSECTION Subsection;

    PAGED_CODE();

    ASSERT (NumberOfBytes != 0);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress, NumberOfBytes);
    FirstPte = MiGetPteAddress (BaseAddress);

    if ((NumberOfBytes > X64K) && (FirstPte->u.Hard.Valid == 0)) {

        ASSERT (MmLargeVideoMapped);
        LargePte = MiPteToProto (FirstPte);
        Subsection = MiGetSubsectionAddress (LargePte);
        ASSERT (Subsection->SubsectionBase == FirstPte);

        NumberOfPages = Subsection->EndingSector;
        ExFreePool (Subsection);
        ExFreePool (LargePte);
        MmLargeVideoMapped = FALSE;
        KeFillFixedEntryTb ((PHARDWARE_PTE)FirstPte, (PVOID)KSEG0_BASE, LARGE_ENTRY);
    }
    MiReleaseSystemPtes(FirstPte, NumberOfPages, SystemPteSpace);
    return;

#else // LARGE_PAGES

    MmUnmapIoSpace (BaseAddress, NumberOfBytes);
    return;
#endif //LARGE_PAGES
}


VOID
MmLockPagedPool (
    IN PVOID Address,
    IN SIZE_T SizeInBytes
    )

/*++

Routine Description:

    Locks the specified address (which MUST reside in paged pool) into
    memory until MmUnlockPagedPool is called.

Arguments:

    Address - Supplies the address in paged pool to lock.

    SizeInBytes - Supplies the size in bytes to lock.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;

    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + (SizeInBytes - 1)));

    MiLockCode (PointerPte, LastPte, MM_LOCK_BY_REFCOUNT);

    return;
}

NTKERNELAPI
VOID
MmUnlockPagedPool (
    IN PVOID Address,
    IN SIZE_T SizeInBytes
    )

/*++

Routine Description:

    Unlocks paged pool that was locked with MmLockPagedPool.

Arguments:

    Address - Supplies the address in paged pool to unlock.

    Size - Supplies the size to unlock.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;

    MmLockPagableSectionByHandle(ExPageLockHandle);
    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + (SizeInBytes - 1)));
    LOCK_PFN (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 35);

        PointerPte += 1;
    } while (PointerPte <= LastPte);

    UNLOCK_PFN (OldIrql);
    MmUnlockPagableImageSection(ExPageLockHandle);
    return;
}

NTKERNELAPI
ULONG
MmGatherMemoryForHibernate (
    IN PMDL Mdl,
    IN BOOLEAN Wait
    )

/*++

Routine Description:

    Finds enough memory to fill in the pages of the MDL for power management
    hibernate function.

Arguments:

    Mdl - Supplies an MDL, the start VA field should be NULL.  The length
          field indicates how many pages to obtain.

    Wait - FALSE to fail immediately if the pages aren't available.

Return Value:

    TRUE if the MDL could be filled in, FALSE otherwise.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER AvailablePages;
    PFN_NUMBER PagesNeeded;
    PPFN_NUMBER Pages;
    PFN_NUMBER i;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    ULONG status;

    status = FALSE;

    PagesNeeded = Mdl->ByteCount >> PAGE_SHIFT;
    Pages = (PPFN_NUMBER)(Mdl + 1);

    i = Wait ? 100 : 1;

    InterlockedIncrement (&MiDelayPageFaults);

    do {

        LOCK_PFN2 (OldIrql);

        MiDeferredUnlockPages (MI_DEFER_PFN_HELD);

        //
        // Don't use MmAvailablePages here because if compression hardware is
        // being used we would bail prematurely.  Check the lists explicitly
        // in order to provide our caller with the maximum number of pages.
        //

        AvailablePages = MmZeroedPageListHead.Total +
                         MmFreePageListHead.Total +
                         MmStandbyPageListHead.Total;

        if (AvailablePages > PagesNeeded) {

            //
            // Fill in the MDL.
            //

            do {
                PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (NULL));
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
#if DBG
                Pfn1->PteAddress = (PVOID) (ULONG_PTR)X64K;
#endif
                MI_SET_PFN_DELETED (Pfn1);
                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                *Pages = PageFrameIndex;
                Pages += 1;
                PagesNeeded -= 1;
            } while (PagesNeeded);
            UNLOCK_PFN2 (OldIrql);
            Mdl->MdlFlags |= MDL_PAGES_LOCKED;
            status = TRUE;
            break;
        }

        UNLOCK_PFN2 (OldIrql);

        //
        // If we're being called at DISPATCH_LEVEL we cannot move pages to
        // the standby list because mutexes must be acquired to do so.
        //

        if (OldIrql > APC_LEVEL) {
            break;
        }

        if (!i) {
            break;
        }

        //
        // Attempt to move pages to the standby list.
        //

        MmEmptyAllWorkingSets ();
        MiFlushAllPages();

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&Mm30Milliseconds);
        i -= 1;

    } while (TRUE);

    InterlockedDecrement (&MiDelayPageFaults);

    return status;
}

NTKERNELAPI
VOID
MmReturnMemoryForHibernate (
    IN PMDL Mdl
    )

/*++

Routine Description:

    Returns memory from MmGatherMemoryForHibername.

Arguments:

    Mdl - Supplies an MDL, the start VA field should be NULL.  The length
          field indicates how many pages to obtain.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PPFN_NUMBER Pages;
    PPFN_NUMBER LastPage;

    Pages = (PPFN_NUMBER)(Mdl + 1);
    LastPage = Pages + (Mdl->ByteCount >> PAGE_SHIFT);

    LOCK_PFN2 (OldIrql);

    do {
        MiDecrementReferenceCount (*Pages);
        Pages += 1;
    } while (Pages < LastPage);

    UNLOCK_PFN2 (OldIrql);
    return;
}


VOID
MmEnablePAT (
     VOID
     )

/*++

Routine Description:

    This routine enables the page attribute capability for individual PTE
    mappings.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/
{
    MiWriteCombiningPtes = TRUE;
}

LOGICAL
MmIsSystemAddressLocked (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine determines whether the specified system address is currently
    locked.

    This routine should only be called for debugging purposes, as it is not
    guaranteed upon return to the caller that the address is still locked.
    (The address could easily have been trimmed prior to return).

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if the address is locked.  FALSE if not.

Environment:

    DISPATCH LEVEL or below.  No memory management locks may be held.

--*/
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;

    if (IS_SYSTEM_ADDRESS (VirtualAddress) == FALSE) {
        return FALSE;
    }

    if (MI_IS_PHYSICAL_ADDRESS (VirtualAddress)) {
        return TRUE;
    }

    //
    // Hyperspace and page maps are not treated as locked down.
    //

    if (MI_IS_PROCESS_SPACE_ADDRESS (VirtualAddress) == TRUE) {
        return FALSE;
    }

#if defined (_IA64_)
    if (MI_IS_KERNEL_PTE_ADDRESS (VirtualAddress) == TRUE) {
        return FALSE;
    }
#endif

    LOCK_PFN2 (OldIrql);

    if (MmIsAddressValid (VirtualAddress) == FALSE) {
        UNLOCK_PFN2 (OldIrql);
        return FALSE;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    //
    // Note that the mapped page may not be in the PFN database.  Treat
    // this as locked.  There is no way to detect if the PFN database is
    // sparse without walking the loader blocks.  Don't bother doing this
    // as few machines are still sparse today.
    //

    if (PageFrameIndex > MmHighestPhysicalPage) {
        UNLOCK_PFN2 (OldIrql);
        return FALSE;
    }

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Check for the page being locked by reference.
    //

    if (Pfn1->u3.e2.ReferenceCount > 1) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    if (Pfn1->u3.e2.ReferenceCount > Pfn1->u2.ShareCount) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    //
    // Check whether the page is locked into the working set.
    //

    if (Pfn1->u1.Event == NULL) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    UNLOCK_PFN2 (OldIrql);

    return FALSE;
}

LOGICAL
MmAreMdlPagesLocked (
    IN PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine determines whether the pages described by the argument
    MDL are currently locked.

    This routine should only be called for debugging purposes, as it is not
    guaranteed upon return to the caller that the pages are still locked.

Arguments:

    MemoryDescriptorList - Supplies the memory descriptor list to check.

Return Value:

    TRUE if ALL the pages described by the argument MDL are locked.
    FALSE if not.

Environment:

    DISPATCH LEVEL or below.  No memory management locks may be held.

--*/
{
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PVOID StartingVa;
    PMMPFN Pfn1;
    KIRQL OldIrql;

    //
    // We'd like to assert that MDL_PAGES_LOCKED is set but can't because
    // some drivers have privately constructed MDLs and they never set the
    // bit properly.
    //

    if ((MemoryDescriptorList->MdlFlags & (MDL_IO_SPACE | MDL_SOURCE_IS_NONPAGED_POOL)) != 0) {
        return TRUE;
    }

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    LOCK_PFN2 (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {

            //
            // There are no more locked pages.
            //

            break;
        }

        //
        // Note that the mapped page may not be in the PFN database.  Treat
        // this as locked.  There is no way to detect if the PFN database is
        // sparse without walking the loader blocks.  Don't bother doing this
        // as few machines are still sparse today.
        //

        if (*Page <= MmHighestPhysicalPage) {

            Pfn1 = MI_PFN_ELEMENT (*Page);

            //
            // Check for the page being locked by reference
            //
            // - or -
            //
            // whether the page is locked into the working set.
            //
        
            if ((Pfn1->u3.e2.ReferenceCount <= Pfn1->u2.ShareCount) &&
                (Pfn1->u3.e2.ReferenceCount <= 1) &&
                (Pfn1->u1.Event != NULL)) {

                //
                // The page is not locked by reference or in a working set.
                //
    
                UNLOCK_PFN2 (OldIrql);
            
                return FALSE;
            }
        }

        Page += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN2 (OldIrql);

    return TRUE;
}

#if DBG

VOID
MiVerifyLockedPageCharges (
    VOID
    )
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER Page;
    PFN_NUMBER LockCharged;

    if (MiPrintLockedPages == 0) {
        return;
    }

    if (KeGetCurrentIrql() > APC_LEVEL) {
        return;
    }

    start = 0;
    LockCharged = 0;

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    LOCK_PFN (OldIrql);

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {
            Pfn1 = MI_PFN_ELEMENT (Page);
            do {
                if (Pfn1->u3.e1.LockCharged == 1) {
                    if (MiPrintLockedPages & 0x4) {
                        DbgPrint ("%x ", Pfn1 - MmPfnDatabase);
                    }
                    LockCharged += 1;
                }
                count -= 1;
                Pfn1 += 1;
            } while (count != 0);
        }

        start += 1;
    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (LockCharged != MmSystemLockPagesCount) {
        if (MiPrintLockedPages & 0x1) {
            DbgPrint ("MM: Locked pages MISMATCH %u %u\n",
                LockCharged, MmSystemLockPagesCount);
        }
    }
    else {
        if (MiPrintLockedPages & 0x2) {
            DbgPrint ("MM: Locked pages ok %u\n",
                LockCharged);
        }
    }

    UNLOCK_PFN (OldIrql);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    return;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mapcache.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

    mapcache.c

Abstract:

    This module contains the routines which implement mapping views
    of sections into the system-wide cache.

Author:

    Lou Perazzoli (loup) 22-May-1990
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/


#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSystemCache)
#pragma alloc_text(PAGE,MiAddMappedPtes)
#endif

extern ULONG MmFrontOfList;

#define X256K 0x40000

PMMPTE MmFirstFreeSystemCache;

PMMPTE MmLastFreeSystemCache;

PMMPTE MmSystemCachePteBase;

ULONG MiMapCacheFailures;

LONG
MiMapCacheExceptionFilter (
    IN PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    );

NTSTATUS
MmMapViewInSystemCache (
    IN PVOID SectionToMap,
    OUT PVOID *CapturedBase,
    IN OUT PLARGE_INTEGER SectionOffset,
    IN OUT PULONG CapturedViewSize
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.  The page protection is identical to that
    of the prototype PTE.

    This function is a kernel mode interface to allow LPC to map
    a section given the section pointer to map.

    This routine assumes all arguments have been probed and captured.

Arguments:

    SectionToMap - Supplies a pointer to the section object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not null, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  null, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and tiled).

    SectionOffset - Supplies the offset from the beginning of the section
                    to the view in bytes. This value must be a multiple
                    of 256k.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view.  The initial
               values of this argument specifies the size of the view
               in bytes and is rounded up to the next host page size
               boundary and must be less than or equal to 256k.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSECTION Section;
    ULONG PteOffset;
    ULONG LastPteOffset;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PSUBSECTION Subsection;
    PVOID EndingVa;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    LOGICAL FlushNeeded;
    ULONG Waited;
#if DBG
    PMMPTE PointerPte2;
    PMMPTE TimeStampPte;
#endif

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    Section = SectionToMap;

    Status = STATUS_SUCCESS;
    FlushNeeded = FALSE;

    //
    // Assert the view size is less than 256k and the section offset
    // is aligned on a 256k boundary.
    //

    ASSERT (*CapturedViewSize <= X256K);
    ASSERT ((SectionOffset->LowPart & (X256K - 1)) == 0);

    //
    // Make sure the section is not an image section or a page file
    // backed section.
    //

    if (Section->u.Flags.Image) {
        return STATUS_NOT_MAPPED_DATA;
    }

    ControlArea = Section->Segment->ControlArea;

    ASSERT (*CapturedViewSize != 0);

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    LOCK_PFN (OldIrql);

    ASSERT (ControlArea->u.Flags.BeingCreated == 0);
    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);
    ASSERT (ControlArea->u.Flags.BeingPurged == 0);

    //
    // Find a free 256k base in the cache.
    //

    if (MmFirstFreeSystemCache == (PMMPTE)MM_EMPTY_LIST) {
        UNLOCK_PFN (OldIrql);
        return STATUS_NO_MEMORY;
    }

    PointerPte = MmFirstFreeSystemCache;

    //
    // Update next free entry.
    //

    ASSERT (PointerPte->u.Hard.Valid == 0);

    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x778,
                      (ULONG_PTR)PointerPte,
                      0,
                      0);
    }
    else {
        MmFirstFreeSystemCache = MmSystemCachePteBase + PointerPte->u.List.NextEntry;
        ASSERT (MmFirstFreeSystemCache <= MiGetPteAddress (MmSystemCacheEnd));
    }

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    //

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfSystemCacheViews += 1;
    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    //
    // Check to see if the TB needs to be flushed.  Note that due to natural
    // TB traffic and the number of system cache views, this is an extremely
    // rare operation.
    //

    if ((PointerPte + 1)->u.List.NextEntry == (KeReadTbFlushTimeStamp() & MM_FLUSH_COUNTER_MASK)) {
        FlushNeeded = TRUE;
    }

    *CapturedBase = MiGetVirtualAddressMappedByPte (PointerPte);

    EndingVa = (PVOID)(((ULONG_PTR)*CapturedBase +
                                *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

    LastPte = MiGetPteAddress (EndingVa);

    //
    // An unoccupied address range has been found, put the PTEs in
    // the range into prototype PTEs.
    //

#if DBG

    //
    //  Zero out the next pointer field.
    //

    PointerPte->u.List.NextEntry = 0;
    TimeStampPte = PointerPte + 1;

    for (PointerPte2 = PointerPte; PointerPte2 <= LastPte; PointerPte2 += 1) {
        ASSERT ((PointerPte2->u.Long == ZeroKernelPte.u.Long) ||
                (PointerPte2 == TimeStampPte));
    }

#endif

    //
    // Calculate the first prototype PTE address.
    //

    PteOffset = (ULONG)(SectionOffset->QuadPart >> PAGE_SHIFT);
    LastPteOffset = PteOffset + (ULONG)(LastPte - PointerPte + 1);

    //
    // Make sure the PTEs are not in the extended part of the
    // segment.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        LastPteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
    }

    //
    // Increment the view count for every subsection spanned by this view,
    // creating prototype PTEs if needed.
    //
    // N.B. This call may release and reacquire the PFN lock.
    //
    // N.B. This call always returns with the PFN lock released !
    //

    if (ControlArea->FilePointer != NULL) {
    
        Status = MiAddViewsForSection ((PMSUBSECTION)Subsection,
                                       LastPteOffset,
                                       OldIrql,
                                       &Waited);

        ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
    }
    else {
        UNLOCK_PFN (OldIrql);
    }

    if (FlushNeeded == TRUE) {
        KeFlushEntireTb (TRUE, TRUE);
    }

    if (!NT_SUCCESS (Status)) {

        //
        // Zero both the next and TB flush stamp PTEs before unmapping so
        // the unmap won't hit entries it can't decode.
        //

        MiMapCacheFailures += 1;
        PointerPte->u.List.NextEntry = 0;
        (PointerPte+1)->u.List.NextEntry = 0;

        MmUnmapViewInSystemCache (*CapturedBase, SectionToMap, FALSE);
        return Status;
    }

    ProtoPte = &Subsection->SubsectionBase[PteOffset];

    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    while (PointerPte <= LastPte) {

        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.
            //

            Subsection = Subsection->NextSubsection;
            ProtoPte = Subsection->SubsectionBase;
            LastProto = &Subsection->SubsectionBase[
                                        Subsection->PtesInSubsection];
        }
        PointerPte->u.Long = MiProtoAddressForKernelPte (ProtoPte);

        ASSERT (((ULONG_PTR)PointerPte & (MM_COLOR_MASK << PTE_SHIFT)) ==
                 (((ULONG_PTR)ProtoPte & (MM_COLOR_MASK << PTE_SHIFT))));

        PointerPte += 1;
        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiAddMappedPtes (
    IN PMMPTE FirstPte,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This function maps a view in the current address space to the
    specified control area.  The page protection is identical to that
    of the prototype PTE.

    This routine assumes the caller has called MiCheckPurgeAndUpMapCount,
    hence the PFN lock is not needed here.

Arguments:

    FirstPte - Supplies a pointer to the first PTE of the current address
               space to initialize.

    NumberOfPtes - Supplies the number of PTEs to initialize.

    ControlArea - Supplies the control area to point the PTEs at.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.
    
--*/

{
    PMMPTE PointerPte;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PMMPTE LastPte;
    PSUBSECTION Subsection;
    NTSTATUS Status;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PointerPte = FirstPte;
    ASSERT (NumberOfPtes != 0);
    LastPte = FirstPte + NumberOfPtes;

    ASSERT (ControlArea->NumberOfMappedViews >= 1);
    ASSERT (ControlArea->NumberOfUserReferences >= 1);
    ASSERT (ControlArea->u.Flags.HadUserReference == 1);
    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    ASSERT (ControlArea->u.Flags.BeingCreated == 0);
    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);
    ASSERT (ControlArea->u.Flags.BeingPurged == 0);

    if ((ControlArea->FilePointer != NULL) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->u.Flags.PhysicalMemory == 0)) {

        //
        // Increment the view count for every subsection spanned by this view.
        //

        Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION)Subsection,
                                              NumberOfPtes);

        if (!NT_SUCCESS (Status)) {
            return Status;
        }
    }

    ProtoPte = Subsection->SubsectionBase;

    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    while (PointerPte < LastPte) {

        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.
            //

            Subsection = Subsection->NextSubsection;
            ProtoPte = Subsection->SubsectionBase;
            LastProto = &Subsection->SubsectionBase[
                                        Subsection->PtesInSubsection];
        }
        ASSERT (PointerPte->u.Long == ZeroKernelPte.u.Long);
        PointerPte->u.Long = MiProtoAddressForKernelPte (ProtoPte);

        ASSERT (((ULONG_PTR)PointerPte & (MM_COLOR_MASK << PTE_SHIFT)) ==
                 (((ULONG_PTR)ProtoPte  & (MM_COLOR_MASK << PTE_SHIFT))));

        PointerPte += 1;
        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

VOID
MmUnmapViewInSystemCache (
    IN PVOID BaseAddress,
    IN PVOID SectionToUnmap,
    IN ULONG AddToFront
    )

/*++

Routine Description:

    This function unmaps a view from the system cache.

    NOTE: When this function is called, no pages may be locked in
    the cache for the specified view.

Arguments:

    BaseAddress - Supplies the base address of the section in the
                  system cache.

    SectionToUnmap - Supplies a pointer to the section which the
                     base address maps.

    AddToFront - Supplies TRUE if the unmapped pages should be
                 added to the front of the standby list (i.e., their
                 value in the cache is low).  FALSE otherwise.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG Waited;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE FirstPte;
    PMMPTE ProtoPte;
    MMPTE ProtoPteContents;
    MMPTE PteContents;
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    PFN_NUMBER i;
    WSLE_NUMBER WorkingSetIndex;
    PCONTROL_AREA ControlArea;
    ULONG WsHeld;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PMSUBSECTION MappedSubsection;
    PMSUBSECTION LastSubsection;
    PETHREAD CurrentThread;
#if DBG
    PFN_NUMBER j;
    PMSUBSECTION SubsectionArray[X256K / PAGE_SIZE];
    PMMPTE PteArray[X256K / PAGE_SIZE];

    RtlZeroMemory (SubsectionArray, sizeof(SubsectionArray));

    RtlCopyMemory (PteArray, MiGetPteAddress (BaseAddress), sizeof (PteArray));
#endif

    WsHeld = FALSE;

    //
    // Initializing OldIrqlWs is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    OldIrqlWs = 0x99;
    CurrentThread = PsGetCurrentThread ();

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    PointerPte = MiGetPteAddress (BaseAddress);
    FirstPte = PointerPte;
    PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress (PointerPte));
    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

    //
    // Get the control area for the segment which is mapped here.
    //

    ControlArea = ((PSECTION)SectionToUnmap)->Segment->ControlArea;
    LastSubsection = NULL;

    ASSERT ((ControlArea->u.Flags.Image == 0) &&
            (ControlArea->u.Flags.PhysicalMemory == 0));

    i = 0;

    do {

        //
        // The cache is organized in chunks of 256k bytes, clear
        // the first chunk then check to see if this is the last
        // chunk.
        //
        // The page table page is always resident for the system cache.
        // Check each PTE: it is in one of three states, either valid or
        // prototype PTE format or zero.
        //

        PteContents = *(volatile MMPTE *)PointerPte;
        if (PteContents.u.Hard.Valid == 1) {

            if (!WsHeld) {
                WsHeld = TRUE;
                LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
                continue;
            }

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            WorkingSetIndex = MiLocateWsle (BaseAddress,
                                            MmSystemCacheWorkingSetList,
                                            Pfn1->u1.WsIndex);

            MiRemoveWsle (WorkingSetIndex,
                          MmSystemCacheWorkingSetList);

            MiReleaseWsle (WorkingSetIndex, &MmSystemCacheWs);

            MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);

            //
            // The PTE is valid.
            //

            //
            // Decrement the view count for every subsection this view spans.
            // But make sure it's only done once per subsection in a given view.
            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            if (ControlArea->FilePointer != NULL) {
                ASSERT (Pfn1->u3.e1.PrototypePte);
                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype);

                if ((LastSubsection != NULL) &&
                    (Pfn1->PteAddress >= LastSubsection->SubsectionBase) &&
                    (Pfn1->PteAddress < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {
                    MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (&Pfn1->OriginalPte);
                    if (MappedSubsection->ControlArea != ControlArea) {
                        KeBugCheckEx (MEMORY_MANAGEMENT,
                                      0x780,
                                      (ULONG_PTR) PointerPte,
                                      (ULONG_PTR) Pfn1,
                                      (ULONG_PTR) Pfn1->OriginalPte.u.Long);
                    }

                    ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

                    if (LastSubsection != MappedSubsection) {

                        if (LastSubsection != NULL) {
#if DBG
                            for (j = 0; j < i; j += 1) {
                                ASSERT (SubsectionArray[j] != MappedSubsection);
                            }
                            SubsectionArray[i] = MappedSubsection;
#endif
                            LOCK_PFN (OldIrql);
                            MiRemoveViewsFromSection (LastSubsection,
                                                  LastSubsection->PtesInSubsection);
                            UNLOCK_PFN (OldIrql);
                        }
                        LastSubsection = MappedSubsection;
                    }
                }
            }

            LOCK_PFN (OldIrql);

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

#if DBG
            if (ControlArea->NumberOfMappedViews == 1) {
                ASSERT (Pfn1->u2.ShareCount == 1);
            }
#endif

            MmFrontOfList = AddToFront;
            MiDecrementShareCountInline (Pfn1, PageFrameIndex);
            MmFrontOfList = FALSE;

            UNLOCK_PFN (OldIrql);
        }
        else {

            ASSERT ((PteContents.u.Long == ZeroKernelPte.u.Long) ||
                    (PteContents.u.Soft.Prototype == 1));

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // Decrement the view count for every subsection this view
                // spans.  But make sure it's only done once per subsection
                // in a given view.
                //
    
                if (ControlArea->FilePointer != NULL) {

                    ProtoPte = MiPteToProto (&PteContents);

                    if ((LastSubsection != NULL) &&
                        (ProtoPte >= LastSubsection->SubsectionBase) &&
                        (ProtoPte < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                        NOTHING;
                    }
                    else {

                        LOCK_PFN (OldIrql);

                        //
                        // PTE is not valid, check the state of the prototype PTE.
                        //

                        if (WsHeld) {
                            Waited = MiMakeSystemAddressValidPfnSystemWs (ProtoPte);
                        }
                        else {
                            Waited = MiMakeSystemAddressValidPfn (ProtoPte);
                        }

                        if (Waited != 0) {

                            //
                            // Page fault occurred, recheck state of original PTE.
                            //

                            UNLOCK_PFN (OldIrql);
                            continue;
                        }

                        ProtoPteContents = *ProtoPte;
    
                        if (ProtoPteContents.u.Hard.Valid == 1) {
                            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
                            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                            ProtoPte = &Pfn1->OriginalPte;
                        }
                        else if ((ProtoPteContents.u.Soft.Transition == 1) &&
                                 (ProtoPteContents.u.Soft.Prototype == 0)) {
                            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
                            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                            ProtoPte = &Pfn1->OriginalPte;
                        }
                        else {
                            Pfn1 = NULL;
                            ASSERT (ProtoPteContents.u.Soft.Prototype == 1);
                        }
    
                        MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (ProtoPte);
                        if (MappedSubsection->ControlArea != ControlArea) {
                            KeBugCheckEx (MEMORY_MANAGEMENT,
                                          0x781,
                                          (ULONG_PTR) PointerPte,
                                          (ULONG_PTR) Pfn1,
                                          (ULONG_PTR) ProtoPte);
                        }
    
                        ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));
    
                        if (LastSubsection != MappedSubsection) {
        
                            if (LastSubsection != NULL) {
    #if DBG
                                for (j = 0; j < i; j += 1) {
                                    ASSERT (SubsectionArray[j] != MappedSubsection);
                                }
                                SubsectionArray[i] = MappedSubsection;
    #endif
                                MiRemoveViewsFromSection (LastSubsection,
                                                          LastSubsection->PtesInSubsection);
                            }
                            LastSubsection = MappedSubsection;
                        }
    
                        UNLOCK_PFN (OldIrql);
                    }
                }
            }

            if (WsHeld) {
                UNLOCK_SYSTEM_WS (OldIrqlWs);
                WsHeld = FALSE;
            }
        }
        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        i += 1;
    } while (i < (X256K / PAGE_SIZE));

    if (WsHeld) {
        UNLOCK_SYSTEM_WS (OldIrqlWs);
    }

    FirstPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

    (FirstPte+1)->u.List.NextEntry = (KeReadTbFlushTimeStamp() & MM_FLUSH_COUNTER_MASK);

    LOCK_PFN (OldIrql);

    //
    // Free this entry to the end of the list.
    //

    MmLastFreeSystemCache->u.List.NextEntry = FirstPte - MmSystemCachePteBase;
    MmLastFreeSystemCache = FirstPte;

    if (LastSubsection != NULL) {
        MiRemoveViewsFromSection (LastSubsection,
                                  LastSubsection->PtesInSubsection);
    }

    //
    // Decrement the number of mapped views for the segment
    // and check to see if the segment should be deleted.
    //

    ControlArea->NumberOfMappedViews -= 1;
    ControlArea->NumberOfSystemCacheViews -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);

    return;
}


VOID
MiRemoveMappedPtes (
    IN PVOID BaseAddress,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea,
    IN PMMSUPPORT WorkingSetInfo
    )

/*++

Routine Description:

    This function unmaps a view from the system cache or a session space.

    NOTE: When this function is called, no pages may be locked in
    the cache (or session space) for the specified view.

Arguments:

    BaseAddress - Supplies the base address of the section in the
                  system cache or session space.

    NumberOfPtes - Supplies the number of PTEs to unmap.

    ControlArea - Supplies the control area mapping the view.

    WorkingSetInfo - Supplies the charged working set structures.

Return Value:

    None.

Environment:

    Kernel mode.
    
    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    ULONG Waited;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    PMMPTE FirstPte;
    PMMPTE ProtoPte;
    MMPTE PteContents;
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    WSLE_NUMBER WorkingSetIndex;
    ULONG DereferenceSegment;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE ProtoPteContents;
    PFN_NUMBER PageFrameIndex;
    ULONG WsHeld;
    PMMPFN Pfn2;
    PFN_NUMBER PageTableFrameIndex;
    PMSUBSECTION MappedSubsection;
    PMSUBSECTION LastSubsection;
    PETHREAD CurrentThread;

    CurrentThread = PsGetCurrentThread ();
    DereferenceSegment = FALSE;
    WsHeld = FALSE;
    LastSubsection = NULL;

    PteFlushList.Count = 0;
    PointerPte = MiGetPteAddress (BaseAddress);
    FirstPte = PointerPte;

    //
    // Initializing OldIrqlWs is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    OldIrqlWs = 0x99;

    //
    // Get the control area for the segment which is mapped here.
    //

    while (NumberOfPtes) {

        //
        // The page table page is always resident for the system space (and
        // for a session space) map.
        //
        // Check each PTE, it is in one of two states, either valid or
        // prototype PTE format.
        //

        PteContents = *PointerPte;
        if (PteContents.u.Hard.Valid == 1) {

            //
            // The system cache is locked by us, all others are locked by
            // the caller.
            //

            if (WorkingSetInfo == &MmSystemCacheWs) {
                if (!WsHeld) {
                    WsHeld = TRUE;
                    LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
                    continue;
                }
            }

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            WorkingSetIndex = MiLocateWsle (BaseAddress,
                                            WorkingSetInfo->VmWorkingSetList,
                                            Pfn1->u1.WsIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            MiRemoveWsle (WorkingSetIndex,
                          WorkingSetInfo->VmWorkingSetList);

            MiReleaseWsle (WorkingSetIndex, WorkingSetInfo);

            MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);

            LOCK_PFN (OldIrql);

            //
            // The PTE is valid.
            //

            //
            // Decrement the view count for every subsection this view spans.
            // But make sure it's only done once per subsection in a given view.
            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            if ((Pfn1->u3.e1.PrototypePte) &&
                (Pfn1->OriginalPte.u.Soft.Prototype)) {

                if ((LastSubsection != NULL) &&
                    (Pfn1->PteAddress >= LastSubsection->SubsectionBase) &&
                    (Pfn1->PteAddress < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {

                    MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (&Pfn1->OriginalPte);
                    if (LastSubsection != MappedSubsection) {

                        ASSERT (ControlArea == MappedSubsection->ControlArea);

                        if ((ControlArea->FilePointer != NULL) &&
                            (ControlArea->u.Flags.Image == 0) &&
                            (ControlArea->u.Flags.PhysicalMemory == 0)) {

                            if (LastSubsection != NULL) {
                                MiRemoveViewsFromSection (LastSubsection,
                                                      LastSubsection->PtesInSubsection);
                            }
                            LastSubsection = MappedSubsection;
                        }
                    }
                }
            }

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Flush the TB for this page.
            //

            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushPte[PteFlushList.Count] = PointerPte;
                PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
                PteFlushList.Count += 1;
            }

            PointerPde = MiGetPteAddress (PointerPte);

#if (_MI_PAGING_LEVELS < 3)

            //
            // The PDE must be carefully checked against the master table
            // because the PDEs are all zeroed in process creation.  If this
            // process has never faulted on any address in this range (all
            // references prior and above were filled directly by the TB as
            // the PTEs are global on non-Hydra), then the PDE reference
            // below to determine the page table frame will be zero.
            //
            // Note this cannot happen on NT64 as no master table is used.
            //

            if (PointerPde->u.Long == 0) {

                PMMPTE MasterPde;

                MasterPde = &MmSystemPagePtes [((ULONG_PTR)PointerPde &
                             (PD_PER_SYSTEM * (sizeof(MMPTE) * PDE_PER_PAGE) - 1)) / sizeof(MMPTE)];

                ASSERT (MasterPde->u.Hard.Valid == 1);
                MI_WRITE_VALID_PTE (PointerPde, *MasterPde);
            }
#endif

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

            MiDecrementShareCount (MI_GET_PAGE_FRAME_FROM_PTE (&PteContents));
            UNLOCK_PFN (OldIrql);

        }
        else {
            if (WorkingSetInfo == &MmSystemCacheWs) {
                if (WsHeld) {
                    UNLOCK_SYSTEM_WS (OldIrqlWs);
                    WsHeld = FALSE;
                }
            }

            ASSERT ((PteContents.u.Long == ZeroKernelPte.u.Long) ||
                    (PteContents.u.Soft.Prototype == 1));

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // Decrement the view count for every subsection this view
                // spans.  But make sure it's only done once per subsection
                // in a given view.
                //
    
                ProtoPte = MiPteToProto (&PteContents);

                if ((LastSubsection != NULL) &&
                    (ProtoPte >= LastSubsection->SubsectionBase) &&
                    (ProtoPte < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {

                    //
                    // PTE is not valid, check the state of the prototype PTE.
                    //

                    LOCK_PFN (OldIrql);

                    if (WsHeld) {
                        Waited = MiMakeSystemAddressValidPfnSystemWs (ProtoPte);
                    }
                    else {
                        Waited = MiMakeSystemAddressValidPfn (ProtoPte);
                    }

                    if (Waited != 0) {

                        //
                        // Page fault occurred, recheck state of original PTE.
                        //

                        UNLOCK_PFN (OldIrql);
                        continue;
                    }

                    ProtoPteContents = *ProtoPte;
    
                    if (ProtoPteContents.u.Hard.Valid == 1) {
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                        ProtoPte = &Pfn1->OriginalPte;
                        if (ProtoPte->u.Soft.Prototype == 0) {
                            ProtoPte = NULL;
                        }
                    }
                    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
                             (ProtoPteContents.u.Soft.Prototype == 0)) {
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                        ProtoPte = &Pfn1->OriginalPte;
                        if (ProtoPte->u.Soft.Prototype == 0) {
                            ProtoPte = NULL;
                        }
                    }
                    else if (ProtoPteContents.u.Soft.Prototype == 1) {
                        NOTHING;
                    }
                    else {
    
                        //
                        // Could be a zero PTE or a demand zero PTE.
                        // Neither belong to a mapped file.
                        //
    
                        ProtoPte = NULL;
                    }
    
                    if (ProtoPte != NULL) {
    
                        MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (ProtoPte);
                        if (LastSubsection != MappedSubsection) {
        
                            ASSERT (ControlArea == MappedSubsection->ControlArea);
        
                            if ((ControlArea->FilePointer != NULL) &&
                                (ControlArea->u.Flags.Image == 0) &&
                                (ControlArea->u.Flags.PhysicalMemory == 0)) {
        
                                if (LastSubsection != NULL) {
                                    MiRemoveViewsFromSection (LastSubsection,
                                                              LastSubsection->PtesInSubsection);
                                }
                                LastSubsection = MappedSubsection;
                            }
                        }
                    }
                    UNLOCK_PFN (OldIrql);
                }
            }
        }
        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        NumberOfPtes -= 1;
    }

    if (WorkingSetInfo == &MmSystemCacheWs) {
        if (WsHeld) {
            UNLOCK_SYSTEM_WS (OldIrqlWs);
        }
    }

    LOCK_PFN (OldIrql);

    if (LastSubsection != NULL) {
        MiRemoveViewsFromSection (LastSubsection,
                                  LastSubsection->PtesInSubsection);
    }

    MiFlushPteList (&PteFlushList, TRUE, ZeroKernelPte);

    if (WorkingSetInfo != &MmSystemCacheWs) {

        //
        // Session space has no ASN - flush the entire TB.
        //
    
        MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
    }

    //
    // Decrement the number of user references as the caller upped them
    // via MiCheckPurgeAndUpMapCount when this was originally mapped.
    //

    ControlArea->NumberOfUserReferences -= 1;

    //
    // Decrement the number of mapped views for the segment
    // and check to see if the segment should be deleted.
    //

    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}

VOID
MiInitializeSystemCache (
    IN ULONG MinimumWorkingSet,
    IN ULONG MaximumWorkingSet
    )

/*++

Routine Description:

    This routine initializes the system cache working set and
    data management structures.

Arguments:

    MinimumWorkingSet - Supplies the minimum working set for the system
                        cache.

    MaximumWorkingSet - Supplies the maximum working set size for the
                        system cache.

Return Value:

    None.

Environment:

    Kernel mode, called only at phase 0 initialization.

--*/

{
    ULONG_PTR SizeOfSystemCacheInPages;
    ULONG_PTR HunksOf256KInCache;
    PMMWSLE WslEntry;
    ULONG NumberOfEntriesMapped;
    PFN_NUMBER i;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    KIRQL OldIrql;

    PointerPte = MiGetPteAddress (MmSystemCacheWorkingSetList);

    PteContents = ValidKernelPte;

    LOCK_PFN (OldIrql);

    i = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    PteContents.u.Hard.PageFrameNumber = i;

    MI_WRITE_VALID_PTE (PointerPte, PteContents);

    MiInitializePfn (i, PointerPte, 1L);

    UNLOCK_PFN (OldIrql);

#if defined (_WIN64)
    MmSystemCacheWsle = (PMMWSLE)(MmSystemCacheWorkingSetList + 1);
#else
    MmSystemCacheWsle =
            (PMMWSLE)(&MmSystemCacheWorkingSetList->UsedPageTableEntries[0]);
#endif

    MmSystemCacheWs.VmWorkingSetList = MmSystemCacheWorkingSetList;
    MmSystemCacheWs.WorkingSetSize = 0;
    MmSystemCacheWs.MinimumWorkingSetSize = MinimumWorkingSet;
    MmSystemCacheWs.MaximumWorkingSetSize = MaximumWorkingSet;
    InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                    &MmSystemCacheWs.WorkingSetExpansionLinks);

    MmSystemCacheWs.Flags.AllowWorkingSetAdjustment = TRUE;

    //
    // Don't use entry 0 as an index of zero in the PFN database
    // means that the page can be assigned to a slot.  This is not
    // a problem for process working sets as page 0 is private.
    //

    MmSystemCacheWorkingSetList->FirstFree = 1;
    MmSystemCacheWorkingSetList->FirstDynamic = 1;
    MmSystemCacheWorkingSetList->NextSlot = 1;
    MmSystemCacheWorkingSetList->LastEntry = (ULONG)MmSystemCacheWsMinimum;
    MmSystemCacheWorkingSetList->HashTable = NULL;
    MmSystemCacheWorkingSetList->HashTableSize = 0;
    MmSystemCacheWorkingSetList->Wsle = MmSystemCacheWsle;

    MmSystemCacheWorkingSetList->HashTableStart = 
       (PVOID)((PCHAR)PAGE_ALIGN (&MmSystemCacheWorkingSetList->Wsle[MM_MAXIMUM_WORKING_SET]) + PAGE_SIZE);

    MmSystemCacheWorkingSetList->HighestPermittedHashAddress = (PVOID)(MM_SYSTEM_CACHE_START);

    NumberOfEntriesMapped = (ULONG)(((PMMWSLE)((PCHAR)MmSystemCacheWorkingSetList +
                                PAGE_SIZE)) - MmSystemCacheWsle);

    LOCK_PFN (OldIrql);

    while (NumberOfEntriesMapped < MmSystemCacheWsMaximum) {

        PointerPte += 1;
	    if (MiIsPteOnPdeBoundary(PointerPte)) {
	        PointerPde = MiGetPteAddress(PointerPte);
	        if (PointerPde->u.Hard.Valid == 0) {
		        i = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
		        PteContents.u.Hard.PageFrameNumber = i;
		        MI_WRITE_VALID_PTE(PointerPde, PteContents);
		        MiInitializePfn (i, PointerPde, 1L);
	        }
	    }

        i = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
        PteContents.u.Hard.PageFrameNumber = i;
        MI_WRITE_VALID_PTE (PointerPte, PteContents);
        MiInitializePfn (i, PointerPte, 1L);
        NumberOfEntriesMapped += PAGE_SIZE / sizeof(MMWSLE);
    }

    UNLOCK_PFN (OldIrql);

    //
    // Initialize the following slots as free.
    //

    WslEntry = MmSystemCacheWsle + 1;

    for (i = 1; i < NumberOfEntriesMapped; i++) {

        //
        // Build the free list, note that the first working
        // set entries (CurrentEntry) are not on the free list.
        // These entries are reserved for the pages which
        // map the working set and the page which contains the PDE.
        //

        WslEntry->u1.Long = (i + 1) << MM_FREE_WSLE_SHIFT;
        WslEntry += 1;
    }

    WslEntry -= 1;
    WslEntry->u1.Long = WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT;  // End of list.

    MmSystemCacheWorkingSetList->LastInitializedWsle = NumberOfEntriesMapped - 1;

    //
    // Build a free list structure in the PTEs for the system cache.
    //

    MmSystemCachePteBase = MI_PTE_BASE_FOR_LOWEST_KERNEL_ADDRESS;

    SizeOfSystemCacheInPages = MI_COMPUTE_PAGES_SPANNED (MmSystemCacheStart,
                                (PCHAR)MmSystemCacheEnd - (PCHAR)MmSystemCacheStart + 1);

    HunksOf256KInCache = SizeOfSystemCacheInPages / (X256K / PAGE_SIZE);

    PointerPte = MiGetPteAddress (MmSystemCacheStart);

    MmFirstFreeSystemCache = PointerPte;

    for (i = 0; i < HunksOf256KInCache; i += 1) {
        PointerPte->u.List.NextEntry = (PointerPte + (X256K / PAGE_SIZE)) - MmSystemCachePteBase;
        PointerPte += X256K / PAGE_SIZE;
    }

    PointerPte -= X256K / PAGE_SIZE;

#if defined(_X86_)

    //
    // Add any extended ranges.
    //

    if (MiSystemCacheEndExtra != MmSystemCacheEnd) {

        SizeOfSystemCacheInPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MiSystemCacheStartExtra,
                                    (PCHAR)MiSystemCacheEndExtra - (PCHAR)MiSystemCacheStartExtra + 1);
    
        HunksOf256KInCache = SizeOfSystemCacheInPages / (X256K / PAGE_SIZE);
    
        if (HunksOf256KInCache) {

            PMMPTE PointerPteExtended;
    
            PointerPteExtended = MiGetPteAddress (MiSystemCacheStartExtra);
            PointerPte->u.List.NextEntry = PointerPteExtended - MmSystemCachePteBase;
            PointerPte = PointerPteExtended;

            for (i = 0; i < HunksOf256KInCache; i += 1) {
                PointerPte->u.List.NextEntry = (PointerPte + (X256K / PAGE_SIZE)) - MmSystemCachePteBase;
                PointerPte += X256K / PAGE_SIZE;
            }
    
            PointerPte -= X256K / PAGE_SIZE;
        }
    }
#endif

    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;
    MmLastFreeSystemCache = PointerPte;

    if (MaximumWorkingSet > ((1536*1024) >> PAGE_SHIFT)) {

        //
        // The working set list consists of more than a single page.
        //

        LOCK_SYSTEM_WS (OldIrql, PsGetCurrentThread ());
        MiGrowWsleHash (&MmSystemCacheWs);
        UNLOCK_SYSTEM_WS (OldIrql);
    }
}

BOOLEAN
MmCheckCachedPageState (
    IN PVOID SystemCacheAddress,
    IN BOOLEAN SetToZero
    )

/*++

Routine Description:

    This routine checks the state of the specified page that is mapped in
    the system cache.  If the specified virtual address can be made valid
    (i.e., the page is already in memory), it is made valid and the value
    TRUE is returned.

    If the page is not in memory, and SetToZero is FALSE, the
    value FALSE is returned.  However, if SetToZero is TRUE, a page of
    zeroes is materialized for the specified virtual address and the address
    is made valid and the value TRUE is returned.

    This routine is for usage by the cache manager.

Arguments:

    SystemCacheAddress - Supplies the address of a page mapped in the
                         system cache.

    SetToZero - Supplies TRUE if a page of zeroes should be created in the
                case where no page is already mapped.

Return Value:

    FALSE if touching this page would cause a page fault resulting
          in a page read.

    TRUE if there is a physical page in memory for this address.

Environment:

    Kernel mode.

--*/

{
    ULONG Flags;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    MMPTE TempPte;
    MMPTE ProtoPteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    LOGICAL BarrierNeeded;
    ULONG BarrierStamp;
    PSUBSECTION Subsection;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;

    PointerPte = MiGetPteAddress (SystemCacheAddress);

    //
    // Make the PTE valid if possible.
    //

    if (PointerPte->u.Hard.Valid == 1) {
        return TRUE;
    }

    BarrierNeeded = FALSE;
    Subsection = NULL;

    //
    // Initializing BarrierStamp is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BarrierStamp = 0;

    LOCK_PFN (OldIrql);

    if (PointerPte->u.Hard.Valid == 1) {
        goto UnlockAndReturnTrue;
    }

    ASSERT (PointerPte->u.Soft.Prototype == 1);

    ProtoPte = MiPteToProto (PointerPte);

    //
    // PTE is not valid, check the state of the prototype PTE.
    //

    if (MiMakeSystemAddressValidPfn (ProtoPte)) {

        //
        // If page fault occurred, recheck state of original PTE.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            goto UnlockAndReturnTrue;
        }
    }

    ProtoPteContents = *ProtoPte;

    if (ProtoPteContents.u.Hard.Valid == 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // The prototype PTE is valid, make the cache PTE
        // valid and add it to the working set.
        //

        TempPte = ProtoPteContents;

    }
    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
               (ProtoPteContents.u.Soft.Prototype == 0)) {

        //
        // Prototype PTE is in the transition state.  Remove the page
        // from the page list and make it valid.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if ((Pfn1->u3.e1.ReadInProgress) || (Pfn1->u4.InPageError)) {

            //
            // Collided page fault, return.
            //

            goto UnlockAndReturnTrue;
        }

        if (MmAvailablePages == 0) {

            //
            // This can only happen if the system is utilizing
            // a hardware compression cache.  This ensures that
            // only a safe amount of the compressed virtual cache
            // is directly mapped so that if the hardware gets
            // into trouble, we can bail it out.
            //
            // Just unlock everything here to give the compression
            // reaper a chance to ravage pages and then retry.
            //

            goto UnlockAndReturnTrue;
        }

        MiUnlinkPageFromList (Pfn1);

        Pfn1->u3.e2.ReferenceCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        MI_SNAP_DATA (Pfn1, ProtoPte, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL );

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);

        //
        // Increment the valid PTE count for the page containing
        // the prototype PTE.
        //

        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

    }
    else {

        //
        // Page is not in memory, if a page of zeroes is requested,
        // get a page of zeroes and make it valid.
        //

        if ((SetToZero == FALSE) || (MmAvailablePages < 8)) {
            UNLOCK_PFN (OldIrql);

            //
            // Fault the page into memory.
            //

            MmAccessFault (FALSE, SystemCacheAddress, KernelMode, (PVOID)0);
            return FALSE;
        }

        //
        // Increment the count of Pfn references for the control area
        // corresponding to this file.
        //

        MiGetSubsectionAddress (
                    ProtoPte)->ControlArea->NumberOfPfnReferences += 1;

        PageFrameIndex = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (ProtoPte));

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // This barrier check is needed after zeroing the page and
        // before setting the PTE (not the prototype PTE) valid.
        // Capture it now, check it at the last possible moment.
        //

        BarrierNeeded = TRUE;
        BarrierStamp = (ULONG)Pfn1->u4.PteFrame;

        MiInitializePfn (PageFrameIndex, ProtoPte, 1);
        Pfn1->u2.ShareCount = 0;
        Pfn1->u3.e1.PrototypePte = 1;

        MI_SNAP_DATA (Pfn1, ProtoPte, 2);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL );

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);
    }

    //
    // Increment the share count since the page is being put into a working
    // set.
    //

    Pfn1->u2.ShareCount += 1;

    if (Pfn1->u1.Event == NULL) {
        Pfn1->u1.Event = (PVOID)PsGetCurrentThread();
    }

    //
    // Increment the reference count of the page table
    // page for this PTE.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

    Pfn2->u2.ShareCount += 1;

    MI_SET_GLOBAL_STATE (TempPte, 1);

#if defined (_WIN64)
    if (MI_DETERMINE_OWNER (PointerPte) == 0) {
        TempPte.u.Long &= ~MM_PTE_OWNER_MASK;
    }
#else
    TempPte.u.Hard.Owner = MI_DETERMINE_OWNER (PointerPte);
#endif

    if (BarrierNeeded) {
        MI_BARRIER_SYNCHRONIZE (BarrierStamp);
    }

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    //
    // Capture prefetch fault information.
    //

    if (CCPF_IS_PREFETCHER_ACTIVE()) {

        TempPte = Pfn1->OriginalPte;

        if (TempPte.u.Soft.Prototype == 1) {

            Subsection = MiGetSubsectionAddress (&TempPte);
        }
    }

    UNLOCK_PFN (OldIrql);

    //
    // Log prefetch fault information now that the PFN lock has been
    // released and the PTE has been made valid.  This minimizes PFN
    // lock contention, allows CcPfLogPageFault to allocate (and fault on)
    // pool, and allows other threads in this process to execute without
    // faulting on this address.
    //

    if (Subsection != NULL) {

        FileObject = Subsection->ControlArea->FilePointer;
        FileOffset = MiStartingOffset (Subsection, ProtoPte);

        Flags = 0;

        ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

        if (Subsection->ControlArea->u.Flags.Rom) {
            Flags |= CCPF_TYPE_ROM;
        }

        CcPfLogPageFault (FileObject, FileOffset, Flags);
    }

    LOCK_SYSTEM_WS (OldIrql, PsGetCurrentThread ());

    WorkingSetIndex = MiLocateAndReserveWsle (&MmSystemCacheWs);

    MiUpdateWsle (&WorkingSetIndex,
                  MiGetVirtualAddressMappedByPte (PointerPte),
                  MmSystemCacheWorkingSetList,
                  Pfn1);

    MmSystemCacheWsle[WorkingSetIndex].u1.e1.SameProtectAsProto = 1;

    MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

    UNLOCK_SYSTEM_WS (OldIrql);

    return TRUE;

UnlockAndReturnTrue:
    UNLOCK_PFN (OldIrql);
    return TRUE;
}

NTSTATUS
MmCopyToCachedPage (
    IN PVOID SystemCacheAddress,
    IN PVOID UserBuffer,
    IN ULONG Offset,
    IN SIZE_T CountInBytes,
    IN BOOLEAN DontZero
    )

/*++

Routine Description:

    This routine checks the state of the specified page that is mapped in
    the system cache.  If the specified virtual address can be made valid
    (i.e., the page is already in memory), it is made valid and the value
    TRUE is returned.

    If the page is not in memory, and SetToZero is FALSE, the
    value FALSE is returned.  However, if SetToZero is TRUE, a page of
    zeroes is materialized for the specified virtual address and the address
    is made valid and the value TRUE is returned.

    This routine is for usage by the cache manager.

Arguments:

    SystemCacheAddress - Supplies the address of a page mapped in the system
                         cache.  This MUST be a page aligned address!

    UserBuffer - Supplies the address of a user buffer to copy into the
                 system cache at the specified address + offset.

    Offset - Supplies the offset into the UserBuffer to copy the data.

    CountInBytes - Supplies the byte count to copy from the user buffer.

    DontZero - Supplies TRUE if the buffer should not be zeroed (the
               caller will track zeroing).  FALSE if it should be zeroed.

Return Value:

    Returns the status of the copy.

Environment:

    Kernel mode, <= APC_LEVEL.

--*/

{
    ULONG Flags;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    MMPTE TempPte;
    MMPTE TempPte2;
    MMPTE ProtoPteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    ULONG TransitionState;
    ULONG AddToWorkingSet;
    LOGICAL ShareCountUpped;
    SIZE_T EndFill;
    PVOID Buffer;
    NTSTATUS status;
    PMMINPAGE_SUPPORT Event;
    PCONTROL_AREA ControlArea;
    PETHREAD Thread;
    ULONG SavedState;
    LOGICAL ApcNeeded;
    PKTHREAD CurrentThread;
    PSUBSECTION Subsection;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;

    TransitionState = FALSE;
    AddToWorkingSet = FALSE;
    CurrentThread = NULL;
    ApcNeeded = FALSE;
    Subsection = NULL;

    //
    // Initializing these is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    ProtoPte = NULL;
    ShareCountUpped = FALSE;
    TempPte.u.Long = 0;
    Event = NULL;
    Pfn1 = NULL;

    ASSERT (((ULONG_PTR)SystemCacheAddress & (PAGE_SIZE - 1)) == 0);
    ASSERT ((CountInBytes + Offset) <= PAGE_SIZE);
    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    PointerPte = MiGetPteAddress (SystemCacheAddress);

    if (PointerPte->u.Hard.Valid == 1) {
        goto Copy;
    }

    //
    // Touch the user's buffer to make it resident.  This is required in
    // order to safely detect the case where both the system and user
    // address are pointing at the same physical page.  This case causes
    // a deadlock during the RtlCopyBytes if the inpage support block needed
    // to be allocated and the PTE for the user page is not valid.  This
    // potential deadlock is resolved because if the user page causes a
    // collided fault, the initiator thread is checked for.  If they are
    // the same, then an exception is thrown by the pager.
    //

    try {

        *(volatile CHAR *)UserBuffer;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    //
    // Make the PTE valid if possible.
    //

    LOCK_PFN (OldIrql);

Recheck:

    if (PointerPte->u.Hard.Valid == 1) {
        goto UnlockAndCopy;
    }

    ASSERT (PointerPte->u.Soft.Prototype == 1);

    ProtoPte = MiPteToProto (PointerPte);

    //
    // Pte is not valid, check the state of the prototype PTE.
    //

    if (MiMakeSystemAddressValidPfn (ProtoPte)) {

        //
        // If page fault occurred, recheck state of original PTE.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            goto UnlockAndCopy;
        }
    }

    ShareCountUpped = FALSE;
    ProtoPteContents = *ProtoPte;

    if (ProtoPteContents.u.Hard.Valid == 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Increment the share count so the prototype PTE will remain
        // valid until this can be added into the system's working set.
        //

        Pfn1->u2.ShareCount += 1;
        ShareCountUpped = TRUE;

        //
        // The prototype PTE is valid, make the cache PTE
        // valid and add it to the working set.
        //

        TempPte = ProtoPteContents;
    }
    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
               (ProtoPteContents.u.Soft.Prototype == 0)) {

        //
        // Prototype PTE is in the transition state.  Remove the page
        // from the page list and make it valid.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if ((Pfn1->u3.e1.ReadInProgress) || (Pfn1->u4.InPageError)) {

            //
            // Collided page fault or in page error, try the copy
            // operation incurring a page fault.
            //

            goto UnlockAndCopy;
        }

        ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);

        if (MmAvailablePages == 0) {

            //
            // This can only happen if the system is utilizing a hardware
            // compression cache.  This ensures that only a safe amount
            // of the compressed virtual cache is directly mapped so that
            // if the hardware gets into trouble, we can bail it out.
            //

            MiEnsureAvailablePageOrWait (NULL, SystemCacheAddress);

            //
            // A wait operation occurred which could have changed the
            // state of the PTE.  Recheck the PTE state.
            //

            goto Recheck;
        }

        MiUnlinkPageFromList (Pfn1);

        Pfn1->u3.e2.ReferenceCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        MI_SET_MODIFIED (Pfn1, 1, 0x6);

        ASSERT (Pfn1->u2.ShareCount == 0);
        Pfn1->u2.ShareCount += 1;
        ShareCountUpped = TRUE;

        MI_SNAP_DATA (Pfn1, ProtoPte, 3);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL);

        MI_SET_PTE_DIRTY (TempPte);

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);

        //
        // Increment the valid PTE count for the page containing
        // the prototype PTE.
        //
    }
    else {

        //
        // Page is not in memory, if a page of zeroes is requested,
        // get a page of zeroes and make it valid.
        //

        if (MiEnsureAvailablePageOrWait (NULL, SystemCacheAddress)) {

            //
            // A wait operation occurred which could have changed the
            // state of the PTE.  Recheck the PTE state.
            //

            goto Recheck;
        }

        Event = MiGetInPageSupportBlock (TRUE, NULL);
        if (Event == NULL) {

            //
            // A delay has already occurred if the allocation really failed
            // so no need to do another here, just retry immediately.
            //

            goto Recheck;
        }

        //
        // Increment the count of Pfn references for the control area
        // corresponding to this file.
        //

        ControlArea = MiGetSubsectionAddress (ProtoPte)->ControlArea;
        ControlArea->NumberOfPfnReferences += 1;
        if (ControlArea->NumberOfUserReferences > 0) {

            //
            // There is a user reference to this file, always zero ahead.
            //

            DontZero = FALSE;
        }

        //
        // Remove any page from the list and turn it into a transition
        // page in the cache with read in progress set.  This causes
        // any other references to this page to block on the specified
        // event while the copy operation to the cache is on-going.
        //

        PageFrameIndex = MiRemoveAnyPage(MI_GET_PAGE_COLOR_FROM_PTE (ProtoPte));

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Increment the valid PTE count for the page containing
        // the prototype PTE.
        //

        MiInitializeTransitionPfn (PageFrameIndex, ProtoPte);

        Pfn1->u2.ShareCount = 0;

        Pfn1->u3.e2.ReferenceCount = 0;     // for the add_locked_page macro
        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 24);

        Pfn1->u3.e2.ReferenceCount = 1;
        Pfn1->u3.e1.PrototypePte = 1;

        MI_SET_MODIFIED (Pfn1, 1, 0x7);

        Pfn1->u3.e1.ReadInProgress = 1;
        Pfn1->u1.Event = &Event->Event;
        Event->Pfn = Pfn1;

        //
        // This is needed in case a special kernel APC fires that ends up
        // referencing the same page (this may even be through a different
        // virtual address from the user/system one here).
        //

        Thread = PsGetCurrentThread ();
        ASSERT (Thread->NestedFaultCount <= 1);
        Thread->NestedFaultCount += 1;

        TransitionState = TRUE;

        MI_SNAP_DATA (Pfn1, ProtoPte, 4);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL);
        MI_SET_PTE_DIRTY (TempPte);

        //
        // APCs must be explicitly disabled to prevent suspend APCs from
        // interrupting this thread before the RtlCopyBytes completes.
        // Otherwise this page can remain in transition indefinitely (until
        // the suspend APC is released) which blocks any other threads that
        // may reference it.
        //

        KeEnterCriticalRegionThread (&Thread->Tcb);
    }

    //
    // Capture prefetch fault information.
    //

    if (CCPF_IS_PREFETCHER_ACTIVE()) {

        TempPte2 = Pfn1->OriginalPte;

        if (TempPte2.u.Soft.Prototype == 1) {

            Subsection = MiGetSubsectionAddress (&TempPte2);
        }
    }

    //
    // Increment the share count of the page table page for this PTE.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

    Pfn2->u2.ShareCount += 1;

    MI_SET_GLOBAL_STATE (TempPte, 1);
#if defined (_WIN64)
    if (MI_DETERMINE_OWNER (PointerPte) == 0) {
        TempPte.u.Long &= ~MM_PTE_OWNER_MASK;
    }
#else
    TempPte.u.Hard.Owner = MI_DETERMINE_OWNER (PointerPte);
#endif
    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    AddToWorkingSet = TRUE;

UnlockAndCopy:

    //
    // Unlock the PFN database and perform the copy.
    //

    UNLOCK_PFN (OldIrql);

Copy:

    Thread = PsGetCurrentThread ();
    MmSavePageFaultReadAhead (Thread, &SavedState);
    MmSetPageFaultReadAhead (Thread, 0);
    status = STATUS_SUCCESS;

    //
    // Copy the user buffer into the cache under an exception handler.
    //

    try {

        Buffer = (PVOID)((PCHAR)SystemCacheAddress + Offset);
        RtlCopyBytes (Buffer, UserBuffer, CountInBytes);

        if (TransitionState) {

            //
            // Only zero the memory outside the range if a page was taken
            // from the free list.
            //

            if (Offset != 0) {
                RtlZeroMemory (SystemCacheAddress, Offset);
            }

            if (DontZero == FALSE) {
                EndFill = PAGE_SIZE - (Offset + CountInBytes);

                if (EndFill != 0) {
                    Buffer = (PVOID)((PCHAR)Buffer + CountInBytes);
                    RtlZeroMemory (Buffer, EndFill);
                }
            }
        }
    } except (MiMapCacheExceptionFilter (&status, GetExceptionInformation())) {

        if (status == STATUS_MULTIPLE_FAULT_VIOLATION) {
            ASSERT (TransitionState == TRUE);
        }

        //
        // Zero out the page if it came from the free list.
        //

        if (TransitionState) {
            RtlZeroMemory (SystemCacheAddress, PAGE_SIZE);
        }
    }

    MmResetPageFaultReadAhead (Thread, SavedState);

    if (AddToWorkingSet) {

        LOCK_PFN (OldIrql);

        ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
        ASSERT (Pfn1->PteAddress == ProtoPte);

        if (TransitionState) {

            KeLeaveCriticalRegionThread (&Thread->Tcb);

            //
            // This is a newly allocated page.
            //

            ASSERT (ShareCountUpped == FALSE);
            ASSERT (Pfn1->u2.ShareCount <= 1);
            ASSERT (Pfn1->u1.Event == &Event->Event);

            MiMakeSystemAddressValidPfn (ProtoPte);
            MI_SET_GLOBAL_STATE (TempPte, 0);
            MI_WRITE_VALID_PTE (ProtoPte, TempPte);
            Pfn1->u1.Event = (PVOID)Thread;
            ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            ASSERT (Event->u1.e1.Completed == 0);
            Event->u1.e1.Completed = 1;

            ASSERT (Pfn1->u2.ShareCount == 0);
            MI_REMOVE_LOCKED_PAGE_CHARGE(Pfn1, 41);
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiCached;

            ASSERT (Pfn1->u3.e1.ReadInProgress == 1);
            Pfn1->u3.e1.ReadInProgress = 0;

            //
            // Increment the share count since the page is
            // being put into a working set.
            //

            Pfn1->u2.ShareCount += 1;

            if (Event->WaitCount != 1) {
                Event->IoStatus.Status = STATUS_SUCCESS;
                Event->IoStatus.Information = 0;
                KeSetEvent (&Event->Event, 0, FALSE);
            }

            if (DontZero != FALSE) {
                MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 40);
                Pfn1->u3.e2.ReferenceCount += 1;
                status = STATUS_CACHE_PAGE_LOCKED;
            }

            ASSERT (Thread->NestedFaultCount <= 3);
            ASSERT (Thread->NestedFaultCount != 0);
    
            Thread->NestedFaultCount -= 1;

            if ((Thread->ApcNeeded == 1) && (Thread->NestedFaultCount == 0)) {
                ApcNeeded = TRUE;
                Thread->ApcNeeded = 0;
            }
            UNLOCK_PFN (OldIrql);
            MiFreeInPageSupportBlock (Event);

        }
        else {

            //
            // This is either a frame that was originally on the transition list
            // or was already valid when this routine began execution.  Either
            // way, the share count (and therefore the systemwide locked pages
            // count) has been dealt with.
            //

            ASSERT (ShareCountUpped == TRUE);

            if (Pfn1->u1.Event == NULL) {
                Pfn1->u1.Event = (PVOID) Thread;
            }
            UNLOCK_PFN (OldIrql);
        }

        //
        // Log prefetch fault information now that the PFN lock has been
        // released and the PTE has been made valid.  This minimizes PFN
        // lock contention, allows CcPfLogPageFault to allocate (and fault on)
        // pool, and allows other threads in this process to execute without
        // faulting on this address.
        //

        if (Subsection != NULL) {

            FileObject = Subsection->ControlArea->FilePointer;
            FileOffset = MiStartingOffset (Subsection, ProtoPte);

            Flags = 0;

            ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

            if (Subsection->ControlArea->u.Flags.Rom) {
                Flags |= CCPF_TYPE_ROM;
            }

            CcPfLogPageFault (FileObject, FileOffset, Flags);
        }

        LOCK_SYSTEM_WS (OldIrql, Thread);

        WorkingSetIndex = MiLocateAndReserveWsle (&MmSystemCacheWs);

        MiUpdateWsle (&WorkingSetIndex,
                      MiGetVirtualAddressMappedByPte (PointerPte),
                      MmSystemCacheWorkingSetList,
                      Pfn1);

        MmSystemCacheWsle[WorkingSetIndex].u1.e1.SameProtectAsProto = 1;

        MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);
    
        UNLOCK_SYSTEM_WS (OldIrql);

        if (ApcNeeded == TRUE) {
            ASSERT (OldIrql < APC_LEVEL);
            ASSERT (Thread->NestedFaultCount == 0);
            ASSERT (Thread->ApcNeeded == 0);
            KeRaiseIrql (APC_LEVEL, &OldIrql);
            IoRetryIrpCompletions ();
            KeLowerIrql (OldIrql);
        }
    }

    return status;
}

LONG
MiMapCacheExceptionFilter (
    IN PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    )

/*++

Routine Description:

    This routine is a filter for exceptions during copying data
    from the user buffer to the system cache.  It stores the
    status code from the exception record into the status argument.
    In the case of an in page i/o error it returns the actual
    error code and in the case of an access violation it returns
    STATUS_INVALID_USER_BUFFER.

Arguments:

    Status - Returns the status from the exception record.

    ExceptionCode - Supplies the exception code to being checked.

Return Value:

    ULONG - returns EXCEPTION_EXECUTE_HANDLER

--*/

{
    NTSTATUS local;

    local = ExceptionPointer->ExceptionRecord->ExceptionCode;

    //
    // If the exception is STATUS_IN_PAGE_ERROR, get the I/O error code
    // from the exception record.
    //

    if (local == STATUS_IN_PAGE_ERROR) {
        if (ExceptionPointer->ExceptionRecord->NumberParameters >= 3) {
            local = (NTSTATUS)ExceptionPointer->ExceptionRecord->ExceptionInformation[2];
        }
    }

    if (local == STATUS_ACCESS_VIOLATION) {
        local = STATUS_INVALID_USER_BUFFER;
    }

    *Status = local;
    return EXCEPTION_EXECUTE_HANDLER;
}


VOID
MmUnlockCachedPage (
    IN PVOID AddressInCache
    )

/*++

Routine Description:

    This routine unlocks a previous locked cached page.

Arguments:

    AddressInCache - Supplies the address where the page was locked
                     in the system cache.  This must be the same
                     address that MmCopyToCachedPage was called with.

Return Value:

    None.

--*/

{
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    KIRQL OldIrql;

    PointerPte = MiGetPteAddress (AddressInCache);

    ASSERT (PointerPte->u.Hard.Valid == 1);
    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

    LOCK_PFN (OldIrql);

    if (Pfn1->u3.e2.ReferenceCount <= 1) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x777,
                      (ULONG_PTR)PointerPte->u.Hard.PageFrameNumber,
                      Pfn1->u3.e2.ReferenceCount,
                      (ULONG_PTR)AddressInCache);
        return;
    }

    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 25);

    UNLOCK_PFN (OldIrql);
    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\miglobal.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   miglobal.c

Abstract:

    This module contains the private global storage for the memory
    management subsystem.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/
#include "mi.h"

//
// Highest user address;
//

PVOID MmHighestUserAddress;

//
// Start of system address range.
//

PVOID MmSystemRangeStart;

//
// User probe address;
//

ULONG_PTR MmUserProbeAddress;

#if !defined(_WIN64)

//
// Virtual bias applied during the loading of the kernel image.
//

ULONG_PTR MmVirtualBias;

#endif

//
// Number of secondary colors, based on level 2 d cache size.
//

ULONG MmSecondaryColors;

//
// The starting color index seed, incremented at each process creation.
//

ULONG MmProcessColorSeed = 0x12345678;

//
// Total number of physical pages available on the system.
//

PFN_COUNT MmNumberOfPhysicalPages;

//
// Lowest physical page number in the system.
//

PFN_NUMBER MmLowestPhysicalPage = (PFN_NUMBER)-1;

//
// Highest physical page number in the system.
//

PFN_NUMBER MmHighestPhysicalPage;

//
// Highest possible physical page number in the system.
//

PFN_NUMBER MmHighestPossiblePhysicalPage;

//
// Total number of available pages in the system.  This
// is the sum of the pages on the zeroed, free and standby lists.
//

PFN_COUNT  MmAvailablePages;
PFN_NUMBER MmThrottleTop;
PFN_NUMBER MmThrottleBottom;

//
// Highest VAD index used to create bitmaps.
//

ULONG MiLastVadBit = 1;

//
// System wide memory management statistics block.
//

MMINFO_COUNTERS MmInfoCounters;

//
// Total number of physical pages which would be usable if every process
// was at its minimum working set size.  This value is initialized
// at system initialization to MmAvailablePages - MM_FLUID_PHYSICAL_PAGES.
// Every time a thread is created, the kernel stack is subtracted from
// this and every time a process is created, the minimum working set
// is subtracted from this.  If the value would become negative, the
// operation (create process/kernel stack/ adjust working set) fails.
// The PFN LOCK must be owned to manipulate this value.
//

SPFN_NUMBER MmResidentAvailablePages;

//
// The total number of pages which would be removed from working sets
// if every working set was at its minimum.
//

PFN_NUMBER MmPagesAboveWsMinimum;

//
// The total number of pages which would be removed from working sets
// if every working set above its maximum was at its maximum.
//

PFN_NUMBER MmPagesAboveWsMaximum;

//
// The number of pages to add to a working set if there are ample
// available pages and the working set is below its maximum.
//

//
// If memory is becoming short and MmPagesAboveWsMinimum is
// greater than MmPagesAboveWsThreshold, trim working sets.
//

PFN_NUMBER MmPagesAboveWsThreshold = 37;

PFN_NUMBER MmWorkingSetSizeIncrement = 6;

//
// The number of pages to extend the maximum working set size by
// if the working set at its maximum and there are ample available pages.

PFN_NUMBER MmWorkingSetSizeExpansion = 20;

//
// The number of pages required to be freed by working set reduction
// before working set reduction is attempted.
//

PFN_NUMBER MmWsAdjustThreshold = 45;

//
// The number of pages available to allow the working set to be
// expanded above its maximum.
//

PFN_NUMBER MmWsExpandThreshold = 90;

//
// The total number of pages to reduce by working set trimming.
//

PFN_NUMBER MmWsTrimReductionGoal = 29;

//
// The total number of pages needed for the loader to successfully hibernate.
//

PFN_NUMBER MmHiberPages = 768;

//
// The following values are frequently used together.  They tend
// not to be modified once the system has initialized so should
// not be grouped with data whose values change frequently to
// eliminate false sharing.
//

ULONG MmSecondaryColorMask;
UCHAR MmSecondaryColorNodeShift;

//
// Registry-settable threshold for using large pages.  x86 only.
//

ULONG MmLargePageMinimum;

PMMPFN MmPfnDatabase;

MMPFNLIST MmZeroedPageListHead = {
                    0, // Total
                    ZeroedPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmFreePageListHead = {
                    0, // Total
                    FreePageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmStandbyPageListHead = {
                    0, // Total
                    StandbyPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmModifiedPageListHead = {
                    0, // Total
                    ModifiedPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmModifiedNoWritePageListHead = {
                    0, // Total
                    ModifiedNoWritePageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmBadPageListHead = {
                    0, // Total
                    BadPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

//
// Note the ROM page listhead is deliberately not in the set
// of MmPageLocationList ranges.
//

MMPFNLIST MmRomPageListHead = {
                    0, // Total
                    StandbyPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };


PMMPFNLIST MmPageLocationList[NUMBER_OF_PAGE_LISTS] = {
                                      &MmZeroedPageListHead,
                                      &MmFreePageListHead,
                                      &MmStandbyPageListHead,
                                      &MmModifiedPageListHead,
                                      &MmModifiedNoWritePageListHead,
                                      &MmBadPageListHead,
                                      NULL,
                                      NULL };

PMMPTE MiHighestUserPte;
PMMPTE MiHighestUserPde;
#if (_MI_PAGING_LEVELS >= 4)
PMMPTE MiHighestUserPpe;
PMMPTE MiHighestUserPxe;
#endif

PMMPTE MiSessionBasePte;
PMMPTE MiSessionLastPte;

//
// Hyper space items.
//

PMMPTE MmFirstReservedMappingPte;

PMMPTE MmLastReservedMappingPte;

//
// Event for available pages, set means pages are available.
//

KEVENT MmAvailablePagesEvent;

//
// Event for the zeroing page thread.
//

KEVENT MmZeroingPageEvent;

//
// Boolean to indicate if the zeroing page thread is currently
// active.  This is set to true when the zeroing page event is
// set and set to false when the zeroing page thread is done
// zeroing all the pages on the free list.
//

BOOLEAN MmZeroingPageThreadActive;

//
// Minimum number of free pages before zeroing page thread starts.
//

PFN_NUMBER MmMinimumFreePagesToZero = 8;

//
// System space sizes - MmNonPagedSystemStart to MM_NON_PAGED_SYSTEM_END
// defines the ranges of PDEs which must be copied into a new process's
// address space.
//

PVOID MmNonPagedSystemStart;

LOGICAL MmProtectFreedNonPagedPool;

//
// This is set in the registry to the maximum number of gigabytes of RAM
// that can be added to this machine (ie: #of DIMM slots times maximum
// supported DIMM size).  This lets configurations that won't use the absolute
// maximum indicate that a smaller (virtually) PFN database size can be used
// thus leaving more virtual address space for things like system PTEs, etc.
//

PFN_NUMBER MmDynamicPfn;

#ifdef MM_BUMP_COUNTER_MAX
SIZE_T MmResTrack[MM_BUMP_COUNTER_MAX];
#endif

#ifdef MM_COMMIT_COUNTER_MAX
SIZE_T MmTrackCommit[MM_COMMIT_COUNTER_MAX];
#endif

//
// Set via the registry to identify which drivers are leaking locked pages.
//

LOGICAL  MmTrackLockedPages;

KSPIN_LOCK MiTrackLockedPagesLock;

//
// Set via the registry to identify drivers which unload without releasing
// resources or still have active timers, etc.
//

LOGICAL MmSnapUnloads = TRUE;

#if DBG
PETHREAD MiExpansionLockOwner;
#endif

//
// Pool sizes.
//

SIZE_T MmSizeOfNonPagedPoolInBytes;

SIZE_T MmMaximumNonPagedPoolInBytes;

ULONG MmMaximumNonPagedPoolPercent;

SIZE_T MmMinimumNonPagedPoolSize = 256 * 1024; // 256k

ULONG MmMinAdditionNonPagedPoolPerMb = 32 * 1024; // 32k

SIZE_T MmDefaultMaximumNonPagedPool = 1024 * 1024;  // 1mb

ULONG MmMaxAdditionNonPagedPoolPerMb = 400 * 1024;  //400k

SIZE_T MmSizeOfPagedPoolInBytes = 32 * 1024 * 1024; // 32 MB.

ULONG MmNumberOfSystemPtes;

ULONG MiRequestedSystemPtes;

PMMPTE MmFirstPteForPagedPool;

PMMPTE MmLastPteForPagedPool;

//
// Pool bit maps and other related structures.
//

PVOID MmPageAlignedPoolBase[2];

ULONG MmExpandedPoolBitPosition;

PFN_NUMBER MmNumberOfFreeNonPagedPool;

//
// MmFirstFreeSystemPte contains the offset from the
// Nonpaged system base to the first free system PTE.
// Note that an offset of -1 indicates an empty list.
//

MMPTE MmFirstFreeSystemPte[MaximumPtePoolTypes];

//
// System cache sizes.
//

PMMWSL MmSystemCacheWorkingSetList = (PMMWSL)MM_SYSTEM_CACHE_WORKING_SET;

MMSUPPORT MmSystemCacheWs;

PMMWSLE MmSystemCacheWsle;

PVOID MmSystemCacheStart = (PVOID)MM_SYSTEM_CACHE_START;

PVOID MmSystemCacheEnd;

PRTL_BITMAP MmSystemCacheAllocationMap;

PRTL_BITMAP MmSystemCacheEndingMap;

//
// This value should not be greater than 256MB in a system with 1GB of
// system space.
//

ULONG_PTR MmSizeOfSystemCacheInPages = 64 * 256; //64MB.

//
// Default sizes for the system cache.
//

PFN_NUMBER MmSystemCacheWsMinimum = 288;

PFN_NUMBER MmSystemCacheWsMaximum = 350;

//
// Cells to track unused thread kernel stacks to avoid TB flushes
// every time a thread terminates.
//

ULONG MmMaximumDeadKernelStacks = 5;
SLIST_HEADER MmDeadStackSListHead;

//
// Cells to track control area synchronization.
//

SLIST_HEADER MmEventCountSListHead;

SLIST_HEADER MmInPageSupportSListHead;

//
// MmSystemPteBase contains the address of 1 PTE before
// the first free system PTE (zero indicates an empty list).
// The value of this field does not change once set.
//

PMMPTE MmSystemPteBase;

PMMADDRESS_NODE MmSectionBasedRoot;

PVOID MmHighSectionBase;

//
// Section object type.
//

POBJECT_TYPE MmSectionObjectType;

//
// Section commit mutex.
//

FAST_MUTEX MmSectionCommitMutex;

//
// Section base address mutex.
//

FAST_MUTEX MmSectionBasedMutex;

//
// Resource for section extension.
//

ERESOURCE MmSectionExtendResource;
ERESOURCE MmSectionExtendSetResource;

//
// Pagefile creation lock.
//

FAST_MUTEX MmPageFileCreationLock;

MMDEREFERENCE_SEGMENT_HEADER MmDereferenceSegmentHeader;

LIST_ENTRY MmUnusedSegmentList;

LIST_ENTRY MmUnusedSubsectionList;

KEVENT MmUnusedSegmentCleanup;

ULONG MmUnusedSegmentCount;

ULONG MmUnusedSubsectionCount;

ULONG MmUnusedSubsectionCountPeak;

SIZE_T MiUnusedSubsectionPagedPool;

SIZE_T MiUnusedSubsectionPagedPoolPeak;

//
// If more than this percentage of pool is consumed and pool allocations
// might fail, then trim unused segments & subsections to get back to
// this percentage.
//

ULONG MmConsumedPoolPercentage;

MMWORKING_SET_EXPANSION_HEAD MmWorkingSetExpansionHead;

MMPAGE_FILE_EXPANSION MmAttemptForCantExtend;

//
// Paging files
//

MMMOD_WRITER_LISTHEAD MmPagingFileHeader;

MMMOD_WRITER_LISTHEAD MmMappedFileHeader;

LIST_ENTRY MmFreePagingSpaceLow;

ULONG MmNumberOfActiveMdlEntries;

PMMPAGING_FILE MmPagingFile[MAX_PAGE_FILES];

ULONG MmNumberOfPagingFiles;

KEVENT MmModifiedPageWriterEvent;

KEVENT MmWorkingSetManagerEvent;

KEVENT MmCollidedFlushEvent;

//
// Total number of committed pages.
//

SIZE_T MmTotalCommittedPages;

//
// Limit on committed pages.  When MmTotalCommittedPages would become
// greater than or equal to this number the paging files must be expanded.
//

SIZE_T MmTotalCommitLimit;

SIZE_T MmTotalCommitLimitMaximum;

//
// Number of pages to overcommit without expanding the paging file.
// MmTotalCommitLimit = (total paging file space) + MmOverCommit.
//

SIZE_T MmOverCommit;

//
// Modified page writer.
//


//
// Minimum number of free pages before working set trimming and
// aggressive modified page writing is started.
//

PFN_NUMBER MmMinimumFreePages = 26;

//
// Stop writing modified pages when MmFreeGoal pages exist.
//

PFN_NUMBER MmFreeGoal = 100;

//
// Start writing pages if more than this number of pages
// is on the modified page list.
//

PFN_NUMBER MmModifiedPageMaximum;

//
// Minimum number of modified pages required before the modified
// page writer is started.
//

PFN_NUMBER MmModifiedPageMinimum;

//
// Amount of disk space that must be free after the paging file is
// extended.
//

ULONG MmMinimumFreeDiskSpace = 1024 * 1024;

//
// Minimum size in pages to extend the paging file by.
//

ULONG MmPageFileExtension = 256;

//
// Size to reduce the paging file by.
//

ULONG MmMinimumPageFileReduction = 256;  //256 pages (1mb)

//
// Number of pages to write in a single I/O.
//

ULONG MmModifiedWriteClusterSize = MM_MAXIMUM_WRITE_CLUSTER;

//
// Number of pages to read in a single I/O if possible.
//

ULONG MmReadClusterSize = 7;

const ULONG MMSECT = 'tSmM';              // This is exported to special pool.

//
// This resource guards the working set list for the system shared
// address space (paged pool, system cache, pagable drivers).
//

ERESOURCE MmSystemWsLock;

PETHREAD MmSystemLockOwner;

//
// Spin lock for allowing working set expansion.
//

KSPIN_LOCK MmExpansionLock;

//
// System process working set sizes.
//

PFN_NUMBER MmSystemProcessWorkingSetMin = 50;

PFN_NUMBER MmSystemProcessWorkingSetMax = 450;

WSLE_NUMBER MmMaximumWorkingSetSize;

PFN_NUMBER MmMinimumWorkingSetSize = 20;


//
// Page color for system working set.
//

ULONG MmSystemPageColor;

//
// Time constants
//

const LARGE_INTEGER MmSevenMinutes = {0, -1};

//
// Note that the following constant is initialized to five seconds,
// but is set to 3 on very small workstations.
//

LARGE_INTEGER MmWorkingSetProtectionTime = {5 * 1000 * 1000 * 10, 0};

const LARGE_INTEGER MmOneSecond = {(ULONG)(-1 * 1000 * 1000 * 10), -1};
const LARGE_INTEGER MmTwentySeconds = {(ULONG)(-20 * 1000 * 1000 * 10), -1};
const LARGE_INTEGER MmShortTime = {(ULONG)(-10 * 1000 * 10), -1}; // 10 milliseconds
const LARGE_INTEGER MmHalfSecond = {(ULONG)(-5 * 100 * 1000 * 10), -1};
const LARGE_INTEGER Mm30Milliseconds = {(ULONG)(-30 * 1000 * 10), -1};

//
// Parameters for user mode passed up via PEB in MmCreatePeb.
//

LARGE_INTEGER MmCriticalSectionTimeout;     // Filled in by mminit.c
SIZE_T MmHeapSegmentReserve = 1024 * 1024;
SIZE_T MmHeapSegmentCommit = PAGE_SIZE * 2;
SIZE_T MmHeapDeCommitTotalFreeThreshold = 64 * 1024;
SIZE_T MmHeapDeCommitFreeBlockThreshold = PAGE_SIZE;

//
// Set from ntos\config\CMDAT3.C  Used by customers to disable paging
// of executive on machines with lots of memory.  Worth a few TPS on a
// database server.
//

ULONG MmDisablePagingExecutive;

BOOLEAN Mm64BitPhysicalAddress;

#if DBG
ULONG MmDebug;
#endif

//
// Map a page protection from the Pte.Protect field into a protection mask.
//

ULONG MmProtectToValue[32] = {
                            PAGE_NOACCESS,
                            PAGE_READONLY,
                            PAGE_EXECUTE,
                            PAGE_EXECUTE_READ,
                            PAGE_READWRITE,
                            PAGE_WRITECOPY,
                            PAGE_EXECUTE_READWRITE,
                            PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_NOCACHE | PAGE_READONLY,
                            PAGE_NOCACHE | PAGE_EXECUTE,
                            PAGE_NOCACHE | PAGE_EXECUTE_READ,
                            PAGE_NOCACHE | PAGE_READWRITE,
                            PAGE_NOCACHE | PAGE_WRITECOPY,
                            PAGE_NOCACHE | PAGE_EXECUTE_READWRITE,
                            PAGE_NOCACHE | PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_GUARD | PAGE_READONLY,
                            PAGE_GUARD | PAGE_EXECUTE,
                            PAGE_GUARD | PAGE_EXECUTE_READ,
                            PAGE_GUARD | PAGE_READWRITE,
                            PAGE_GUARD | PAGE_WRITECOPY,
                            PAGE_GUARD | PAGE_EXECUTE_READWRITE,
                            PAGE_GUARD | PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_READONLY,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_READ,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_READWRITE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_WRITECOPY,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_READWRITE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_WRITECOPY
                          };

#if (defined(_WIN64) || defined(_X86PAE_))
ULONGLONG
#else
ULONG
#endif
MmProtectToPteMask[32] = {
                       MM_PTE_NOACCESS,
                       MM_PTE_READONLY | MM_PTE_CACHE,
                       MM_PTE_EXECUTE | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_READ | MM_PTE_CACHE,
                       MM_PTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_NOACCESS,
                       MM_PTE_NOCACHE | MM_PTE_READONLY,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_READ,
                       MM_PTE_NOCACHE | MM_PTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_WRITECOPY,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_WRITECOPY,
                       MM_PTE_NOACCESS,
                       MM_PTE_GUARD | MM_PTE_READONLY | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_READ | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_NOACCESS,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_READONLY,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_READ,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_WRITECOPY,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_WRITECOPY
                    };

//
// Conversion which takes a Pte.Protect and builds a new Pte.Protect which
// is not copy-on-write.
//

ULONG MmMakeProtectNotWriteCopy[32] = {
                       MM_NOACCESS,
                       MM_READONLY,
                       MM_EXECUTE,
                       MM_EXECUTE_READ,
                       MM_READWRITE,
                       MM_READWRITE,        //not copy
                       MM_EXECUTE_READWRITE,
                       MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_NOCACHE | MM_READONLY,
                       MM_NOCACHE | MM_EXECUTE,
                       MM_NOCACHE | MM_EXECUTE_READ,
                       MM_NOCACHE | MM_READWRITE,
                       MM_NOCACHE | MM_READWRITE,
                       MM_NOCACHE | MM_EXECUTE_READWRITE,
                       MM_NOCACHE | MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_GUARD_PAGE | MM_READONLY,
                       MM_GUARD_PAGE | MM_EXECUTE,
                       MM_GUARD_PAGE | MM_EXECUTE_READ,
                       MM_GUARD_PAGE | MM_READWRITE,
                       MM_GUARD_PAGE | MM_READWRITE,
                       MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READONLY,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READ,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READWRITE
                       };

//
// Converts a protection code to an access right for section access.
// This uses only the lower 3 bits of the 5 bit protection code.
//

ACCESS_MASK MmMakeSectionAccess[8] = { SECTION_MAP_READ,
                                       SECTION_MAP_READ,
                                       SECTION_MAP_EXECUTE,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_READ,
                                       SECTION_MAP_WRITE,
                                       SECTION_MAP_READ,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_WRITE,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_READ };

//
// Converts a protection code to an access right for file access.
// This uses only the lower 3 bits of the 5 bit protection code.
//

ACCESS_MASK MmMakeFileAccess[8] = { FILE_READ_DATA,
                                FILE_READ_DATA,
                                FILE_EXECUTE,
                                FILE_EXECUTE | FILE_READ_DATA,
                                FILE_WRITE_DATA | FILE_READ_DATA,
                                FILE_READ_DATA,
                                FILE_EXECUTE | FILE_WRITE_DATA | FILE_READ_DATA,
                                FILE_EXECUTE | FILE_READ_DATA };

MM_PAGED_POOL_INFO MmPagedPoolInfo;

//
// Some Hydra variables.
//

ULONG_PTR MmSessionBase;

PMM_SESSION_SPACE MmSessionSpace;

ULONG_PTR MiSessionSpaceWs;

SIZE_T MmSessionSize;

LIST_ENTRY MiSessionWsList;

ULONG_PTR MiSystemViewStart;

SIZE_T MmSystemViewSize;

ULONG_PTR MiSessionPoolStart;

ULONG_PTR MiSessionPoolEnd;

ULONG_PTR MiSessionSpaceEnd;

ULONG_PTR MiSessionViewStart;

ULONG MiSessionSpacePageTables;

SIZE_T MmSessionViewSize;

SIZE_T MmSessionPoolSize;

ULONG_PTR MiSessionImageStart;

ULONG_PTR MiSessionImageEnd;

PMMPTE MiSessionImagePteStart;

PMMPTE MiSessionImagePteEnd;

SIZE_T MmSessionImageSize;

//
// Cache control stuff.  Note this may be overridden by deficient hardware
// platforms at startup.
//

MI_PFN_CACHE_ATTRIBUTE MiPlatformCacheAttributes[2 * MmMaximumCacheType] =
{
    //
    // Memory space
    //

    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiCached,
    MiNonCached,
    MiWriteCombined,

    //
    // I/O space
    //

    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiCached,
    MiNonCached,
    MiWriteCombined
};

//
// Note the Driver Verifier can reinitialize the mask values.
//

ULONG MiIoRetryMask = 0x1f;
ULONG MiFaultRetryMask = 0x1f;
ULONG MiUserFaultRetryMask = 0xF;

#if defined (_MI_INSTRUMENT_PFN)

//
// Instrumentation code to track PFN lock duration.
//

ULONG MiPfnTimings;
ULONG_PTR MiPfnAcquiredAddress;
LARGE_INTEGER MiPfnAcquired;
LARGE_INTEGER MiPfnThreshold;

MMPFNTIMINGS MiPfnSorted[MI_MAX_PFN_CALLERS];
ULONG MiMaxPfnTimings = MI_MAX_PFN_CALLERS;

ULONG_PTR
MiGetExecutionAddress(
    VOID
    )
{
#if defined(_X86_)
    _asm {
        push    dword ptr [esp]
        pop     eax
    }
#else
    PVOID CallingAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);
    return (ULONG_PTR) CallingAddress;
#endif
}
#endif

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("INIT")
#endif

WCHAR MmVerifyDriverBuffer[MI_SUSPECT_DRIVER_BUFFER_LENGTH] = {0};
ULONG MmVerifyDriverBufferType = REG_NONE;
ULONG MmVerifyDriverLevel = (ULONG)-1;
ULONG MmCritsectTimeoutSeconds = 2592000;

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

ULONG MmVerifyDriverBufferLength = sizeof(MmVerifyDriverBuffer);
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mmfault.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mmfault.c

Abstract:

    This module contains the handlers for access check, page faults
    and write faults.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#define PROCESS_FOREGROUND_PRIORITY (9)

LONG MiDelayPageFaults;

#if DBG
ULONG MmProtoPteVadLookups = 0;
ULONG MmProtoPteDirect = 0;
ULONG MmAutoEvaluate = 0;

PMMPTE MmPteHit = NULL;
extern PVOID PsNtosImageEnd;
ULONG MmInjectUserInpageErrors;
ULONG MmInjectedUserInpageErrors;
ULONG MmInpageFraction = 0x1F;      // Fail 1 out of every 32 inpages.

#define MI_INPAGE_BACKTRACE_LENGTH 6

typedef struct _MI_INPAGE_TRACES {

    PVOID InstructionPointer;
    PETHREAD Thread;
    PVOID StackTrace [MI_INPAGE_BACKTRACE_LENGTH];

} MI_INPAGE_TRACES, *PMI_INPAGE_TRACES;

#define MI_INPAGE_TRACE_SIZE 64

LONG MiInpageIndex;

MI_INPAGE_TRACES MiInpageTraces[MI_INPAGE_TRACE_SIZE];

VOID
FORCEINLINE
MiSnapInPageError (
    IN PVOID InstructionPointer
    )
{
    PMI_INPAGE_TRACES Information;
    ULONG Index;
    ULONG Hash;

    Index = InterlockedIncrement(&MiInpageIndex);
    Index &= (MI_INPAGE_TRACE_SIZE - 1);
    Information = &MiInpageTraces[Index];

    Information->InstructionPointer = InstructionPointer;
    Information->Thread = PsGetCurrentThread ();

    RtlZeroMemory (&Information->StackTrace[0], MI_INPAGE_BACKTRACE_LENGTH * sizeof(PVOID)); 

    RtlCaptureStackBackTrace (0, MI_INPAGE_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#endif


NTSTATUS
MmAccessFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAddress,
    IN KPROCESSOR_MODE PreviousMode,
    IN PVOID TrapInformation
    )

/*++

Routine Description:

    This function is called by the kernel on data or instruction
    access faults.  The access fault was detected due to either
    an access violation, a PTE with the present bit clear, or a
    valid PTE with the dirty bit clear and a write operation.

    Also note that the access violation and the page fault could
    occur because of the Page Directory Entry contents as well.

    This routine determines what type of fault it is and calls
    the appropriate routine to handle the page fault or the write
    fault.

Arguments:

    FaultStatus - Supplies fault status information bits.

    VirtualAddress - Supplies the virtual address which caused the fault.

    PreviousMode - Supplies the mode (kernel or user) in which the fault
                   occurred.

    TrapInformation - Opaque information about the trap, interpreted by the
                      kernel, not Mm.  Needed to allow fast interlocked access
                      to operate correctly.

Return Value:

    Returns the status of the fault handling operation.  Can be one of:
        - Success.
        - Access Violation.
        - Guard Page Violation.
        - In-page Error.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    ULONG ProtoProtect;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE PointerProtoPte;
    ULONG ProtectionCode;
    MMPTE TempPte;
    PEPROCESS CurrentProcess;
    KIRQL PreviousIrql;
    NTSTATUS status;
    ULONG ProtectCode;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PPAGE_FAULT_NOTIFY_ROUTINE NotifyRoutine;
    PEPROCESS FaultProcess;
    PMMSUPPORT Ws;
    LOGICAL SessionAddress;
    PVOID UsedPageTableHandle;
    ULONG BarrierStamp;
    LOGICAL ApcNeeded;
    LOGICAL RecheckAccess;
#if (_MI_PAGING_LEVELS < 3)
    NTSTATUS SessionStatus;
#endif

    PointerProtoPte = NULL;

    //
    // If the address is not canonical then return FALSE as the caller (which
    // may be the kernel debugger) is not expecting to get an unimplemented
    // address bit fault.
    //

    if (MI_RESERVED_BITS_CANONICAL(VirtualAddress) == FALSE) {

        if (PreviousMode == UserMode) {
            return STATUS_ACCESS_VIOLATION;
        }

        if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
            return STATUS_ACCESS_VIOLATION;
        }

        KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                      (ULONG_PTR)VirtualAddress,
                      FaultStatus,
                      (ULONG_PTR)TrapInformation,
                      4);
    }

    //
    // Block APCs and acquire the working set mutex.  This prevents any
    // changes to the address space and it prevents valid PTEs from becoming
    // invalid.
    //

    CurrentProcess = PsGetCurrentProcess ();

#if DBG
    if (MmDebug & MM_DBG_SHOW_FAULTS) {

        PETHREAD CurThread;

        CurThread = PsGetCurrentThread();
        DbgPrint("MM:**access fault - va %p process %p thread %p\n",
                 VirtualAddress, CurrentProcess, CurThread);
    }
#endif //DBG

    PreviousIrql = KeGetCurrentIrql ();

    //
    // Get the pointer to the PDE and the PTE for this page.
    //

    PointerPte = MiGetPteAddress (VirtualAddress);
    PointerPde = MiGetPdeAddress (VirtualAddress);
    PointerPpe = MiGetPpeAddress (VirtualAddress);
    PointerPxe = MiGetPxeAddress (VirtualAddress);

#if DBG
    if (PointerPte == MmPteHit) {
        DbgPrint("MM: PTE hit at %p\n", MmPteHit);
        DbgBreakPoint();
    }
#endif

    ApcNeeded = FALSE;

    if (PreviousIrql > APC_LEVEL) {

        //
        // The PFN database lock is an executive spin-lock.  The pager could
        // get dirty faults or lock faults while servicing and it already owns
        // the PFN database lock.
        //

#if (_MI_PAGING_LEVELS < 3)
        MiCheckPdeForPagedPool (VirtualAddress);

        if (PointerPde->u.Hard.Valid == 1) {
            if (PointerPde->u.Hard.LargePage == 1) {
                return STATUS_SUCCESS;
            }
        }
#endif

        if (
#if (_MI_PAGING_LEVELS >= 4)
            (PointerPxe->u.Hard.Valid == 0) ||
#endif
#if (_MI_PAGING_LEVELS >= 3)
            (PointerPpe->u.Hard.Valid == 0) ||
#endif
            (PointerPde->u.Hard.Valid == 0) ||
            (PointerPte->u.Hard.Valid == 0)) {

            KdPrint(("MM:***PAGE FAULT AT IRQL > 1  Va %p, IRQL %lx\n",
                     VirtualAddress,
                     PreviousIrql));

            if (TrapInformation != NULL) {
                MI_DISPLAY_TRAP_INFORMATION (TrapInformation);
            }

            //
            // use reserved bit to signal fatal error to trap handlers
            //

            return STATUS_IN_PAGE_ERROR | 0x10000000;

        }

        if ((MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
            (PointerPte->u.Hard.CopyOnWrite != 0)) {

            KdPrint(("MM:***PAGE FAULT AT IRQL > 1  Va %p, IRQL %lx\n",
                     VirtualAddress,
                     PreviousIrql));

            if (TrapInformation != NULL) {
                MI_DISPLAY_TRAP_INFORMATION (TrapInformation);
            }

            //
            // use reserved bit to signal fatal error to trap handlers
            //

            return STATUS_IN_PAGE_ERROR | 0x10000000;
        }

        //
        // The PTE is valid and accessible, another thread must
        // have faulted the PTE in already, or the access bit
        // is clear and this is a access fault; Blindly set the
        // access bit and dismiss the fault.
        //
#if DBG
        if (MmDebug & MM_DBG_SHOW_FAULTS) {
            DbgPrint("MM:no fault found - PTE is %p\n", PointerPte->u.Long);
        }
#endif

        //
        // If PTE mappings with various protections are active and the faulting
        // address lies within these mappings, resolve the fault with
        // the appropriate protections.
        //

        if (!IsListEmpty (&MmProtectedPteList)) {

            if (MiCheckSystemPteProtection (
                                MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus),
                                VirtualAddress) == TRUE) {

                return STATUS_SUCCESS;
            }
        }

        if (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) {

            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

            if (((PointerPte->u.Long & MM_PTE_WRITE_MASK) == 0) &&
                ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0)) {
                
                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                              (ULONG_PTR)VirtualAddress,
                              (ULONG_PTR)PointerPte->u.Long,
                              (ULONG_PTR)TrapInformation,
                              10);
            }
        }

        MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, FALSE);
        return STATUS_SUCCESS;
    }

    if (VirtualAddress >= MmSystemRangeStart) {

        //
        // This is a fault in the system address space.  User
        // mode access is not allowed.
        //

        if (PreviousMode == UserMode) {
            return STATUS_ACCESS_VIOLATION;
        }

#if (_MI_PAGING_LEVELS >= 4)
        if (PointerPxe->u.Hard.Valid == 0) {

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          7);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        if (PointerPpe->u.Hard.Valid == 0) {

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          5);
        }
#endif

RecheckPde:

        if (PointerPde->u.Hard.Valid == 1) {
#ifdef _X86_
            if (PointerPde->u.Hard.LargePage == 1) {
                return STATUS_SUCCESS;
            }
#endif //X86

            if (PointerPte->u.Hard.Valid == 1) {

                //
                // Session space faults cannot early exit here because
                // it may be a copy on write which must be checked for
                // and handled below.
                //

                if (MI_IS_SESSION_ADDRESS (VirtualAddress) == FALSE) {

                    //
                    // If PTE mappings with various protections are active
                    // and the faulting address lies within these mappings,
                    // resolve the fault with the appropriate protections.
                    //

                    if (!IsListEmpty (&MmProtectedPteList)) {

                        if (MiCheckSystemPteProtection (
                                MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus),
                                VirtualAddress) == TRUE) {
                            return STATUS_SUCCESS;
                        }
                    }

                    //
                    // Acquire the PFN lock, check to see if the address is
                    // still valid if writable, update dirty bit.
                    //

                    LOCK_PFN (OldIrql);
                    TempPte = *(volatile MMPTE *)PointerPte;
                    if (TempPte.u.Hard.Valid == 1) {

                        Pfn1 = MI_PFN_ELEMENT (TempPte.u.Hard.PageFrameNumber);
    
                        if ((MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
                            ((TempPte.u.Long & MM_PTE_WRITE_MASK) == 0) &&
                            ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0)) {
                
                            KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                          (ULONG_PTR)VirtualAddress,
                                          (ULONG_PTR)TempPte.u.Long,
                                          (ULONG_PTR)TrapInformation,
                                          11);
                        }
                        MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
                    }
                    UNLOCK_PFN (OldIrql);
                    return STATUS_SUCCESS;
                }
            }
#if (_MI_PAGING_LEVELS < 3)
            else {

                //
                // Handle trimmer references to paged pool PTEs where the PDE
                // might not be present.  Only needed for
                // MmTrimAllSystemPagable memory.
                //

                MiCheckPdeForPagedPool (VirtualAddress);
                TempPte = *(volatile MMPTE *)PointerPte;
                if (TempPte.u.Hard.Valid == 1) {
                    return STATUS_SUCCESS;
                }
            }
#endif
        } else {

            //
            // Due to G-bits in kernel mode code, accesses to paged pool
            // PDEs may not fault even though the PDE is not valid.  Make
            // sure the PDE is valid so PteFrames in the PFN database are
            // tracked properly.
            //

#if (_MI_PAGING_LEVELS >= 3)
            if ((VirtualAddress >= (PVOID)PTE_BASE) && (VirtualAddress < (PVOID)MiGetPteAddress (HYPER_SPACE))) {
                //
                // This is a user mode PDE entry being faulted in by the Mm
                // referencing the page table page.  This needs to be done
                // with the working set lock so that the PPE validity can be
                // relied on throughout the fault processing.
                //
                // The case when Mm faults in PPE entries by referencing the
                // page directory page is correctly handled by falling through
                // the below code.
                //
    
                goto UserFault;
            }

#if defined (_MIALT4K_)
            if ((VirtualAddress >= (PVOID)ALT4KB_PERMISSION_TABLE_START) && 
                (VirtualAddress < (PVOID)ALT4KB_PERMISSION_TABLE_END)) {
    
                goto UserFault;
            }
#endif

#else
            MiCheckPdeForPagedPool (VirtualAddress);
#endif

            if (PointerPde->u.Hard.Valid == 0) {
                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    return STATUS_ACCESS_VIOLATION;
                }
                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              2);
            }

            //
            // Now that the PDE is valid, go look at the PTE again.
            //

            goto RecheckPde;
        }

#if (_MI_PAGING_LEVELS < 3)

        //
        // First check to see if it's in the session space data
        // structures or page table pages.
        //

        SessionStatus = MiCheckPdeForSessionSpace (VirtualAddress);

        if (SessionStatus == STATUS_ACCESS_VIOLATION) {

            //
            // This thread faulted on a session space access, but this
            // process does not have one.  This could be the system
            // process attempting to access a working buffer passed
            // to it from WIN32K or a driver loaded in session space
            // (video, printer, etc).
            //
            // The system process which contains the worker threads
            // NEVER has a session space - if code accidentally queues a
            // worker thread that points to a session space buffer, a
            // fault will occur.  This must be bug checked since drivers
            // are responsible for making sure this never occurs.
            //
            // The only exception to this is when the working set manager
            // attaches to a session to age or trim it.  However, the
            // working set manager will never fault and so the bugcheck
            // below is always valid.  Note that a worker thread can get
            // away with a bad access if it happens while the working set
            // manager is attached, but there's really no way to prevent
            // this case which is a driver bug anyway.
            //

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          6);
        }

#endif

        //
        // Fall though to further fault handling.
        //

        SessionAddress = MI_IS_SESSION_ADDRESS (VirtualAddress);

        if (SessionAddress == TRUE ||
            ((!MI_IS_PAGE_TABLE_ADDRESS(VirtualAddress)) &&
             (!MI_IS_HYPER_SPACE_ADDRESS(VirtualAddress)))) {

            if (SessionAddress == FALSE) {

                //
                // Acquire system working set lock.  While this lock
                // is held, no pages may go from valid to invalid.
                //
                // HOWEVER - transition pages may go to valid, but
                // may not be added to the working set list.  This
                // is done in the cache manager support routines to
                // shortcut faults on transition prototype PTEs.
                //

                PETHREAD Thread;
                
                Thread = PsGetCurrentThread();

                if (Thread == MmSystemLockOwner) {

                    if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                        return STATUS_ACCESS_VIOLATION;
                    }

                    //
                    // Recursively trying to acquire the system working set
                    // fast mutex - cause an IRQL > 1 bug check.
                    //

                    return STATUS_IN_PAGE_ERROR | 0x10000000;
                }

                LOCK_SYSTEM_WS (PreviousIrql, Thread);
            }

            //
            // Note that for session space the below check is done without
            // acquiring the session WSL lock.  This is because this thread
            // may already own it - ie: it may be adding a page to the
            // session space working set and the session's working set list is
            // not mapped in and causes a fault.  The MiCheckPdeForSessionSpace
            // call above will fill in the PDE and then we must check the PTE
            // below - if that's not present then we couldn't possibly be
            // holding the session WSL lock, so we'll acquire it below.
            //

#if defined (_X86PAE_)
            //
            // PAE PTEs are subject to write tearing due to the cache manager
            // shortcut routines that insert PTEs without acquiring the working
            // set lock.  Synchronize here via the PFN lock.
            //
            LOCK_PFN (OldIrql);
#endif
            TempPte = *PointerPte;
#if defined (_X86PAE_)
            UNLOCK_PFN (OldIrql);
#endif

            //
            // If the PTE is valid, make sure we do not have a copy on write.
            //

            if (TempPte.u.Hard.Valid != 0) {

                //
                // PTE is already valid, return.  Unless it's Hydra where
                // kernel mode copy-on-write must be handled properly.
                //

                LOGICAL FaultHandled;

                //
                // If PTE mappings with various protections are active
                // and the faulting address lies within these mappings,
                // resolve the fault with the appropriate protections.
                //

                if (!IsListEmpty (&MmProtectedPteList)) {

                    if (MiCheckSystemPteProtection (
                            MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus),
                            VirtualAddress) == TRUE) {
                        return STATUS_SUCCESS;
                    }
                }

                FaultHandled = FALSE;

                LOCK_PFN (OldIrql);
                TempPte = *(volatile MMPTE *)PointerPte;
                if (TempPte.u.Hard.Valid == 1) {

                    Pfn1 = MI_PFN_ELEMENT (TempPte.u.Hard.PageFrameNumber);

                    if ((MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
                        (TempPte.u.Hard.CopyOnWrite == 0) &&
                        ((TempPte.u.Long & MM_PTE_WRITE_MASK) == 0) &&
                        ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0)) {
                
                            KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                          (ULONG_PTR)VirtualAddress,
                                          (ULONG_PTR)TempPte.u.Long,
                                          (ULONG_PTR)TrapInformation,
                                          12);
                    }

                    //
                    // Set the dirty bit in the PTE and the page frame.
                    //

                    if (SessionAddress == FALSE || TempPte.u.Hard.Write == 1) {
                        FaultHandled = TRUE;
                        MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
                    }
                }
                UNLOCK_PFN (OldIrql);
                if (SessionAddress == FALSE) {
                    UNLOCK_SYSTEM_WS (PreviousIrql);
                }
                if (SessionAddress == FALSE || FaultHandled == TRUE) {
                    return STATUS_SUCCESS;
                }
            }

            if (SessionAddress == TRUE) {

                //
                // Acquire the session space working set lock.  While this lock
                // is held, no session pages may go from valid to invalid.
                //

                PETHREAD CurrentThread;

                CurrentThread = PsGetCurrentThread ();

                if (CurrentThread == MmSessionSpace->WorkingSetLockOwner) {

                    //
                    // Recursively trying to acquire the session working set
                    // lock - cause an IRQL > 1 bug check.
                    //

                    return STATUS_IN_PAGE_ERROR | 0x10000000;
                }

                LOCK_SESSION_SPACE_WS (PreviousIrql, CurrentThread);

                TempPte = *PointerPte;

                //
                // The PTE could have become valid while we waited
                // for the session space working set lock.
                //

                if (TempPte.u.Hard.Valid == 1) {

                    LOCK_PFN (OldIrql);
                    TempPte = *(volatile MMPTE *)PointerPte;

                    //
                    // Check for copy-on-write.
                    //

                    if (TempPte.u.Hard.Valid == 1) {

                        if ((MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
                            (TempPte.u.Hard.Write == 0)) {

                            //
                            // Copy on write only for loaded drivers...
                            //

                            ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress));

                            UNLOCK_PFN (OldIrql);

                            if (TempPte.u.Hard.CopyOnWrite == 0) {
                    
                                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                              (ULONG_PTR)VirtualAddress,
                                              (ULONG_PTR)TempPte.u.Long,
                                              (ULONG_PTR)TrapInformation,
                                              13);
                            }

                            MiSessionCopyOnWrite (VirtualAddress,
                                                  PointerPte);

                            UNLOCK_SESSION_SPACE_WS (PreviousIrql);

                            return STATUS_SUCCESS;
                        }

#if DBG
                        //
                        // If we are allowing a store, it better be writable.
                        //

                        if (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) {
                            ASSERT (TempPte.u.Hard.Write == 1);
                        }
#endif
                        //
                        // PTE is already valid, return.
                        //

                        MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
                    }

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SESSION_SPACE_WS (PreviousIrql);
                    return STATUS_SUCCESS;
                }
            }

            if (TempPte.u.Soft.Prototype != 0) {

                if (MmProtectFreedNonPagedPool == TRUE) {

                    if (((VirtualAddress >= MmNonPagedPoolStart) &&
                        (VirtualAddress < (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes))) ||
                        ((VirtualAddress >= MmNonPagedPoolExpansionStart) &&
                        (VirtualAddress < MmNonPagedPoolEnd))) {
    
                        //
                        // This is an access to previously freed
                        // non paged pool - bugcheck!
                        //
    
                        if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                            goto AccessViolation;
                        }
    
                        KeBugCheckEx (DRIVER_CAUGHT_MODIFYING_FREED_POOL,
                                      (ULONG_PTR)VirtualAddress,
                                      FaultStatus,
                                      PreviousMode,
                                      4);
                    }
                }

                //
                // This is a PTE in prototype format, locate the corresponding
                // prototype PTE.
                //

                PointerProtoPte = MiPteToProto (&TempPte);

                if (SessionAddress == TRUE) {

                    if (TempPte.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {
                        PointerProtoPte = MiCheckVirtualAddress (VirtualAddress,
                                                                 &ProtectionCode);
                        if (PointerProtoPte == NULL) {
                            UNLOCK_SESSION_SPACE_WS (PreviousIrql);
                            return STATUS_IN_PAGE_ERROR | 0x10000000;
                        }
                    }
                    else if (TempPte.u.Proto.ReadOnly == 1) {
        
                        //
                        // Writes are not allowed to this page.
                        //

                    } else if (MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress)) {

                        //
                        // Copy on write this page.
                        //
    
                        MI_WRITE_INVALID_PTE (PointerPte, PrototypePte);
                        PointerPte->u.Soft.Protection = MM_EXECUTE_WRITECOPY;
                    }
                }
            } else if ((TempPte.u.Soft.Transition == 0) &&
                        (TempPte.u.Soft.Protection == 0)) {

                //
                // Page file format.  If the protection is ZERO, this
                // is a page of free system PTEs - bugcheck!
                //

                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    goto AccessViolation;
                }

                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              0);
            }
            else if (TempPte.u.Soft.Protection == MM_NOACCESS) {

                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    goto AccessViolation;
                }

                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              1);
            }
            else if (TempPte.u.Soft.Protection == MM_KSTACK_OUTSWAPPED) {

                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    goto AccessViolation;
                }

                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              3);
            }

            if (SessionAddress == TRUE) {

                MM_SESSION_SPACE_WS_LOCK_ASSERT ();

                //
                // If it's a write to a session space page that is ultimately
                // mapped by a prototype PTE, it's a copy-on-write piece of
                // a session driver.  Since the page isn't even present yet,
                // turn the write access into a read access to fault it in.
                // We'll get a write fault on the present page when we retry
                // the operation at which point we'll sever the copy on write.
                //

                if ((PointerProtoPte != NULL) &&
                    (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
                    (MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress))) {

                    MI_CLEAR_FAULT_STATUS (FaultStatus);
                }

                FaultProcess = HYDRA_PROCESS;
            }
            else {
                FaultProcess = NULL;

                if (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) {

                    if ((TempPte.u.Hard.Valid == 0) && (PointerProtoPte == NULL)) {
                        if (TempPte.u.Soft.Transition == 1) {
            
                            if ((TempPte.u.Trans.Protection & MM_READWRITE) == 0) {
                                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                              (ULONG_PTR)VirtualAddress,
                                              (ULONG_PTR)TempPte.u.Long,
                                              (ULONG_PTR)TrapInformation,
                                              14);
                            }
                        }
                        else {
                            if ((TempPte.u.Soft.Protection & MM_READWRITE) == 0) {
                    
                                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                              (ULONG_PTR)VirtualAddress,
                                              (ULONG_PTR)TempPte.u.Long,
                                              (ULONG_PTR)TrapInformation,
                                              15);
                            }
                        }
                    }
                }
            }

            status = MiDispatchFault (FaultStatus,
                                      VirtualAddress,
                                      PointerPte,
                                      PointerProtoPte,
                                      FaultProcess,
                                      &ApcNeeded);

            ASSERT (ApcNeeded == FALSE);
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);

            if (SessionAddress == TRUE) {
                Ws = &MmSessionSpace->Vm;
                PageFrameIndex = Ws->PageFaultCount;
                MM_SESSION_SPACE_WS_LOCK_ASSERT();
            }
            else {
                Ws = &MmSystemCacheWs;
                PageFrameIndex = MmSystemCacheWs.PageFaultCount;
            }

            if (Ws->Flags.AllowWorkingSetAdjustment == MM_GROW_WSLE_HASH) {
                MiGrowWsleHash (Ws);
                Ws->Flags.AllowWorkingSetAdjustment = TRUE;
            }

            if (SessionAddress == TRUE) {
                UNLOCK_SESSION_SPACE_WS (PreviousIrql);
            }
            else {
                UNLOCK_SYSTEM_WS (PreviousIrql);
            }

            if (((PageFrameIndex & 0xFFF) == 0) &&
                (MmAvailablePages < MmMoreThanEnoughFreePages + 220)) {

                //
                // The system cache or this session is taking too many faults,
                // delay execution so the modified page writer gets a quick
                // shot and increase the working set size.
                //

                if (PsGetCurrentThread()->MemoryMaker == 0) {

                    KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                }
            }
            PERFINFO_FAULT_NOTIFICATION(VirtualAddress, TrapInformation);
            NotifyRoutine = MmPageFaultNotifyRoutine;
            if (NotifyRoutine) {
                if (status != STATUS_SUCCESS) {
                    (*NotifyRoutine) (
                        status,
                        VirtualAddress,
                        TrapInformation
                        );
                }
            }
            return status;
        }

#if (_MI_PAGING_LEVELS < 3)
        if (MiCheckPdeForPagedPool (VirtualAddress) == STATUS_WAIT_1) {
            return STATUS_SUCCESS;
        }
#endif
    }

#if (_MI_PAGING_LEVELS >= 3)
UserFault:
#endif

    if (MiDelayPageFaults ||
        ((MmModifiedPageListHead.Total >= (MmModifiedPageMaximum + 100)) &&
        (MmAvailablePages < (1024*1024 / PAGE_SIZE)) &&
            (CurrentProcess->ModifiedPageCount > ((64*1024)/PAGE_SIZE)))) {

        //
        // This process has placed more than 64k worth of pages on the modified
        // list.  Delay for a short period and set the count to zero.
        //

        KeDelayExecutionThread (KernelMode,
                                FALSE,
             (CurrentProcess->Pcb.BasePriority < PROCESS_FOREGROUND_PRIORITY) ?
                                    (PLARGE_INTEGER)&MmHalfSecond : (PLARGE_INTEGER)&Mm30Milliseconds);
        CurrentProcess->ModifiedPageCount = 0;
    }

    //
    // FAULT IN USER SPACE OR PAGE DIRECTORY/PAGE TABLE PAGES.
    //

    //
    // Block APCs and acquire the working set lock.
    //

    LOCK_WS (CurrentProcess);

#if DBG
    if (PreviousMode == KernelMode) {

#if defined(MM_SHARED_USER_DATA_VA)
        if (PAGE_ALIGN(VirtualAddress) != (PVOID) MM_SHARED_USER_DATA_VA) {
#endif

        LARGE_INTEGER CurrentTime;
        ULONG_PTR InstructionPointer;

        if ((MmInjectUserInpageErrors & 0x2) ||
            (CurrentProcess->Flags & PS_PROCESS_INJECT_INPAGE_ERRORS)) {

            KeQueryTickCount(&CurrentTime);

            if ((CurrentTime.LowPart & MmInpageFraction) == 0) {

                if (TrapInformation != NULL) {
#if defined(_X86_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->Eip;
#elif defined(_IA64_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->StIIP;
#elif defined(_AMD64_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->Rip;
#else
                    error
#endif

                    if (MmInjectUserInpageErrors & 0x1) {
                        MmInjectedUserInpageErrors += 1;
                        MiSnapInPageError ((PVOID)InstructionPointer);
                        status = STATUS_NO_MEMORY;
                        goto ReturnStatus2;
                    }

                    if ((InstructionPointer >= (ULONG_PTR) PsNtosImageBase) &&
                        (InstructionPointer < (ULONG_PTR) PsNtosImageEnd)) {

                        MmInjectedUserInpageErrors += 1;
                        MiSnapInPageError ((PVOID)InstructionPointer);
                        status = STATUS_NO_MEMORY;
                        goto ReturnStatus2;
                    }
                }
            }
        }
#if defined(MM_SHARED_USER_DATA_VA)
        }
#endif
    }
#endif

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Locate the Extended Page Directory Parent Entry which maps this virtual
    // address and check for accessibility and validity.  The page directory
    // parent page must be made valid before any other checks are made.
    //

    if (PointerPxe->u.Hard.Valid == 0) {

        //
        // If the PXE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPxe->u.Long == MM_ZERO_PTE) ||
            (PointerPxe->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode);

#ifdef LARGE_PAGES
            if (ProtectCode == MM_LARGE_PAGES) {
                status = STATUS_SUCCESS;
                goto ReturnStatus2;
            }
#endif //LARGE_PAGES

            if (ProtectCode == MM_NOACCESS) {
                status = STATUS_ACCESS_VIOLATION;
                if (PointerPxe->u.Hard.Valid == 1) {
                    status = STATUS_SUCCESS;
                }

#if DBG
                if ((MmDebug & MM_DBG_STOP_ON_ACCVIO) &&
                    (status == STATUS_ACCESS_VIOLATION)) {
                    DbgPrint("MM:access violation - %p\n",VirtualAddress);
                    MiFormatPte(PointerPxe);
                    DbgBreakPoint();
                }
#endif //DEBUG

                goto ReturnStatus2;

            }

            //
            // Build a demand zero PXE and operate on it.
            //

#if (_MI_PAGING_LEVELS > 4)
            ASSERT (FALSE);     // UseCounts will need to be kept.
#endif

            *PointerPxe = DemandZeroPde;
        }

        //
        // The PXE is not valid, call the page fault routine passing
        // in the address of the PXE.  If the PXE is valid, determine
        // the status of the corresponding PPE.
        //
        // Note this call may result in ApcNeeded getting set to TRUE.
        // This is deliberate as there may be another call to MiDispatchFault
        // issued later in this routine and we don't want to lose the APC
        // status.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPpe,   // Virtual address
                                  PointerPxe,   // PTE (PXE in this case)
                                  NULL,
                                  CurrentProcess,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);
        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // The PXE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPxe, PointerPde, FALSE);

        //
        // Now that the PXE is accessible, get the PPE - let this fall
        // through.
        //
    }
#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Locate the Page Directory Parent Entry which maps this virtual
    // address and check for accessibility and validity.  The page directory
    // page must be made valid before any other checks are made.
    //

    if (PointerPpe->u.Hard.Valid == 0) {

        //
        // If the PPE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPpe->u.Long == MM_ZERO_PTE) ||
            (PointerPpe->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode);

#ifdef LARGE_PAGES
            if (ProtectCode == MM_LARGE_PAGES) {
                status = STATUS_SUCCESS;
                goto ReturnStatus2;
            }
#endif //LARGE_PAGES

            if (ProtectCode == MM_NOACCESS) {
                status = STATUS_ACCESS_VIOLATION;
                if (PointerPpe->u.Hard.Valid == 1) {
                    status = STATUS_SUCCESS;
                }

#if DBG
                if ((MmDebug & MM_DBG_STOP_ON_ACCVIO) &&
                    (status == STATUS_ACCESS_VIOLATION)) {
                    DbgPrint("MM:access violation - %p\n",VirtualAddress);
                    MiFormatPte(PointerPpe);
                    DbgBreakPoint();
                }
#endif //DEBUG

                goto ReturnStatus2;

            }

#if (_MI_PAGING_LEVELS >= 4)

            //
            // Increment the count of non-zero page directory parent entries
            // for this page directory parent.
            //

            if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }
#endif

            //
            // Build a demand zero PPE and operate on it.
            //

            *PointerPpe = DemandZeroPde;
        }

        //
        // The PPE is not valid, call the page fault routine passing
        // in the address of the PPE.  If the PPE is valid, determine
        // the status of the corresponding PDE.
        //
        // Note this call may result in ApcNeeded getting set to TRUE.
        // This is deliberate as there may be another call to MiDispatchFault
        // issued later in this routine and we don't want to lose the APC
        // status.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPde,   //Virtual address
                                  PointerPpe,   // PTE (PPE in this case)
                                  NULL,
                                  CurrentProcess,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);
        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // The PPE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPpe, PointerPde, FALSE);

        //
        // Now that the PPE is accessible, get the PDE - let this fall
        // through.
        //
    }
#endif

    //
    // Locate the Page Directory Entry which maps this virtual
    // address and check for accessibility and validity.
    //

    //
    // Check to see if the page table page (PDE entry) is valid.
    // If not, the page table page must be made valid first.
    //

    if (PointerPde->u.Hard.Valid == 0) {

        //
        // If the PDE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPde->u.Long == MM_ZERO_PTE) ||
            (PointerPde->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode);

#ifdef LARGE_PAGES
            if (ProtectCode == MM_LARGE_PAGES) {
                status = STATUS_SUCCESS;
                goto ReturnStatus2;
            }
#endif

            if (ProtectCode == MM_NOACCESS) {
                status = STATUS_ACCESS_VIOLATION;
#if (_MI_PAGING_LEVELS < 3)
                MiCheckPdeForPagedPool (VirtualAddress);
#endif

                if (PointerPde->u.Hard.Valid == 1) {
                    status = STATUS_SUCCESS;
                }

#if DBG
                if ((MmDebug & MM_DBG_STOP_ON_ACCVIO) &&
                    (status == STATUS_ACCESS_VIOLATION)) {
                    DbgPrint("MM:access violation - %p\n",VirtualAddress);
                    MiFormatPte(PointerPde);
                    DbgBreakPoint();
                }
#endif

                goto ReturnStatus2;

            }

#if (_MI_PAGING_LEVELS >= 3)

            //
            // Increment the count of non-zero page directory entries for this
            // page directory.
            //

            if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }
#endif
            //
            // Build a demand zero PDE and operate on it.
            //

            MI_WRITE_INVALID_PTE (PointerPde, DemandZeroPde);
        }

        //
        // The PDE is not valid, call the page fault routine passing
        // in the address of the PDE.  If the PDE is valid, determine
        // the status of the corresponding PTE.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPte,   //Virtual address
                                  PointerPde,   // PTE (PDE in this case)
                                  NULL,
                                  CurrentProcess,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Note that the page directory parent page itself could have been
        // paged out or deleted while MiDispatchFault was executing without
        // the working set lock, so this must be checked for here in the PXE.
        //

        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // The PXE is not valid, return the status.
            //

            goto ReturnStatus1;
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Note that the page directory page itself could have been paged out
        // or deleted while MiDispatchFault was executing without the working
        // set lock, so this must be checked for here in the PPE.
        //

        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // The PPE is not valid, return the status.
            //

            goto ReturnStatus1;
        }
#endif

        if (PointerPde->u.Hard.Valid == 0) {

            //
            // The PDE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPde, PointerPte, FALSE);

        //
        // Now that the PDE is accessible, get the PTE - let this fall
        // through.
        //
    }

    //
    // The PDE is valid and accessible, get the PTE contents.
    //

    TempPte = *PointerPte;
    if (TempPte.u.Hard.Valid != 0) {

        //
        // The PTE is valid and accessible, is this a write fault
        // copy on write or setting of some dirty bit?
        //

#if DBG
        if (MmDebug & MM_DBG_PTE_UPDATE) {
            MiFormatPte(PointerPte);
        }
#endif

        status = STATUS_SUCCESS;

        if (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) {

            //
            // This was a write operation.  If the copy on write
            // bit is set in the PTE perform the copy on write,
            // else check to ensure write access to the PTE.
            //

            if (TempPte.u.Hard.CopyOnWrite != 0) {
                MiCopyOnWrite (VirtualAddress, PointerPte);
                status = STATUS_PAGE_FAULT_COPY_ON_WRITE;
                goto ReturnStatus2;

            } else {
                if (TempPte.u.Hard.Write == 0) {
                    status = STATUS_ACCESS_VIOLATION;
                }
            }

        } else if (MI_FAULT_STATUS_INDICATES_EXECUTION(FaultStatus)) {

            //
            // Ensure execute access is enabled in the PTE.
            //

            if (!MI_IS_PTE_EXECUTABLE(&TempPte)) {
                status = STATUS_ACCESS_VIOLATION;
            }

        } else {

            //
            // The PTE is valid and accessible, another thread must
            // have faulted the PTE in already, or the access bit
            // is clear and this is a access fault; Blindly set the
            // access bit and dismiss the fault.
            //

#if DBG
            if (MmDebug & MM_DBG_SHOW_FAULTS) {
                DbgPrint("MM:no fault found - PTE is %p\n", PointerPte->u.Long);
            }
#endif
        }

        if (status == STATUS_SUCCESS) {
            LOCK_PFN (OldIrql);
            if (PointerPte->u.Hard.Valid != 0) {
                MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
            }
            UNLOCK_PFN (OldIrql);
        }

        goto ReturnStatus2;
    }

    //
    // If the PTE is zero, check to see if there is a virtual address
    // mapped at this location, and if so create the necessary
    // structures to map it.
    //

    //
    // Check explicitly for demand zero pages.
    //

    if (TempPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE) {
        MiResolveDemandZeroFault (VirtualAddress,
                                  PointerPte,
                                  CurrentProcess,
                                  0);

        status = STATUS_PAGE_FAULT_DEMAND_ZERO;
        goto ReturnStatus1;
    }

    RecheckAccess = FALSE;

    if ((TempPte.u.Long == MM_ZERO_PTE) ||
        (TempPte.u.Long == MM_ZERO_KERNEL_PTE)) {

        //
        // PTE is needs to be evaluated with respect to its virtual
        // address descriptor (VAD).  At this point there are 3
        // possibilities, bogus address, demand zero, or refers to
        // a prototype PTE.
        //

        PointerProtoPte = MiCheckVirtualAddress (VirtualAddress,
                                                 &ProtectionCode);
        if (ProtectionCode == MM_NOACCESS) {
            status = STATUS_ACCESS_VIOLATION;

            //
            // Check to make sure this is not a page table page for
            // paged pool which needs extending.
            //

#if (_MI_PAGING_LEVELS < 3)
            MiCheckPdeForPagedPool (VirtualAddress);
#endif

            if (PointerPte->u.Hard.Valid == 1) {
                status = STATUS_SUCCESS;
            }

#if DBG
            if ((MmDebug & MM_DBG_STOP_ON_ACCVIO) &&
                (status == STATUS_ACCESS_VIOLATION)) {
                DbgPrint("MM:access vio - %p\n",VirtualAddress);
                MiFormatPte(PointerPte);
                DbgBreakPoint();
            }
#endif //DEBUG
            goto ReturnStatus2;
        }

        //
        // Increment the count of non-zero page table entries for this
        // page table.
        //

        if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (VirtualAddress);
            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
        }
#if (_MI_PAGING_LEVELS >= 3)
        else if (MI_IS_PAGE_TABLE_ADDRESS(VirtualAddress)) {
            PVOID RealVa;

            RealVa = MiGetVirtualAddressMappedByPte(VirtualAddress);

            if (RealVa <= MM_HIGHEST_USER_ADDRESS) {

                //
                // This is really a page table page.  Increment the use count
                // on the appropriate page directory.
                //

                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (VirtualAddress);
                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }
        }
#endif

        //
        // Is this page a guard page?
        //

        if (ProtectionCode & MM_GUARD_PAGE) {

            //
            // This is a guard page exception.
            //

            PointerPte->u.Soft.Protection = ProtectionCode & ~MM_GUARD_PAGE;

            if (PointerProtoPte != NULL) {

                //
                // This is a prototype PTE, build the PTE to not
                // be a guard page.
                //

                PointerPte->u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;
                PointerPte->u.Soft.Prototype = 1;
            }

            UNLOCK_WS (CurrentProcess);
            ASSERT (KeGetCurrentIrql() == PreviousIrql);

            if (ApcNeeded == TRUE) {
                ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
                ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
                ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
                KeRaiseIrql (APC_LEVEL, &PreviousIrql);
                IoRetryIrpCompletions ();
                KeLowerIrql (PreviousIrql);
            }

            return MiCheckForUserStackOverflow (VirtualAddress);
        }

        if (PointerProtoPte == NULL) {

            //
            // Assert that this is not for a PDE.
            //

            if (PointerPde == MiGetPdeAddress((PVOID)PTE_BASE)) {

                //
                // This PTE is really a PDE, set contents as such.
                //

                MI_WRITE_INVALID_PTE (PointerPte, DemandZeroPde);
            } else {
                PointerPte->u.Soft.Protection = ProtectionCode;
            }

            //
            // If a fork operation is in progress and the faulting thread
            // is not the thread performing the fork operation, block until
            // the fork is completed.
            //

            if (CurrentProcess->ForkInProgress != NULL) {
                if (MiWaitForForkToComplete (CurrentProcess, FALSE) == TRUE) {
                    status = STATUS_SUCCESS;
                    goto ReturnStatus1;
                }
            }

            LOCK_PFN (OldIrql);

            if (!MiEnsureAvailablePageOrWait (CurrentProcess,
                                              VirtualAddress)) {

                ULONG Color;
                Color = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                                &CurrentProcess->NextPageColor);
                PageFrameIndex = MiRemoveZeroPageIfAny (Color);
                if (PageFrameIndex == 0) {
                    PageFrameIndex = MiRemoveAnyPage (Color);
                    UNLOCK_PFN (OldIrql);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    MiZeroPhysicalPage (PageFrameIndex, Color);

#if MI_BARRIER_SUPPORTED

                    //
                    // Note the stamping must occur after the page is zeroed.
                    //

                    MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
                    Pfn1->u4.PteFrame = BarrierStamp;
#endif

                    LOCK_PFN (OldIrql);
                }

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                CurrentProcess->NumberOfPrivatePages += 1;
                MmInfoCounters.DemandZeroCount += 1;

                //
                // This barrier check is needed after zeroing the page and
                // before setting the PTE valid.
                // Capture it now, check it at the last possible moment.
                //

                BarrierStamp = (ULONG)Pfn1->u4.PteFrame;

                MiInitializePfn (PageFrameIndex, PointerPte, 1);

                UNLOCK_PFN (OldIrql);

                //
                // As this page is demand zero, set the modified bit in the
                // PFN database element and set the dirty bit in the PTE.
                //

                MI_MAKE_VALID_PTE (TempPte,
                                   PageFrameIndex,
                                   PointerPte->u.Soft.Protection,
                                   PointerPte);

                if (TempPte.u.Hard.Write != 0) {
                    MI_SET_PTE_DIRTY (TempPte);
                }

                MI_BARRIER_SYNCHRONIZE (BarrierStamp);

                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                ASSERT (Pfn1->u1.Event == 0);

                Pfn1->u1.Event = (PVOID)PsGetCurrentThread();

                WorkingSetIndex = MiLocateAndReserveWsle (&CurrentProcess->Vm);
                MiUpdateWsle (&WorkingSetIndex,
                              VirtualAddress,
                              MmWorkingSetList,
                              Pfn1);

                MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

                KeFillEntryTb ((PHARDWARE_PTE)PointerPte,
                                VirtualAddress,
                                FALSE);
            } else {
                UNLOCK_PFN (OldIrql);
            }

            status = STATUS_PAGE_FAULT_DEMAND_ZERO;
            goto ReturnStatus1;
        }

        //
        // This is a prototype PTE.
        //

        if (ProtectionCode == MM_UNKNOWN_PROTECTION) {

            //
            // The protection field is stored in the prototype PTE.
            //

            PointerPte->u.Long = MiProtoAddressForPte (PointerProtoPte);

        } else {

            MI_WRITE_INVALID_PTE (PointerPte, PrototypePte);
            PointerPte->u.Soft.Protection = ProtectionCode;
        }

        TempPte = *PointerPte;

    } else {

        //
        // The PTE is non-zero and not valid, see if it is a prototype PTE.
        //

        ProtectionCode = MI_GET_PROTECTION_FROM_SOFT_PTE(&TempPte);

        if (TempPte.u.Soft.Prototype != 0) {
            if (TempPte.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {
#if DBG
                MmProtoPteVadLookups += 1;
#endif //DBG
                PointerProtoPte = MiCheckVirtualAddress (VirtualAddress,
                                                         &ProtectCode);
                if (PointerProtoPte == NULL) {
                    status = STATUS_ACCESS_VIOLATION;
                    goto ReturnStatus1;
                }

            } else {
#if DBG
                MmProtoPteDirect += 1;
#endif //DBG

                //
                // Protection is in the prototype PTE, indicate an
                // access check should not be performed on the current PTE.
                //

                PointerProtoPte = MiPteToProto (&TempPte);
                ProtectionCode = MM_UNKNOWN_PROTECTION;

                //
                // Check to see if the proto protection has been overridden.
                //

                if (TempPte.u.Proto.ReadOnly != 0) {
                    ProtectionCode = MM_READONLY;
                }
                else {
                    ProtectionCode = MM_UNKNOWN_PROTECTION;
                    if (CurrentProcess->CloneRoot != NULL) {
                        RecheckAccess = TRUE;
                    }
                }
            }
        }
    }

    if (ProtectionCode != MM_UNKNOWN_PROTECTION) {
        status = MiAccessCheck (PointerPte,
                                MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus),
                                PreviousMode,
                                ProtectionCode,
                                FALSE );

        if (status != STATUS_SUCCESS) {
#if DBG
            if ((MmDebug & MM_DBG_STOP_ON_ACCVIO) && (status == STATUS_ACCESS_VIOLATION)) {
                DbgPrint("MM:access violate - %p\n",VirtualAddress);
                MiFormatPte(PointerPte);
                DbgBreakPoint();
            }
#endif //DEBUG

            UNLOCK_WS (CurrentProcess);
            ASSERT (KeGetCurrentIrql() == PreviousIrql);

            if (ApcNeeded == TRUE) {
                ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
                ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
                ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
                KeRaiseIrql (APC_LEVEL, &PreviousIrql);
                IoRetryIrpCompletions ();
                KeLowerIrql (PreviousIrql);
            }

            //
            // Check to see if this is a guard page violation
            // and if so, should the user's stack be extended.
            //

            if (status == STATUS_GUARD_PAGE_VIOLATION) {
                return MiCheckForUserStackOverflow (VirtualAddress);
            }

            return status;
        }
    }

    //
    // Initializing Pfn1 is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Pfn1 = NULL;

    //
    // This is a page fault, invoke the page fault handler.
    //

    if (PointerProtoPte != NULL) {

        //
        // Lock page containing prototype PTEs in memory by
        // incrementing the reference count for the page.
        //

        ASSERT (!MI_IS_PHYSICAL_ADDRESS(PointerProtoPte));

        PointerPde = MiGetPteAddress (PointerProtoPte);

        LOCK_PFN (OldIrql);

        if (PointerPde->u.Hard.Valid == 0) {
            MiMakeSystemAddressValidPfn (PointerProtoPte);
        }

        if (RecheckAccess == TRUE) {

            //
            // This is a forked process so shared prototype PTEs may actually
            // be fork clone prototypes.  These have the protection within the
            // fork clone yet the hardware PTEs always share it.  This must be
            // checked here for the case where the NO_ACCESS permission has
            // been put into the fork clone because it would not necessarily
            // be in the hardware PTEs (like it is for normal prototypes).
            //
            // First make sure the proto is in transition or paged out as only
            // these states can be no access.
            //

            if ((PointerProtoPte->u.Hard.Valid == 0) &&
                (PointerProtoPte->u.Soft.Prototype == 0)) {

                ProtoProtect = MI_GET_PROTECTION_FROM_SOFT_PTE (PointerProtoPte);
                if (ProtoProtect == MM_NOACCESS) {
                    ASSERT (MiLocateCloneAddress (CurrentProcess, PointerProtoPte) != NULL);
                    UNLOCK_PFN (OldIrql);
                    UNLOCK_WS (CurrentProcess);
                    ASSERT (KeGetCurrentIrql() == PreviousIrql);
                    return STATUS_ACCESS_VIOLATION;
                }
            }
        }

        Pfn1 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 2);
        Pfn1->u3.e2.ReferenceCount += 1;
        ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

        UNLOCK_PFN (OldIrql);
    }
    status = MiDispatchFault (FaultStatus,
                              VirtualAddress,
                              PointerPte,
                              PointerProtoPte,
                              CurrentProcess,
                              &ApcNeeded);

#if DBG
    if (ApcNeeded == TRUE) {
        ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
        ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
    }
#endif

    if (PointerProtoPte != NULL) {

        //
        // Unlock page containing prototype PTEs.
        //

        ASSERT (PointerProtoPte != NULL);
        LOCK_PFN (OldIrql);

        //
        // The reference count on the prototype PTE page will always be greater
        // than 1 if it is a genuine prototype PTE pool allocation.  However,
        // if it is a fork prototype PTE allocation, it is possible the pool has
        // already been deallocated and in this case, the Pfn1 frame below will
        // be in transition limbo with a share count of 0 and a reference count
        // of 1 awaiting our final dereference below which will put it on the
        // free list.
        //

        ASSERT (Pfn1->u3.e2.ReferenceCount >= 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 3);
        UNLOCK_PFN (OldIrql);
    }

ReturnStatus1:

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);
    if (CurrentProcess->Vm.Flags.AllowWorkingSetAdjustment == MM_GROW_WSLE_HASH) {
        MiGrowWsleHash (&CurrentProcess->Vm);
        CurrentProcess->Vm.Flags.AllowWorkingSetAdjustment = TRUE;
    }

ReturnStatus2:

    PageFrameIndex = CurrentProcess->Vm.WorkingSetSize - CurrentProcess->Vm.MinimumWorkingSetSize;

    UNLOCK_WS (CurrentProcess);
    ASSERT (KeGetCurrentIrql() == PreviousIrql);

    if (ApcNeeded == TRUE) {
        ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
        ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
        KeRaiseIrql (APC_LEVEL, &PreviousIrql);
        IoRetryIrpCompletions ();
        KeLowerIrql (PreviousIrql);
    }

    if (MmAvailablePages < MmMoreThanEnoughFreePages + 220) {

        if (((SPFN_NUMBER)PageFrameIndex > 100) &&
            (PsGetCurrentThread()->Tcb.Priority >= LOW_REALTIME_PRIORITY)) {

            //
            // This thread is realtime and is well over the process'
            // working set minimum.  Delay execution so the trimmer & the
            // modified page writer get a quick shot at making pages.
            //

            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        }
    }

    PERFINFO_FAULT_NOTIFICATION(VirtualAddress, TrapInformation);
    NotifyRoutine = MmPageFaultNotifyRoutine;
    if (NotifyRoutine) {
        if (status != STATUS_SUCCESS) {
            (*NotifyRoutine) (
                status,
                VirtualAddress,
                TrapInformation
                );
        }
    }

    return status;

AccessViolation:
    if (SessionAddress == TRUE) {
        UNLOCK_SESSION_SPACE_WS (PreviousIrql);
    }
    else {
        UNLOCK_SYSTEM_WS (PreviousIrql);
    }
    return STATUS_ACCESS_VIOLATION;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mminit.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    mminit.c

Abstract:

    This module contains the initialization for the memory management
    system.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

PMMPTE MmSharedUserDataPte;

extern ULONG_PTR MmSystemPtesStart[MaximumPtePoolTypes];
extern PMMPTE MiSpecialPoolFirstPte;
extern ULONG MmPagedPoolCommit;
extern ULONG MmInPageSupportMinimum;
extern PFN_NUMBER MiExpansionPoolPagesInitialCharge;
extern ULONG MmAllocationPreference;

extern PVOID BBTBuffer;
extern PFN_COUNT BBTPagesToReserve;

ULONG_PTR MmSubsectionBase;
ULONG_PTR MmSubsectionTopPage;
ULONG MmDataClusterSize;
ULONG MmCodeClusterSize;
PFN_NUMBER MmResidentAvailableAtInit;
PPHYSICAL_MEMORY_DESCRIPTOR MmPhysicalMemoryBlock;
LIST_ENTRY MmLockConflictList;
LIST_ENTRY MmProtectedPteList;
KSPIN_LOCK MmProtectedPteLock;
LOGICAL MmPagedPoolMaximumDesired = FALSE;

#if defined (_MI_DEBUG_SUB)
ULONG MiTrackSubs = 0x2000; // Set to nonzero to enable subsection tracking code.
LONG MiSubsectionIndex;
PMI_SUB_TRACES MiSubsectionTraces;
#endif

#if defined (_MI_DEBUG_DIRTY)
ULONG MiTrackDirtys = 0x10000; // Set to nonzero to enable subsection tracking code.
LONG MiDirtyIndex;
PMI_DIRTY_TRACES MiDirtyTraces;
#endif

#if defined (_MI_DEBUG_DATA)
ULONG MiTrackData = 0x10000; // Set to nonzero to enable data tracking code.
LONG MiDataIndex;
PMI_DATA_TRACES MiDataTraces;
#endif

VOID
MiMapBBTMemory (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiEnablePagingTheExecutive(
    VOID
    );

VOID
MiEnablePagingOfDriverAtInit (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    );

VOID
MiBuildPagedPool (
    );

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    );

VOID
MiInitializePfnTracing (
    VOID
    );

PFN_NUMBER
MiPagesInLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType
    );

#ifndef NO_POOL_CHECKS
VOID
MiInitializeSpecialPoolCriteria (
    IN VOID
    );
#endif

#ifdef _MI_MESSAGE_SERVER
VOID
MiInitializeMessageQueue (
    VOID
    );
#endif

static
VOID
MiMemoryLicense (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiInitializeCacheOverrides (
    VOID
    );

//
// The thresholds can be overridden by the registry.
//

PFN_NUMBER MmLowMemoryThreshold;
PFN_NUMBER MmHighMemoryThreshold;

PKEVENT MiLowMemoryEvent;
PKEVENT MiHighMemoryEvent;

NTSTATUS
MiCreateMemoryEvent (
    IN PUNICODE_STRING EventName,
    OUT PKEVENT *Event
    );

LOGICAL
MiInitializeMemoryEvents (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MmInitSystem)
#pragma alloc_text(INIT,MiMapBBTMemory)
#pragma alloc_text(INIT,MmInitializeMemoryLimits)
#pragma alloc_text(INIT,MmFreeLoaderBlock)
#pragma alloc_text(INIT,MiBuildPagedPool)
#pragma alloc_text(INIT,MiFindInitializationCode)
#pragma alloc_text(INIT,MiEnablePagingTheExecutive)
#pragma alloc_text(INIT,MiEnablePagingOfDriverAtInit)
#pragma alloc_text(INIT,MiPagesInLoaderBlock)
#pragma alloc_text(INIT,MiCreateMemoryEvent)
#pragma alloc_text(INIT,MiInitializeMemoryEvents)
#pragma alloc_text(INIT,MiInitializeCacheOverrides)
#pragma alloc_text(INIT,MiMemoryLicense)
#pragma alloc_text(PAGELK,MiFreeInitializationCode)
#endif

//
// Default is a 300 second life span for modified mapped pages -
// This can be overridden in the registry.
//

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("INITDATA")
#endif
ULONG MmModifiedPageLifeInSeconds = 300;
#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

LARGE_INTEGER MiModifiedPageLife;

BOOLEAN MiTimerPending = FALSE;

KEVENT MiMappedPagesTooOldEvent;

KDPC MiModifiedPageWriterTimerDpc;

KTIMER MiModifiedPageWriterTimer;

//
// The following constants are based on the number PAGES not the
// memory size.  For convenience the number of pages is calculated
// based on a 4k page size.  Hence 12mb with 4k page is 3072.
//

#define MM_SMALL_SYSTEM ((13*1024*1024) / 4096)

#define MM_MEDIUM_SYSTEM ((19*1024*1024) / 4096)

#define MM_MIN_INITIAL_PAGED_POOL ((32*1024*1024) >> PAGE_SHIFT)

#define MM_DEFAULT_IO_LOCK_LIMIT (2 * 1024 * 1024)

extern WSLE_NUMBER MmMaximumWorkingSetSize;

extern ULONG MmEnforceWriteProtection;

extern CHAR MiPteStr[];

extern LONG MiTrimInProgressCount;

#if (_MI_PAGING_LEVELS < 3)
PFN_NUMBER MmSystemPageDirectory[PD_PER_SYSTEM];
PMMPTE MmSystemPagePtes;
#endif

ULONG MmTotalSystemCodePages;

MM_SYSTEMSIZE MmSystemSize;

ULONG MmLargeSystemCache;

ULONG MmProductType;

extern ULONG MiVerifyAllDrivers;

LIST_ENTRY MmLoadedUserImageList;
PPAGE_FAULT_NOTIFY_ROUTINE MmPageFaultNotifyRoutine;

#if defined (_WIN64)
#define MM_ALLOCATION_FRAGMENT (64 * 1024 * 1024)
#else
#define MM_ALLOCATION_FRAGMENT (64 * 1024)
#endif

//
// Registry-settable.
//

SIZE_T MmAllocationFragment;


#if defined(MI_MULTINODE)

HALNUMAPAGETONODE
MiNonNumaPageToNodeColor (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    Return the node color of the page.

Arguments:

    PageFrameIndex - Supplies the physical page number.

Return Value:

    Node color is always zero in non-NUMA configurations.

--*/

{
    UNREFERENCED_PARAMETER (PageFrameIndex);

    return 0;
}

//
// This node determination function pointer is initialized to return 0.
//
// Architecture-dependent initialization may repoint it to a HAL routine
// for NUMA configurations.
//

PHALNUMAPAGETONODE MmPageToNode = MiNonNumaPageToNodeColor;


VOID
MiDetermineNode (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPFN Pfn
    )

/*++

Routine Description:

    This routine is called during initial freelist population or when
    physical memory is being hot-added.  It then determines which node
    (in a multinode NUMA system) the physical memory resides in, and
    marks the PFN entry accordingly.

    N.B.  The actual page to node determination is machine dependent
    and done by a routine in the chipset driver or the HAL, called
    via the MmPageToNode function pointer.

Arguments:

    PageFrameIndex - Supplies the physical page number.

    Pfn - Supplies a pointer to the PFN database element.

Return Value:

    None.

Environment:

    None although typically this routine is called with the PFN
    database locked.

--*/

{
    ULONG Temp;

    ASSERT (Pfn == MI_PFN_ELEMENT(PageFrameIndex));

    Temp = MmPageToNode (PageFrameIndex);

    ASSERT (Temp < MAXIMUM_CCNUMA_NODES);

    Pfn->u3.e1.PageColor = Temp;
}

#endif


BOOLEAN
MmInitSystem (
    IN ULONG Phase,
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function is called during Phase 0, phase 1 and at the end
    of phase 1 ("phase 2") initialization.

    Phase 0 initializes the memory management paging functions,
    nonpaged and paged pool, the PFN database, etc.

    Phase 1 initializes the section objects, the physical memory
    object, and starts the memory management system threads.

    Phase 2 frees memory used by the OsLoader.

Arguments:

    Phase - System initialization phase.

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    Returns TRUE if the initialization was successful.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PEPROCESS Process;
    PSINGLE_LIST_ENTRY SingleListEntry;
    PFN_NUMBER NumberOfPages;
    HANDLE ThreadHandle;
    OBJECT_ATTRIBUTES ObjectAttributes;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE StartPde;
    PMMPTE EndPde;
    PMMPFN Pfn1;
    PFN_NUMBER i, j;
    PFN_NUMBER DeferredMdlEntries;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER DirectoryFrameIndex;
    MMPTE TempPte;
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG MaximumSystemCacheSize;
    ULONG MaximumSystemCacheSizeTotal;
    PIMAGE_NT_HEADERS NtHeaders;
    ULONG_PTR SystemPteMultiplier;
    ULONG_PTR DefaultSystemViewSize;
    ULONG_PTR SessionEnd;
    SIZE_T SystemViewMax;
    SIZE_T HydraImageMax;
    SIZE_T HydraViewMax;
    SIZE_T HydraPoolMax;
    SIZE_T HydraSpaceUsedForSystemViews;
    BOOLEAN IncludeType[LoaderMaximum];
    LOGICAL AutosizingFragment;
    ULONG VerifierFlags;
#if DBG
    MMPTE Pointer;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    LOGICAL FirstPpe;
    PMMPTE StartPpe;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    LOGICAL FirstPxe;
    PMMPTE StartPxe;
#endif
#if defined(_X86_)
    PCHAR ReducedUserVaOption;
    ULONG UserVaLimit;
    ULONG ReductionInBytes;
#endif

    j = 0;
    PointerPde = NULL;

    //
    // Make sure structure alignment is okay.
    //

    if (Phase == 0) {
        MmThrottleTop = 450;
        MmThrottleBottom = 127;

        //
        // Set the highest user address, the system range start address, the
        // user probe address, and the virtual bias.
        //

#if defined(_WIN64)

        MmHighestUserAddress = MI_HIGHEST_USER_ADDRESS;
        MmUserProbeAddress = MI_USER_PROBE_ADDRESS;
        MmSystemRangeStart = MI_SYSTEM_RANGE_START;

#else

        MmHighestUserAddress = (PVOID)(KSEG0_BASE - 0x10000 - 1);
        MmUserProbeAddress = KSEG0_BASE - 0x10000;
        MmSystemRangeStart = (PVOID)KSEG0_BASE;

#endif

        MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
        MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);

#if (_MI_PAGING_LEVELS >= 4)
        MiHighestUserPpe = MiGetPpeAddress (MmHighestUserAddress);
        MiHighestUserPxe = MiGetPxeAddress (MmHighestUserAddress);
#endif

#if defined(_X86_) || defined(_AMD64_)

        MmBootImageSize = LoaderBlock->Extension->LoaderPagesSpanned;
        MmBootImageSize *= PAGE_SIZE;

        MmBootImageSize = MI_ROUND_TO_SIZE (MmBootImageSize,
                                            MM_VA_MAPPED_BY_PDE);

        ASSERT ((MmBootImageSize % MM_VA_MAPPED_BY_PDE) == 0);
#endif

#if defined(_X86_)
        MmVirtualBias = LoaderBlock->u.I386.VirtualBias;
#endif

        //
        // Initialize system and Hydra mapped view sizes.
        //

        DefaultSystemViewSize = MM_SYSTEM_VIEW_SIZE;
        MmSessionSize = MI_SESSION_SPACE_DEFAULT_TOTAL_SIZE;
        SessionEnd = (ULONG_PTR) MM_SESSION_SPACE_DEFAULT_END;

#define MM_MB_MAPPED_BY_PDE (MM_VA_MAPPED_BY_PDE / (1024*1024))

        //
        // A PDE of virtual space is the minimum system view size allowed.
        //

        if (MmSystemViewSize < (MM_VA_MAPPED_BY_PDE / (1024*1024))) {
            MmSystemViewSize = DefaultSystemViewSize;
        }
        else {

            //
            // The view size has been specified (in megabytes) by the registry.
            // Validate it.
            //

            if (MmVirtualBias == 0) {

                //
                // Round the system view size (in megabytes) to a PDE multiple.
                //

                MmSystemViewSize = MI_ROUND_TO_SIZE (MmSystemViewSize,
                                                     MM_MB_MAPPED_BY_PDE);

                //
                // NT64 locates system views just after systemwide paged pool,
                // so the size of the system views are not limited by session
                // space.  Arbitrarily make the maximum a PPE's worth.
                //
                //
                // NT32 shares system view VA space with session VA space due
                // to the shortage of virtual addresses.  Thus increasing the
                // system view size means potentially decreasing the maximum
                // session space size.
                //

                SystemViewMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE) / (1024*1024);

#if !defined(_WIN64)

                //
                // Ensure at least enough space is left for
                // the standard default session layout.
                //

                SystemViewMax -= (MmSessionSize / (1024*1024));
#endif

                //
                // Note a view size of -1 will be rounded to zero.  Treat -1
                // as requesting the maximum.
                //

                if ((MmSystemViewSize > SystemViewMax) ||
                    (MmSystemViewSize == 0)) {

                    MmSystemViewSize = SystemViewMax;
                }

                MmSystemViewSize *= (1024*1024);
            }
            else {
                MmSystemViewSize = DefaultSystemViewSize;
            }
        }

#if defined(_WIN64)
        HydraSpaceUsedForSystemViews = 0;
#else
        HydraSpaceUsedForSystemViews = MmSystemViewSize;
#endif
        MiSessionImageEnd = SessionEnd;

        //
        // Select reasonable Hydra image, pool and view virtual sizes.
        // A PDE of virtual space is the minimum size allowed for each type.
        //

        if (MmVirtualBias == 0) {

            if (MmSessionImageSize < MM_MB_MAPPED_BY_PDE) {
                MmSessionImageSize = MI_SESSION_DEFAULT_IMAGE_SIZE;
            }
            else {

                //
                // The Hydra image size has been specified (in megabytes)
                // by the registry.
                //
                // Round it to a PDE multiple and validate it.
                //

                MmSessionImageSize = MI_ROUND_TO_SIZE (MmSessionImageSize,
                                                        MM_MB_MAPPED_BY_PDE);

                HydraImageMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_IMAGE_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionImageSize > HydraImageMax) ||
                    (MmSessionImageSize == 0)) {
                    MmSessionImageSize = HydraImageMax;
                }

                MmSessionImageSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_IMAGE_SIZE;
                MmSessionSize += MmSessionImageSize;
            }

            MiSessionImageStart = SessionEnd - MmSessionImageSize;

            //
            // The session image start and size has been established.
            //
            // Now initialize the session pool and view ranges which lie
            // virtually below it.
            //

            if (MmSessionViewSize < MM_MB_MAPPED_BY_PDE) {
                MmSessionViewSize = MI_SESSION_DEFAULT_VIEW_SIZE;
            }
            else {

                //
                // The Hydra view size has been specified (in megabytes)
                // by the registry.  Validate it.
                //
                // Round the Hydra view size to a PDE multiple.
                //

                MmSessionViewSize = MI_ROUND_TO_SIZE (MmSessionViewSize,
                                                      MM_MB_MAPPED_BY_PDE);

                HydraViewMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_VIEW_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionViewSize > HydraViewMax) ||
                    (MmSessionViewSize == 0)) {
                    MmSessionViewSize = HydraViewMax;
                }

                MmSessionViewSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_VIEW_SIZE;
                MmSessionSize += MmSessionViewSize;
            }

            MiSessionViewStart = SessionEnd - MmSessionImageSize - MI_SESSION_SPACE_WS_SIZE - MI_SESSION_SPACE_STRUCT_SIZE - MmSessionViewSize;

            //
            // The session view start and size has been established.
            //
            // Now initialize the session pool start and size which lies
            // virtually just below it.
            //

            MiSessionPoolEnd = MiSessionViewStart;

            if (MmSessionPoolSize < MM_MB_MAPPED_BY_PDE) {

#if !defined(_WIN64)

                //
                // Professional and below use systemwide paged pool for session
                // allocations (this decision is made in win32k.sys).  Server
                // and above use real session pool and 16mb isn't enough to
                // play high end game applications, etc.  Since we're not
                // booted /3GB, try for an additional 16mb now.
                //

                if ((MmSessionPoolSize == 0) && (MmProductType != 0x00690057)) {

                    HydraPoolMax = MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - MmSessionSize;
                    if (HydraPoolMax >= 2 * MI_SESSION_DEFAULT_POOL_SIZE) {
                        MmSessionPoolSize = 2 * MI_SESSION_DEFAULT_POOL_SIZE;
                        MmSessionSize -= MI_SESSION_DEFAULT_POOL_SIZE;
                        MmSessionSize += MmSessionPoolSize;
                    }
                    else {
                        MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
                    }
                }
                else
#endif
                MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
            }
            else {

                //
                // The Hydra pool size has been specified (in megabytes)
                // by the registry.  Validate it.
                //
                // Round the Hydra pool size to a PDE multiple.
                //

                MmSessionPoolSize = MI_ROUND_TO_SIZE (MmSessionPoolSize,
                                                      MM_MB_MAPPED_BY_PDE);

                HydraPoolMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_POOL_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionPoolSize > HydraPoolMax) ||
                    (MmSessionPoolSize == 0)) {
                    MmSessionPoolSize = HydraPoolMax;
                }

                MmSessionPoolSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_POOL_SIZE;
                MmSessionSize += MmSessionPoolSize;
            }

            MiSessionPoolStart = MiSessionPoolEnd - MmSessionPoolSize;

            MmSessionBase = (ULONG_PTR) MiSessionPoolStart;

#if defined (_WIN64)

            //
            // Session special pool immediately follows session regular pool
            // assuming the user has enabled either the verifier or special
            // pool.
            //

            if ((MmVerifyDriverBufferLength != (ULONG)-1) ||
                ((MmSpecialPoolTag != 0) && (MmSpecialPoolTag != (ULONG)-1))) {

                MmSessionSize = MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE;
                MmSessionSpecialPoolEnd = (PVOID) MiSessionPoolStart;
                MmSessionBase = MM_SESSION_SPACE_DEFAULT;
                MmSessionSpecialPoolStart = (PVOID) MmSessionBase;
            }
#endif

            ASSERT (MmSessionBase + MmSessionSize == SessionEnd);
            MiSessionSpaceEnd = SessionEnd;
            MiSessionSpacePageTables = (ULONG)(MmSessionSize / MM_VA_MAPPED_BY_PDE);
#if !defined (_WIN64)
            MiSystemViewStart = MmSessionBase - MmSystemViewSize;
#endif

        }
        else {

            //
            // When booted /3GB, no size overrides are allowed due to the
            // already severely limited virtual address space.
            // Initialize the other Hydra variables after the system cache.
            //

            MmSessionViewSize = MI_SESSION_DEFAULT_VIEW_SIZE;
            MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
            MmSessionImageSize = MI_SESSION_DEFAULT_IMAGE_SIZE;

            MiSessionImageStart = MiSessionImageEnd - MmSessionImageSize;
        }

        //
        // Set the highest section base address.
        //
        // N.B. In 32-bit systems this address must be 2gb or less even for
        //      systems that run with 3gb enabled. Otherwise, it would not
        //      be possible to map based sections identically in all processes.
        //

        MmHighSectionBase = ((PCHAR)MmHighestUserAddress - 0x800000);

        MaximumSystemCacheSize = (MM_SYSTEM_CACHE_END - MM_SYSTEM_CACHE_START) >> PAGE_SHIFT;

#if defined(_X86_)

	    //
        // If boot.ini specified a sane number of MB that the administrator
        // wants to use for user virtual address space then use it.
        //

        UserVaLimit = 0;
        ReducedUserVaOption = strstr(LoaderBlock->LoadOptions, "USERVA");

        if (ReducedUserVaOption != NULL) {

            ReducedUserVaOption = strstr(ReducedUserVaOption,"=");

            if (ReducedUserVaOption != NULL) {

                UserVaLimit = atol(ReducedUserVaOption+1);

                UserVaLimit = MI_ROUND_TO_SIZE (UserVaLimit, ((MM_VA_MAPPED_BY_PDE) / (1024*1024)));
	        }

	        //
	        // Ignore the USERVA switch if the limit is too small.
	        //

	        if (UserVaLimit <= (2048 + 16)) {
		        UserVaLimit = 0;
	        }
        }

        if (MmVirtualBias != 0) {

            //
            // If the size of the boot image (likely due to a large registry)
            // overflows into where paged pool would normally start, then
            // move paged pool up now.  This costs virtual address space (ie:
            // performance) but more importantly, allows the system to boot.
            //

            if (MmBootImageSize > 16 * 1024 * 1024) {
                MmPagedPoolStart = (PVOID)((PCHAR)MmPagedPoolStart + (MmBootImageSize - 16 * 1024 * 1024));
                ASSERT (((ULONG_PTR)MmPagedPoolStart % MM_VA_MAPPED_BY_PDE) == 0);
            }

            //
            // The system has been biased to an alternate base address to
            // allow 3gb of user address space, set the user probe address
            // and the maximum system cache size.
            //
            // If the system has been biased to an alternate base address to
            // allow 3gb of user address space, then set the user probe address
            // and the maximum system cache size.

	        if ((UserVaLimit > 2048) && (UserVaLimit < 3072)) {

                //
                // Use any space between the maximum user virtual address
                // and the system for extra system PTEs.
                //
                // Convert input MB to bytes.
                //

                UserVaLimit -= 2048;
                UserVaLimit *= (1024*1024);

                //
                // Don't let the user specify a value which would cause us to
                // prematurely overwrite portions of the kernel & loader block.
                //

                if (UserVaLimit < MmBootImageSize) {
                    UserVaLimit = MmBootImageSize;
                }
            }
            else {
                UserVaLimit = 0x40000000;
            }

            MmHighestUserAddress = ((PCHAR)MmHighestUserAddress + UserVaLimit);
            MmSystemRangeStart = ((PCHAR)MmSystemRangeStart + UserVaLimit);
            MmUserProbeAddress += UserVaLimit;
            MiMaximumWorkingSet += UserVaLimit >> PAGE_SHIFT;

            if (UserVaLimit != 0x40000000) {
                MiUseMaximumSystemSpace = (ULONG_PTR)MmSystemRangeStart;
                MiUseMaximumSystemSpaceEnd = 0xC0000000;
            }

	        MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
            MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);

            //
            // Moving to 3GB means moving session space to just above
            // the system cache (and lowering the system cache max size
            // accordingly).  Here's the visual:
            //
            //                 +------------------------------------+
            //        C1000000 | System cache resides here          |
            //                 | and grows upward.                  |
            //                 |               |                    |
            //                 |               |                    |
            //                 |              \/                    |
            //                 |                                    |
            //                 +------------------------------------+
	        //		       | Session space (Hydra).		    |
            //                 +------------------------------------+
	        //		       | Systemwide global mapped views.    |
            //                 +------------------------------------+
            //                 |                                    |
            //                 |               ^                    |
            //                 |               |                    |
            //                 |               |                    |
            //                 |                                    |
            //                 | Kernel, HAL & boot loaded images   |
            //                 | grow downward from E1000000.       |
            //                 | Total size is specified by         |
            //                 | LoaderBlock->u.I386.BootImageSize. |
            //                 | Note only ntldrs after Build 2195  |
            //                 | are capable of loading the boot    |
            //                 | images in descending order from    |
            //                 | a hardcoded E1000000 on down.      |
            //        E1000000 +------------------------------------+
            //

            MaximumSystemCacheSize -= MmBootImageSize >> PAGE_SHIFT;

            MaximumSystemCacheSize -= MmSessionSize >> PAGE_SHIFT;

            MaximumSystemCacheSize -= MmSystemViewSize >> PAGE_SHIFT;

            MmSessionBase = (ULONG_PTR)(MM_SYSTEM_CACHE_START +
                                  (MaximumSystemCacheSize << PAGE_SHIFT));

            MiSystemViewStart = MmSessionBase + MmSessionSize;

            MiSessionPoolStart = MmSessionBase;
            MiSessionPoolEnd = MiSessionPoolStart + MmSessionPoolSize;
            MiSessionViewStart = MiSessionPoolEnd;

            MiSessionSpaceEnd = (ULONG_PTR)MmSessionBase + MmSessionSize;
            MiSessionSpacePageTables = MmSessionSize / MM_VA_MAPPED_BY_PDE;

            MiSessionImageEnd = MiSessionSpaceEnd;
            MiSessionImageStart = MiSessionImageEnd - MmSessionImageSize;
	}
	else if ((UserVaLimit >= 64) && (UserVaLimit < 2048)) {

	    //
	    // Convert input MB to bytes.
	    //

	    UserVaLimit *= (1024*1024);
	    ReductionInBytes = 0x80000000 - UserVaLimit;

	    MmHighestUserAddress = ((PCHAR)MmHighestUserAddress - ReductionInBytes);
	    MmSystemRangeStart = ((PCHAR)MmSystemRangeStart - ReductionInBytes);
	    MmUserProbeAddress -= ReductionInBytes;
	    MiMaximumWorkingSet -= ReductionInBytes >> PAGE_SHIFT;

	    MiUseMaximumSystemSpace = (ULONG_PTR)MmSystemRangeStart;
	    MiUseMaximumSystemSpaceEnd = (ULONG_PTR)MiUseMaximumSystemSpace + ReductionInBytes;

	    MmHighSectionBase = (PVOID)((PCHAR)MmHighSectionBase - ReductionInBytes);

	    MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
	    MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);
	}

#else

#if !defined (_WIN64)
        MaximumSystemCacheSize -= (MmSystemViewSize >> PAGE_SHIFT);
#endif

#endif

        //
        // Initialize some global session variables.
        //

        MmSessionSpace = (PMM_SESSION_SPACE)((ULONG_PTR)MmSessionBase + MmSessionSize - MmSessionImageSize - MI_SESSION_SPACE_STRUCT_SIZE);

        MiSessionImagePteStart = MiGetPteAddress ((PVOID) MiSessionImageStart);
        MiSessionImagePteEnd = MiGetPteAddress ((PVOID) MiSessionImageEnd);

        MiSessionBasePte = MiGetPteAddress ((PVOID)MmSessionBase);

        MiSessionSpaceWs = MiSessionViewStart + MmSessionViewSize;

        MiSessionLastPte = MiGetPteAddress ((PVOID)MiSessionSpaceEnd);

#if DBG
        //
        // A few sanity checks to ensure things are as they should be.
        //

        if ((sizeof(CONTROL_AREA) % 8) != 0) {
            DbgPrint("control area list is not a quadword sized structure\n");
        }

        if ((sizeof(SUBSECTION) % 8) != 0) {
            DbgPrint("subsection list is not a quadword sized structure\n");
        }

        //
        // Some checks to make sure prototype PTEs can be placed in
        // either paged or nonpaged (prototype PTEs for paged pool are here)
        // can be put into PTE format.
        //

        PointerPte = (PMMPTE)MmPagedPoolStart;
        Pointer.u.Long = MiProtoAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = MiPteToProto(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map start of paged pool as prototype PTE %p %p\n",
                     PointerPde,
                     PointerPte);
        }

        PointerPte =
                (PMMPTE)((ULONG_PTR)MM_NONPAGED_POOL_END & ~((1 << PTE_SHIFT) - 1));

        Pointer.u.Long = MiProtoAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = MiPteToProto(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map end of nonpaged pool as prototype PTE %p %p\n",
                     PointerPde,
                     PointerPte);
        }

        PointerPte = (PMMPTE)(((ULONG_PTR)NON_PAGED_SYSTEM_END -
                        0x37000 + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1));

        for (j = 0; j < 20; j += 1) {
            Pointer.u.Long = MiProtoAddressForPte (PointerPte);
            TempPte = Pointer;
            PointerPde = MiPteToProto(&TempPte);
            if (PointerPte != PointerPde) {
                DbgPrint("unable to map end of nonpaged pool as prototype PTE %p %p\n",
                         PointerPde,
                         PointerPte);
            }

            PointerPte += 1;
        }

        PointerPte = (PMMPTE)(((ULONG_PTR)MM_NONPAGED_POOL_END - 0x133448) & ~(ULONG_PTR)7);
        Pointer.u.Long = MiGetSubsectionAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = (PMMPTE)MiGetSubsectionAddress(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map end of nonpaged pool as section PTE %p %p\n",
                     PointerPde,
                     PointerPte);

            MiFormatPte(&TempPte);
        }

        //
        // End of sanity checks.
        //

#endif

        if (MmEnforceWriteProtection) {
            MiPteStr[0] = (CHAR)1;
        }

        InitializeListHead (&MmLoadedUserImageList);
        InitializeListHead (&MmLockConflictList);
        InitializeListHead (&MmProtectedPteList);

        KeInitializeSpinLock (&MmProtectedPteLock);

        MmCriticalSectionTimeout.QuadPart = Int32x32To64(
                                                 MmCritsectTimeoutSeconds,
                                                -10000000);

        //
        // Initialize System Address Space creation mutex.
        //

        ExInitializeFastMutex (&MmSectionCommitMutex);
        ExInitializeFastMutex (&MmSectionBasedMutex);
        ExInitializeFastMutex (&MmDynamicMemoryMutex);

        KeInitializeMutant (&MmSystemLoadLock, FALSE);

        KeInitializeEvent (&MmAvailablePagesEvent, NotificationEvent, TRUE);
        KeInitializeEvent (&MmAvailablePagesEventMedium, NotificationEvent, TRUE);
        KeInitializeEvent (&MmAvailablePagesEventHigh, NotificationEvent, TRUE);
        KeInitializeEvent (&MmMappedFileIoComplete, NotificationEvent, FALSE);

        KeInitializeEvent (&MmZeroingPageEvent, SynchronizationEvent, FALSE);
        KeInitializeEvent (&MmCollidedFlushEvent, NotificationEvent, FALSE);
        KeInitializeEvent (&MmCollidedLockEvent, NotificationEvent, FALSE);
        KeInitializeEvent (&MiMappedPagesTooOldEvent, NotificationEvent, FALSE);

        KeInitializeDpc (&MiModifiedPageWriterTimerDpc,
                         MiModifiedPageWriterTimerDispatch,
                         NULL);

        KeInitializeTimerEx (&MiModifiedPageWriterTimer, SynchronizationTimer);

        MiModifiedPageLife.QuadPart = Int32x32To64(
                                                 MmModifiedPageLifeInSeconds,
                                                -10000000);

        InitializeListHead (&MmWorkingSetExpansionHead.ListHead);

        InitializeSListHead (&MmDeadStackSListHead);

        InitializeSListHead (&MmEventCountSListHead);

        InitializeSListHead (&MmInPageSupportSListHead);

        MmZeroingPageThreadActive = FALSE;

        MiMemoryLicense (LoaderBlock);

        //
        // include all memory types ...
        //

        for (i = 0; i < LoaderMaximum; i += 1) {
            IncludeType[i] = TRUE;
        }

        //
        // ... expect these..
        //

        IncludeType[LoaderBad] = FALSE;
        IncludeType[LoaderFirmwarePermanent] = FALSE;
        IncludeType[LoaderSpecialMemory] = FALSE;
        IncludeType[LoaderBBTMemory] = FALSE;

        //
        // Compute number of pages in the system.
        //

        NumberOfPages = MiPagesInLoaderBlock (LoaderBlock, IncludeType);

#if defined (_MI_MORE_THAN_4GB_)
        Mm64BitPhysicalAddress = TRUE;
#endif

        //
        // When safebooting, don't enable special pool, the verifier or any
        // other options that track corruption regardless of registry settings.
        //

        if (strstr(LoaderBlock->LoadOptions, SAFEBOOT_LOAD_OPTION_A)) {
            MmVerifyDriverBufferLength = (ULONG)-1;
            MiVerifyAllDrivers = 0;
            MmVerifyDriverLevel = 0;
            MmDontVerifyRandomDrivers = TRUE;
            MmSpecialPoolTag = (ULONG)-1;
            MmSnapUnloads = FALSE;
            MmProtectFreedNonPagedPool = FALSE;
            MmEnforceWriteProtection = 0;
            MmTrackLockedPages = FALSE;
            MmTrackPtes = 0;

#if defined (_WIN64)
            MmSessionSpecialPoolEnd = NULL;
            MmSessionSpecialPoolStart = NULL;
#endif
            SharedUserData->SafeBootMode = TRUE;
        }
        else {
            MiTriageSystem (LoaderBlock);
        }

        SystemPteMultiplier = 0;

        if (MmNumberOfSystemPtes == 0) {
#if defined (_WIN64)

            //
            // 64-bit NT is not constrained by virtual address space.  No
            // tradeoffs between nonpaged pool, paged pool and system PTEs
            // need to be made.  So just allocate PTEs on a linear scale as
            // a function of the amount of RAM.
            //
            // For example on a Hydra NT64, 4gb of RAM gets 128gb of PTEs.
            // The page table cost is the inversion of the multiplier based
            // on the PTE_PER_PAGE.
            //

            if (ExpMultiUserTS == TRUE) {
                SystemPteMultiplier = 32;
            }
            else {
                SystemPteMultiplier = 16;
            }
            if (NumberOfPages < 0x8000) {
                SystemPteMultiplier >>= 1;
            }
#else
            if (NumberOfPages < MM_MEDIUM_SYSTEM) {
                MmNumberOfSystemPtes = MM_MINIMUM_SYSTEM_PTES;
            }
            else {
                MmNumberOfSystemPtes = MM_DEFAULT_SYSTEM_PTES;
                if (NumberOfPages > 8192) {
                    MmNumberOfSystemPtes += MmNumberOfSystemPtes;

                    //
                    // Any reasonable Hydra machine gets the maximum.
                    //

                    if (ExpMultiUserTS == TRUE) {
                        MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
                    }
                }
            }
#endif
        }
        else if (MmNumberOfSystemPtes == (ULONG)-1) {

            //
            // This registry setting indicates the maximum number of
            // system PTEs possible for this machine must be allocated.
            // Snap this for later reference.
            //

            MiRequestedSystemPtes = MmNumberOfSystemPtes;

#if defined (_WIN64)
            SystemPteMultiplier = 256;
#else
            MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
#endif
        }

        if (SystemPteMultiplier != 0) {
            if (NumberOfPages * SystemPteMultiplier > MM_MAXIMUM_SYSTEM_PTES) {
                MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
            }
            else {
                MmNumberOfSystemPtes = (ULONG)(NumberOfPages * SystemPteMultiplier);
            }
        }

        if (MmNumberOfSystemPtes > MM_MAXIMUM_SYSTEM_PTES)  {
            MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
        }

        if (MmNumberOfSystemPtes < MM_MINIMUM_SYSTEM_PTES) {
            MmNumberOfSystemPtes = MM_MINIMUM_SYSTEM_PTES;
        }

        if (MmHeapSegmentReserve == 0) {
            MmHeapSegmentReserve = 1024 * 1024;
        }

        if (MmHeapSegmentCommit == 0) {
            MmHeapSegmentCommit = PAGE_SIZE * 2;
        }

        if (MmHeapDeCommitTotalFreeThreshold == 0) {
            MmHeapDeCommitTotalFreeThreshold = 64 * 1024;
        }

        if (MmHeapDeCommitFreeBlockThreshold == 0) {
            MmHeapDeCommitFreeBlockThreshold = PAGE_SIZE;
        }

#ifndef NO_POOL_CHECKS
        MiInitializeSpecialPoolCriteria ();
#endif

        //
        // If the registry indicates drivers are in the suspect list,
        // extra system PTEs need to be allocated to support special pool
        // for their allocations.
        //

        if ((MmVerifyDriverBufferLength != (ULONG)-1) ||
            ((MmSpecialPoolTag != 0) && (MmSpecialPoolTag != (ULONG)-1))) {
            MmNumberOfSystemPtes += MM_SPECIAL_POOL_PTES;
        }

        MmNumberOfSystemPtes += BBTPagesToReserve;

#if defined(_X86_)

        //
        // The allocation preference key must be carefully managed.  This is
        // because doing every allocation top-down can caused failures if
        // an ntdll process startup allocation (like the stack trace database)
        // gets a high address which then causes a subsequent system DLL rebase
        // collision.
        //
        // This is circumvented as follows:
        //
        // 1.  For 32-bit machines, the allocation preference key is only
        //     useful when booted /3GB as only then can this key help track
        //     down apps with high virtual address bit sign extension problems.
        //     In 3GB mode, the system DLLs are based just below 2GB so ntdll
        //     would have to allocate more than 1GB of VA space before this
        //     becomes a problem.  So really the problem can only occur for
        //     machines in 2GB mode and since the key doesn't help these
        //     machines anyway, just turn it off in these cases.
        //
        // 2.  For 64-bit machines, there is plenty of VA space above the
        //     addresses system DLLs are based at so it is a non-issue.
        //     EXCEPT for wow64 binaries which run in sandboxed 2GB address
        //     spaces.  Explicit checks are made to detect a wow64 process in
        //     the Mm APIs which check this key and the key is ignored in
        //     this case as it doesn't provide any sign extension help and
        //     therefore we don't allow it to burn up any valuable VA space
	//     which could cause a collision.
	//

        if (MmVirtualBias == 0) {
            MmAllocationPreference = 0;
        }
#endif

        if (MmAllocationPreference != 0) {
            MmAllocationPreference = MEM_TOP_DOWN;
        }

        ExInitializeResourceLite (&MmSystemWsLock);

        MiInitializeDriverVerifierList (LoaderBlock);

        //
        // Set the initial commit page limit high enough so initial pool
        // allocations (which happen in the machine dependent init) can
        // succeed.
        //

        MmTotalCommitLimit = _2gb / PAGE_SIZE;
        MmTotalCommitLimitMaximum = MmTotalCommitLimit;

        //
        // Pick a reasonable size for the default prototype PTE allocation
        // chunk size.  Make sure it's always a PAGE_SIZE multiple.  The
        // registry entry is treated as the number of 1K chunks.
        //

        if (MmAllocationFragment == 0) {
            AutosizingFragment = TRUE;
            MmAllocationFragment = MM_ALLOCATION_FRAGMENT;
#if !defined (_WIN64)
            if (NumberOfPages < 64 * 1024) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT / 4;
            }
            else if (NumberOfPages < 256 * 1024) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT / 2;
            }
#endif
        }
	else {

	    //
            // Convert the registry entry from 1K chunks into bytes.
            // Then round it to a PAGE_SIZE multiple.  Finally bound it
            // reasonably.
            //

            AutosizingFragment = FALSE;
            MmAllocationFragment *= 1024;
            MmAllocationFragment = ROUND_TO_PAGES (MmAllocationFragment);

            if (MmAllocationFragment > MM_ALLOCATION_FRAGMENT) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT;
            }
            else if (MmAllocationFragment < PAGE_SIZE) {
                MmAllocationFragment = PAGE_SIZE;
            }
        }

        MiInitializeIoTrackers ();

        MiInitializeCacheOverrides ();

        //
        // Initialize the machine dependent portion of the hardware.
        //

        MiInitMachineDependent (LoaderBlock);

        MmPhysicalMemoryBlock = MmInitializeMemoryLimits (LoaderBlock,
                                                          IncludeType,
                                                          NULL);

        if (MmPhysicalMemoryBlock == NULL) {
            KeBugCheckEx (INSTALL_MORE_MEMORY,
                          MmNumberOfPhysicalPages,
                          MmLowestPhysicalPage,
                          MmHighestPhysicalPage,
                          0x100);
        }

#if defined(_X86_)
        MiReportPhysicalMemory ();
#endif

#if defined (_MI_MORE_THAN_4GB_)
        if (MiNoLowMemory != 0) {
            MiRemoveLowPages (0);
        }
#endif

        //
        // Initialize listhead, spinlock and semaphore for
        // segment dereferencing thread.
        //

        KeInitializeSpinLock (&MmDereferenceSegmentHeader.Lock);
        InitializeListHead (&MmDereferenceSegmentHeader.ListHead);
        KeInitializeSemaphore (&MmDereferenceSegmentHeader.Semaphore, 0, MAXLONG);

        InitializeListHead (&MmUnusedSegmentList);
        InitializeListHead (&MmUnusedSubsectionList);
        KeInitializeEvent (&MmUnusedSegmentCleanup, NotificationEvent, FALSE);


        MiInitializeCommitment ();

        MiInitializePfnTracing ();

#if defined(_X86_)

        //
        // Virtual bias indicates the offset that needs to be added to
        // 0x80000000 to get to the start of the loaded images.  Update it
        // now to indicate the offset to MmSessionBase as that is the lowest
        // system address that process creation needs to make sure to duplicate.
        //
        // This is not done until after machine dependent initialization runs
        // as that initialization relies on the original meaning of VirtualBias.
	//
	// Note if the system is booted with both /3GB & /USERVA, then system
        // PTEs will be allocated below virtual 3GB and that will end up being
        // the lowest system address the process creation needs to duplicate.
        //

        if (MmVirtualBias != 0) {
            MmVirtualBias = (ULONG_PTR)MmSessionBase - CODE_START;
        }
#endif

        if (MmMirroring & MM_MIRRORING_ENABLED) {

#if defined (_WIN64)

            //
            // All page frame numbers must fit in 32 bits because the bitmap
            // package is currently 32-bit.
            //
            // The bitmaps are deliberately not initialized as each mirroring
            // must reinitialize them anyway.
            //

            if (MmHighestPossiblePhysicalPage + 1 < _4gb) {
#endif

                MiCreateBitMap (&MiMirrorBitMap,
                                MmHighestPossiblePhysicalPage + 1,
                                NonPagedPool);

                if (MiMirrorBitMap != NULL) {
                    MiCreateBitMap (&MiMirrorBitMap2,
                                    MmHighestPossiblePhysicalPage + 1,
                                    NonPagedPool);

                    if (MiMirrorBitMap2 == NULL) {
                        MiRemoveBitMap (&MiMirrorBitMap);
                    }
                }
#if defined (_WIN64)
            }
#endif
        }

#if !defined (_WIN64)
        if ((AutosizingFragment == TRUE) &&
            (NumberOfPages >= 256 * 1024)) {

            //
            // This is a system with at least 1GB of RAM.  Presumably it
            // will be used to cache many files.  Maybe we should factor in
            // pool size here and adjust it accordingly.
            //

            MmAllocationFragment;
        }
#endif

        MiReloadBootLoadedDrivers (LoaderBlock);

#if defined (_MI_MORE_THAN_4GB_)
        if (MiNoLowMemory != 0) {
            MiRemoveLowPages (1);
        }
#endif
        MiInitializeVerifyingComponents (LoaderBlock);

        //
        // Setup the system size as small, medium, or large depending
        // on memory available.
        //
        // For internal MM tuning, the following applies
        //
        // 12Mb  is small
        // 12-19 is medium
        // > 19 is large
        //
        //
        // For all other external tuning,
        // < 19 is small
        // 19 - 31 is medium for workstation
        // 19 - 63 is medium for server
        // >= 32 is large for workstation
        // >= 64 is large for server
        //

        if (MmNumberOfPhysicalPages <= MM_SMALL_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 0;
            MmModifiedPageMinimum = 40;
            MmModifiedPageMaximum = 100;
            MmDataClusterSize = 0;
            MmCodeClusterSize = 1;
            MmReadClusterSize = 2;
            MmInPageSupportMinimum = 2;
        }
        else if (MmNumberOfPhysicalPages <= MM_MEDIUM_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 2;
            MmModifiedPageMinimum = 80;
            MmModifiedPageMaximum = 150;
            MmSystemCacheWsMinimum += 100;
            MmSystemCacheWsMaximum += 150;
            MmDataClusterSize = 1;
            MmCodeClusterSize = 2;
            MmReadClusterSize = 4;
            MmInPageSupportMinimum = 3;
        }
        else {
            MmSystemSize = MmMediumSystem;
            MmMaximumDeadKernelStacks = 5;
            MmModifiedPageMinimum = 150;
            MmModifiedPageMaximum = 300;
            MmSystemCacheWsMinimum += 400;
            MmSystemCacheWsMaximum += 800;
            MmDataClusterSize = 3;
            MmCodeClusterSize = 7;
            MmReadClusterSize = 7;
            MmInPageSupportMinimum = 4;
        }

        if (MmNumberOfPhysicalPages < ((24*1024*1024)/PAGE_SIZE)) {
            MmSystemCacheWsMinimum = 32;
        }

        if (MmNumberOfPhysicalPages >= ((32*1024*1024)/PAGE_SIZE)) {

            //
            // If we are on a workstation, 32Mb and above are considered
            // large systems.
            //

            if (MmProductType == 0x00690057) {
                MmSystemSize = MmLargeSystem;

            }
            else {

                //
                // For servers, 64Mb and greater is a large system
                //

                if (MmNumberOfPhysicalPages >= ((64*1024*1024)/PAGE_SIZE)) {
                    MmSystemSize = MmLargeSystem;
                }
            }
        }

        if (MmNumberOfPhysicalPages > ((33*1024*1024)/PAGE_SIZE)) {
            MmModifiedPageMinimum = 400;
            MmModifiedPageMaximum = 800;
            MmSystemCacheWsMinimum += 500;
            MmSystemCacheWsMaximum += 900;
            MmInPageSupportMinimum += 4;
        }
        if (MmNumberOfPhysicalPages > ((220*1024*1024)/PAGE_SIZE)) {  // bump max cache size a bit more
            if ( (LONG)MmSystemCacheWsMinimum < (LONG)((24*1024*1024) >> PAGE_SHIFT)  &&
                 (LONG)MmSystemCacheWsMaximum < (LONG)((24*1024*1024) >> PAGE_SHIFT)){
                MmSystemCacheWsMaximum =  ((24*1024*1024) >> PAGE_SHIFT);
            }
            ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
        } 
        else if (MmNumberOfPhysicalPages > ((110*1024*1024)/PAGE_SIZE)) {  // bump max cache size a bit
            if ( (LONG)MmSystemCacheWsMinimum < (LONG)((16*1024*1024) >> PAGE_SHIFT)  &&
                 (LONG)MmSystemCacheWsMaximum < (LONG)((16*1024*1024) >> PAGE_SHIFT)){
                MmSystemCacheWsMaximum =  ((16*1024*1024) >> PAGE_SHIFT);
            }
            ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
        }

        if (NT_SUCCESS (MmIsVerifierEnabled (&VerifierFlags))) {

            //
            // The verifier is enabled so don't defer any MDL unlocks because
            // without state, debugging driver bugs in this area is very
            // difficult.
            //

            DeferredMdlEntries = 0;
        }
        else if (MmNumberOfPhysicalPages > ((255*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 32;
        }
        else if (MmNumberOfPhysicalPages > ((127*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 8;
        }
        else {
            DeferredMdlEntries = 4;
        }

#if defined(MI_MULTINODE)
        for (i = 0; i < KeNumberNodes; i += 1) {

            InitializeSListHead (&KeNodeBlock[i]->PfnDereferenceSListHead);
            KeNodeBlock[i]->PfnDeferredList = NULL;

            for (j = 0; j < DeferredMdlEntries; j += 1) {

                SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
                if (SingleListEntry != NULL) {
                    InterlockedPushEntrySList (&KeNodeBlock[i]->PfnDereferenceSListHead,
                                               SingleListEntry);
                }
            }
        }
#else
        InitializeSListHead (&MmPfnDereferenceSListHead);

        for (j = 0; j < DeferredMdlEntries; j += 1) {
            SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
            if (SingleListEntry != NULL) {
                InterlockedPushEntrySList (&MmPfnDereferenceSListHead,
                                           SingleListEntry);
            }
        }
#endif
        
        ASSERT (SharedUserData->NumberOfPhysicalPages == 0);

        SharedUserData->NumberOfPhysicalPages = (ULONG) MmNumberOfPhysicalPages;

        //
        // Determine if we are on an AS system (Winnt is not AS).
        //

        if (MmProductType == 0x00690057) {
            SharedUserData->NtProductType = NtProductWinNt;
            MmProductType = 0;
            MmThrottleTop = 250;
            MmThrottleBottom = 30;

        }
        else {
            if (MmProductType == 0x0061004c) {
                SharedUserData->NtProductType = NtProductLanManNt;
            }
            else {
                SharedUserData->NtProductType = NtProductServer;
            }

            MmProductType = 1;
            MmThrottleTop = 450;
            MmThrottleBottom = 80;
            MmMinimumFreePages = 81;
            MmInPageSupportMinimum += 8;
        }

        MiAdjustWorkingSetManagerParameters ((LOGICAL)(MmProductType == 0 ? TRUE : FALSE));

        //
        // Set the ResidentAvailablePages to the number of available
        // pages minus the fluid value.
        //

        MmResidentAvailablePages = MmAvailablePages - MM_FLUID_PHYSICAL_PAGES;

        //
        // Subtract off the size of future nonpaged pool expansion
        // so that nonpaged pool will always be able to expand regardless of
        // prior system load activity.
        //

        MmResidentAvailablePages -= MiExpansionPoolPagesInitialCharge;

        //
        // Subtract off the size of the system cache working set.
        //

        MmResidentAvailablePages -= MmSystemCacheWsMinimum;
        MmResidentAvailableAtInit = MmResidentAvailablePages;

        if (MmResidentAvailablePages < 0) {
#if DBG
            DbgPrint("system cache working set too big\n");
#endif
            return FALSE;
        }

        //
        // Initialize spin lock for allowing working set expansion.
        //

        KeInitializeSpinLock (&MmExpansionLock);

        ExInitializeFastMutex (&MmPageFileCreationLock);

        //
        // Initialize resource for extending sections.
        //

        ExInitializeResourceLite (&MmSectionExtendResource);
        ExInitializeResourceLite (&MmSectionExtendSetResource);

        //
        // Build the system cache structures.
        //

        StartPde = MiGetPdeAddress (MmSystemCacheWorkingSetList);
        PointerPte = MiGetPteAddress (MmSystemCacheWorkingSetList);

#if (_MI_PAGING_LEVELS >= 3)

        TempPte = ValidKernelPte;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPdeAddress(StartPde);

        if (StartPxe->u.Hard.Valid == 0) {

            //
            // Map in a page directory parent page for the system cache working
            // set.  Note that we only populate one page table for this.
            //

            DirectoryFrameIndex = MiRemoveAnyPage(
                MI_GET_PAGE_COLOR_FROM_PTE (StartPxe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
            *StartPxe = TempPte;

            MiInitializePfn (DirectoryFrameIndex, StartPxe, 1);

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte(StartPxe),
                             PAGE_SIZE,
                             ZeroKernelPte.u.Long);
        }
#endif

        StartPpe = MiGetPteAddress(StartPde);

        if (StartPpe->u.Hard.Valid == 0) {

            //
            // Map in a page directory page for the system cache working set.
            // Note that we only populate one page table for this.
            //

            DirectoryFrameIndex = MiRemoveAnyPage(
                MI_GET_PAGE_COLOR_FROM_PTE (StartPpe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
            *StartPpe = TempPte;

            MiInitializePfn (DirectoryFrameIndex, StartPpe, 1);

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte(StartPpe),
                             PAGE_SIZE,
                             ZeroKernelPte.u.Long);
        }

#if (_MI_PAGING_LEVELS >= 4)

        //
        // The shared user data is already initialized and it shares the
        // page table page with the system cache working set list.
        //

        ASSERT (StartPde->u.Hard.Valid == 1);
#else

        //
        // Map in a page table page.
        //

        ASSERT (StartPde->u.Hard.Valid == 0);

        PageFrameIndex = MiRemoveAnyPage(
                                MI_GET_PAGE_COLOR_FROM_PTE (StartPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (StartPde, TempPte);

        MiInitializePfn (PageFrameIndex, StartPde, 1);

        MiFillMemoryPte (MiGetVirtualAddressMappedByPte (StartPde),
                         PAGE_SIZE,
                         ZeroKernelPte.u.Long);
#endif

        StartPpe = MiGetPpeAddress(MmSystemCacheStart);
        StartPde = MiGetPdeAddress(MmSystemCacheStart);
        PointerPte = MiGetVirtualAddressMappedByPte (StartPde);

#else
#if !defined(_X86PAE_)
        ASSERT ((StartPde + 1) == MiGetPdeAddress (MmSystemCacheStart));
#endif
#endif

        MaximumSystemCacheSizeTotal = MaximumSystemCacheSize;

#if defined(_X86_)
        MaximumSystemCacheSizeTotal += MiMaximumSystemCacheSizeExtra;
#endif

        //
        // Size the system cache based on the amount of physical memory.
        //

        i = (MmNumberOfPhysicalPages + 65) / 1024;

        if (i >= 4) {

            //
            // System has at least 4032 pages.  Make the system
            // cache 128mb + 64mb for each additional 1024 pages.
            //

            MmSizeOfSystemCacheInPages = (PFN_COUNT)(
                            ((128*1024*1024) >> PAGE_SHIFT) +
                            ((i - 4) * ((64*1024*1024) >> PAGE_SHIFT)));
            if (MmSizeOfSystemCacheInPages > MaximumSystemCacheSizeTotal) {
                MmSizeOfSystemCacheInPages = MaximumSystemCacheSizeTotal;
            }
        }

        MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                    MmSizeOfSystemCacheInPages * PAGE_SIZE) - 1);

#if defined(_X86_)
        if (MmSizeOfSystemCacheInPages > MaximumSystemCacheSize) {
            ASSERT (MiMaximumSystemCacheSizeExtra != 0);
            MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                        MaximumSystemCacheSize * PAGE_SIZE) - 1);

            MiSystemCacheStartExtra = (PVOID)MiExtraResourceStart;
            MiSystemCacheEndExtra = (PVOID)(((PCHAR)MiSystemCacheStartExtra +
                        (MmSizeOfSystemCacheInPages - MaximumSystemCacheSize) * PAGE_SIZE) - 1);
        }
        else {
            MiSystemCacheStartExtra = MmSystemCacheStart;
            MiSystemCacheEndExtra = MmSystemCacheEnd;
        }
#endif

        EndPde = MiGetPdeAddress(MmSystemCacheEnd);

        TempPte = ValidKernelPte;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPxeAddress(MmSystemCacheStart);
        if (StartPxe->u.Hard.Valid == 0) {
            FirstPxe = TRUE;
            FirstPpe = TRUE;
        }
        else {
            FirstPxe = FALSE;
            FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
        }
#elif (_MI_PAGING_LEVELS >= 3)
        FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
#else
        DirectoryFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PDE_BASE));
#endif

        LOCK_PFN (OldIrql);
        while (StartPde <= EndPde) {

#if (_MI_PAGING_LEVELS >= 4)
            if (FirstPxe == TRUE || MiIsPteOnPpeBoundary(StartPde)) {
                FirstPxe = FALSE;
                StartPxe = MiGetPdeAddress(StartPde);

                //
                // Map in a page directory page.
                //

                DirectoryFrameIndex = MiRemoveAnyPage(
                                        MI_GET_PAGE_COLOR_FROM_PTE (StartPxe));
                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
                MI_WRITE_VALID_PTE (StartPxe, TempPte);

                MiInitializePfn (DirectoryFrameIndex,
                                 StartPxe,
                                 1);

                MiFillMemoryPte (MiGetVirtualAddressMappedByPte(StartPxe),
                                 PAGE_SIZE,
                                 ZeroKernelPte.u.Long);
            }
#endif

#if (_MI_PAGING_LEVELS >= 3)
            if (FirstPpe == TRUE || MiIsPteOnPdeBoundary(StartPde)) {
                FirstPpe = FALSE;
                StartPpe = MiGetPteAddress(StartPde);

                //
                // Map in a page directory page.
                //

                DirectoryFrameIndex = MiRemoveAnyPage(
                                        MI_GET_PAGE_COLOR_FROM_PTE (StartPpe));
                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
                MI_WRITE_VALID_PTE (StartPpe, TempPte);

                MiInitializePfn (DirectoryFrameIndex,
                                 StartPpe,
  