L)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ((pinst)?pinst->m_lVerPagesMax:g_lVerPagesMax) * (cbBucket / 16384);
		break;

	case JET_paramGlobalMinVerPages:	/* Global minimum number of modified pages for all instances */
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		if ( NULL != pinst )
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = g_lVerPagesMin * (cbBucket / 16384);
		break;

	case JET_paramPreferredVerPages:     /* Preferred number of modified pages */
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ((pinst)?pinst->m_lVerPagesPreferredMax:g_lVerPagesPreferredMax) * (cbBucket / 16384);
		break;

	case JET_paramMaxCursors:      /* maximum number of open cursors */
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = (pinst)?pinst->m_lCursorsMax:g_lCursorsMax;
		break;

	case JET_paramLogBuffers:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = (pinst)?pinst->m_lLogBuffers:g_lLogBuffers;
		break;

	case JET_paramLogFileSize:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = (pinst)?pinst->m_lLogFileSize:g_lLogFileSize;
		break;

	case JET_paramLogFilePath:     /* Path to the log file directory */
			{
			char *szFrom;
			if (pinst)
				{
				Assert(pinst->m_plog);
				szFrom = pinst->m_plog->m_szLogFilePath;				
				}
			else
				{
				szFrom = g_szLogFilePath;
				}
				
			Assert(szFrom);
			cch = (ULONG)strlen(szFrom) + 1;
			if ( cch > (int)cbMax )
				cch = (int)cbMax;
			UtilMemCpy( sz,  szFrom, cch  );
			sz[cch-1] = '\0';
			}
		break;

	case JET_paramRecovery:
			{
			char *szFrom;
			if (pinst)
				{
				Assert(pinst->m_plog);
				szFrom = pinst->m_plog->m_szRecovery;				
				}
			else
				{
				szFrom = g_szRecovery;
				}
			Assert(szFrom);
			
			cch = (ULONG)strlen(szFrom) + 1;
			if ( cch > (int)cbMax )
				cch = (int)cbMax;
			UtilMemCpy( sz,  szFrom, cch  );
			sz[cch-1] = '\0';
			}
		break;

	case JET_paramEventLogCache:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = g_cbEventHeapMax;
		break;

	case JET_paramExceptionAction:
#ifdef CATCH_EXCEPTIONS
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( g_fCatchExceptions ) ? JET_ExceptionMsgBox : JET_ExceptionNone;
#else  //  !CATCH_EXCEPTIONS
		return ErrERRCheck( JET_errInvalidParameter );
#endif	//	CATCH_EXCEPTIONS
		break;

#ifdef DEBUG
	case JET_paramTransactionLevel:
		ErrIsamGetTransaction( sesid, plParam );
		break;

	case JET_paramPrintFunction:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
#ifdef _WIN64
		//	UNDONE: Must change lParam to a ULONG_PTR
		*plParam = NULL;
		return ErrERRCheck( JET_wrnNyi );
#else		
		*plParam = (ULONG_PTR)DBGFPrintF;
		break;
#endif

#endif	//	DEBUG

	case JET_paramCommitDefault:
		if ( plParam == NULL )
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_grbitsCommitDefault : g_grbitsCommitDefault );
		break;

	case JET_paramPageFragment:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_lPageFragment : g_lPageFragment );
		break;

	case JET_paramVersionStoreTaskQueueMax:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_pver->m_ulVERTasksPostMax : g_ulVERTasksPostMax );
		break;
		
	case JET_paramDeleteOldLogs:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_plog->m_fDeleteOldLogs : g_fDeleteOldLogs );
		break;

	case JET_paramDeleteOutOfRangeLogs:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_plog->m_fDeleteOutOfRangeLogs : g_fDeleteOutOfRangeLogs );
		break;

	case JET_paramAccessDeniedRetryPeriod:
		//	silently cap retry period at 1 minute
		if ( NULL == plParam )
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = IFileSystemAPI::cmsecAccessDeniedRetryPeriod;
		break;

	case JET_paramEnableOnlineDefrag:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = ( pinst ? pinst->m_fOLDLevel : g_fGlobalOLDLevel );
		break;
		
	case JET_paramCircularLog:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		if ( pinst )
			{
			Assert ( NULL != pinst->m_plog );
			*plParam = pinst->m_plog->m_fLGCircularLogging;
			}
		else
			{
			*plParam = g_fLGCircularLogging;
			}
		break;

#ifdef RFS2
	case JET_paramRFS2AllocsPermitted:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = g_cRFSAlloc;
		break;

	case JET_paramRFS2IOsPermitted:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = g_cRFSIO;
		break;
#endif

	case JET_paramBaseName:
		cch = (ULONG)strlen( pinst ? pinst->m_plog->m_szBaseName : szBaseName ) + 1;
		if (cch > (int)cbMax)
			cch = (int)cbMax;
		UtilMemCpy( sz, ( pinst ? pinst->m_plog->m_szBaseName : szBaseName ), cch );
		sz[cch-1] = '\0';
		break;

	case JET_paramBatchIOBufferMax:
		break;

	case JET_paramCacheSizeMin:
		return ErrBFGetCacheSizeMin( plParam );

	case JET_paramCacheSize:
		return ErrBFGetCacheSize( plParam );

	case JET_paramCacheSizeMax:
		return ErrBFGetCacheSizeMax( plParam );

	case JET_paramCheckpointDepthMax:
		return ErrBFGetCheckpointDepthMax( plParam );

	case JET_paramLRUKCorrInterval:
		return ErrBFGetLRUKCorrInterval( plParam );

	case JET_paramLRUKHistoryMax:
		*plParam = 0;
		break;

	case JET_paramLRUKPolicy:
		return ErrBFGetLRUKPolicy( plParam );

	case JET_paramLRUKTimeout:
		return ErrBFGetLRUKTimeout( plParam );

	case JET_paramLRUKTrxCorrInterval:
		*plParam = 0;
		break;

	case JET_paramOutstandingIOMax:
		break;

	case JET_paramStartFlushThreshold:
		return ErrBFGetStartFlushThreshold( plParam );

	case JET_paramStopFlushThreshold:
		return ErrBFGetStopFlushThreshold( plParam );

	case JET_paramTableClassName:
		memset( sz, 0, cbMax );
		break;

	case JET_paramCacheRequests:
		{
		*plParam = 0;
		return JET_errSuccess;
		}

	case JET_paramCacheHits:
		{
		*plParam = 0;
		return JET_errSuccess;
		}

	case JET_paramEnableIndexChecking:
		*plParam = fGlobalIndexChecking;
		break;

	case JET_paramLogFileFailoverPath:
			{
			char *szFrom;
			if (pinst)
				{
				Assert(pinst->m_plog);
				szFrom = pinst->m_plog->m_szLogFileFailoverPath;				
				}
			else
				{
				szFrom = g_szLogFileFailoverPath;
				}
			Assert(szFrom);
			cch = (ULONG)strlen(szFrom) + 1;
			if ( cch > (int)cbMax )
				cch = (int)cbMax;
			UtilMemCpy( sz,  szFrom, cch  );
			sz[cch-1] = '\0';
			}
		break;
		
	case JET_paramLogFileCreateAsynch:
		if ( !plParam )
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = pinst ? pinst->m_plog->m_fCreateAsynchLogFile : g_fLogFileCreateAsynch;
		break;
		
	case JET_paramEnableImprovedSeekShortcut:
		*plParam = g_fImprovedSeekShortcut;
		break;

	case JET_paramEnableSortedRetrieveColumns:
		*plParam = g_fSortedRetrieveColumns;
		break;

	case JET_paramDatabasePageSize:
		*plParam = g_cbPage;
		break;
		
	case JET_paramEventSource:
			{
			char *szFrom;
			szFrom = (pinst) ?pinst->m_szEventSource:g_szEventSource;
			Assert(szFrom);

			cch = (ULONG)strlen(szFrom) + 1;
			if (cch > (int)cbMax)
				cch = (int)cbMax;
			UtilMemCpy( sz, szFrom, cch );
			sz[cch-1] = '\0';
			}
		break;

	case JET_paramEventSourceKey:
			{
			char *szFrom;
			szFrom = (pinst) ?pinst->m_szEventSourceKey:g_szEventSourceKey;
			Assert(szFrom);
			cch = (ULONG)strlen(szFrom) + 1;
			if (cch > (int)cbMax)
				cch = (int)cbMax;
			UtilMemCpy( sz, szFrom, cch );
			sz[cch-1] = '\0';
			}
		break;

	case JET_paramNoInformationEvent:
		*plParam = ( pinst ? pinst->m_fNoInformationEvent : g_fNoInformationEvent );
		break;

	case JET_paramEventLoggingLevel:
		if (plParam == NULL)
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		*plParam = (pinst)? pinst->m_lEventLoggingLevel : g_lEventLoggingLevel;
		break;

	case JET_paramDisableCallbacks:
		*plParam = g_fCallbacksDisabled;
		break;

	case JET_paramBackupChunkSize:
		*plParam = g_cpgBackupChunk;
		break;

	case JET_paramBackupOutstandingReads:
		*plParam = g_cBackupRead;
		break;

	case JET_paramErrorToString:
		{
		if( NULL == plParam )
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
			
		const ERR err = *((ERR *)plParam);
		const CHAR * const szSep = ", ";

		const CHAR * szError;
		const CHAR * szErrorText;
		JetErrorToString( err, &szError, &szErrorText );

		cch = (ULONG)(strlen( szError ) + strlen( szErrorText ) + strlen( szSep ));
		if (cch > cbMax)
			{
			return ErrERRCheck( JET_errBufferTooSmall );
			}
		sz[0] = 0;
		strcat( sz, szError );
		strcat( sz, szSep );
		strcat( sz, szErrorText );
		break;
		}

	case JET_paramSLVProviderEnable:
		*plParam = (pinst)?pinst->m_fSLVProviderEnabled:g_fSLVProviderEnabled;
		break;

	case JET_paramRuntimeCallback:
		*plParam = (ULONG_PTR)( pinstNil == pinst ? g_pfnRuntimeCallback : pinst->m_pfnRuntimeCallback );
		break;

	case JET_paramUnicodeIndexDefault:
		if ( cbMax < sizeof(JET_UNICODEINDEX) )
			return ErrERRCheck( JET_errBufferTooSmall );

		*(IDXUNICODE *)plParam = idxunicodeDefault;
		break;

	case JET_paramIndexTuplesLengthMin:
		*plParam = g_chIndexTuplesLengthMin;
		break;

	case JET_paramIndexTuplesLengthMax:
		*plParam = g_chIndexTuplesLengthMax;
		break;

	case JET_paramIndexTuplesToIndexMax:
		*plParam = g_chIndexTuplesToIndexMax;
		break;

	case JET_paramPageHintCacheSize:
		*plParam = ULONG_PTR( g_cbPageHintCache );
		break;

	case JET_paramRecordUpgradeDirtyLevel:
		switch( CPAGE::bfdfRecordUpgradeFlags )
			{
			case bfdfClean:
				*plParam = 0;
				break;
			case bfdfUntidy:
				*plParam = 1;
				break;
			case bfdfDirty:
				*plParam = 2;
				break;
			case bfdfFilthy:
				*plParam = 3;
				break;
			default:
				AssertSz( fFalse, "Unknown BFDirtyFlag" );
				return ErrERRCheck( JET_errInternalError );
				break;
			}
		break;

	case JET_paramRecoveryCurrentLogfile:
		if ( pinst )
			{
			*plParam = ( pinst->m_plog->m_plgfilehdr ) ? pinst->m_plog->m_plgfilehdr->lgfilehdr.le_lGeneration : 0;
			}
		else
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		break;

	case JET_paramReplayingReplicatedLogfiles:
		if ( pinst )
			{
			*plParam = pinst->m_plog->m_fReplayingReplicatedLogFiles;
			}
		else
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		break;

	case JET_paramCleanupMismatchedLogFiles:
		if ( !plParam )
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		*plParam = ULONG_PTR( pinst ? pinst->m_fCleanupMismatchedLogFiles : g_fCleanupMismatchedLogFiles );
		break;

	case JET_paramOSSnapshotTimeout:
		return ErrOSSnapshotGetTimeout( plParam );		
		break;

	case JET_paramUnicodeIndexLibrary:
		if ( !sz )
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		{
		CHAR* const szSource = pinst ? pinst->m_szUnicodeIndexLibrary : g_szUnicodeIndexLibrary;

		cch = LOSSTRLengthA( szSource ) + 1;
		cch = min( cch, cbMax );
		memcpy( sz, szSource, cch );
		szSource[ cch - 1 ] = '\0';
		}
		break;

	case JET_paramMaxInstances:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );

		//	WARNING: this value will not match the
		//	current max. instances in effect if
		//	we're running in single-instance mode
		//	(this system param only controls max.
		//	instances for multi-instance mode)
		*plParam = g_cMaxInstancesRequestedByUser;
		break;

	case JET_paramMaxDatabasesPerInstance:
		if (plParam == NULL)
			return ErrERRCheck( JET_errInvalidParameter );
		*plParam = dbidMax;
		break;

	default:
		return ErrERRCheck( JET_errInvalidParameter );
		}

	return JET_errSuccess;
	}


extern "C" {

/***********************************************************************/
/***********************  JET API FUNCTIONS  ***************************/
/***********************************************************************/

/*	APICORE.CPP
 */


/*=================================================================
JetIdle

Description:
  Performs idle time processing.

Parameters:
  sesid			uniquely identifies session
  grbit			processing options

Return Value:
  Error code

Errors/Warnings:
  JET_errSuccess		some idle processing occurred
  JET_wrnNoIdleActivity no idle processing occurred
=================================================================*/

LOCAL JET_ERR JetIdleEx( JET_SESID sesid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opIdle );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamIdle( sesid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetIdle( JET_SESID sesid, JET_GRBIT grbit )
	{
	JET_TRY( JetIdleEx( sesid, grbit ) );
	}


LOCAL JET_ERR JetGetTableIndexInfoEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	void			*pvResult,
	unsigned long	cbResult,
	unsigned long	InfoLevel )
	{
	ERR				err;
	ULONG			cbMin;
	APICALL_SESID	apicall( opGetTableIndexInfo );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	FillClientBuffer( pvResult, cbResult );

	switch( InfoLevel )
		{
		case JET_IdxInfo:
		case JET_IdxInfoList:
			cbMin = sizeof(JET_INDEXLIST) - cbIDXLISTNewMembersSinceOriginalFormat;
			break;
		case JET_IdxInfoSpaceAlloc:
		case JET_IdxInfoCount:
			cbMin = sizeof(ULONG);
			break;
		case JET_IdxInfoLangid:
			cbMin = sizeof(LANGID);
			break;
		case JET_IdxInfoVarSegMac:
			cbMin = sizeof(USHORT);
			break;
		case JET_IdxInfoIndexId:
			cbMin = sizeof(INDEXID);
			break;

		case JET_IdxInfoSysTabCursor:
		case JET_IdxInfoOLC:
		case JET_IdxInfoResetOLC:
			AssertTracking();
			err = ErrERRCheck( JET_errFeatureNotAvailable );
			goto HandleError;
			
		default:
			err = ErrERRCheck( JET_errInvalidParameter );
			goto HandleError;
		}

	if ( cbResult >= cbMin )
		{
		err = ErrDispGetTableIndexInfo(
					sesid,
					tableid,
					szIndexName,
					pvResult,
					cbResult,
					InfoLevel );
		}
	else
		{
		err = ErrERRCheck( JET_errBufferTooSmall );
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetGetTableIndexInfo(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	void			*pvResult,
	unsigned long	cbResult,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetTableIndexInfoEx( sesid, tableid, szIndexName, pvResult, cbResult, InfoLevel ) );
	}


LOCAL JET_ERR JetGetIndexInfoEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const char		*szIndexName,
	void			*pvResult,
	unsigned long	cbResult,
	unsigned long	InfoLevel )
	{
	ERR				err;
	ULONG			cbMin;
	APICALL_SESID	apicall( opGetIndexInfo );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	FillClientBuffer( pvResult, cbResult );

	switch( InfoLevel )
		{
		case JET_IdxInfo:
		case JET_IdxInfoList:
			cbMin = sizeof(JET_INDEXLIST) - cbIDXLISTNewMembersSinceOriginalFormat;
			break;
		case JET_IdxInfoSpaceAlloc:
		case JET_IdxInfoCount:
			cbMin = sizeof(ULONG);
			break;
		case JET_IdxInfoLangid:
			cbMin = sizeof(LANGID);
			break;
		case JET_IdxInfoVarSegMac:
			cbMin = sizeof(USHORT);
			break;
		case JET_IdxInfoIndexId:
			cbMin = sizeof(INDEXID);
			break;

		case JET_IdxInfoSysTabCursor:
		case JET_IdxInfoOLC:
		case JET_IdxInfoResetOLC:
			AssertTracking();
			err = ErrERRCheck( JET_errFeatureNotAvailable );
			goto HandleError;
			
		default :
			err = ErrERRCheck( JET_errInvalidParameter );
			goto HandleError;
		}

	if ( cbResult >= cbMin )
		{
		err = ErrIsamGetIndexInfo(
					sesid,
					(JET_DBID)ifmp,
					szTableName,
					szIndexName,
					pvResult,
					cbResult,
					InfoLevel );
		}
	else
		{
		err = ErrERRCheck( JET_errBufferTooSmall );
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetGetIndexInfo(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const char		*szIndexName,
	void			*pvResult,
	unsigned long	cbResult,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetIndexInfoEx( sesid, ifmp, szTableName, szIndexName, pvResult, cbResult, InfoLevel ) );
	}


LOCAL JET_ERR JetGetObjectInfoEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_OBJTYP		objtyp,
	const char		*szContainerName,
	const char		*szObjectName,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	ERR				err;
	ULONG			cbMin;
	APICALL_SESID	apicall( opGetObjectInfo );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	FillClientBuffer( pvResult, cbMax );

	switch( InfoLevel )
		{
		case JET_ObjInfo:
		case JET_ObjInfoNoStats:
			cbMin = sizeof(JET_OBJECTINFO);
			break;
		case JET_ObjInfoListNoStats:
		case JET_ObjInfoList:
			cbMin = sizeof(JET_OBJECTLIST);
			break;

		case JET_ObjInfoSysTabCursor:
		case JET_ObjInfoSysTabReadOnly:
		case JET_ObjInfoListACM:
		case JET_ObjInfoRulesLoaded:
			AssertTracking();
			err = ErrERRCheck( JET_errFeatureNotAvailable );
			goto HandleError;

		default:
			err = ErrERRCheck( JET_errInvalidParameter );
			goto HandleError;
		}

	if ( cbMax >= cbMin )
		{
		err = ErrIsamGetObjectInfo(
					sesid,
					(JET_DBID)ifmp,
					objtyp,
					szContainerName,
					szObjectName,
					pvResult,
					cbMax,
					InfoLevel );
		}
	else
		{
		err = ErrERRCheck( JET_errBufferTooSmall );
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetGetObjectInfo(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_OBJTYP		objtyp,
	const char		*szContainerName,
	const char		*szObjectName,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetObjectInfoEx( sesid, ifmp, objtyp, szContainerName, szObjectName, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetGetTableInfoEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	APICALL_SESID	apicall( opGetTableInfo );

	if ( apicall.FEnter( sesid ) )
		{
		FillClientBuffer( pvResult, cbMax );
		apicall.LeaveAfterCall( ErrDispGetTableInfo(
										sesid,
										tableid,
										pvResult,
										cbMax,
										InfoLevel ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetTableInfo(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetTableInfoEx( sesid, tableid, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetBeginTransactionEx( JET_SESID sesid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opBeginTransaction );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamBeginTransaction( sesid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetBeginTransaction( JET_SESID sesid )
	{
	JET_TRY( JetBeginTransactionEx( sesid, NO_GRBIT ) );
	}
JET_ERR JET_API JetBeginTransaction2( JET_SESID sesid, JET_GRBIT grbit )
	{
	JET_TRY( JetBeginTransactionEx( sesid, grbit ) );
	}


LOCAL JET_ERR JetPrepareToCommitTransactionEx(
	JET_SESID		sesid,
	const void		* pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opPrepareToCommitTransaction );

	if ( apicall.FEnter( sesid ) )
		{
#ifdef DTC
		//	grbit currently unsupported
		apicall.LeaveAfterCall( 0 != grbit ?
									ErrERRCheck( JET_errInvalidGrbit ) :
									ErrIsamPrepareToCommitTransaction( sesid, pvData, cbData ) );
#else
		apicall.LeaveAfterCall( ErrERRCheck( JET_errFeatureNotAvailable ) );
#endif
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetPrepareToCommitTransaction(
	JET_SESID		sesid,
	const void		* pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetPrepareToCommitTransactionEx( sesid, pvData, cbData, grbit ) );
	}


LOCAL JET_ERR JetCommitTransactionEx( JET_SESID sesid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opCommitTransaction );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( grbit & ~(JET_bitCommitLazyFlush|JET_bitWaitLastLevel0Commit) ?
										ErrERRCheck( JET_errInvalidGrbit ) :
										ErrIsamCommitTransaction( sesid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCommitTransaction( JET_SESID sesid, JET_GRBIT grbit )
	{
	JET_TRY( JetCommitTransactionEx( sesid, grbit ) );
	}


LOCAL JET_ERR JetRollbackEx( JET_SESID sesid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opRollback );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		apicall.LeaveAfterCall( grbit & ~JET_bitRollbackAll ?
										ErrERRCheck( JET_errInvalidGrbit ) :
										ErrIsamRollback( sesid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRollback( JET_SESID sesid, JET_GRBIT grbit )
	{
	JET_TRY( JetRollbackEx( sesid, grbit ) );
	}


LOCAL JET_ERR JetOpenTableEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const void		*pvParameters,
	unsigned long	cbParameters,
	JET_GRBIT		grbit,
	JET_TABLEID		*ptableid )
	{
	APICALL_SESID	apicall( opOpenTable );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamOpenTable(
										sesid,
										(JET_DBID)ifmp,
										ptableid,
										(CHAR *)szTableName,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetOpenTable(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const void		*pvParameters,
	unsigned long	cbParameters,
	JET_GRBIT		grbit,
	JET_TABLEID		*ptableid )
	{
	JET_TRY( JetOpenTableEx( sesid, ifmp, szTableName, pvParameters, cbParameters, grbit, ptableid ) );
	}


//  ================================================================
LOCAL JET_ERR JetSetTableSequentialEx(
	const JET_SESID 	sesid,
	const JET_TABLEID	tableid,
	const JET_GRBIT 	grbit )
//  ================================================================
	{
	APICALL_SESID		apicall( opSetTableSequential );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSetTableSequential( sesid, tableid, grbit ) );
		}

	return apicall.ErrResult();
	}
//  ================================================================
JET_ERR JET_API JetSetTableSequential(
	JET_SESID 	sesid,
	JET_TABLEID	tableid,
	JET_GRBIT	grbit )
//  ================================================================
	{
	JET_TRY( JetSetTableSequentialEx( sesid, tableid, grbit ) );
	}


//  ================================================================
LOCAL JET_ERR JetResetTableSequentialEx(
	JET_SESID	 	sesid,
	JET_TABLEID		tableid,
	JET_GRBIT	 	grbit )
//  ================================================================
	{
	APICALL_SESID	apicall( opResetTableSequential );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispResetTableSequential( sesid, tableid, grbit ) );
		}

	return apicall.ErrResult();
	}
//  ================================================================
JET_ERR JET_API JetResetTableSequential(
	JET_SESID 	sesid,
	JET_TABLEID	tableid,
	JET_GRBIT	grbit )
//  ================================================================
	{
	JET_TRY( JetResetTableSequentialEx( sesid, tableid, grbit ) );
	}

LOCAL JET_ERR JetRegisterCallbackEx(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_CBTYP		cbtyp, 
	JET_CALLBACK	pCallback, 
	VOID *			pvContext,
	JET_HANDLE		*phCallbackId )
	{	
	APICALL_SESID	apicall( opRegisterCallback );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispRegisterCallback(
										sesid,
										tableid,
										cbtyp,
										pCallback,
										pvContext,
										phCallbackId ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRegisterCallback(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_CBTYP		cbtyp, 
	JET_CALLBACK	pCallback, 
	VOID *			pvContext,
	JET_HANDLE		*phCallbackId )
	{
	JET_TRY( JetRegisterCallbackEx( sesid, tableid, cbtyp, pCallback, pvContext, phCallbackId ) );
	}

	
LOCAL JET_ERR JetUnregisterCallbackEx(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_CBTYP		cbtyp, 
	JET_HANDLE		hCallbackId )
	{
	APICALL_SESID	apicall( opUnregisterCallback );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispUnregisterCallback(
										sesid,
										tableid,
										cbtyp,
										hCallbackId ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetUnregisterCallback(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_CBTYP		cbtyp, 
	JET_HANDLE		hCallbackId )
	{
	JET_TRY( JetUnregisterCallbackEx( sesid, tableid, cbtyp, hCallbackId ) );
	}


LOCAL JET_ERR JetSetLSEx(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_LS			ls, 
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opSetLS );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSetLS( sesid, tableid, ls, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetLS(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_LS			ls, 
	JET_GRBIT		grbit )
	{
	JET_TRY( JetSetLSEx( sesid, tableid, ls, grbit ) );
	}


LOCAL JET_ERR JetGetLSEx(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_LS			*pls,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opGetLS );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetLS( sesid, tableid, pls, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetLS(
	JET_SESID 		sesid,
	JET_TABLEID		tableid,
	JET_LS			*pls, 
	JET_GRBIT		grbit )
	{
	JET_TRY( JetGetLSEx( sesid, tableid, pls, grbit ) );
	}


LOCAL JET_ERR JetDupCursorEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_TABLEID		*ptableid,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opDupCursor );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispDupCursor( sesid, tableid, ptableid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDupCursor(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_TABLEID		*ptableid,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetDupCursorEx( sesid, tableid, ptableid, grbit ) );
	}


LOCAL JET_ERR JetCloseTableEx( JET_SESID sesid, JET_TABLEID tableid )
	{
	APICALL_SESID	apicall( opCloseTable );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		apicall.LeaveAfterCall( ErrDispCloseTable( sesid, tableid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCloseTable( JET_SESID sesid, JET_TABLEID tableid )
	{
	JET_TRY( JetCloseTableEx( sesid, tableid ) );
	}


LOCAL JET_ERR JetGetTableColumnInfoEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*pColumnNameOrId,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	ERR				err;
	ULONG			cbMin;
	APICALL_SESID	apicall( opGetTableColumnInfo );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	FillClientBuffer( pvResult, cbMax );

	switch( InfoLevel )
		{
		case JET_ColInfo:
		case JET_ColInfoByColid:
			cbMin = sizeof(JET_COLUMNDEF);
			break;
		case JET_ColInfoList:
		case JET_ColInfoListCompact:
		case 2:
		case JET_ColInfoListSortColumnid:
			cbMin = sizeof(JET_COLUMNLIST);
			break;
		case JET_ColInfoBase:
			cbMin = sizeof(JET_COLUMNBASE);
			break;
			
		case JET_ColInfoSysTabCursor:
			AssertTracking();
			err = ErrERRCheck( JET_errFeatureNotAvailable );
			goto HandleError;
		
		default:
			err = ErrERRCheck( JET_errInvalidParameter );
			goto HandleError;
		}

	if ( cbMax >= cbMin )
		{
		if ( InfoLevel == JET_ColInfoByColid )
			{
			err = ErrDispGetTableColumnInfo(
							sesid,
							tableid,
							NULL,
							(JET_COLUMNID *)pColumnNameOrId,
							pvResult,
							cbMax,
							InfoLevel );
			}
		else
			{
			err = ErrDispGetTableColumnInfo(
							sesid,
							tableid,
							pColumnNameOrId,
							NULL,
							pvResult,
							cbMax,
							InfoLevel );
			}
		}
	else
		{
		err = ErrERRCheck( JET_errBufferTooSmall );
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetGetTableColumnInfo(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*pColumnNameOrId,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetTableColumnInfoEx( sesid, tableid, pColumnNameOrId, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetGetColumnInfoEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const char		*pColumnNameOrId,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	ERR				err;
	ULONG			cbMin;
	APICALL_SESID	apicall( opGetColumnInfo );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	FillClientBuffer( pvResult, cbMax );

	switch( InfoLevel )
		{
		case JET_ColInfo:
		case JET_ColInfoByColid:
			cbMin = sizeof(JET_COLUMNDEF);
			break;
		case JET_ColInfoList:
		case JET_ColInfoListCompact:
		case 2 :
		case JET_ColInfoListSortColumnid:
			cbMin = sizeof(JET_COLUMNLIST);
			break;
		case JET_ColInfoBase:
			cbMin = sizeof(JET_COLUMNBASE);
			break;
			
		case JET_ColInfoSysTabCursor:
			AssertTracking();
			err = ErrERRCheck( JET_errFeatureNotAvailable );
			goto HandleError;
		
		default:
			err = ErrERRCheck( JET_errInvalidParameter );
			goto HandleError;
		}

	if ( cbMax >= cbMin )
		{
		if ( InfoLevel == JET_ColInfoByColid )
			{
			err = ErrIsamGetColumnInfo(
							sesid,
							(JET_DBID)ifmp,
							szTableName,
							NULL,
							(JET_COLUMNID *)pColumnNameOrId,
							pvResult,
							cbMax,
							InfoLevel );
			}
		else
			{
			err = ErrIsamGetColumnInfo(
							sesid,
							(JET_DBID)ifmp,
							szTableName,
							pColumnNameOrId,
							NULL,
							pvResult,
							cbMax,
							InfoLevel );
			}
		}
	else
		{
		err = ErrERRCheck( JET_errBufferTooSmall );
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetGetColumnInfo(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	const char		*pColumnNameOrId,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetColumnInfoEx( sesid, ifmp, szTableName, pColumnNameOrId, pvResult, cbMax, InfoLevel ) );
	}

LOCAL JET_ERR JetRetrieveColumnEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	void			*pvData,
	unsigned long	cbData,
	unsigned long	*pcbActual,
	JET_GRBIT		grbit,
	JET_RETINFO		*pretinfo )
	{
	APICALL_SESID	apicall( opRetrieveColumn );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispRetrieveColumn(
										sesid,
										tableid,
										columnid,
										pvData,
										cbData,
										pcbActual,
										grbit,
										pretinfo ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRetrieveColumn(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	void			*pvData,
	unsigned long	cbData,
	unsigned long	*pcbActual,
	JET_GRBIT		grbit,
	JET_RETINFO		*pretinfo )
	{
	JET_TRY( JetRetrieveColumnEx( sesid, tableid, columnid, pvData, cbData, pcbActual, grbit, pretinfo ) );
	}


LOCAL JET_ERR JetRetrieveColumnsEx(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	JET_RETRIEVECOLUMN	*pretrievecolumn,
	unsigned long		cretrievecolumn )
	{
	APICALL_SESID		apicall( opRetrieveColumns );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispRetrieveColumns(
										sesid,
										tableid,
										pretrievecolumn,
										cretrievecolumn ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRetrieveColumns(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	JET_RETRIEVECOLUMN	*pretrievecolumn,
	unsigned long		cretrievecolumn )
	{
	JET_TRY( JetRetrieveColumnsEx( sesid, tableid, pretrievecolumn, cretrievecolumn ) );
	}


LOCAL JET_ERR JetEnumerateColumnsEx(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	unsigned long		cEnumColumnId,
	JET_ENUMCOLUMNID*	rgEnumColumnId,
	unsigned long*		pcEnumColumn,
	JET_ENUMCOLUMN**	prgEnumColumn,
	JET_PFNREALLOC		pfnRealloc,
	void*				pvReallocContext,
	unsigned long		cbDataMost,
	JET_GRBIT			grbit )
	{
	APICALL_SESID		apicall( opEnumerateColumns );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispEnumerateColumns(
										sesid,
										tableid,
										cEnumColumnId,
										rgEnumColumnId,
										pcEnumColumn,
										prgEnumColumn,
										pfnRealloc,
										pvReallocContext,
										cbDataMost,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetEnumerateColumns(
	JET_SESID				sesid,
	JET_TABLEID				tableid,
	unsigned long			cEnumColumnId,
	JET_ENUMCOLUMNID*		rgEnumColumnId,
	unsigned long*			pcEnumColumn,
	JET_ENUMCOLUMN**		prgEnumColumn,
	JET_PFNREALLOC			pfnRealloc,
	void*					pvReallocContext,
	unsigned long			cbDataMost,
	JET_GRBIT				grbit )
	{
	JET_TRY( JetEnumerateColumnsEx(	sesid,
									tableid,
									cEnumColumnId,
									rgEnumColumnId,
									pcEnumColumn,
									prgEnumColumn,
									pfnRealloc,
									pvReallocContext,
									cbDataMost,
									grbit ) );
	}


LOCAL JET_ERR JetRetrieveTaggedColumnListEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	*pcColumns,
	void			*pvData,
	unsigned long	cbData,
	JET_COLUMNID	columnidStart,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opRetrieveTaggedColumnList );

	if ( apicall.FEnter( sesid ) )
		{
		FillClientBuffer( pvData, cbData, fTrue );
		apicall.LeaveAfterCall( ErrDispRetrieveTaggedColumnList(
										sesid,
										tableid,
										pcColumns,
										pvData,
										cbData,
										columnidStart,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRetrieveTaggedColumnList(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	*pcColumns,
	void			*pvData,
	unsigned long	cbData,
	JET_COLUMNID	columnidStart,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetRetrieveTaggedColumnListEx( sesid, tableid, pcColumns, pvData, cbData, columnidStart, grbit ) );
	}


LOCAL JET_ERR JetSetColumnEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	const void		*pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit,
	JET_SETINFO		*psetinfo )
	{
	APICALL_SESID	apicall( opSetColumn );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSetColumn(
										sesid,
										tableid,
										columnid,
										pvData,
										cbData,
										grbit,
										psetinfo ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetColumn(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	const void		*pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit,
	JET_SETINFO		*psetinfo )
	{
	JET_TRY( JetSetColumnEx( sesid, tableid, columnid, pvData, cbData, grbit, psetinfo ) );
	}


LOCAL JET_ERR JetSetColumnsEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_SETCOLUMN	*psetcolumn,
	unsigned long	csetcolumn )
	{
	APICALL_SESID	apicall( opSetColumns );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSetColumns(
										sesid,
										tableid,
										psetcolumn,
										csetcolumn ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetColumns(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_SETCOLUMN	*psetcolumn,
	unsigned long	csetcolumn )
	{
	JET_TRY( JetSetColumnsEx( sesid, tableid, psetcolumn, csetcolumn ) );
	}


LOCAL JET_ERR JetPrepareUpdateEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	prep )
	{
	APICALL_SESID	apicall( opPrepareUpdate );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispPrepareUpdate( sesid, tableid, prep ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetPrepareUpdate(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	prep )
	{
	JET_TRY( JetPrepareUpdateEx( sesid, tableid, prep ) );
	}


LOCAL JET_ERR JetUpdateEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	APICALL_SESID	apicall( opUpdate );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispUpdate(
										sesid,
										tableid,
										pvBookmark,
										cbMax,
										pcbActual,
										NO_GRBIT ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetUpdate(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	JET_TRY( JetUpdateEx( sesid, tableid, pvBookmark, cbMax, pcbActual ) );
	}


LOCAL JET_ERR JetEscrowUpdateEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	void			*pv,
	unsigned long	cbMax,
	void			*pvOld,
	unsigned long	cbOldMax,
	unsigned long	*pcbOldActual,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opEscrowUpdate );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispEscrowUpdate(
										sesid,
										tableid,
										columnid,
										pv,
										cbMax,
										pvOld,
										cbOldMax,
										pcbOldActual,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetEscrowUpdate(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_COLUMNID	columnid,
	void			*pv,
	unsigned long	cbMax,
	void			*pvOld,
	unsigned long	cbOldMax,
	unsigned long	*pcbOldActual,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetEscrowUpdateEx( sesid, tableid, columnid, pv, cbMax, pvOld, cbOldMax, pcbOldActual, grbit ) );
	}


LOCAL JET_ERR JetDeleteEx( JET_SESID sesid, JET_TABLEID tableid )
	{
	APICALL_SESID	apicall( opDelete );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispDelete( sesid, tableid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDelete( JET_SESID sesid, JET_TABLEID tableid )
	{
	JET_TRY( JetDeleteEx( sesid, tableid ) );
	}


LOCAL JET_ERR JetGetCursorInfoEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	APICALL_SESID	apicall( opGetCursorInfo );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetCursorInfo(
										sesid,
										tableid,
										pvResult,
										cbMax,
										InfoLevel ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetCursorInfo(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetCursorInfoEx( sesid, tableid, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetGetCurrentIndexEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	char			*szIndexName,
	unsigned long	cchIndexName )
	{
	APICALL_SESID	apicall( opGetCurrentIndex );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetCurrentIndex(
										sesid,
										tableid,
										szIndexName,
										cchIndexName ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetCurrentIndex(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	char			*szIndexName,
	unsigned long	cchIndexName )
	{
	JET_TRY( JetGetCurrentIndexEx( sesid, tableid, szIndexName, cchIndexName ) );
	}

LOCAL JET_ERR JetSetCurrentIndexEx(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	const CHAR			*szIndexName,
	const JET_INDEXID	*pindexid,
	const JET_GRBIT		grbit,
	const ULONG			itagSequence )
	{
	APICALL_SESID		apicall( opSetCurrentIndex );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( grbit & ~(JET_bitMoveFirst|JET_bitNoMove) ?
										ErrERRCheck( JET_errInvalidGrbit ) :
										ErrDispSetCurrentIndex( sesid, tableid, szIndexName, pindexid, grbit, itagSequence ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetCurrentIndex(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName )
	{
	JET_TRY( JetSetCurrentIndexEx( sesid, tableid, szIndexName, NULL, JET_bitMoveFirst, 1 ) );
	}
JET_ERR JET_API JetSetCurrentIndex2(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetSetCurrentIndexEx( sesid, tableid, szIndexName, NULL, grbit, 1 ) );
	}
JET_ERR JET_API JetSetCurrentIndex3(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	JET_GRBIT		grbit,
	unsigned long	itagSequence )
	{
	JET_TRY( JetSetCurrentIndexEx( sesid, tableid, szIndexName, NULL, grbit, itagSequence ) );
	}
JET_ERR JET_API JetSetCurrentIndex4(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	JET_INDEXID		*pindexid,
	JET_GRBIT		grbit,
	unsigned long	itagSequence )
	{
	JET_TRY( JetSetCurrentIndexEx( sesid, tableid, szIndexName, pindexid, grbit, itagSequence ) );
	}


LOCAL JET_ERR JetMoveEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	signed long		cRow,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opMove );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispMove( sesid, tableid, cRow, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetMove(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	signed long		cRow,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetMoveEx( sesid, tableid, cRow, grbit ) );
	}


LOCAL JET_ERR JetMakeKeyEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const void		*pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opMakeKey );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispMakeKey(
										sesid,
										tableid,
										pvData,
										cbData,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetMakeKey(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const void		*pvData,
	unsigned long	cbData,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetMakeKeyEx( sesid, tableid, pvData, cbData, grbit ) );
	}


LOCAL JET_ERR JetSeekEx( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opSeek );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSeek( sesid, tableid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSeek( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	JET_TRY( JetSeekEx( sesid, tableid, grbit ) );
	}


LOCAL JET_ERR JetGetBookmarkEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	APICALL_SESID	apicall( opGetBookmark );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetBookmark(
										sesid,
										tableid,
										pvBookmark,
										cbMax,
										pcbActual ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetBookmark(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	JET_TRY( JetGetBookmarkEx( sesid, tableid, pvBookmark, cbMax, pcbActual ) );
	}

JET_ERR JET_API JetGetSecondaryIndexBookmarkEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void *			pvSecondaryKey,
	unsigned long	cbSecondaryKeyMax,
	unsigned long *	pcbSecondaryKeyActual,
	void *			pvPrimaryBookmark,
	unsigned long	cbPrimaryBookmarkMax,
	unsigned long *	pcbPrimaryBookmarkActual )
	{
	APICALL_SESID	apicall( opGetBookmark );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetIndexBookmark(
										sesid,
										tableid,
										pvSecondaryKey,
										cbSecondaryKeyMax,
										pcbSecondaryKeyActual,
										pvPrimaryBookmark,
										cbPrimaryBookmarkMax,
										pcbPrimaryBookmarkActual ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetSecondaryIndexBookmark(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void *			pvSecondaryKey,
	unsigned long	cbSecondaryKeyMax,
	unsigned long *	pcbSecondaryKeyActual,
	void *			pvPrimaryBookmark,
	unsigned long	cbPrimaryBookmarkMax,
	unsigned long *	pcbPrimaryBookmarkActual,
	const JET_GRBIT	grbit )
	{
	JET_TRY( JetGetSecondaryIndexBookmarkEx(
						sesid,
						tableid,
						pvSecondaryKey,
						cbSecondaryKeyMax,
						pcbSecondaryKeyActual,
						pvPrimaryBookmark,
						cbPrimaryBookmarkMax,
						pcbPrimaryBookmarkActual ) );
	}


LOCAL JET_ERR JetGotoBookmarkEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbBookmark )
	{
	APICALL_SESID	apicall( opGotoBookmark );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGotoBookmark(
										sesid,
										tableid,
										pvBookmark,
										cbBookmark ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGotoBookmark(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvBookmark,
	unsigned long	cbBookmark )
	{
	JET_TRY( JetGotoBookmarkEx( sesid, tableid, pvBookmark, cbBookmark ) );
	}

JET_ERR JET_API JetGotoSecondaryIndexBookmarkEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void *			pvSecondaryKey,
	unsigned long	cbSecondaryKey,
	void *			pvPrimaryBookmark,
	unsigned long	cbPrimaryBookmark,
	const JET_GRBIT	grbit )
	{
	APICALL_SESID	apicall( opGotoBookmark );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGotoIndexBookmark(
										sesid,
										tableid,
										pvSecondaryKey,
										cbSecondaryKey,
										pvPrimaryBookmark,
										cbPrimaryBookmark,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGotoSecondaryIndexBookmark(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void *			pvSecondaryKey,
	unsigned long	cbSecondaryKey,
	void *			pvPrimaryBookmark,
	unsigned long	cbPrimaryBookmark,
	const JET_GRBIT	grbit )
	{
	JET_TRY( JetGotoSecondaryIndexBookmarkEx(
						sesid,
						tableid,
						pvSecondaryKey,
						cbSecondaryKey,
						pvPrimaryBookmark,
						cbPrimaryBookmark,
						grbit ) );
	}


JET_ERR JET_API JetIntersectIndexesEx(
	JET_SESID			sesid,
	JET_INDEXRANGE *	rgindexrange,
	unsigned long		cindexrange,
	JET_RECORDLIST *	precordlist,
	JET_GRBIT			grbit )
	{
	APICALL_SESID		apicall( opIntersectIndexes );

	if ( apicall.FEnter( sesid ) )
		{
		//  not dispatched because we don't have a tableid to dispatch on
		apicall.LeaveAfterCall( ErrIsamIntersectIndexes(
										sesid,
										rgindexrange,
										cindexrange,
										precordlist,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetIntersectIndexes(
	JET_SESID sesid,
	JET_INDEXRANGE * rgindexrange,
	unsigned long cindexrange,
	JET_RECORDLIST * precordlist,
	JET_GRBIT grbit )
	{
	JET_TRY( JetIntersectIndexesEx( sesid, rgindexrange, cindexrange, precordlist, grbit ) );
	}


LOCAL JET_ERR JetGetRecordPositionEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_RECPOS		*precpos,
	unsigned long	cbKeypos )
	{
	APICALL_SESID	apicall( opGetRecordPosition );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetRecordPosition(
										sesid,
										tableid,
										precpos,
										cbKeypos ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetRecordPosition(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_RECPOS		*precpos,
	unsigned long	cbKeypos )
	{
	JET_TRY( JetGetRecordPositionEx( sesid, tableid, precpos, cbKeypos ) );
	}


LOCAL JET_ERR JetGotoPositionEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_RECPOS		*precpos )
	{
	APICALL_SESID	apicall( opGotoPosition );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGotoPosition(
										sesid,
										tableid,
										precpos ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGotoPosition(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_RECPOS		*precpos )
	{
	JET_TRY( JetGotoPositionEx( sesid, tableid, precpos ) );
	}


LOCAL JET_ERR JetRetrieveKeyEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvKey,
	unsigned long	cbMax,
	unsigned long	*pcbActual,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opRetrieveKey );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispRetrieveKey(
										sesid,
										tableid,
										pvKey,
										cbMax,
										pcbActual,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRetrieveKey(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	void			*pvKey,
	unsigned long	cbMax,
	unsigned long	*pcbActual,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetRetrieveKeyEx( sesid, tableid, pvKey, cbMax, pcbActual, grbit ) );
	}


LOCAL JET_ERR JetGetLockEx( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opGetLock );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispGetLock( sesid, tableid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetLock( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	JET_TRY( JetGetLockEx( sesid, tableid, grbit ) );
	}


LOCAL JET_ERR JetGetVersionEx( JET_SESID sesid, unsigned long  *pVersion )
	{
	APICALL_SESID	apicall( opGetVersion );

	if ( apicall.FEnter( sesid ) )
		{
		//	assert no aliasing of version information
		Assert( DwUtilImageVersionMajor() < 1<<8 );
		Assert( DwUtilImageBuildNumberMajor() < 1<<16 );
		Assert( DwUtilImageBuildNumberMinor() < 1<<8 );

		*pVersion = g_ulVersion;

		apicall.LeaveAfterCall( JET_errSuccess );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetVersion( JET_SESID sesid, unsigned long  *pVersion )
	{
	JET_TRY( JetGetVersionEx( sesid, pVersion ) );
	}

/*=================================================================
JetGetSystemParameter

Description:
  This function returns the current settings of the system parameters.

Parameters:
  sesid 		is the optional session identifier for dynamic parameters.
  paramid		is the system parameter code identifying the parameter.
  plParam		is the returned parameter value.
  sz			is the zero terminated string parameter buffer.
  cbMax			is the size of the string parameter buffer.

Return Value:
  JET_errSuccess if the routine can perform all operations cleanly;
  some appropriate error value otherwise.

Errors/Warnings:
  JET_errInvalidParameter:
	Invalid parameter code.
  JET_errInvalidSesid:
	Dynamic parameters require a valid session id.

Side Effects:
  None.
=================================================================*/
LOCAL JET_ERR JetGetSystemParameterEx(
	JET_INSTANCE	instance,
	JET_SESID		sesid,
	unsigned long	paramid,
	ULONG_PTR		*plParam,
	char			*sz,
	unsigned long	cbMax )
	{
	APICALL_INST	apicall( opGetSystemParameter );
	INST 			*pinst;

	SetPinst( &instance, sesid, &pinst );

	if ( !pinst )
		{
		//	setting for global default]
		Assert( !sesid || sesid == JET_sesidNil );
		return ErrGetSystemParameter( 0, 0, paramid, plParam, sz, cbMax );
		}

	if ( apicall.FEnterWithoutInit( pinst, fFalse ) )
		{
		apicall.LeaveAfterCall( ErrGetSystemParameter(
										(JET_INSTANCE)pinst,
										sesid,
										paramid,
										plParam,
										sz,
										cbMax ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetSystemParameter(
	JET_INSTANCE	instance,
	JET_SESID		sesid,
	unsigned long	paramid,
	ULONG_PTR		*plParam,
	char			*sz,
	unsigned long	cbMax )
	{
	JET_TRY( JetGetSystemParameterEx( instance, sesid, paramid, plParam, sz, cbMax ) );
	}


/*=================================================================
JetBeginSession

Description:
  This function signals the start of a session for a given user.  It must
  be the first function called by the application on behalf of that user.

  The username and password supplied must correctly identify a user account
  in the security accounts subsystem of the engine for which this session
  is being started.  Upon proper identification and authentication, a SESID
  is allocated for the session, a user token is created for the security
  subject, and that user token is specifically associated with the SESID
  of this new session for the life of that SESID (until JetEndSession is
  called).

Parameters:
  psesid		is the unique session identifier returned by the system.
  szUsername	is the username of the user account for logon purposes.
  szPassword	is the password of the user account for logon purposes.

Return Value:
  JET_errSuccess if the routine can perform all operations cleanly;
  some appropriate error value otherwise.

Errors/Warnings:

Side Effects:
  * Allocates resources which must be freed by JetEndSession().
=================================================================*/

LOCAL JET_ERR JetBeginSessionEx(
	JET_INSTANCE	instance,
	JET_SESID		*psesid,
	const char		*szUsername,
	const char		*szPassword )
	{
	APICALL_INST	apicall( opBeginSession );

	if ( apicall.FEnter( instance ) )
		{
		//	tell the ISAM to start a new session
		apicall.LeaveAfterCall( ErrIsamBeginSession(
										(JET_INSTANCE)apicall.Pinst(),
										psesid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetBeginSession(
	JET_INSTANCE	instance,
	JET_SESID		*psesid,
	const char		*szUsername,
	const char		*szPassword )
	{
	JET_TRY( JetBeginSessionEx( instance, psesid, szUsername, szPassword ) );
	}


LOCAL JET_ERR JetDupSessionEx( JET_SESID sesid, JET_SESID *psesid )
	{
	APICALL_SESID	apicall( opDupSession );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamBeginSession(
										(JET_INSTANCE)PinstFromSesid( sesid ),
										psesid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDupSession( JET_SESID sesid, JET_SESID *psesid )
	{
	JET_TRY( JetDupSessionEx( sesid, psesid ) );
	}

/*=================================================================
JetEndSession

Description:
  This routine ends a session with a Jet engine.

Parameters:
  sesid 		identifies the session uniquely

Return Value:
  JET_errSuccess if the routine can perform all operations cleanly;
  some appropriate error value otherwise.

Errors/Warnings:
  JET_errInvalidSesid:
	The SESID supplied is invalid.

Side Effects:
=================================================================*/
LOCAL JET_ERR JetEndSessionEx( JET_SESID sesid, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opEndSession );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		apicall.LeaveAfterCall( ErrIsamEndSession( sesid, grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetEndSession( JET_SESID sesid, JET_GRBIT grbit )
	{
	JET_TRY( JetEndSessionEx( sesid, grbit ) );
	}

LOCAL JET_ERR JetCreateDatabaseEx(
	JET_SESID		sesid,
	const CHAR		*szDatabaseName,
	const CHAR		*szSLVName,
	const CHAR		*szSLVRoot,
	const ULONG		cpgDatabaseSizeMax,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opCreateDatabase );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamCreateDatabase(
										sesid,
										szDatabaseName,
										szSLVName,
										szSLVRoot,
										cpgDatabaseSizeMax,
										pifmp,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCreateDatabase(
	JET_SESID		sesid,
	const char		*szFilename,
	const char		*szConnect,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetCreateDatabaseEx( sesid, szFilename, NULL, NULL, 0, pifmp, grbit ) );
	}
JET_ERR JET_API JetCreateDatabase2(
	JET_SESID		sesid,
	const CHAR		*szFilename,
	const ULONG		cpgDatabaseSizeMax,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetCreateDatabaseEx( sesid, szFilename, NULL, NULL, cpgDatabaseSizeMax, pifmp, grbit ) );
	}
JET_ERR JET_API JetCreateDatabaseWithStreaming(
	JET_SESID		sesid,
	const CHAR		*szDbFileName,
	const CHAR		*szSLVFileName,
	const CHAR		*szSLVRootName,
	const ULONG		cpgDatabaseSizeMax,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetCreateDatabaseEx(
					sesid,
					szDbFileName,
					szSLVFileName,
					szSLVRootName,
					cpgDatabaseSizeMax,
					pifmp,
					grbit|JET_bitDbCreateStreamingFile ) );
	}


LOCAL JET_ERR JetOpenDatabaseEx(
	JET_SESID		sesid,
	const char		*szDatabase,
	const char		*szConnect,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opOpenDatabase );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamOpenDatabase(
										sesid,
										szDatabase,
										szConnect,
										pifmp,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetOpenDatabase(
	JET_SESID		sesid,
	const char		*szDatabase,
	const char		*szConnect,
	JET_DBID		*pifmp,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetOpenDatabaseEx( sesid, szDatabase, szConnect, pifmp, grbit ) );
	}


LOCAL JET_ERR JetGetDatabaseInfoEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	APICALL_SESID	apicall( opGetDatabaseInfo );

	if ( apicall.FEnter( sesid ) )
		{
		FillClientBuffer( pvResult, cbMax );
		apicall.LeaveAfterCall( ErrIsamGetDatabaseInfo(
										sesid,
										(JET_DBID)ifmp,
										pvResult,
										cbMax,
										InfoLevel ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetDatabaseInfo(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{	
	JET_TRY( JetGetDatabaseInfoEx( sesid, ifmp, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetGetDatabaseFileInfoEx(
	const char		*szDatabaseName,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	ERR				err				= JET_errSuccess;
	IFileSystemAPI*	pfsapi			= NULL;
	IFileFindAPI*	pffapi			= NULL;
	CHAR			szFullDbName[ IFileSystemAPI::cchPathMax ];
	QWORD			cbFileSize		= 0;
	INT				cSystemInitCalls= 0;
	BOOL			fInINSTCrit		= fFalse;

	Call( ErrOSFSCreate( &pfsapi ) );
	Call( pfsapi->ErrFileFind( szDatabaseName, &pffapi ) );
	Call( pffapi->ErrNext() );
	Call( pffapi->ErrPath( szFullDbName ) );
   	Call( pffapi->ErrSize( &cbFileSize ) );
		
	switch ( InfoLevel )
		{
		case JET_DbInfoFilesize:
	 		if ( sizeof( QWORD ) != cbMax )
	 			{
	    		Error( ErrERRCheck( JET_errInvalidBufferSize ), HandleError );
	    		}
	    	FillClientBuffer( pvResult, cbMax );

	    	memcpy( (BYTE*)pvResult, (BYTE*)&cbFileSize, sizeof( QWORD ) );
	    	break;
	    		
		case JET_DbInfoUpgrade:
			{
			if ( sizeof(JET_DBINFOUPGRADE ) != cbMax )
				{
				Call( ErrERRCheck( JET_errInvalidBufferSize ) );
				}

			JET_DBINFOUPGRADE	*pdbinfoupgd	= (JET_DBINFOUPGRADE *)pvResult;
			DBFILEHDR_FIX		*pdbfilehdr;

			if ( sizeof(JET_DBINFOUPGRADE) != pdbinfoupgd->cbStruct )
				{
				Call( ErrERRCheck( JET_errInvalidParameter ) );
				}

			memset( pdbinfoupgd, 0, sizeof(JET_DBINFOUPGRADE) );
			pdbinfoupgd->cbStruct = sizeof(JET_DBINFOUPGRADE);
			
			pdbinfoupgd->cbFilesizeLow	= DWORD( cbFileSize );
			pdbinfoupgd->cbFilesizeHigh	= DWORD( cbFileSize >> 32 );

			//	UNDONE:	need more accurate estimates of space and time requirements
			pdbinfoupgd->csecToUpgrade = ULONG( ( cbFileSize * 3600 ) >> 30 );	// shr 30 == divide by 1Gb
			cbFileSize = ( cbFileSize * 10 ) >> 6;								// shr 6 == divide by 64; 10/64 is roughly 15%
			pdbinfoupgd->cbFreeSpaceRequiredLow		= DWORD( cbFileSize );
			pdbinfoupgd->cbFreeSpaceRequiredHigh	= DWORD( cbFileSize >> 32 );

			fInINSTCrit = fTrue;
			INST::EnterCritInst();

			Assert( 0 == cSystemInitCalls );

			if ( 0 == ipinstMac )
				{
				//	HACK: must init I/O manager
				Call( INST::ErrINSTSystemInit() );
				cSystemInitCalls++;
				Call( ErrOSUInit() );
				cSystemInitCalls++;
				}

			//	bring in the database and check its header
			pdbfilehdr = (DBFILEHDR_FIX * )PvOSMemoryPageAlloc( g_cbPage, NULL );
			if ( NULL == pdbfilehdr )
				{
				Call( ErrERRCheck( JET_errOutOfMemory ) );
				}
				
			//	need to zero out header because we try to read it
			//	later on even on failure
			memset( pdbfilehdr, 0, g_cbPage );

			//	verify flags initialised
			Assert( !pdbinfoupgd->fUpgradable );
			Assert(	!pdbinfoupgd->fAlreadyUpgraded );

			err = ErrUtilReadShadowedHeader(
						pfsapi, 
						szFullDbName,
						(BYTE*)pdbfilehdr,
						g_cbPage,
						OffsetOf( DBFILEHDR_FIX, le_cbPageSize ) );
			Assert( err <= JET_errSuccess );	// shouldn't get warnings
						
			//	Checksumming may have changed, which is probably
			//	why we got errors, so just force success.
			//	If there truly was an error, then the fUpgradable flag
			//	will be fFalse to indicate the database is not upgradable.
			//	If the user later tries to upgrade anyways, the error will
			//	be detected when the database is opened.
			err = JET_errSuccess;
			
			//	If able to read header, ignore any errors and check version
			//	Note that the magic number stays the same since 500.
			if ( ulDAEMagic == pdbfilehdr->le_ulMagic )
				{
				switch ( pdbfilehdr->le_ulVersion )
					{
					case ulDAEVersion:
						Assert( !pdbinfoupgd->fUpgradable );
						pdbinfoupgd->fAlreadyUpgraded = fTrue;
						if ( pdbfilehdr->Dbstate() == JET_dbstateBeingConverted )
							err = ErrERRCheck( JET_errDatabaseIncompleteUpgrade );
						break;
					case ulDAEVersion500:
					case ulDAEVersion550:
						pdbinfoupgd->fUpgradable = fTrue;
						Assert(	!pdbinfoupgd->fAlreadyUpgraded );
						break;
					case ulDAEVersion400:
						{
						CHAR szDbPath[IFileSystemAPI::cchPathMax];
						CHAR szDbFileName[IFileSystemAPI::cchPathMax];
						CHAR szDbFileExt[IFileSystemAPI::cchPathMax];

						Call( pfsapi->ErrPathParse( szFullDbName, szDbPath, szDbFileName, szDbFileExt ) );
						strcat( szDbFileName, szDbFileExt );
						
						//	HACK! HACK! HACK!
						//	there was a bug where the Exchange 4.0 skeleton DIR.EDB
						//	was mistakenly stamped with ulDAEVersion400.
						pdbinfoupgd->fUpgradable = ( 0 == _stricmp( szDbFileName, "dir.edb" ) ? fTrue : fFalse );
						Assert(	!pdbinfoupgd->fAlreadyUpgraded );
						break;
						}
					default:
						//	unsupported upgrade format
						Assert( !pdbinfoupgd->fUpgradable );
						Assert(	!pdbinfoupgd->fAlreadyUpgraded );
						break;
					}
				}

			OSMemoryPageFree( pdbfilehdr );

			break;
			}
	
	    case JET_DbInfoMisc:
		case JET_DbInfoPageSize:
		case JET_DbInfoHasSLVFile:
	    	{
	    	if ( ( InfoLevel == JET_DbInfoMisc && sizeof( JET_DBINFOMISC ) != cbMax )
	    		|| ( InfoLevel == JET_DbInfoPageSize && sizeof( ULONG ) != cbMax )
	    		|| ( InfoLevel == JET_DbInfoHasSLVFile && cbMax != sizeof( BOOL ) ) )
	    		{
		    	Call( ErrERRCheck( JET_errInvalidBufferSize ) );
		    	}
	    	FillClientBuffer( pvResult, cbMax );

			fInINSTCrit = fTrue;
			INST::EnterCritInst();

			Assert( 0 == cSystemInitCalls );

			if ( 0 == ipinstMac )
				{
				//	HACK: must init I/O manager
				Call( INST::ErrINSTSystemInit() );
				cSystemInitCalls++;
				Call( ErrOSUInit() );
				cSystemInitCalls++;
				}

 			DBFILEHDR *	pdbfilehdr	= (DBFILEHDR * )PvOSMemoryPageAlloc( g_cbPage, NULL );
			if ( NULL == pdbfilehdr )
				{
				err = ErrERRCheck( JET_errOutOfMemory );
				}

			else if ( JET_DbInfoPageSize == InfoLevel )
				{
				INT		bHeaderDamaged;
				err = ErrUtilOnlyReadShadowedHeader(	pfsapi, 
														szFullDbName, 
														(BYTE *)pdbfilehdr, 
														g_cbPage, 
														OffsetOf( DBFILEHDR_FIX, le_cbPageSize ),
														&bHeaderDamaged,
														NULL );
				if ( JET_errSuccess == err )
					{
					*( ULONG *)pvResult = ( 0 != pdbfilehdr->le_cbPageSize ?
											pdbfilehdr->le_cbPageSize :
											cbPageDefault );
					}

				OSMemoryPageFree( pdbfilehdr );
				}

			else
				{
		    	err = ErrUtilReadShadowedHeader(	pfsapi, 
		    										szFullDbName, 
		    										(BYTE *)pdbfilehdr, 
		    										g_cbPage, 
		    										OffsetOf( DBFILEHDR_FIX, le_cbPageSize ) );

		    	if ( JET_errSuccess == err )
		    		{
		    		switch ( InfoLevel )
		    			{
		    			case JET_DbInfoMisc:
							UtilLoadDbinfomiscFromPdbfilehdr( (JET_DBINFOMISC *)pvResult, (DBFILEHDR_FIX*)pdbfilehdr );
							break;
		    			case JET_DbInfoHasSLVFile:
							*( BOOL *)pvResult = !!pdbfilehdr->FSLVExists();
							break;
						default:
							Assert( fFalse );
		    			}
					}

				OSMemoryPageFree( pdbfilehdr );
				}

   			break;
	    	}	

		case JET_DbInfoDBInUse:
			{

	    	if ( sizeof( BOOL ) != cbMax )
	    		{
		    	Error( ErrERRCheck( JET_errInvalidBufferSize ), HandleError );
		    	}
		    		
			*(BOOL *)pvResult = fFalse;

			// no fmp table
			if ( NULL == rgfmp)
				{
				break;
				}
				
			FMP::EnterCritFMPPool();

			for ( IFMP ifmp = FMP::IfmpMinInUse(); ifmp <= FMP::IfmpMacInUse(); ifmp++ )
				{
				FMP	* pfmp = &rgfmp[ ifmp ];

				if ( !pfmp->FInUse() )
					{
					continue;
					}
					
				if ( 0 != UtilCmpFileName( pfmp->SzDatabaseName(), szFullDbName ) )
					{
					continue;
					}

				*(BOOL *)pvResult = fTrue;
				break;
				}
				
			FMP::LeaveCritFMPPool();
			}
			break;
		default:
			 err = ErrERRCheck( JET_errInvalidParameter );
		}

HandleError:
	//	===> Initializing OS I/O manager hack error handling
	Assert( 2 >= cSystemInitCalls );
	if ( 2 == cSystemInitCalls )
		{
		OSUTerm();
		cSystemInitCalls--;
		}

	if ( 1 == cSystemInitCalls )
		{
		INST::INSTSystemTerm();
		cSystemInitCalls--;
		}

	if ( fInINSTCrit )
		{
		INST::LeaveCritInst();
		fInINSTCrit = fFalse;
		}
		
	Assert( 0 == cSystemInitCalls );
	Assert( !fInINSTCrit );
	//	<=== Initializing OS I/O manager hack error handling
	delete pffapi;
	delete pfsapi;
	return err;
	}


JET_ERR JET_API JetGetDatabaseFileInfo(
	const char		*szDatabaseName,
	void			*pvResult,
	unsigned long	cbMax,
	unsigned long	InfoLevel )
	{
	JET_TRY( JetGetDatabaseFileInfoEx( szDatabaseName, pvResult, cbMax, InfoLevel ) );
	}


LOCAL JET_ERR JetCloseDatabaseEx( JET_SESID sesid, JET_DBID ifmp, JET_GRBIT grbit )
	{
	APICALL_SESID	apicall( opCloseDatabase );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		apicall.LeaveAfterCall( ErrIsamCloseDatabase(
										sesid,
										(JET_DBID)ifmp,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCloseDatabase( JET_SESID sesid, JET_DBID ifmp, JET_GRBIT grbit )
	{
	JET_TRY( JetCloseDatabaseEx( sesid, ifmp, grbit ) );
	}


LOCAL JET_ERR JetCreateTableEx(
	JET_SESID			sesid,
	JET_DBID			ifmp,
	const char			*szTableName,
	unsigned long		lPage,
	unsigned long		lDensity,
	JET_TABLEID			*ptableid )
	{
	APICALL_SESID		apicall( opCreateTable );
	JET_TABLECREATE2	tablecreate	=
							{	sizeof(JET_TABLECREATE2),
								(CHAR *)szTableName,
								NULL,
								lPage,
								lDensity,
								NULL,
								0,
								NULL,
								0,	// No columns/indexes
								NULL,
								0,	// No callbacks
								0,	// grbit
								JET_tableidNil,	// returned tableid
								0	// returned count of objects created
								};

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamCreateTable2(
										sesid,
										(JET_DBID)ifmp,
										&tablecreate ) );

		//	the following statement automatically set to tableid Nil on error
		*ptableid = tablecreate.tableid;

		//	either the table was created or it was not
		Assert( 0 == tablecreate.cCreated || 1 == tablecreate.cCreated );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCreateTable(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	const char		*szTableName,
	unsigned long	lPage,
	unsigned long	lDensity,
	JET_TABLEID		*ptableid )
	{
	JET_TRY( JetCreateTableEx( sesid, ifmp, szTableName, lPage, lDensity, ptableid ) );
	}
	

LOCAL JET_ERR JetCreateTableColumnIndexEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_TABLECREATE	*ptablecreate )
	{
	APICALL_SESID	apicall( opCreateTableColumnIndex );

	if ( apicall.FEnter( sesid ) )
		{
		const BOOL	fInvalidParam	= ( NULL == ptablecreate
										|| sizeof(JET_TABLECREATE) != ptablecreate->cbStruct );
		apicall.LeaveAfterCall( fInvalidParam ?
									ErrERRCheck( JET_errInvalidParameter ) :
									ErrIsamCreateTable( sesid, (JET_DBID)ifmp, ptablecreate ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCreateTableColumnIndex(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_TABLECREATE	*ptablecreate )
	{
	JET_TRY( JetCreateTableColumnIndexEx( sesid, ifmp, ptablecreate ) );
	}


LOCAL JET_ERR JetCreateTableColumnIndexEx2(
	JET_SESID			sesid,
	JET_DBID			ifmp,
	JET_TABLECREATE2	*ptablecreate )
	{
	APICALL_SESID		apicall( opCreateTableColumnIndex );

	if ( apicall.FEnter( sesid ) )
		{
		const BOOL	fInvalidParam	= ( NULL == ptablecreate
										|| sizeof(JET_TABLECREATE2) != ptablecreate->cbStruct );
		apicall.LeaveAfterCall( fInvalidParam ?
									ErrERRCheck( JET_errInvalidParameter ) :
									ErrIsamCreateTable2( sesid, (JET_DBID)ifmp, ptablecreate ) );
	}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCreateTableColumnIndex2(
	JET_SESID			sesid,
	JET_DBID			ifmp,
	JET_TABLECREATE2	*ptablecreate )
	{
	JET_TRY( JetCreateTableColumnIndexEx2( sesid, ifmp, ptablecreate ) );
	}


LOCAL JET_ERR JetDeleteTableEx( JET_SESID sesid, JET_DBID ifmp, const char *szName )
	{
	APICALL_SESID	apicall( opDeleteTable );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamDeleteTable(
										sesid,
										(JET_DBID)ifmp,
										(CHAR *)szName ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDeleteTable( JET_SESID sesid, JET_DBID ifmp, const char *szName )
	{
	JET_TRY( JetDeleteTableEx( sesid, ifmp, szName ) );
	}


LOCAL JET_ERR JetRenameTableEx(
	JET_SESID		sesid,
	JET_DBID		dbid, 
	const CHAR *	szName,
	const CHAR *	szNameNew )
	{
	APICALL_SESID	apicall( opRenameTable );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamRenameTable(
										sesid,
										dbid,
										szName,
										szNameNew ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRenameTable(
	JET_SESID	sesid,
	JET_DBID	dbid, 
	const CHAR	*szName,
	const CHAR	*szNameNew )
	{
	JET_TRY( JetRenameTableEx( sesid, dbid, szName, szNameNew ) );
	}


LOCAL JET_ERR JetRenameColumnEx(
	const JET_SESID		sesid,
	const JET_TABLEID	tableid, 
	const CHAR * const	szName,
	const CHAR * const	szNameNew,
	const JET_GRBIT 	grbit )
	{
	APICALL_SESID		apicall( opRenameColumn );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispRenameColumn(
										sesid,
										tableid,
										szName,
										szNameNew,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRenameColumn(
	JET_SESID	sesid,
	JET_TABLEID	tableid, 
	const CHAR	*szName,
	const CHAR	*szNameNew,
	JET_GRBIT	grbit )
	{
	JET_TRY( JetRenameColumnEx( sesid, tableid, szName, szNameNew, grbit ) );
	}


LOCAL JET_ERR JetAddColumnEx(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	const char			*szColumnName,
	const JET_COLUMNDEF	*pcolumndef,
	const void			*pvDefault,
	unsigned long		cbDefault,
	JET_COLUMNID		*pcolumnid )
	{
	APICALL_SESID		apicall( opAddColumn );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispAddColumn(
										sesid,
										tableid,
										szColumnName,
										pcolumndef,
										pvDefault,
										cbDefault,
										pcolumnid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetAddColumn(
	JET_SESID			sesid,
	JET_TABLEID			tableid,
	const char			*szColumnName,
	const JET_COLUMNDEF	*pcolumndef,
	const void			*pvDefault,
	unsigned long		cbDefault,
	JET_COLUMNID		*pcolumnid )
	{
	JET_TRY( JetAddColumnEx( sesid, tableid, szColumnName, pcolumndef, pvDefault, cbDefault, pcolumnid ) );
	}


LOCAL JET_ERR JetDeleteColumnEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szColumnName,
	const JET_GRBIT	grbit )
	{
	APICALL_SESID	apicall( opDeleteColumn );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispDeleteColumn(
										sesid,
										tableid,
										szColumnName,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDeleteColumn(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szColumnName )
	{
	JET_TRY( JetDeleteColumnEx( sesid, tableid, szColumnName, NO_GRBIT ) );
	}	
JET_ERR JET_API JetDeleteColumn2(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szColumnName,
	const JET_GRBIT	grbit )
	{
	JET_TRY( JetDeleteColumnEx( sesid, tableid, szColumnName, grbit ) );
	}	


LOCAL JET_ERR JetSetColumnDefaultValueEx(
	JET_SESID			sesid,
	JET_DBID			ifmp,
	const char			*szTableName,
	const char			*szColumnName,
	const void			*pvData,
	const unsigned long	cbData,
	const JET_GRBIT		grbit )
	{	
	APICALL_SESID		apicall( opSetColumnDefaultValue );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamSetColumnDefaultValue(
										sesid,
										ifmp,
										szTableName,
										szColumnName,
										pvData,
										cbData,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetColumnDefaultValue(
	JET_SESID			sesid,
	JET_DBID			ifmp,
	const char			*szTableName,
	const char			*szColumnName,
	const void			*pvData,
	const unsigned long	cbData,
	const JET_GRBIT		grbit )
	{
	JET_TRY( JetSetColumnDefaultValueEx( sesid, ifmp, szTableName, szColumnName, pvData, cbData, grbit ) );
	}
	
LOCAL JET_ERR JetCreateIndexEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_INDEXCREATE *pindexcreate,
	unsigned long 	cIndexCreate )
	{	
	APICALL_SESID	apicall( opCreateIndex );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispCreateIndex(
										sesid,
										tableid,
										pindexcreate,
										cIndexCreate ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCreateIndex(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName,
	JET_GRBIT		grbit,
	const char		*szKey,
	unsigned long	cbKey,
	unsigned long	lDensity )
	{	
	JET_INDEXCREATE	idxcreate;
	idxcreate.cbStruct 		= sizeof( JET_INDEXCREATE );
	idxcreate.szIndexName 	= (CHAR *)szIndexName;
	idxcreate.szKey			= (CHAR *)szKey;
	idxcreate.cbKey			= cbKey;
	idxcreate.grbit			= grbit;
	idxcreate.ulDensity		= lDensity;
	idxcreate.lcid			= 0;
	idxcreate.cbVarSegMac	= 0;
	idxcreate.rgconditionalcolumn 	= 0;
	idxcreate.cConditionalColumn	= 0;
	idxcreate.err			= JET_errSuccess;

	JET_TRY( JetCreateIndexEx( sesid, tableid, &idxcreate, 1 ) );
	}
JET_ERR JET_API JetCreateIndex2(
	JET_SESID 		sesid,
	JET_TABLEID 	tableid,
	JET_INDEXCREATE *pindexcreate,
	unsigned long 	cIndexCreate )
	{
	JET_TRY( JetCreateIndexEx( sesid, tableid, pindexcreate, cIndexCreate ) );
	}


LOCAL JET_ERR JetDeleteIndexEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName )
	{
	APICALL_SESID	apicall( opDeleteIndex );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispDeleteIndex(
										sesid,
										tableid,
										szIndexName ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDeleteIndex(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	const char		*szIndexName )
	{
	JET_TRY( JetDeleteIndexEx( sesid, tableid, szIndexName ) );
	}


LOCAL JET_ERR JetComputeStatsEx( JET_SESID sesid, JET_TABLEID tableid )
	{
	APICALL_SESID	apicall( opComputeStats );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispComputeStats( sesid, tableid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetComputeStats( JET_SESID sesid, JET_TABLEID tableid )
	{
	JET_TRY( JetComputeStatsEx( sesid, tableid ) );
	}


LOCAL JET_ERR JetAttachDatabaseEx(
	JET_SESID		sesid,
	const CHAR		*szDatabaseName,
	const CHAR		*szSLVName,
	const CHAR		*szSLVRoot,
	const ULONG		cpgDatabaseSizeMax,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opAttachDatabase );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamAttachDatabase(
										sesid,
										szDatabaseName,
										szSLVName,
										szSLVRoot,
										cpgDatabaseSizeMax,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetAttachDatabase(
	JET_SESID		sesid,
	const CHAR		*szFilename,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetAttachDatabaseEx( sesid, szFilename, NULL, NULL, 0, grbit ) );
	}
JET_ERR JET_API JetAttachDatabase2(
	JET_SESID		sesid,
	const CHAR		*szFilename,
	const ULONG		cpgDatabaseSizeMax,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetAttachDatabaseEx( sesid, szFilename, NULL, NULL, cpgDatabaseSizeMax, grbit ) );
	}
JET_ERR JET_API JetAttachDatabaseWithStreaming(
	JET_SESID		sesid,
	const CHAR		*szDbFileName,
	const CHAR		*szSLVFileName,
	const CHAR		*szSLVRootName,
	const ULONG		cpgDatabaseSizeMax,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetAttachDatabaseEx( sesid, szDbFileName, szSLVFileName, szSLVRootName, cpgDatabaseSizeMax, grbit ) );
	}


LOCAL JET_ERR JetDetachDatabaseEx( JET_SESID sesid, const char *szFilename, JET_GRBIT grbit)
	{
	APICALL_SESID	apicall( opDetachDatabase );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		const BOOL	fForceDetach	= ( JET_bitForceDetach & grbit );
		if ( fForceDetach )
			{
#ifdef INDEPENDENT_DB_FAILURE		
			apicall.LeaveAfterCall( ErrIsamForceDetachDatabase( sesid, szFilename, grbit ) );
#else
			apicall.LeaveAfterCall( ErrERRCheck( JET_errForceDetachNotAllowed ) );
#endif
			}
		else
			{
			apicall.LeaveAfterCall( ErrIsamDetachDatabase( sesid, NULL, szFilename, grbit ) );
			}
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDetachDatabase( JET_SESID sesid, const char *szFilename )
	{
	JET_TRY( JetDetachDatabaseEx( sesid, szFilename, NO_GRBIT ) );
	}
JET_ERR JET_API JetDetachDatabase2( JET_SESID sesid, const char *szFilename, JET_GRBIT grbit )
	{
	JET_TRY( JetDetachDatabaseEx( sesid, szFilename, grbit ) );
	}


LOCAL JET_ERR JetBackupInstanceEx(
	JET_INSTANCE 	instance,
	const char		*szBackupPath,
	JET_GRBIT		grbit,
	JET_PFNSTATUS	pfnStatus )
	{
	APICALL_INST	apicall( opBackupInstance );
	
	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( apicall.Pinst()->m_fBackupAllowed ?
										ErrIsamBackup( (JET_INSTANCE)apicall.Pinst(), szBackupPath, grbit, pfnStatus ) :
										ErrERRCheck( JET_errBackupNotAllowedYet ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetBackupInstance(
	JET_INSTANCE 	instance,
	const char		*szBackupPath,
	JET_GRBIT		grbit,
	JET_PFNSTATUS	pfnStatus )
	{
	JET_TRY( JetBackupInstanceEx( instance, szBackupPath, grbit, pfnStatus ) );
	}
JET_ERR JET_API JetBackup(
	const char		*szBackupPath,
	JET_GRBIT		grbit,
	JET_PFNSTATUS	pfnStatus )
	{
	ERR				err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetBackupInstance( (JET_INSTANCE)g_rgpinst[0], szBackupPath, grbit, pfnStatus );
	}


JET_ERR JET_API JetRestore(	const char *sz, JET_PFNSTATUS pfn )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetRestoreInstance( (JET_INSTANCE)g_rgpinst[0], sz, NULL, pfn );
	}
JET_ERR JET_API JetRestore2( const char *sz, const char *szDest, JET_PFNSTATUS pfn )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetRestoreInstance( (JET_INSTANCE)g_rgpinst[0], sz, szDest, pfn );
	}

	
LOCAL JET_ERR JetOpenTempTableEx(
	JET_SESID			sesid,
	const JET_COLUMNDEF	*prgcolumndef,
	unsigned long		ccolumn,
	JET_UNICODEINDEX	*pidxunicode,
	JET_GRBIT			grbit,
	JET_TABLEID			*ptableid,
	JET_COLUMNID		*prgcolumnid )
	{
	APICALL_SESID		apicall( opOpenTempTable );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamOpenTempTable(
										sesid,
										prgcolumndef,
										ccolumn,
										pidxunicode,
										grbit,
										ptableid,
										prgcolumnid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetOpenTempTable(
	JET_SESID			sesid,
	const JET_COLUMNDEF	*prgcolumndef,
	unsigned long		ccolumn,
	JET_GRBIT			grbit,
	JET_TABLEID			*ptableid,
	JET_COLUMNID		*prgcolumnid )
	{
	JET_TRY( JetOpenTempTableEx( sesid, prgcolumndef, ccolumn, NULL, grbit, ptableid, prgcolumnid ) );
	}
JET_ERR JET_API JetOpenTempTable2(
	JET_SESID			sesid,
	const JET_COLUMNDEF	*prgcolumndef,
	unsigned long		ccolumn,
	LCID				lcid,
	JET_GRBIT			grbit,
	JET_TABLEID			*ptableid,
	JET_COLUMNID		*prgcolumnid )
	{
	JET_UNICODEINDEX	idxunicode	= { lcid, dwLCMapFlagsDefault };

	JET_TRY( JetOpenTempTableEx( sesid, prgcolumndef, ccolumn, &idxunicode, grbit, ptableid, prgcolumnid ) );
	}
JET_ERR JET_API JetOpenTempTable3(
	JET_SESID			sesid,
	const JET_COLUMNDEF	*prgcolumndef,
	unsigned long		ccolumn,
	JET_UNICODEINDEX	*pidxunicode,
	JET_GRBIT			grbit,
	JET_TABLEID			*ptableid,
	JET_COLUMNID		*prgcolumnid )
	{
	JET_TRY( JetOpenTempTableEx( sesid, prgcolumndef, ccolumn, pidxunicode, grbit, ptableid, prgcolumnid ) );
	}


LOCAL JET_ERR JetSetIndexRangeEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opSetIndexRange );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispSetIndexRange(
										sesid,
										tableid,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetIndexRange(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetSetIndexRangeEx( sesid, tableid, grbit ) );
	}


LOCAL JET_ERR JetIndexRecordCountEx(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	*pcrec,
	unsigned long	crecMax )
	{
	APICALL_SESID	apicall( opIndexRecordCount );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDispIndexRecordCount(
										sesid,
										tableid,
										pcrec,
										crecMax ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetIndexRecordCount(
	JET_SESID		sesid,
	JET_TABLEID		tableid,
	unsigned long	*pcrec,
	unsigned long	crecMax )
	{
	JET_TRY( JetIndexRecordCountEx( sesid, tableid, pcrec, crecMax ) );
	}


/***********************************************************
/************* EXTERNAL BACKUP JET API *********************
/*****/
LOCAL JET_ERR JetBeginExternalBackupInstanceEx( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	APICALL_INST	apicall( opBeginExternalBackupInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( apicall.Pinst()->m_fBackupAllowed ?
										ErrIsamBeginExternalBackup( (JET_INSTANCE)apicall.Pinst() , grbit ) :
										ErrERRCheck( JET_errBackupNotAllowedYet ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetBeginExternalBackupInstance( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	JET_TRY( JetBeginExternalBackupInstanceEx( instance, grbit ) );
	}
JET_ERR JET_API JetBeginExternalBackup( JET_GRBIT grbit )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetBeginExternalBackupInstance( (JET_INSTANCE)g_rgpinst[0], grbit );
	}
	

LOCAL JET_ERR JetGetAttachInfoInstanceEx(
	JET_INSTANCE	instance,
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	APICALL_INST	apicall( opGetAttachInfoInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamGetAttachInfo(
										(JET_INSTANCE)apicall.Pinst(),
										pv,
										cbMax,
										pcbActual ) );
		}

	return apicall.ErrResult();
	}	
JET_ERR JET_API JetGetAttachInfoInstance(
	JET_INSTANCE	instance,
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	JET_TRY( JetGetAttachInfoInstanceEx( instance, pv, cbMax, pcbActual ) );
	}	
JET_ERR JET_API JetGetAttachInfo(
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	ERR				err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetGetAttachInfoInstance( (JET_INSTANCE)g_rgpinst[0], pv, cbMax, pcbActual );
	}	


LOCAL JET_ERR JetOpenFileInstanceEx(
	JET_INSTANCE	instance,
	const char		*szFileName,
	JET_HANDLE		*phfFile,
	unsigned long	*pulFileSizeLow,
	unsigned long	*pulFileSizeHigh )
	{
	APICALL_INST	apicall( opOpenFileInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamOpenFile(
										(JET_INSTANCE)apicall.Pinst(),
										szFileName,
										phfFile,
										pulFileSizeLow,
										pulFileSizeHigh ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetOpenFileInstance(
	JET_INSTANCE	instance,
	const char		*szFileName,
	JET_HANDLE		*phfFile,
	unsigned long	*pulFileSizeLow,
	unsigned long	*pulFileSizeHigh )
	{
	JET_TRY( JetOpenFileInstanceEx( instance, szFileName, phfFile, pulFileSizeLow, pulFileSizeHigh ) );
	}

JET_ERR JET_API JetOpenFileSectionInstance(
										JET_INSTANCE ulInstance,
										char *szFile,
										JET_HANDLE *phFile,
										long iSection,
										long cSections,
										unsigned long *pulSectionSizeLow,
										long *plSectionSizeHigh)
	{
	if ( cSections > 1 )
		return JET_wrnNyi;

	return JetOpenFileInstance( ulInstance,
								szFile,
								phFile,
								pulSectionSizeLow,
								(unsigned long *)plSectionSizeHigh );
	};
	
JET_ERR JET_API JetOpenFile(
	const char		*szFileName,
	JET_HANDLE		*phfFile,
	unsigned long	*pulFileSizeLow,
	unsigned long	*pulFileSizeHigh )
	{
	ERR				err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetOpenFileInstance( (JET_INSTANCE)g_rgpinst[0], szFileName, phfFile, pulFileSizeLow, pulFileSizeHigh );
	}


LOCAL JET_ERR JetReadFileInstanceEx(
	JET_INSTANCE	instance,
	JET_HANDLE		hfFile,
	void			*pv,
	unsigned long	cb,
	unsigned long	*pcbActual )
	{
	APICALL_INST	apicall( opReadFileInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamReadFile(
										(JET_INSTANCE)apicall.Pinst(),
										hfFile,
										pv,
										cb,
										pcbActual ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetReadFileInstance(
	JET_INSTANCE	instance,
	JET_HANDLE		hfFile,
	void			*pv,
	unsigned long	cb,
	unsigned long	*pcbActual )
	{
	JET_TRY( JetReadFileInstanceEx( instance, hfFile, pv, cb, pcbActual ) );
	}
JET_ERR JET_API JetReadFile(
	JET_HANDLE		hfFile,
	void			*pv,
	unsigned long	cb,
	unsigned long	*pcbActual )
	{
	ERR				err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetReadFileInstance( (JET_INSTANCE)g_rgpinst[0], hfFile, pv, cb, pcbActual );
	}

LOCAL JET_ERR JetCloseFileInstanceEx( JET_INSTANCE instance, JET_HANDLE hfFile )
	{
	APICALL_INST	apicall( opCloseFileInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamCloseFile(
										(JET_INSTANCE)apicall.Pinst(),
										hfFile ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCloseFileInstance( JET_INSTANCE instance, JET_HANDLE hfFile )
	{
	JET_TRY( JetCloseFileInstanceEx( instance, hfFile ) );
	}
JET_ERR JET_API JetCloseFile( JET_HANDLE hfFile )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetCloseFileInstance( (JET_INSTANCE)g_rgpinst[0], hfFile );
	}


LOCAL JET_ERR JetGetLogInfoInstanceEx(
	JET_INSTANCE	instance,
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual,
	JET_LOGINFO 	*pLogInfo)
	{
	APICALL_INST	apicall( opGetLogInfoInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamGetLogInfo(
										(JET_INSTANCE)apicall.Pinst(),
										pv,
										cbMax,
										pcbActual,
										pLogInfo ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetLogInfoInstance(
	JET_INSTANCE	instance,
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	JET_TRY( JetGetLogInfoInstanceEx( instance, pv, cbMax, pcbActual, NULL ) );
	}
JET_ERR JET_API JetGetLogInfoInstance2(	JET_INSTANCE instance,
										void *pv,
										unsigned long cbMax,
										unsigned long *pcbActual,
										JET_LOGINFO * pLogInfo)
	{
	JET_TRY( JetGetLogInfoInstanceEx( instance, pv, cbMax, pcbActual, pLogInfo ) );
	}
JET_ERR JET_API JetGetLogInfo(
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual )
	{
	ERR				err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetGetLogInfoInstance( (JET_INSTANCE)g_rgpinst[0], pv, cbMax, pcbActual );
	}

LOCAL JET_ERR JetGetTruncateLogInfoInstanceEx(
	JET_INSTANCE	instance,
	void			*pv,
	unsigned long	cbMax,
	unsigned long	*pcbActual)
	{
	APICALL_INST	apicall( opGetTruncateLogInfoInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamGetTruncateLogInfo(
										(JET_INSTANCE)apicall.Pinst(),
										pv,
										cbMax,
										pcbActual ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetTruncateLogInfoInstance(	JET_INSTANCE instance,
												void *pv,
												unsigned long cbMax,
												unsigned long *pcbActual )
	{
	JET_TRY( JetGetTruncateLogInfoInstanceEx( instance, pv, cbMax, pcbActual ) );
	}


LOCAL JET_ERR JetTruncateLogInstanceEx( JET_INSTANCE instance )
	{
	APICALL_INST	apicall( opTruncateLogInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamTruncateLog( (JET_INSTANCE)apicall.Pinst() ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetTruncateLogInstance( JET_INSTANCE instance )
	{
	JET_TRY( JetTruncateLogInstanceEx( instance ) );
	}
JET_ERR JET_API JetTruncateLog( void )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetTruncateLogInstance( (JET_INSTANCE)g_rgpinst[0] );
	}


LOCAL JET_ERR JetEndExternalBackupInstanceEx( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	APICALL_INST	apicall( opEndExternalBackupInstance );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamEndExternalBackup(
										(JET_INSTANCE)apicall.Pinst(),
										grbit ) );
		}

	return apicall.ErrResult();
	}

JET_ERR JET_API JetEndExternalBackupInstance2( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	JET_TRY( JetEndExternalBackupInstanceEx( instance, grbit ) );
	}

	
JET_ERR JET_API JetEndExternalBackupInstance( JET_INSTANCE instance )
	{
	// we had no flag to specify from the client side how the backup ended
	//and we assumed success until now.
	JET_TRY( JetEndExternalBackupInstanceEx( instance, JET_bitBackupEndNormal  ) );
	}
JET_ERR JET_API JetEndExternalBackup( void )
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetEndExternalBackupInstance( (JET_INSTANCE)g_rgpinst[0] );
	}


volatile DWORD g_cRestoreInstance = 0;

	
ERR ErrINSTPrepareTargetInstance( INST * pinstTarget, char * szTargetInstanceLogPath, LONG * plGenHighTargetInstance  )
	{
	ERR				err = JET_errSuccess;
	
	DATA 			rgdata[3];
	LREXTRESTORE 	lrextrestore;
	LGPOS 			lgposLogRec;
	CHAR * 			szTargetInstanceName;
	
	Assert ( szTargetInstanceLogPath );
	Assert ( plGenHighTargetInstance );

	*plGenHighTargetInstance = 0;
	szTargetInstanceLogPath[0] = '\0';

	// if we found an instance, call APIEnter so that it can't get away until we are done with it
	// also this will check if the found instance is initalized and if it is restoring

	// if the target instance is restoring, error out at this point it will be a problem to force a new
	// generation in order to don't conflict on log files. They have to just try later
	
	CallR ( pinstTarget->ErrAPIEnter( fFalse ) );
	
	Assert ( pinstTarget );	
	Assert ( pinstTarget->m_fJetInitialized );
	Assert ( pinstTarget->m_szInstanceName );
			
	// with circular logging we don't try to play forward logs, they are probably missing anyway
	// and we don't want to error out because of that
	if ( pinstTarget->m_plog->m_fLGCircularLogging )
		{
		// set as "no target instance"
		Assert ( 0 == *plGenHighTargetInstance );
		Assert ( '\0' == szTargetInstanceLogPath[0] );

// UNDONE: no changes in the resources are taken at this poin for PT RTM
// so the event log for skipping logs on hard recovery if circular logging is on
// will need to wait until post RTM
#if 0
		Assert ( pinstTarget->m_plog->m_szLogFilePath );
		const CHAR * rgszT[] = { pinstTarget->m_szInstanceName?pinstTarget->m_szInstanceName:"", pinstTarget->m_plog->m_szLogFilePath };

		UtilReportEvent(	eventWarning,
							LOGGING_RECOVERY_CATEGORY,
							RESTORE_CIRCULAR_LOGGING_SKIP_ID,
							2, rgszT);

MessageId=219
SymbolicName=RESTORE_CIRCULAR_LOGGING_SKIP_ID
Language=English
%1 (%2) The running instnace %3 has circular logging turned on. Restore will not attempt to replay the logfiles in folder %4.
%n%nFor more information, click http://www.microsoft.com/contentredirect.asp.
.
#endif // 0
		Assert ( JET_errSuccess == err );
		goto HandleError;
		}

	szTargetInstanceName = pinstTarget->m_szInstanceName?pinstTarget->m_szInstanceName:"";
	
	lrextrestore.lrtyp = lrtypExtRestore;
	
	rgdata[0].SetPv( (BYTE *)&lrextrestore );
	rgdata[0].SetCb( sizeof(LREXTRESTORE) );
	
	rgdata[1].SetPv( (BYTE *) pinstTarget->m_plog->m_szLogFilePath );
	rgdata[1].SetCb( strlen( pinstTarget->m_plog->m_szLogFilePath ) + 1 );
	
	rgdata[2].SetPv( (BYTE *) szTargetInstanceName );
	rgdata[2].SetCb( strlen( szTargetInstanceName ) + 1 );

	lrextrestore.SetCbInfo( USHORT( strlen( pinstTarget->m_plog->m_szLogFilePath ) + 1 + strlen( szTargetInstanceName ) + 1 ) );

	Call( pinstTarget->m_plog->ErrLGLogRec( rgdata, 3, fLGCreateNewGen, &lgposLogRec ) );
	
	while ( lgposLogRec.lGeneration > pinstTarget->m_plog->m_plgfilehdr->lgfilehdr.le_lGeneration )
		{
		if ( pinstTarget->m_plog->m_fLGNoMoreLogWrite )
			{
			Call ( ErrERRCheck( JET_errLogWriteFail ) );
			}
		UtilSleep( cmsecWaitGeneric );
		}

	Assert ( lgposLogRec.lGeneration > 1 );
	*plGenHighTargetInstance = lgposLogRec.lGeneration - 1;
	strcpy( szTargetInstanceLogPath, pinstTarget->m_plog->m_szLogFilePath  );

	CallS( err );
	
HandleError:
	// can't call APILeave() because we are in a critical section
	{
	const LONG	lOld	= AtomicExchangeAdd( &(pinstTarget->m_cSessionInJetAPI), -1 );
	Assert( lOld >= 1 );	
	}

	return err;
	}

// this cristical section is entered then initalizing a restore instance
// We need it because we have 2 steps: check the target instance,
// then based on that complete the init step (after setting the log+system path)
// If we are doing it in a critical section, we may end with 2 restore instances 
// that are finding no running instance and trying to start in the instance directory
// One will error out with LogPath in use instead of "Restore in Progress"
CCriticalSection critRestoreInst( CLockBasicInfo( CSyncBasicInfo( szRestoreInstance ), rankRestoreInstance, 0 ) );


LOCAL JET_ERR JetExternalRestoreEx(
	char			*szCheckpointFilePath,
	char			*szLogPath,
	JET_RSTMAP		*rgrstmap,
	long			crstfilemap,
	char			*szBackupLogPath,
	long			genLow,
	long			genHigh,
	char *			szLogBaseName,
	char *			szTargetInstanceName,
	char *			szTargetInstanceCheckpointPath,
	char *			szTargetInstanceLogPath,
	JET_PFNSTATUS	pfn )
	{
	APICALL_INST	apicall( opInit );
	INST *			pinst = NULL;
	INT				ipinst = ipinstMax;
	ERR				err = JET_errSuccess;

	CHAR *			szRestoreSystemPath = NULL;
	CHAR *			szRestoreLogPath = NULL;

	INST * 			pinstTarget = NULL;
	LONG 			lGenHighTarget;
	CHAR			szTargetLogPath[IFileSystemAPI::cchPathMax];
	
	const BOOL		fTargetName = (NULL != szTargetInstanceName);
	const BOOL		fTargetDirs = (NULL != szTargetInstanceLogPath);

	// used for unique TemDatabase
	CHAR			szTempDatabase[IFileSystemAPI::cchPathMax];

	CHAR * szNewInstanceName = NULL;
	BOOL fInCritInst = fFalse;
	BOOL fInCritRestoreInst = fFalse;

	CCriticalSection *pcritInst = NULL;


	lGenHighTarget = 0;
	szTargetLogPath[0] = '\0';

	// Target Dirs may be both present or both NULL
	if ( (szTargetInstanceLogPath && !szTargetInstanceCheckpointPath ) ||
		( !szTargetInstanceLogPath && szTargetInstanceCheckpointPath ) )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	// can have TarrgetName AND TargetDirs
	if ( szTargetInstanceName && szTargetInstanceLogPath )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}
		
	//	Get a new instance

	// new instance name is "RestoreXXXX"
	szNewInstanceName = new char[strlen(szRestoreInstanceName) + 4 /* will use a 4 digit counter */ + 1];
	if ( NULL == szNewInstanceName )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
		
	sprintf( szNewInstanceName, "%s%04lu", szRestoreInstanceName, AtomicIncrement( (long*)&g_cRestoreInstance ) % 10000L );

	critRestoreInst.Enter();
	fInCritRestoreInst = fTrue;
	
	INST::EnterCritInst();
	fInCritInst = fTrue;

	// not allowed in one instance mode
	if ( runInstModeOneInst == RUNINSTGetMode() )
		{
		Call ( ErrERRCheck ( JET_errRunningInOneInstanceMode ) );
		}
	// force to multi instance mode as we want multiple restore
	else if ( runInstModeNoSet == RUNINSTGetMode() )
		{
		RUNINSTSetModeMultiInst();		
		}

	Assert ( runInstModeMultiInst == RUNINSTGetMode() );

	// we want to run the restore instance using szCheckpointFilePath/szLogPath
	// the Target Instance is running or not provided. If provided but not running,
	// we want to run in the Target instance place (szTargetInstanceCheckpointPath/szTargetInstanceLogPath)


	Assert ( !fTargetDirs || !fTargetName );
		
	if ( !fTargetDirs && !fTargetName )
		{
		szRestoreSystemPath = szCheckpointFilePath;
		szRestoreLogPath = szLogPath;	
		}
	else
		{
		// Target specified by Name XOR Dirs
		Assert ( fTargetDirs ^ fTargetName );
		
		if ( fTargetName )
			{
			pinstTarget = INST::GetInstanceByName( szTargetInstanceName );
			}
		else	
			{
			pinstTarget = INST::GetInstanceByFullLogPath( szTargetInstanceLogPath );
			}

		if ( pinstTarget )
			{
			// Target Instance is Running
			szRestoreSystemPath = szCheckpointFilePath;
			szRestoreLogPath = szLogPath;	

			Assert ( pinstTarget->m_szInstanceName );
			}
		else
			{
			// Target Not Running
			if ( fTargetName )
				{
				// the instance is provided not running and we have just the Instance Name
				// so we can't find out where the log files are. Error out
				Assert ( !fTargetDirs );
				Call ( ErrERRCheck ( JET_errBadRestoreTargetInstance ) );
				}
				
			Assert ( fTargetDirs );
			// instance found and not running, use TargetDirs
			szRestoreSystemPath = szTargetInstanceCheckpointPath;
			szRestoreLogPath = szTargetInstanceLogPath;
			}	
		}
		
	// we have to set the system params first for the new instance
	Assert ( RUNINSTGetMode() == runInstModeMultiInst );

	if ( pinstTarget )
		{
		Call ( ErrINSTPrepareTargetInstance( pinstTarget, szTargetLogPath, &lGenHighTarget ) );
		}
	
	INST::LeaveCritInst();		
	fInCritInst = fFalse;
		
	Call ( ErrNewInst( &pinst, szNewInstanceName, NULL, &ipinst ) );

	// we have to set the system params first
	if ( NULL != szLogBaseName)
		{
		Call ( ErrSetSystemParameter( (JET_INSTANCE)pinst, 0, JET_paramBaseName, 0, szLogBaseName ) );
		}

	// set restore path, we have to do this before ErrIsamExternalRestore (like it was before)
	// because the check against the runnign instances is in ErrInit			

	if ( szRestoreLogPath )
		{
		Call ( ErrSetSystemParameter( (JET_INSTANCE)pinst, 0, JET_paramLogFilePath, 0, szRestoreLogPath ) );
		}

	if ( szRestoreSystemPath )
		{
		Call ( ErrSetSystemParameter( (JET_INSTANCE)pinst, 0, JET_paramSystemPath, 0, szRestoreSystemPath ) );

		// put the tem db in the checkpoint directory
		strcpy( szTempDatabase, szRestoreSystemPath );
		OSSTRAppendPathDelimiter( szTempDatabase, fTrue );
		Call ( ErrSetSystemParameter( (JET_INSTANCE) pinst, 0, JET_paramTempPath, 0, szTempDatabase ) );
		}

	//	Start to do JetRestore on this instance

	pcritInst = &critpoolPinstAPI.Crit(&g_rgpinst[ipinst]);
	pcritInst->Enter();

	if ( !apicall.FEnterForInit( pinst ) )
		{
		pcritInst->Leave();
		err = apicall.ErrResult();
		goto HandleError;
		}
	
	pinst->APILock( pinst->fAPIRestoring );
	pcritInst->Leave();

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );

	err = ErrInit(	pinst, 
					fTrue,		//	fSkipIsamInit
					0 );		//	grbit
	Assert( JET_errAlreadyInitialized != err );

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );

	if ( err >= 0 )
		{
		pinst->m_fJetInitialized = fTrue;

		// now, other restore what will try to do restore on the same instance
		// will error out with "restore in progress" from ErrINSTPrepareTargetInstance (actually ErrAPIEnter)
		// because we set fAPIRestoring AND m_fJetInitialized
		critRestoreInst.Leave();
		fInCritRestoreInst = fFalse;
		
		strcpy( pinst->m_plog->m_szTargetInstanceLogPath , szTargetLogPath );
		pinst->m_plog->m_lGenHighTargetInstance = lGenHighTarget;
		
		err = ErrIsamExternalRestore(
					(JET_INSTANCE) pinst,
					szRestoreSystemPath,
					szRestoreLogPath,
					rgrstmap,
					crstfilemap,
					szBackupLogPath,
					genLow,
					genHigh,
					pfn );

//		OSUTerm();

		}
	else
		{
		critRestoreInst.Leave();
		fInCritRestoreInst = fFalse;
		}
		
	pinst->m_fJetInitialized = fFalse;

	pinst->APIUnlock( pinst->fAPIRestoring );

//	pcritInst->Leave();

	//	Return and delete the instance
	Assert ( !fInCritInst );

	apicall.LeaveAfterCall( err );

	FreePinst( pinst );

	Assert( NULL != szNewInstanceName );
	delete[] szNewInstanceName;

	return apicall.ErrResult();	

HandleError:
	if ( pinst )
		{
		FreePinst( pinst );						
		}

	if ( fInCritInst )
		{
		INST::LeaveCritInst();			
		}

	if ( fInCritRestoreInst )
		{
		critRestoreInst.Leave();
		fInCritRestoreInst = fFalse;
		}			

	if ( szNewInstanceName )
		{
		delete[] szNewInstanceName;
		}
		
	return err;
	}

LOCAL JET_ERR JetExternalRestore2Ex(
	char			*szCheckpointFilePath,
	char			*szLogPath,
	JET_RSTMAP		*rgrstmap,
	long			crstfilemap,
	char			*szBackupLogPath,
	JET_LOGINFO * pLogInfo,
	char *			szTargetInstanceName,
	char *			szTargetInstanceCheckpointPath,
	char *			szTargetInstanceLogPath,
	JET_PFNSTATUS	pfn )
	{
	
	if ( !pLogInfo || pLogInfo->cbSize != sizeof(JET_LOGINFO) )
		{
		return ErrERRCheck( JET_errInvalidParameter );		
		}
		
	return JetExternalRestoreEx(	szCheckpointFilePath,
									szLogPath,
									rgrstmap,
									crstfilemap,
									szBackupLogPath,
									pLogInfo->ulGenLow,
									pLogInfo->ulGenHigh,
									pLogInfo->szBaseName,
									szTargetInstanceName,
									szTargetInstanceCheckpointPath,
									szTargetInstanceLogPath,
									pfn );
	
	}
	
JET_ERR JET_API JetExternalRestore(
	char			*szCheckpointFilePath,
	char			*szLogPath,
	JET_RSTMAP		*rgrstmap,
	long			crstfilemap,
	char			*szBackupLogPath,
	long			genLow,
	long			genHigh,
	JET_PFNSTATUS	pfn )
	{
	JET_TRY( JetExternalRestoreEx( szCheckpointFilePath, szLogPath, rgrstmap, crstfilemap, szBackupLogPath, genLow, genHigh, NULL, NULL, NULL, NULL, pfn ) );
	}

JET_ERR JET_API JetExternalRestore2( 	char *szCheckpointFilePath,
										char *szLogPath,
										JET_RSTMAP *rgrstmap,
										long crstfilemap,
										char *szBackupLogPath,
										JET_LOGINFO * pLogInfo,
										char *szTargetInstanceName,
										char *szTargetInstanceCheckpointPath,
										char *szTargetInstanceLogPath,
										JET_PFNSTATUS pfn )
	{	
	JET_TRY( JetExternalRestore2Ex( szCheckpointFilePath, szLogPath, rgrstmap, crstfilemap, szBackupLogPath, pLogInfo, szTargetInstanceName, szTargetInstanceCheckpointPath, szTargetInstanceLogPath, pfn ) );
	}

LOCAL JET_ERR JetSnapshotStartEx(	JET_INSTANCE 		instance,
									char * 				szDatabases,
									JET_GRBIT			grbit)
	{
	APICALL_INST	apicall( opSnapshotStart );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamSnapshotStart(
										(JET_INSTANCE)apicall.Pinst(),
										szDatabases,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSnapshotStart( 		JET_INSTANCE 		instance,
										char * 				szDatabases,
										JET_GRBIT			grbit)
	{
	JET_TRY( JetSnapshotStartEx( instance, szDatabases, grbit ) );
	}

LOCAL JET_ERR JetSnapshotStopEx(	JET_INSTANCE 		instance,
									JET_GRBIT			grbit)
	{
	APICALL_INST	apicall( opSnapshotStop );

	if ( apicall.FEnter( instance ) )
		{
		apicall.LeaveAfterCall( ErrIsamSnapshotStop(
										(JET_INSTANCE)apicall.Pinst(),
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSnapshotStop( 		JET_INSTANCE 				instance,
										JET_GRBIT 					grbit)
	{
	JET_TRY( JetSnapshotStopEx( instance, grbit ) );
	}


LOCAL JET_ERR JetResetCounterEx( JET_SESID sesid, long CounterType )
	{
	APICALL_SESID	apicall( opResetCounter );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamResetCounter( sesid, CounterType ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetResetCounter( JET_SESID sesid, long CounterType )
	{
	JET_TRY( JetResetCounterEx( sesid, CounterType ) );
	}


LOCAL JET_ERR JetGetCounterEx( JET_SESID sesid, long CounterType, long *plValue )
	{
	APICALL_SESID	apicall( opGetCounter );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamGetCounter(
										sesid,
										CounterType,
										plValue ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGetCounter( JET_SESID sesid, long CounterType, long *plValue )
	{
	JET_TRY( JetGetCounterEx( sesid, CounterType, plValue ) );
	}


LOCAL JET_ERR JetCompactEx(
	JET_SESID		sesid,
	const char		*szDatabaseSrc,
	const char		*szDatabaseDest,
	JET_PFNSTATUS	pfnStatus,
	JET_CONVERT		*pconvert,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opCompact );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamCompact(
										sesid,
										szDatabaseSrc,
										PinstFromSesid( sesid )->m_pfsapi,
										szDatabaseDest,
										NULL,
										pfnStatus,
										pconvert,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetCompact(
	JET_SESID		sesid,
	const char		*szDatabaseSrc,
	const char		*szDatabaseDest,
	JET_PFNSTATUS	pfnStatus,
	JET_CONVERT		*pconvert,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetCompactEx( sesid, szDatabaseSrc, szDatabaseDest, pfnStatus, pconvert, grbit ) );
	}


LOCAL JET_ERR JetConvertDDLEx(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_OPDDLCONV	convtyp,
	void			*pvData,
	unsigned long	cbData )
	{
	ERR 			err;
	APICALL_SESID	apicall( opConvertDDL );

	if ( !apicall.FEnter( sesid ) )
		return apicall.ErrResult();

	PIB * const ppib = (PIB *)sesid;
	Call( ErrPIBCheck( ppib ) );
	Call( ErrPIBCheckUpdatable( ppib ) );
	Call( ErrPIBCheckIfmp( ppib, ifmp ) );

	if( NULL == pvData
		|| 0 == cbData )
		{
		Call( ErrERRCheck( JET_errInvalidParameter ) );
		}
	
	switch( convtyp )
		{
		case opDDLConvAddCallback:
			{
			if( sizeof( JET_DDLADDCALLBACK ) != cbData )
				{
				return ErrERRCheck( JET_errInvalidParameter );
				}
			const JET_DDLADDCALLBACK * const paddcallback = (JET_DDLADDCALLBACK *)pvData;
			const CHAR * const szTable 		= paddcallback->szTable;
			const JET_CBTYP cbtyp 			= paddcallback->cbtyp;
			const CHAR * const szCallback 	= paddcallback->szCallback;
			err = ErrCATAddCallbackToTable( ppib, ifmp, szTable, cbtyp, szCallback );
			}
			break;
			
		case opDDLConvChangeColumn:
			{
			if( sizeof( JET_DDLCHANGECOLUMN ) != cbData )
				{
				return ErrERRCheck( JET_errInvalidParameter );
				}
			const JET_DDLCHANGECOLUMN * const pchangecolumn = (JET_DDLCHANGECOLUMN *)pvData;
			const CHAR * const szTable 	= pchangecolumn->szTable;
			const CHAR * const szColumn = pchangecolumn->szColumn;
			const JET_COLTYP coltyp 	= pchangecolumn->coltypNew;
			const JET_GRBIT grbit 		= pchangecolumn->grbitNew;
			err = ErrCATConvertColumn( ppib, ifmp, szTable, szColumn, coltyp, grbit );
			}
			break;

		case opDDLConvAddConditionalColumnsToAllIndexes:
			{
			if( sizeof( JET_DDLADDCONDITIONALCOLUMNSTOALLINDEXES ) != cbData )
				{
				return ErrERRCheck( JET_errInvalidParameter );
				}
			const JET_DDLADDCONDITIONALCOLUMNSTOALLINDEXES * const paddcondidx 	= (JET_DDLADDCONDITIONALCOLUMNSTOALLINDEXES *)pvData;
			const CHAR * const szTable 											= paddcondidx->szTable;
			const JET_CONDITIONALCOLUMN * const rgconditionalcolumn				= paddcondidx->rgconditionalcolumn;
			const ULONG cConditionalColumn 										= paddcondidx->cConditionalColumn;
			err = ErrCATAddConditionalColumnsToAllIndexes( ppib, ifmp, szTable, rgconditionalcolumn, cConditionalColumn );
			}
			break;
			
		case opDDLConvAddColumnCallback:
			{
			if( sizeof( JET_DDLADDCOLUMNCALLBACK ) != cbData )
				{
				return ErrERRCheck( JET_errInvalidParameter );
				}
			const JET_DDLADDCOLUMNCALLBACK * const paddcolumncallback = (JET_DDLADDCOLUMNCALLBACK *)pvData;
			
			const CHAR * const szTable 			= paddcolumncallback->szTable;
			const CHAR * const szColumn			= paddcolumncallback->szColumn;
			const CHAR * const szCallback 		= paddcolumncallback->szCallback;
			const VOID * const pvCallbackData	= paddcolumncallback->pvCallbackData;
			const unsigned long cbCallbackData 	= paddcolumncallback->cbCallbackData;
			
			err = ErrCATAddColumnCallback( ppib, ifmp, szTable, szColumn, szCallback, pvCallbackData, cbCallbackData );
			}
			break;

		case opDDLConvIncreaseMaxColumnSize:
			{
			if ( sizeof( JET_DDLMAXCOLUMNSIZE ) != cbData )
				{
				return ErrERRCheck( JET_errInvalidParameter );
				}

			const JET_DDLMAXCOLUMNSIZE * const	pmaxcolumnsize	= (JET_DDLMAXCOLUMNSIZE *)pvData;

			const CHAR * const	szTable		= pmaxcolumnsize->szTable;
			const CHAR * const	szColumn	= pmaxcolumnsize->szColumn;
			const ULONG			cbMaxLen	= pmaxcolumnsize->cbMax;

			err = ErrCATIncreaseMaxColumnSize( ppib, ifmp, szTable, szColumn, cbMaxLen );
			break;
			}

		case opDDLConvNull:
		case opDDLConvMax:
		default:
			err = ErrERRCheck( JET_errInvalidParameter );
			break;
		}

HandleError:
	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

JET_ERR JET_API JetConvertDDL(
	JET_SESID		sesid,
	JET_DBID		ifmp,
	JET_OPDDLCONV	convtyp,
	void			*pvData,
	unsigned long	cbData )
	{
	JET_TRY( JetConvertDDLEx( sesid, ifmp, convtyp, pvData, cbData ) );
	}


LOCAL JET_ERR JetUpgradeDatabaseEx(
	JET_SESID		sesid,
	const CHAR		*szDbFileName,
	const CHAR		*szSLVFileName,
	const JET_GRBIT	grbit )
	{
	APICALL_SESID	apicall( opUpgradeDatabase );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrDBUpgradeDatabase(
										sesid,
										szDbFileName,
										szSLVFileName,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetUpgradeDatabase(
	JET_SESID		sesid,
	const CHAR		*szDbFileName,
	const CHAR		*szSLVFileName,
	const JET_GRBIT	grbit )
	{
	JET_TRY( JetUpgradeDatabaseEx( sesid, szDbFileName, szSLVFileName, grbit ) );
	}


INLINE JET_ERR JetIUtilities( JET_SESID sesid, JET_DBUTIL *pdbutil )
	{
	APICALL_SESID	apicall( opDBUtilities );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamDBUtilities( sesid, pdbutil ) );
		}

	return apicall.ErrResult();
	}	

extern "C" LOCAL JET_ERR JetInitEx(	JET_INSTANCE	*pinstance,
									JET_RSTMAP		*rgrstmap,
									long			crstfilemap,
									JET_GRBIT		grbit );

LOCAL JET_ERR JetDBUtilitiesEx( JET_DBUTIL *pdbutil )
	{
	AssertSzRTL( (JET_DBUTIL*)0 != pdbutil, "Invalid (NULL) pdbutil. Call JET dev." );
	AssertSzRTL( (JET_DBUTIL*)(-1) != pdbutil, "Invalid (-1) pdbutil. Call JET dev." );
	JET_ERR			err				= JET_errSuccess;
	JET_INSTANCE	instance		= 0;
	JET_SESID		sesid			= pdbutil->sesid;
	BOOL			fInit			= fFalse;

	if ( pdbutil->cbStruct != sizeof(JET_DBUTIL) )
		return ErrERRCheck( JET_errInvalidParameter );

	// Don't init if we're only dumping the logfile/checkpoint/DB header.
	switch( pdbutil->op )
		{
		case opDBUTILDumpHeader:
		case opDBUTILDumpLogfile:
		case opDBUTILDumpCheckpoint:
		case opDBUTILDumpLogfileTrackNode:
		case opDBUTILDumpPage:
		case opDBUTILDumpNode:
		case opDBUTILSLVMove:
			Call( JetInit( &instance ) );
			fInit = fTrue;
			Call( JetBeginSession( instance, &sesid, "user", "" ) );
			
			Call( ErrIsamDBUtilities( sesid, pdbutil ) );
			break;

		case opDBUTILEDBDump:
		case opDBUTILEDBRepair:
		case opDBUTILEDBScrub:
		case opDBUTILDBConvertRecords:
		case opDBUTILDBDefragment:
			Call( ErrIsamDBUtilities( pdbutil->sesid, pdbutil ) );
			break;

		case opDBUTILDumpData:
		default:
			if ( 0 == sesid || JET_sesidNil == sesid )
				{
				Call( JetInit( &instance ) );
				fInit = fTrue;
				Call( JetBeginSession( instance, &sesid, "user", "" ) );
				}
			
			Call( JetIUtilities( sesid, pdbutil ) );
			break;
		}

HandleError:				
	if ( fInit )
		{
		if( sesid != 0 )
			{
			JetEndSession( sesid, 0 );		
			}
		JetTerm2( instance, err < 0 ? JET_bitTermAbrupt : JET_bitTermComplete );
		}

	return err;		
	}
JET_ERR JET_API JetDBUtilities( JET_DBUTIL *pdbutil )
	{
	JET_TRY( JetDBUtilitiesEx( pdbutil ) );
	}


LOCAL JET_ERR JetDefragmentEx(
	JET_SESID		sesid,
	JET_DBID		dbid,
	const char		*szDatabaseName,
	const char		*szTableName,
	unsigned long	*pcPasses,
	unsigned long  	*pcSeconds,
	JET_CALLBACK	callback,
	void			*pvContext,
	JET_GRBIT		grbit )
	{
	APICALL_SESID	apicall( opDefragment );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamDefragment(
										sesid,
										dbid,
										szTableName,
										pcPasses,
										pcSeconds,
										callback,
										grbit ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetDefragment(
	JET_SESID		vsesid,
	JET_DBID		vdbid,
	const char		*szTableName,
	unsigned long	*pcPasses,
	unsigned long	*pcSeconds,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetDefragmentEx( vsesid, vdbid, NULL, szTableName, pcPasses, pcSeconds, NULL, NULL, grbit ) );
	}
JET_ERR JET_API JetDefragment2(
	JET_SESID		vsesid,
	JET_DBID		vdbid,
	const char		*szTableName,
	unsigned long	*pcPasses,
	unsigned long	*pcSeconds,
	JET_CALLBACK	callback,
	JET_GRBIT		grbit )
	{
	JET_TRY( JetDefragmentEx( vsesid, vdbid, NULL, szTableName, pcPasses, pcSeconds, callback, NULL, grbit ) );
	}


LOCAL JET_ERR JetSetDatabaseSizeEx(
	JET_SESID		sesid,
	const CHAR		*szDatabaseName,
	ULONG			cpg,
	ULONG			*pcpgReal )
	{
	APICALL_SESID	apicall( opSetDatabaseSize );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamSetDatabaseSize(
										sesid,
										szDatabaseName,
										cpg,
										pcpgReal ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetDatabaseSize(
	JET_SESID		vsesid,
	const CHAR		*szDatabaseName,
	ULONG			cpg,
	ULONG			*pcpgReal )
	{
	JET_TRY( JetSetDatabaseSizeEx( vsesid, szDatabaseName, cpg, pcpgReal ) );
	}


LOCAL JET_ERR JetGrowDatabaseEx(
	JET_SESID		sesid,
	JET_DBID		dbid,
	ULONG			cpg,
	ULONG			*pcpgReal )
	{
	APICALL_SESID	apicall( opGrowDatabase );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamGrowDatabase(
										sesid,
										dbid,
										cpg,
										pcpgReal ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetGrowDatabase(
	JET_SESID		vsesid,
	JET_DBID		dbid,
	ULONG			cpg,
	ULONG			*pcpgReal )
	{
	JET_TRY( JetGrowDatabaseEx( vsesid, dbid, cpg, pcpgReal ) );
	}


LOCAL JET_ERR JetSetSessionContextEx( JET_SESID sesid, ULONG_PTR ulContext )
	{
	APICALL_SESID	apicall( opSetSessionContext );

	if ( apicall.FEnter( sesid ) )
		{
		apicall.LeaveAfterCall( ErrIsamSetSessionContext( sesid, ulContext ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetSessionContext( JET_SESID vsesid, ULONG_PTR ulContext )
	{
	JET_TRY( JetSetSessionContextEx( vsesid, ulContext ) );
	}

LOCAL JET_ERR JetResetSessionContextEx( JET_SESID sesid )
	{
	APICALL_SESID	apicall( opResetSessionContext );

	if ( apicall.FEnter( sesid, fTrue ) )
		{
		apicall.LeaveAfterCall( ErrIsamResetSessionContext( sesid ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetResetSessionContext( JET_SESID sesid )
	{
	JET_TRY( JetResetSessionContextEx( sesid ) );
	}


/*=================================================================
JetSetSystemParameter

Description:
  This function sets system parameter values.  It calls ErrSetSystemParameter
  to actually set the parameter values.

Parameters:
  sesid 	is the optional session identifier for dynamic parameters.
  paramid	is the system parameter code identifying the parameter.
  lParam	is the parameter value.
  sz		is the zero terminated string parameter.

Return Value:
  JET_errSuccess if the routine can perform all operations cleanly;
  some appropriate error value otherwise.

Errors/Warnings:
  JET_errInvalidParameter:
	  Invalid parameter code.
  JET_errAlreadyInitialized:
	  Initialization parameter cannot be set after the system is initialized.
  JET_errInvalidSesid:
	  Dynamic parameters require a valid session id.

Side Effects:
  * May allocate memory
=================================================================*/

LOCAL JET_ERR JetSetSystemParameterEx(
	JET_INSTANCE	*pinstance,
	JET_SESID		sesid,
	unsigned long	paramid,
	ULONG_PTR		lParam,
	const char		*sz )
	{
	APICALL_INST	apicall( opSetSystemParameter );
	INST 			*pinst;

	if ( FAllowSetGlobalSysParamAfterStart( paramid ) )
		{
		return ErrSetSystemParameter( 0, 0, paramid, lParam, sz );
		}

	SetPinst( pinstance, sesid, &pinst );

	switch ( RUNINSTGetMode() )
		{
		default:
			Assert( fFalse );
			//	fall through to no current mode:
		case runInstModeNoSet:
			//	setting for global default
			Assert( !sesid || sesid == JET_sesidNil );		
			return ErrSetSystemParameter( 0, 0, paramid, lParam, sz );

		case runInstModeOneInst:
			if ( !sesid || JET_sesidNil == sesid )
				return ErrERRCheck( JET_errInvalidParameter );
			Assert( NULL != pinst );
			break;

		case runInstModeMultiInst:
			// in multi-inst mode, global setting only through JetEnableMultiInstance
			if ( !pinst )
				return ErrERRCheck( JET_errInvalidParameter );
			break;
		}

	Assert( NULL != pinst );

	//	this flag is a hack for logshipping to allow it to abort
	//	UNDONE: instead of this hack, should add a new JetAbortLogshipping() API
	const BOOL		fAllowInitInProgress	= ( JET_paramReplayingReplicatedLogfiles == paramid && !lParam );

	if ( apicall.FEnterWithoutInit( pinst, fAllowInitInProgress ) )
		{
		apicall.LeaveAfterCall( ErrSetSystemParameter(
										(JET_INSTANCE)pinst,
										sesid,
										paramid,
										lParam,
										sz ) );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetSetSystemParameter(
	JET_INSTANCE	*pinstance,
	JET_SESID		sesid,
	ULONG			paramid,
	ULONG_PTR		lParam,
	const char		*sz )
	{
	JET_TRY( JetSetSystemParameterEx( pinstance, sesid, paramid, lParam, sz ) );
	}


ERR ErrInitAlloc( 
	INST **ppinst, 
	const char * szInstanceName = NULL, 
	const char * szDisplayName = NULL,
	int *pipinst = NULL )
	{
	ERR		err = JET_errSuccess;
	
	Assert( ppinst );
	//	Get a new instance
	err = ErrNewInst( ppinst, szInstanceName, szDisplayName, pipinst );	
	if ( err < 0 )
		return err;

	Assert( !(*ppinst)->m_fJetInitialized );
	return err;
	}
	
ERR ErrTermAlloc( JET_INSTANCE instance )
	{
	ERR		err 	= JET_errSuccess;
	INST	*pinst;

	CallR( ErrFindPinst( instance, &pinst ) );
	
	FreePinst( pinst );						
	
	return err;
	}
	

ERR ErrInitComplete(	JET_INSTANCE	instance, 
						JET_RSTMAP		*rgrstmap,
						long			crstfilemap,
						JET_GRBIT		grbit )
	{
	APICALL_INST	apicall( opInit );
	ERR				err;
	INST *			pinst;
	INT				ipinst;

	CallR( ErrFindPinst( instance, &pinst, &ipinst ) );

	//	Enter API for this instance

	CCriticalSection *pcritInst = &critpoolPinstAPI.Crit(&g_rgpinst[ipinst]);
	pcritInst->Enter();

	if ( !apicall.FEnterForInit( pinst ) )
		{
		pcritInst->Leave();
		return apicall.ErrResult();
		}

	pinst->APILock( pinst->fAPIInitializing );
	pcritInst->Leave();

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );
	
	Assert ( err >= 0 );

	Call( pinst->m_plog->ErrLGRSTBuildRstmapForSoftRecovery( rgrstmap, crstfilemap ) );
	
	err = ErrInit(	pinst, 
					fFalse,			//	fSkipIsamInit
					grbit );

	Assert( JET_errAlreadyInitialized != err );

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );

	if ( err >= 0 )
		{
		pinst->m_fJetInitialized = fTrue;

		//	backup allowed only after Jet is properly initialized.

		pinst->m_fBackupAllowed = fTrue;
		}

	Assert( !pinst->m_fTermInProgress );

HandleError:

	pinst->m_plog->LGRSTFreeRstmap();

	pinst->APIUnlock( pinst->fAPIInitializing );

	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}

ERR ErrTermComplete( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	APICALL_INST	apicall( opTerm );
	ERR				err;
	INST *			pinst;
	INT				ipinst;

	CallR( ErrFindPinst( instance, &pinst, &ipinst ) );

	CCriticalSection *pcritInst = &critpoolPinstAPI.Crit(&g_rgpinst[ipinst]);
	pcritInst->Enter();

	if ( !apicall.FEnterForTerm( pinst ) )
		{
		pcritInst->Leave();
		return apicall.ErrResult();
		}

	Assert( !pinst->m_fTermInProgress );
	pinst->m_fTermInProgress = fTrue;

	size_t cpibInJetAPI;
	do
		{
		cpibInJetAPI = 0;
		
		pinst->m_critPIB.Enter();
		for ( PIB* ppibScan = pinst->m_ppibGlobal; ppibScan; ppibScan = ppibScan->ppibNext )
			{
			if ( ppibScan->m_fInJetAPI )
				{
				cpibInJetAPI++;
				}
			}
		pinst->m_critPIB.Leave();

		if ( cpibInJetAPI )
			{
			UtilSleep( cmsecWaitGeneric );
			}
		}
	while ( cpibInJetAPI );

	pinst->APILock( pinst->fAPITerminating );
	pcritInst->Leave();

	Assert( pinst->m_fJetInitialized );

	pinst->m_fBackupAllowed = fFalse;
	if ( grbit & JET_bitTermStopBackup )
		{
		// BUG_125711 (also in jet.h, JET_errBackupAbortByServer added)
		// will close the backup resources only if needed. Will return JET_errNoBackup if no backup.
		CallSx( pinst->m_plog->ErrLGBKIExternalBackupCleanUp( pinst->m_pfsapi, JET_errBackupAbortByServer ), JET_errNoBackup );
		// thats' all for this bug
		
		pinst->m_plog->m_fBackupInProgress = fFalse;
		pinst->m_plog->m_fBackupStatus = LOG::backupStateNotStarted;
		}

	err = ErrIsamTerm( (JET_INSTANCE)pinst, grbit );
	Assert( pinst->m_fJetInitialized );
	
	if ( pinst->m_fSTInit == fSTInitNotDone )	// Is ISAM initialised?
		{
//		OSUTerm();
		
		pinst->m_fJetInitialized = fFalse;
		}

	pinst->m_fTermInProgress = fFalse;

	pinst->APIUnlock( pinst->fAPITerminating );

	apicall.LeaveAfterCall( err );
	return apicall.ErrResult();
	}


/*
Teh runing mode must be "NoSet" then set the mode to multi-instance.
Used to set the global param.

In:
	psetsysparam - array of parameters to set
	csetsysparam - items count in array

Out:
	pcsetsucceed - count of params set w/o error (pointer can de NULL)
	psetsysparam[i].err - set to error code from setting  psetsysparam[i].param
	
Return:	
	JET_errSuccess or first error in array

Obs:
	it stops at the first error
*/
LOCAL JET_ERR JetEnableMultiInstanceEx(
	JET_SETSYSPARAM	*psetsysparam,
	unsigned long	csetsysparam,
	unsigned long	*pcsetsucceed )
	{
	ERR	 			err 			= JET_errSuccess;
	unsigned int 	i;
	unsigned long 	csetsucceed;

	csetsucceed = 0;	
	
	INST::EnterCritInst();

	if ( RUNINSTGetMode() != runInstModeNoSet)
		{
		// function can't be called twice (JET_errSystemParamsAlreadySet) or in One Instance Mode (JET_errRunningInOneInstanceMode)
		Call( ErrERRCheck( RUNINSTGetMode() == runInstModeMultiInst ?
								JET_errSystemParamsAlreadySet :
								JET_errRunningInOneInstanceMode ) );
		}
	
	Assert ( 0 == csetsysparam ||NULL != psetsysparam );
	for (i = 0; i < csetsysparam; i++)
		{
		psetsysparam[i].err = ErrSetSystemParameter(0, 0, psetsysparam[i].paramid, psetsysparam[i].lParam, psetsysparam[i].sz);
		if ( JET_errSuccess > err)
			{
			Call( psetsysparam[i].err );
			}
		csetsucceed++;
		}

	// if setting was successful, set the multi-instance mode
	Assert( err >= JET_errSuccess );
	RUNINSTSetModeMultiInst();

HandleError:		
	// UNDONE: if error: restore the params changed before the error	
	//

	INST::LeaveCritInst();

	if (pcsetsucceed)
		{
		*pcsetsucceed = csetsucceed;
		}
	
	Assert ( err < JET_errSuccess || !pcsetsucceed || csetsysparam == *pcsetsucceed );
	return err;
	}
	
JET_ERR JET_API JetEnableMultiInstance(
	JET_SETSYSPARAM	*psetsysparam,
	unsigned long	csetsysparam,
	unsigned long	*pcsetsucceed )
	{
	JET_TRY( JetEnableMultiInstanceEx( psetsysparam, csetsysparam, pcsetsucceed ) );
	}


LOCAL JET_ERR JetInitEx(
	JET_INSTANCE *	pinstance,
	JET_RSTMAP		*rgrstmap,
	LONG			crstfilemap,
	JET_GRBIT		grbit )
	{
	ERR				err 				= JET_errSuccess;
	BOOL			fAllocateInstance 	= ( NULL == pinstance || FINSTInvalid( *pinstance ) );
	INST *			pinst;

	if ( fAllocateInstance && NULL != pinstance )
		*pinstance = JET_instanceNil;			

	INST::EnterCritInst();
		
	// instance value to be returned must be provided in multi-instance
	if ( RUNINSTGetMode() == runInstModeMultiInst && !pinstance )
		{
		INST::LeaveCritInst();
		return ErrERRCheck( JET_errRunningInMultiInstanceMode );
		}
		
	// just one instance accepted in one instance mode	
	if ( RUNINSTGetMode() == runInstModeOneInst && 0 != ipinstMac )
		{
		INST::LeaveCritInst();
		return ErrERRCheck( JET_errAlreadyInitialized ); // one instance already started
		}

	// set the instance mode to multi-instance if not set. That means:
	// no previous call to JetSetSystemParam, JetEnableMultiInstance, JetCreateInstance, JetInit 
	// or all the previous started instances are no longer active
	if ( RUNINSTGetMode() == runInstModeNoSet )
		{
		Assert ( 0 == ipinstMac);
//		RUNINSTSetModeMultiInst();
		RUNINSTSetModeOneInst();
		}
		
	// in one instance mode, allocate instance even if pinstance (and *pinstance) not null
	// because this values can be bogus
	fAllocateInstance = fAllocateInstance | (RUNINSTGetMode() == runInstModeOneInst);
	
	INST::LeaveCritInst();

	// alocate a new instance or find the one provided (previously allocate with JetCreateInstance)
	if ( fAllocateInstance )
		{
		Call( ErrInitAlloc( &pinst ) );
		}
	else
		{
		Call( ErrFindPinst( *pinstance, &pinst ) );		
		}

	// make the initialization

	CallJ( ErrInitComplete(	JET_INSTANCE( pinst ), 
							rgrstmap, 
							crstfilemap, 
							grbit ), TermAlloc );

	Assert( err >= 0 );
	
	if ( fAllocateInstance && pinstance )
		*pinstance = (JET_INSTANCE) pinst;			
		
	return err;

TermAlloc:
	Assert( err < 0 );
	// if instance allocated in this function call
	// or created by create instance, clean it
	if ( fAllocateInstance || ( NULL != pinstance && !FINSTInvalid( *pinstance ) ) )
		{
		ErrTermAlloc( (JET_INSTANCE)pinst );
		if ( NULL != pinstance )
			{	
			*pinstance = NULL;
			}
		}

HandleError:
	Assert( err < 0 );
	return err;
	}
JET_ERR JET_API JetInit( JET_INSTANCE *pinstance )
	{
	return JetInit2( pinstance, NO_GRBIT );
	}

JET_ERR JET_API JetInit2( JET_INSTANCE *pinstance, JET_GRBIT grbit )
	{
	JET_TRY( JetInitEx( pinstance, NULL, 0, grbit ) );
	}

JET_ERR JET_API JetInit3( 
	JET_INSTANCE *pinstance, 
	JET_RSTMAP *rgrstmap,
	long crstfilemap,
	JET_GRBIT grbit )
	{
	JET_TRY( JetInitEx( pinstance, rgrstmap, crstfilemap, grbit ) );
	}



/*
Used to allocate an instance. This allowes to change 
the instance parameters before calling JetInit for that instance.
The system params for this instance are set to the global ones
*/
LOCAL JET_ERR JetCreateInstanceEx( 
	JET_INSTANCE *pinstance, 
	const char * szInstanceName,
	const char * szDisplayName )
	{
	ERR			err 	= JET_errSuccess;
	INST *		pinst;

	// the allocated instance must be returned
	if ( !pinstance )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	Assert ( pinstance );	
	*pinstance = JET_instanceNil;			

	INST::EnterCritInst();

	// not allowed in one instance mode
	if ( RUNINSTGetMode() == runInstModeOneInst )
		{
		INST::LeaveCritInst();
		return ErrERRCheck( JET_errRunningInOneInstanceMode );
		}
		
	// set to multi-instance mode if not set
	if ( RUNINSTGetMode() == runInstModeNoSet)
		{
		RUNINSTSetModeMultiInst();
		}
	INST::LeaveCritInst();

	Assert ( RUNINSTGetMode() == runInstModeMultiInst );
	
	err = ErrInitAlloc( &pinst, szInstanceName, szDisplayName );

	if (err >= JET_errSuccess)
		{
		Assert ( pinstance );	
		*pinstance = (JET_INSTANCE) pinst;			
		}
	
	return err;
	}
	
JET_ERR JET_API JetCreateInstance( 
	JET_INSTANCE *pinstance, 
	const char * szInstanceName)
	{
	return JetCreateInstance2( pinstance, szInstanceName, NULL, NO_GRBIT );	
	}

// Note: grbit is currently unused
JET_ERR JET_API JetCreateInstance2( 
	JET_INSTANCE *pinstance, 
	const char * szInstanceName,
	const char * szDisplayName,
	JET_GRBIT grbit )
	{
	JET_TRY( JetCreateInstanceEx( pinstance, szInstanceName, szDisplayName ) );
	}


LOCAL JET_ERR JetRestoreInstanceEx(
	JET_INSTANCE	instance,
	const char		*sz,
	const char		*szDest,
	JET_PFNSTATUS	pfn )
	{
	APICALL_INST	apicall( opInit );
	ERR				err;
	BOOL			fAllocateInstance 	= FINSTInvalid( instance ) ;
	INST			*pinst;
	INT				ipinst;
	CCriticalSection *pcritInst;

	if ( fAllocateInstance )
		{
		CallR( ErrInitAlloc( &pinst, szRestoreInstanceName, NULL, &ipinst ) );
		}
	else
		{
		CallR( ErrFindPinst( instance, &pinst, &ipinst ) );		
		}

	pcritInst = &critpoolPinstAPI.Crit(&g_rgpinst[ipinst]);
	pcritInst->Enter();

	if ( !apicall.FEnterForInit( pinst ) )
		{
		pcritInst->Leave();
		if ( fAllocateInstance )
			{
			ErrTermAlloc( (JET_INSTANCE) pinst );
			}
		return apicall.ErrResult();
		}

	pinst->APILock( pinst->fAPIRestoring );
	pcritInst->Leave();

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );

	err = ErrInit(	pinst, 
					fTrue,		//	fSkipIsamInit
					0 );

	Assert( JET_errAlreadyInitialized != err );

	Assert( !pinst->m_fJetInitialized );
	Assert( !pinst->m_fTermInProgress );

	if ( err >= 0 )
		{
		pinst->m_fJetInitialized = fTrue;

		err = ErrIsamRestore( (JET_INSTANCE) pinst, (char *)sz, (char *)szDest, pfn );

//		OSUTerm();

		pinst->m_fJetInitialized = fFalse;
		}

	pinst->APIUnlock( pinst->fAPIRestoring );

	apicall.LeaveAfterCall( err );

	if ( fAllocateInstance )
		{
		FreePinst( pinst );
		}

	return apicall.ErrResult();
	}
JET_ERR JET_API JetRestoreInstance( JET_INSTANCE instance, const char *sz, const char *szDest, JET_PFNSTATUS pfn )
	{
	JET_TRY( JetRestoreInstanceEx( instance, sz, szDest, pfn ) );
	}

LOCAL JET_ERR JetTermEx( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	ERR		err 	= JET_errSuccess;
	INST	*pinst 	= NULL;

	CallR( ErrFindPinst( instance, &pinst ) );

	AtomicIncrement( (LONG *)&g_cTermsInProgress );

	if ( pinst->m_fJetInitialized ) 
		{
		err = ErrTermComplete( instance, grbit );
		}

	if ( pinst->m_fSTInit == fSTInitNotDone )
		{
		ERR errT;
		errT = ErrTermAlloc( instance );
		Assert ( JET_errSuccess == errT );
		}

	AtomicDecrement( (LONG *)&g_cTermsInProgress );

	return err;
	}
	
JET_ERR JET_API JetTerm( JET_INSTANCE instance )
	{
	JET_TRY( JetTermEx( instance, JET_bitTermAbrupt ) );
	}
JET_ERR JET_API JetTerm2( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	JET_TRY( JetTermEx( instance, grbit ) );
	}


JET_ERR JET_API JetStopServiceInstanceEx( JET_INSTANCE instance )
	{
	ERR		err;
	INST	*pinst;

	CallR( ErrFindPinst( instance, &pinst ) );

	pinst->m_fStopJetService = fTrue;
	return JET_errSuccess;
	}
JET_ERR JET_API JetStopServiceInstance( JET_INSTANCE instance )
	{
	JET_TRY( JetStopServiceInstanceEx( instance ) );
	}
JET_ERR JET_API JetStopService()
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetStopServiceInstance( (JET_INSTANCE)g_rgpinst[0] );
	}


LOCAL JET_ERR JetStopBackupInstanceEx( JET_INSTANCE instance )
	{
	ERR		err;
	INST	*pinst;

	CallR( ErrFindPinst( instance, &pinst ) );

	if ( pinst->m_plog )
		{
		pinst->m_plog->m_fBackupInProgress = fFalse;
		pinst->m_plog->m_fBackupStatus = LOG::backupStateNotStarted;
		}

	return JET_errSuccess;
	}
JET_ERR JET_API JetStopBackupInstance( JET_INSTANCE instance )
	{
	JET_TRY( JetStopBackupInstanceEx( instance ) );
	}
JET_ERR JET_API JetStopBackup()
	{
	ERR		err;

	CallR( ErrRUNINSTCheckAndSetOneInstMode() );

	return JetStopBackupInstance( (JET_INSTANCE)g_rgpinst[0] ) ;
	}
	
LOCAL JET_ERR JetGetInstanceInfoEx( unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo, const BOOL fSnapshot = fFalse )
	{
	return ErrIsamGetInstanceInfo( pcInstanceInfo, paInstanceInfo );
	}
	
JET_ERR JET_API JetGetInstanceInfo( unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo )
	{
	JET_TRY( JetGetInstanceInfoEx( pcInstanceInfo, paInstanceInfo ) );
	}
	
JET_ERR JET_API JetFreeBuffer(
	char *pbBuf )
	{
	// use to free buffers returned by JetGetInstanceInfo
	OSMemoryHeapFree( (void* const)pbBuf );
	return JET_errSuccess;
	};


LOCAL JET_ERR JetOSSnapshotPrepareEx( JET_OSSNAPID * psnapId, const JET_GRBIT grbit )
	{
	return ErrIsamOSSnapshotPrepare( psnapId, grbit );
	}

JET_ERR JET_API JetOSSnapshotPrepare( JET_OSSNAPID * psnapId, const JET_GRBIT grbit )
	{
	JET_TRY( JetOSSnapshotPrepareEx( psnapId, grbit ) );
	}


LOCAL JET_ERR JetOSSnapshotFreezeEx( const JET_OSSNAPID snapId, unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo, const JET_GRBIT grbit )
	{
	return ErrIsamOSSnapshotFreeze( snapId, pcInstanceInfo, paInstanceInfo, grbit );
	}

JET_ERR JET_API JetOSSnapshotFreeze( const JET_OSSNAPID snapId, unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo, const JET_GRBIT grbit )
	{
	JET_TRY( JetOSSnapshotFreezeEx( snapId, pcInstanceInfo, paInstanceInfo, grbit ) );
	}


LOCAL JET_ERR JetOSSnapshotThawEx( const JET_OSSNAPID snapId, const JET_GRBIT grbit )
	{
	return ErrIsamOSSnapshotThaw( snapId, grbit );
	}
	
JET_ERR JET_API JetOSSnapshotThaw( const JET_OSSNAPID snapId, const JET_GRBIT grbit )
	{
	JET_TRY( JetOSSnapshotThawEx( snapId, grbit ) );
	}

}	// extern "C"
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\io.cxx ===
#include "std.hxx"


//  perfmon support
PM_CEF_PROC LIODatabaseFileExtensionStallCEFLPv;
PERFInstanceG<> cIODatabaseFileExtensionStall;
long LIODatabaseFileExtensionStallCEFLPv( long iInstance, void* pvBuf )
	{
	cIODatabaseFileExtensionStall.PassTo( iInstance, pvBuf );
	return 0;
	}


/******************************************************************/
/*				FMP Routines			                          */
/******************************************************************/

FMP	*	rgfmp		= NULL;						/* database file map */
IFMP	ifmpMax		= cMaxInstancesSingleInstanceDefault * cMaxDatabasesPerInstanceDefault;

CCriticalSection FMP::critFMPPool( CLockBasicInfo( CSyncBasicInfo( szFMPGlobal ), rankFMPGlobal, 0 ) );

/*	The folowing functions only deal with the following fields in FMP
 *		critFMPPool & pfmp->szDatabaseName	- protect fmp id (szDatabaseName field)
 *		pfmp->critLatch - protect any fields except szDatabaseName.
 *		pfmp->fWriteLatch
 *		pfmp->gateWriteLatch
 *		pfmp->cPin - when getting fmp pointer for read, the cPin must > 0
 */

VOID FMP::GetWriteLatch( PIB *ppib )
	{
	//	At least pinned (except dbidTemp which always sit
	//	in memory) or latched by the caller.

	Assert( ( m_cPin || Dbid() == dbidTemp ) || this->FWriteLatchByMe(ppib) );

	m_critLatch.Enter();
	
Start:
	Assert( m_critLatch.FOwner() );
	if ( !( this->FWriteLatchByOther(ppib) ) )
		{
		this->SetWriteLatch( ppib );
		m_critLatch.Leave();
		}
	else
		{
		m_gateWriteLatch.Wait( m_critLatch );
		m_critLatch.Enter();
		goto Start;
		}
	}


VOID FMP::ReleaseWriteLatch( PIB *ppib )
	{
	m_critLatch.Enter();

	/*	Free write latch.
	 */
	this->ResetWriteLatch( ppib );
	if ( m_gateWriteLatch.CWait() > 0 )
		{
		m_gateWriteLatch.Release( m_critLatch );
		}
	else
		{
		m_critLatch.Leave();
		}
	}


//  ================================================================
ERR FMP::RegisterTask()
//  ================================================================
	{
	AtomicIncrement( (LONG *)&m_ctasksActive );
	return JET_errSuccess;
	}

	
//  ================================================================
ERR FMP::UnregisterTask()
//  ================================================================
	{
	AtomicDecrement( (LONG *)&m_ctasksActive );
	return JET_errSuccess;
	}

	
//  ================================================================
ERR FMP::ErrWaitForTasksToComplete()
//  ================================================================
	{
	//  very ugly, but hopefully we don't detach often
	while( 0 != m_ctasksActive )
		{
		UtilSleep( cmsecWaitGeneric );
		}
	return JET_errSuccess;
	}


/*	ErrFMPWriteLatchByNameSz returns the ifmp of the database with the
 *	given name if return errSuccess, otherwise, it return DatabaseNotFound.
 */
ERR FMP::ErrWriteLatchByNameSz( const CHAR *szFileName, IFMP *pifmp, PIB *ppib )
	{
	IFMP	ifmp;
	BOOL	fFound = fFalse;

Start:
	critFMPPool.Enter();
	for ( ifmp = IfmpMinInUse(); ifmp <= IfmpMacInUse(); ifmp++ )
		{
		FMP *pfmp = &rgfmp[ ifmp ];

		if ( pfmp->FInUse()
			&& UtilCmpFileName( szFileName, pfmp->SzDatabaseName() ) == 0 &&
			// there can be 2 FMP's with same name recovering
			pfmp->Pinst() == PinstFromPpib( ppib ) )
			{
			pfmp->CritLatch().Enter();
			if ( pfmp->FWriteLatchByOther(ppib) )
				{
				/*	found an entry and the entry is write latched.
				 */
				critFMPPool.Leave();
				pfmp->GateWriteLatch().Wait( pfmp->CritLatch() );
				goto Start;
				}

			/*	found an entry and the entry is not write latched.
			 */
			*pifmp = ifmp;
			pfmp->SetWriteLatch( ppib );
			pfmp->CritLatch().Leave();
			fFound = fTrue;
			break;
			}
		}

	critFMPPool.Leave();

	return ( fFound ? JET_errSuccess : ErrERRCheck( JET_errDatabaseNotFound ) );
	}

IFMP FMP::ErrSearchAttachedByNameSz( CHAR *szFileName )
	{
	IFMP	ifmp;
	
	critFMPPool.Enter();
	for ( ifmp = IfmpMinInUse(); ifmp <= IfmpMacInUse(); ifmp++ )
		{
		FMP *pfmp = &rgfmp[ ifmp ];

		if ( pfmp->FInUse()
			&& UtilCmpFileName( szFileName, pfmp->SzDatabaseName() ) == 0 
			&& pfmp->FAttached() )
			{
			Assert ( !pfmp->FSkippedAttach() );
			critFMPPool.Leave();
			return ifmp;
			}
		}

	critFMPPool.Leave();

	// not found
	return ( ifmpMax );
	}



ERR FMP::ErrWriteLatchBySLVNameSz( CHAR *szSLVFileName, IFMP *pifmp, PIB *ppib )
	{
	IFMP	ifmp;
	BOOL	fFound = fFalse;

Start:
	critFMPPool.Enter();
	for ( ifmp = IfmpMinInUse(); ifmp <= IfmpMacInUse(); ifmp++ )
		{
		FMP *pfmp = &rgfmp[ ifmp ];

		if ( pfmp->FInUse() && pfmp->SzSLVName()
			&& UtilCmpFileName( szSLVFileName, pfmp->SzSLVName() ) == 0 &&
			// there can be 2 FMP's with same name recovering
			pfmp->Pinst() == PinstFromPpib( ppib ) )
			{
			pfmp->CritLatch().Enter();
			if ( pfmp->FWriteLatchByOther(ppib) )
				{
				/*	found an entry and the entry is write latched.
				 */
				critFMPPool.Leave();
				pfmp->GateWriteLatch().Wait( pfmp->CritLatch() );
				goto Start;
				}

			/*	found an entry and the entry is not write latched.
			 */
			*pifmp = ifmp;
			pfmp->SetWriteLatch( ppib );
			pfmp->CritLatch().Leave();
			fFound = fTrue;
			break;
			}
		}

	critFMPPool.Leave();

	return ( fFound ? JET_errSuccess : ErrERRCheck( JET_errDatabaseNotFound ) );
	}



ERR FMP::ErrWriteLatchByIfmp( IFMP ifmp, PIB *ppib )
	{
	FMP *pfmp = &rgfmp[ ifmp ];

Start:
	critFMPPool.Enter();

	//	If no identity for this FMP, return.

	if ( !pfmp->FInUse() || PinstFromPpib( ppib ) != pfmp->Pinst() )
		{
		critFMPPool.Leave();
		return ErrERRCheck( JET_errDatabaseNotFound );
		}

	//	check if it is latchable.

	pfmp->CritLatch().Enter();
	if ( pfmp->FWriteLatchByOther(ppib) )
		{
		/*	found an entry and the entry is write latched.
		 */
		critFMPPool.Leave();
		pfmp->GateWriteLatch().Wait( pfmp->CritLatch() );
		goto Start;
		}

	//	found an entry and the entry is not write latched.

	pfmp->SetWriteLatch( ppib );
	pfmp->CritLatch().Leave();

	critFMPPool.Leave();

	return JET_errSuccess;
	}


VOID FMP::GetExtendingDBLatch()
	{
	//	At least pinned by the caller.

	cIODatabaseFileExtensionStall.Inc( m_pinst );

	//  wait to own extending the database

	SemExtendingDB().Acquire();
	}


VOID FMP::ReleaseExtendingDBLatch()
	{
	//  release ownership of extending the database

	SemExtendingDB().Release();
	}


VOID FMP::GetShortReadLatch( PIB *ppib )
	{
Start:
	this->CritLatch().Enter();
	Assert( this->CritLatch().FOwner() );
	if ( ( this->FWriteLatchByOther(ppib) ) )
		{
		this->GateWriteLatch().Wait( this->CritLatch() );
		goto Start;
		}

	return;
	}


VOID FMP::ReleaseShortReadLatch()
	{
	this->CritLatch().Leave();
	}

LOCAL VOID FMPIBuildRelocatedDbPath(
	IFileSystemAPI * const	pfsapi,
	const CHAR * const		szOldPath,
	const CHAR * const		szNewDir,
	CHAR * const			szNewPath )
	{
	CHAR					szDirT[ IFileSystemAPI::cchPathMax ];
	CHAR					szFileBaseT[ IFileSystemAPI::cchPathMax ];
	CHAR					szFileExtT[ IFileSystemAPI::cchPathMax ];

	CallS( pfsapi->ErrPathParse(
						szOldPath,
						szDirT,
						szFileBaseT,
						szFileExtT ) );

	CallS( pfsapi->ErrPathBuild(
						szNewDir,
						szFileBaseT,
						szFileExtT,
						szNewPath ) );
	}

ERR FMP::ErrStoreDbAndSLVNames(
	IFileSystemAPI * const	pfsapi,
	const CHAR *			szDbName,
	const CHAR *			szSLVName,
	const CHAR *			szSLVRoot,
	const BOOL				fValidatePaths )
	{
	ERR						err;
	CHAR 	 				rgchDbFullName[IFileSystemAPI::cchPathMax];
	CHAR  					rgchSLVFullName[IFileSystemAPI::cchPathMax];
	CHAR *					szRelocatedDb	= NULL;
	CHAR *					szRelocatedSLV	= NULL;

	Assert( NULL != szDbName );
	Assert( NULL == szSLVRoot || NULL != szSLVName );

	if ( g_fAlternateDbDirDuringRecovery )
		{
		FMPIBuildRelocatedDbPath(
					pfsapi,
					szDbName,
					g_szAlternateDbDirDuringRecovery,
					rgchDbFullName );
		szRelocatedDb = rgchDbFullName;

		if ( NULL != szSLVName )
			{
			FMPIBuildRelocatedDbPath(
						pfsapi,
						szSLVName,
						g_szAlternateDbDirDuringRecovery,
						rgchSLVFullName );
			szRelocatedSLV = rgchSLVFullName;
			}
		}

	else if ( fValidatePaths )
		{
		err = pfsapi->ErrPathComplete( szDbName, rgchDbFullName );
		CallR( err == JET_errInvalidPath ? ErrERRCheck( JET_errDatabaseInvalidPath ) : err );
		szDbName = rgchDbFullName;

		if ( NULL != szSLVName )
			{
			err = pfsapi->ErrPathComplete( szSLVName, rgchSLVFullName );
			CallR( JET_errInvalidPath == err ? ErrERRCheck( JET_errSLVInvalidPath ) : err );
			szSLVName = rgchSLVFullName;
			}
		}

	const SIZE_T	cbRelocatedDb	= ( g_fAlternateDbDirDuringRecovery ? strlen( szRelocatedDb ) + 1 : 0 );
	const SIZE_T	cbRelocatedSLV	= ( NULL != szSLVName && g_fAlternateDbDirDuringRecovery ? strlen( szRelocatedSLV ) + 1 : 0 );
	const SIZE_T	cbDbName		= cbRelocatedDb + strlen( szDbName ) + 1;
	const SIZE_T	cbSLVName		= cbRelocatedSLV + ( NULL != szSLVName ? strlen( szSLVName ) + 1 : 0 );
	const SIZE_T	cbSLVRoot		= ( NULL != szSLVRoot ? strlen( szSLVRoot ) + 1 : 0 );
	const SIZE_T	cbAllocate		= cbRelocatedDb + cbDbName + cbRelocatedSLV + cbSLVName + cbSLVRoot;
	CHAR *			pch				= static_cast<CHAR *>( PvOSMemoryHeapAlloc( cbAllocate ) );
	CHAR *			pchNext			= pch;

	if ( NULL == pch )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	Assert( NULL == SzDatabaseName() );
	Assert( NULL == SzSLVName() );
	Assert( NULL == SzSLVRoot() );

	SetSzDatabaseName( pch );

	if ( g_fAlternateDbDirDuringRecovery )
		{
		strcpy( pchNext, szRelocatedDb );
		pchNext += cbRelocatedDb;
		}

	strcpy( pchNext, szDbName );
	pchNext += cbDbName;

	if ( NULL != szSLVName )
		{
		SetSzSLVName( pchNext );

		if ( g_fAlternateDbDirDuringRecovery )
			{
			strcpy( pchNext, szRelocatedSLV );
			pchNext += cbRelocatedSLV;
			}

		strcpy( pchNext, szSLVName );
		pchNext += cbSLVName;

		Assert( SzSLVName() > pch );
		Assert( NULL != szSLVRoot );
		Assert( pchNext < pch + cbAllocate );
		}
	else
		{
		SetSzSLVName( NULL );
		Assert( NULL == szSLVRoot );
		Assert( 0 == cbSLVRoot );
		Assert( pch + cbAllocate == pchNext );
		}

	if ( NULL != szSLVRoot )
		{
		Assert( NULL != szSLVName );
		Assert( cbSLVName > 0 );
		SetSzSLVRoot( pchNext );
		strcpy( SzSLVRoot(), szSLVRoot );
		Assert( SzSLVRoot() > pch );
		Assert( SzSLVRoot() + cbSLVRoot == pch + cbAllocate );
		}
	else
		{
		SetSzSLVRoot( NULL );
		Assert( NULL == szSLVName );
		Assert( 0 == cbSLVRoot );
		Assert( pch + cbAllocate == pchNext );
		}
		
	return JET_errSuccess;
	}


ERR FMP::ErrCopyAtchchk()
	{
	if ( NULL == PatchchkRestored() )
		{
		ATCHCHK *patchchkRestored;

		patchchkRestored = static_cast<ATCHCHK *>( PvOSMemoryHeapAlloc( sizeof( ATCHCHK ) ) );
		if ( NULL == patchchkRestored )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
		memset( patchchkRestored, 0, sizeof( ATCHCHK ) );
		SetPatchchkRestored( patchchkRestored );
		}

	*PatchchkRestored() = *Patchchk();

	return JET_errSuccess;
	}


IFMP FMP::ifmpMinInUse = ifmpMax;
IFMP FMP::ifmpMacInUse = 0;
/*
 *	NewAndWriteLatch 
 *		IFMP *pifmp				returned fmp entry
 *		CHAR *szDatabaseName	used to check if an entry has the same database name and instance
 *		PIB ppib				used to set the latch
 *		INST *pinst				instance that request this latch
 *
 *	This function look for an entry for setting up the database of the given instance.
 *	During recovery, we may allocate one entry and fill the database name. In that case,
 *	the database name is null, and ppib is ppibSurrogate.
 */
ERR FMP::ErrNewAndWriteLatch(
	IFMP						*pifmp,
	const CHAR					*szDatabaseName,
	const CHAR					*szSLVName,
	const CHAR					*szSLVRoot,
	PIB							*ppib,
	INST						*pinst,
	IFileSystemAPI *const	pfsapi,
	DBID						dbidGiven )
	{
	ERR			err				= JET_errSuccess;
	IFMP		ifmp;
	CHAR		szRestoreDestDbName = NULL;
	BOOL		fSkippedAttach 	= fFalse;
	BOOL		fDeferredAttach	= fFalse;

	Assert( NULL != szDatabaseName );

	//	verify that the database and SLV file are NOT the same file

	if ( szSLVName != NULL )
		{
		if ( UtilCmpFileName( szDatabaseName, szSLVName ) == 0 )
			{
			//	they are the same file -- the database wins and SLV file loses
			return ErrERRCheck( JET_errSLVStreamingFileInUse );
			}
		}

	// if we are restoring in a different location that the usage collison check must be
	// done between existing entries in fmp array and the new destination name.
	if ( pinst->FRecovering() )
		{
		Assert( pinst->m_plog );
		LOG *plog = pinst->m_plog;
		
		// build the destination file name
		if ( plog->m_fExternalRestore || ( plog->m_irstmapMac > 0 && !plog->m_fHardRestore ) )
			{
			INT  irstmap = plog->IrstmapLGGetRstMapEntry( szDatabaseName );
			if ( 0 > plog->IrstmapSearchNewName( szDatabaseName ) )
				{
				fSkippedAttach = fTrue;
				}
			}
		}

	//	lock the IFMP pool
	
	critFMPPool.Enter();

	//	look for unused file map entry
	//	and make sure that the SLV file is not in use by anyone else
	//	except the matching database

	for ( ifmp = IfmpMinInUse(); ifmp <= IfmpMacInUse(); ifmp++ )
		{
		FMP	*pfmp = &rgfmp[ ifmp ];

		if ( pfmp->FInUse() )
			{
			if ( 0 == UtilCmpFileName( pfmp->SzDatabaseName(), szDatabaseName ) )
				{
				if ( pfmp->Pinst() == pinst )
					{
					pfmp->CritLatch().Enter();
					
					if ( pfmp->FAttached()
						|| pfmp->FWriteLatchByOther( ppib )
						|| pfmp->CPin() )
						{
						err = ErrERRCheck( JET_wrnDatabaseAttached );
						}
					else
						{
						//	Same database, different stream file in use?
						if ( ( NULL == szSLVName && NULL != pfmp->SzSLVName() )
							|| ( NULL != szSLVName
								&& ( NULL == pfmp->SzSLVName()
									|| 0 != UtilCmpFileName( szSLVName, pfmp->SzSLVName() ) ) ) )
							{
							Call( ErrERRCheck( JET_errSLVStreamingFileInUse ) );
							}

						pfmp->SetWriteLatch( ppib );
						Assert( !( pfmp->FExclusiveOpen() ) );
						*pifmp = ifmp;
						CallS( err );
						}

					pfmp->CritLatch().Leave();
					}
				else
					{
					// check if one of the 2 members just compared is in SkippedAttach mode
					if ( fSkippedAttach
						|| pfmp->FSkippedAttach() )
						{
						continue;
						}
					else if ( pinst->FRecovering() )
						{
						fDeferredAttach = fTrue;
						continue;
						}
					
					err = ErrERRCheck( JET_errDatabaseSharingViolation );
					}
				
				goto HandleError;
				}
			//	Different databases, check against stream files
			else
				{
				if (	NULL != szSLVName && 
						(	0 == UtilCmpFileName( pfmp->SzDatabaseName(), szSLVName ) ||
							(	NULL != pfmp->SzSLVName() && 
								0 == UtilCmpFileName( szSLVName, pfmp->SzSLVName() ) ) ) )
					{
					Call( ErrERRCheck( JET_errSLVStreamingFileInUse ) );
					}
				if ( NULL != pfmp->SzSLVName() && UtilCmpFileName( szDatabaseName, pfmp->SzSLVName() ) == 0 )
					{
					Call( ErrERRCheck( JET_errDatabaseSharingViolation ) );
					}
				}
			}
		}

	//	Allocate entry from pinst->m_mpdbidifmp

	if ( dbidGiven >= dbidMax )
		{
		DBID dbid;
		
		for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( pinst->m_mpdbidifmp[ dbid ] >= ifmpMax )
				break;
			}
			
		if ( dbid >= dbidMax )
			{
			err = ErrERRCheck( JET_errTooManyAttachedDatabases );
			goto HandleError;
			}
			
		dbidGiven = dbid;
		}

	//	Allocate entry from rgfmp

	for ( ifmp = IfmpMinInUse(); ifmp < IfmpMacInUse(); ifmp++ )
		{
		if ( !rgfmp[ifmp].FInUse() )
			{
			// found empty fmp
			break;
			}
		}

	//	there is no FMPs at all
	if ( IfmpMacInUse() < ifmp )
		{
		Assert( IfmpMinInUse() > IfmpMacInUse() );
		ifmp = 0;
		ifmpMinInUse = ifmp;
		ifmpMacInUse = ifmp;
		}
	//	all FMPs in Min, Mac range are in use
	else if ( IfmpMacInUse() == ifmp && rgfmp[ifmp].FInUse() )
		{
		Assert( IfmpMinInUse() <= IfmpMacInUse() );
		if ( IfmpMinInUse() > 0 )
			{
			ifmp = IfmpMinInUse() - 1;
			ifmpMinInUse = ifmp;
			}
		else if ( IfmpMacInUse()+1 < ifmpMax )
			{
			ifmp = IfmpMacInUse() + 1;
			ifmpMacInUse = ifmp;
			}
		else
			{
			ifmp = ifmpMax;
			}
		}
	else
		{
		Assert( IfmpMacInUse() >= ifmp );
		Assert( !rgfmp[ifmp].FInUse() );
		}

	if ( ifmp < ifmpMax )		
		{

		//	Partially AssertVALIDIFMP
		Assert( IfmpMinInUse() <= ifmp );
		Assert( IfmpMacInUse() >= ifmp );
		FMP	*pfmp = &rgfmp[ ifmp ];

		//	if we can find one entry here, latch it.

		Assert( !pfmp->FInUse() );
		Call( pfmp->ErrStoreDbAndSLVNames( pfsapi, szDatabaseName, szSLVName, szSLVRoot, fFalse ) );

		pfmp->SetPinst( pinst );
		pfmp->SetDbid( dbidGiven );
		pinst->m_mpdbidifmp[ dbidGiven ] = ifmp;

		pfmp->CritLatch().Enter();
		
		pfmp->ResetFlags();
		
		if ( fDeferredAttach )
			{
			Assert( pinst->FRecovering() );
			pfmp->SetDeferredAttach();
			pfmp->SetLogOn();
			}

		//	set trxOldestTarget to trxMax to make these
		//	variables unusable until they get reset
		//	by Create/AttachDatabase
		pfmp->SetDbtimeOldestGuaranteed( 0 );
		pfmp->SetDbtimeOldestCandidate( 0 );
		pfmp->SetDbtimeOldestTarget( 0 );
		pfmp->SetTrxOldestCandidate( trxMax );
		pfmp->SetTrxOldestTarget( trxMax );
		pfmp->SetTrxNewestWhenDiscardsLastReported( trxMin );

		pfmp->SetWriteLatch( ppib );
		pfmp->SetLgposAttach( lgposMin );
		pfmp->SetLgposDetach( lgposMin );
		Assert( !pfmp->FExclusiveOpen() );

		pfmp->CritLatch().Leave();

		*pifmp = ifmp;
		err = JET_errSuccess;
		}
	else
		{
		err = ErrERRCheck( JET_errTooManyAttachedDatabases );
		}

HandleError:
	critFMPPool.Leave();
	return err;
	}


VOID FMP::ReleaseWriteLatchAndFree( PIB *ppib )
	{
	Assert ( critFMPPool.FOwner() );
	CritLatch().Enter();

	/*	Identity must exists. It was just newed, so
	 *	if it is exclusively opened, it must be opened by me
	 */
	Assert( !FExclusiveOpen() || PpibExclusiveOpen() == ppib );
	IFMP ifmp = PinstFromPpib( ppib )->m_mpdbidifmp[ Dbid() ];
	FMP::AssertVALIDIFMP( ifmp );
	OSMemoryHeapFree( SzDatabaseName() );
	SetSzDatabaseName( NULL );
	SetSzSLVName( NULL );
	SetSzSLVRoot( NULL );
	
	SetPinst( NULL );
	PinstFromPpib( ppib )->m_mpdbidifmp[ Dbid() ] = ifmpMax;
	SetDbid( dbidMax );
		
	Assert( !FExclusiveOpen() );

	ResetFlags();
	if ( IfmpMinInUse() == ifmp )
		{
		
		IFMP ifmpT = IfmpMinInUse() + 1;
		for ( ; ifmpT <= IfmpMacInUse(); ifmpT++ )
			{
			if ( NULL != rgfmp[ ifmpT ].SzDatabaseName() )
				{
				break;
				}
			}
		
		if ( ifmpT > IfmpMacInUse() )	//	no more ocupied FMPs  
			{
			Assert( IfmpMacInUse()+1 == ifmpT );
			ifmpMacInUse = 0;
			ifmpMinInUse = ifmpMax;
			}
		else
			{
			ifmpMinInUse = ifmpT;
			}
		}
	else if ( IfmpMacInUse() == ifmp )
		{
		IFMP ifmpT = ifmp - 1;
		for ( ; ifmpT > IfmpMinInUse(); ifmpT-- )
			{
			if ( NULL != rgfmp[ ifmpT ].SzDatabaseName() )
				{
				break;
				}
			}
		ifmpMacInUse = ifmpT;
		}

	/*	Free write latch.
	 */
	ResetWriteLatch( ppib );
	if ( GateWriteLatch().CWait() > 0 )
		{
		GateWriteLatch().Release( CritLatch() );
		}
	else
		{
		CritLatch().Leave();
		}
	}


ERR FMP::ErrFMPInit( )
	{
	ERR		err;
	IFMP	ifmp	= 0;
	
	/* initialize the file map array */

	if ( !( rgfmp = new FMP[ ifmpMax ] ) )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	ifmpMinInUse = ifmpMax;
	ifmpMacInUse = 0;

	for ( ifmp = 0; ifmp < ifmpMax; ifmp++ )
		{
#ifdef DEBUG
		rgfmp[ ifmp ].SetDatabaseSizeMax( 0xFFFFFFFF );
#endif  //  DEBUG

		//  initialize sentry range locks
		//  CONSIDER:  make default number of ranges bigger based on backup's needs
		
		SIZE_T crangeMax	= 1;
		SIZE_T cbrangelock	= sizeof( RANGELOCK ) + crangeMax * sizeof( RANGE );

		if (	!( rgfmp[ ifmp ].m_rgprangelock[ 0 ] = (RANGELOCK*)PvOSMemoryHeapAlloc( cbrangelock ) ) ||
				!( rgfmp[ ifmp ].m_rgprangelock[ 1 ] = (RANGELOCK*)PvOSMemoryHeapAlloc( cbrangelock ) ) )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		rgfmp[ ifmp ].m_rgprangelock[ 0 ]->crange		= 0;
		rgfmp[ ifmp ].m_rgprangelock[ 0 ]->crangeMax	= crangeMax;
		
		rgfmp[ ifmp ].m_rgprangelock[ 1 ]->crange		= 0;
		rgfmp[ ifmp ].m_rgprangelock[ 1 ]->crangeMax	= crangeMax;
		}

	return JET_errSuccess;

HandleError:
	if ( rgfmp )
		{
		for ( ifmp = 0; ifmp < ifmpMax; ifmp++ )
			{
			OSMemoryHeapFree( rgfmp[ ifmp ].m_rgprangelock[ 0 ] );
			OSMemoryHeapFree( rgfmp[ ifmp ].m_rgprangelock[ 1 ] );
			}

		delete[] rgfmp;
		rgfmp = NULL;
		}
	return err;
	}

	
VOID FMP::Term( )
	{
	INT	ifmp;

	for ( ifmp = 0; ifmp < ifmpMax; ifmp++ )
		{
		FMP *pfmp = &rgfmp[ifmp];

		OSMemoryHeapFree( pfmp->SzDatabaseName() );
		OSMemoryPageFree( pfmp->Pdbfilehdr() );
		OSMemoryHeapFree( pfmp->Patchchk() );
		OSMemoryHeapFree( pfmp->PatchchkRestored() );
		OSMemoryHeapFree( pfmp->SzPatchPath() );
		OSMemoryHeapFree( pfmp->m_rgprangelock[ 0 ] );
		OSMemoryHeapFree( pfmp->m_rgprangelock[ 1 ] );
		}

	/*	free FMP
	/**/
	delete [] rgfmp;
	rgfmp = NULL;
	}


//  waits until the given PGNO range can no longer be written to by the buffer
//  manager.  this is used to provide a coherent view of the underlying file
//  for this PGNO range for backup purposes

ERR FMP::ErrRangeLock( PGNO pgnoStart, PGNO pgnoEnd )
	{
	ERR err = JET_errSuccess;

	//  prevent others from modifying the range lock while we are modifying the
	//  range lock

	m_semRangeLock.Acquire();

	//  get the pointers to the pointers to the current and new range locks so
	//  that we can manipulate them easily

	RANGELOCK** pprangelockCur = &m_rgprangelock[ m_msRangeLock.ActiveGroup() ];
	RANGELOCK** pprangelockNew = &m_rgprangelock[ 1 - m_msRangeLock.ActiveGroup() ];

	//  the new range lock doesn't have enough room to store all the ranges we need

	if ( (*pprangelockNew)->crangeMax < (*pprangelockCur)->crange + 1 )
		{
		//  double the size of the new range lock

		SIZE_T crangeMax	= 2 * (*pprangelockNew)->crangeMax;
		SIZE_T cbrangelock	= sizeof( RANGELOCK ) + crangeMax * sizeof( RANGE );

		RANGELOCK* prangelock = (RANGELOCK*)PvOSMemoryHeapAlloc( cbrangelock );
		if ( !prangelock )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		prangelock->crangeMax = crangeMax;

		OSMemoryHeapFree( *pprangelockNew );

		*pprangelockNew = prangelock;
		}

	//  copy the state of the current range lock to the new range lock

	SIZE_T irange;
	for ( irange = 0; irange < (*pprangelockCur)->crange; irange++ )
		{
		(*pprangelockNew)->rgrange[ irange ].pgnoStart	= (*pprangelockCur)->rgrange[ irange ].pgnoStart;
		(*pprangelockNew)->rgrange[ irange ].pgnoEnd	= (*pprangelockCur)->rgrange[ irange ].pgnoEnd;
		}

	//  append the new range to the new range lock

	(*pprangelockNew)->rgrange[ irange ].pgnoStart	= pgnoStart;
	(*pprangelockNew)->rgrange[ irange ].pgnoEnd	= pgnoEnd;

	//  set the number of ranges in the new range lock

	(*pprangelockNew)->crange = (*pprangelockCur)->crange + 1;

	//  cause new writers to see the new range lock and wait until all writers
	//  that saw the old range lock are done writing

	m_msRangeLock.Partition();

	//  allow others to modify the range lock now that it is set

HandleError:
	m_semRangeLock.Release();
	return err;
	}

//  permits writes to resume to the given PGNO range locked using FMP::ErrRangeLock

void FMP::RangeUnlock( PGNO pgnoStart, PGNO pgnoEnd )
	{
	//  prevent others from modifying the range lock while we are modifying the
	//  range lock

	m_semRangeLock.Acquire();

	//  get the pointers to the pointers to the current and new range locks so
	//  that we can manipulate them easily

	RANGELOCK** pprangelockCur = &m_rgprangelock[ m_msRangeLock.ActiveGroup() ];
	RANGELOCK** pprangelockNew = &m_rgprangelock[ 1 - m_msRangeLock.ActiveGroup() ];

	//  we should always have enough room to store all the ranges we need because
	//  we only add or remove one range at a time

	Assert( (*pprangelockNew)->crangeMax >= (*pprangelockCur)->crange - 1 );

	//  copy all ranges but the specified range to the new range lock

	for ( SIZE_T irangeSrc = 0, irangeDest = 0; irangeSrc < (*pprangelockCur)->crange; irangeSrc++ )
		{
		if (	(*pprangelockCur)->rgrange[ irangeSrc ].pgnoStart != pgnoStart ||
				(*pprangelockCur)->rgrange[ irangeSrc ].pgnoEnd != pgnoEnd )
			{
			(*pprangelockNew)->rgrange[ irangeDest ].pgnoStart	= (*pprangelockCur)->rgrange[ irangeSrc ].pgnoStart;
			(*pprangelockNew)->rgrange[ irangeDest ].pgnoEnd	= (*pprangelockCur)->rgrange[ irangeSrc ].pgnoEnd;

			irangeDest++;
			}
		}

	//  we had better have found the range specified!!!!!

	Assert( irangeDest == irangeSrc - 1 );

	//  set the number of ranges in the new range lock

	(*pprangelockNew)->crange = irangeDest;

	//  cause new writers to see the new range lock and wait until all writers
	//  that saw the old range lock are done writing

	m_msRangeLock.Partition();

	//  allow others to modify the range lock now that it is set

	m_semRangeLock.Release();
	}

//  enters the range lock, returning the active range lock

int FMP::EnterRangeLock()
	{
	return m_msRangeLock.Enter();
	}

//  tests the given pgno against the specified range lock.  if fTrue is returned,
//  that pgno cannot be written at this time and the caller must LeaveRangeLock().
//  if fFalse is returned, that pgno may be written.  if the caller decides to
//  write that pgno, the caller must not call LeaveRangeLock() until the write
//  has completed.  if the caller decides not to write that pgno, the caller
//  must still LeaveRangeLock()

BOOL FMP::FRangeLocked( int irangelock, PGNO pgno )
	{
	//  get a pointer to the specified range lock.  because the caller called
	//  EnterRangeLock() to get this range lock, it will be stable until they
	//  call LeaveRangeLock()

	RANGELOCK* prangelock = m_rgprangelock[ irangelock ];

	//  scan all ranges looking for this pgno

	for ( SIZE_T irange = 0; irange < prangelock->crange; irange++ )
		{
		//  the current range contains this pgno

		if (	prangelock->rgrange[ irange ].pgnoStart <= pgno &&
				prangelock->rgrange[ irange ].pgnoEnd >= pgno )
			{
			//  this pgno is range locked

			return fTrue;
			}
		}

	//  we did not find a range that contains this pgno, so the pgno is not
	//  range locked

	return fFalse;
	}

//  leaves the specified range lock

void FMP::LeaveRangeLock( int irangelock )
	{
	m_msRangeLock.Leave( irangelock );
	}

ERR FMP::ErrSnapshotStart( IFileSystemAPI *const pfsapi, BKINFO * pbkInfo )
	{
	ERR err = JET_errSuccess;
	
	DBFILEHDR_FIX *pdbfilehdr	= Pdbfilehdr();				
	Assert( pdbfilehdr );
	PGNO pgnoMost = PgnoLast();

	Assert ( FInUse() && FLogOn() && FAttached() );

	if ( FInBackupSession() || FDuringSnapshot() )
		{
		return ErrERRCheck ( JET_errInvalidBackupSequence );
		}
		
	Assert ( !FInBackupSession() );
	Assert ( !FDuringSnapshot() );
	
	Assert( !FSkippedAttach() );
	Assert( !FDeferredAttach() );
		
	CallR ( ErrRangeLock( 0, pgnoMost ) );
			
	Assert ( 0 == PgnoMost() );
	SetPgnoMost( pgnoMost );
	Assert ( 0 < PgnoMost() );
	
	// set the start log position
	pdbfilehdr->bkinfoSnapshotCur = *pbkInfo;

	Call ( ErrUtilWriteShadowedHeader(	pfsapi, 
										SzDatabaseName(), 
										fTrue,
										(BYTE *)pdbfilehdr, 
										g_cbPage,
										Pfapi() ) );
										
	// after the db header is updated, we don't error out
	// in any way. If we do, we need to reset the db header
	// If this fails, we need to stop the backup session.
	
	SetInBackupSession();
	SetDuringSnapshot();

	return err;
	
HandleError:

	Assert ( !FInBackupSession() );
	Assert ( !FDuringSnapshot() );

	memset ( &(pdbfilehdr->bkinfoSnapshotCur), 0, sizeof( BKINFO ) );

	Assert ( PgnoMost() );
	RangeUnlock( 0, PgnoMost() );
	SetPgnoMost( 0 );

	return err;
	}
	
ERR FMP::ErrSnapshotStop( IFileSystemAPI *const pfsapi )
	{

	ERR err = JET_errSuccess;
	
	DBFILEHDR_FIX *pdbfilehdr	= Pdbfilehdr();				
	Assert( pdbfilehdr );
	PGNO pgnoMost = PgnoLast();

	Assert ( FInUse() && FLogOn() && FAttached() );

	if ( !FInBackupSession() || !FDuringSnapshot() )
		{
		return ErrERRCheck ( JET_errInvalidBackupSequence );
		}

	Assert ( 0 != CmpLgpos ( &(pdbfilehdr->bkinfoSnapshotCur.le_lgposMark), &lgposMin) ); 
	memset ( &(pdbfilehdr->bkinfoSnapshotCur), 0, sizeof( BKINFO ) );

	Assert ( PgnoMost() );
	RangeUnlock( 0, PgnoMost() );
	SetPgnoMost( 0 );

	Assert ( SzDatabaseName() );
	Assert ( Pfapi() );

	// we want to reset the DuringSnapshot flag anyway so the db can flush
 	err = ErrUtilWriteShadowedHeader( pfsapi, SzDatabaseName(), fTrue, (BYTE *) pdbfilehdr, g_cbPage, Pfapi() );				

	Assert ( FInBackupSession() );
	ResetDuringSnapshot();

	return err;	
	}

ERR FMP::ErrSetPdbfilehdr(DBFILEHDR_FIX * pdbfilehdr)
	{
	ERR err = JET_errSuccess;

	Assert( NULL == m_pdbfilehdr || NULL == pdbfilehdr );

	if ( NULL != pdbfilehdr && !FReadOnlyAttach() )
		{
		const INST * 	pinst 		= Pinst();
		DBID			dbid;

		Assert ( SzDatabaseName() );
		Assert ( !FSkippedAttach() );

		EnterCritFMPPool();
		for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( pinst->m_mpdbidifmp[ dbid ] >= ifmpMax || dbid == m_dbid )
				continue;

			FMP	*pfmpT;
					
			pfmpT = &rgfmp[ pinst->m_mpdbidifmp[ dbid ] ];

			if ( pfmpT->Pdbfilehdr() 
				&& !pfmpT->FReadOnlyAttach()
				&& 0 == memcmp( &( pdbfilehdr->signDb), &(pfmpT->Pdbfilehdr()->signDb), sizeof( SIGNATURE ) ) )
				{
				LeaveCritFMPPool();
				Call( ErrERRCheck ( JET_errDatabaseSignInUse ) );
				}
			}

		LeaveCritFMPPool();
		Assert ( JET_errSuccess == err );
		}
	m_pdbfilehdr = pdbfilehdr;
HandleError:
	return err;
	}

VOID FMP::UpdateDbtimeOldest()
	{
	const DBTIME	dbtimeLast	= DbtimeGet();	//	grab current dbtime before next trx begins

	Assert( critFMPPool.FOwner() );

	//	don't call this function during recovery
	//	or when we're terminating
	//	UNDONE: is there a better way to tell if we're
	//	terminating besides this m_ppibGlobal check?
	Assert( !m_pinst->FRecovering() );
	Assert( ppibNil != m_pinst->m_ppibGlobal );

	m_pinst->m_critPIBTrxOldest.Enter();
	Assert( ppibNil != m_pinst->m_ppibTrxOldest );		//	at minimum, there's a sentinel
	const TRX		trxOldest	= m_pinst->m_ppibTrxOldest->trxBegin0;
	const TRX		trxNewest	= m_pinst->m_trxNewest;
	m_pinst->m_critPIBTrxOldest.Leave();

	//	if we've reached the target trxOldest, then we
	//	can update the guaranteed dbtimeOldest
	const TRX		trxOldestCandidate	= TrxOldestCandidate();
	const TRX		trxOldestTarget		= TrxOldestTarget();

	Assert( trxMax == trxOldestTarget
		|| TrxCmp( trxOldestCandidate, trxOldestTarget ) <= 0 );

	if ( trxMax == trxOldestTarget )
		{
		//	need to wait until the current candidate is
		//	older than the oldest transaction before
		//	we can establish a new target
		if ( TrxCmp( trxOldestCandidate, trxOldest ) < 0 )
			{
			SetDbtimeOldestTarget( dbtimeLast );
			SetTrxOldestTarget( trxNewest );
			}
		}
	else if ( TrxCmp( trxOldestTarget, trxOldest ) < 0 )
		{
		//	the target trxOldest has now been reached
		//	(ie. it is older than the oldest transaction,
		//	so the candidate dbtimeOldest is now the
		//	guaranteed dbtimeOldest
		//	UNDONE: need a better explanation than
		//	that to explain this complicated logic
		const DBTIME	dbtimeOldestGuaranteed	= DbtimeOldestGuaranteed();
		const DBTIME	dbtimeOldestCandidate	= DbtimeOldestCandidate();
		const DBTIME	dbtimeOldestTarget		= DbtimeOldestTarget();

		Assert( dbtimeOldestGuaranteed <= dbtimeOldestCandidate );
		Assert( dbtimeOldestCandidate <= dbtimeOldestTarget );
		Assert( dbtimeOldestTarget <= dbtimeLast );

		SetDbtimeOldestGuaranteed( dbtimeOldestCandidate );
		SetDbtimeOldestCandidate( dbtimeOldestTarget );
		SetDbtimeOldestTarget( dbtimeLast );
		SetTrxOldestCandidate( trxOldestTarget );
		SetTrxOldestTarget( trxMax );
		}
	}

ERR FMP::ErrObjidLastIncrementAndGet( OBJID *pobjid )		
	{
	Assert( NULL != pobjid );

	if ( !FAtomicIncrementMax( &m_objidLast, pobjid, objidFDPMax ) )
		{
		const _TCHAR *rgpszT[1] = { SzDatabaseName() };
		UtilReportEvent( eventError, GENERAL_CATEGORY, OUT_OF_OBJID, 1, rgpszT );
		return ErrERRCheck( JET_errOutOfObjectIDs );
		}

	//	FAtomicIncrementMax() returns the objid before the increment
	(*pobjid)++;

	// Notify user to defrag database if neccessary
	if ( objidFDPThreshold == *pobjid )
		{
		const _TCHAR *rgpszT[1] = { SzDatabaseName() };
		UtilReportEvent( eventWarning, GENERAL_CATEGORY, ALMOST_OUT_OF_OBJID, 1, rgpszT );
		}

	return JET_errSuccess;
	}


/******************************************************************/
/*				IO                                                */
/******************************************************************/

ERR ErrIOInit( INST *pinst )
	{
	//	Set up ifmp fmp maps
	
	pinst->m_plog->m_fLGFMPLoaded = fFalse;

#ifdef UNLIMITED_DB	
	pinst->m_plog->LGIInitDbListBuffer();
#endif	
	
	return JET_errSuccess;
	}
	
/*	go through FMP closing files.
/**/
ERR ErrIOTerm( INST *pinst, IFileSystemAPI *const pfsapi, BOOL fNormal )
	{
	ERR			err, errT = JET_errSuccess;
	DBID		dbid;
	LGPOS		lgposShutDownMarkRec = lgposMin;
	LOG			*plog = pinst->m_plog;

	//	Reset global variables.
	//
	Assert( pinst->m_ppibGlobal == ppibNil );

	/*	update checkpoint before fmp is cleaned if m_plog->m_fFMPLoaded is true.
	 */
	err = plog->ErrLGUpdateCheckpointFile( pfsapi, fTrue );
	
	//	There should be no attaching/detaching/creating going on
	Assert( err != JET_errDatabaseSharingViolation );
	
	if ( err < 0 && plog->m_fRecovering )
		{
		//	disable log writing but clean fmps
		plog->m_fLGNoMoreLogWrite = fTrue;
		}

	/*	No more checkpoint update from now on. Now I can safely clean up the
	 *	rgfmp.
	 */
	plog->m_fLGFMPLoaded = fFalse;
	
	/*	Set proper shut down mark.
	 */
	if ( fNormal && !plog->m_fLogDisabled )
		{
		//	If we are doing recovering and
		//	if it is in redo mode or
		//	if it is in undo mode but last record seen is shutdown mark, then no
		//	need to log and set shutdown mark again. Note the latter case (undo mode)
		//	is to prevent to log two shutdown marks in a row after recovery and
		//	invalidate the previous one which the attached database have used as
		//	the consistent point.

		if ( plog->m_fRecovering
			&& ( plog->m_fRecoveringMode == fRecoveringRedo
				|| ( plog->m_fRecoveringMode == fRecoveringUndo && plog->m_fLastLRIsShutdown ) ) )
			{
			lgposShutDownMarkRec = plog->m_lgposRedoShutDownMarkGlobal;
			}
		else
			{
			errT = ErrLGShutDownMark( pinst->m_plog, &lgposShutDownMarkRec );
			if ( errT < 0 )
				{
				fNormal = fFalse;
				}
			}
		}

	if ( errT < 0 && err >= 0 )
		{
		err = errT;
		}
	for ( dbid = dbidMin; dbid < dbidMax; dbid++ )
		{
		//	maintain the attach checker.
		IFMP ifmp = pinst->m_mpdbidifmp[ dbid ];
		if ( ifmp >= ifmpMax )
			continue;

		FMP *pfmp = &rgfmp[ ifmp ];
		
		pfmp->RwlDetaching().EnterAsWriter();
		pfmp->SetDetachingDB( );
		pfmp->RwlDetaching().LeaveAsWriter();
		
		if ( fNormal && plog->m_fRecovering )
			{
			Assert( !pfmp->FReadOnlyAttach() );
			if ( pfmp->Patchchk() )
				{
				Assert( dbidTemp != pfmp->Dbid() );
				pfmp->Patchchk()->lgposConsistent = lgposShutDownMarkRec;
				}
			}

		/*	free file handle and pdbfilehdr
		 */
		Assert( pfmp->Pfapi()
				|| NULL == pfmp->Pdbfilehdr()
				|| !fNormal );		//	on error, may have dbfilehdr and no file handle
		if ( pfmp->Pfapi() )
			{
			Assert( NULL != pfmp->Pdbfilehdr() );

			if ( pfmp->FSLVAttached() )
				{
				SLVClose( ifmp );
				}

			delete pfmp->Pfapi();
			pfmp->SetPfapi( NULL );

			if ( fNormal
				&& pfmp->Dbid() != dbidTemp
				&& !pfmp->FReadOnlyAttach() )
				{
				DBFILEHDR	*pdbfilehdr	= pfmp->Pdbfilehdr();

				/*	Update database header.
				 */
				pdbfilehdr->SetDbstate( JET_dbstateConsistent );
				pdbfilehdr->le_dbtimeDirtied = pfmp->DbtimeLast();
				Assert( pdbfilehdr->le_dbtimeDirtied != 0 );
				pdbfilehdr->le_objidLast = pfmp->ObjidLast();
				Assert( pdbfilehdr->le_objidLast != 0 );
				
				if ( plog->m_fRecovering )
					{
					BKINFO * pbkInfoToCopy;
					
					if ( plog->FSnapshotRestore() )
						{
						pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoSnapshotCur);
						}
					else
						{
						pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoFullCur);
						}
						
					if ( pbkInfoToCopy->le_genLow != 0 )
						{
						Assert(pbkInfoToCopy->le_genHigh != 0 );
						pfmp->Pdbfilehdr()->bkinfoFullPrev = (*pbkInfoToCopy);
						memset(	&pfmp->Pdbfilehdr()->bkinfoFullCur, 0, sizeof( BKINFO ) );
						memset(	&pfmp->Pdbfilehdr()->bkinfoSnapshotCur, 0, sizeof( BKINFO ) );
						memset(	&pfmp->Pdbfilehdr()->bkinfoIncPrev, 0, sizeof( BKINFO ) );
						}

#ifdef BKINFO_DELETE_ON_HARD_RECOVERY
					// BUG: 175058: delete the previous backup info on any hard recovery
					// this will prevent a incremental backup and log truncation problems
					// UNDONE: the above logic to copy bkinfoFullPrev is probably not needed
					// (we may consider this and delete it)
					if ( plog->m_fHardRestore )
						{
						memset(	&pfmp->Pdbfilehdr()->bkinfoFullPrev, 0, sizeof( BKINFO ) );
						}
#endif // BKINFO_DELETE_ON_HARD_RECOVERY

					}

				Assert( !pfmp->FLogOn() || !plog->m_fLogDisabled );
				if ( pfmp->FLogOn() )
					{
					Assert( 0 != CmpLgpos( &lgposShutDownMarkRec, &lgposMin ) );
				
					Assert( FSIGSignSet( &pdbfilehdr->signLog ) );
					pdbfilehdr->le_lgposConsistent = lgposShutDownMarkRec;
					pdbfilehdr->le_lgposDetach = lgposShutDownMarkRec;
					}

				LGIGetDateTime( &pdbfilehdr->logtimeConsistent );

				pdbfilehdr->logtimeDetach = pdbfilehdr->logtimeConsistent;
			
				Assert( pdbfilehdr->le_objidLast );
				ERR errT2 = JET_errSuccess;
				if ( !fGlobalRepair )
					{
					if ( pdbfilehdr->FSLVExists() )
						{
						errT2 = ErrSLVSyncHeader(	pfsapi, 
													rgfmp[ifmp].FReadOnlyAttach(),
													rgfmp[ifmp].SzSLVName(),
													pdbfilehdr );
						}
					}
				if ( errT2 >= 0 )
					{
					errT2 = ErrUtilWriteShadowedHeader(
							pfsapi, 
							pfmp->SzDatabaseName(),
							fTrue,
							(BYTE*)pdbfilehdr,
							g_cbPage );
					}
				if ( errT >= 0 && errT2 < 0 )
					{
					errT = errT2;
					}
				}
			}

		if ( pfmp->Pdbfilehdr() )
			{
			pfmp->FreePdbfilehdr();
			}

		// memory leak fix: if the backup was stoped (JetTerm with grbit JET_bitTermStopBackup)
		// the backup header may be still allocated
		if ( pfmp->Ppatchhdr() )
			{
			OSMemoryPageFree( pfmp->Ppatchhdr() );
			pfmp->SetPpatchhdr( NULL );
			}
		}
		
	if ( errT < 0 && err >= 0 )
		{
		err = errT;
		}

	//	no longer persist attachments
	Assert( !pinst->m_pbAttach );
				
	for ( dbid = dbidMin; dbid < dbidMax; dbid++ )
		{
		//	maintain the attach checker.
		IFMP ifmp = pinst->m_mpdbidifmp[ dbid ];
		if ( ifmp >= ifmpMax )
			continue;

		//	purge all the buffers

		BFPurge( ifmp );
		BFPurge( ifmp | ifmpSLV );

		FMP *pfmp = &rgfmp[ ifmp ];

		//	reset fmp fields

		pfmp->RwlDetaching().EnterAsWriter();
		
		DBResetFMP( pfmp, plog, fFalse );

		pfmp->ResetFlags();

		//	free the fmp entry

		if ( pfmp->SzDatabaseName() )
			{
			//	SLV name, if any, is allocated in same space as db name
			OSMemoryHeapFree( pfmp->SzDatabaseName() );
			pfmp->SetSzDatabaseName( NULL );
			pfmp->SetSzSLVName( NULL );
			pfmp->SetSzSLVRoot( NULL );
			}
		else
			{
			Assert( NULL == pfmp->SzSLVName() );
			}

		pfmp->Pinst()->m_mpdbidifmp[ pfmp->Dbid() ] = ifmpMax;
		pfmp->SetDbid( dbidMax );
		pfmp->SetPinst( NULL );
		pfmp->SetCPin( 0 );		// User may term without close the db
		pfmp->RwlDetaching().LeaveAsWriter();

		}

	return err;
	}

	
ERR ErrIONewSize( IFMP ifmp, CPG cpg )
	{
	ERR			err;

	/*	set new EOF pointer
	/**/
	QWORD cbSize = OffsetOfPgno( cpg + 1 );

	rgfmp[ifmp].SemIOExtendDB().Acquire();

	err = rgfmp[ ifmp ].Pfapi()->ErrSetSize( cbSize );
	Assert( err < 0 || err == JET_errSuccess );
	if ( err == JET_errSuccess )
		{
		/*	set database size in FMP -- this value should NOT include the reserved pages
		/**/
		cbSize = QWORD( cpg ) * g_cbPage;
		rgfmp[ifmp].SetFileSize( cbSize );
		}
	rgfmp[ifmp].SemIOExtendDB().Release();

	return err;
	}


/*
 *  opens database file, returns JET_errSuccess if file is already open
 */
ERR ErrIOOpenDatabase( IFileSystemAPI *const pfsapi, IFMP ifmp, CHAR *szDatabaseName )
	{
	FMP::AssertVALIDIFMP( ifmp );

	if ( FIODatabaseOpen( ifmp ) )
		{
		return JET_errSuccess;
		}

	ERR				err;
	IFileAPI	*pfapi;
	FMP				*pfmp 		= &rgfmp[ ifmp ];
	BOOL	fReadOnly	= pfmp->FReadOnlyAttach();
	CallR( pfsapi->ErrFileOpen( szDatabaseName, &pfapi, fReadOnly ) );

	QWORD cbSize;
	// just opened the file, so the file size must be correctly buffered
	Call( pfapi->ErrSize( &cbSize ) );

	pfmp->SetPfapi( pfapi );
	pfmp->SetFileSize( cbSize - cpgDBReserved * g_cbPage );
	
	return err;
	
HandleError:
	delete pfapi;
	return err;
	}


VOID IOCloseDatabase( IFMP ifmp )
	{
	FMP::AssertVALIDIFMP( ifmp );
	FMP *pfmp = &rgfmp[ ifmp ];

	if ( pfmp->FSLVAttached() )
		{
		SLVClose( ifmp );
		}

//	Assert( PinstFromIfmp( ifmp )->m_plog->m_fRecovering || FDBIDWriteLatch(ifmp) == fTrue );
	Assert( pfmp->Pfapi() );

	delete pfmp->Pfapi();
	pfmp->SetPfapi( NULL );
	}
	

ERR ErrIODeleteDatabase( IFileSystemAPI *const pfsapi, IFMP ifmp )
	{
	ERR err;
	
	FMP::AssertVALIDIFMP( ifmp );
//	Assert( FDBIDWriteLatch(ifmp) == fTrue );
	
	CallR( pfsapi->ErrFileDelete( rgfmp[ ifmp ].SzDatabaseName() ) );
	return JET_errSuccess;
	}

	
/*
the function return an inside allocated array of structures. 
It will be one structure for each runnign instance.
For each isstance it will exist information about instance name and attached databases

We compute in advence the needed memory for all the returned data: array, databases and names
so that everythink is alocated in one chunk and can be freed with JetFreeBuffer()
*/

ERR ISAMAPI ErrIsamGetInstanceInfo(
	unsigned long *			pcInstanceInfo,
	JET_INSTANCE_INFO **	paInstanceInfo,
	const BOOL				fSnapshot )
	{
	Assert( pcInstanceInfo && paInstanceInfo);

	if ( NULL == pcInstanceInfo || NULL == paInstanceInfo )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	// protected by critInst
	CHAR*		pMemoryBuffer 		= NULL;
	CHAR*		pCurrentPosArrays	= NULL;
	CHAR*		pCurrentPosNames	= NULL;
	SIZE_T		cbMemoryBuffer	 	= 0;
	
	JET_ERR		err 				= JET_errSuccess;

	ULONG		cInstances		 	= 0;
	ULONG		cDatabasesTotal	 	= 0;
	SIZE_T		cbNamesSize		 	= 0;

	ULONG		ipinst			 	= 0;
	IFMP		ifmp			 	= ifmpMax;

	// during snapshot, we already own those critical sections
	// in the snapshot thread
	if ( !fSnapshot )
		{
		INST::EnterCritInst();
		FMP::EnterCritFMPPool();
		}

	if ( 0 == ipinstMac )
		{
		*pcInstanceInfo = 0;
		*paInstanceInfo = NULL;
		goto HandleError;
		}

	// we count the number of instances, of databases
	// and of characters to be used by all names
	for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
		{
		INST * 	pinst = g_rgpinst[ ipinst ];
		if ( pinstNil == pinst )
			continue;			

		if ( NULL != pinst->m_szInstanceName )
			{
			cbNamesSize += strlen( pinst->m_szInstanceName ) + 1;			
			}
		cInstances++;
		}
	Assert( cInstances == ipinstMac );	

	for ( ifmp = FMP::IfmpMinInUse(); ifmp <= FMP::IfmpMacInUse(); ifmp++ )
		{
		FMP * 	pfmp = &rgfmp[ ifmp ];
		if ( !pfmp->FInUse() )
			continue;

		if ( !pfmp->FLogOn() || !pfmp->FAttached() )
			continue;

		Assert( !pfmp->FSkippedAttach() );
		Assert( !pfmp->FDeferredAttach() );

		cbNamesSize += strlen(pfmp->SzDatabaseName()) + 1;
		if ( NULL != pfmp->SzSLVName() )
			{
			cbNamesSize += strlen(pfmp->SzSLVName()) + 1;
			}
		cDatabasesTotal++;
		}

	// we allocate memory for the result in one chunck
	cbMemoryBuffer = 0;
	// memory for the array of structures
	cbMemoryBuffer += cInstances * sizeof(JET_INSTANCE_INFO);
	// memory for pointers to database names (file name, display name, slv file name)
	cbMemoryBuffer += 3 * cDatabasesTotal * sizeof(CHAR *);
	// memory for all names (database names and instance names)
	cbMemoryBuffer += cbNamesSize * sizeof(CHAR);
	
	pMemoryBuffer = (char *) PvOSMemoryHeapAlloc ( cbMemoryBuffer );
	if ( NULL == pMemoryBuffer )
		{
		Call ( ErrERRCheck ( JET_errOutOfMemory ) );
		}
	
	memset( pMemoryBuffer, '\0', cbMemoryBuffer );


	*pcInstanceInfo = cInstances;
	*paInstanceInfo = (JET_INSTANCE_INFO *)pMemoryBuffer;
	
	pCurrentPosArrays = pMemoryBuffer + ( cInstances * sizeof(JET_INSTANCE_INFO) );
	Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosArrays );

	pCurrentPosNames = pCurrentPosArrays + ( 3 * cDatabasesTotal * sizeof(CHAR *) );

	for ( ipinst = 0, cInstances = 0; ipinst < ipinstMax; ipinst++ )
		{
		INST * 				pinst				= g_rgpinst[ ipinst ];
		JET_INSTANCE_INFO *	pInstInfo			= &(*paInstanceInfo)[cInstances];
		ULONG				cDatabasesCurrInst;
		DBID 				dbid;
		
		if ( pinstNil == pinst )
			continue;			
			
		pInstInfo->hInstanceId = (JET_INSTANCE) pinst;
		Assert( NULL == pInstInfo->szInstanceName );
		if ( NULL != pinst->m_szInstanceName )
			{
			strcpy( pCurrentPosNames, pinst->m_szInstanceName );
			pInstInfo->szInstanceName = pCurrentPosNames;
			pCurrentPosNames += strlen( pCurrentPosNames ) + 1;
			Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosNames );
			}

		cDatabasesCurrInst = 0;
		for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( pinst->m_mpdbidifmp[ dbid ] >= ifmpMax )
				continue;

			const FMP * const	pfmp	= &rgfmp[pinst->m_mpdbidifmp[ dbid ] ];
			Assert( pfmp );
			Assert( pfmp->FInUse() );
			Assert( 0 < strlen ( pfmp->SzDatabaseName() ) );

			if ( !pfmp->FLogOn() || !pfmp->FAttached() )
				continue;

			Assert( !pfmp->FSkippedAttach() );
			Assert( !pfmp->FDeferredAttach() );

			cDatabasesCurrInst++;
			}
			
		pInstInfo->cDatabases = cDatabasesCurrInst;
		Assert( NULL == pInstInfo->szDatabaseFileName);
		Assert( NULL == pInstInfo->szDatabaseDisplayName);
		Assert( NULL == pInstInfo->szDatabaseSLVFileName);
		
		if ( 0 != pInstInfo->cDatabases )
			{
			pInstInfo->szDatabaseFileName = (CHAR **)pCurrentPosArrays;
			pCurrentPosArrays += pInstInfo->cDatabases * sizeof(CHAR *);
			Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosArrays );

			pInstInfo->szDatabaseDisplayName = (CHAR **)pCurrentPosArrays;
			pCurrentPosArrays += pInstInfo->cDatabases * sizeof(CHAR *);
			Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosArrays );			

			pInstInfo->szDatabaseSLVFileName = (CHAR **)pCurrentPosArrays;
			pCurrentPosArrays += pInstInfo->cDatabases * sizeof(CHAR *);
			Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosArrays );						
			}

		cDatabasesCurrInst = 0;
		for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( pinst->m_mpdbidifmp[ dbid ] >= ifmpMax )
				continue;

			const FMP * const	pfmp	= &rgfmp[pinst->m_mpdbidifmp[ dbid ] ];
			Assert( pfmp );
			Assert( pfmp->FInUse() );
			Assert( 0 < strlen ( pfmp->SzDatabaseName() ) );

			if ( !pfmp->FLogOn() || !pfmp->FAttached() )
				continue;

			Assert( !pfmp->FSkippedAttach() );
			Assert( !pfmp->FDeferredAttach() );

			Assert( NULL == pInstInfo->szDatabaseFileName[cDatabasesCurrInst] );
			strcpy( pCurrentPosNames, pfmp->SzDatabaseName() );
			pInstInfo->szDatabaseFileName[cDatabasesCurrInst] = pCurrentPosNames;
			pCurrentPosNames += strlen( pCurrentPosNames ) + 1;
			Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosNames );

			Assert( NULL == pInstInfo->szDatabaseSLVFileName[cDatabasesCurrInst] );
			if ( NULL != pfmp->SzSLVName() )
				{
				strcpy( pCurrentPosNames, pfmp->SzSLVName() );
				pInstInfo->szDatabaseSLVFileName[cDatabasesCurrInst] = pCurrentPosNames;
				pCurrentPosNames += strlen( pCurrentPosNames ) + 1;
				Assert( pMemoryBuffer + cbMemoryBuffer >= pCurrentPosNames );
				}			
				
			// unused by now
			pInstInfo->szDatabaseDisplayName[cDatabasesCurrInst] = NULL;
			
			cDatabasesCurrInst++;
			}			
		Assert( pInstInfo->cDatabases == cDatabasesCurrInst );

		cInstances++;			
		}

	Assert( cInstances == *pcInstanceInfo );
	Assert( pMemoryBuffer
				+ ( cInstances * sizeof(JET_INSTANCE_INFO) )
				+ ( 3 * cDatabasesTotal * sizeof(CHAR *) )
			== pCurrentPosArrays );
	Assert( pMemoryBuffer + cbMemoryBuffer == pCurrentPosNames );

HandleError:

	if ( !fSnapshot )
		{
		FMP::LeaveCritFMPPool();
		INST::LeaveCritInst();
		}

	if ( JET_errSuccess > err )
		{
		*pcInstanceInfo = 0;
		*paInstanceInfo = NULL;

		if ( NULL != pMemoryBuffer )
			{
	 		OSMemoryHeapFree( pMemoryBuffer );
			}
		}
	
	return err;
	}

//  ================================================================
void INST::WaitForDBAttachDetach( )
//  ================================================================
	{
	BOOL fDetachAttach = fTrue;

	// we check this only during backup
	Assert ( m_plog->m_fBackupInProgress );
	
	while ( fDetachAttach )
		{
		fDetachAttach = fFalse;
		for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			IFMP ifmp = m_mpdbidifmp[ dbid ];
			if ( ifmp >= ifmpMax )
				continue;
			
			if ( ( fDetachAttach = rgfmp[ifmp].CrefWriteLatch() != 0 ) != fFalse )
				break;
			}

		if ( fDetachAttach )
			{
			UtilSleep( cmsecWaitGeneric );
			}
		}
	}

#ifdef OS_SNAPSHOT

//#define OS_SNAPSHOT_TRACE


// Class describing the status of a snapshot operation
class CESESnapshotSession
	{
// Constructors and destructors
public:
	CESESnapshotSession():
		m_state( stateStart ),
		m_thread( 0 ),
		m_errFreeze ( JET_errSuccess ),
		m_asigSnapshotThread( CSyncBasicInfo( _T( "asigSnapshotThread" ) ) ),
		m_asigSnapshotStarted( CSyncBasicInfo( _T( "asigSnapshotStarted" ) ) )
		{ }
	
	typedef enum { stateStart, statePrepare, stateFreeze, stateEnd, stateTimeOut } SNAPSHOT_STATE;

public:
	BOOL	FCanSwitchTo( const SNAPSHOT_STATE stateNew ) const ;
	SNAPSHOT_STATE	State( ) const { return m_state; }	
	BOOL	FFreeze( ) const { return stateFreeze == m_state; }	
	BOOL	FCheckId( const JET_OSSNAPID snapId )	const;
	
	JET_OSSNAPID	GetId( );	
	void	SwitchTo( const SNAPSHOT_STATE stateNew );

	// start the snapshot thread
	ERR 	ErrStartSnapshotThreadProc( );
	
	// signal the thread to stop
	void 	StopSnapshotThreadProc( const BOOL fWait = fFalse );
	
	// snapshot thread: the only that moves the state to Ended or TimeOut
	void 	SnapshotThreadProc( const ULONG ulTimeOut );

private:
	ERR 	ErrFreeze();
	void 	Thaw();

	// freeze/thaw operations on ALL instances
	ERR 	ErrFreezeInstance();
	void 	ThawInstance(const BOOL fFullStop = fTrue, const int ipinstLast = ipinstMax );


	// freeze/thaw operations on ALL databases
	ERR 	ErrFreezeDatabase();
	void 	ThawDatabase( const IFMP ifmpLast = ifmpMax );
	
private:
	SNAPSHOT_STATE 		m_state;		// state of the snapshot session
	THREAD 				m_thread;
	ERR 				m_errFreeze;	// error at freeze start-up
	
	CAutoResetSignal	m_asigSnapshotThread;
	CAutoResetSignal	m_asigSnapshotStarted;

// static members
public:
	void 	SnapshotCritEnter() { CESESnapshotSession::m_critOSSnapshot.Enter(); }
	void 	SnapshotCritLeave() { CESESnapshotSession::m_critOSSnapshot.Leave(); }

	ULONG	GetTimeout() const;
	void 	SetTimeout( const ULONG ulTimeOut ) ;

private:
	//  critical section protecting all OS level snapshot information
	static CCriticalSection m_critOSSnapshot;

	// counter used to keep a global identifer of the snapshots
	static unsigned int m_idSnapshot;

	// timeout for the freeze in ms
	static ULONG m_ulTimeOut;	

	};

CCriticalSection CESESnapshotSession::m_critOSSnapshot( CLockBasicInfo( CSyncBasicInfo( "OSSnapshot" ), rankOSSnapshot, 0 ) );
unsigned int CESESnapshotSession::m_idSnapshot = 0;
ULONG CESESnapshotSession::m_ulTimeOut = 20 * 1000; // default is 20 ms


// At this moment the implementation will allow one snapshot
// session at a time. This is the global object describing the 
// snapshot session status
CESESnapshotSession g_SnapshotSession;

BOOL CESESnapshotSession::FCheckId( const JET_OSSNAPID snapId )	const
	{ 
	return ( snapId == (JET_OSSNAPID)m_idSnapshot ); 
	}
	
JET_OSSNAPID CESESnapshotSession::GetId( )
	{
	Assert ( m_critOSSnapshot.FOwner() );
	m_idSnapshot++;
	return (JET_OSSNAPID)m_idSnapshot; 
	}

void CESESnapshotSession::SetTimeout( const ULONG ulTimeOut )
	{
	Assert ( m_critOSSnapshot.FOwner() );
	CESESnapshotSession::m_ulTimeOut = ulTimeOut; 
	}
	
ULONG CESESnapshotSession::GetTimeout() const 
	{ 
	return CESESnapshotSession::m_ulTimeOut; 
	}

ERR CESESnapshotSession::ErrFreezeInstance( )
	{
	int 	ipinst 	= 0;
	ERR 	err 	= JET_errSuccess;

	INST::EnterCritInst();

	for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
		{
		if ( pinstNil == g_rgpinst[ ipinst ] )
			{
			continue;
			}

		LOG * pLog;
		pLog = g_rgpinst[ ipinst ]->m_plog;
		Assert ( pLog );
			
		pLog->m_critLGFlush.Enter(); // protect the recovery flag
		pLog->m_critBackupInProgress.Enter();		

		if ( pLog->m_fBackupInProgress || pLog->m_fRecovering )
			{
			pLog->m_critBackupInProgress.Leave();
			pLog->m_critLGFlush.Leave();
			Call ( ErrERRCheck( JET_errOSSnapshotNotAllowed ) );
			}

		pLog->m_fBackupInProgress = fTrue;

		// marked as during backup: will prevent attach/detach
		pLog->m_critBackupInProgress.Leave();

		// leave LGFlush to allow attach/detach in progress to complete
		pLog->m_critLGFlush.Leave();

		g_rgpinst[ ipinst ]->WaitForDBAttachDetach();
		}


	// all set, now stop log flushing
	// then stop checkpoint (including db headers update)
	for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
		{
		if ( pinstNil == g_rgpinst[ ipinst ] )
			{
			continue;
			}
			
		Assert ( g_rgpinst[ ipinst ]->m_plog );		
		g_rgpinst[ ipinst ]->m_plog->m_critLGFlush.Enter();
		}

	// UNDONE: consider not entering m_critCheckpoint and
	// keeping it during the snapshot (this will prevent 
	// db header and checkpoint update) but instead
	// enter() set m_fLogDisable leavr()
	// and reset the same way on Thaw
	for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
		{
		if ( pinstNil == g_rgpinst[ ipinst ] )
			{
			continue;
			}
			
		Assert ( g_rgpinst[ ipinst ]->m_plog );		
		g_rgpinst[ ipinst ]->m_plog->m_critCheckpoint.Enter();
		}
		
	Assert ( JET_errSuccess == err );
	
HandleError:
	if ( JET_errSuccess > err )
		{
		ThawInstance( fFalse, ipinst );
		}
	return err;
	}

void CESESnapshotSession::ThawInstance( const BOOL fFullStop, const int ipinstLast )
	{
	int ipinst;

	// if Thaw all, we have to leave LGFlush, critCheckpoint
	if ( fFullStop )
		{
		for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
			{
			if ( pinstNil == g_rgpinst[ ipinst ] )
				{
				continue;
				}
				
			Assert ( g_rgpinst[ ipinst ]->m_plog );		
			g_rgpinst[ ipinst ]->m_plog->m_critCheckpoint.Leave();
			}

		for ( ipinst = 0; ipinst < ipinstMax; ipinst++ )
			{
			if ( pinstNil == g_rgpinst[ ipinst ] )
				{
				continue;
				}
				
			Assert ( g_rgpinst[ ipinst ]->m_plog );		
			g_rgpinst[ ipinst ]->m_plog->m_critLGFlush.Leave();
			}
		}

	for ( ipinst = 0; ipinst < ipinstLast; ipinst++ )
		{
		if ( pinstNil == g_rgpinst[ ipinst ] )
			{
			continue;
			}
			
		LOG * pLog;
		pLog = g_rgpinst[ ipinst ]->m_plog;
		Assert ( pLog );

		pLog->m_critBackupInProgress.Enter();
		Assert ( pLog->m_fBackupInProgress );
		pLog->m_fBackupInProgress = fFalse;
		pLog->m_critBackupInProgress.Leave();
		}		
		
	INST::LeaveCritInst();

	}

ERR CESESnapshotSession::ErrFreezeDatabase( )
	{
	ERR 	err 		= JET_errSuccess;
	IFMP 	ifmp 		= 0;

	// not needed as the already have all instances set "during backup"
	// which will prevent new attach/detach and we have waited for
	// all attache/detach in progress
//	FMP::EnterCritFMPPool();
	
	for ( ifmp = FMP::IfmpMinInUse(); ifmp <= FMP::IfmpMacInUse(); ifmp++ )
		{
		FMP	* pfmp = &rgfmp[ ifmp ];

		if ( !pfmp->FInUse() )
			{
			continue;
			}
			
		if ( dbidTemp == pfmp->Dbid() )
			{
			continue;
			}
			
		Assert ( pfmp->Dbid() >= dbidUserLeast );	
		Assert ( pfmp->Dbid() < dbidMax );	

		Assert ( PinstFromIfmp(ifmp)->m_plog->m_fBackupInProgress );	
		Assert ( pfmp->FAttached() );	
		
		// prevent database size changes
		pfmp->SemIOExtendDB().Acquire();
		}

	for ( ifmp = FMP::IfmpMinInUse(); ifmp <= FMP::IfmpMacInUse(); ifmp++ )
		{
		FMP	* pfmp = &rgfmp[ ifmp ];

		if ( !pfmp->FInUse() )
			{
			continue;
			}
			
		if ( dbidTemp == pfmp->Dbid() )
			{
			continue;
			}
			
		Assert ( pfmp->Dbid() >= dbidUserLeast );	
		Assert ( pfmp->Dbid() < dbidMax );	

		// use PgnoLast() as we have ExtendingDBLatch
		// so that the database can't change size
		err = pfmp->ErrRangeLock( 0, pfmp->PgnoLast() );
		if ( JET_errSuccess > err )
			{
			Call ( err );
			Assert ( fFalse );
			}			
		}

	CallS( err );

HandleError:

	if ( JET_errSuccess > err )
		{
		ThawDatabase( ifmp );		
		}
		
	return err;
	}
	
void CESESnapshotSession::ThawDatabase( const IFMP ifmpLast )
	{
	const IFMP	ifmpRealLast	= ( ifmpLast < ifmpMax ? ifmpLast : FMP::IfmpMacInUse() + 1 );

	for ( IFMP ifmp = FMP::IfmpMinInUse(); ifmp < ifmpRealLast; ifmp++ )
		{
		FMP	* pfmp = &rgfmp[ ifmp ];

		if ( !pfmp->FInUse() )
			{
			continue;
			}
			
		if ( dbidTemp == pfmp->Dbid() )
			{
			continue;
			}
			
		Assert ( pfmp->Dbid() >= dbidUserLeast );	
		Assert ( pfmp->Dbid() < dbidMax );	

		Assert ( PinstFromIfmp(ifmp)->m_plog->m_fBackupInProgress );	
		Assert ( pfmp->FAttached() );	
		
		pfmp->RangeUnlock( 0, pfmp->PgnoLast() );
		}

	for ( ifmp = FMP::IfmpMinInUse(); ifmp <= FMP::IfmpMacInUse(); ifmp++ )
		{
		FMP	* pfmp = &rgfmp[ ifmp ];

		if ( !pfmp->FInUse() )
			{
			continue;
			}
			
		if ( dbidTemp == pfmp->Dbid() )
			{
			continue;
			}
			
		Assert ( pfmp->Dbid() >= dbidUserLeast );	
		Assert ( pfmp->Dbid() < dbidMax );	

		pfmp->SemIOExtendDB().Release();
		}
	
//	FMP::LeaveCritFMPPool();
	}

ERR CESESnapshotSession::ErrFreeze( )
	{
	ERR err = JET_errSuccess;
	
	CallR ( ErrFreezeInstance() );

	Call ( ErrFreezeDatabase() );

	Assert ( JET_errSuccess == err );

HandleError:

	if ( JET_errSuccess > err )
		{
		ThawInstance();
		}
		
	return err;		
	}

void CESESnapshotSession::Thaw( )
	{
	ThawDatabase();
	ThawInstance();
	}

void CESESnapshotSession::SnapshotThreadProc( const ULONG ulTimeOut )
	{

	CHAR szSnap[12];
	CHAR szError[12];
	const _TCHAR* rgszT[2] = { szSnap, szError };
	
	sprintf( szSnap, "%lu", m_idSnapshot );

	UtilReportEvent( eventInformation, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_FREEZE_START_ID, 1, rgszT );
	
	m_errFreeze = ErrFreeze();

	if ( JET_errSuccess > m_errFreeze )
		{
		UtilThreadEnd( m_thread );
		m_thread = 0;

		// signal the starting thread that snapshot couldn't start
		m_asigSnapshotStarted.Set();

		sprintf( szError, "%d", m_errFreeze );
		UtilReportEvent( eventError, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_FREEZE_START_ERROR_ID, 2, rgszT );
		return;
		}
		
	// signal the starting thread that snapshot did started
	// so that the Freeze API returns
	m_asigSnapshotStarted.Set();

	
	BOOL fTimeOut = !m_asigSnapshotThread.FWait( ulTimeOut );	
	SNAPSHOT_STATE newState = fTimeOut?stateTimeOut:stateEnd;

	Thaw();

	SnapshotCritEnter();
	Assert ( stateFreeze == State() );
	Assert ( FCanSwitchTo( newState ) );
	SwitchTo ( newState );

	// on time-out, we need to close the thread handle
	if ( fTimeOut )
		{
		UtilThreadEnd( m_thread );
		m_thread = 0;

#ifdef OS_SNAPSHOT_TRACE
		{
		CHAR szError[12];
		const _TCHAR* rgszT[2] = { "thread SnapshotThreadProc", szError };
		_itoa( JET_errOSSnapshotTimeOut, szError, 10 );
	
		UtilReportEvent( eventError, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_TRACE_ID, 2, rgszT );
		}
#endif // OS_SNAPSHOT_TRACE
		}
	SnapshotCritLeave();

	if ( fTimeOut )
		{
		sprintf( szError, "%lu", ulTimeOut );
		UtilReportEvent( eventError, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_TIME_OUT_ID, 2, rgszT );
		}
	else
		{
		UtilReportEvent( eventInformation, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_FREEZE_STOP_ID, 1, rgszT );
		}
	}

DWORD DwSnapshotThreadProc( DWORD_PTR lpParameter )
	{
	CESESnapshotSession * pSession = (CESESnapshotSession *)lpParameter;
	Assert ( pSession == &g_SnapshotSession );

	pSession->SnapshotThreadProc( pSession->GetTimeout() );

	return 0;
	}

ERR CESESnapshotSession::ErrStartSnapshotThreadProc( )
	{
	ERR err = JET_errSuccess;
	
	Assert ( CESESnapshotSession::m_critOSSnapshot.FOwner() );
	Assert ( statePrepare == State() );

	m_asigSnapshotStarted.Reset();
	Call ( ErrUtilThreadCreate(	DwSnapshotThreadProc,
								OSMemoryPageReserveGranularity(),
								priorityNormal,
								&m_thread,
								DWORD_PTR( this ) ) );

	// wait until snapshot thread is starting
	m_asigSnapshotStarted.Wait();

	// freeze start failed
	if ( JET_errSuccess > m_errFreeze )
		{
		// thread is gone
		Assert ( 0 == m_thread);
		Call ( m_errFreeze );
		Assert ( fFalse );
		}

	SwitchTo( CESESnapshotSession::stateFreeze );		

	Assert ( JET_errSuccess == err );
	
HandleError:
	// QUESTION: on error, return in Start or leave it in Prepare ?
	if ( JET_errSuccess > err )
		{
		// don't FCanSwitchTo which is not allowed
		SwitchTo( CESESnapshotSession::stateStart );		
		}

	return JET_errSuccess;
	}


void CESESnapshotSession::StopSnapshotThreadProc( const BOOL fWait )
	{
	m_asigSnapshotThread.Set();

	if ( fWait )
		{
		(void) UtilThreadEnd( m_thread );
		m_thread = 0;
		}
	
	return;
	}


BOOL CESESnapshotSession::FCanSwitchTo( const SNAPSHOT_STATE stateNew ) const
	{
	Assert ( CESESnapshotSession::m_critOSSnapshot.FOwner() );

	switch ( stateNew )
		{
		case stateStart:
			if ( m_state != stateEnd && 	// normal Thaw
				m_state != stateTimeOut )	// time-out end
				{
				return fFalse;
				}
			break;
			
		case statePrepare:
			if ( m_state != stateStart && 	// normal start
				m_state != stateTimeOut && 	// re-start after a time-out without Thaw call
				m_state != statePrepare ) 	// we can get a prepare, then nothing else (coordinator died), then restarts
				{
				return fFalse;
				}
			break;
			
		case stateFreeze:
			if ( m_state != statePrepare )	// just after prepare
				{
				return fFalse;
				}
			break;
			
		case stateEnd:
			if ( m_state != stateFreeze )		// normal end
				{
				return fFalse;
				}				
			break;
			
		case stateTimeOut:
			if ( m_state != stateFreeze )	// timeout only from the freeze state
				{
				return fFalse;
				}				
			break;
			
		default:
			Assert ( fFalse );			
			return fFalse;
		}
		
	return fTrue;	
	}

void CESESnapshotSession::SwitchTo( const SNAPSHOT_STATE stateNew )
	{
	Assert ( CESESnapshotSession::m_critOSSnapshot.FOwner() );
	m_state = stateNew;

	// do special action depending on the new state
	switch ( m_state )
		{
		case stateStart:
		case statePrepare:
			m_asigSnapshotThread.Reset();
			break;
		default:
			break;
		}
		
	}

ERR ISAMAPI ErrIsamOSSnapshotPrepare( JET_OSSNAPID * psnapId, const JET_GRBIT	grbit )
	{
	ERR 					err 		= JET_errSuccess;
	CESESnapshotSession * 	pSession 	= &g_SnapshotSession;

	NotUsed( grbit );

	if ( !psnapId )
		{
		CallR ( ErrERRCheck( JET_errInvalidParameter ) );
		}
	pSession->SnapshotCritEnter();
	
	if ( !pSession->FCanSwitchTo( CESESnapshotSession::statePrepare ) )
		{
		pSession->SnapshotCritLeave();
		Call ( ErrERRCheck( JET_errOSSnapshotInvalidSequence ) );
		}

	pSession->SwitchTo( CESESnapshotSession::statePrepare );
	*psnapId = pSession->GetId( );	
	
	pSession->SnapshotCritLeave();

	Assert ( JET_errSuccess == err );
	
HandleError:
#ifdef OS_SNAPSHOT_TRACE
	{
	CHAR szError[12];
	const _TCHAR* rgszT[2] = { "JetOSSnapshotPrepare", szError };
	_itoa( err, szError, 10 );
	
	UtilReportEvent( JET_errSuccess > err?eventError:eventInformation, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_TRACE_ID, 2, rgszT );
	}
#endif // OS_SNAPSHOT_TRACE
	return err;
	}

ERR ISAMAPI ErrIsamOSSnapshotFreeze( const JET_OSSNAPID snapId, unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo, const	JET_GRBIT grbit )
	{
	CESESnapshotSession * 	pSession 		= &g_SnapshotSession;
	ERR 					err 			= JET_errSuccess;

	NotUsed( grbit );

	if ( NULL == pcInstanceInfo || NULL == paInstanceInfo )
		{
		CallR ( ErrERRCheck(JET_errInvalidParameter ) );
		}
	*pcInstanceInfo = 0;
	*paInstanceInfo = NULL;
	
	pSession->SnapshotCritEnter();

	// we allow Freeze/Thaw calls (without error) if we missed the Prepare
	// because we were not registered when snapshot started moment)
	if ( CESESnapshotSession::stateStart == pSession->State() )
		{
		Assert ( JET_errSuccess == err );
		goto HandleError;
		}
	
	if ( !pSession->FCanSwitchTo( CESESnapshotSession::stateFreeze ) )
		{
		Call ( ErrERRCheck( JET_errOSSnapshotInvalidSequence ) );
		}
		
	AssertRTL( pSession->FCheckId( snapId ) );

	Call ( pSession->ErrStartSnapshotThreadProc() );

	Assert ( CESESnapshotSession::stateFreeze == pSession->State() );
	Assert ( JET_errSuccess == err );

	err = ErrIsamGetInstanceInfo( pcInstanceInfo, paInstanceInfo, fTrue /* in snapshot */);
	if ( JET_errSuccess > err )
		{
		// need to thaw as we got an error at this point
		ERR errT;
		
		pSession->SnapshotCritLeave();		
		errT = ErrIsamOSSnapshotThaw( snapId, grbit );
		Assert( JET_errSuccess == errT );
		pSession->SnapshotCritEnter();
		}
		
	Call ( err );
	Assert ( JET_errSuccess == err );
	
HandleError:		
	pSession->SnapshotCritLeave();

#ifdef OS_SNAPSHOT_TRACE
	{
	CHAR szError[12];
	const _TCHAR* rgszT[2] = { "JetOSSnapshotFreeze", szError };
	_itoa( err, szError, 10 );
	
	UtilReportEvent( JET_errSuccess > err?eventError:eventInformation, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_TRACE_ID, 2, rgszT );
	}
#endif // OS_SNAPSHOT_TRACE

	return err;	
	}

ERR ISAMAPI ErrIsamOSSnapshotThaw(	const JET_OSSNAPID snapId, const	JET_GRBIT grbit )
	{
	CESESnapshotSession * 	pSession 	= &g_SnapshotSession;
	ERR 					err 		= JET_errSuccess;

	NotUsed( grbit );
	
	pSession->SnapshotCritEnter();

	// we allow Freeze/Thaw calls (without error) if we missed the Prepare
	// because we were not registered when snapshot started moment)
	if ( CESESnapshotSession::stateStart ==  pSession->State() )
		{
		Assert ( JET_errSuccess == err );
		goto HandleError;
		}
		
	// we can end it if it is freezed or time-out already
	if ( CESESnapshotSession::stateFreeze != pSession->State() && // normal end from freeze
		CESESnapshotSession::stateTimeOut != pSession->State() ) // we arealdy got the time-out
		{
		Call ( ErrERRCheck( JET_errOSSnapshotInvalidSequence ) );
		}

	AssertRTL( pSession->FCheckId( snapId ) );

	// the state can't be End as long as we haven't signaled the thread
	Assert ( CESESnapshotSession::stateFreeze == pSession->State() || CESESnapshotSession::stateTimeOut == pSession->State() );

	pSession->SnapshotCritLeave();
	
RetryStop:

	pSession->SnapshotCritEnter();	

	Assert ( CESESnapshotSession::stateFreeze == pSession->State() || 
			CESESnapshotSession::stateEnd == pSession->State() ||
			CESESnapshotSession::stateTimeOut == pSession->State() );
	
	if ( CESESnapshotSession::stateEnd == pSession->State() )
		{
		err = JET_errSuccess;
		}
	else if ( CESESnapshotSession::stateTimeOut == pSession->State() )
		{
		err = ErrERRCheck( JET_errOSSnapshotTimeOut );
		}
	else
		{
		// thread is signaled but not ended yet ... 
		Assert ( CESESnapshotSession::stateFreeze == pSession->State() ); 	

		// let the thread change the status and wait for it's complition
		pSession->SnapshotCritLeave();

		// signal thread to stop and wait thread complition
		pSession->StopSnapshotThreadProc( fTrue /* wait thread complition*/ );
		
		goto RetryStop;
		}

	Assert ( pSession->FCanSwitchTo( CESESnapshotSession::stateStart ) );
	pSession->SwitchTo( CESESnapshotSession::stateStart );

HandleError:
	pSession->SnapshotCritLeave();

#ifdef OS_SNAPSHOT_TRACE
	{
	CHAR szError[12];
	const _TCHAR* rgszT[2] = { "JetOSSnapshotThaw", szError };
	_itoa( err, szError, 10 );
	
	UtilReportEvent( JET_errSuccess > err?eventError:eventInformation, OS_SNAPSHOT_BACKUP, OS_SNAPSHOT_TRACE_ID, 2, rgszT );
	}
#endif // OS_SNAPSHOT_TRACE

	return err;	
	} 

// functions to set the timeout using JetSetSystemParam with JET_paramOSSnapshotTimeout
ERR ErrOSSnapshotSetTimeout( const ULONG_PTR ms )
	{
	CESESnapshotSession * 	pSession 	= &g_SnapshotSession;
	ERR 					err 		= JET_errSuccess;

	//	UNDONE: impose a realistic cap on the timeout value
	if ( ms >= LONG_MAX )
		return ErrERRCheck( JET_errInvalidParameter );

#ifndef OS_SNAPSHOT
	return ErrERRCheck( JET_wrnNyi );
#endif // OS_SNAPSHOT
	
	pSession->SnapshotCritEnter();

	// can't set the timeout unless we can start a new snapshot
	// we can eventualy relax this condition but probably it is fine
	if ( !pSession->FCanSwitchTo( CESESnapshotSession::statePrepare ) )
		{
		pSession->SnapshotCritLeave();
		CallR ( ErrERRCheck( JET_errOSSnapshotInvalidSequence ) );
		}

	pSession->SetTimeout( (ULONG)ms );
	pSession->SnapshotCritLeave();

	return JET_errSuccess;
	}

// functions to get the timeout using JetGetSystemParam with JET_paramOSSnapshotTimeout
ERR ErrOSSnapshotGetTimeout( ULONG_PTR * pms )
	{
	CESESnapshotSession * 	pSession 	= &g_SnapshotSession;
	
	if ( pms == NULL )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	pSession->SnapshotCritEnter();

	Assert ( pms );		
	*pms = pSession->GetTimeout();
	
	pSession->SnapshotCritLeave();
	return JET_errSuccess;
	}

#else // OS_SNAPSHOT

ERR ISAMAPI ErrIsamOSSnapshotPrepare( JET_OSSNAPID * psnapId, const JET_GRBIT	grbit )
	{
	return ErrERRCheck( JET_wrnNyi );
	}

ERR ISAMAPI ErrIsamOSSnapshotFreeze( const JET_OSSNAPID snapId, unsigned long *pcInstanceInfo, JET_INSTANCE_INFO ** paInstanceInfo,const	JET_GRBIT grbit )
	{
	return ErrERRCheck( JET_wrnNyi );
	}

ERR ISAMAPI ErrIsamOSSnapshotThaw(	const JET_OSSNAPID snapId, const	JET_GRBIT grbit )
	{
	return ErrERRCheck( JET_wrnNyi );
	}

ERR ErrOSSnapshotSetTimeout( const ULONG_PTR ms )
	{
	return ErrERRCheck( JET_wrnNyi );
	}

ERR ErrOSSnapshotGetTimeout( ULONG_PTR * pms )
	{
	return ErrERRCheck( JET_wrnNyi );
	}

#endif // OS_SNAPSHOT
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\node.cxx ===
/*******************************************************************

Operations that modify a node are done in three stages:

1. Create a 'tentative' version (before image) for the record
1. Dirty the page -- this will increase the timestamp
2. Log the operation.
3. If the log operation succeeded:
	3.1	Update the page. Set its lgpos to the position we got
   	from the log operation. As the page is write latched
	we do not need to worry about the page being flushed yet

The item that a currency references must be there

*******************************************************************/


#include "std.hxx"


//  ****************************************************************
//  MACROS
//  ****************************************************************

#ifdef DEBUG

//  check that the node is in order at each entry point
///#define DEBUG_NODE

//  check that the node is in order after each insertion
///#define DEBUG_NODE_INSERT

//  check the binary-search seeks with linear scans
///#define DEBUG_NODE_SEEK

#endif	// DEBUG


//  ****************************************************************
//  CONSTANTS
//  ****************************************************************

//  the number of DATA structs that a KEYDATAFLAGS is converted to
const ULONG cdataKDF	= 4;

//  the number of DATA structs that a prefix becomes
const ULONG cdataPrefix	= 3;


//  ****************************************************************
//  PROTOTYPES
//  ****************************************************************

LOCAL VOID NDILineToKeydataflags		( const LINE * pline, KEYDATAFLAGS * pkdf );
LOCAL INT CdataNDIPrefixAndKeydataflagsToDataflags ( 
	KEYDATAFLAGS 	* const pkdf,
	DATA 			* const rgdata,
	INT 			* const pfFlags,
	LE_KEYLEN		* const ple_keylen );
LOCAL INT CdataNDIKeydataflagsToDataflags (
	const KEYDATAFLAGS 	* const pkdf,
	DATA 				* const rgdata,
	INT 				* const pfFlags,
	LE_KEYLEN			* const ple_keylen );
INLINE VOID NDISetCompressed			( KEYDATAFLAGS& kdf );
INLINE VOID NDIResetCompressed			( KEYDATAFLAGS& kdf );
INLINE VOID NDISetFlag					( CSR * pcsr, INT fFlag );
INLINE VOID NDIResetFlag				( CSR * pcsr, INT fFlag );

INLINE VOID NDIGetBookmark				( const FUCB * pfucb, const CPAGE& cpage, INT iline, BOOKMARK * pbookmark );
LOCAL INT IlineNDISeekGEQ				( const FUCB * pfucb, const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare );
LOCAL INT IlineNDISeekGEQInternal		( const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare );
LOCAL ERR ErrNDISeekInternalPage		( CSR * pcsr, const BOOKMARK& bm );
LOCAL ERR ErrNDISeekLeafPage			( const FUCB * pfucb, CSR * pcsr, const BOOKMARK& bm );

#ifdef DEBUG
INLINE VOID NDIAssertCurrency			( const FUCB * pfucb, const CSR * pcsr );
INLINE VOID NDIAssertCurrencyExists		( const FUCB * pfucb, const CSR * pcsr );

#ifdef DEBUG_NODE_SEEK
LOCAL INT IlineNDISeekGEQDebug			( const FUCB * pfucb, const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare );
LOCAL INT IlineNDISeekGEQInternalDebug	( const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare );
#endif	//	DEBUG_NODE_SEEK

#else
#define NDIAssertCurrency( pfucb, pcsr )		((VOID) 0)
#define NDIAssertCurrencyExists( pfucb, pcsr )	((VOID) 0)
#endif	//  DEBUG


//  ****************************************************************
//  INTERNAL ROUTINES
//  ****************************************************************


//  ================================================================
LOCAL VOID NDILineToKeydataflags( const LINE * pline, KEYDATAFLAGS * pkdf )
//  ================================================================
	{
	const BYTE	*pb				= (BYTE*)pline->pv;
	INT			cbKeyCountTotal	= cbKeyCount;

	if ( pline->fFlags & fNDCompressed )
		{
		//  prefix key compressed
		pkdf->key.prefix.SetCb( *((UnalignedLittleEndian< SHORT > *)pb) );
		pb += cbPrefixOverhead;
		cbKeyCountTotal += cbPrefixOverhead;
		}
	else
		{
		//  no prefix compression
		pkdf->key.prefix.SetCb( 0 );
		}

	pkdf->key.suffix.SetCb( *((UnalignedLittleEndian< SHORT > *)pb) );
	pkdf->key.suffix.SetPv( const_cast<BYTE *>( pb ) + cbKeyCount );
	
	pkdf->data.SetCb( pline->cb - pkdf->key.suffix.Cb() - cbKeyCountTotal );
	pkdf->data.SetPv( const_cast<BYTE *>( pb ) + cbKeyCount + pkdf->key.suffix.Cb() );
	
	pkdf->fFlags	= pline->fFlags;

	Assert( pkdf->key.suffix.Cb() + pkdf->data.Cb() + cbKeyCountTotal == ( ULONG )pline->cb );

	//	kdf can not yet be validated since prefix.pv is not yet set
	}


//  ================================================================
LOCAL INT CdataNDIPrefixAndKeydataflagsToDataflags(
	KEYDATAFLAGS 	* const pkdf,
	DATA 			* const rgdata,
	INT 			* const pfFlags,
	LE_KEYLEN		* const ple_keylen )
//  ================================================================
//
//	sets up rgdata to insert kdf with *pcbPrefix as prefix.cb
//	adjust kdf.suffix.cb to new suffix.cb
//  rgdata must have at least cdataKDF + 1 elements
//
//-
	{
	ASSERT_VALID( pkdf );
	
	INT idata = 0;

	Assert( ple_keylen->le_cbPrefix >= 0 );
	if ( ple_keylen->le_cbPrefix > 0 )
		{
		//  prefix compresses
		Assert( ple_keylen->le_cbPrefix > cbPrefixOverhead );
		Assert( sizeof(ple_keylen->le_cbPrefix) == cbPrefixOverhead );
		
		rgdata[idata].SetPv( &ple_keylen->le_cbPrefix );
		rgdata[idata].SetCb( cbPrefixOverhead );
		++idata;
		}
		
	//	suffix count will be updated correctly below
	rgdata[idata].SetPv( &ple_keylen->le_cbSuffix );
	rgdata[idata].SetCb( cbKeyCount );
	++idata;

	//	leave ple_keylen->le_cbPrefix out of given key to get suffix to insert
	Assert( pkdf->key.Cb() >= ple_keylen->le_cbPrefix );
	if ( pkdf->key.prefix.Cb() <= ple_keylen->le_cbPrefix )
		{
		//	get suffix to insert from pkdf->suffix
		const INT	cbPrefixInSuffix = ple_keylen->le_cbPrefix - pkdf->key.prefix.Cb();

		rgdata[idata].SetPv( (BYTE *) pkdf->key.suffix.Pv() + cbPrefixInSuffix );
		rgdata[idata].SetCb( pkdf->key.suffix.Cb() - cbPrefixInSuffix );
		idata++;

		//	decrease size of suffix
		pkdf->key.suffix.DeltaCb( - cbPrefixInSuffix );
		}
	else
		{
		//	get suffix to insert from pkdf->prefix and suffix
		rgdata[idata].SetPv( (BYTE *)pkdf->key.prefix.Pv() + ple_keylen->le_cbPrefix );
		rgdata[idata].SetCb( pkdf->key.prefix.Cb() - ple_keylen->le_cbPrefix );
		++idata;

		rgdata[idata] = pkdf->key.suffix;
		++idata;

		//	decrease size of suffix
		pkdf->key.suffix.DeltaCb( pkdf->key.prefix.Cb() - ple_keylen->le_cbPrefix );
		}

	//	update suffix count in output buffer
	ple_keylen->le_cbSuffix = (USHORT)pkdf->key.suffix.Cb();

	//	get data of kdf
	rgdata[idata] = pkdf->data;
	++idata;

	*pfFlags = pkdf->fFlags;
	if ( ple_keylen->le_cbPrefix > 0 )
		{
		*pfFlags |= fNDCompressed;
		}
	else
		{
		*pfFlags &= ~fNDCompressed;
		}
	
	return idata;
	}


//  ================================================================
LOCAL INT CdataNDIKeydataflagsToDataflags(
	const KEYDATAFLAGS 	* const pkdf,
	DATA 				* const rgdata,
	INT 				* const pfFlags,
	LE_KEYLEN			* const ple_keylen )
//  ================================================================
//
//  rgdata must have at least cdataKDF elements
//
//-
	{
	ASSERT_VALID( pkdf );
	
	INT idata = 0;

	if ( !pkdf->key.prefix.FNull() )
		{
		//  prefix compresses
		Assert( pkdf->key.prefix.Cb() > cbPrefixOverhead );
		Assert( FNDCompressed( *pkdf ) );

		ple_keylen->le_cbPrefix = USHORT( pkdf->key.prefix.Cb() );
		rgdata[idata].SetPv( &ple_keylen->le_cbPrefix );
		rgdata[idata].SetCb( cbPrefixOverhead );
		++idata;
		}
	else
		{
		Assert( !FNDCompressed( *pkdf ) );
		}

	ple_keylen->le_cbSuffix = USHORT( pkdf->key.suffix.Cb() );
	rgdata[idata].SetPv( &ple_keylen->le_cbSuffix );
	rgdata[idata].SetCb( cbKeyCount );
	++idata;

	rgdata[idata] = pkdf->key.suffix;
	++idata;

	rgdata[idata] = pkdf->data;
	++idata;

	*pfFlags = pkdf->fFlags;
	
	return idata;
	}


//  ================================================================
VOID NDIGetKeydataflags( const CPAGE& cpage, INT iline, KEYDATAFLAGS * pkdf )
//  ================================================================
	{
	LINE line;
	cpage.GetPtr( iline, &line );
	NDILineToKeydataflags( &line, pkdf );
	if( FNDCompressed( *pkdf ) )
		{
		cpage.GetPtrExternalHeader( &line );
		if ( !cpage.FRootPage() )
			{
			pkdf->key.prefix.SetPv( (BYTE *)line.pv );
			}
		else
			{
			Assert( fFalse );
			if ( cpage.FSpaceTree() )
				{
				pkdf->key.prefix.SetPv( (BYTE *)line.pv + sizeof( SPLIT_BUFFER ) );
				}
			else
				{
				pkdf->key.prefix.SetPv( (BYTE *)line.pv + sizeof( SPACE_HEADER ) );
				}
			}
		}
	else
		{
		Assert( 0 == pkdf->key.prefix.Cb() );
		pkdf->key.prefix.SetPv( NULL );
		}
	
	ASSERT_VALID( pkdf );
	}


//  ================================================================
INLINE VOID NDIGetBookmark( const FUCB * pfucb, const CPAGE& cpage, INT iline, BOOKMARK * pbookmark )
//  ================================================================
//
//  returns the correct bookmark for the node. this depends on wether the index is
//  unique or not. may need the FUCB to determine if the index is unique
//
//-
	{
	KEYDATAFLAGS keydataflags;
	NDIGetKeydataflags( cpage, iline, &keydataflags );

	NDGetBookmarkFromKDF( pfucb, keydataflags, pbookmark );
	
	ASSERT_VALID( pbookmark );
	}


//  ================================================================
INLINE VOID NDISetCompressed( KEYDATAFLAGS& kdf )
//  ================================================================
	{
	kdf.fFlags |= fNDCompressed;
#ifdef DEBUG_NODE
	Assert( FNDCompressed( kdf ) );
#endif	//	DEBUG_NODE
	}

	
//  ================================================================
INLINE VOID NDIResetCompressed( KEYDATAFLAGS& kdf )
//  ================================================================
	{
	kdf.fFlags &= ~fNDCompressed;
#ifdef DEBUG_NODE
	Assert( !FNDCompressed( kdf ) );
#endif	//	DEBUG_NODE
	}

	
//  ================================================================
INLINE VOID NDISetFlag( CSR * pcsr, INT fFlag )
//  ================================================================
	{
	KEYDATAFLAGS kdf;
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdf );
	pcsr->Cpage().ReplaceFlags( pcsr->ILine(), kdf.fFlags | fFlag );

#ifdef DEBUG_NODE
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdf );
	Assert( kdf.fFlags & fFlag );	//  assert the flag is set
#endif	//  DEBUG_NODE
	}


//  ================================================================
INLINE VOID NDIResetFlag( CSR * pcsr, INT fFlag )
//  ================================================================
	{
	Assert( pcsr );

	KEYDATAFLAGS kdf;
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdf );
///	Assert( kdf.fFlags & fFlag );	//  unsetting a flag that is not set?!
	pcsr->Cpage().ReplaceFlags( pcsr->ILine(), kdf.fFlags & ~fFlag );

#ifdef DEBUG_NODE
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdf );
	Assert( !( kdf.fFlags & fFlag ) );
#endif	//  DEBUG_NODE
	}


//  ================================================================
LOCAL INT IlineNDISeekGEQ( const FUCB * pfucb, const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare )
//  ================================================================
//
//  finds the first item greater or equal to the
//  given bookmark, returning the result of the last comparison
//
//-
	{
	Assert( cpage.FLeafPage() );
	
	const INT		clines 		= cpage.Clines( );	
	Assert( clines > 0 );	// will not work on an empty page
	INT				ilineFirst	= 0;
	INT				ilineLast	= clines - 1;
	INT				ilineMid	= 0;
	INT				iline		= -1;
	INT				compare		= 0;
	BOOKMARK		bmNode;

	while( ilineFirst <= ilineLast )	
		{
		ilineMid = (ilineFirst + ilineLast)/2;
		NDIGetBookmark ( pfucb, cpage, ilineMid, &bmNode );
		compare = CmpKeyData( bmNode, bm );
			
		if ( compare < 0 )
			{
			//  the midpoint item is less than what we are looking for. look in top half
			ilineFirst = ilineMid + 1;
			}
		else if ( 0 == compare )
			{
			//  we have found the item
			iline = ilineMid;
			break;
			}
		else	// ( compare > 0 )
			{
			//  the midpoint item is greater than what we are looking for. look in bottom half
			ilineLast = ilineMid;
			if ( ilineLast == ilineFirst )
				{
				iline = ilineMid;
				break;
				}
			}
		}

	if( 0 != compare && 0 == bm.data.Cb() )
		{
		*plastCompare = CmpKey( bmNode.key, bm.key );	//lint !e772
		}
	else
		{
		*plastCompare = compare;
		}
			
#ifdef DEBUG_NODE_SEEK
	INT compareT;
	const INT	ilineT = IlineNDISeekGEQDebug( pfucb, cpage, bm, &compareT );
	if ( ilineT >= 0 )
		{
		if ( compareT > 0 )
			{
			Assert( *plastCompare > 0 );
			}
		else if ( compareT < 0 )
			{
			Assert( *plastCompare < 0 );
			}
		else
			{
			Assert( 0 == *plastCompare );
			}
		}
	Assert( ilineT == iline );
#endif
	return iline;
	}


//  ================================================================
LOCAL INT IlineNDISeekGEQInternal(
	const CPAGE& cpage,
	const BOOKMARK& bm,
	INT * plastCompare )
//  ================================================================
//
//	seeks for bookmark in an internal page. We use binary search. At any
//  time the iline ilineLast is >= bm. To preserve this we include the
//  midpoint in the new range when the item we are seeking for is in the
//  lower half of the range.
//
//-
	{
	Assert( !cpage.FLeafPage() || fGlobalRepair );
	
	const INT		clines 		= cpage.Clines( );	
	Assert( clines > 0 );	// will not work on an empty page
	KEYDATAFLAGS	kdfNode;
	INT				ilineFirst	= 0;
	INT				ilineLast	= clines - 1;
	INT				ilineMid	= 0;
	INT				compare		= 0;

	LINE			lineExternalHeader;
	LINE			line;
	cpage.GetPtrExternalHeader( &lineExternalHeader );
	kdfNode.key.prefix.SetPv( lineExternalHeader.pv );

	while( ilineFirst <= ilineLast )
		{
		ilineMid = (ilineFirst + ilineLast)/2;

		cpage.GetPtr( ilineMid, &line );
		NDILineToKeydataflags( &line, &kdfNode );
		Assert( kdfNode.key.prefix.Pv() == lineExternalHeader.pv );
		
		Assert( sizeof( PGNO ) == kdfNode.data.Cb() );
		compare = CmpKeyWithKeyData( kdfNode.key, bm );

		if ( compare < 0 )
			{
			//  the midpoint item is less than what we are looking for. look in top half
			ilineFirst = ilineMid + 1;
			}
		else if ( 0 == compare )
			{
			//  we have found the item
			break;
			}
		else	// ( compare > 0 )
			{
			//  the midpoint item is greater than what we are looking for. look in bottom half
			ilineLast = ilineMid;
			if ( ilineLast == ilineFirst )
				{
				break;
				}
			}
		}

	Assert( ilineMid >= 0 );
	Assert( ilineMid < clines );

	if ( 0 == compare )
		{
		//  we found the item
		*plastCompare = 0;
		}
	else
		{
		*plastCompare	= 1;
		}

#ifdef DEBUG_NODE_SEEK
	INT compareT;
	const INT	ilineT = IlineNDISeekGEQInternalDebug( cpage, bm, &compareT );
	if ( compareT > 0 )
		{
		Assert( *plastCompare > 0 );
		}
	else if ( compareT < 0 )
		{
		Assert( *plastCompare < 0 );
		}
	else
		{
		Assert( 0 == *plastCompare );
		}
	Assert( ilineT == ilineMid );
#endif

	return ilineMid;
	}


#ifdef DEBUG_NODE_SEEK
//  ================================================================
LOCAL INT IlineNDISeekGEQDebug( const FUCB * pfucb, const CPAGE& cpage, const BOOKMARK& bm, INT * plastCompare )
//  ================================================================
//
//  finds the first item greater or equal to the
//  given bookmark, returning the result of the last comparison
//
//-
	{
	Assert( cpage.FLeafPage() );
	
	const INT		clines 	= cpage.Clines( );
	BOOKMARK		bmNode;

	INT				iline	= 0;
	for ( ; iline < clines; iline++ )
		{
		NDIGetBookmark ( pfucb, cpage, iline, &bmNode );
		INT cmp = CmpKey( bmNode.key, bm.key );
		
		if ( cmp < 0 )
			{
			//  keep on looking
			}
		else if ( cmp > 0 || bm.data.Cb() == 0 )
			{
			Assert( cmp > 0 || 
					bm.data.Cb() == 0 && cmp == 0 );
			*plastCompare = cmp;
			return iline;
			}
		else
			{
			//	key is same
			//	check data -- only if we are seeking for a key-data
			Assert( cmp == 0 );
			Assert( bm.data.Cb() != 0 );

			cmp = CmpData( bmNode.data, bm.data );
			if ( cmp >= 0 )
				{
				*plastCompare = cmp;
				return iline;
				}
			}
		}

	return -1;
	}

	
//  ================================================================
LOCAL INT IlineNDISeekGEQInternalDebug(
	const CPAGE& cpage,
	const BOOKMARK& bm,
	INT * plastCompare )
//  ================================================================
//
//	seeks for bookmark in an internal page
//
//-
	{
	Assert( !cpage.FLeafPage() );
	
	const INT		clines = cpage.Clines( ) - 1;
	KEYDATAFLAGS	kdfNode;
	INT				iline;

	for ( iline = 0; iline < clines; iline++ )
		{
		NDIGetKeydataflags ( cpage, iline, &kdfNode );
		Assert( sizeof( PGNO ) == kdfNode.data.Cb() );

		const INT compare = CmpKeyWithKeyData( kdfNode.key, bm );
		if ( compare < 0 )
			{
			//  keep on looking
			}
		else
			{
			Assert( compare >= 0 );
			*plastCompare = compare;
			return iline;
			}
		}

	//	for internal pages
	//	last key or NULL is greater than everything else
#ifdef DEBUG
	Assert( cpage.Clines() - 1 == iline );
	NDIGetKeydataflags( cpage, iline, &kdfNode );
	Assert( kdfNode.key.FNull() ||
			CmpKeyWithKeyData( kdfNode.key, bm ) > 0 );
#endif

	*plastCompare = 1;
	return iline;
	}

#endif	//	DEBUG_NODE_SEEK


//  ================================================================
LOCAL ERR ErrNDISeekInternalPage( CSR * pcsr, const BOOKMARK& bm )
//  ================================================================
//
//	Seek on an internal page never returns wrnNDFoundLess
//	if bookmark.data is not null, never returns wrnNDEqual
//
//-
	{
	ASSERT_VALID( &bm );

	ERR			err;
	INT			compare;
	const INT	iline	= IlineNDISeekGEQInternal( pcsr->Cpage(), bm, &compare );	
	Assert( iline >= 0 );
	Assert( iline < pcsr->Cpage().Clines( ) );

	if ( 0 == compare )
		{
		//  we found an exact match
		//	page delimiter == bookmark of search 
		//	the cursor is placed on node next to S
		//	because no node in the subtree rooted at S 
		//	can have a key == XY -- such cursor placement obviates a moveNext at BT level
		//	Also, S can not be the last node in page because if it were,
		//	we would not have seeked down to this page!
		pcsr->SetILine( iline + 1 );
		err = JET_errSuccess;
		}
	else
		{
		//  we are on the first node greater than the BOOKMARK
		pcsr->SetILine( iline );
		err = ErrERRCheck( wrnNDFoundGreater );
		}

	return err;
	}

			
//  ================================================================
LOCAL ERR ErrNDISeekLeafPage( const FUCB * pfucb, CSR * pcsr, const BOOKMARK& bm )
//  ================================================================
//
//	Seek on leaf page returns wrnNDFoundGreater/Less/Equal
//
//-
	{
	ASSERT_VALID( &bm );

	ERR			err;
	INT			compare;
	const INT	iline	= IlineNDISeekGEQ(
											pfucb,
											pcsr->Cpage(), 
											bm, 
											&compare );
	Assert( iline < pcsr->Cpage().Clines( ) );

	if ( iline >= 0 && 0 == compare )
		{
		//  great! we found the node 
		pcsr->SetILine( iline );
		err = JET_errSuccess;
		}
	else if ( iline < 0 )
		{
		//	all nodes in page are less than XY
		//  place cursor on last node in page;
		pcsr->SetILine( pcsr->Cpage().Clines( ) - 1 );
		err = ErrERRCheck( wrnNDFoundLess );
		}
	else
		{
		//	node S exists && key-data(S) > XY
		pcsr->SetILine( iline );
		err = ErrERRCheck( wrnNDFoundGreater );
		}

	return err;
	}


//  ****************************************************************
//  EXTERNAL ROUTINES
//  ****************************************************************


//  ================================================================
VOID NDMoveFirstSon( FUCB * pfucb, CSR * pcsr )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );

	pcsr->SetILine( 0 );
	NDGet( pfucb, pcsr );
	}


//  ================================================================
VOID NDMoveLastSon( FUCB * pfucb, CSR * pcsr )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	
	pcsr->SetILine( pcsr->Cpage().Clines() - 1 );
	NDGet( pfucb, pcsr );
	}


//  ================================================================
ERR ErrNDVisibleToCursor( FUCB * pfucb, BOOL * pfVisibleToCursor )
//  ================================================================
//
//	returns true if the node, as seen by cursor, exists
//	and is not deleted
//
//-
	{
	ERR				err						= JET_errSuccess;
	BOOL			fVisible;
	
	ASSERT_VALID( pfucb );
	AssertBTType( pfucb );
	AssertNDGet( pfucb );

	//	if session cursor isolation model is not dirty and node
	//	has version, then call version store for appropriate version.
	if ( FNDPossiblyVersioned( pfucb, Pcsr( pfucb ) )
		&& !FPIBDirty( pfucb->ppib ) )
		{
		//	get bookmark from node in page
		BOOKMARK		bm;
		
		NDGetBookmarkFromKDF( pfucb, pfucb->kdfCurr, &bm );
		
		NS ns;
		Call( ErrVERAccessNode( pfucb, bm, &ns ) );
		switch( ns )
			{
			case nsVersion:
			case nsVerInDB:
				fVisible = fTrue;
				break;
			case nsDatabase:
				fVisible = !FNDDeleted( pfucb->kdfCurr );
				break;
			case nsNone:
			case nsInvalid:
			default:
				Assert( nsInvalid == ns );
				fVisible = fFalse;
				break;
			}
		}
	else
		{
		fVisible = !FNDDeleted( pfucb->kdfCurr );
		}

	*pfVisibleToCursor = fVisible;

HandleError:
	Assert( JET_errSuccess == err || 0 == pfucb->ppib->level );
	return err;
	}


//  ================================================================
BOOL FNDPotVisibleToCursor( const FUCB * pfucb, CSR * pcsr )
//  ================================================================
//
//	returns true if node, as seen by cursor,
//	potentially exists
//	i.e., node is uncommitted by other or not deleted in page
//
//-
	{
	ASSERT_VALID( pfucb );

	BOOL	fVisible = !( FNDDeleted( pfucb->kdfCurr ) );

	//	if session cursor isolation model is not dirty and node
	//	has version, then call version store for appropriate version.

	//	UNDONE: Use FNDPossiblyVersioned() if this function
	//	is called in any active code paths (currently, it's
	//	only called in DEBUG code or in code paths that
	//	should be impossible

	if ( FNDVersion( pfucb->kdfCurr )
		&& !FPIBDirty( pfucb->ppib ) )
		{
		BOOKMARK	bm;
		NDGetBookmarkFromKDF( pfucb, pfucb->kdfCurr, &bm );
		const VS vs = VsVERCheck( pfucb, pcsr, bm );
		fVisible = FVERPotThere( vs, !fVisible );
		}

	return fVisible;
	}


//  ================================================================
VOID NDBulkDelete( CSR * pcsr, INT clines )
//  ================================================================
//
//	deletes clines lines from current position in page
//		- expected to be at the end of page
//
//-
	{
	ASSERT_VALID( pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );
	Assert( 0 <= clines );
	Assert( pcsr->ILine() >= 0 );
	Assert( pcsr->Cpage().Clines() == pcsr->ILine() + clines );

	//	delete lines from the end to avoid reorganization of tag array
	INT	iline = pcsr->ILine() + clines - 1;
	for ( ; iline >= pcsr->ILine(); --iline )
		{
		Assert( pcsr->Cpage().Clines() - 1 == iline );
		pcsr->Cpage().Delete( iline );
		}
	}


//  ================================================================
VOID NDInsert( FUCB *pfucb, CSR * pcsr, const KEYDATAFLAGS * pkdf, INT cbPrefix )
//  ================================================================
//
//	inserts node into page at current cursor position
//	cbPrefix bytes of *pkdf is prefix key part
//	no logging/versioning
//	used by split to perform split operation
//
//-
	{
#ifdef DEBUG
	if( pfucbNil != pfucb )
		{
		ASSERT_VALID( pfucb );
		Assert( pfucb->ppib->level > 0 || !rgfmp[pfucb->ifmp].FLogOn() );
		}
	ASSERT_VALID( pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );
	ASSERT_VALID( pkdf );
#endif

	DATA			rgdata[cdataKDF+1];
	INT				fFlagsLine;
	KEYDATAFLAGS	kdf					= *pkdf;
	LE_KEYLEN		le_keylen;
	le_keylen.le_cbPrefix = (USHORT)cbPrefix;

	const INT 		cdata				= CdataNDIPrefixAndKeydataflagsToDataflags(
													&kdf,
													rgdata,
													&fFlagsLine,
													&le_keylen );
	Assert( cdata <= cdataKDF + 1 );

	pcsr->Cpage().Insert( pcsr->ILine(), 
						  rgdata, 
						  cdata,
						  fFlagsLine );

#ifdef DEBUG_NODE_INSERT
	if( pfucbNil != pfucb )
		{
		NDAssertNDInOrder( pfucb, pcsr );
		}
#endif	// DEBUG_NODE
	}


//  ================================================================
VOID NDInsert( FUCB *pfucb, CSR * pcsr, const KEYDATAFLAGS * pkdf )
//  ================================================================
//
//	inserts node into page at current cursor position
//	no logging/versioning
//	used by split to perform split operation
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );
	ASSERT_VALID( pkdf );
	Assert( pfucb->ppib->level > 0 || !rgfmp[pfucb->ifmp].FLogOn() );

	DATA		rgdata[cdataKDF];
	INT			fFlagsLine;
	LE_KEYLEN	le_keylen;
	const INT	cdata			= CdataNDIKeydataflagsToDataflags(
											pkdf,
											rgdata,
											&fFlagsLine,
											&le_keylen );
	pcsr->Cpage().Insert( pcsr->ILine(), 
						  rgdata, 
						  cdata,
						  fFlagsLine );
	Assert( cdata <= cdataKDF );

#ifdef DEBUG_NODE_INSERT
	NDAssertNDInOrder( pfucb, pcsr );
#endif	// DEBUG_NODE

	NDGet( pfucb, pcsr );
	}


//  ================================================================
VOID NDReplaceForUpgrade(
	CPAGE * const pcpage,
	const INT iline,
	const DATA * const pdata,
	const KEYDATAFLAGS& kdfOld )
//  ================================================================
//
//	replaces data in page at current cursor position
//	no logging/versioning
//	used by upgrade to change record format
//  does not do a NDGet!
//
//-
	{
	KEYDATAFLAGS	kdf = kdfOld;
	DATA 			rgdata[cdataKDF];
	INT  			fFlagsLine;
	LE_KEYLEN		le_keylen;

#ifdef DEBUG
	//	make sure the correct KDF was passed in
	KEYDATAFLAGS kdfDebug;
	NDIGetKeydataflags( *pcpage, iline, &kdfDebug );
	Assert( kdfDebug.key.prefix.Pv() == kdfOld.key.prefix.Pv() );
	Assert( kdfDebug.key.prefix.Cb() == kdfOld.key.prefix.Cb() );
	Assert( kdfDebug.key.suffix.Pv() == kdfOld.key.suffix.Pv() );
	Assert( kdfDebug.key.suffix.Cb() == kdfOld.key.suffix.Cb() );
	Assert( kdfDebug.data.Pv() == kdfOld.data.Pv() );
	Assert( kdfDebug.data.Cb() == kdfOld.data.Cb() );
#endif	//	DEBUG

	Assert( kdf.data.Cb() >= pdata->Cb() );	//	cannot grow the record as we have pointers to data on the page
	kdf.data = *pdata;

	const INT	cdata		= CdataNDIKeydataflagsToDataflags(
										&kdf,
										rgdata,
										&fFlagsLine,
										&le_keylen );
	Assert( cdata <= ( sizeof( rgdata ) / sizeof( rgdata[0]) ) );
	
	pcpage->Replace( iline, rgdata, cdata, fFlagsLine );
	}


//  ================================================================
VOID NDReplace( CSR * pcsr, const DATA * pdata )
//  ================================================================
//
//	replaces data in page at current cursor position
//	no logging/versioning
//	used by split to perform split operation
//  does not do a NDGet!
//
//-
	{
	ASSERT_VALID( pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );
	ASSERT_VALID( pdata );

	KEYDATAFLAGS	kdf;
	DATA 			rgdata[cdataKDF];
	INT  			fFlagsLine;
	LE_KEYLEN		le_keylen;

	//	get key and flags from page
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdf );
	kdf.data = *pdata;

	//	replace node with new data
	const INT		cdata			= CdataNDIKeydataflagsToDataflags(
												&kdf,
												rgdata,
												&fFlagsLine,
												&le_keylen );
	Assert( cdata <= cdataKDF );
	pcsr->Cpage().Replace( pcsr->ILine(), rgdata, cdata, fFlagsLine );
	}


//  ================================================================
VOID NDDelete( CSR *pcsr )
//  ================================================================
//
//	deletes node from page at current cursor position
//	no logging/versioning
//
//-
	{
	ASSERT_VALID( pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );
	Assert( pcsr->ILine() < pcsr->Cpage().Clines() );
	
	pcsr->Cpage().Delete( pcsr->ILine() );
	}


//  ================================================================
ERR ErrNDSeek( FUCB * pfucb, CSR * pcsr, const BOOKMARK& bookmark )
//  ================================================================
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );
	ASSERT_VALID( &bookmark );
	AssertSz( pcsr->Cpage().Clines() > 0, "Seeking on an empty page" );

	const BOOL	fInternalPage	= !pcsr->Cpage().FLeafPage();
	ERR			err				= JET_errSuccess;

	if ( fInternalPage )
		{
		err = ErrNDISeekInternalPage( pcsr, bookmark );
		}
	else
		{
		err = ErrNDISeekLeafPage( pfucb, pcsr, bookmark );
		}
	
	NDGet( pfucb, pcsr );
	return err; 
	}


//  ================================================================
VOID NDGet( FUCB * pfucb, const CSR * pcsr )
//  ================================================================
	{
	NDIAssertCurrencyExists( pfucb, pcsr );

	KEYDATAFLAGS keydataflags;
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &keydataflags );
	pfucb->kdfCurr = keydataflags;
	pfucb->fBookmarkPreviouslySaved = fFalse;
	}


//  ================================================================
ERR ErrNDInsert(
		FUCB * const pfucb,
		CSR * const pcsr,
		const KEYDATAFLAGS * const pkdf,
		const DIRFLAG dirflag,
		const RCEID rceid,
		const VERPROXY * const pverproxy )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	ASSERT_VALID( pkdf );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( NULL == pverproxy || proxyCreateIndex == pverproxy->proxy );
	
	ERR			err				= JET_errSuccess;
	const BOOL	fVersion		= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL	fLogging		= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();

	KEYDATAFLAGS kdfT = *pkdf;
	
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceid );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	//	BUGFIX #151574: caller should have already ascertained that the node will
	//	fit on the page, but at that time, the caller may not have had the
	//	write-latch on the page and couldn't update the cbUncommittedFree, so
	//	now that we have the write-latch, we have re-compute to guarantee that
	//	the cbFree doesn't become less than the cbUncommittedFree
	const ULONG	cbReq			= CbNDNodeSizeCompressed( kdfT );
	if ( cbReq > pcsr->Cpage().CbFree( ) - pcsr->Cpage().CbUncommittedFree( ) )
		{
		//	this call to FNDFreePageSpace() is now guaranteed to succeed and
		//	update the cbUncommittedFree if necessary
		const BOOL	fFreePageSpace	= FNDFreePageSpace( pfucb, pcsr, cbReq );
		Assert( fFreePageSpace );

		//	now that we have the write-latch on the page, we are guaranteed to
		//	be able to upgrade cbUncommittedFree on the page, and that in turn
		//	should guarantee that we should now have enough page space, so we
		//	Enforce() this to ensure we don't corrupt the page
		Enforce( cbReq <= pcsr->Cpage().CbFree() - pcsr->Cpage().CbUncommittedFree() );
		}

	if ( fLogging )
		{
		//	log the operation, getting the lgpos
		LGPOS	lgpos;

		Call( ErrLGInsert( pfucb, pcsr, kdfT, rceid, dirflag, &lgpos, pverproxy, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		pcsr->Dirty( );
		}

	//	insert operation can not fail after this point
	//	since we have already logged the operation
	if( fVersion )
		{
		kdfT.fFlags |= fNDVersion;
		}
	NDInsert( pfucb, pcsr, &kdfT );
	NDGet( pfucb, pcsr );

HandleError:
	Assert( JET_errSuccess == err || fLogging );
#ifdef DEBUG_NODE_INSERT
	NDAssertNDInOrder( pfucb, pcsr );
#endif	// DEBUG_NODE_INSERT

	return err;
	}


INLINE ERR ErrNDILogReplace(
	FUCB			* const pfucb,
	CSR				* const pcsr,
	const DATA&		data,
	const DIRFLAG	dirflag,
	const RCEID		rceid )
	{
	ERR				err;
	DATA			dataDiff;
	SIZE_T			cbDiff					= 0;
	BOOL			fDiffBufferAllocated	= fFalse;

	AssertNDGet( pfucb, pcsr );		// logdiff needs access to kdfCurr

	if ( dirflag & fDIRLogColumnDiffs )
		{
		Assert( data.Cb() <= cbRECRecordMost );
		dataDiff.SetPv( PvOSMemoryHeapAlloc( cbRECRecordMost ) );
		if ( NULL != dataDiff.Pv() )
			{
			fDiffBufferAllocated = fTrue;
			LGSetColumnDiffs(
					pfucb,
					data,
					pfucb->kdfCurr.data,
					(BYTE *)dataDiff.Pv(),
					&cbDiff );
			Assert( cbDiff <= cbRECRecordMost );
			}
		}
	else if ( dirflag & fDIRLogChunkDiffs )
		{
		Assert( data.Cb() <= g_cbColumnLVChunkMost );
		dataDiff.SetPv( PvOSMemoryHeapAlloc( g_cbColumnLVChunkMost ) );
		if ( NULL != dataDiff.Pv() )
			{
			fDiffBufferAllocated = fTrue;
			LGSetLVDiffs(
					pfucb,
					data,
					pfucb->kdfCurr.data,
					(BYTE *)dataDiff.Pv(),
					&cbDiff );
			Assert( cbDiff <= g_cbColumnLVChunkMost );
			}
		}

	Assert( fDiffBufferAllocated || 0 == cbDiff );
	dataDiff.SetCb( cbDiff );

	//	log the operation, getting the lgpos
	LGPOS	lgpos;
	Call( ErrLGReplace( pfucb, 
						pcsr, 
						pfucb->kdfCurr.data, 
						data,
						( cbDiff > 0 ? &dataDiff : NULL ),
						rceid,
						dirflag, 
						&lgpos,
						fMustDirtyCSR ) );
	pcsr->Cpage().SetLgposModify( lgpos );

HandleError:
	if ( fDiffBufferAllocated )
		OSMemoryHeapFree( dataDiff.Pv() );

	return err;
	}

//  ================================================================
ERR ErrNDReplace(
		FUCB * const pfucb,
		CSR * const pcsr,
		const DATA * const pdata,
		const DIRFLAG dirflag,
		const RCEID rceid,
		const RCE * const prceReplace )
//  ================================================================
//
//  UNDONE: we only need to replace the data, not the key
//			the page level should take the length of the key
//			and not replace the data
//
//-
	{
	NDIAssertCurrencyExists( pfucb, pcsr );	//  we should have done a FlagReplace
	ASSERT_VALID( pdata );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	
	ERR			err			= JET_errSuccess;

	NDGet( pfucb, pcsr );
	const INT	cbDataOld	= pfucb->kdfCurr.data.Cb();
	const INT	cbReq 		= pdata->Cb() - cbDataOld;
	const BOOL	fDirty		= !( dirflag & fDIRNoDirty );

	if ( cbReq > 0 && !FNDFreePageSpace( pfucb, pcsr, cbReq ) )
		{
		//	requested space not available in page
		//	check if same node has enough uncommitted freed space to be used
		Assert( fDirty );

		const ULONG		cbReserved	= CbNDReservedSizeOfNode( pfucb, pcsr );
		if ( cbReq > cbReserved + pcsr->Cpage().CbFree() - pcsr->Cpage().CbUncommittedFree() )
			{
			err = ErrERRCheck( errPMOutOfPageSpace );
			Assert( fDirty );
			return err;
			}
		}

	const BOOL		fVersion			= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL		fLogging			= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceid );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
	Assert( ( prceNil == prceReplace ) == !fVersion || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	INT				cdata;
	INT  			fFlagsLine;
	KEYDATAFLAGS	kdf;
	DATA 			rgdata[cdataKDF+1];
	LE_KEYLEN		le_keylen;

	Assert( fDirty || pcsr->FDirty() );

	if ( fLogging )
		{
		//	we have to dirty
		Assert( fDirty );
		Call( ErrNDILogReplace(
					pfucb,
					pcsr,
					*pdata,
					dirflag,
					rceid ) );
		}
	else if ( fDirty )
		{
		pcsr->Dirty( );
		}

	//	operation cannot fail from here on

	//	get key and flags from fucb
	kdf.key 	= pfucb->bmCurr.key;
	kdf.data 	= *pdata;
	kdf.fFlags 	= pfucb->kdfCurr.fFlags | ( fVersion ? fNDVersion : 0 );

#ifdef DEBUG
		{
		AssertNDGet( pfucb, pcsr );

		KEYDATAFLAGS	kdfT;

		//	get key and flags from page. compare with bookmark
		NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &kdfT );
		Assert( CmpKey( kdf.key, kdfT.key ) == 0 );
		}
#endif	//  DEBUG

	//	replace node with new data
	le_keylen.le_cbPrefix = USHORT( pfucb->kdfCurr.key.prefix.Cb() );
	cdata = CdataNDIPrefixAndKeydataflagsToDataflags( &kdf, rgdata, &fFlagsLine, &le_keylen );
	Assert( cdata <= cdataKDF + 1 );

	if ( prceNil != prceReplace && cbReq > 0 )
		{
		Assert( fVersion || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		//	set uncommitted freed space in page for growing node
		VERSetCbAdjust( pcsr, prceReplace, pdata->Cb(), cbDataOld, fDoUpdatePage );
		}

	pcsr->Cpage().Replace( pcsr->ILine(), rgdata, cdata, fFlagsLine );

	if ( prceNil != prceReplace && cbReq < 0 )
		{
		Assert( fVersion || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		//	set uncommitted freed space for shrinking node
		VERSetCbAdjust( pcsr, prceReplace, pdata->Cb(), cbDataOld, fDoUpdatePage );
		}

	NDGet( pfucb, pcsr );

HandleError:
	Assert( JET_errSuccess == err || fLogging );	
	return err;
	}


//  ================================================================
ERR ErrNDFlagInsert(
		FUCB * const pfucb,
		CSR * const pcsr,
		const DIRFLAG dirflag,
		const RCEID rceid,
		const VERPROXY * const pverproxy )
//  ================================================================
//
//  this is a flag-undelete with the correct logging and version store stuff
//
//-
	{
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( NULL == pverproxy || proxyCreateIndex == pverproxy->proxy );

	ERR			err				= JET_errSuccess;
	const BOOL	fVersion		= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL	fLogging		= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	const BOOL	fDirty		= !( dirflag & fDIRNoDirty );
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceid );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	NDGet( pfucb, pcsr );
	
	Assert( fDirty || pcsr->FDirty() );

	if ( fLogging )
		{		
		//	log the operation, getting the lgpos
		LGPOS	lgpos;

		// we have to dirty
		Assert (fDirty);
		
		Call( ErrLGFlagInsert( pfucb, pcsr, pfucb->kdfCurr, rceid, dirflag, &lgpos, pverproxy, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		if ( fDirty )
			{
			pcsr->Dirty( );
			}
		}
	
	NDIResetFlag( pcsr, fNDDeleted );
	if ( fVersion )
		{
		NDISetFlag ( pcsr, fNDVersion );
		}
	NDGet( pfucb, pcsr );
		
HandleError:
	Assert( JET_errSuccess == err || fLogging );
#ifdef DEBUG_NODE_INSERT
	NDAssertNDInOrder( pfucb, pcsr );
#endif	// DEBUG_NODE_INSERT

	return err;
	}


//  ================================================================
ERR ErrNDFlagInsertAndReplaceData(
	FUCB				* const pfucb,
	CSR					* const pcsr,
	const KEYDATAFLAGS	* const pkdf,
	const DIRFLAG		dirflag,
	const RCEID			rceidInsert,
	const RCEID			rceidReplace,
	const RCE			* const prceReplace,
	const VERPROXY * const pverproxy )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pkdf->data.Cb() > 0 );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( NULL == pverproxy || proxyCreateIndex == pverproxy->proxy );
	
	ERR			err				= JET_errSuccess;
	const BOOL	fVersion		= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL	fLogging		= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceidInsert );
	Assert( !fVersion || rceidNull != rceidReplace );
	Assert( !fVersion || prceNil != prceReplace );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	NDGet( pfucb, pcsr );
	Assert( FNDDeleted( pfucb->kdfCurr ) );
	Assert( FKeysEqual( pkdf->key, pfucb->kdfCurr.key ) );

	const INT	cbDataOld	= pfucb->kdfCurr.data.Cb();
	const INT 	cbReq 		= pkdf->data.Cb() - cbDataOld;
							
	if ( cbReq > 0 && !FNDFreePageSpace ( pfucb, pcsr, cbReq ) )
		{
		//	requested space not available in page
		//	check if same node has enough uncommitted freed space to be used
		const ULONG		cbReserved	= CbNDReservedSizeOfNode( pfucb, pcsr );
		if ( cbReq > cbReserved + pcsr->Cpage().CbFree() - pcsr->Cpage().CbUncommittedFree() )
			{
			err = ErrERRCheck( errPMOutOfPageSpace );
			return err;
			}
		}
	
	KEYDATAFLAGS	kdf;
	INT 			cdata;
	DATA 			rgdata[cdataKDF+1];
	INT				fFlagsLine;
	LE_KEYLEN		le_keylen;

 				
	Assert( !(dirflag & fDIRNoDirty ) );

	if ( fLogging )
		{
		//	log the operation, getting the lgpos
		LGPOS	lgpos;

		Call( ErrLGFlagInsertAndReplaceData( pfucb, 
											 pcsr, 
											 *pkdf, 
											 rceidInsert,
											 rceidReplace,
											 dirflag, 
											 &lgpos,
											 pverproxy,
											 fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		pcsr->Dirty( );
		}
		
	//	get key from fucb
	kdf.key 		= pkdf->key;
	kdf.data 		= pkdf->data;
	kdf.fFlags 		= pfucb->kdfCurr.fFlags | ( fVersion ? fNDVersion : 0 );

	Assert( FKeysEqual( kdf.key, pfucb->kdfCurr.key ) );

	le_keylen.le_cbPrefix = USHORT( pfucb->kdfCurr.key.prefix.Cb() );

	//	replace node with new data
	cdata = CdataNDIPrefixAndKeydataflagsToDataflags(
					&kdf,
					rgdata,
					&fFlagsLine,
					&le_keylen );


	if ( prceNil != prceReplace && cbReq > 0 )
		{
		Assert( fVersion || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		//	set uncommitted freed space in page for growing node
		VERSetCbAdjust( pcsr, prceReplace, pkdf->data.Cb(), cbDataOld, fDoUpdatePage );
		}

	pcsr->Cpage().Replace( pcsr->ILine(), rgdata, cdata, fFlagsLine );

	if ( prceNil != prceReplace && cbReq < 0 )
		{
		Assert( fVersion || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		//	set uncommitted freed space for shrinking node
		VERSetCbAdjust( pcsr, prceReplace, pkdf->data.Cb(), cbDataOld, fDoUpdatePage );
		}
		
	NDIResetFlag( pcsr, fNDDeleted );
	
	NDGet( pfucb, pcsr );
	
HandleError:
	Assert( JET_errSuccess == err || fLogging );	
#ifdef DEBUG_NODE_INSERT
	NDAssertNDInOrder( pfucb, pcsr );
#endif	// DEBUG_NODE_INSERT

	return err;
	}


//  ================================================================
ERR ErrNDDelta(
		FUCB		* const pfucb,
		CSR			* const pcsr,
		const INT	cbOffset,
		const VOID	* const pv,
		const ULONG	cbMax,
		VOID		* const pvOld,
		const ULONG	cbMaxOld,
		ULONG		* const pcbOldActual,
		const DIRFLAG	dirflag,
		const RCEID rceid )
//  ================================================================
//
//  No VERPROXY because it is not used by concurrent create index
//
	{
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pv );
	Assert( sizeof( LONG ) == cbMax );

	const BOOL	fVersion		= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL	fLogging		= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	const BOOL	fDirty			= !( dirflag & fDIRNoDirty );
	const LONG	lDelta			= *(UnalignedLittleEndian< LONG > *)pv;
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceid );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	KEYDATAFLAGS keydataflags;
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &keydataflags );
	Assert( cbOffset <= (INT)keydataflags.data.Cb() );
	Assert( cbOffset >= 0 );

	ERR						err		= JET_errSuccess;
	RCE				* 		prce	= prceNil;
	UnalignedLittleEndian< LONG > * const plDelta = (UnalignedLittleEndian< LONG > *)( (BYTE *)keydataflags.data.Pv() + cbOffset );

	BOOKMARK		bookmark;
	NDIGetBookmark( pfucb, pcsr->Cpage(), pcsr->ILine(), &bookmark );

	if ( pvOld )
		{
		Assert( sizeof( LONG ) == cbMaxOld );
		//	Endian conversion
		*(LONG*)pvOld = *plDelta;
		if ( pcbOldActual )
			{
			*pcbOldActual = sizeof( LONG );
			}
		}

	Assert( fDirty || pcsr->FDirty() );

	if ( fLogging )
		{
		//	log the operation, getting the lgpos
		LGPOS	lgpos;

		// we have to dirty
		Assert (fDirty);
		
		Call( ErrLGDelta( pfucb, pcsr, bookmark, cbOffset, lDelta, rceid, dirflag, &lgpos, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		if ( fDirty )
			{
			pcsr->Dirty( );
			}
		}

	//  we have a pointer to the data on the page so we can modify the data directly 
	*plDelta += lDelta;

	if ( fVersion )
		{
		NDISetFlag ( pcsr, fNDVersion );
		}

HandleError:
	Assert( JET_errSuccess == err || fLogging );
	return err;
	}


//  ================================================================
ERR ErrNDFlagDelete(
		FUCB * const pfucb,
		CSR * const pcsr,
		const DIRFLAG dirflag,
		const RCEID rceid,
		const VERPROXY * const pverproxy )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( NULL == pverproxy || proxyCreateIndex == pverproxy->proxy );

	const BOOL	fVersion		= !( dirflag & fDIRNoVersion ) && !rgfmp[pfucb->ifmp].FVersioningOff();
	const BOOL	fLogging		= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	const BOOL	fDirty		= !( dirflag & fDIRNoDirty );
	const ULONG fFlags		= fNDDeleted | ( fVersion ? fNDVersion : 0 );
	
	Assert( !fLogging || pfucb->ppib->level > 0 );
	Assert( !fVersion || rceidNull != rceid );
	Assert( !fVersion || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	ERR			err				= JET_errSuccess;
	RCE			*prce			= prceNil;

	Assert( fDirty || pcsr->FDirty() );

	if ( fLogging )
		{
		//	log the operation, getting the lgpos 
		LGPOS	lgpos;

		// we have to dirty
		Assert (fDirty);

		Call( ErrLGFlagDelete( pfucb, pcsr, rceid, dirflag, &lgpos, pverproxy, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		if ( fDirty )
			{
			pcsr->Dirty( );
			}
		}

	NDISetFlag( pcsr, fFlags );

HandleError:
	Assert( JET_errSuccess == err || fLogging );	
	return err;
	}


//  ================================================================
VOID NDResetFlagDelete( CSR * pcsr )
//  ================================================================
//
//  this is called by VER to undo. the undo	is already logged
//  so we don't need to log or version
//
//-
	{
	Assert( pcsr->ILine() >= 0 );
	Assert( pcsr->ILine() < pcsr->Cpage().Clines( ) );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	
	NDIResetFlag( pcsr, fNDDeleted );
	}


//  ================================================================
VOID NDDeferResetNodeVersion( CSR * pcsr )
//  ================================================================
//
//  we want to reset the bit but only flush the page if necessary
//  this will update the checksum
//
//-
	{
	Assert( pcsr->ILine() >= 0 );
	Assert( pcsr->ILine() < pcsr->Cpage().Clines( ) );

	LATCH latchOld;
	if ( pcsr->ErrUpgradeToWARLatch( &latchOld ) == JET_errSuccess )
		{
		NDIResetFlag( pcsr, fNDVersion );
		BFDirty( pcsr->Cpage().PBFLatch(), bfdfUntidy );
		pcsr->DowngradeFromWARLatch( latchOld );
		}
	}


//  ================================================================
VOID NDResetVersionInfo( CPAGE * const pcpage )
//  ================================================================
	{
	if ( pcpage->FLeafPage() && !pcpage->FSpaceTree() )
		{
		pcpage->ResetAllFlags( fNDVersion );
		pcpage->SetCbUncommittedFree( 0 );
		}
	}


//  ================================================================
ERR ErrNDFlagVersion( CSR * pcsr )
//  ================================================================
//
//  not logged
//
//-
	{
	Assert( pcsr->ILine() >= 0 );
	Assert( pcsr->ILine() < pcsr->Cpage().Clines( ) );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );

	NDISetFlag( pcsr, fNDVersion );
	return JET_errSuccess;
	}


//  ================================================================
ERR ErrNDDelete( FUCB * pfucb, CSR * pcsr )
//  ================================================================
//
//  delete is called by cleanup to delete *visible* nodes
//	that have been flagged as deleted.  This operation is logged
//	for redo only.
//
//-
	{
#ifdef DEBUG
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	NDGet( pfucb, pcsr );
	Assert( FNDDeleted( pfucb->kdfCurr ) ||
			!FNDVersion( pfucb->kdfCurr ) );
#endif

	ERR			err			= JET_errSuccess;
	const BOOL	fLogging	= rgfmp[pfucb->ifmp].FLogOn() && !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering;
	Assert( !rgfmp[pfucb->ifmp].FLogOn() || !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fLogDisabled );
	Assert( pfucb->ppib->level > 0 || !fLogging );

	//	log the operation
	if ( fLogging )
		{
		LGPOS	lgpos;

		CallR( ErrLGDelete( pfucb, pcsr, &lgpos, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		pcsr->Dirty( );
		}
	
	NDDelete( pcsr );
	
	return err;
	}


//  ================================================================
ERR ErrNDMungeSLVSpace( FUCB * const pfucb,
						CSR * const pcsr,
						const SLVSPACEOPER slvspaceoper,
						const LONG ipage,
						const LONG cpages,
						const DIRFLAG dirflag,
						const RCEID rceid )
//  ================================================================
	{
	NDIAssertCurrencyExists( pfucb, pcsr );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );

	FMP* const	pfmp		= &rgfmp[pfucb->ifmp];
	const BOOL	fLogging	= !( dirflag & fDIRNoLog ) && pfmp->FLogOn();
	const BOOL	fDirty		= !( dirflag & fDIRNoDirty );

	KEYDATAFLAGS keydataflags;
	NDIGetKeydataflags( pcsr->Cpage(), pcsr->ILine(), &keydataflags );
	
	BOOKMARK		bookmark;
	NDIGetBookmark( pfucb, pcsr->Cpage(), pcsr->ILine(), &bookmark );

	Assert( sizeof( SLVSPACENODE ) == keydataflags.data.Cb() );
	SLVSPACENODE * const pslvspacenode = (SLVSPACENODE *)keydataflags.data.Pv();
	ASSERT_VALID( pslvspacenode );

	SLVSPACENODECACHE * const pslvspacenodecache = pfmp->Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache 
			|| PinstFromPfucb( pfucb )->FRecovering() || fGlobalRepair );

	PGNO pgnoLast;
	LongFromKey( &pgnoLast, keydataflags.key );
	Assert( 0 == ( pgnoLast % SLVSPACENODE::cpageMap ) );

	//  before we log the operation, determine if it is a valid one
	//  otherwise, we would log the operation, crash and rollback and
	//  invalid transition -- possibly undoing previous, valid transitions
	switch( slvspaceoper )
		{
		case slvspaceoperFreeToReserved:
			pslvspacenode->CheckReserve( ipage, cpages );
			break;
		case slvspaceoperReservedToCommitted:
			pslvspacenode->CheckCommitFromReserved( ipage, cpages );
			break;
		case slvspaceoperFreeToCommitted:
			pslvspacenode->CheckCommitFromFree( ipage, cpages );
			break;
		case slvspaceoperCommittedToDeleted:
			pslvspacenode->CheckDeleteFromCommitted( ipage, cpages );
			break;
		case slvspaceoperDeletedToFree:
			pslvspacenode->CheckFree( ipage, cpages );
			break;
		case slvspaceoperFree:
			pslvspacenode->CheckFree( ipage, cpages );
			break;
		case slvspaceoperFreeReserved:
			pslvspacenode->CheckFreeReserved( ipage, cpages );
			break;
		case slvspaceoperDeletedToCommitted:
			pslvspacenode->CheckCommitFromDeleted( ipage, cpages );
			break;
 		default:
			AssertSz( fFalse, "Unknown SLVSPACEOPER" );
		}

	Assert( fDirty || pcsr->FDirty() );

	if ( fLogging )
		{
		ERR err;
		LGPOS	lgpos;

		// we have to dirty
		Assert (fDirty);
		
		CallR( ErrLGSLVSpace( pfucb, pcsr, bookmark, slvspaceoper, ipage, cpages, rceid, dirflag, &lgpos, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		if ( fDirty )
			{
			pcsr->Dirty( );
			}
		}

	switch( slvspaceoper )
		{
		case slvspaceoperFreeToReserved:
			pslvspacenode->Reserve( ipage, cpages );
			if( pslvspacenodecache )
				{
				pslvspacenodecache->DecreaseCpgAvail( pgnoLast, cpages );
				}
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sReserved, cpages );
			break;
		case slvspaceoperReservedToCommitted:
			pslvspacenode->CommitFromReserved( ipage, cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sReserved, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, cpages );
			break;
		case slvspaceoperFreeToCommitted:
			pslvspacenode->CommitFromFree( ipage, cpages );
			if( pslvspacenodecache )
				{
				pslvspacenodecache->DecreaseCpgAvail( pgnoLast, cpages );
				}
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, cpages );
			break;
		case slvspaceoperCommittedToDeleted:
			pslvspacenode->DeleteFromCommitted( ipage, cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sDeleted, cpages );
			break;
		case slvspaceoperDeletedToFree:
			pslvspacenode->Free( ipage, cpages );
			if( pslvspacenodecache )
				{
				pslvspacenodecache->IncreaseCpgAvail( pgnoLast, cpages );
				}
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sDeleted, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, cpages );
			break;
		case slvspaceoperFree:
			{
			//  yeah, this isn't good but it only happens rarely
			LONG ipageT;
			LONG cpagesBI[ SLVSPACENODE::sMax ] = { 0 };
			for ( ipageT = ipage; ipageT < ipage + cpages; ipageT++ )
				{
				cpagesBI[ pslvspacenode->GetState( ipageT ) ]++;
				}
			Assert( cpages == cpagesBI[ SLVSPACENODE::sReserved ] + cpagesBI[ SLVSPACENODE::sCommitted ] + cpagesBI[ SLVSPACENODE::sDeleted ] );
			pslvspacenode->Free( ipage, cpages );
			if( pslvspacenodecache )
				{
				pslvspacenodecache->IncreaseCpgAvail( pgnoLast, cpages );
				}
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sReserved, -cpagesBI[ SLVSPACENODE::sReserved ] );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, -cpagesBI[ SLVSPACENODE::sCommitted ] );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sDeleted, -cpagesBI[ SLVSPACENODE::sDeleted ] );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, cpages );
			}
			break;
		case slvspaceoperFreeReserved:
			{
			LONG cpagesFreed;
			pslvspacenode->FreeReserved( ipage, cpages, &cpagesFreed );
			if( pslvspacenodecache )
				{
				pslvspacenodecache->IncreaseCpgAvail( pgnoLast, cpagesFreed );
				}
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sReserved, -cpagesFreed );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, cpagesFreed );
			}
			break;
		case slvspaceoperDeletedToCommitted:
			pslvspacenode->CommitFromDeleted( ipage, cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sDeleted, -cpages );
			pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, cpages );
			break;
 		default:
			AssertSz( fFalse, "Unknown SLVSPACEOPER" );
		}

	pfmp->IncSLVSpaceOperCount( slvspaceoper, cpages );

	return JET_errSuccess;
	}



//  ================================================================
VOID NDGetExternalHeader( KEYDATAFLAGS * const pkdf, const CSR * pcsr )
//  ================================================================
	{
	LINE line;
	pcsr->Cpage().GetPtrExternalHeader( &line );
	pkdf->key.Nullify();
	pkdf->data.SetCb( line.cb );
	pkdf->data.SetPv( line.pv );
	pkdf->fFlags 	= 0;
	}


//  ================================================================
VOID NDGetExternalHeader( FUCB * pfucb, const CSR * pcsr )
//  ================================================================
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );

	LINE line;
	pcsr->Cpage().GetPtrExternalHeader( &line );
	pfucb->kdfCurr.key.Nullify();
	pfucb->kdfCurr.data.SetCb( line.cb );
	pfucb->kdfCurr.data.SetPv( line.pv );
	pfucb->kdfCurr.fFlags 	= 0;
	}


//  ================================================================
VOID NDGetPrefix( FUCB * pfucb, const CSR * pcsr )
//  ================================================================
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );

	LINE line;
	pcsr->Cpage().GetPtrExternalHeader( &line );

	pfucb->kdfCurr.Nullify();
	pfucb->kdfCurr.fFlags = 0;

	if ( !pcsr->Cpage().FRootPage() )
		{
		pfucb->kdfCurr.key.prefix.SetPv( line.pv );
		pfucb->kdfCurr.key.prefix.SetCb( line.cb );
		}
	else if ( pcsr->Cpage().FSpaceTree() )
		{
		if ( line.cb >= sizeof( SPLIT_BUFFER ) )
			{
			pfucb->kdfCurr.key.prefix.SetPv( (BYTE *)line.pv + sizeof( SPLIT_BUFFER ) );
			pfucb->kdfCurr.key.prefix.SetCb( line.cb - sizeof( SPLIT_BUFFER ) );
			}
		else
			{
			Assert( 0 == line.cb );
			Assert( FFUCBSpace( pfucb ) );

/*	Can't make this assertion because sometimes we are calling this code without having
	setup the SPLIT_BUFFER (eg. merge of the space tree itself)
			//	split buffer may be missing from page if upgrading from ESE97 and it couldn't
			//	fit on the page, in which case it should be hanging off the FCB
			Assert( NULL != pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) )
				|| PinstFromPfucb( pfucb )->FRecovering() );
*/				
			pfucb->kdfCurr.key.prefix.SetPv( line.pv );
			pfucb->kdfCurr.key.prefix.SetCb( 0 );
			}
		}		
	else
		{
		Assert( line.cb >= sizeof( SPACE_HEADER ) );
		pfucb->kdfCurr.key.prefix.SetPv( (BYTE *)line.pv + sizeof( SPACE_HEADER ) );
		pfucb->kdfCurr.key.prefix.SetCb( line.cb - sizeof( SPACE_HEADER ) );
		}
	return;
	}


//  ================================================================
ERR ErrNDSetExternalHeader( FUCB * pfucb, CSR * pcsr, const DATA * pdata, DIRFLAG dirflag )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	ASSERT_VALID( pdata );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );

	ERR			err			= JET_errSuccess;
	const BOOL	fLogging	= !( dirflag & fDIRNoLog ) && rgfmp[pfucb->ifmp].FLogOn();
	
	if ( fLogging )
		{
		//	log the operation, getting the lgpos
		//
		LGPOS	lgpos;
		
		Call( ErrLGSetExternalHeader( pfucb, pcsr, *pdata, &lgpos, fMustDirtyCSR ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}
	else
		{
		pcsr->Dirty( );
		}
	
	pcsr->Cpage().SetExternalHeader( pdata, 1, 0 ); //  external header has no flags -- yet

HandleError:
	return err;
	}


//  ================================================================
VOID NDSetExternalHeader( FUCB * pfucb, CSR * pcsr, const DATA * pdata )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	ASSERT_VALID( pdata );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );

	Assert( pcsr->FDirty( ) );
	pcsr->Cpage().SetExternalHeader( pdata, 1, 0 ); //  external header has no flags -- yet
	return;
	}


//  ================================================================
VOID NDSetPrefix( CSR * pcsr, const KEY& key )
//  ================================================================
	{
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );

	//	JLIEM: if this is the source page of the split, I don't
	//	follow how we can guarantee this assert won't go off.
	//	Ultimately, it's probably okay because the cbUncommittedFree
	//	will eventually be set to the right value in BTISplitMoveNodes().
	Assert( pcsr->Cpage().CbFree( ) - pcsr->Cpage().CbUncommittedFree( ) >= key.Cb() );
	
	DATA			rgdata[cdataPrefix];
	INT				idata = 0;

	BYTE			rgexthdr[ max( sizeof(SPLIT_BUFFER), sizeof(SPACE_HEADER) ) ];

	if ( pcsr->Cpage().FRootPage() )
		{
		if ( key.FNull() )
			return;

		//	should currently be a dead code path because we never append prefix data to
		//	the SPLIT_BUFFER or SPACE_HEADER
		//	UNDONE: code below won't even work properly because it doesn't properly
		//	handle the case of the SPLIT_BUFFER being cached in the FCB
		Assert( fFalse );
			
		//	copy space header and reinsert
		const ULONG	cbExtHdr	= ( pcsr->Cpage().FSpaceTree() ? sizeof(SPLIT_BUFFER) : sizeof(SPACE_HEADER) );
		LINE		line;

		pcsr->Cpage().GetPtrExternalHeader( &line );
		if ( 0 != line.cb )
			{
			//	don't currently support prefix data following SPLIT_BUFFER/SPACE_HEADER
			Assert( line.cb == cbExtHdr );	
		
			UtilMemCpy( rgexthdr, line.pv, cbExtHdr );

			rgdata[idata].SetPv( rgexthdr );
			rgdata[idata].SetCb( cbExtHdr );
			++idata;
			}
		else
			{
			//	SPLIT_BUFFER must be cached in the FCB
			Assert( pcsr->Cpage().FSpaceTree() );
			}
		}
		
	rgdata[idata] = key.prefix;
	++idata;
	rgdata[idata] = key.suffix;
	++idata;

	pcsr->Cpage().SetExternalHeader( rgdata, idata, 0 ); //  external header has no flags -- yet
	}


//  ================================================================
VOID NDGrowCbPrefix( const FUCB *pfucb, CSR * pcsr, INT cbPrefixNew )
//  ================================================================
//
//	grows cbPrefix in current node to cbPrefixNew -- thereby shrinking node
//
//-
	{
	NDIAssertCurrency( pfucb, pcsr );
	AssertNDGet( pfucb, pcsr );
	Assert( pfucb->kdfCurr.key.prefix.Cb() < cbPrefixNew );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );

	KEYDATAFLAGS	kdf = pfucb->kdfCurr;
	
	//	set node as compressed
	Assert( cbPrefixNew > cbPrefixOverhead );
	NDISetCompressed( kdf );

	DATA 		rgdata[cdataKDF+1];
	INT			fFlagsLine;
	LE_KEYLEN	le_keylen;
	le_keylen.le_cbPrefix = (USHORT)cbPrefixNew;

	//	replace node with new key
	const INT	cdata			= CdataNDIPrefixAndKeydataflagsToDataflags(
											&kdf,
											rgdata,
											&fFlagsLine,
											&le_keylen );
	Assert( cdata <= cdataKDF + 1 );
	pcsr->Cpage().Replace( pcsr->ILine(), rgdata, cdata, fFlagsLine );
	}


//  ================================================================
VOID NDShrinkCbPrefix( FUCB *pfucb, CSR * pcsr, const DATA *pdataOldPrefix, INT cbPrefixNew )
//  ================================================================
//
//	shrinks cbPrefix in current node to cbPrefixNew -- thereby growing node
//	expensive operation -- perf needed
//
//-
	{
	NDIAssertCurrency( pfucb, pcsr );
	AssertNDGet( pfucb, pcsr );
	Assert( pfucb->kdfCurr.key.prefix.Cb() > cbPrefixNew );
	Assert( pcsr->Cpage().FAssertWriteLatch( ) );
	Assert( pcsr->FDirty() );

	KEYDATAFLAGS	kdf = pfucb->kdfCurr;

	//	set flags in kdf to compressed
	//	point prefix to old prefix [which has been deleted]
	if ( cbPrefixNew > 0 )
		{
		Assert( cbPrefixNew > cbPrefixOverhead );
		NDISetCompressed( kdf );
		}
	else
		{
		NDIResetCompressed( kdf );
		}

	//	fix prefix to point to old prefix
	kdf.key.prefix.SetPv( pdataOldPrefix->Pv() );
	Assert( (INT)pdataOldPrefix->Cb() >= kdf.key.prefix.Cb() );
	Assert( kdf.key.prefix.Cb() == pfucb->kdfCurr.key.prefix.Cb() );

	BYTE *rgb;
//	BYTE	rgb[g_cbPageMax];
	BFAlloc( (VOID **)&rgb );
	
	kdf.key.suffix.SetPv( rgb );
	pfucb->kdfCurr.key.suffix.CopyInto( kdf.key.suffix );

	kdf.data.SetPv( rgb + kdf.key.suffix.Cb() );
	pfucb->kdfCurr.data.CopyInto( kdf.data );

	DATA 		rgdata[cdataKDF+1];
	INT  		fFlagsLine;
	LE_KEYLEN	le_keylen;
	le_keylen.le_cbPrefix = (USHORT)cbPrefixNew;

	//	replace node with new key
	const INT	cdata			= CdataNDIPrefixAndKeydataflagsToDataflags(
											&kdf,
											rgdata,
											&fFlagsLine,
											&le_keylen );
	Assert( cdata <= cdataKDF + 1 );
	pcsr->Cpage().Replace( pcsr->ILine(), rgdata, cdata, fFlagsLine );

	BFFree( rgb );
	}


//  ================================================================
INT CbNDUncommittedFree( const FUCB * const pfucb, const CSR * const pcsr )
//  ================================================================
	{
	ASSERT_VALID( pfucb );

	Assert( !pcsr->Cpage().FSpaceTree() );

	LONG	cbActualUncommitted		= 0;

	if ( pcsr->FPageRecentlyDirtied( pfucb->ifmp ) )
		{
		for ( INT iline = 0; iline < pcsr->Cpage().Clines( ); iline++ )
			{
			KEYDATAFLAGS keydataflags;
			NDIGetKeydataflags( pcsr->Cpage(), iline, &keydataflags );

			if ( FNDVersion( keydataflags ) )
				{
				BOOKMARK bookmark;
				NDIGetBookmark( pfucb, pcsr->Cpage(), iline, &bookmark );

				cbActualUncommitted += CbVERGetNodeReserve( ppibNil, pfucb, bookmark, keydataflags.data.Cb() );
				}
			}
		}
	Assert(	cbActualUncommitted >= 0 );
	Assert( cbActualUncommitted <= pcsr->Cpage().CbFree() );
		
	return cbActualUncommitted;
	}


//  ****************************************************************
//  DEBUG ROUTINES
//  ****************************************************************

#ifdef DEBUG

//  ================================================================
INLINE VOID NDIAssertCurrency( const FUCB * pfucb, const CSR * pcsr )
//  ================================================================
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );
	
	Assert( pcsr->FLatched() );
#ifdef DEBUG_NODE
	NDAssertNDInOrder( pfucb, pcsr );
#endif	//	DEBUG_NODE
	}


//  ================================================================
INLINE VOID NDIAssertCurrencyExists( const FUCB * pfucb, const CSR * pcsr )
//  ================================================================
	{
	NDIAssertCurrency( pfucb, pcsr );
	Assert( pcsr->ILine() >= 0 );
	Assert( pcsr->ILine() < pcsr->Cpage().Clines( ) );
	}


//  ================================================================
VOID NDAssertNDInOrder( const FUCB * pfucb, const CSR * pcsr )
//  ================================================================
//
//	assert nodes in page are in bookmark order
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );

	const INT 	clines 	= pcsr->Cpage().Clines( );

	//  we could optimize this loop by only fetching one node each time
	INT 		iline	=	0;
	for ( ;iline < (clines - 1); iline++ )
		{
		BOOKMARK	bmLess;
		BOOKMARK	bmGreater;

		NDIGetBookmark( pfucb, pcsr->Cpage(), iline, &bmLess );
		NDIGetBookmark( pfucb, pcsr->Cpage(), iline+1, &bmGreater );

		if ( bmGreater.key.FNull() )
			{
			//	if key is null, then must be last node in internal page
			Assert( !pcsr->Cpage().FLeafPage() || FFUCBRepair( pfucb ) );
			Assert( iline + 1 == clines - 1 );
			continue;
			}

		Assert( !bmLess.key.FNull() );
		const INT cmp = pcsr->Cpage().FLeafPage() ? 
							CmpBM( bmLess, bmGreater ) :
							CmpKey( bmLess.key, bmGreater.key );
		if (	cmp > 0 
				|| ( FFUCBUnique( pfucb ) && 0 == cmp ) ) 
			{
			AssertSz( fFalse, "NDAssertNDInOrder: node is out of order" );
			}
		}
	}

#endif		//  DEBUG
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\pib.cxx ===
#include "std.hxx"


PM_CEF_PROC LPIBInUseCEFLPv;
PERFInstanceG<> cPIBInUse;
long LPIBInUseCEFLPv( long iInstance, void* pvBuf )
	{
	cPIBInUse.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LPIBTotalCEFLPv;
PERFInstanceG<> cPIBTotal;
long LPIBTotalCEFLPv( long iInstance, void* pvBuf )
	{
	cPIBTotal.PassTo( iInstance, pvBuf );
	return 0;
	}


#ifndef RTM
//  ================================================================
template< class T >
VOID CSIMPLELIST<T>::UnitTest()
//  ================================================================
	{
	ERR err;
	CSIMPLELIST<T> simplelist;

	AssertRTL( simplelist.FEmpty() );
	simplelist.MakeEmpty();
	AssertRTL( simplelist.FEmpty() );
	simplelist.MakeEmpty();
	AssertRTL( simplelist.FEmpty() );

	Call( simplelist.ErrInsert( 1 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 2 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 3 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 4 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 5 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 6 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 7 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 8 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 1 ) );
	AssertRTL( !simplelist.FEmpty() );

	CallS( simplelist.ErrDelete( 911 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 0 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 0xffffffff ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 10 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 11 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 11 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 11 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 11 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 0 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 0xffffffff ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 1 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 6 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 7 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 8 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 2 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 3 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 4 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 5 ) );
	AssertRTL( !simplelist.FEmpty() );
	CallS( simplelist.ErrDelete( 1 ) );
	AssertRTL( simplelist.FEmpty() );

	Call( simplelist.ErrInsert( 6 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 7 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 8 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 1 ) );
	AssertRTL( !simplelist.FEmpty() );

	simplelist.MakeEmpty();
	AssertRTL( simplelist.FEmpty() );	
	
	Call( simplelist.ErrInsert( 1 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 2 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 3 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 4 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 5 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 6 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 7 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 8 ) );
	AssertRTL( !simplelist.FEmpty() );
	Call( simplelist.ErrInsert( 1 ) );
	AssertRTL( !simplelist.FEmpty() );

	T t;
	
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 1 == t );	
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 8 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 7 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 6 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 5 == t );	
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 4 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 3 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( !simplelist.FEmpty() );
	AssertRTL( 2 == t );
	CallS( simplelist.ErrPop( &t ) );
	AssertRTL( simplelist.FEmpty() );
	AssertRTL( 1 == t );

HandleError:
	switch( err )
		{
		case JET_errSuccess:
		case JET_errOutOfMemory:
			break;
		default:
			CallS( err );
		}
	simplelist.MakeEmpty();
	}
#endif	//	!RTM


#ifdef DEBUG
//  ================================================================
template< class T >
VOID CSIMPLELIST<T>::AssertValid() const
//  ================================================================
	{
	Assert( m_itMac >= 0 );
	Assert( m_itMax >= 0 );
	
	if( m_pt )
		{
		Assert( m_itMac <= m_itMax );
		}
	else
		{
		Assert( 0 == m_itMac );
		Assert( 0 == m_itMax );
		}
	}
#endif	//	DEBUG		


//  ================================================================
ERR PIB::ErrRegisterDeferredRceid( const RCEID& rceid )
//  ================================================================
	{
	Assert( rceidNull != rceid );
	return m_simplelistRceidDeferred.ErrInsert( rceid );
	}


//  ================================================================
ERR	PIB::ErrDeregisterDeferredRceid( const RCEID& rceid )
//  ================================================================
	{
	Assert( rceidNull != rceid );
	return m_simplelistRceidDeferred.ErrDelete( rceid );
	}


//  ================================================================
VOID PIB::RemoveAllDeferredRceid()
//  ================================================================
	{
	m_simplelistRceidDeferred.MakeEmpty();
	}


//  ================================================================
VOID PIB::AssertNoDeferredRceid() const
//  ================================================================
	{
	AssertRTL( m_simplelistRceidDeferred.FEmpty() );
	}


//  ================================================================
TRX TrxOldest( INST *pinst )
//  ================================================================
//
//  UNDONE:  consider inlining this
//
	{
	pinst->m_critPIBTrxOldest.Enter();
	TRX trxOldest;
	if ( pinst->m_ppibTrxOldest )
		trxOldest = pinst->m_ppibTrxOldest->trxBegin0;
	else
		trxOldest = trxMin;
	pinst->m_critPIBTrxOldest.Leave();
	Assert( !pinst->m_plog->m_fRecovering || trxMax == trxOldest );
	return trxOldest;
	}

//  ================================================================
VOID PIBInsertTrxListTail( PIB * ppib )
//  ================================================================
	{
	INST *pinst = PinstFromPpib( ppib );
	LOG  *plog = pinst->m_plog;
	
	Assert( !plog->m_fRecovering );
	Assert( pinst->m_critPIBTrxOldest.FOwner() );
	Assert( ppib != pinst->m_ppibSentinel );

	Assert( NULL == ppib->pppibTrxPrevPpibNext );
	Assert( NULL == ppib->ppibTrxNext );

	ppib->pppibTrxPrevPpibNext 		= pinst->m_ppibSentinel->pppibTrxPrevPpibNext;
	*(ppib->pppibTrxPrevPpibNext) 	= ppib;
	ppib->ppibTrxNext				= pinst->m_ppibSentinel;
	ppib->ppibTrxNext->pppibTrxPrevPpibNext = &( ppib->ppibTrxNext );

	Assert( trxMax != pinst->m_ppibTrxOldest->trxBegin0 );
	Assert( NULL != ppib->pppibTrxPrevPpibNext );
	Assert( NULL != ppib->ppibTrxNext );
	Assert( (*ppib->pppibTrxPrevPpibNext) == ppib );
	Assert( ppib->ppibTrxNext->pppibTrxPrevPpibNext == &(ppib->ppibTrxNext) );
	}


//  ================================================================
VOID PIBInsertTrxListHead( PIB * ppib )
//  ================================================================
	{
	INST *pinst = PinstFromPpib( ppib );
	
	Assert( !pinst->m_plog->m_fRecovering );
	Assert( pinst->m_critPIBTrxOldest.FOwner() );
	Assert( ppib != pinst->m_ppibSentinel );

	Assert( NULL == ppib->pppibTrxPrevPpibNext );
	Assert( NULL == ppib->ppibTrxNext );

	ppib->ppibTrxNext				= const_cast<PIB *>( pinst->m_ppibTrxOldest );
	ppib->pppibTrxPrevPpibNext 		= const_cast<PIB **>( &pinst->m_ppibTrxOldest );
	*(ppib->pppibTrxPrevPpibNext) 	= ppib;
	ppib->ppibTrxNext->pppibTrxPrevPpibNext = &( ppib->ppibTrxNext );

	Assert( trxMax != pinst->m_ppibTrxOldest->trxBegin0 );
	Assert( NULL != ppib->pppibTrxPrevPpibNext );
	Assert( NULL != ppib->ppibTrxNext );
	Assert( (*ppib->pppibTrxPrevPpibNext) == ppib );
	Assert( ppib->ppibTrxNext->pppibTrxPrevPpibNext == &(ppib->ppibTrxNext) );
	}


//  ================================================================
VOID PIBDeleteFromTrxList( PIB * ppib )
//  ================================================================
	{
	Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
	Assert( PinstFromPpib( ppib )->m_critPIBTrxOldest.FOwner() );
	Assert( ppib != PinstFromPpib( ppib )->m_ppibSentinel );

	Assert( trxMax != PinstFromPpib( ppib )->m_ppibTrxOldest->trxBegin0 );
	Assert( NULL != ppib->pppibTrxPrevPpibNext );
	Assert( NULL != ppib->ppibTrxNext );
	Assert( (*ppib->pppibTrxPrevPpibNext) == ppib );
	Assert( ppib->ppibTrxNext->pppibTrxPrevPpibNext == &(ppib->ppibTrxNext) );

	ppib->ppibTrxNext->pppibTrxPrevPpibNext 	= ppib->pppibTrxPrevPpibNext;
	(*ppib->pppibTrxPrevPpibNext) 			= ppib->ppibTrxNext;
	
	ppib->pppibTrxPrevPpibNext 	= NULL;
	ppib->ppibTrxNext 			= NULL;

	Assert( NULL == ppib->pppibTrxPrevPpibNext );
	Assert( NULL == ppib->ppibTrxNext );
	}

#define PpibMEMAlloc( pinst ) reinterpret_cast<PIB*>( pinst->m_pcresPIBPool->PbAlloc( __FILE__, __LINE__ ) )


ERR PIB::ErrAbortAllMacros( BOOL fLogEndMacro )
	{
	ASSERT_VALID( this );

	ERR		err;
	MACRO	*pMacro, *pMacroNext;
	for ( pMacro = m_pMacroNext; pMacro != NULL; pMacro = pMacroNext )
		{
		pMacroNext = pMacro->PMacroNext();
		if ( pMacro->Dbtime() != dbtimeNil )
			{
			//	Record LGMacroAbort.
			//
			LGPOS	lgpos;
			DBTIME	dbtime = pMacro->Dbtime();

			if ( fLogEndMacro )
				{
				CallR( ErrLGMacroAbort( this, dbtime, &lgpos ) );
				}
			
			//	release recorded log
			//
			this->ResetMacroGoing( dbtime );
			}
		}

	//	release last macro
	//
	if ( m_pMacroNext != NULL )
		{
		Assert( m_pMacroNext->PMacroNext() == NULL );
		Assert( dbtimeNil == m_pMacroNext->Dbtime() );
		OSMemoryHeapFree( m_pMacroNext );
		m_pMacroNext = NULL;
		}

	return JET_errSuccess;
	}


INLINE VOID MEMReleasePpib( PIB *& ppib )		
	{
	INST *pinst = PinstFromPpib( ppib );
	pinst->m_pcresPIBPool->Release( (BYTE *)ppib );
#ifdef DEBUG
	ppib = 0;
#endif
	}

ERR	ErrPIBInit( INST *pinst, int cpibMax )
	{
	ERR 	err 	= JET_errSuccess;

#ifndef RTM
	CSIMPLELIST<RCEID>::UnitTest();
#endif	//	RTM

	Assert( IbAlignForAllPlatforms( sizeof(PIB) ) == sizeof(PIB) );
#ifdef PCACHE_OPTIMIZATION
	Assert( sizeof(PIB) % 32 == 0 );
#endif

	pinst->m_pcresPIBPool = new CRES( pinst, residPIB, sizeof( PIB ), cpibMax, &err );
	if ( err < JET_errSuccess )
		{
		delete pinst->m_pcresPIBPool;
		pinst->m_pcresPIBPool = NULL;
		}
	else if ( NULL == pinst->m_pcresPIBPool )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	CallR( err );

	Assert(NULL != pinst->m_pcresPIBPool);
	pinst->m_ppibGlobalMin = (PIB *) pinst->m_pcresPIBPool->PbMin();
	pinst->m_ppibGlobalMax = (PIB *) pinst->m_pcresPIBPool->PbMax();
	pinst->m_ppibGlobal = ppibNil;

	pinst->m_ppibSentinel = PpibMEMAlloc( pinst );
	if ( ppibNil == pinst->m_ppibSentinel)
		{
		delete pinst->m_pcresPIBPool;
		pinst->m_ppibGlobalMin = NULL;
		pinst->m_ppibGlobalMax = NULL;
		pinst->m_ppibGlobal = ppibNil;
		CallR( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	Assert( ppibNil != pinst->m_ppibSentinel );
	Assert( FAlignedForAllPlatforms( pinst->m_ppibSentinel ) );

	memset( pinst->m_ppibSentinel, 0xFF, sizeof( PIB ) );
	pinst->m_ppibSentinel->m_pinst = pinst;
	
	pinst->m_ppibSentinel->trxBegin0 			= trxMax;
	pinst->m_ppibSentinel->pppibTrxPrevPpibNext = const_cast<PIB **>( &pinst->m_ppibTrxOldest );
	pinst->m_ppibSentinel->ppibTrxNext 			= NULL;

	pinst->m_ppibTrxOldest = pinst->m_ppibSentinel;

	Assert( pinst->m_ppibTrxOldest == pinst->m_ppibSentinel );
	Assert( trxMax == pinst->m_ppibTrxOldest->trxBegin0 );
	Assert( NULL != pinst->m_ppibSentinel->pppibTrxPrevPpibNext );
	Assert( pinst->m_ppibTrxOldest == *(pinst->m_ppibSentinel->pppibTrxPrevPpibNext) );
	Assert( NULL == pinst->m_ppibSentinel->ppibTrxNext );
	Assert( (*pinst->m_ppibSentinel->pppibTrxPrevPpibNext) == pinst->m_ppibSentinel );

	cPIBInUse.Clear( pinst );
	cPIBTotal.Set( pinst, cpibMax );

	return JET_errSuccess;
	}

VOID PIBTerm( INST *pinst )
	{
	PIB     *ppib;

	pinst->m_critPIB.Enter();

	cPIBTotal.Clear( pinst );
	cPIBInUse.Clear( pinst );

	for ( ppib = pinst->m_ppibGlobal; ppib != ppibNil; ppib = pinst->m_ppibGlobal )
		{
		PIB             *ppibCur;
		PIB             *ppibPrev;

		Assert( !ppib->FLGWaiting() );

		ppibPrev = (PIB *)((BYTE *)&pinst->m_ppibGlobal - OffsetOf(PIB, ppibNext));
		while( ( ppibCur = ppibPrev->ppibNext ) != ppib && ppibCur != ppibNil )
			{
			ppibPrev = ppibCur;
			}

		if ( ppibCur != ppibNil )
			{
			ppibPrev->ppibNext = ppibCur->ppibNext;
			Assert( ppibPrev != ppibPrev->ppibNext );
			}

		//	internal sessions may not access the temp. database
		if( FPIBUserOpenedDatabase( ppib, dbidTemp ) )
			{
			DBResetOpenDatabaseFlag( ppib, pinst->m_mpdbidifmp[ dbidTemp ] );
			}

		ppib->FreePvRecordFormatConversionBuffer();
		
		//	close all cursors still open
		//	should only be sort file cursors
		//
		while( pfucbNil != ppib->pfucbOfSession )
			{
			FUCB	*pfucb	= ppib->pfucbOfSession;

			if ( FFUCBSort( pfucb ) )
				{
				Assert( !( FFUCBIndex( pfucb ) ) );
				CallS( ErrIsamSortClose( ppib, pfucb ) );
				}
			else
				{
				AssertSz( fFalse, "PIBTerm: At least one non sort fucb is left open." );
				break;
				}
			}
		
		ppib->~PIB();
		MEMReleasePpib( ppib );
		}

	pinst->m_ppibGlobal = ppibNil;
	MEMReleasePpib( const_cast<PIB *>( pinst->m_ppibSentinel ) );
	pinst->m_ppibSentinel = ppibNil;

	pinst->m_critPIB.Leave();

	//	Release mem resource

	delete pinst->m_pcresPIBPool;

	//	Reset PIB global variables

	pinst->m_ppibGlobalMin = NULL;
	pinst->m_ppibGlobalMax = NULL;
	pinst->m_ppibGlobal = ppibNil;
	}

	
ERR ErrPIBBeginSession( INST *pinst, PIB **pppib, PROCID procidTarget, BOOL fForCreateTempDatabase )
	{
	ERR		err;
	PIB		*ppib;

	Assert( pinst->FRecovering() || procidTarget == procidNil );

	pinst->m_critPIB.Enter();

	if ( procidTarget != procidNil )
		{
		PIB *ppibTarget;
		
		/*  allocate inactive PIB according to procidTarget
		/**/
		Assert( pinst->FRecovering() );
		ppibTarget = PpibOfProcid( pinst, procidTarget );
		for ( ppib = pinst->m_ppibGlobal; ppib != ppibTarget && ppib != ppibNil; ppib = ppib->ppibNext );
		if ( ppib != ppibNil )
			{
			/*  we found a reusable one.
			/*	Set level to hold the pib
			/**/
			Assert( ppib->level == levelNil );
			Assert( ppib->procid == ProcidPIBOfPpib( ppib ) );
			Assert( ppib->procid == procidTarget );
			Assert( !FPIBUserOpenedDatabase( ppib, dbidTemp ) );
			ppib->level = 0;
			}
		}
	else
		{
		/*	allocate inactive PIB on anchor list
		/**/
		for ( ppib = pinst->m_ppibGlobal; ppib != ppibNil; ppib = ppib->ppibNext )
			{
			if ( ppib->level == levelNil )
				{
#ifdef DEBUG
				//	at the point where we go to create the temp
				//	database, there shouldn't be any pibs
				//	available for reuse
				Assert( !fForCreateTempDatabase );
				if ( FPIBUserOpenedDatabase( ppib, dbidTemp ) )
					{
					Assert( !pinst->FRecovering() );
					Assert( pinst->m_lTemporaryTablesMax > 0 );
					}
				else
					{
					Assert( pinst->FRecovering()
						|| 0 == pinst->m_lTemporaryTablesMax );
					}
#endif

				/*  we found a reusable one.
				/*	Set level to hold the pib
				/**/
				ppib->level = 0;
				break;
				}
			}
		}

	/*	return success if found PIB
	/**/
	if ( ppib != ppibNil )
		{
		/*  we found a reusable one.
		/*	Do not reset non-common items.
		/**/
		Assert( ppib->level == 0 );
		
		Assert( !FSomeDatabaseOpen( ppib ) );
		
		Assert( ppib->pfucbOfSession == pfucbNil );
		Assert( ppib->procid != procidNil );
		
		/*  set PIB procid from parameter or native for session
		/**/
		Assert( ppib->procid == ProcidPIBOfPpib( ppib ) );
		Assert( ppib->procid != procidNil );
		}
	else
		{
		pinst->m_critPIB.Leave();
NewPib:
		/*  allocate PIB from free list and
		/*	set non-common items.
		/**/
		ppib = PpibMEMAlloc( pinst );
		if ( ppib == NULL )
			{
			err = ErrERRCheck( JET_errOutOfSessions );
			goto HandleError;
			}

		Assert( FAlignedForAllPlatforms( ppib ) );

		ppib->prceNewest = prceNil;
		memset( (BYTE *)ppib, 0, sizeof(PIB) );
	
		new( ppib ) PIB;
		ppib->m_pinst = pinst;

		/*  general initialization for each new pib.
		/**/
		ppib->procid = ProcidPIBOfPpib( ppib );
		Assert( ppib->procid != procidNil );
		ppib->grbitsCommitDefault = NO_GRBIT;	/* set default commit flags in IsamBeginSession */

		if ( !fForCreateTempDatabase
			&& pinst->m_lTemporaryTablesMax > 0
			&& !pinst->FRecovering() )
			{
			/*  the temporary database is always open when not in recovery mode
			/**/
			DBSetOpenDatabaseFlag( ppib, pinst->m_mpdbidifmp[ dbidTemp ] );
			}

		//	Initialize critical section for accessing the pib's RCE

		/*  link PIB into list
		/**/
		pinst->m_critPIB.Enter();
		ppib->ppibNext = pinst->m_ppibGlobal;
		Assert( ppib != ppib->ppibNext );
		pinst->m_ppibGlobal = ppib;

		if ( procidTarget != procidNil && ppib != PpibOfProcid( pinst, procidTarget ) )
			{
			ppib->level = levelNil;

			/*  set non-zero items used by version store so that version store
			/*  will not mistaken it.
			/**/
			ppib->lgposStart = lgposMax;
			ppib->trxBegin0 = trxMax;

			pinst->m_critPIB.Leave();
			goto NewPib;
			}
		}

	/*  set common PIB initialization items
	/**/

	/*  set non-zero items
	/**/
	ppib->lgposStart = lgposMax;
	ppib->trxBegin0 = trxMax;
	
	ppib->lgposCommit0 = lgposMax;
	
	/*  set zero items including flags and monitor fields.
	/**/


	/*  default mark this a system session
	/**/
	ppib->ResetFlags();
	Assert( !ppib->FUserSession() );	//	default marks this as a system session
	Assert( !ppib->FAfterFirstBT() );
	Assert( !ppib->FRecoveringEndAllSessions() );
	Assert( !ppib->FLGWaiting() );
	Assert( !ppib->FBegin0Logged() );
	Assert( !ppib->FSetAttachDB() );
	Assert( !ppib->FSessionOLD() );
	Assert( !ppib->FSessionOLDSLV() );

	ppib->levelBegin = 0;
	ppib->clevelsDeferBegin = 0;
	ppib->levelRollback = 0;
	ppib->updateid = updateidNil;

	ppib->InitDistributedTrxData();

#ifdef DEBUG
	if ( FPIBUserOpenedDatabase( ppib, dbidTemp ) )
		{
		Assert( !pinst->FRecovering() );
		Assert( pinst->m_lTemporaryTablesMax > 0 );
		Assert( !fForCreateTempDatabase );
		}
	else
		{
		Assert( pinst->FRecovering()
			|| 0 == pinst->m_lTemporaryTablesMax
			|| fForCreateTempDatabase );
		}
#endif

	Assert( dwPIBSessionContextNull == ppib->dwSessionContext
		|| dwPIBSessionContextUnusable == ppib->dwSessionContext );
	ppib->dwSessionContext = dwPIBSessionContextNull;
	ppib->dwSessionContextThreadId = 0;
	ppib->dwTrxContext = 0;

	ppib->m_ifmpForceDetach = ifmpMax;

	*pppib = ppib;
	err = JET_errSuccess;

	cPIBInUse.Inc( pinst );
	
	pinst->m_critPIB.Leave();

HandleError:
	return err;
	}


VOID PIBEndSession( PIB *ppib )
	{
	INST *pinst = PinstFromPpib( ppib );
	
	pinst->m_critPIB.Enter();

	cPIBInUse.Dec( pinst );

	Assert( dwPIBSessionContextNull == ppib->dwSessionContext
		|| dwPIBSessionContextUnusable == ppib->dwSessionContext );
	ppib->dwSessionContext = dwPIBSessionContextUnusable;
	ppib->dwSessionContextThreadId = 0;
	ppib->dwTrxContext = 0;

	/*  all session resources except version buckets should have been
	/*  released to free pools.
	/**/
	Assert( pfucbNil == ppib->pfucbOfSession );

	ppib->level = levelNil;
	ppib->lgposStart = lgposMax;
	ppib->ResetFlags();

	ppib->FreeDistributedTrxData();
	ppib->FreePvRecordFormatConversionBuffer();
	
	pinst->m_critPIB.Leave();
	return;
	}


ERR VTAPI ErrIsamSetCommitDefault( JET_SESID sesid, long grbits )
	{
	((PIB *)sesid)->grbitsCommitDefault = grbits;
	return JET_errSuccess;
	}

ERR VTAPI ErrIsamSetSessionContext( JET_SESID sesid, DWORD_PTR dwContext )
	{
	PIB*	ppib	= (PIB *)sesid;

	//	verify not using reserved values
	if ( dwPIBSessionContextNull == dwContext
		|| dwPIBSessionContextUnusable == dwContext )
		return ErrERRCheck( JET_errInvalidParameter );

	return ErrPIBSetSessionContext( ppib, dwContext );
	}

ERR VTAPI ErrIsamResetSessionContext( JET_SESID sesid )
	{
	PIB*	ppib		= (PIB *)sesid;

	if ( ppib->dwSessionContextThreadId != DwUtilThreadId()
		|| dwPIBSessionContextNull == ppib->dwSessionContext )
		return ErrERRCheck( JET_errSessionContextNotSetByThisThread );

	Assert(	dwPIBSessionContextUnusable != ppib->dwSessionContext );
	PIBResetSessionContext( ppib );

	return JET_errSuccess;
	}

extern ULONG cBTSplits;

ERR VTAPI ErrIsamResetCounter( JET_SESID sesid, int CounterType )
	{
	return JET_errFeatureNotAvailable;
	}

extern PM_CEF_PROC LBTSplitsCEFLPv;

ERR VTAPI ErrIsamGetCounter( JET_SESID sesid, int CounterType, long *plValue )
	{
	return JET_errFeatureNotAvailable;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\rec.cxx ===
#include "std.hxx"

const WORD ibRECStartFixedColumns	= REC::cbRecordMin;

static JET_COLUMNDEF rgcolumndefJoinlist[] =
	{
	{ sizeof(JET_COLUMNDEF), 0, JET_coltypBinary, 0, 0, 0, 0, 0, JET_bitColumnTTKey },
	};


/*=================================================================
ErrIsamGetLock

Description:
	get lock on the record from the specified file.

Parameters:

	PIB			*ppib			PIB of user
	FUCB	 	*pfucb	  		FUCB for file
	JET_GRBIT	grbit 			options

Return Value: standard error return

Errors/Warnings:
<List of any errors or warnings, with any specific circumstantial
 comments supplied on an as-needed-only basis>

Side Effects:
=================================================================*/

ERR VTAPI ErrIsamGetLock( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
 	PIB		*ppib = reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucb = reinterpret_cast<FUCB *>( vtid );
	ERR		err;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );
	
	if ( ppib->level <= 0 )
		return ErrERRCheck( JET_errNotInTransaction );

	if ( JET_bitReadLock == grbit )	
		{
		Call( ErrDIRGetLock( pfucb, readLock ) );
		}
	else if ( JET_bitWriteLock == grbit )
		{
		//	ensure that table is updatable
		//
		CallR( ErrFUCBCheckUpdatable( pfucb )  );
		CallR( ErrPIBCheckUpdatable( ppib ) );

		Call( ErrDIRGetLock( pfucb, writeLock ) );
		}
	else
		{
		err = ErrERRCheck( JET_errInvalidGrbit );
		}

HandleError:
	return err;
	}


/*=================================================================
ErrIsamMove

Description:
	Retrieves the first, last, (nth) next, or (nth) previous
	record from the specified file.

Parameters:

	PIB			*ppib			PIB of user
	FUCB	 	*pfucb	  		FUCB for file
	LONG	 	crow			number of rows to move
	JET_GRBIT	grbit 			options

Return Value: standard error return

Errors/Warnings:
<List of any errors or warnings, with any specific circumstantial
 comments supplied on an as-needed-only basis>

Side Effects:
=================================================================*/

ERR VTAPI ErrIsamMove( JET_SESID sesid, JET_VTID vtid, LONG crow, JET_GRBIT grbit )
	{
 	PIB		*ppib = reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucb = reinterpret_cast<FUCB *>( vtid );
	ERR		err = JET_errSuccess;
 	FUCB	*pfucbSecondary;			// FUCB for secondary index (if any)
	FUCB	*pfucbIdx;				// FUCB of selected index (pri or sec)
	DIB		dib;					// Information block for DirMan

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	if ( FFUCBUpdatePrepared( pfucb ) )
		{
		CallR( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
		}

	AssertDIRNoLatch( ppib );
	dib.dirflag = fDIRNull;

	// Get secondary index FUCB if any
	pfucbSecondary = pfucb->pfucbCurIndex;
	if ( pfucbSecondary == pfucbNil )
		pfucbIdx = pfucb;
	else
		pfucbIdx = pfucbSecondary;

	if ( crow == JET_MoveLast )
		{
		DIRResetIndexRange( pfucb );

		dib.pos = posLast;

		//	move to DATA root
		//
		DIRGotoRoot( pfucbIdx );

		err = ErrDIRDown( pfucbIdx, &dib );
		}
	else if ( crow > 0 )
		{
		LONG crowT = crow;

		if ( grbit & JET_bitMoveKeyNE )
			dib.dirflag |= fDIRNeighborKey;

		//	Move forward number of rows given
		//
		while ( crowT-- > 0 )
			{
			err = ErrDIRNext( pfucbIdx, dib.dirflag );
			if ( err < 0 )
				{
				Assert( !Pcsr( pfucbIdx )->FLatched() );
				break;
				}

			Assert( Pcsr( pfucbIdx )->FLatched() );
			
			if ( ( grbit & JET_bitMoveKeyNE ) && crowT > 0 )
				{
				// Need to do neighbour-key checking, so bookmark
				// must always be up-to-date.
				Call( ErrDIRRelease( pfucbIdx ) );
				Call( ErrDIRGet( pfucbIdx ) );
				Assert( Pcsr( pfucbIdx )->FLatched() );
				}
			}
		}
	else if ( crow == JET_MoveFirst )
		{
		DIRResetIndexRange( pfucb );

		dib.pos 		= posFirst;

		//	move to DATA root
		//
		DIRGotoRoot( pfucbIdx );

		err = ErrDIRDown( pfucbIdx, &dib );
		}
	else if ( crow == 0 )
		{
		err = ErrDIRGet( pfucbIdx );
		}
	else
		{
		LONG crowT = crow;

		if ( grbit & JET_bitMoveKeyNE )
			dib.dirflag |= fDIRNeighborKey;

		while ( crowT++ < 0 )
			{
			err = ErrDIRPrev( pfucbIdx, dib.dirflag );
			if ( err < 0 )
				{
				AssertDIRNoLatch( ppib );
				break;
				}

			Assert( Pcsr( pfucbIdx )->FLatched() );
			if ( ( grbit & JET_bitMoveKeyNE ) && crowT < 0 )
				{
				// Need to do neighbour-key checking, so bookmark
				// must always be up-to-date.
				Call( ErrDIRRelease( pfucbIdx ) );
				Call( ErrDIRGet( pfucbIdx ) );
				Assert( Pcsr( pfucbIdx )->FLatched() );
				}
			}
		}

	//	if the movement was successful and a secondary index is
	//	in use, then position primary index to record.
	//
	if ( err == JET_errSuccess && pfucbSecondary != pfucbNil )
		{
		BOOKMARK	bmRecord;
		
		Assert( pfucbSecondary->kdfCurr.data.Pv() != NULL );
		Assert( pfucbSecondary->kdfCurr.data.Cb() > 0 );
		Assert( Pcsr( pfucbIdx )->FLatched() );

		bmRecord.key.prefix.Nullify();
		bmRecord.key.suffix = pfucbSecondary->kdfCurr.data;
		bmRecord.data.Nullify();

		//	We will need to touch the data page buffer.

		CallJ( ErrDIRGotoBookmark( pfucb, bmRecord ), ReleaseLatch );

		Assert( !Pcsr( pfucb )->FLatched() );
		Assert( PgnoFDP( pfucb ) != pgnoSystemRoot );
		Assert( pfucb->u.pfcb->FPrimaryIndex() );
		}

	if ( JET_errSuccess == err )
		{
ReleaseLatch:
		ERR		errT;

		Assert( Pcsr( pfucbIdx )->FLatched() );

		errT = ErrDIRRelease( pfucbIdx );
		AssertDIRNoLatch( ppib );

		if ( JET_errSuccess == err && JET_errSuccess == errT )
			 {
			 return err;
			 }

		if ( err >= 0 && errT < 0 )
			{
			//	return the more severe error
			//
			err = errT;
			}
		}

HandleError:
	AssertDIRNoLatch( ppib );

	if ( crow > 0 )
		{
		DIRAfterLast( pfucbIdx );
		DIRAfterLast(pfucb);
		}
	else if ( crow < 0 )
		{
		DIRBeforeFirst(pfucbIdx);
		DIRBeforeFirst(pfucb);
		}

	switch ( err )
		{
		case JET_errRecordNotFound:
			err = ErrERRCheck( JET_errNoCurrentRecord );
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
			break;
		default:
			Assert( JET_errSuccess != err );
			DIRBeforeFirst( pfucbIdx );
			if ( pfucbSecondary != pfucbNil )
				DIRBeforeFirst( pfucbSecondary );
		}

	AssertDIRNoLatch( ppib );
	return err;
	}


ERR	ErrRECIMove( FUCB *pfucbTable, LONG crow, JET_GRBIT grbit )
	{
 	PIB		*ppib = pfucbTable->ppib;
	ERR		err = JET_errSuccess;
	FUCB	*pfucbIdx = pfucbTable->pfucbCurIndex ? 
							pfucbTable->pfucbCurIndex : 
							pfucbTable;
						
	DIRFLAG	dirflag = fDIRNull;
	
	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucbTable );
	CheckSecondary( pfucbTable );

	Assert( Pcsr( pfucbIdx )->FLatched() );
	Assert( !FFUCBUpdatePrepared( pfucbTable ) );
	
	if ( crow > 0 )
		{
		LONG crowT = crow;

		if ( grbit & JET_bitMoveKeyNE )
			dirflag |= fDIRNeighborKey;

		//	Move forward number of rows given
		//
		while ( crowT-- > 0 )
			{
			if ( grbit & JET_bitMoveKeyNE )
				{
				// Need to do neighbour-key checking, so bookmark
				// must always be up-to-date.
				Call( ErrDIRRelease( pfucbIdx ) );
				Call( ErrDIRGet( pfucbIdx ) );
				}
			
			err = ErrDIRNext( pfucbIdx, dirflag );
			if ( err < 0 )
				{
				AssertDIRNoLatch( ppib );
				break;
				}
				
			Assert( Pcsr( pfucbIdx )->FLatched() );
			}
		}
	else
		{
		Assert( crow < 0 );
		LONG crowT = crow;

		if ( grbit & JET_bitMoveKeyNE )
			dirflag |= fDIRNeighborKey;

		while ( crowT++ < 0 )
			{
			if ( grbit & JET_bitMoveKeyNE )
				{
				// Need to do neighbour-key checking, so bookmark
				// must always be up-to-date.
				Call( ErrDIRRelease( pfucbIdx ) );
				Call( ErrDIRGet( pfucbIdx ) );
				}
			
			err = ErrDIRPrev( pfucbIdx, dirflag );
			if ( err < 0 )
				{
				AssertDIRNoLatch( ppib );
				break;
				}
				
			Assert( Pcsr( pfucbIdx )->FLatched() );
			}
		}

	//	if the movement was successful and a secondary index is
	//	in use, then position primary index to record.
	//
	if ( JET_errSuccess == err )
		{
		if ( pfucbIdx != pfucbTable )
			{
			BOOKMARK	bmRecord;
			
			Assert( pfucbIdx->kdfCurr.data.Pv() != NULL );
			Assert( pfucbIdx->kdfCurr.data.Cb() > 0 );
			Assert( pfucbIdx->locLogical == locOnCurBM );
			Assert( Pcsr( pfucbIdx )->FLatched() );

			bmRecord.key.prefix.Nullify();
			bmRecord.key.suffix = pfucbIdx->kdfCurr.data;
			bmRecord.data.Nullify();;
			
			//	We will need to touch the data page buffer.

			Call( ErrDIRGotoBookmark( pfucbTable, bmRecord ) );

			Assert( !Pcsr( pfucbTable )->FLatched() );
			Assert( PgnoFDP( pfucbTable ) != pgnoSystemRoot );
			Assert( pfucbTable->u.pfcb->FPrimaryIndex() );
			}
			
		Assert( Pcsr( pfucbIdx )->FLatched() );
		}
		
HandleError:
	if ( err < 0 )
		{
		AssertDIRNoLatch( ppib );
		
		if ( crow > 0 )
			{
			DIRAfterLast( pfucbIdx );
			DIRAfterLast( pfucbTable );
			}
		else if ( crow < 0 )
			{
			DIRBeforeFirst( pfucbIdx );
			DIRBeforeFirst( pfucbTable );
			}

		Assert( err != JET_errRecordNotFound );
		switch ( err )
			{
			case JET_errNoCurrentRecord:
			case JET_errRecordDeleted:
				break;
			default:
				Assert( JET_errSuccess != err );
				DIRBeforeFirst( pfucbIdx );
				DIRBeforeFirst( pfucbTable );
			}
		}
		
	return err;
	}

	
//	=================================================================
//	ErrIsamSeek

//	Description:
//	Retrieve the record specified by the given key or the
//	one just after it (SeekGT or SeekGE) or the one just
//	before it (SeekLT or SeekLE).

//	Parameters:

//	PIB			*ppib			PIB of user
//	FUCB		*pfucb 			FUCB for file
//	JET_GRBIT 	grbit			grbit

//	Return Value: standard error return

//	Errors/Warnings:
//	<List of any errors or warnings, with any specific circumstantial
//	comments supplied on an as-needed-only basis>

//	Side Effects:
//	=================================================================

ERR VTAPI ErrIsamSeek( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
 	PIB 		*ppib			= reinterpret_cast<PIB *>( sesid );
	FUCB 		*pfucbTable		= reinterpret_cast<FUCB *>( vtid );

	ERR			err;
	BOOKMARK	bm;			  			//	for search key
	DIB			dib;
	FUCB 		*pfucbSeek;				//	pointer to current FUCB
	BOOL		fFoundLess;
	BOOL		fFoundGreater;
	BOOL		fFoundEqual;
	BOOL		fRelease;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucbTable );
	CheckSecondary( pfucbTable );
	AssertDIRNoLatch( ppib );

	//	find cursor to seek on
	//
	pfucbSeek = pfucbTable->pfucbCurIndex == pfucbNil ? 
					pfucbTable :
					pfucbTable->pfucbCurIndex;

	if ( !FKSPrepared( pfucbSeek ) )
		{
		return ErrERRCheck( JET_errKeyNotMade );
		}
	FUCBAssertValidSearchKey( pfucbSeek );

	//	Reset copy buffer status
	//
	if ( FFUCBUpdatePrepared( pfucbTable ) )
		{
		CallR( ErrIsamPrepareUpdate( ppib, pfucbTable, JET_prepCancel ) );
		}

	//	reset index range limit
	//
	DIRResetIndexRange( pfucbTable );

	//	ignore segment counter
	//
	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( pfucbSeek->dataSearchKey.Pv() );
	bm.key.suffix.SetCb( pfucbSeek->dataSearchKey.Cb() );
	bm.data.Nullify();

	dib.pos = posDown;
	dib.pbm = &bm;

	if ( grbit & (JET_bitSeekLT|JET_bitSeekLE) )
		dib.dirflag = fDIRFavourPrev;
	else if ( grbit & JET_bitSeekGE )
		dib.dirflag = fDIRFavourNext;
	else if ( grbit & JET_bitSeekGT )
		{
		if ( !FFUCBUnique( pfucbSeek )
			&& bm.key.suffix.Cb() < cbKeyMostWithOverhead )		//	may be equal if Limit already set or client used JET_bitNormalizedKey
			{
			//	PERF: seek on Limit of key, otherwise we would
			//	end up on the first index entry for this key
			//	(because of the trailing bookmark) and we
			//	would have to laterally navigate past it
			//	(and possibly others)
			( (BYTE *)bm.key.suffix.Pv() )[bm.key.suffix.Cb()] = 0xff;
			bm.key.suffix.DeltaCb( 1 );
			}
		dib.dirflag = fDIRFavourNext;
		}
	else if ( grbit & JET_bitSeekEQ )
		dib.dirflag = fDIRExact;
	else
		dib.dirflag = fDIRNull;

	err = ErrDIRDown( pfucbSeek, &dib );

	//	remember return from seek
	//
	fFoundLess = ( err == wrnNDFoundLess );
	fFoundGreater = ( err == wrnNDFoundGreater );
	fFoundEqual = ( !fFoundGreater && 
							!fFoundLess &&
							err >= 0 );
	fRelease = ( err >= 0 );

	Assert( !fRelease || Pcsr( pfucbSeek )->FLatched() );
	Assert( err < 0 || fFoundEqual || fFoundGreater || fFoundLess );
	
#define bitSeekAll (JET_bitSeekEQ | JET_bitSeekGE | JET_bitSeekGT |	JET_bitSeekLE | JET_bitSeekLT)

	if ( fFoundEqual )
		{
		Assert( Pcsr( pfucbSeek )->FLatched() );
		Assert( locOnCurBM == pfucbSeek->locLogical );
		if ( pfucbTable->pfucbCurIndex != pfucbNil )
			{
			//	if a secondary index is in use,
			//	then position primary index on record
			//
			Assert( FFUCBSecondary( pfucbSeek ) );
			Assert( pfucbSeek == pfucbTable->pfucbCurIndex );

			//	goto bookmark pointed to by secondary index node
			//
			BOOKMARK	bmRecord;
			
			Assert(pfucbSeek->kdfCurr.data.Pv() != NULL);
			Assert(pfucbSeek->kdfCurr.data.Cb() > 0 );

			bmRecord.key.prefix.Nullify();
			bmRecord.key.suffix = pfucbSeek->kdfCurr.data;
			bmRecord.data.Nullify();

			//	We will need to touch the data page buffer.

			Call( ErrDIRGotoBookmark( pfucbTable, bmRecord ) );
			
			Assert( PgnoFDP( pfucbTable ) != pgnoSystemRoot );
			Assert( pfucbTable->u.pfcb->FPrimaryIndex() );
			}

		switch ( grbit & bitSeekAll )
			{
			case JET_bitSeekEQ:
				Assert( fRelease );
				Assert( Pcsr( pfucbSeek )->FLatched() );
				Call( ErrDIRRelease( pfucbSeek ) );
				fRelease = fFalse;
		
				//	found equal on seek equal.  If index range grbit is
				//	set then set index range upper inclusive.
				//
				if ( grbit & JET_bitSetIndexRange )
					{
					CallR( ErrIsamSetIndexRange( ppib, pfucbTable, JET_bitRangeInclusive | JET_bitRangeUpperLimit ) );
					}

				goto Release;
				break;

			case JET_bitSeekGE:
			case JET_bitSeekLE:
				//	release and return
				//
				CallS( err );
				goto Release;
				break;

			case JET_bitSeekGT:
				//	move to next node with different key
				//	release and return
				//
				err = ErrRECIMove( pfucbTable, JET_MoveNext, JET_bitMoveKeyNE );
				
				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				CallS( err );
				goto Release;
				break;

			case JET_bitSeekLT:
				//	move to previous node with different key
				//	release and return
				//
				err = ErrRECIMove( pfucbTable, JET_MovePrevious, JET_bitMoveKeyNE );
				
				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				CallS( err );
				goto Release;
				break;

			default:
				Assert( fFalse );
				return err;
			}
		}
	else if ( fFoundLess )
		{
		Assert( Pcsr( pfucbSeek )->FLatched() );
		Assert( locBeforeSeekBM == pfucbSeek->locLogical );
		
		switch ( grbit & bitSeekAll )
			{
			case JET_bitSeekEQ:
				if ( FFUCBUnique( pfucbSeek ) )
					{
					//	see RecordNotFound case below for an
					//	explanation of why we need to set
					//	the locLogical of the primary cursor
					//	(note: the secondary cursor will
					//	narmally get set to locLogical when
					//	we call ErrDIRRelease() below, but
					//	may get set to locOnFDPRoot if we
					//	couldn't save the bookmark)
					if ( pfucbTable != pfucbSeek )
						{
						Assert( !Pcsr( pfucbTable )->FLatched() );
						pfucbTable->locLogical = locOnSeekBM;
						}
					err = ErrERRCheck( JET_errRecordNotFound );
					goto Release;
					}
					
				//	For non-unique indexes, because we use
				//	key+data for keys of internal nodes,
				//	and because child nodes have a key
				//	strictly less than its parent, we
				//	might end up on the wrong leaf node.
				//	We want to go to the right sibling and
				//	check the first node there to see if
				//	the key-only matches.
				Assert( pfucbSeek->u.pfcb->FTypeSecondaryIndex() );	//	only secondary index can be non-unique

				//	FALL THROUGH

			case JET_bitSeekGE:
			case JET_bitSeekGT:
				//	move to next node 
				//	release and return
MoveNextOnNonUnique:
				err = ErrRECIMove( pfucbTable, JET_MoveNext, NO_GRBIT );

				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				if ( !FFUCBUnique( pfucbSeek ) )
					{
					//	For a non-unique index, there are some complexities
					//	because the keys are stored as key+data but we're
					//	doing a key-only search.  This might cause us to
					//	fall short of the node we truly want, so we have
					//	to laterally navigate.
					Assert( pfucbSeek == pfucbTable->pfucbCurIndex ); 
					Assert( Pcsr( pfucbSeek )->FLatched() );
					const INT	cmp = CmpKey( pfucbSeek->kdfCurr.key, bm.key );
					Assert( cmp >= 0 );
					
					if ( grbit & JET_bitSeekGE )
						{
						if ( 0 != cmp )
							err = ErrERRCheck( JET_wrnSeekNotEqual );
						}
					else if ( grbit & JET_bitSeekGT )
						{
						//	the keys match exactly, but we're doing
						//	a strictly greater than search, so must
						//	keep navigating
						if ( 0 == cmp )
							goto MoveNextOnNonUnique;
						}
					else
						{
						Assert( grbit & JET_bitSeekEQ );
						if ( 0 != cmp )
							err = ErrERRCheck( JET_errRecordNotFound );
						else if ( grbit & JET_bitSetIndexRange )
							{
							Assert( fRelease );
							Assert( Pcsr( pfucbSeek )->FLatched() );
							Call( ErrDIRRelease( pfucbSeek ) );
							fRelease = fFalse;
							
							CallR( ErrIsamSetIndexRange( ppib, pfucbTable, JET_bitRangeInclusive | JET_bitRangeUpperLimit ) );
							}
						}
					}

				else if ( grbit & JET_bitSeekGE )
					{
					err = ErrERRCheck( JET_wrnSeekNotEqual );
					}

				goto Release;
				break;
			
			case JET_bitSeekLE:
			case JET_bitSeekLT:
				//	move to previous node -- to adjust DIR level locLogical
				//	release and return
				//
				err = ErrRECIMove( pfucbTable, JET_MovePrevious, NO_GRBIT );

				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				if ( grbit & JET_bitSeekLE )
					{
					err = ErrERRCheck( JET_wrnSeekNotEqual );
					}
				goto Release;
				break;

			default:
				Assert( fFalse );
				return err;
			}
		}
	else if ( fFoundGreater )
		{
		Assert( Pcsr( pfucbSeek )->FLatched() );
		Assert( locAfterSeekBM == pfucbSeek->locLogical );
		
		switch ( grbit & bitSeekAll )
			{
			case JET_bitSeekEQ:
				//	see RecordNotFound case below for an
				//	explanation of why we need to set
				//	the locLogical of the primary cursor
				//	(note: the secondary cursor will
				//	narmally get set to locLogical when
				//	we call ErrDIRRelease() below, but
				//	may get set to locOnFDPRoot if we
				//	couldn't save the bookmark)
				if ( pfucbTable != pfucbSeek )
					{
					Assert( !Pcsr( pfucbTable )->FLatched() );
					pfucbTable->locLogical = locOnSeekBM;
					}
				err = ErrERRCheck( JET_errRecordNotFound );
				goto Release;
				break;

			case JET_bitSeekGE:
			case JET_bitSeekGT:
				//	move next to fix DIR level locLogical
				//	release and return
				//
				err = ErrRECIMove( pfucbTable, JET_MoveNext, NO_GRBIT );

				Assert( err >= 0 );
				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				if ( grbit & JET_bitSeekGE )
					{
					err = ErrERRCheck( JET_wrnSeekNotEqual );
					}
				goto Release;
				break;
			
			case JET_bitSeekLE:
			case JET_bitSeekLT:
				//	move previous
				//	release and return
				//
				err = ErrRECIMove( pfucbTable, JET_MovePrevious, NO_GRBIT );

				if ( err < 0 )
					{
					AssertDIRNoLatch( ppib );
					if ( JET_errNoCurrentRecord == err )
						{
						KSReset( pfucbSeek );
						return ErrERRCheck( JET_errRecordNotFound );
						}

					goto HandleError;
					}

				Assert( CmpKey( pfucbSeek->kdfCurr.key, bm.key ) < 0 );
				if ( grbit & JET_bitSeekLE )
					{
					err = ErrERRCheck( JET_wrnSeekNotEqual );
					}
				goto Release;
				break;

			default:
				Assert( fFalse );
				return err;
			}
		}
	else
		{
		Assert( err < 0 );
		Assert( JET_errNoCurrentRecord != err );
		Assert( !Pcsr( pfucbSeek )->FLatched() );

		if ( JET_errRecordNotFound == err )
			{
			//	The secondary index cursor has been placed on a
			//	virtual record, so we must update the primary
			//	index cursor as well (if not, then it's possible
			//	to do, for instance, a RetrieveColumn on the
			//	primary cursor and you'll get back data from the
			//	record you were on before the seek but a
			//	RetrieveFromIndex on the secondary cursor will
			//	return JET_errNoCurrentRecord).
			//	Note that although the locLogical of the primary
			//	cursor is being updated, it's not necessary to
			//	update the primary cursor's bmCurr, because it
			//	will never be accessed (the secondary cursor takes
			//	precedence).  The only reason we reset the
			//	locLogical is for error-handling so that we
			//	properly err out with JET_errNoCurrentRecord if
			//	someone tries to use the cursor to access a record
			//	before repositioning the secondary cursor to a true
			//	record.
			Assert( locOnSeekBM == pfucbSeek->locLogical );
			if ( pfucbTable != pfucbSeek )
				{
				Assert( !Pcsr( pfucbTable )->FLatched() );
				pfucbTable->locLogical = locOnSeekBM;
				}
			}

		KSReset( pfucbSeek );
		AssertDIRNoLatch( ppib );
		return err;
		}

Release:
	//	release latched page and return error
	//
	if ( fRelease )
		{
		Assert( Pcsr( pfucbSeek ) ->FLatched() );
		const ERR	errT	= ErrDIRRelease( pfucbSeek );
		if ( errT < 0 )
			{
			err = errT;
			goto HandleError;
			}
		}
	KSReset( pfucbSeek );
	AssertDIRNoLatch( ppib );
	return err;

HandleError:
	//	reset cursor to before first
	//
	Assert( err < 0 );
	KSReset( pfucbSeek );
	DIRUp( pfucbSeek );
	DIRBeforeFirst( pfucbSeek );
	if ( pfucbTable != pfucbSeek )
		{
		Assert( !Pcsr( pfucbTable )->FLatched() );
		DIRBeforeFirst( pfucbTable );
		}
	AssertDIRNoLatch( ppib );
	return err;
	}

	
//	=================================================================
LOCAL ERR ErrRECICheckIndexrangesForUniqueness(
			const JET_SESID sesid,
			const JET_INDEXRANGE * const rgindexrange,
			const ULONG cindexrange )
//	=================================================================
	{
 	PIB	* const ppib		= reinterpret_cast<PIB *>( sesid );

	//  check that all the tableid's are on the same table and different indexes
	INT itableid;
	for( itableid = 0; itableid < cindexrange; ++itableid )
		{
		if( sizeof( JET_INDEXRANGE ) != rgindexrange[itableid].cbStruct )
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}
		
		const FUCB * const pfucb	= reinterpret_cast<FUCB *>( rgindexrange[itableid].tableid );
		CheckTable( ppib, pfucb );
		CheckSecondary( pfucb );		

		//  don't do a join on the primary index!
			
		if( !pfucb->pfucbCurIndex )
			{
			AssertSz( fFalse, "Don't do a join on the primary index!" );
			return ErrERRCheck( JET_errInvalidParameter );
			}

		//  check the GRBITs

		if( JET_bitRecordInIndex != rgindexrange[itableid].grbit 
			&& JET_bitRecordNotInIndex != rgindexrange[itableid].grbit )
			{
			AssertSz( fFalse, "Invalid grbit in JET_INDEXRANGE" );
			return ErrERRCheck( JET_errInvalidGrbit );
			}

		//  check against all other indexes for duplications
		
		INT itableidT;
		for( itableidT = 0; itableidT < cindexrange; ++itableidT )
			{
			if( itableidT == itableid )
				{
				continue;
				}
				
			const FUCB * const pfucbT	= reinterpret_cast<FUCB *>( rgindexrange[itableidT].tableid );

			//  don't do a join on the primary index!
			
			if( !pfucbT->pfucbCurIndex )
				{
				AssertSz( fFalse, "Don't do a join on the primary index!" );
				return ErrERRCheck( JET_errInvalidParameter );
				}

			//  compare FCB's to make sure we are on the same table
			
			if( pfucbT->u.pfcb != pfucb->u.pfcb )
				{
				AssertSz( fFalse, "Indexes are not on the same table" );
				return ErrERRCheck( JET_errInvalidParameter );
				}

			//  compare secondary indexes to make sure the indexes are different

			if( pfucb->pfucbCurIndex->u.pfcb == pfucbT->pfucbCurIndex->u.pfcb )
				{
				AssertSz( fFalse, "Indexes are the same" );
				return ErrERRCheck( JET_errInvalidParameter );
				}
			}
		}
	return JET_errSuccess;
	}


//	=================================================================
LOCAL ERR ErrRECIInsertBookmarksIntoSort(
			PIB * const ppib,
			FUCB * const pfucb,
			FUCB * const pfucbSort,
			const JET_GRBIT grbit )
//	=================================================================
	{
	ERR err;

	INT cBookmarks = 0;
	
	KEY key;
	key.prefix.Nullify();

	DATA data;
	data.Nullify();
	
	Call( ErrDIRGet( pfucb ) );
	do
		{
		key.suffix = pfucb->kdfCurr.data;
		Call( ErrSORTInsert( pfucbSort, key, data ) );
		++cBookmarks;
		}
	while( ( err = ErrDIRNext( pfucb, fDIRNull ) ) == JET_errSuccess );

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	DIRUp( pfucb );
	FUCBResetPreread( pfucb );
	
	return err;
	}


//	=================================================================
LOCAL ERR ErrRECIJoinFindDuplicates(
			JET_SESID sesid,
			const JET_INDEXRANGE * const rgindexrange,			
			FUCB * rgpfucbSort[],
			const ULONG cindexes,
			JET_RECORDLIST * const precordlist,
			JET_GRBIT grbit )
//	=================================================================
	{
 	BOOL		rgfSortIsMin[64];
	memset( rgfSortIsMin, 0, sizeof( rgfSortIsMin ) );
	INT isort;
	
	ERR err;

	
	//  move all the sorts to the first record
	
	for( isort = 0; isort < cindexes; ++isort )
		{
		err = ErrSORTNext( rgpfucbSort[isort] );
		if( JET_errNoCurrentRecord == err )
			{
			//  no bookmarks to return
			err = JET_errSuccess;
			return err;
			}
		Call( err );
		}

	//  create the temp table for the bookmarks

	Assert( 1 == sizeof( rgcolumndefJoinlist ) / sizeof( rgcolumndefJoinlist[0] ) );
	Call( ErrIsamOpenTempTable(
				sesid,
				rgcolumndefJoinlist,
				sizeof( rgcolumndefJoinlist ) / sizeof( rgcolumndefJoinlist[0] ),
				NULL,
				NO_GRBIT,
				&(precordlist->tableid),
				&(precordlist->columnidBookmark ) ) );

	//  take a unique list of bookmarks from the sorts

	while( 1 )
		{
		INT 	isortMin 	= 0;
				
		//  find the index of the sort with the smallest bookmark
		
		for( isort = 1; isort < cindexes; ++isort )
			{
			const INT cmp = CmpKey( rgpfucbSort[isortMin]->kdfCurr.key, rgpfucbSort[isort]->kdfCurr.key );
			if( 0 == cmp )
				{
				}
			else if( cmp < 0 )
				{
				//  the current min is smaller
				}
			else
				{
				//  we have a new minimum				
				Assert( cmp > 0 );
				isortMin = isort;
				}
			}

		//  see if all keys are the same as the minimum

		BOOL 	fDuplicate 	= fTrue;
		memset( rgfSortIsMin, 0, sizeof( rgfSortIsMin ) );
		
		rgfSortIsMin[isortMin] = fTrue;
		for( isort = 0; isort < cindexes; ++isort )
			{
			if( isort == isortMin )
				{
				continue;
				}
			const INT cmp = CmpKey( rgpfucbSort[isortMin]->kdfCurr.key, rgpfucbSort[isort]->kdfCurr.key );
			if( 0 == cmp )
				{
				if( JET_bitRecordInIndex == rgindexrange[isort].grbit )
					{
					}
				else if( JET_bitRecordNotInIndex == rgindexrange[isort].grbit )
					{
					fDuplicate = fFalse;
					}
				else
					{
					Assert( fFalse );
					}
				rgfSortIsMin[isort] = fTrue;
				}
			else
				{
				if( JET_bitRecordInIndex == rgindexrange[isort].grbit )
					{
					fDuplicate = fFalse;
					}
				else if( JET_bitRecordNotInIndex == rgindexrange[isort].grbit )
					{
					}
				else
					{
					Assert( fFalse );
					}

				fDuplicate = fFalse;
				}
			}

		//  if there are duplicates, insert into the temp table
		
		if( fDuplicate )
			{
			Call( ErrDispPrepareUpdate( sesid, precordlist->tableid, JET_prepInsert ) );

			Assert( rgpfucbSort[isortMin]->kdfCurr.key.prefix.FNull() );
			Call( ErrDispSetColumn(
						sesid,
						precordlist->tableid,
						precordlist->columnidBookmark,
						rgpfucbSort[isortMin]->kdfCurr.key.suffix.Pv(),
						rgpfucbSort[isortMin]->kdfCurr.key.suffix.Cb(),
						NO_GRBIT,
						NULL ) );

			Call( ErrDispUpdate( sesid, precordlist->tableid, NULL, 0, NULL, NO_GRBIT ) );

			++(precordlist->cRecord);
			}

		//  remove all minimums
		
		for( isort = 0; isort < cindexes; ++isort )
			{
			if( rgfSortIsMin[isort] )
				{
				err = ErrSORTNext( rgpfucbSort[isort] );
				if( JET_errNoCurrentRecord == err )
					{
					err = JET_errSuccess;
					return err;
					}
				Call( err );
				}	
			}
		}

HandleError:

	if( err < 0 && JET_tableidNil != precordlist->tableid )
		{
		CallS( ErrDispCloseTable( sesid, precordlist->tableid ) );
		precordlist->tableid = JET_tableidNil;
		}

	return err;
	}


//	=================================================================
ERR ErrIsamIntersectIndexes(
	const JET_SESID sesid,
	const JET_INDEXRANGE * const rgindexrange,
	const ULONG cindexrange,
	JET_RECORDLIST * const precordlist,
	const JET_GRBIT grbit )
//	=================================================================
	{
 	PIB	* const ppib		= reinterpret_cast<PIB *>( sesid );
 	
 	const INT	cSort = 64;
 	FUCB * 		rgpfucbSort[64];
 	
 	INT 		isort;

	ERR			err;

	//  check input parameters
	
	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );

	if( NULL == precordlist
		|| sizeof( JET_RECORDLIST ) != precordlist->cbStruct )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	if( 0 == cindexrange )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}
		
	if( cindexrange > cSort )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	//  check that all the tableid's are on the same table and different indexes
	
	CallR( ErrRECICheckIndexrangesForUniqueness( sesid, rgindexrange, cindexrange ) );

	//  set all the sort's to NULL

	for( isort = 0; isort < cindexrange; ++isort )
		{
		rgpfucbSort[isort] = pfucbNil;
		}

	//  initialize the pjoinlist

	precordlist->tableid	= JET_tableidNil;
	precordlist->cRecord 	= 0;
	precordlist->columnidBookmark = 0;
	
	//  create the sorts

	for( isort = 0; isort < cindexrange; ++isort )
		{
		Call( ErrSORTOpen( ppib, rgpfucbSort + isort, fTrue, fTrue ) );	
		}

	//  for each index, put all the primary keys in its index range into its sort

	for( isort = 0; isort < cindexrange; ++isort )
		{
		FUCB * const pfucb	= reinterpret_cast<FUCB *>( rgindexrange[isort].tableid );
		Call( ErrRECIInsertBookmarksIntoSort( ppib, pfucb->pfucbCurIndex, rgpfucbSort[isort], grbit ) );
		Call( ErrSORTEndInsert( rgpfucbSort[isort] ) );
		}

	//  insert duplicate bookmarks into a new temp table
	
	Call( ErrRECIJoinFindDuplicates(
			sesid,
			rgindexrange,
			rgpfucbSort,
			cindexrange,
			precordlist,
			grbit ) );

HandleError:

	//  close all the sorts

	for( isort = 0; isort < cindexrange; ++isort )
		{
		if( pfucbNil != rgpfucbSort[isort] )
			{
			SORTClose( rgpfucbSort[isort] );
			rgpfucbSort[isort] = pfucbNil;
			}
		}
	
	return err;
	}


LOCAL ERR ErrRECIGotoBookmark(
	PIB*				ppib,
	FUCB *				pfucb,
	const VOID * const	pvBookmark,
	const ULONG			cbBookmark,
	const ULONG			itagSequence )
	{
	ERR					err;
	BOOKMARK			bm;
	
	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	Assert( FFUCBIndex( pfucb ) );
	Assert( FFUCBPrimary( pfucb ) );
	Assert( pfucb->u.pfcb->FPrimaryIndex() );
	CheckSecondary( pfucb );

	if( 0 == cbBookmark || NULL == pvBookmark )
		{
		//  don't pass a NULL bookmark into the DIR level
		return ErrERRCheck( JET_errInvalidBookmark );
		}

	//	reset copy buffer status
	//
	if ( FFUCBUpdatePrepared( pfucb ) )
		{
		CallR( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
		}

	//	reset index range limit
	//
	DIRResetIndexRange( pfucb );

	KSReset( pfucb );

	//	get node, and return error if this node is not there for caller.
	//
	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( const_cast<VOID *>( pvBookmark ) );
	bm.key.suffix.SetCb( cbBookmark );
	bm.data.Nullify();

	Call( ErrDIRGotoJetBookmark( pfucb, bm ) );
	AssertDIRNoLatch( ppib );
	
	Assert( pfucb->u.pfcb->FPrimaryIndex() );
	Assert( PgnoFDP( pfucb ) != pgnoSystemRoot );
	
	//	goto bookmark record build key for secondary index
	//	to bookmark record
	//
	if ( pfucb->pfucbCurIndex != pfucbNil )
		{
		//	get secondary index cursor
		//
		FUCB	*pfucbIdx = pfucb->pfucbCurIndex;
		IDB		*pidb;
		KEY 	key;
		
		Assert( pfucbIdx->u.pfcb != pfcbNil );
		Assert( pfucbIdx->u.pfcb->FTypeSecondaryIndex() );
		
		pidb = pfucbIdx->u.pfcb->Pidb();
		Assert( pidb != pidbNil );
		Assert( !pidb->FPrimary() );
		
		KSReset( pfucbIdx );

		//	allocate goto bookmark resources
		//
		if ( NULL == pfucbIdx->dataSearchKey.Pv() )
			{
			pfucbIdx->dataSearchKey.SetPv( PvOSMemoryHeapAlloc( cbKeyMostWithOverhead ) );
			if ( NULL == pfucbIdx->dataSearchKey.Pv() )
				return ErrERRCheck( JET_errOutOfMemory );
			pfucbIdx->dataSearchKey.SetCb( cbKeyMostWithOverhead );
			}

		//	make key for record for secondary index
		//
		key.prefix.Nullify();
		key.suffix.SetPv( pfucbIdx->dataSearchKey.Pv() );
		key.suffix.SetCb( pfucbIdx->dataSearchKey.Cb() );
		Call( ErrRECRetrieveKeyFromRecord( pfucb, pidb, &key, itagSequence, 0, fFalse ) );
		AssertDIRNoLatch( ppib );
		if ( wrnFLDOutOfKeys == err )
			{
			Assert( itagSequence > 1 );
			err = ErrERRCheck( JET_errBadItagSequence );
			goto HandleError;
			}

		//	record must honor index no NULL segment requirements
		//
		if ( pidb->FNoNullSeg() )
			{
			Assert( wrnFLDNullSeg != err );
			Assert( wrnFLDNullFirstSeg != err );
			Assert( wrnFLDNullKey != err );
			}

		//	if item is not index, then move before first instead of seeking
		//
		Assert( err > 0 || JET_errSuccess == err );
		if ( err > 0
			&& ( ( wrnFLDNullKey == err && !pidb->FAllowAllNulls() )
				|| ( wrnFLDNullFirstSeg == err && !pidb->FAllowFirstNull() )
				|| ( wrnFLDNullSeg == err && !pidb->FAllowSomeNulls() ) )
				|| wrnFLDOutOfTuples == err
				|| wrnFLDNotPresentInIndex == err )
			{
			//	assumes that NULLs sort low
			//
			DIRBeforeFirst( pfucbIdx );
			err = ErrERRCheck( JET_errNoCurrentRecord );
			}
		else
			{
			//	move to DATA root
			//
			DIRGotoRoot( pfucbIdx );

			//	seek on secondary key and primary key as data
			//
			Assert( bm.key.prefix.FNull() );
			Call( ErrDIRDownKeyData( pfucbIdx, key, bm.key.suffix ) );
			CallS( err );
			}
		}

	CallSx( err, JET_errNoCurrentRecord );

HandleError:
	AssertDIRNoLatch( ppib );
	return err;
	}

ERR VTAPI ErrIsamGotoBookmark(
	JET_SESID			sesid,
	JET_VTID			vtid,
	const VOID * const	pvBookmark,
	const ULONG			cbBookmark )
	{
	return ErrRECIGotoBookmark(
						reinterpret_cast<PIB *>( sesid ),
						reinterpret_cast<FUCB *>( vtid ),
						pvBookmark,
						cbBookmark,
						1 );
	}

ERR VTAPI ErrIsamGotoIndexBookmark(
	JET_SESID			sesid,
	JET_VTID			vtid,
	const VOID * const	pvSecondaryKey,
	const ULONG			cbSecondaryKey,
	const VOID * const	pvPrimaryBookmark,
	const ULONG			cbPrimaryBookmark,
	const JET_GRBIT		grbit )
	{
	ERR					err;
 	PIB *				ppib		= reinterpret_cast<PIB *>( sesid );
	FUCB *				pfucb		= reinterpret_cast<FUCB *>( vtid );
	FUCB * const		pfucbIdx	= pfucb->pfucbCurIndex;
	BOOL				fInTrx		= fFalse;
	BOOKMARK			bm;

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	Assert( FFUCBIndex( pfucb ) );
	Assert( FFUCBPrimary( pfucb ) );
	Assert( pfucb->u.pfcb->FPrimaryIndex() );

	if ( pfucbNil == pfucbIdx )
		{
		return ErrERRCheck( JET_errNoCurrentIndex );
		}

	Assert( FFUCBSecondary( pfucbIdx ) );
	Assert( pfucbIdx->u.pfcb->FTypeSecondaryIndex() );

	if( 0 == cbSecondaryKey || NULL == pvSecondaryKey )
		{
		//  don't pass a NULL bookmark into the DIR level
		return ErrERRCheck( JET_errInvalidBookmark );
		}

	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( const_cast<VOID *>( pvSecondaryKey ) );
	bm.key.suffix.SetCb( cbSecondaryKey );

	if ( FFUCBUnique( pfucbIdx ) )
		{
		//	don't need primary bookmark, even if one was specified
		bm.data.Nullify();
		}
	else
		{
		if ( 0 == cbPrimaryBookmark || NULL == pvPrimaryBookmark )
			return ErrERRCheck( JET_errInvalidBookmark );

		bm.data.SetPv( const_cast<VOID *>( pvPrimaryBookmark ) );
		bm.data.SetCb( cbPrimaryBookmark );
		}


	//	reset copy buffer status
	//
	if ( FFUCBUpdatePrepared( pfucb ) )
		{
		Call( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
		}

	//	reset index range limit
	//
	DIRResetIndexRange( pfucb );

	KSReset( pfucb );
	KSReset( pfucbIdx );

	if ( 0 == ppib->level )
		{
		Call( ErrDIRBeginTransaction( ppib, JET_bitTransactionReadOnly ) );
		fInTrx = fTrue;
		}

	//	move secondary cursor to desired location
	err = ErrDIRGotoJetBookmark( pfucbIdx, bm );
	Assert( !Pcsr( pfucbIdx )->FLatched() );

	//	Within ErrDIRGotoJetBookmark(), JET_errRecordDeleted
	//	from ErrBTGotoBookmark() means that the node with the
	//	specified bookmark was physically expunged, and
	//	JET_errRecordDeleted from ErrBTGet() means that the
	//	node is still physically present, but not visible
	//	to this cursor. In either case, establish virtual
	//	currency (so DIRNext/Prev() will work) if permitted.
	if ( JET_errRecordDeleted == err
		&& ( grbit & JET_bitBookmarkPermitVirtualCurrency ) )
		{
		Call( ErrBTDeferGotoBookmark( pfucbIdx, bm, fFalse/*no touch*/ ) );
		pfucbIdx->locLogical = locOnSeekBM;
		}
	else
		{
		Call( err );
		}

	CallS( err );
	AssertDIRNoLatch( ppib );

	Assert( locOnCurBM == pfucbIdx->locLogical
		|| locOnSeekBM == pfucbIdx->locLogical );
	Assert( bm.key.prefix.FNull() );
	bm.key.suffix = pfucbIdx->bmCurr.data;
	bm.data.Nullify();

	//	move primary cursor to match secondary cursor
	err = ErrBTDeferGotoBookmark( pfucb, bm, fTrue/*touch*/ );
	CallSx( err, JET_errOutOfMemory );
	if ( err < 0 )
		{
		//	force both cursors to a virtual currency
		//	UNDONE: I'm not sure what exactly the
		//	appropriate currency should be if we
		//	err out here
		pfucbIdx->locLogical = locOnSeekBM;
		pfucb->locLogical = locOnSeekBM;
		goto HandleError;
		}

	//	The secondary index cursor may have been placed on a
	//	virtual record, so we must update the primary
	//	index cursor as well (if not, then it's possible
	//	to do, for instance, a RetrieveColumn on the
	//	primary cursor and you'll get back data from the
	//	record you were on before the seek but a
	//	RetrieveFromIndex on the secondary cursor will
	//	return JET_errNoCurrentRecord).
	//	Note that although the locLogical of the primary
	//	cursor is being updated, it's not necessary to
	//	update the primary cursor's bmCurr, because it
	//	will never be accessed (the secondary cursor takes
	//	precedence).  The only reason we reset the
	//	locLogical is for error-handling so that we
	//	properly err out with JET_errNoCurrentRecord if
	//	someone tries to use the cursor to access a record
	//	before repositioning the secondary cursor to a true
	//	record.
	Assert( locOnCurBM == pfucbIdx->locLogical
		|| locOnSeekBM == pfucbIdx->locLogical );
	pfucb->locLogical = pfucbIdx->locLogical;

	Assert( FDataEqual( pfucbIdx->bmCurr.data, pfucb->bmCurr.key.suffix ) );

	if ( locOnSeekBM == pfucbIdx->locLogical )
		{
		//	if currency was placed on virtual bookmark, record must have gotten deleted
		Call( ErrERRCheck( JET_errRecordDeleted ) );
		}

	else if ( FFUCBUnique( pfucbIdx ) && 0 != cbPrimaryBookmark )
		{
		//	on a unique index, we'll ignore any primary bookmark the
		//	user may have specified, so need to verify this is truly
		//	the record the user requested
		if ( pfucbIdx->bmCurr.data.Cb() != cbPrimaryBookmark
			|| 0 != memcmp( pfucbIdx->bmCurr.data.Pv(), pvPrimaryBookmark, cbPrimaryBookmark ) )
			{
			//	index entry was found, but has a different primary
			//	bookmark, so must assume that original record
			//	was deleted (and new record subsequently got
			//	inserted with same secondary index key, but
			//	different primary key)
			//	NOTE: the user's currency will be left on the
			//	index entry matching the desired key, even
			//	though we're erring out because the primary'
			//	bookmark doesn't match
			Call( ErrERRCheck( JET_errRecordDeleted ) );
			}
		}

	CallS( err );

HandleError:
	if ( fInTrx )
		{
		CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
		}

	AssertDIRNoLatch( ppib );
	return err;
	}


ERR VTAPI ErrIsamGotoPosition( JET_SESID sesid, JET_VTID vtid, JET_RECPOS *precpos )
	{
 	PIB *ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB *pfucb = reinterpret_cast<FUCB *>( vtid );

	ERR		err;
	FUCB 	*pfucbSecondary;

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	if ( precpos->centriesLT > precpos->centriesTotal )
		{
		err = ErrERRCheck( JET_errInvalidParameter );
		return err;
		}

	//	Reset copy buffer status
	//
	if ( FFUCBUpdatePrepared( pfucb ) )
		{
		CallR( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
		}

	//	reset index range limit
	//
	DIRResetIndexRange( pfucb );

	//	reset key stat
	//
	KSReset( pfucb );

	//	set non primary index pointer, may be null
	//
	pfucbSecondary = pfucb->pfucbCurIndex;

	if ( pfucbSecondary == pfucbNil )
		{
		//	move to DATA root
		//
		DIRGotoRoot( pfucb );

		err = ErrDIRGotoPosition( pfucb, precpos->centriesLT, precpos->centriesTotal );

		if ( err >= 0 )
			{
			Assert( Pcsr( pfucb )->FLatched( ) );

			err = ErrDIRRelease( pfucb );
			}
		}
	else
		{
		ERR			errT = JET_errSuccess;
		
		//	move to DATA root
		//
		DIRGotoRoot( pfucbSecondary );

		err = ErrDIRGotoPosition( pfucbSecondary, precpos->centriesLT, precpos->centriesTotal );

		//	if the movement was successful and a secondary index is
		//	in use, then position primary index to record.
		//
		if ( JET_errSuccess == err )
			{
			//	goto bookmark pointed to by secondary index node
			//
			BOOKMARK	bmRecord;
			
			Assert(pfucbSecondary->kdfCurr.data.Pv() != NULL);
			Assert(pfucbSecondary->kdfCurr.data.Cb() > 0 );

			bmRecord.key.prefix.Nullify();
			bmRecord.key.suffix = pfucbSecondary->kdfCurr.data;
			bmRecord.data.Nullify();

			//	We will need to touch the data page buffer.

			errT = ErrDIRGotoBookmark( pfucb, bmRecord );

			Assert( PgnoFDP( pfucb ) != pgnoSystemRoot );
			Assert( pfucb->u.pfcb->FPrimaryIndex() );
			}

		if ( err >= 0 )
			{
			//	release latch
			//
			err = ErrDIRRelease( pfucbSecondary );

			if ( errT < 0 && err >= 0 )
				{
				//	propagate the more severe error
				//
				err = errT;
				}
			}
		else
			{
			AssertDIRNoLatch( ppib );
			}
		}

	AssertDIRNoLatch( ppib );
	
	//	if no records then return JET_errRecordNotFound
	//	otherwise return error from called routine
	//
	if ( err < 0 )
		{
		DIRBeforeFirst( pfucb );

		if ( pfucbSecondary != pfucbNil )
			{
			DIRBeforeFirst( pfucbSecondary );
			}
		}
	else
		{
		Assert ( JET_errSuccess == err || wrnNDFoundLess == err || wrnNDFoundGreater == err );
		err = JET_errSuccess;
		}

	return err;
	}


ERR VTAPI ErrIsamSetIndexRange( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
	ERR		err			= JET_errSuccess;
 	PIB		*ppib		= reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucbTable	= reinterpret_cast<FUCB *>( vtid );
	FUCB	*pfucb		= pfucbTable->pfucbCurIndex ? 
							pfucbTable->pfucbCurIndex : pfucbTable;

	/*	ppib is not used in this function
	/**/
	NotUsed( ppib );
	AssertDIRNoLatch( ppib );

	/*	if instant duration index range, then reset index range.
	/**/
	if ( grbit & JET_bitRangeRemove )
		{
		if ( FFUCBLimstat( pfucb ) )
			{
			DIRResetIndexRange( pfucbTable );
			CallS( err );
			goto HandleError;
			}
		else
			{
			Call( ErrERRCheck( JET_errInvalidOperation ) );
			}
		}

	/*	must be on index
	/**/
	if ( pfucb == pfucbTable )
		{
		FCB		*pfcbTable = pfucbTable->u.pfcb;
		BOOL	fPrimaryIndexTemplate = fFalse;

		Assert( pfcbNil != pfcbTable );
		Assert( pfcbTable->FPrimaryIndex() );
		
		if ( pfcbTable->FDerivedTable() )
			{
			Assert( pfcbTable->Ptdb() != ptdbNil );
			Assert( pfcbTable->Ptdb()->PfcbTemplateTable() != pfcbNil );
			FCB	*pfcbTemplateTable = pfcbTable->Ptdb()->PfcbTemplateTable();
			if ( !pfcbTemplateTable->FSequentialIndex() )
				{
				//	If template table has a primary index,
				//	we must have inherited it,
				fPrimaryIndexTemplate = fTrue;
				Assert( pfcbTemplateTable->Pidb() != pidbNil );
				Assert( pfcbTemplateTable->Pidb()->FTemplateIndex() );
				Assert( pfcbTable->Pidb() == pfcbTemplateTable->Pidb() );
				}
			else
				{
				Assert( pfcbTemplateTable->Pidb() == pidbNil );
				}
			}

		//	if primary index was present when schema was faulted in,
		//	no need for further check because we don't allow
		//	the primary index to be deleted
		if ( !fPrimaryIndexTemplate && !pfcbTable->FInitialIndex() )
			{
			pfcbTable->EnterDML();
			if ( pidbNil == pfcbTable->Pidb() )
				{
				err = ErrERRCheck( JET_errNoCurrentIndex );
				}
			else
				{
				err = ErrFILEIAccessIndex( ppib, pfcbTable, pfcbTable );
				if ( JET_errIndexNotFound == err )
					err = ErrERRCheck( JET_errNoCurrentIndex );
				}
			pfcbTable->LeaveDML();
			Call( err );
			}
		}

	/*	key must be prepared
	/**/
	if ( !( FKSPrepared( pfucb ) ) )
		{
		Call( ErrERRCheck( JET_errKeyNotMade ) );
		}

	FUCBAssertValidSearchKey( pfucb );
	
	/*	set index range and check current position
	/**/
	DIRSetIndexRange( pfucb, grbit );
	err = ErrDIRCheckIndexRange( pfucb );

	/*	reset key status
	/**/
	KSReset( pfucb );

	/*	if instant duration index range, then reset index range.
	/**/
	if ( grbit & JET_bitRangeInstantDuration )
		{
		DIRResetIndexRange( pfucbTable );
		}

HandleError:
	AssertDIRNoLatch( ppib );
	return err;
	}


ERR ErrIsamSetCurrentIndex(
	JET_SESID			vsesid,
	JET_VTID			vtid,
	const CHAR			*szName,
	const JET_INDEXID	*pindexid,			//	default = NULL
	const JET_GRBIT		grbit,				//	default = JET_bitMoveFirst
	const ULONG			itagSequence )		//	default = 1
	{
	ERR					err;
	PIB					*ppib			= (PIB *)vsesid;
	FUCB				*pfucb			= (FUCB *)vtid;
	CHAR				szIndex[ (JET_cbNameMost + 1) ];

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );
	Assert(	JET_bitMoveFirst == grbit || JET_bitNoMove == grbit );

	//	index name may be Null string for no index
	//
	if ( szName == NULL || *szName == '\0' )
		{
		*szIndex = '\0';
		}
	else
		{
		Call( ErrUTILCheckName( szIndex, szName, (JET_cbNameMost + 1) ) );
		}

	if ( JET_bitMoveFirst == grbit )
		{
		//	Reset copy buffer status
		//
		if ( FFUCBUpdatePrepared( pfucb ) )
			{
			CallR( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
			}
		
		//	change index and defer move first
		//
		Call( ErrRECSetCurrentIndex( pfucb, szIndex, (INDEXID *)pindexid ) );
		AssertDIRNoLatch( ppib );

		if( pfucb->u.pfcb->FPreread() && pfucbNil != pfucb->pfucbCurIndex )
			{
			//  Preread the root of the index
			BFPrereadPageRange( pfucb->ifmp, pfucb->pfucbCurIndex->u.pfcb->PgnoFDP(), 1 );
			}
		
		RECDeferMoveFirst( ppib, pfucb );
		}
	else
		{
		Assert( JET_bitNoMove == grbit );
		
		//	get bookmark of current record, change index,
		//	and goto bookmark.
		BOOKMARK	*pbm;
			
		Call( ErrDIRGetBookmark( pfucb, &pbm ) );
		AssertDIRNoLatch( ppib );
			
		Call( ErrRECSetCurrentIndex( pfucb, szIndex, (INDEXID *)pindexid ) );
			
		//	UNDONE:	error handling.  We should not have changed
		//	currency or current index, if set current index
		//	fails for any reason.  Note that this functionality
		//	could be provided by duplicating the cursor, on 
		//	the primary index, setting the current index to the
		//	new index, getting the bookmark from the original 
		//	cursor, goto bookmark on the duplicate cursor, 
		//	instating the duplicate cursor for the table id of
		//	the original cursor, and closing the original cursor.
		//
		Assert( pbm->key.Cb() > 0 );
		Assert( pbm->data.Cb() == 0 );
			
		Assert( pbm->key.prefix.FNull() );
		Call( ErrRECIGotoBookmark(
					pfucb->ppib,
					pfucb,
					pbm->key.suffix.Pv(),
					pbm->key.suffix.Cb(),
					max( 1, itagSequence ) ) );
		}

HandleError:
	AssertDIRNoLatch( ppib );
	return err;
	}


ERR ErrRECSetCurrentIndex(
	FUCB			*pfucb,
	const CHAR		*szIndex,
	const INDEXID	*pindexid )
	{
	ERR				err;
	FCB				*pfcbTable;
	FCB				*pfcbSecondary;
	FUCB			**ppfucbCurIdx;
	FUCB			* pfucbOldIndex			= pfucbNil;
	BOOL			fSettingToPrimaryIndex	= fFalse;
	BOOL			fInDMLLatch				= fFalse;
	BOOL			fIncrementedRefCount	= fFalse;
	BOOL			fClosedPreviousIndex	= fFalse;

	Assert( pfucb != pfucbNil );
	Assert( FFUCBIndex( pfucb ) );
	AssertDIRNoLatch( pfucb->ppib );

	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );
	Assert( pfcbTable->FPrimaryIndex() );

	Assert( pfcbTable->Ptdb() != ptdbNil );

	ppfucbCurIdx = &pfucb->pfucbCurIndex;

	//	szIndex == primary index or NULL
	//
	if ( NULL != pindexid )
		{
		const FCB	* pfcbIndex	= pindexid->pfcbIndex;

		if ( sizeof(INDEXID) != pindexid->cbStruct )
			{
			return ErrERRCheck( JET_errInvalidParameter );
			}

		else if ( !pfcbIndex->FValid( PinstFromPfucb( pfucb ) ) )
			{
			AssertSz( fFalse, "Bogus FCB pointer." );
			return ErrERRCheck( JET_errInvalidIndexId );
			}

		else if ( pfcbIndex->ObjidFDP() != pindexid->objidFDP
				|| pfcbIndex->PgnoFDP() != pindexid->pgnoFDP )
			{
			return ErrERRCheck( JET_errInvalidIndexId );
			}

		else if ( pfcbIndex == pfcbTable )
			{
			fSettingToPrimaryIndex = fTrue;
			}

		else if ( NULL != *ppfucbCurIdx && pfcbIndex == (*ppfucbCurIdx)->u.pfcb )
			{
			//	switching to the current index
			return JET_errSuccess;
			}

		else if ( !pfcbIndex->FTypeSecondaryIndex()
				|| pfcbIndex->PfcbTable() != pfcbTable )
			{
			return ErrERRCheck( JET_errInvalidIndexId );
			}

		else
			{
			//	verify index visibility
			pfcbTable->EnterDML();

			err = ErrFILEIAccessIndex( pfucb->ppib, pfcbTable, pfcbIndex );

			if ( err < 0 )
				{
				pfcbTable->LeaveDML();

				//	return JET_errInvalidIndexId if index not visible to us
				return ( JET_errIndexNotFound == err ?
								ErrERRCheck( JET_errInvalidIndexId ) :
								err );
				}

			else if ( pfcbIndex->Pidb()->FTemplateIndex() )
				{
				// Don't need refcount on template indexes, since we
				// know they'll never go away.
				Assert( pfcbIndex->Pidb()->CrefCurrentIndex() == 0 );
				}

			else
				{
				//	pin the index until we're ready to open a cursor on it
				pfcbIndex->Pidb()->IncrementCurrentIndex();
				fIncrementedRefCount = fTrue;
				}

			pfcbTable->LeaveDML();
			}

		}

	else if ( szIndex == NULL || *szIndex == '\0' )
		{
		fSettingToPrimaryIndex = fTrue;
		}

	else
		{
		BOOL	fPrimaryIndexTemplate	= fFalse;

		//	see if we're switching to the derived
		//	primary index (if any)
		if ( pfcbTable->FDerivedTable() )
			{
			Assert( pfcbTable->Ptdb() != ptdbNil );
			Assert( pfcbTable->Ptdb()->PfcbTemplateTable() != pfcbNil );
			const FCB * const	pfcbTemplateTable	= pfcbTable->Ptdb()->PfcbTemplateTable();

			if ( !pfcbTemplateTable->FSequentialIndex() )
				{
				// If template table has a primary index, we must have inherited it.

				fPrimaryIndexTemplate = fTrue;
				Assert( pfcbTemplateTable->Pidb() != pidbNil );
				Assert( pfcbTemplateTable->Pidb()->FTemplateIndex() );
				Assert( pfcbTable->Pidb() == pfcbTemplateTable->Pidb() );

				const TDB * const	ptdb			= pfcbTemplateTable->Ptdb();
				const IDB * const	pidb			= pfcbTemplateTable->Pidb();
				const CHAR * const	szPrimaryIdx	= ptdb->SzIndexName( pidb->ItagIndexName() );

				if ( 0 == UtilCmpName( szIndex, szPrimaryIdx ) )
					{			
					fSettingToPrimaryIndex = fTrue;
					}
				}
			else
				{
				Assert( pfcbTemplateTable->Pidb() == pidbNil );
				}
			}
			
		//	see if we're switching to the primary index
		if ( !fPrimaryIndexTemplate )
			{
			pfcbTable->EnterDML();

			if	( pfcbTable->Pidb() != pidbNil )
				{
				Assert( pfcbTable->Pidb()->ItagIndexName() != 0 );
				err = ErrFILEIAccessIndexByName( pfucb->ppib, pfcbTable, pfcbTable, szIndex );
				if ( err < 0 )
					{
					if ( JET_errIndexNotFound != err )
						{
						pfcbTable->LeaveDML();
						return err;
						}
					}
				else
					{
					fSettingToPrimaryIndex = fTrue;
					}
				}

			if ( !fSettingToPrimaryIndex
				&& pfucbNil != *ppfucbCurIdx
				&& !( (*ppfucbCurIdx)->u.pfcb->FDerivedIndex() ) )
				{
				//	don't let go of the DML latch because
				//	we're going to need it below to check
				//	if we're switching to the current
				fInDMLLatch = fTrue;
				}
			else
				{
				pfcbTable->LeaveDML();
				}
			}
		}

	if ( fSettingToPrimaryIndex )
		{
		Assert( !fInDMLLatch );

		if ( *ppfucbCurIdx != pfucbNil )
			{
			Assert( FFUCBIndex( *ppfucbCurIdx ) );
			Assert( FFUCBSecondary( *ppfucbCurIdx ) );
			Assert( (*ppfucbCurIdx)->u.pfcb != pfcbNil );
			Assert( (*ppfucbCurIdx)->u.pfcb->FTypeSecondaryIndex() );
			Assert( (*ppfucbCurIdx)->u.pfcb->Pidb() != pidbNil );
			Assert( (*ppfucbCurIdx)->u.pfcb->Pidb()->ItagIndexName() != 0 );
			Assert( (*ppfucbCurIdx)->u.pfcb->Pidb()->CrefCurrentIndex() > 0
				|| (*ppfucbCurIdx)->u.pfcb->Pidb()->FTemplateIndex() );

			//  move the sequential flag back to this index
			//  we are about to close this FUCB so we don't need to reset its flags
			if( FFUCBSequential( *ppfucbCurIdx ) )
				{
				FUCBSetSequential( pfucb );
				}
				
			//	really changing index, so close old one
			
			DIRClose( *ppfucbCurIdx );
			*ppfucbCurIdx = pfucbNil;
		
			//	changing to primary index.  Reset currency to beginning.
			ppfucbCurIdx = &pfucb;
			goto ResetCurrency;
			}
			
		//	UNDONE:	this case should honor grbit move expectations
		return JET_errSuccess;
		}

	
	Assert( NULL != szIndex );

	//	have a current secondary index
	//
	if ( *ppfucbCurIdx != pfucbNil )
		{
		const FCB * const	pfcbSecondaryIdx	= (*ppfucbCurIdx)->u.pfcb;
		
		Assert( FFUCBIndex( *ppfucbCurIdx ) );
		Assert( FFUCBSecondary( *ppfucbCurIdx ) );
		Assert( pfcbSecondaryIdx != pfcbNil );
		Assert( pfcbSecondaryIdx->FTypeSecondaryIndex() );
		Assert( pfcbSecondaryIdx->Pidb() != pidbNil );
		Assert( pfcbSecondaryIdx->Pidb()->ItagIndexName() != 0 );
		Assert( pfcbSecondaryIdx->Pidb()->CrefCurrentIndex() > 0
			|| pfcbSecondaryIdx->Pidb()->FTemplateIndex() );

		if ( NULL != pindexid )
			{
			//	already verified above that we're not
			//	switching to the current index
			Assert( !fInDMLLatch );
			Assert( pfcbSecondaryIdx != pindexid->pfcbIndex );
			}

		else if ( pfcbSecondaryIdx->FDerivedIndex() )
			{
			Assert( !fInDMLLatch );
			Assert( pfcbSecondaryIdx->Pidb()->FTemplateIndex() );
			Assert( pfcbTable->Ptdb() != ptdbNil );
			Assert( pfcbTable->Ptdb()->PfcbTemplateTable() != pfcbNil );
			Assert( pfcbTable->Ptdb()->PfcbTemplateTable()->Ptdb() != ptdbNil );
			const TDB * const	ptdb		= pfcbTable->Ptdb()->PfcbTemplateTable()->Ptdb();
			const IDB * const	pidb		= pfcbSecondaryIdx->Pidb();
			const CHAR * const	szCurrIdx	= ptdb->SzIndexName( pidb->ItagIndexName() );
			if ( 0 == UtilCmpName( szIndex, szCurrIdx ) )
				{
				//	changing to the current secondary index, so do nothing.
				return JET_errSuccess;
				}
			}
		else
			{
			// See if the desired index is the current one
			if ( !fInDMLLatch )
				pfcbTable->EnterDML();

			const TDB * const	ptdb		= pfcbTable->Ptdb();
			const FCB * const	pfcbIndex	= (*ppfucbCurIdx)->u.pfcb;
			const IDB * const	pidb		= pfcbIndex->Pidb();
			const CHAR * const	szCurrIdx	= ptdb->SzIndexName(
														pidb->ItagIndexName(),
														pfcbIndex->FDerivedIndex() );
			const INT			cmp			= UtilCmpName( szIndex, szCurrIdx );

			pfcbTable->LeaveDML();
			fInDMLLatch = fFalse;

			if ( 0 == cmp )
				{
				//	changing to the current secondary index, so do nothing.
				return JET_errSuccess;
				}
			}


		Assert( !fInDMLLatch );

		//	really changing index, so close old one
		//
		if ( FFUCBVersioned( *ppfucbCurIdx ) )
			{
			//	if versioned, must defer-close cursor
			DIRClose( *ppfucbCurIdx );
			}
		else
			{
			pfucbOldIndex = *ppfucbCurIdx;
			Assert( !PinstFromPfucb( pfucbOldIndex )->FRecovering() );
			Assert( !Pcsr( pfucbOldIndex )->FLatched() );
			Assert( !FFUCBDeferClosed( pfucbOldIndex ) );
			Assert( !FFUCBVersioned( pfucbOldIndex ) );
			Assert( !FFUCBDenyRead( pfucbOldIndex ) );
			Assert( !FFUCBDenyWrite( pfucbOldIndex ) );
			Assert( JET_LSNil == pfucbOldIndex->ls );
			Assert( pfcbNil != pfucbOldIndex->u.pfcb );
			Assert( pfucbOldIndex->u.pfcb->FTypeSecondaryIndex() );
			FILEReleaseCurrentSecondary( pfucbOldIndex );
			FCBUnlinkWithoutMoveToAvailList( pfucbOldIndex );
			KSReset( pfucbOldIndex );
			pfucbOldIndex->bmCurr.Nullify();
			}

		*ppfucbCurIdx = pfucbNil;
		fClosedPreviousIndex = fTrue;
		}

	//	set new current secondary index
	//
	Assert( pfucbNil == *ppfucbCurIdx );
	Assert( !fInDMLLatch );

	if ( NULL != pindexid )
		{
		//	IDB was already pinned above
		pfcbSecondary = pindexid->pfcbIndex;
		Assert( pfcbNil != pfcbSecondary );
		Assert( pfcbSecondary->Pidb()->CrefCurrentIndex() > 0
			|| pfcbSecondary->Pidb()->FTemplateIndex() );
		}
	else
		{
		//	verify visibility on desired index and pin it
		pfcbTable->EnterDML();
	
		for ( pfcbSecondary = pfcbTable->PfcbNextIndex();
			pfcbSecondary != pfcbNil;
			pfcbSecondary = pfcbSecondary->PfcbNextIndex() )
			{
			Assert( pfcbSecondary->Pidb() != pidbNil );
			Assert( pfcbSecondary->Pidb()->ItagIndexName() != 0 );

			err = ErrFILEIAccessIndexByName( pfucb->ppib, pfcbTable, pfcbSecondary, szIndex );
			if ( err < 0 )
				{
				if ( JET_errIndexNotFound != err )
					{
					pfcbTable->LeaveDML();
					goto HandleError;
					}
				}
			else
				{
				// Found the index we want.
				break;
				}
			}

		if ( pfcbNil == pfcbSecondary )
			{
			pfcbTable->LeaveDML();
			Call( ErrERRCheck( JET_errIndexNotFound ) );
			}

		else if ( pfcbSecondary->Pidb()->FTemplateIndex() )
			{
			// Don't need refcount on template indexes, since we
			// know they'll never go away.
			Assert( pfcbSecondary->Pidb()->CrefCurrentIndex() == 0 );
			}
		else
			{
			pfcbSecondary->Pidb()->IncrementCurrentIndex();
			fIncrementedRefCount = fTrue;
			}

		pfcbTable->LeaveDML();
		}

	Assert( !pfcbSecondary->FDomainDenyRead( pfucb->ppib ) );
	Assert( pfcbSecondary->FTypeSecondaryIndex() );

	Assert( ( fIncrementedRefCount
			&& pfcbSecondary->Pidb()->CrefCurrentIndex() > 0 )
		|| ( !fIncrementedRefCount
			&& pfcbSecondary->Pidb()->FTemplateIndex()
			&& pfcbSecondary->Pidb()->CrefCurrentIndex() == 0 ) );

	//	open an FUCB for the new index
	//
	Assert( pfucbNil == *ppfucbCurIdx );
	Assert( pfucb->ppib != ppibNil );
	Assert( pfucb->ifmp == pfcbSecondary->Ifmp() );
	Assert( !fInDMLLatch );

	if ( pfucbNil != pfucbOldIndex )
		{
		const BOOL	fUpdatable	= FFUCBUpdatable( pfucbOldIndex );

		FUCBResetFlags( pfucbOldIndex );
		FUCBResetPreread( pfucbOldIndex );

		if ( fUpdatable )
			{
			FUCBSetUpdatable( pfucbOldIndex );
			}

		//	initialize CSR in fucb
		//	this allocates page structure
		//
		new( Pcsr( pfucbOldIndex ) ) CSR;

		pfcbSecondary->Link( pfucbOldIndex );

		pfucbOldIndex->levelOpen = pfucbOldIndex->ppib->level;
		DIRInitOpenedCursor( pfucbOldIndex, pfucbOldIndex->ppib->level );

		*ppfucbCurIdx = pfucbOldIndex;
		pfucbOldIndex = pfucbNil;

		err = JET_errSuccess;
		}
	else
		{
		err = ErrDIROpen( pfucb->ppib, pfcbSecondary, ppfucbCurIdx );
		if ( err < 0 )
			{
			if ( fIncrementedRefCount )
				pfcbSecondary->Pidb()->DecrementCurrentIndex();
			goto HandleError;
			}
		}

	Assert( !fInDMLLatch );
	Assert( pfucbNil != *ppfucbCurIdx );
	FUCBSetIndex( *ppfucbCurIdx );
	FUCBSetSecondary( *ppfucbCurIdx );
	FUCBSetCurrentSecondary( *ppfucbCurIdx );

	//  move the sequential flag to this index
	//  we don't want the sequential flag set on the primary index any more
	//  because we don't want to preread while seeking on the bookmarks from
	//  the secondary index
	if( FFUCBSequential( pfucb ) )
		{
		FUCBResetSequential( pfucb );
		FUCBResetPreread( pfucb );
		FUCBSetSequential( *ppfucbCurIdx );
		}

	//	reset the index and file currency
	//
ResetCurrency:
	Assert( !fInDMLLatch );
	DIRBeforeFirst( *ppfucbCurIdx );
	if ( pfucb != *ppfucbCurIdx )
		{
		DIRBeforeFirst( pfucb );
		}

	AssertDIRNoLatch( pfucb->ppib );
	return JET_errSuccess;

HandleError:
	Assert( err < JET_errSuccess );
	Assert( !fInDMLLatch );

	if ( pfucbNil != pfucbOldIndex )
		{
		RECReleaseKeySearchBuffer( pfucbOldIndex );
		BTReleaseBM( pfucbOldIndex );
		FUCBClose( pfucbOldIndex );
		}

	Assert( pfucbNil == pfucb->pfucbCurIndex );
	if ( fClosedPreviousIndex )
		{
		DIRBeforeFirst( pfucb );
		}

	return err;
	}


LOCAL ERR ErrRECIIllegalNulls(
	FUCB			* pfucb,
	FCB				* pfcb,
	const DATA&		dataRec,
	BOOL			*pfIllegalNulls )
	{
	ERR				err					= JET_errSuccess;

	Assert( pfcbNil != pfcb );
	const TDB		* const ptdb		= pfcb->Ptdb();
	const BOOL		fTemplateTable		= ptdb->FTemplateTable();
	COLUMNID		columnid;

	Assert( pfIllegalNulls );
	Assert( !( *pfIllegalNulls ) );

	//	check fixed fields
	if ( ptdb->FidFixedLast() >= ptdb->FidFixedFirst() )
		{
		const COLUMNID	columnidFixedLast	= ColumnidOfFid( ptdb->FidFixedLast(), fTemplateTable );
		for ( columnid = ColumnidOfFid( ptdb->FidFixedFirst(), fTemplateTable );
			columnid <= columnidFixedLast;
			columnid++ )
			{
			FIELD	field;

			//	template columns are guaranteed to exist
			if ( fTemplateTable )
				{
				Assert( JET_coltypNil != ptdb->PfieldFixed( columnid )->coltyp );
				field.ffield = ptdb->PfieldFixed( columnid )->ffield;
				}
			else
				{
				Assert( !FCOLUMNIDTemplateColumn( columnid ) );
				err = ErrRECIAccessColumn( pfucb, columnid, &field );
				if ( err < 0 )
					{
					if ( JET_errColumnNotFound == err  )
						continue;
					else
						return err;
					}

				Assert( JET_coltypNil != field.coltyp );
				}

			if ( FFIELDNotNull( field.ffield ) )
				{
				const ERR	errCheckNull	= ErrRECIFixedColumnInRecord( columnid, pfcb, dataRec );
				BOOL		fNull;

				if ( JET_errColumnNotFound == errCheckNull )
					{
					// Column not in record -- it's null if there's no default value.
					fNull = !FFIELDDefault( field.ffield );
					}
				else
					{
					CallSx( errCheckNull, JET_wrnColumnNull );
					fNull = ( JET_wrnColumnNull == errCheckNull );
					}

				if ( fNull )
					{
					*pfIllegalNulls = fTrue;
					return JET_errSuccess;
					}
				}
			}
		}

	//	check var fields
	if ( ptdb->FidVarLast() >= ptdb->FidVarFirst() )
		{
		const COLUMNID	columnidVarLast		= ColumnidOfFid( ptdb->FidVarLast(), fTemplateTable );
		for ( columnid = ColumnidOfFid( ptdb->FidVarFirst(), fTemplateTable );
			columnid <= columnidVarLast;
			columnid++ )
			{
			//	template columns are guaranteed to exist
			if ( !fTemplateTable )
				{
				Assert( !FCOLUMNIDTemplateColumn( columnid ) );
				err = ErrRECIAccessColumn( pfucb, columnid );
				if ( err < 0 )
					{
					if ( JET_errColumnNotFound == err  )
						continue;
					else
						return err;
					}
				}

			if ( columnid > ptdb->FidVarLastInitial() )
				pfcb->EnterDML();

			Assert( JET_coltypNil != ptdb->PfieldVar( columnid )->coltyp );
			const FIELDFLAG		ffield	= ptdb->PfieldVar( columnid )->ffield;

			if ( columnid > ptdb->FidVarLastInitial() )
				pfcb->LeaveDML();

			if ( FFIELDNotNull( ffield ) )
				{
				const ERR	errCheckNull	= ErrRECIVarColumnInRecord( columnid, pfcb, dataRec );
				BOOL		fNull;

				if ( JET_errColumnNotFound == errCheckNull )
					{
					// Column not in record -- it's null if there's no default value.
					fNull = !FFIELDDefault( ffield );
					}
				else
					{
					CallSx( errCheckNull, JET_wrnColumnNull );
					fNull = ( JET_wrnColumnNull == errCheckNull );
					}
					
				if ( fNull )
					{
					*pfIllegalNulls = fTrue;
					return JET_errSuccess;
					}
				}
			}
		}

	*pfIllegalNulls = fFalse;

	CallSx( err, JET_errColumnNotFound );
	return JET_errSuccess;
	}


ERR ErrRECIIllegalNulls(
	FUCB		*pfucb,
	const DATA&	dataRec,
	BOOL		*pfIllegalNulls )
	{
	ERR			err;
	FCB			* pfcb = pfucb->u.pfcb;

	Assert( pfIllegalNulls );
	*pfIllegalNulls = fFalse;

	if ( pfcbNil != pfcb->Ptdb()->PfcbTemplateTable() )
		{
		pfcb->Ptdb()->AssertValidDerivedTable();
		CallR( ErrRECIIllegalNulls(
				pfucb,
				pfcb->Ptdb()->PfcbTemplateTable(),
				dataRec,
				pfIllegalNulls ) );
		}

	if ( !( *pfIllegalNulls ) )
		{
		CallR( ErrRECIIllegalNulls(
			pfucb,
			pfcb,
			dataRec,
			pfIllegalNulls ) );
		}

	return JET_errSuccess;
	}
	
ERR VTAPI ErrIsamGetCurrentIndex( JET_SESID sesid, JET_VTID vtid, CHAR *szCurIdx, ULONG cbMax )
	{
 	PIB		*ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucb = reinterpret_cast<FUCB *>( vtid );
	FCB		*pfcbTable;

	ERR		err = JET_errSuccess;
	CHAR	szIndex[JET_cbNameMost+1];

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	if ( cbMax < 1 )
		{
		return JET_wrnBufferTruncated;
		}

	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbNil != pfcbTable );
	Assert( ptdbNil != pfcbTable->Ptdb() );

	if ( pfucb->pfucbCurIndex != pfucbNil )
		{
		Assert( pfucb->pfucbCurIndex->u.pfcb != pfcbNil );
		Assert( pfucb->pfucbCurIndex->u.pfcb->FTypeSecondaryIndex() );
		Assert( !pfucb->pfucbCurIndex->u.pfcb->FDeletePending() );
		Assert( !pfucb->pfucbCurIndex->u.pfcb->FDeleteCommitted() );
		Assert( pfucb->pfucbCurIndex->u.pfcb->Pidb() != pidbNil );
		Assert( pfucb->pfucbCurIndex->u.pfcb->Pidb()->ItagIndexName() != 0 );
		
		const FCB	* const pfcbIndex = pfucb->pfucbCurIndex->u.pfcb;
		if ( pfcbIndex->FDerivedIndex() )
			{
			Assert( pfcbIndex->Pidb()->FTemplateIndex() );
			const FCB	* const pfcbTemplateTable = pfcbTable->Ptdb()->PfcbTemplateTable();
			Assert( pfcbNil != pfcbTemplateTable );
			strcpy(
				szIndex,
				pfcbTemplateTable->Ptdb()->SzIndexName( pfcbIndex->Pidb()->ItagIndexName() ) );
			}
		else
			{
			pfcbTable->EnterDML();
			strcpy(
				szIndex,
				pfcbTable->Ptdb()->SzIndexName(	pfcbIndex->Pidb()->ItagIndexName() ) );
			pfcbTable->LeaveDML();
			}
		}
	else
		{
		BOOL	fPrimaryIndexTemplate = fFalse;

		if ( pfcbTable->FDerivedTable() )
			{
			Assert( pfcbTable->Ptdb() != ptdbNil );
			Assert( pfcbTable->Ptdb()->PfcbTemplateTable() != pfcbNil );
			const FCB	* const pfcbTemplateTable = pfcbTable->Ptdb()->PfcbTemplateTable();
			if ( !pfcbTemplateTable->FSequentialIndex() )
				{
				// If template table has a primary index, we must have inherited it.
				fPrimaryIndexTemplate = fTrue;
				Assert( pfcbTemplateTable->Pidb() != pidbNil );
				Assert( pfcbTemplateTable->Pidb()->FTemplateIndex() );
				Assert( pfcbTable->Pidb() == pfcbTemplateTable->Pidb() );
				Assert( pfcbTemplateTable->Pidb()->ItagIndexName() != 0 );
				strcpy(
					szIndex,
					pfcbTemplateTable->Ptdb()->SzIndexName( pfcbTable->Pidb()->ItagIndexName() ) );
				}
			else
				{
				Assert( pfcbTemplateTable->Pidb() == pidbNil );
				}
			}

		if ( !fPrimaryIndexTemplate )
			{
			pfcbTable->EnterDML();
			if ( pfcbTable->Pidb() != pidbNil )
				{
				ERR	errT = ErrFILEIAccessIndex( ppib, pfcbTable, pfcbTable );
				if ( errT < 0 )
					{
					if ( JET_errIndexNotFound != errT )
						{
						pfcbTable->LeaveDML();
						return errT;
						}
					szIndex[0] = '\0';	// Primary index not visible - return sequential index
					}
				else
					{
					Assert( pfcbTable->Pidb()->ItagIndexName() != 0 );
					strcpy(
						szIndex,
						pfcbTable->Ptdb()->SzIndexName( pfcbTable->Pidb()->ItagIndexName() ) );
					}
				}
			else
				{
				szIndex[0] = '\0';
				}
			pfcbTable->LeaveDML();
			}
		}


	if ( cbMax > JET_cbNameMost + 1 )
		cbMax = JET_cbNameMost + 1;
	strncpy( szCurIdx, szIndex, (USHORT)cbMax - 1 );
	szCurIdx[ cbMax - 1 ] = '\0';
	CallS( err );
	AssertDIRNoLatch( ppib );
	return err;
	}


ERR VTAPI ErrIsamGetChecksum( JET_SESID sesid, JET_VTID vtid, ULONG *pulChecksum )
	{
 	PIB *ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB *pfucb = reinterpret_cast<FUCB *>( vtid );

	ERR   	err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
 	CheckFUCB( ppib, pfucb );
	Call( ErrDIRGet( pfucb ) );
	*pulChecksum = UlChecksum( pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );

	CallS( ErrDIRRelease( pfucb ) );

HandleError:
	AssertDIRNoLatch( ppib );
	return err;
	}


ULONG UlChecksum( VOID *pv, ULONG cb )
	{
	//	UNDONE:	find a way to compute check sum in longs independant
	//				of pb, byte offset in page

	/*	compute checksum by adding bytes in data record and shifting
	/*	result 1 bit to left after each operation.
	/**/
	BYTE   	*pbT = (BYTE *) pv;
	BYTE  	*pbMax = pbT + cb;
	ULONG  	ulChecksum = 0;

	/*	compute checksum
	/**/
	for ( ; pbT < pbMax; pbT++ )
		{
		ulChecksum += *pbT;
		ulChecksum <<= 1;
		}

	return ulChecksum;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\scrub.cxx ===
#include "std.hxx"

extern VOID NDIGetKeydataflags( const CPAGE& cpage, INT iline, KEYDATAFLAGS * pkdf );
LOCAL ERR ErrSCRUBGetObjidsFromCatalog(
		PIB * const ppib,
		const IFMP ifmp,
		OBJIDINFO ** ppobjidinfo,
		LONG * pcobjidinfo );


//  ================================================================
BOOL OBJIDINFO::CmpObjid( const OBJIDINFO& left, const OBJIDINFO& right )
//  ================================================================
	{
	return left.objidFDP < right.objidFDP;
	}


//  ================================================================
ERR ErrDBUTLScrub( JET_SESID sesid, const JET_DBUTIL *pdbutil )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	PIB * const ppib = reinterpret_cast<PIB *>( sesid );
	SCRUBDB * pscrubdb = NULL;

	LOGTIME logtimeScrub;

	const CPG cpgPreread = 256;
	
	Call( ErrIsamAttachDatabase( sesid, pdbutil->szDatabase, pdbutil->szSLV, pdbutil->szSLV, 0, NO_GRBIT ) );

	IFMP	ifmp;	
	Call( ErrIsamOpenDatabase(
		sesid,
		pdbutil->szDatabase,
		NULL,
		reinterpret_cast<JET_DBID *>( &ifmp ),
		NO_GRBIT
		) );

	PGNO pgnoLast;
	pgnoLast = rgfmp[ifmp].PgnoLast();

	DBTIME dbtimeLastScrubNew;
	dbtimeLastScrubNew = rgfmp[ifmp].DbtimeLast();
	
	pscrubdb = new SCRUBDB( ifmp );
	if( NULL == pscrubdb )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	Call( pscrubdb->ErrInit( ppib, CUtilProcessProcessor() ) );

	ULONG cpgBuffers;
	cpgBuffers = (ULONG)max( 512, ( ( OSMemoryAvailable() - ( 16 * 1024 * 1024 ) ) / ( g_cbPage + 128 ) ) );
	if( JET_errSuccess == ErrBFSetCacheSize( cpgBuffers ) )
		{
		(*CPRINTFSTDOUT::PcprintfInstance())( "got %d buffers\r\n", cpgBuffers );
		CallS( ErrBFSetCacheSize( 0 ) );
		}
	else
		{
		(*CPRINTFSTDOUT::PcprintfInstance())( "failed to get %d buffers\r\n", cpgBuffers );
		cpgBuffers = 1024;
		}

	JET_SNPROG snprog;
	memset( &snprog, 0, sizeof( snprog ) );	
	snprog.cunitTotal 	= pgnoLast;
	snprog.cunitDone 	= 0;

	JET_PFNSTATUS pfnStatus;
	pfnStatus = reinterpret_cast<JET_PFNSTATUS>( pdbutil->pfnCallback );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpScrub, JET_sntBegin, NULL );	
		}

	PGNO pgno;
	pgno = 1;

	while( pgnoLast	>= pgno )
		{
		const CPG cpgChunk = 256;
		const CPG cpgPreread = min( cpgChunk, pgnoLast - pgno + 1 );
		BFPrereadPageRange( ifmp, pgno, cpgPreread );

		Call( pscrubdb->ErrScrubPages( pgno, cpgPreread ) );
		pgno += cpgPreread;

		snprog.cunitDone = pscrubdb->Scrubstats().cpgSeen;
		if ( NULL != pfnStatus )
			{
			(VOID)pfnStatus( sesid, JET_snpScrub, JET_sntProgress, &snprog );
			}

		while( ( pscrubdb->Scrubstats().cpgSeen + ( cpgBuffers / 4 ) ) < pgno )
			{
			snprog.cunitDone = pscrubdb->Scrubstats().cpgSeen;
			if ( NULL != pfnStatus )
				{
				(VOID)pfnStatus( sesid, JET_snpScrub, JET_sntProgress, &snprog );
				}
			UtilSleep( cmsecWaitGeneric );
			}
		}

	snprog.cunitDone = pscrubdb->Scrubstats().cpgSeen;
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpScrub, JET_sntProgress, &snprog );
		}

	Call( pscrubdb->ErrTerm() );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );
		}

	rgfmp[ifmp].SetDbtimeLastScrub( dbtimeLastScrubNew );
	LGIGetDateTime( &logtimeScrub );	
	rgfmp[ifmp].SetLogtimeScrub( logtimeScrub );
	
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages seen\n", pscrubdb->Scrubstats().cpgSeen );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d blank pages seen\n", pscrubdb->Scrubstats().cpgUnused );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d unchanged pages seen\n", pscrubdb->Scrubstats().cpgUnchanged );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d unused pages zeroed\n", pscrubdb->Scrubstats().cpgZeroed );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d used pages seen\n", pscrubdb->Scrubstats().cpgUsed );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages with unknown objid\n", pscrubdb->Scrubstats().cpgUnknownObjid );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d nodes seen\n", pscrubdb->Scrubstats().cNodes );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d flag-deleted nodes zeroed\n", pscrubdb->Scrubstats().cFlagDeletedNodesZeroed );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d flag-deleted nodes not zeroed\n", pscrubdb->Scrubstats().cFlagDeletedNodesNotZeroed );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d version bits reset seen\n", pscrubdb->Scrubstats().cVersionBitsReset );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d orphaned LVs\n", pscrubdb->Scrubstats().cOrphanedLV );

	err = pscrubdb->Scrubstats().err;
	
	delete pscrubdb;
	pscrubdb = NULL;

	Call( err );

	if ( rgfmp[ifmp].FSLVAttached() )
	{
		CPG cpgSeen;
		CPG	cpgScrubbed;
		Call( ErrSCRUBScrubStreamingFile( ppib, ifmp, &cpgSeen, &cpgScrubbed, pfnStatus ) );

		(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages seen\n", cpgSeen );
		(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages scrubbed\n", cpgScrubbed );	
	}
	
HandleError:
	if( NULL != pscrubdb )
		{
		const ERR errT = pscrubdb->ErrTerm();
		if( err >= 0 && errT < 0 )
			{
			err = errT;
			}
		delete pscrubdb;
		}
	return err;
	}


//  ================================================================
SCRUBDB::SCRUBDB( const IFMP ifmp ) :
	m_ifmp( ifmp )
//  ================================================================
	{
	m_constants.dbtimeLastScrub = 0;
	m_constants.pcprintfVerbose = CPRINTFSTDOUT::PcprintfInstance();
	m_constants.pcprintfDebug	= CPRINTFSTDOUT::PcprintfInstance();
	m_constants.objidMax		= 0;
	m_constants.pobjidinfo		= 0;
	m_constants.cobjidinfo		= 0;

	m_stats.err					= JET_errSuccess;
	m_stats.cpgSeen 			= 0;
	m_stats.cpgUnused 			= 0;
	m_stats.cpgUnchanged 		= 0;
	m_stats.cpgZeroed 			= 0;
	m_stats.cpgUsed 			= 0;
	m_stats.cpgUnknownObjid		= 0;
	m_stats.cNodes 				= 0;
	m_stats.cFlagDeletedNodesZeroed 	= 0;
	m_stats.cFlagDeletedNodesNotZeroed 	= 0;	
	m_stats.cOrphanedLV 		= 0;
	m_stats.cVersionBitsReset 	= 0;

	m_context.pconstants 	= &m_constants;
	m_context.pstats		= &m_stats;
	}


//  ================================================================
SCRUBDB::~SCRUBDB()
//  ================================================================
	{
	delete [] m_constants.pobjidinfo;
	}


//  ================================================================
volatile const SCRUBSTATS& SCRUBDB::Scrubstats() const
//  ================================================================
	{
	return m_stats;
	}


//  ================================================================
ERR SCRUBDB::ErrInit( PIB * const ppib, const INT cThreads )
//  ================================================================
	{
	ERR err;
	
	INST * const pinst = PinstFromIfmp( m_ifmp );

#ifdef DEBUG
	INT cLoop = 0;
#endif	//	DEBUG

	Call( m_taskmgr.ErrInit( pinst, cThreads ) );

	CallS( ErrDIRBeginTransaction( ppib, JET_bitTransactionReadOnly ) );
	//  get an objidMax
	//  wait until this is the oldest transaction in the system
	//  at which point all objids < objidMax have been committed to
	//  the catalog. then commit the transaction and read the catalog
	m_constants.objidMax = rgfmp[m_ifmp].ObjidLast();
	while( TrxOldest( pinst ) != ppib->trxBegin0 )
		{
		UtilSleep( cmsecWaitGeneric );
		AssertSz( ++cLoop < 36000, "Waiting a long time for all transactions to commit (Deadlocked?)" );
		}
	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
	m_constants.dbtimeLastScrub = rgfmp[m_ifmp].DbtimeLastScrub();

	Call( ErrSCRUBGetObjidsFromCatalog(
			ppib,
			m_ifmp,
			&m_constants.pobjidinfo,
			&m_constants.cobjidinfo ) );

	return err;
	
HandleError:
	CallS( ErrTerm() );
	return err;
	}


//  ================================================================
ERR SCRUBDB::ErrTerm()
//  ================================================================
	{
	ERR err;
	
	Call( m_taskmgr.ErrTerm() );

	delete [] m_constants.pobjidinfo;
	m_constants.pobjidinfo = NULL;

	err = m_stats.err;
	
HandleError:
	return err;
	}


//  ================================================================
ERR SCRUBDB::ErrScrubPages( const PGNO pgnoFirst, const CPG cpg )
//  ================================================================
	{
	SCRUBTASK * ptask = new SCRUBTASK( m_ifmp, pgnoFirst, cpg, &m_context );
	if( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	const ERR err = m_taskmgr.ErrPostTask( TASK::Dispatch, (ULONG_PTR)ptask );
	if( err < JET_errSuccess )
		{
		//  The task was not enqued sucessfully.
		delete ptask;
		}
	return err;
	}


//  ================================================================
SCRUBTASK::SCRUBTASK( const IFMP ifmp, const PGNO pgnoFirst, const CPG cpg, const SCRUBCONTEXT * const pcontext ) :
	DBTASK( ifmp ),
	m_pgnoFirst( pgnoFirst ),
	m_cpg( cpg ),
	m_pcontext( pcontext ),
	m_pobjidinfoCached( NULL ),
	m_objidNotFoundCached( objidNil )
//  ================================================================
	{
	}

	
//  ================================================================
SCRUBTASK::~SCRUBTASK()
//  ================================================================
	{
	}

		
//  ================================================================
ERR SCRUBTASK::ErrExecute( PIB * const ppib )
//  ================================================================
	{
	ERR	err	= JET_errSuccess;

	for ( PGNO pgno = m_pgnoFirst; pgno < m_pgnoFirst + m_cpg; ++pgno )
		{
		CSR	csr;
		err = csr.ErrGetRIWPage( ppib, m_ifmp, pgno );
		if ( JET_errPageNotInitialized == err )
			{
			//  this page is already zeroed out
			AtomicIncrement( &( m_pcontext->pstats->cpgUnused ) );
			err = JET_errSuccess;
			}
		else if ( err >= 0 )
			{
			err = ErrProcessPage_( ppib, &csr );
			}

		AtomicIncrement( &( m_pcontext->pstats->cpgSeen ) );
		csr.ReleasePage( fTrue );
		Call( err );
		}

HandleError:
	if ( err < 0 )
		{
		AtomicCompareExchange( &m_pcontext->pstats->err, JET_errSuccess, err );
		}
	return err;
	}


//  ================================================================
ERR SCRUBTASK::ErrProcessPage_(
		PIB * const ppib,
		CSR * const pcsr )
//  ================================================================
	{
	const OBJID objid = pcsr->Cpage().ObjidFDP();

	// do not scrub space tree pages
	if ( pcsr->Cpage().FSpaceTree() )
		{
		return JET_errSuccess;
		}

	if( objid > m_pcontext->pconstants->objidMax )
		{
		AtomicIncrement( &( m_pcontext->pstats->cpgUnknownObjid ) );
		return JET_errSuccess;
		}
		
	if( objid == m_objidNotFoundCached )
		{
		return ErrProcessUnusedPage_( ppib, pcsr );
		}
	else if( m_pobjidinfoCached && objid == m_pobjidinfoCached->objidFDP )
		{
		}
	else if( objidSystemRoot == objid )
		{
		//  this is not in the catalog
		return JET_errSuccess;
		}
	else
		{
		OBJIDINFO objidinfoSearch;
		objidinfoSearch.objidFDP = objid;
		
		OBJIDINFO * pobjidinfoT = m_pcontext->pconstants->pobjidinfo;
		pobjidinfoT = lower_bound(
						pobjidinfoT, 
						m_pcontext->pconstants->pobjidinfo + m_pcontext->pconstants->cobjidinfo,
						objidinfoSearch,
						OBJIDINFO::CmpObjid );
											
		if( pobjidinfoT == m_pcontext->pconstants->pobjidinfo + m_pcontext->pconstants->cobjidinfo
			|| pobjidinfoT->objidFDP != objid )
			{
			m_objidNotFoundCached = objid;
			return ErrProcessUnusedPage_( ppib, pcsr );
			}
		else
			{
			m_pobjidinfoCached = pobjidinfoT;
			}
		}

	if( pcsr->Cpage().Dbtime() < m_pcontext->pconstants->dbtimeLastScrub )
		{
		//  this page should not have been empty the last time we saw it
		//  if so we would have zeroed it out and never should have ended up here
		AtomicIncrement( &( m_pcontext->pstats->cpgUnchanged ) );
		return JET_errSuccess;
		}

	if( pcsr->Cpage().FEmptyPage() )
		{
		return ErrProcessUnusedPage_( ppib, pcsr );
		}
		
	return ErrProcessUsedPage_( ppib, pcsr );
	}


//  ================================================================
ERR SCRUBTASK::ErrProcessUnusedPage_(
		PIB * const ppib,
		CSR * const pcsr )
//  ================================================================
//
//  Can't call CPAGE::ResetHeader because of concurrency issues
//  we don't want to lose our latch on the page, let someone else
//  use the page and then zero it out
//
//-
	{
	pcsr->UpgradeFromRIWLatch();

	//  Don't call CSR::Dirty() or CPAGE::Dirty() because we don't
	//  want the dbtime on the page changed
	BFDirty( pcsr->Cpage().PBFLatch() );	

	//  Delete all the data on the page
	const INT clines = pcsr->Cpage().Clines();
	INT iline;
	for( iline = 0; iline < clines; ++iline )
		{
		pcsr->Cpage().Delete( 0 );
		}
		
	//  reorganize the page and zero all unused data
	pcsr->Cpage().ReorganizeAndZero( 'u' );
	
	pcsr->Downgrade( latchReadNoTouch );

	AtomicIncrement( &( m_pcontext->pstats->cpgZeroed ) );

	return JET_errSuccess;
	}


//  ================================================================
ERR SCRUBTASK::FNodeHasVersions_( const OBJID objidFDP, const KEYDATAFLAGS& kdf, const TRX trxOldest ) const
//  ================================================================
	{
	Assert( m_pobjidinfoCached->objidFDP == objidFDP );

	BOOKMARK bm;
	bm.key = kdf.key;
	if( m_pobjidinfoCached->fUnique )
		{
		bm.data.Nullify();
		}
	else
		{
		bm.data = kdf.data;
		}

	const BOOL fActiveVersion = FVERActive( m_ifmp, m_pobjidinfoCached->pgnoFDP, bm, trxOldest );
	return fActiveVersion;
	}


//  ================================================================
ERR SCRUBTASK::ErrProcessUsedPage_(
		PIB * const ppib,
		CSR * const pcsr )
//  ================================================================
//
//  for each node on the page
//    if it is flag-deleted zero out the data
//    check for an orphaned LV
//
//-
	{
	Assert( m_pobjidinfoCached->objidFDP == pcsr->Cpage().ObjidFDP() );
	
	ERR err = JET_errSuccess;
	
	AtomicIncrement( &( m_pcontext->pstats->cpgUsed ) );

	const TRX trxOldest = TrxOldest( PinstFromPpib( ppib ) );
	
	pcsr->UpgradeFromRIWLatch();

	//  Don't call CSR::Dirty() or CPAGE::Dirty() because we don't
	//  want the dbtime on the page changed
	BFDirty( pcsr->Cpage().PBFLatch() );	

	if( pcsr->Cpage().FLeafPage() )
		{
		LONG cVersionBitsReset 			= 0;
		LONG cFlagDeletedNodesZeroed 	= 0;
		LONG cFlagDeletedNodesNotZeroed = 0;
		
		KEYDATAFLAGS kdf;
		INT iline;
		for( iline = 0; iline < pcsr->Cpage().Clines(); ++iline )
			{
			NDIGetKeydataflags( pcsr->Cpage(), iline, &kdf );
			
			if( FNDDeleted( kdf ) )
				{
				if( !FNDVersion( kdf )
					|| !FNodeHasVersions_( pcsr->Cpage().ObjidFDP(), kdf, trxOldest ) )
					{
					//  don't zero out the data for non-unique indexes
					//  the data is part of the bookmark and can't be removed or the sort order
					//  will be wrong
					if( m_pobjidinfoCached->fUnique )
						{
						++cFlagDeletedNodesZeroed;
						//  the kdf is pointing directly into the page
						memset( kdf.data.Pv(), 'd', kdf.data.Cb() );
						}

					if( FNDVersion( kdf ) )
						{
						++cVersionBitsReset;
						pcsr->Cpage().ReplaceFlags( iline, kdf.fFlags & ~fNDVersion );
						}
					}
				else
					{
					++cFlagDeletedNodesNotZeroed;
					}
				}
			else if( pcsr->Cpage().FLongValuePage() 
					&& !pcsr->Cpage().FSpaceTree() )
				{
				if( sizeof( LID ) == kdf.key.Cb() )
					{
					if( sizeof( LVROOT ) != kdf.data.Cb() )
						{
						AssertSz( fFalse, "Corrupted LV: corrupted LVROOT" );
						Call( ErrERRCheck( JET_errLVCorrupted ) );
						}
					const LVROOT * const plvroot = reinterpret_cast<LVROOT *>( kdf.data.Pv() );
					if( 0 == plvroot->ulReference 
						&& !FNodeHasVersions_( pcsr->Cpage().ObjidFDP(), kdf, trxOldest ) )
						{
						AtomicIncrement( &( m_pcontext->pstats->cOrphanedLV ) );
						Call( ErrSCRUBZeroLV( ppib, m_ifmp, pcsr, iline ) );
						}
					}
				}
			}

		//  report stats for all nodes at one time to reduce contention
		AtomicExchangeAdd( &( m_pcontext->pstats->cNodes ), pcsr->Cpage().Clines() );
		AtomicExchangeAdd( &( m_pcontext->pstats->cVersionBitsReset ), cVersionBitsReset );
		AtomicExchangeAdd( &( m_pcontext->pstats->cFlagDeletedNodesZeroed ), cFlagDeletedNodesZeroed );
		AtomicExchangeAdd( &( m_pcontext->pstats->cFlagDeletedNodesNotZeroed ), cFlagDeletedNodesNotZeroed );
		}
		
	//  reorganize the page and zero all unused data
	pcsr->Cpage().ReorganizeAndZero( 'z' );
	
HandleError:
	pcsr->Downgrade( latchReadNoTouch );
	CallS( err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrSCRUBGetObjidsFromCatalog(
		PIB * const ppib,
		const IFMP ifmp,
		OBJIDINFO ** ppobjidinfo,
		LONG * pcobjidinfo )
//  ================================================================
	{
	FUCB * pfucbCatalog = pfucbNil;
	ERR err;
	DATA data;

	*ppobjidinfo	= NULL;
	*pcobjidinfo	= 0;

	const LONG		cobjidinfoChunk 	= 64;
	LONG			iobjidinfoNext 		= 0;
	LONG			cobjidinfoAllocated = cobjidinfoChunk;
	OBJIDINFO	* 	pobjidinfo			= new OBJIDINFO[cobjidinfoChunk];

	if( NULL == pobjidinfo )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
			
	SYSOBJ	sysobj;

	OBJID	objidFDP;
	PGNO	pgnoFDP;
	BOOL	fUnique;
	BOOL	fPrimaryIndex;

	Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog ) );
	FUCBSetSequential( pfucbCatalog );
	FUCBSetPrereadForward( pfucbCatalog, cpgPrereadSequential );

	err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveFirst, 0 );
	while ( err >= 0 )
		{
		BOOL fBtree = fFalse;
		
		Call( ErrDIRGet( pfucbCatalog ) );

		Assert( FFixedFid( fidMSO_Type ) );
		Call( ErrRECIRetrieveFixedColumn(
					pfcbNil,
					pfucbCatalog->u.pfcb->Ptdb(),
					fidMSO_Type,
					pfucbCatalog->kdfCurr.data,
					&data ) );
		CallS( err );

		sysobj = (SYSOBJ)(*( reinterpret_cast<UnalignedLittleEndian< SYSOBJ > *>( data.Pv() ) ) );
		switch( sysobj )
			{
			case sysobjNil:
			case sysobjColumn:
			case sysobjCallback:
				//  These are not B-trees
				break;
				
			case sysobjTable:
			case sysobjIndex:
			case sysobjLongValue:
			case sysobjSLVAvail:
			case sysobjSLVOwnerMap:
				//  These are B-trees
				//  Obtain their objids and pgnoFDPs
				fBtree = fTrue;
				
				Assert( FFixedFid( fidMSO_Id ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_Id,
							pfucbCatalog->kdfCurr.data,
							&data ) );
				CallS( err );

				objidFDP = (OBJID)(*( reinterpret_cast<UnalignedLittleEndian< OBJID > *>( data.Pv() ) ));

				Assert( FFixedFid( fidMSO_PgnoFDP ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_PgnoFDP,
							pfucbCatalog->kdfCurr.data,
							&data ) );
				CallS( err );

				pgnoFDP = (PGNO)(*( reinterpret_cast<UnalignedLittleEndian< PGNO > *>( data.Pv() ) ));

				break;

			default:
				Assert( fFalse );
				break;
			}			

		//  we need a list of unique objids for version store lookups
		fUnique			= fTrue;
		fPrimaryIndex	= fFalse;
		if( sysobjIndex == sysobj )
			{
			IDBFLAG idbflag;
			Assert( FFixedFid( fidMSO_Flags ) );
			Call( ErrRECIRetrieveFixedColumn(
						pfcbNil,
						pfucbCatalog->u.pfcb->Ptdb(),
						fidMSO_Flags,
						pfucbCatalog->kdfCurr.data,
						&data ) );
			CallS( err );
			Assert( data.Cb() == sizeof(ULONG) );
			UtilMemCpy( &idbflag, data.Pv(), sizeof(IDBFLAG) );

			fUnique = FIDBUnique( idbflag );
			fPrimaryIndex = FIDBPrimary( idbflag );
			}

		Call( ErrDIRRelease( pfucbCatalog ) );

		//  the primary index is already recorded as the B-Tree
		if( fBtree && !fPrimaryIndex )
			{
			if( iobjidinfoNext >= cobjidinfoAllocated )
				{
				const LONG cobjidinfoAllocatedOld = cobjidinfoAllocated;
				const LONG cobjidinfoAllocatedNew = cobjidinfoAllocated + cobjidinfoChunk;
				OBJIDINFO * const pobjidinfoOld = pobjidinfo;
				OBJIDINFO * const pobjidinfoNew = new OBJIDINFO[cobjidinfoAllocatedNew];
				
				if( NULL == pobjidinfoNew )
					{
					Call( ErrERRCheck( JET_errOutOfMemory ) );
					}
					
				memcpy( pobjidinfoNew, pobjidinfoOld, sizeof( OBJIDINFO ) * cobjidinfoAllocatedOld );

				pobjidinfo 		= pobjidinfoNew;
				cobjidinfoAllocated = cobjidinfoAllocatedNew;

				delete [] pobjidinfoOld;
				}
			pobjidinfo[iobjidinfoNext].objidFDP = objidFDP;
			pobjidinfo[iobjidinfoNext].pgnoFDP 	= pgnoFDP;
			pobjidinfo[iobjidinfoNext].fUnique 	= fUnique;

			++iobjidinfoNext;
			}
			
		err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveNext, 0 );
		}
		
	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:
	if( pfucbNil != pfucbCatalog )
		{
		DIRUp( pfucbCatalog );
		CallS( ErrCATClose( ppib, pfucbCatalog ) );
		}

	if( err >= 0 )
		{
		sort( pobjidinfo, pobjidinfo + iobjidinfoNext, OBJIDINFO::CmpObjid );
		*ppobjidinfo	= pobjidinfo;
		*pcobjidinfo	= iobjidinfoNext;
		}
	else
		{
		delete [] pobjidinfo;
		*ppobjidinfo	= NULL;
		*pcobjidinfo		= 0;
		}
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubOneSLVPageUsingData( 
		PIB * const ppib,
		const IFMP ifmpDb,
		const PGNO pgno,
		const VOID * const pvPageScrubbed )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	BFLatch bfl;
	Call( ErrBFWriteLatchPage( &bfl, ifmpDb | ifmpSLV, pgno, bflfNew ) );

	memcpy( bfl.pv, pvPageScrubbed, g_cbPage );

	BFWriteUnlatch( &bfl );
	
HandleError:
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubOneSLVPageUsingFiles( 
		PIB * const ppib,
		const IFMP ifmpDb,
		const PGNO pgno,
		const VOID * const pvPageScrubbed )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const QWORD ibOffset = OffsetOfPgno( pgno );

	//  write the page syncronously
	//	CONSIDER:  issue an async write
	
	Call( rgfmp[ifmpDb].PfapiSLV()->ErrIOWrite(	ibOffset,
												g_cbPage,
												(BYTE *)pvPageScrubbed ) );
	
HandleError:
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubOneSLVPage( 
		PIB * const ppib,
		const IFMP ifmpDb,
		const PGNO pgno,
		const VOID * const pvPageScrubbed )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if ( PinstFromPpib( ppib )->FSLVProviderEnabled() )
		{
		//  zero the SLV page using the SLV Provider

		Call( ErrSCRUBIScrubOneSLVPageUsingFiles( ppib, ifmpDb, pgno, pvPageScrubbed ) );
		}
	else
		{
		//  zero the SLV page using data

		Call( ErrSCRUBIScrubOneSLVPageUsingData( ppib, ifmpDb, pgno, pvPageScrubbed ) );
		}
	
HandleError:
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubSLVPages( 
		PIB * const ppib,
		const IFMP ifmpDb,
		const PGNO pgnoStart,
		const SLVSPACENODE * const pspacenode,
		CPG * const pcpgSeen,
		CPG * const pcpgScrubbed,
		const VOID * const pvPageScrubbed,
		JET_PFNSTATUS pfnStatus,
		JET_SNPROG * const psnprog )
//  ================================================================
//
//	For each free page in the SLVSPACENODE
//	write a blank page into the database
//
//-
	{
	ERR err = JET_errSuccess;

	Assert ( pcpgSeen );
	Assert ( pcpgScrubbed );
	*pcpgSeen = CPG(0);
	*pcpgScrubbed = CPG(0);

	INT ipage;
	for( ipage = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		SLVSPACENODE::STATE state = pspacenode->GetState( ipage );
		(*pcpgSeen)++;
		if( SLVSPACENODE::sFree == state )
			{
			Call( ErrSCRUBIScrubOneSLVPage( ppib, ifmpDb, pgnoStart + ipage, pvPageScrubbed ) );
			(*pcpgScrubbed)++;
			}
		++(psnprog->cunitDone);
		if ( ( 0 == ( psnprog->cunitDone % 128 ) )
			&& NULL != pfnStatus )
			{
			(VOID)pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntProgress, psnprog );
			}			
		}
	
HandleError:
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubOneSlvspacenode( 
		PIB * const ppib,
		const IFMP ifmpDb,
		const FUCB * const pfucbSLVAvail,
		CPG * const pcpgSeen,
		CPG * const pcpgScrubbed,
		const VOID * const pvPageScrubbed,
		JET_PFNSTATUS pfnStatus,
		JET_SNPROG * const psnprog )		
//  ================================================================
//
//	CONSIDER:	make a copy of the SLVSPACENODE
//				transition all free pages to Deleted
//				update the SLVSPACENODECACHE
//				unlatch the page
//				scrub the copy of the SLVSPACENODe
//				copy the original SLVSPACENODE back
//
//-
	{
	ERR err = JET_errSuccess;

	PGNO pgno;
	LongFromKey( &pgno, pfucbSLVAvail->kdfCurr.key );
	pgno -= ( SLVSPACENODE::cpageMap - 1 );	//	first page is 1

	const SLVSPACENODE * const pspacenode = (SLVSPACENODE *)pfucbSLVAvail->kdfCurr.data.Pv();

	Call( ErrSCRUBIScrubSLVPages( 
			ppib,
			ifmpDb,
			pgno,
			pspacenode,
			pcpgSeen,
			pcpgScrubbed,
			pvPageScrubbed,
			pfnStatus,
			psnprog ) );

HandleError:
	return err;
	}


//  ================================================================
ERR ErrSCRUBIScrubStreamingFile( 
		PIB * const ppib,
		const IFMP ifmpDb,
		FUCB * const pfucbSLVAvail,
		CPG * const pcpgSeen,
		CPG * const pcpgScrubbed,
		const VOID * const pvPageScrubbed,
		JET_PFNSTATUS pfnStatus,
		JET_SNPROG * const psnprog )		
//  ================================================================
	{
	ERR err = JET_errSuccess;

	DIB dib;
	dib.pos = posFirst;
	dib.pbm = NULL;
	dib.dirflag = fDIRNull;

	Call( ErrBTDown( pfucbSLVAvail, &dib, latchReadNoTouch ) );

	Assert ( pcpgSeen );
	Assert ( pcpgScrubbed );
	*pcpgSeen = 0;
	*pcpgScrubbed = 0;

	do
		{
		CPG cpgSeenStep;
		CPG cpgScrubbedStep;
		
		Call( ErrSCRUBIScrubOneSlvspacenode( 
				ppib,
				ifmpDb,
				pfucbSLVAvail,
				&cpgSeenStep,
				&cpgScrubbedStep,
				pvPageScrubbed,
				pfnStatus,
				psnprog ) );

		*pcpgSeen += cpgSeenStep;
		*pcpgScrubbed += cpgScrubbedStep;
				
		} while( ( err = ErrBTNext( pfucbSLVAvail, fDIRNull ) ) == JET_errSuccess );

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
		
HandleError:
	Assert ( *pcpgSeen >= *pcpgScrubbed );

	return err;
	}


//  ================================================================
ERR ErrSCRUBScrubStreamingFile( 
		PIB * const ppib,
		const IFMP ifmpDb,
		CPG * const pcpgSeen,
		CPG * const pcpgScrubbed,
		JET_PFNSTATUS pfnStatus )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbSLVAvail = pfucbNil;

	// initialize out params
	Assert ( pcpgSeen );
	Assert ( pcpgScrubbed );
	*pcpgSeen = CPG(0);
	*pcpgScrubbed = CPG(0);
	
	VOID * const pvPageScrubbed = PvOSMemoryPageAlloc( g_cbPage, NULL );
	if( NULL == pvPageScrubbed )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	memset( pvPageScrubbed, '?', g_cbPage );

	//	init the status bar

	JET_SNPROG snprog;
	memset( &snprog, 0, sizeof( snprog ) );	
	snprog.cunitTotal 	= rgfmp[ifmpDb].PgnoSLVLast();
	snprog.cunitDone 	= 0;

	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );	
		}

	//	open the SLVAvail tree
	Assert ( rgfmp[ifmpDb].FSLVAttached() );

	Call( ErrBTOpen( ppib, rgfmp[ifmpDb].PfcbSLVAvail(), &pfucbSLVAvail, fFalse ) );

	Call( ErrSCRUBIScrubStreamingFile( 
			ppib,
			ifmpDb,
			pfucbSLVAvail,
			pcpgSeen,
			pcpgScrubbed,
			pvPageScrubbed,
			pfnStatus,
			&snprog ) );

HandleError:
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntComplete, NULL );	
		}

	if( pfucbNil != pfucbSLVAvail )
		{
		BTClose( pfucbSLVAvail );
		}

	OSMemoryPageFree( pvPageScrubbed );
	return err;
	}
	

//  ================================================================
SCRUBSLVTASK::SCRUBSLVTASK( SCRUBSLVCONTEXT * const pcontext ) :
//  ================================================================
	m_pcontext( pcontext ),
	DBTASK( pcontext->ifmp ),
	m_fUnattended ( fFalse )
	{
	}

//  ================================================================
SCRUBSLVTASK::SCRUBSLVTASK( const IFMP ifmp ) :
//  ================================================================
	m_pcontext( NULL ),
	DBTASK( ifmp ),
	m_fUnattended ( fTrue )
	{
	}	

//  ================================================================
SCRUBSLVTASK::~SCRUBSLVTASK()
//  ================================================================
	{
	}

		
//  ================================================================
ERR SCRUBSLVTASK::ErrExecute( PIB * const ppib )
//  ================================================================
	{
	ERR 	 err 				= JET_errSuccess;
	ULONG_PTR ulSecsStartScrub 	= 0;
	CPG		 cpgSeen 			= 0;
	CPG		 cpgScrubbed 		= 0;

	
	Assert ( m_fUnattended ^ (ULONG_PTR) m_pcontext );
	
	if ( m_fUnattended )
		{
		// EventLog start
		const CHAR * rgszT[1];
		INT isz = 0;
		
		rgszT[isz++] = rgfmp[m_ifmp].SzSLVName();
		Assert( isz <= sizeof( rgszT ) / sizeof( rgszT[0] ) );
		
		UtilReportEvent( eventInformation, DATABASE_ZEROING_CATEGORY, DATABASE_SLV_ZEROING_STARTED_ID, isz, rgszT );				
		}
	else
		{
		Assert ( m_ifmp == m_pcontext->ifmp );
		}

	ulSecsStartScrub = UlUtilGetSeconds();
		
	Call( ErrSCRUBScrubStreamingFile( 
			ppib,
			m_ifmp,
			&cpgSeen,
			&cpgScrubbed,
			NULL ) );
	
HandleError:

	if ( !m_fUnattended )
		{
		m_pcontext->err	= err;
		m_pcontext->cpgSeen = cpgSeen;
		m_pcontext->cpgScrubbed = cpgScrubbed;

		m_pcontext->signal.Set();
		}
	else
		{
		const ULONG_PTR ulSecFinished 	= UlUtilGetSeconds();
		const ULONG_PTR ulSecs 			= ulSecFinished - ulSecsStartScrub;

		// EventLog stop		
		const CHAR * rgszT[8];
		INT isz = 0;

		CHAR	szSeconds[16];
		CHAR	szErr[16];
		CHAR	szPages[16];
		CHAR	szScrubPages[16];

		sprintf( szSeconds, "%"FMTSZ3264"d", ulSecs );
		sprintf( szErr, "%d", err );
		sprintf( szPages, "%d", cpgSeen );
		sprintf( szScrubPages, "%d", cpgScrubbed );

		rgszT[isz++] = rgfmp[m_ifmp].SzSLVName();
		rgszT[isz++] = szSeconds;
		rgszT[isz++] = szErr;
		rgszT[isz++] = szPages;
		rgszT[isz++] = szScrubPages;
		
		Assert( isz <= sizeof( rgszT ) / sizeof( rgszT[0] ) );
		UtilReportEvent( 	(err >= JET_errSuccess)?eventInformation:eventError,
							DATABASE_ZEROING_CATEGORY,
							DATABASE_SLV_ZEROING_STOPPED_ID,
							isz, rgszT );
		// UNDONE: the task msg is deleting the task object ?
		}

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\old.cxx ===
#include "std.hxx"

//  ================================================================
class RECCHECKOLD : public RECCHECK
//  ================================================================
	{
	protected:
		RECCHECKOLD( const PGNO pgnoFDP, const IFMP ifmp, FUCB * pfucb, INST * const pinst );

	protected:
		const PGNO m_pgnoFDP;
		const IFMP m_ifmp;
		FUCB * const m_pfucb;				//  used to see if there are active versions
		INST * const m_pinst;

	private:
		RECCHECKOLD( const RECCHECKOLD& );
		RECCHECKOLD& operator=( const RECCHECKOLD& );
	};


//  ================================================================
class RECCHECKFINALIZE : public RECCHECKOLD
//  ================================================================
	{
	public:
		RECCHECKFINALIZE( 	const FID fid,
							const USHORT ibRecordOffset,
							const PGNO pgnoFDP,
							const IFMP ifmp,
							FUCB * const pfucb,
							INST * const pinst );
		ERR operator()( const KEYDATAFLAGS& kdf );

	protected:
		const FID		m_fid;
		const USHORT	m_ibRecordOffset;

	private:
		RECCHECKFINALIZE( const RECCHECKFINALIZE& );
		RECCHECKFINALIZE& operator=( const RECCHECKFINALIZE& );
	};


//  ================================================================
class RECCHECKDELETELV : public RECCHECKOLD
//  ================================================================
	{
	public:
		RECCHECKDELETELV( const PGNO pgnoFDP, const IFMP ifmp, FUCB * const pfucb, INST * const pinst );
		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		RECCHECKDELETELV( const RECCHECKDELETELV& );
		RECCHECKDELETELV& operator=( RECCHECKDELETELV& );
	};


LOCAL const _TCHAR		szOLD[]							= _T( "MSysDefrag1" );		//	table name for OLD Phase I

LOCAL JET_COLUMNCREATE	rgjccOLD[]						=
	{
	sizeof(JET_COLUMNCREATE),	"ObjidFDP",		JET_coltypLong,			0,	JET_bitColumnNotNULL,	NULL,	0,	0,	0,	0,
	sizeof(JET_COLUMNCREATE),	"DefragType",	JET_coltypUnsignedByte,	0,	NO_GRBIT,				NULL,	0,	0,	0,	0,
	sizeof(JET_COLUMNCREATE),	"Sentinel",		JET_coltypLong,			0,	NO_GRBIT,				NULL,	0,	0,	0,	0,
	sizeof(JET_COLUMNCREATE),	"Status",		JET_coltypShort,		0,	NO_GRBIT,				NULL,	0,	0,	0,	0,
	sizeof(JET_COLUMNCREATE),	"CurrentKey",	JET_coltypLongBinary,	0,	NO_GRBIT,				NULL,	0,	0,	0,	0,
	};
LOCAL const ULONG		ccolOLD							= sizeof(rgjccOLD) / sizeof(JET_COLUMNCREATE);

LOCAL JET_CONDITIONALCOLUMN	rgcondcolOLD[]				=
	{
	sizeof(JET_CONDITIONALCOLUMN),	"ObjidFDP",	JET_bitIndexColumnMustBeNonNull
	};
LOCAL const ULONG		ccondcolOLD						= sizeof(rgcondcolOLD) / sizeof(JET_CONDITIONALCOLUMN);

LOCAL JET_INDEXCREATE	rgjicOLD[]						=
	{
	sizeof(JET_INDEXCREATE),	"TablesToDefrag",	"+ObjidFDP\0",	(ULONG)strlen( "+ObjidFDP" ) + 2, JET_bitIndexUnique, 0, 0, 0, rgcondcolOLD, ccondcolOLD, 0
	};
LOCAL const ULONG		cidxOLD							= sizeof(rgjicOLD) / sizeof(JET_INDEXCREATE);

LOCAL const FID			fidOLDObjidFDP					= fidFixedLeast;
LOCAL const FID			fidOLDType						= fidFixedLeast+1;
LOCAL const FID			fidOLDObjidFDPSentinel			= fidFixedLeast+2;
LOCAL const FID			fidOLDStatus					= fidFixedLeast+3;
LOCAL const FID			fidOLDCurrentKey				= fidTaggedLeast;

LOCAL CAutoResetSignal*	rgasigOLDSLV;

LOCAL CCriticalSection	critOLD( CLockBasicInfo( CSyncBasicInfo( szOLD ), rankOLD, 0 ) );

LOCAL const CPG 		cpgOLDUpdateBookmarkThreshold	= 500;		// number of pages to clean before updating catalog

BOOL FOLDSystemTable( const CHAR *szTableName )
	{
	return ( 0 == UtilCmpName( szTableName, szOLD ) );
	}


//  ================================================================
INLINE VOID OLDDB_STATUS::Reset()
//  ================================================================
	{
	Assert( critOLD.FOwner() );

#ifdef OLD_DEPENDENCY_CHAIN_HACK
	m_pgnoPrevPartialMerge = 0;
	m_cpgAdjacentPartialMerges = 0;
#endif

	Reset_();
	}


//  ================================================================
INLINE VOID OLDSLV_STATUS::Reset()
//  ================================================================
	{
	Assert( critOLD.FOwner() );

	m_cChunksMoved = 0;
	Reset_();
	}


//  ****************************************************************
//	RECCHECKOLD
//  ****************************************************************

RECCHECKOLD::RECCHECKOLD( const PGNO pgnoFDP, const IFMP ifmp, FUCB * const pfucb, INST * const pinst ) :
	m_pgnoFDP( pgnoFDP ),
	m_ifmp( ifmp ),
	m_pfucb( pfucb ),
	m_pinst( pinst )
	{
	//	UNDONE: eliminate superfluous pgnoFDP param
	Assert( pgnoFDP == pfucb->u.pfcb->PgnoFDP() );
	}
	

//  ****************************************************************
//	RECCHECKFINALIZE
//  ****************************************************************

RECCHECKFINALIZE::RECCHECKFINALIZE(
	const FID fid,
	const USHORT ibRecordOffset,
	const PGNO pgnoFDP,
	const IFMP ifmp,
	FUCB * const pfucb,
	INST * const pinst ) :
	RECCHECKOLD( pgnoFDP, ifmp, pfucb, pinst ),
	m_fid( fid ),
	m_ibRecordOffset( ibRecordOffset )
	{
	Assert( FFixedFid( m_fid ) );
	}

ERR RECCHECKFINALIZE::operator()( const KEYDATAFLAGS& kdf )
	{
	const REC * prec = (REC *)kdf.data.Pv();
	if( m_fid > prec->FidFixedLastInRec() )
		{
		//  Column not present in record. Ignore the default value
		Assert( fFalse );		//  A finalizable column should always be present in the record
		return JET_errSuccess;
		}

	//	NULL bit is not set: column is NULL
	const UINT	ifid			= m_fid - fidFixedLeast;
	const BYTE	*prgbitNullity	= prec->PbFixedNullBitMap() + ifid/8;
	if ( FFixedNullBit( prgbitNullity, ifid ) )
		{
		//  Column is NULL
		return JET_errSuccess;
		}

	//  Currently all finalizable columns are ULONGs
	const ULONG * pulColumn = (ULONG *)((BYTE *)prec + m_ibRecordOffset );
	if( 0 == *pulColumn )
		{
		BOOKMARK bm;
		bm.key = kdf.key;
		bm.data.Nullify();
			
		if( !FVERActive( m_pfucb, bm ) )
			{
			//  This record should be finalized
			FINALIZETASK * ptask = new FINALIZETASK( m_pgnoFDP, m_pfucb->u.pfcb, m_ifmp, bm, m_ibRecordOffset );
			if( NULL == ptask )
				{
				return ErrERRCheck( JET_errOutOfMemory );
				}
			const ERR err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
			if( err < JET_errSuccess )
				{
				//  The task was not enqued sucessfully.
				delete ptask;
				return err;
				}
			}
		}
		
	return JET_errSuccess;
	}


//  ****************************************************************
//	RECCHECKDELETELV
//  ****************************************************************

RECCHECKDELETELV::RECCHECKDELETELV(
	const PGNO pgnoFDP,
	const IFMP ifmp,
	FUCB * const pfucb,
	INST * const pinst ) :
	RECCHECKOLD( pgnoFDP, ifmp, pfucb, pinst )
	{
	}

ERR RECCHECKDELETELV::operator()( const KEYDATAFLAGS& kdf )
	{
	//  See if we are on a LVROOT
	if( sizeof( LID ) == kdf.key.Cb() )
		{
		if( sizeof( LVROOT ) != kdf.data.Cb() )
			{
			return ErrERRCheck( JET_errLVCorrupted );
			}
		//  We are on a LVROOT, is the refcount 0?
		const LVROOT * const plvroot = (LVROOT *)kdf.data.Pv();
		if( 0 == plvroot->ulReference )
			{
			BOOKMARK bm;
			bm.key = kdf.key;
			bm.data.Nullify();

			Assert( sizeof( LID ) == bm.key.Cb() );
			Assert( 0 == bm.data.Cb() );
			
			if( !FVERActive( m_pfucb, bm ) )
				{
				//  This LV has a refcount of zero and has no versions
				DELETELVTASK * ptask = new DELETELVTASK( m_pgnoFDP, m_pfucb->u.pfcb, m_ifmp, bm );
				if( NULL == ptask )
					{
					return ErrERRCheck( JET_errOutOfMemory );
					}
				const ERR err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
				if( err < JET_errSuccess )
					{
					//  The task was not enqued sucessfully.
					delete ptask;
					return err;
					}
				}
			}
		}
	return JET_errSuccess;
	}

	

//	end OLD for all the active databases of an instance

VOID OLDTermInst( INST *pinst )
	{
	DBID	dbid;

	critOLD.Enter();

	for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
		{
		Assert( dbidTemp != dbid );

		OLDDB_STATUS * const	poldstatDB	= pinst->m_rgoldstatDB + dbid;
		if ( poldstatDB->FRunning()
			&& !poldstatDB->FTermRequested() )
			{
			poldstatDB->SetFTermRequested();
			critOLD.Leave();

			poldstatDB->SetSignal();
			if ( poldstatDB->FRunning() )
				{
				poldstatDB->ThreadEnd();
#ifdef OLD_SCRUB_DB				
				poldstatDB->ScrubThreadEnd();
#endif				
				}

			critOLD.Enter();
			poldstatDB->Reset();
			}

		OLDSLV_STATUS * const	poldstatSLV	= pinst->m_rgoldstatSLV + dbid;
		if ( poldstatSLV->FRunning()
			&& !poldstatSLV->FTermRequested() )
			{
			poldstatSLV->SetFTermRequested();
			critOLD.Leave();

			poldstatSLV->SetSignal();
			if ( poldstatSLV->FRunning() )
				{
				poldstatSLV->ThreadEnd();
				}

			critOLD.Enter();
			poldstatSLV->Reset();
			}
		}

	critOLD.Leave();
	}


INLINE BOOL FOLDContinue( const IFMP ifmp )
	{
	const INST * const			pinst		= PinstFromIfmp( ifmp );
	const DBID					dbid		= rgfmp[ifmp].Dbid();
	const OLDDB_STATUS * const	poldstatDB	= pinst->m_rgoldstatDB + dbid;

	//	Continue with OLD until signalled to terminate or until we
	//	hit specified timeout
	return ( !poldstatDB->FTermRequested()
		&& !poldstatDB->FReachedMaxElapsedTime() );
	}

INLINE BOOL FOLDContinueTree( const FUCB * pfucb )
	{
	return ( !pfucb->u.pfcb->FDeletePending() && FOLDContinue( pfucb->ifmp ) );
	}

//  ================================================================
VOID OLDSLVSleep( const IFMP ifmp, const ULONG cmsecSleep )
//  ================================================================
	{
	PinstFromIfmp( ifmp )->m_rgoldstatSLV[ rgfmp[ifmp].Dbid() ].FWaitOnSignal( cmsecSleep );
	}


//  ================================================================
VOID OLDSLVCompletedPass( const IFMP ifmp )
//  ================================================================
	{
	PinstFromIfmp( ifmp )->m_rgoldstatSLV[ rgfmp[ifmp].Dbid() ].IncCPasses();
	}


//  ================================================================
BOOL FOLDSLVContinue( const IFMP ifmp )
//  ================================================================
	{
	const INST * const			pinst		= PinstFromIfmp( ifmp );
	const DBID					dbid		= rgfmp[ifmp].Dbid();
	const OLDSLV_STATUS * const	poldstatSLV	= pinst->m_rgoldstatSLV + dbid;

	//	Continue with OLD until signalled to terminate or until we
	//	hit specified timeout
	return ( !poldstatSLV->FTermRequested()
		&& !poldstatSLV->FReachedMaxPasses()
		&& !poldstatSLV->FReachedMaxElapsedTime() );
	}


LOCAL ERR ErrOLDUpdateDefragStatus(
	PIB *						ppib,
	FUCB *						pfucbDefrag,
	DEFRAG_STATUS&				defragstat )
	{
	ERR							err;
	const INST * const			pinst		= PinstFromPpib( ppib );
	const DBID					dbid		= rgfmp[ pfucbDefrag->ifmp ].Dbid();
	const OLDDB_STATUS * const	poldstatDB	= pinst->m_rgoldstatDB + dbid;
	OBJID						objid		= defragstat.ObjidCurrentTable();
	DEFRAGTYPE					defragtype	= defragstat.DefragType();
	DATA						dataT;

	if ( poldstatDB->FAvailExtOnly() )
		return JET_errSuccess;
	
	//	update MSysDefrag to reflect next table to defrag
	Assert( !Pcsr( pfucbDefrag )->FLatched() );

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	Call( ErrIsamPrepareUpdate( ppib, pfucbDefrag, JET_prepReplace ) )
	
	dataT.SetPv( &objid );
	dataT.SetCb( sizeof(OBJID) );
	Call( ErrRECSetColumn(
				pfucbDefrag,
				fidOLDObjidFDP,
				1,
				&dataT ) );
	Call( ErrRECSetColumn(
				pfucbDefrag,
				fidOLDObjidFDPSentinel,
				1,
				&dataT ) );
				
	dataT.SetPv( &defragtype );
	dataT.SetCb( sizeof(DEFRAGTYPE) );
	Call( ErrRECSetColumn(
				pfucbDefrag,
				fidOLDStatus,
				1,
				defragstat.FTypeNull() ? NULL : &dataT ) );
				
	if ( defragstat.CbCurrentKey() > 0 )
		{
		dataT.SetPv( defragstat.RgbCurrentKey() );
		dataT.SetCb( defragstat.CbCurrentKey() );
		
		Assert( FTaggedFid( fidOLDCurrentKey ) );
		err = ErrRECSetLongField(
					pfucbDefrag,
					fidOLDCurrentKey,
					1,
					&dataT );
		Assert( JET_errRecordTooBig != err );
		Call( err );
		}
	else
		{
		Call( ErrRECSetColumn(
					pfucbDefrag,
					fidOLDCurrentKey,
					1,
					NULL ) );
		}
				
	Call( ErrIsamUpdate( ppib, pfucbDefrag, NULL, 0, NULL, NO_GRBIT ) );
	
	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	
	return JET_errSuccess;

HandleError:
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
	return err;
	}


LOCAL ERR ErrOLDDefragOneTree(
	PIB				*ppib,
	FUCB			*pfucb,
	FUCB 			*pfucbDefrag,
	DEFRAG_STATUS&	defragstat,
	const BOOL		fResumingTree,
	RECCHECK		*preccheck )
	{
	ERR				err;
	INST * 			const pinst				= PinstFromPpib( ppib );
	DIB				dib;
	BOOKMARK		bmStart;
	BOOKMARK		bmNext;
	BYTE			rgbKeyBuf[KEY::cbKeyMax];
	BOOL			fInTrx 					= fFalse;

	Assert( pfucbNil != pfucb );
	Assert( dbidTemp != rgfmp[ pfucb->ifmp ].Dbid() );
	Assert( ( !fResumingTree && defragstat.FTypeSpace() )
		|| defragstat.FTypeTable()
		|| defragstat.FTypeLV() );

	//	UNDONE: Non-unique keys not currently supported.  In order to support
	//	non-unique keys, the rgbKeyBuf and defragstat.RgbCurrentKey() buffers
	//	would have to be doubled in size (to accommodate the primary key bookmark).
	Assert( FFUCBUnique( pfucb ) );
	Assert( !pfucb->u.pfcb->FTypeSecondaryIndex() );

	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( !Pcsr( pfucbDefrag )->FLatched() );

	//	small trees should have been filtered out by ErrOLDDefragOneTable()
	Assert( pfucb->u.pfcb->FSpaceInitialized() );
	Assert( pgnoNull != pfucb->u.pfcb->PgnoOE() );
	Assert( pgnoNull != pfucb->u.pfcb->PgnoAE() );

	bmStart.Nullify();
	bmStart.key.suffix.SetPv( defragstat.RgbCurrentKey() );
	bmNext.Nullify();
	bmNext.key.suffix.SetPv( rgbKeyBuf );

	if ( fResumingTree && defragstat.CbCurrentKey() > 0 )
		{
		bmStart.key.suffix.SetCb( defragstat.CbCurrentKey() );
		dib.pos = posDown;
		dib.pbm = &bmStart;
		}
	else
		{
		dib.pos = posLast;
		}

	dib.dirflag = fDIRAllNode;
	err = ErrBTDown( pfucb, &dib, latchReadTouch );
	if ( err < 0 )
		{
		if ( JET_errRecordNotFound == err )
			err = JET_errSuccess;		// no records in table

		return err;
		}
	
	Assert( Pcsr( pfucb )->FLatched() );

	NDGetBookmarkFromKDF( pfucb, pfucb->kdfCurr, &bmStart );

	//	normalised key cannot be NULL (at minimum, there will be a prefix byte)
	Assert( !bmStart.key.FNull() );

	//	UNDONE: Currently, must copy into bmNext so it will get copied back to bmStart
	//	in the loop below.  Is there a better way?
	Assert( bmNext.key.suffix.Pv() == rgbKeyBuf );
	Assert( 0 == bmNext.key.prefix.Cb() );
	bmStart.key.CopyIntoBuffer( bmNext.key.suffix.Pv(), bmStart.key.Cb() );
	bmNext.key.suffix.SetCb( bmStart.key.Cb() );

	bmNext.data.SetPv( (BYTE *)bmNext.key.suffix.Pv() + bmNext.key.Cb() );
	bmStart.data.CopyInto( bmNext.data );

	//	must reset bmStart, because it got set to elsewhere by NDGetBookmarkFromKDF()
	bmStart.Nullify();
	bmStart.key.suffix.SetPv( defragstat.RgbCurrentKey() );

#ifdef OLD_DEPENDENCY_CHAIN_HACK
	pinst->m_rgoldstatDB[ rgfmp[pfucb->ifmp].Dbid() ].SetPgnoPrevPartialMerge( 0 );
	pinst->m_rgoldstatDB[ rgfmp[pfucb->ifmp].Dbid() ].ResetCpgAdjacentPartialMerges();
#endif	

	while ( !bmNext.key.FNull()
		&& FOLDContinueTree( pfucb ) )
		{
		BOOL	fPerformedRCEClean	= fFalse;
		
		VOID	*pvSwap	= bmStart.key.suffix.Pv();
		bmStart.key.suffix.SetPv( bmNext.key.suffix.Pv() );
		bmStart.key.suffix.SetCb( bmNext.key.suffix.Cb() );
		bmStart.data.SetPv( bmNext.data.Pv() );
		bmStart.data.SetCb( bmNext.data.Cb() );

		bmNext.Nullify();
		bmNext.key.suffix.SetPv( pvSwap );

		Assert( bmStart.key.prefix.Cb() == 0 );
		Assert( bmNext.key.prefix.Cb() == 0 );

		//	UNDONE: secondary indexes not currently supported
		Assert( bmStart.data.Cb() == 0 );
		Assert( bmNext.data.Cb() == 0 );

		BTUp( pfucb );

		Assert( !fInTrx );
		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fInTrx = fTrue;

		forever
			{
			err = ErrBTIMultipageCleanup( pfucb, bmStart, &bmNext, preccheck );
			BTUp( pfucb );

			if ( err < 0 )
				{
				//	if out of version store, try once to clean up
				if ( JET_errVersionStoreOutOfMemory == err && !fPerformedRCEClean )
					{
					CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
					fInTrx = fFalse;
					
					UtilSleep( 1000 );
					
					Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
					fInTrx = fTrue;

					//	only try cleanup once
					fPerformedRCEClean = fTrue;
					}
				else if ( errBTMergeNotSynchronous != err )
					{
					goto HandleError;
					}
				}
			else
				{
				break;
				}
			}

		//	see if we need to update catalog with our progress
		if ( !defragstat.FTypeSpace() )
			{
			Assert( !FFUCBSpace( pfucb ) );
			if ( bmNext.key.suffix.Pv() == defragstat.RgbCurrentKey() )
				{
				defragstat.SetCbCurrentKey( bmNext.key.suffix.Cb() );

				Assert( bmStart.key.suffix.Pv() == rgbKeyBuf );
				}
			else
				{
				Assert( bmStart.key.suffix.Pv() == defragstat.RgbCurrentKey() );
				Assert( bmStart.key.suffix.Cb() == defragstat.CbCurrentKey() );
				Assert( bmNext.key.suffix.Pv() == rgbKeyBuf );
				}

			defragstat.IncrementCpgCleaned();
			if ( defragstat.CpgCleaned() > cpgOLDUpdateBookmarkThreshold )
				{
				defragstat.ResetCpgCleaned();

				//	UNDONE: Don't currently support non-unique indexes;
				Assert( 0 == bmNext.data.Cb() );
				Assert( 0 == bmNext.key.prefix.Cb() );
			
				//	UNDONE: Don't currently support non-unique indexes
				Assert( defragstat.CbCurrentKey() <= KEY::cbKeyMax );

				//	Ensure LV doesn't get burst.
				Assert( defragstat.CbCurrentKey() < cbLVIntrinsicMost );

				Assert( defragstat.FTypeTable() || defragstat.FTypeLV() );
				Call( ErrOLDUpdateDefragStatus( ppib, pfucbDefrag, defragstat ) );
				}
			}
		else
			{
			Assert( FFUCBSpace( pfucb ) );
			if ( bmNext.key.suffix.Pv() == defragstat.RgbCurrentKey() )
				{
				Assert( bmStart.key.suffix.Pv() == rgbKeyBuf );
				}
			else
				{
				Assert( bmStart.key.suffix.Pv() == defragstat.RgbCurrentKey() );
				Assert( bmNext.key.suffix.Pv() == rgbKeyBuf );
				}
			}

		Assert( fInTrx );
		Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
		fInTrx = fFalse;

		while ( pinst->m_plog->m_fBackupInProgress && FOLDContinueTree( pfucb ) )
			{
			//	suspend OLD if this process is performing online backup
			pinst->m_rgoldstatDB[ rgfmp[pfucb->ifmp].Dbid() ].FWaitOnSignal( cmsecWaitForBackup );
			}
		}

	Assert( !fInTrx );

HandleError:
	BTUp( pfucb );
	
	if ( fInTrx )
		{
		Assert( err < 0 );
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	return err;
	}


LOCAL ERR ErrOLDDefragSpaceTree(
	PIB				* ppib,
	FUCB			* pfucb,
	FUCB 			* pfucbDefrag,
	const BOOL		fAvailExt )
	{
	ERR				err;
	FUCB			* pfucbT	= pfucbNil;
	DEFRAG_STATUS	defragstat;

	Assert( !FFUCBSpace( pfucb ) );

	Assert( pfucb->u.pfcb->FSpaceInitialized() );
	Assert( pgnoNull != pfucb->u.pfcb->PgnoAE() );
	Assert( pgnoNull != pfucb->u.pfcb->PgnoOE() );

	if ( fAvailExt )
		{
		CallR( ErrSPIOpenAvailExt( ppib, pfucb->u.pfcb, &pfucbT ) );
		}
	else
		{
		CallR( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbT ) );
		}
	Assert( FFUCBSpace( pfucbT ) );
	
	defragstat.SetTypeSpace();
	err = ErrOLDDefragOneTree(
				ppib,
				pfucbT,
				pfucbDefrag,
				defragstat,
				fFalse,
				NULL );


	Assert( pfucbNil != pfucbT );
	BTClose( pfucbT );

	return err;
	}

//  ================================================================
LOCAL ERR ErrOLDCheckForFinalize(
			PIB * const ppib,
			FUCB * const pfucb,
			INST * const pinst,
			RECCHECK ** ppreccheck )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	*ppreccheck = NULL;
	
	FCB * const pfcb = pfucb->u.pfcb;
	pfcb->EnterDDL();
	
	Assert( NULL != pfcb );
	Assert( pfcb->FTypeTable() );
	Assert( NULL != pfcb->Ptdb() );
	TDB * const ptdb = pfcb->Ptdb();

	//  Does this table have a finalize callback
	const CBDESC * pcbdesc = ptdb->Pcbdesc();
	while( NULL != pcbdesc )
		{
		if( pcbdesc->cbtyp & JET_cbtypFinalize )
			{
			break;
			}
		pcbdesc = pcbdesc->pcbdescNext;
		}
	if( NULL == pcbdesc )
		{
		//  No finalize callback
		pfcb->LeaveDDL();
		return JET_errSuccess;
		}

	//  Now find the first column that is finalizable
	//  UNDONE: find all finalizable columns and build a RECCHECKMACRO
	const FID	fidLast	= ptdb->FidFixedLast();
	FID			fid;
	for( fid = fidFixedLeast; fid <= fidLast; ++fid )
		{
		const BOOL	fTemplateColumn	= ptdb->FFixedTemplateColumn( fid );
		const FIELD	* const pfield	= ptdb->PfieldFixed( ColumnidOfFid( fid, fTemplateColumn ) );
		if( FFIELDFinalize( pfield->ffield ) )
			{
			//  we have found the column
			*ppreccheck = new RECCHECKFINALIZE(
								fid,
								pfield->ibRecordOffset,
								pfcb->PgnoFDP(),
								pfcb->Ifmp(),
								pfucb,
								pinst );
			err = ( NULL == *ppreccheck ) ? ErrERRCheck( JET_errOutOfMemory ) : JET_errSuccess;
			break;
			}
		}
	pfcb->LeaveDDL();
	return err;
	}


LOCAL ERR ErrOLDDefragOneTable(
	PIB	*						ppib,
	FUCB *						pfucbCatalog,
	FUCB *						pfucbDefrag,
	DEFRAG_STATUS&				defragstat )
	{
	ERR							err;
	const IFMP					ifmp			= pfucbCatalog->ifmp;
	const OLDDB_STATUS * const	poldstatDB		= PinstFromIfmp( ifmp )->m_rgoldstatDB + rgfmp[ifmp].Dbid();
	FUCB *						pfucb			= pfucbNil;
	FUCB *						pfucbLV			= pfucbNil;
	OBJID						objidTable;
	DATA						dataField;
	CHAR						szTable[JET_cbNameMost+1];
	BOOL						fLatchedCatalog	= fFalse;

	Assert( !Pcsr( pfucbDefrag )->FLatched() );
	
	Assert( !Pcsr( pfucbCatalog )->FLatched() );
	err = ErrDIRGet( pfucbCatalog );
	if ( err < 0 )
		{
		if ( JET_errRecordDeleted == err )
			err = JET_errSuccess;		//	since we're at level 0, table may have just gotten deleted, so skip it
		goto HandleError;
		}
	fLatchedCatalog = fTrue;

	//	first record with this objidFDP should always be the Table object.
	Assert( FFixedFid( fidMSO_Type ) );
	CallS( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Type,
				pfucbCatalog->kdfCurr.data,
				&dataField ) );
	Assert( dataField.Cb() == sizeof(SYSOBJ) );

	if ( sysobjTable != *( (UnalignedLittleEndian< SYSOBJ > *)dataField.Pv() ) )
		{
		//	We might end up not on a table record because we do our seek at level 0
		//	and may be seeking while someone is in the middle of committing a table
		//	creation to level 0 - hence, we miss the table record, but suddenly
		//	start seeing the column records.
		//	This could only happen if the table was new, so it wouldn't require
		//	defragmentation anyway.  Just skip the table.
		err = JET_errSuccess;
		goto HandleError;
		}

	Assert( FFixedFid( fidMSO_ObjidTable ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_ObjidTable,
				pfucbCatalog->kdfCurr.data,
				&dataField ) );
	CallS( err );
	Assert( dataField.Cb() == sizeof(OBJID) );
	UtilMemCpy( &objidTable, dataField.Pv(), sizeof(OBJID) );

	Assert( objidTable >= objidFDPMSO );
	if ( defragstat.ObjidCurrentTable() != objidTable )
		{
		defragstat.SetObjidCurrentTable( objidTable );

		//	must force to restart from top of table in case we were trying to resume
		//	a tree that no longer exists
		defragstat.SetTypeNull();
		}

	Assert( FVarFid( fidMSO_Name ) );
	Call( ErrRECIRetrieveVarColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Name,
				pfucbCatalog->kdfCurr.data,
				&dataField ) );
	CallS( err );
	Assert( dataField.Cb() > 0 );
	Assert( dataField.Cb() <= JET_cbNameMost );
	UtilMemCpy( szTable, dataField.Pv(), dataField.Cb() );
	szTable[dataField.Cb()] = '\0';

	Assert( fLatchedCatalog );
	CallS( ErrDIRRelease( pfucbCatalog ) );
	fLatchedCatalog = fFalse;

	//	UNDONE: defragment secondary index trees
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucb, szTable, NO_GRBIT );
	if ( err < 0 )
		{
		Assert( pfucbNil == pfucb );
		if ( JET_errTableLocked == err || JET_errObjectNotFound == err )
			err = JET_errSuccess;		//	if table is exclusively held or has since been deleted, just skip it
		goto HandleError;
		}

	Assert( pfucbNil != pfucb );

	//	need access to space info immediately
	if ( !pfucb->u.pfcb->FSpaceInitialized() )
		{
		Call( ErrSPDeferredInitFCB( pfucb ) );
		}
	
	Assert( pfucb->u.pfcb->FSpaceInitialized() );		//	OLD forces space info to be retrieved on OpenTable
	if ( pgnoNull == pfucb->u.pfcb->PgnoOE() )
		{
		Assert( pgnoNull == pfucb->u.pfcb->PgnoAE() );
		}
	else
		{
		Assert( pgnoNull != pfucb->u.pfcb->PgnoAE() );
		Assert( pfucb->u.pfcb->PgnoAE() == pfucb->u.pfcb->PgnoOE()+1 );
		}
	
	//	don't defrag tables without space trees -- they're so small
	//	they're not worth defragging
	if ( FOLDContinueTree( pfucb )
		&& pgnoNull != pfucb->u.pfcb->PgnoOE() )
		{
		BOOL	fResumingTree	= !defragstat.FTypeNull();

		//	ALWAYS defrag space trees, regardless of whether we're resuming or not
		Call( ErrOLDDefragSpaceTree(
					ppib,
					pfucb,
					pfucbDefrag,
					fTrue ) );
		if ( !FOLDContinueTree( pfucb ) )
			{
			goto HandleError;
			}

		if ( !poldstatDB->FAvailExtOnly() )
			{
			Call( ErrOLDDefragSpaceTree(
						ppib,
						pfucb,
						pfucbDefrag,
						fFalse ) );
			if ( !FOLDContinueTree( pfucb ) )
				{
				goto HandleError;
				}

			if ( defragstat.FTypeNull() || defragstat.FTypeTable() )
				{
				//  determine if there are any columns to be finalized
				RECCHECK * preccheck = NULL;
				Call( ErrOLDCheckForFinalize( ppib, pfucb, PinstFromPpib( ppib ), &preccheck ) );

				defragstat.SetTypeTable();
				err = ErrOLDDefragOneTree(
							ppib,
							pfucb,
							pfucbDefrag,
							defragstat,
							fResumingTree,
							preccheck );

				if( NULL != preccheck )
					{
					delete preccheck;
					}

				if ( err >= 0 && FOLDContinueTree( pfucb ) )
					{
					defragstat.SetTypeLV();
					defragstat.SetCbCurrentKey( 0 );
					}
				else
					{
					goto HandleError;
					}
				}
			else
				{
				Assert( defragstat.FTypeLV() );
				Assert( fResumingTree );
				}
			}

		if ( defragstat.FTypeLV() || poldstatDB->FAvailExtOnly() )
			{
			Call( ErrFILEOpenLVRoot( pfucb, &pfucbLV, fFalse ) );	// UNDONE: Call ErrDIROpenLongRoot() instead
			if ( wrnLVNoLongValues == err )
				{
				Assert( pfucbNil == pfucbLV );
				}
			else
				{
				CallS( err );
				Assert( pfucbNil != pfucbLV );
				Assert( pfucbLV->u.pfcb->FSpaceInitialized() );	//	we don't defer space init for LV trees

				if ( pgnoNull == pfucbLV->u.pfcb->PgnoAE() )
					{
					Assert( pgnoNull == pfucbLV->u.pfcb->PgnoOE() );
					goto HandleError;
					}
				else
					{
					Assert( pgnoNull != pfucb->u.pfcb->PgnoOE() );
					Assert( pfucbLV->u.pfcb->PgnoAE() == pfucbLV->u.pfcb->PgnoOE()+1 );
					}
				
				Call( ErrOLDDefragSpaceTree(
							ppib,
							pfucbLV,
							pfucbDefrag,
							fTrue ) );
				if ( FOLDContinueTree( pfucb )				//	use table's cursor to check if we should continue
					&& !poldstatDB->FAvailExtOnly() )
					{
					RECCHECKDELETELV reccheck(
						pfucbLV->u.pfcb->PgnoFDP(),
						pfucbLV->ifmp,
						pfucbLV,
						PinstFromPfucb( pfucb ) );
					
					Call( ErrOLDDefragSpaceTree(
								ppib,
								pfucbLV,
								pfucbDefrag,
								fFalse ) );
					if ( !FOLDContinueTree( pfucb )	)			//	use table's cursor to check if we should continue
						{
						goto HandleError;
						}
					
					Call( ErrOLDDefragOneTree(
								ppib,
								pfucbLV,
								pfucbDefrag,
								defragstat,
								fResumingTree,
								&reccheck ) );
					}
				}
			}
		}

HandleError:
	if ( fLatchedCatalog )
		{
		CallS( ErrDIRRelease( pfucbCatalog ) );
		}
		
	if ( pfucbNil != pfucbLV )
		{
		DIRClose( pfucbLV );
		}
		
	if ( pfucbNil != pfucb )
		{
		CallS( ErrFILECloseTable( ppib, pfucb ) );
		}

	return err;
	}


LOCAL ERR ErrOLDDefragTables(
	PIB *					ppib,
	FUCB *					pfucbDefrag,
	DEFRAG_STATUS&			defragstat )
	{
	ERR						err;
	INST * const			pinst			= PinstFromPpib( ppib );
	const IFMP				ifmp 			= pfucbDefrag->ifmp;
	OLDDB_STATUS * const	poldstatDB		= pinst->m_rgoldstatDB + rgfmp[ifmp].Dbid();
	FUCB *					pfucbCatalog	= pfucbNil;
	OBJID					objidNext		= defragstat.ObjidCurrentTable();
	ULONG_PTR				csecStartPass;
	const CHAR *			rgszT[1];
	CHAR 					szTrace[64];

	Assert( dbidTemp != rgfmp[ ifmp ].Dbid() );
	Assert( 0 == poldstatDB->CPasses() );
	Assert( 0 == defragstat.CpgCleaned() );
	Assert( defragstat.FPassNull() );

	CallR( ErrCATOpen( ppib, ifmp, &pfucbCatalog ) );
	Assert( pfucbNil != pfucbCatalog );

	rgszT[0] = rgfmp[ifmp].SzDatabaseName();

	if ( objidNext > objidFDPMSO )
		{
		UtilReportEvent(
			eventInformation,
			ONLINE_DEFRAG_CATEGORY,
			OLD_RESUME_PASS_ID,
			1,
			rgszT );
		defragstat.SetPassPartial();

		sprintf( szTrace, "OLD RESUME (ifmp %d)", ifmp );
		}
	else
		{
		UtilReportEvent(
			eventInformation,
			ONLINE_DEFRAG_CATEGORY,
			OLD_BEGIN_FULL_PASS_ID,
			1,
			rgszT );
		defragstat.SetPassFull();

		sprintf( szTrace, "OLD BEGIN (ifmp %d)", ifmp );
		}
		
	Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );		

	csecStartPass = UlUtilGetSeconds();

	while ( FOLDContinue( ifmp ) )
		{
		Call( ErrIsamMakeKey(
					ppib,
					pfucbCatalog,
					(BYTE *)&objidNext,
					sizeof(objidNext),
					JET_bitNewKey ) );
		err = ErrIsamSeek( ppib, pfucbCatalog, JET_bitSeekGE );
		if ( JET_errRecordNotFound == err )
			{
			err = JET_errSuccess;

			//	reset status
			defragstat.SetTypeNull();
			defragstat.SetObjidCurrentTable( objidFDPMSO );
			defragstat.ResetCpgCleaned();

			Call( ErrOLDUpdateDefragStatus( ppib, pfucbDefrag, defragstat ) );

			sprintf( szTrace, "OLD COMPLETED PASS (ifmp %d)", ifmp );
			Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );

			//	if we reached the end, start over
			if ( defragstat.FPassPartial() )
				{
				UtilReportEvent(
					eventInformation,
					ONLINE_DEFRAG_CATEGORY,
					OLD_COMPLETE_RESUMED_PASS_ID,
					1,
					rgszT );
				defragstat.SetPassFull();		//	completed partial pass, begin full pass
				
				}
			else
				{
				UtilReportEvent(
					eventInformation,
					ONLINE_DEFRAG_CATEGORY,
					OLD_COMPLETE_FULL_PASS_ID,
					1,
					rgszT );

				poldstatDB->IncCPasses();
				if ( poldstatDB->FReachedMaxPasses() )
					{
					defragstat.SetPassCompleted();
					break;
					}
				}

			//	if performing a finite number of passes, then just wait long enough for
			//	background cleanup to catch up before doing next pass.
			ULONG	cmsecWait	=	cmsecAsyncBackgroundCleanup;
			if ( poldstatDB->FInfinitePasses() )
				{
				//	if performing an infinite number of passes, then pad the wait time
				//	such that each pass will take at least 1 hour
				const ULONG_PTR		csecCurrentPass	= UlUtilGetSeconds() - csecStartPass;
				if ( csecCurrentPass < csecOLDMinimumPass )
					{
					cmsecWait = (ULONG)max( ( csecOLDMinimumPass - csecCurrentPass ) * 1000, cmsecAsyncBackgroundCleanup );
					}
				}
				
			poldstatDB->FWaitOnSignal( cmsecWait );

			if ( FOLDContinue( ifmp ) )
				{
				UtilReportEvent(
					eventInformation,
					ONLINE_DEFRAG_CATEGORY,
					OLD_BEGIN_FULL_PASS_ID,
					1,
					rgszT );
				
				Assert( defragstat.FPassFull() );

				sprintf( szTrace, "OLD RESTART FULL PASS (ifmp %d)", ifmp );
				Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );

				csecStartPass = UlUtilGetSeconds();
				}
			}
		else
			{
			Call( err );
			Assert( JET_wrnSeekNotEqual == err );

			//	NOTE: the only time defragstat.Type should be non-NULL is the very
			//	first iteration if we're resuming the tree

			Call( ErrOLDDefragOneTable(
						ppib,
						pfucbCatalog,
						pfucbDefrag,
						defragstat ) );

			if ( !FOLDContinue( ifmp ) )
				break;
				
			//	prepare for next table
			defragstat.SetTypeNull();
			defragstat.SetObjidNextTable();
			}
			
		objidNext = defragstat.ObjidCurrentTable();
		}

	if ( !defragstat.FPassCompleted() )
		{
		UtilReportEvent(
			eventInformation,
			ONLINE_DEFRAG_CATEGORY,
			OLD_INTERRUPTED_PASS_ID,
			1,
			rgszT );
		}

HandleError:

	sprintf( szTrace, "OLD END (ifmp %d, err %d)", ifmp, err );
	(void) ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );

	Assert( pfucbNil != pfucbCatalog );	
	CallS( ErrCATClose( ppib, pfucbCatalog ) );
	
	return err;
	}


LOCAL ERR ErrOLDCreate( PIB *ppib, const IFMP ifmp )
	{
	ERR				err;
	FUCB			*pfucb;
	DATA			dataField;
	OBJID			objidFDP	= objidFDPMSO;
	JET_TABLECREATE2	jtcOLD		=
		{
		sizeof(JET_TABLECREATE2),
		(CHAR *)szOLD,
		NULL,					// Template table
		0,
		100,					//	set to 100% density, because we will always be appending
		rgjccOLD,
		ccolOLD,
		rgjicOLD,
		cidxOLD,
		NULL,
		0,
		JET_bitTableCreateFixedDDL|JET_bitTableCreateSystemTable,
		0,
		0
		};

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		
	//	MSysDefrag doesn't exist, so create it
	Call( ErrFILECreateTable( ppib, ifmp, &jtcOLD ) );
	pfucb = (FUCB *)jtcOLD.tableid;

	//	insert initial record
	Call( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepInsert ) )
	dataField.SetPv( &objidFDP );
	dataField.SetCb( sizeof(OBJID) );
	Call( ErrRECSetColumn(
				pfucb,
				fidOLDObjidFDP,
				1,
				&dataField ) );
	Call( ErrRECSetColumn(
				pfucb,
				fidOLDObjidFDPSentinel,
				1,
				&dataField ) );
	Call( ErrIsamUpdate( ppib, pfucb, NULL, 0, NULL, NO_GRBIT ) );

	Call( ErrFILECloseTable( ppib, pfucb ) );

	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	
	return JET_errSuccess;

HandleError:
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	return err;
	}


DWORD OLDDefragDb( DWORD_PTR dw )
	{
	ERR						err;
	IFMP					ifmp			= (IFMP)dw;
	INST * const			pinst			= PinstFromIfmp( ifmp );
	OLDDB_STATUS * const	poldstatDB		= pinst->m_rgoldstatDB + rgfmp[ifmp].Dbid();
	PIB *					ppib			= ppibNil;
	FUCB *					pfucb			= pfucbNil;
	BOOL					fOpenedDb		= fFalse;
	DEFRAG_STATUS			defragstat;
	DATA					dataField;

	Assert( 0 == poldstatDB->CPasses() );

	CallR( ErrPIBBeginSession( pinst, &ppib, procidNil, fFalse ) );
	Assert( ppibNil != ppib );

	ppib->SetFUserSession();					//	we steal a user session in order to do OLD
	ppib->SetFSessionOLD();
	ppib->grbitsCommitDefault = JET_bitCommitLazyFlush;

	Call( ErrDBOpenDatabaseByIfmp( ppib, ifmp ) );
	fOpenedDb = fTrue;
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucb, szOLD, NO_GRBIT );
	if ( err < 0 )
		{
		if ( JET_errObjectNotFound != err )
			goto HandleError;

		//	Create the table, then re-open it.  Can't just use the cursor returned from CreateTable
		//	because that cursor has exclusive use of the table, meaning that it will be visible
		//	to Info calls (because it's in the catalog) but not accessible.
		Call( ErrOLDCreate( ppib, ifmp ) );
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucb, szOLD, NO_GRBIT ) );
		}

	Assert( pfucbNil != pfucb );

	//	UNDONE: Switch to secondary index, see if any tables have been
	//	specifically requested to be defragmented, and process those first

	//	move to first record, which defines the "defrag window"
	err = ErrIsamMove( ppib, pfucb, JET_MoveFirst, NO_GRBIT );
	Assert( JET_errNoCurrentRecord != err );
	Assert( JET_errRecordNotFound != err );
	Call( err );

	Assert( !Pcsr( pfucb )->FLatched() );
	Call( ErrDIRGet( pfucb ) );
	
#ifdef DEBUG
	//	verify Type is NULL, so that this record doesn't get included
	//	in the secondary index
	Assert( FFixedFid( fidOLDType ) );
	err = ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucb->u.pfcb->Ptdb(),
				fidOLDType,
				pfucb->kdfCurr.data,
				&dataField );
	Assert( JET_wrnColumnNull == err );
#endif

	Assert( FFixedFid( fidOLDObjidFDP ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucb->u.pfcb->Ptdb(),
				fidOLDObjidFDP,
				pfucb->kdfCurr.data,
				&dataField ) );
	CallS( err );
	Assert( dataField.Cb() == sizeof(OBJID) );
	defragstat.SetObjidCurrentTable( *( (UnalignedLittleEndian< OBJID > *)dataField.Pv() ) );

#ifdef DEBUG
	//	UNDONE: Multi-threaded OLD support.  ObjidBegin and ObjidEnd define
	//	the window of tables on which all threads are working.
	OBJID	objidFDPEnd;
	Assert( FFixedFid( fidOLDObjidFDPSentinel ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucb->u.pfcb->Ptdb(),
				fidOLDObjidFDPSentinel,
				pfucb->kdfCurr.data,
				&dataField ) );
	CallS( err );
	Assert( dataField.Cb() == sizeof(OBJID) );
	UtilMemCpy( &objidFDPEnd, dataField.Pv(), sizeof(OBJID) );

	Assert( objidFDPEnd == defragstat.ObjidCurrentTable() );
#endif

	Assert( FFixedFid( fidOLDStatus ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucb->u.pfcb->Ptdb(),
				fidOLDStatus,
				pfucb->kdfCurr.data,
				&dataField ) );
	if ( JET_errSuccess == err )
		{
		Assert( dataField.Cb() == sizeof(DEFRAGTYPE) );
		defragstat.SetType( *( (UnalignedLittleEndian< DEFRAGTYPE > *)dataField.Pv() ) );
		Assert( defragstat.FTypeTable() || defragstat.FTypeLV() );

		Assert( FTaggedFid( fidOLDCurrentKey ) );
		Call( ErrRECIRetrieveTaggedColumn(
					pfucb->u.pfcb,
					ColumnidOfFid( fidOLDCurrentKey, fFalse ),
					1,
					pfucb->kdfCurr.data,
					&dataField ) );
		Assert( dataField.Cb() <= KEY::cbKeyMax );
		Assert( wrnRECUserDefinedDefault != err );
		Assert( wrnRECSeparatedSLV != err );
		Assert( wrnRECIntrinsicSLV != err );
		Assert( wrnRECSeparatedLV != err );

		Assert( wrnRECLongField != err );
		if ( JET_errSuccess == err || wrnRECIntrinsicLV == err )
			{
			memcpy( defragstat.RgbCurrentKey(), dataField.Pv(), dataField.Cb() );
			defragstat.SetCbCurrentKey( dataField.Cb() );
			}
		else
			{
			Assert( JET_wrnColumnNull == err );
			Assert( 0 == dataField.Cb() );
			defragstat.SetCbCurrentKey( 0 );
			}
		}
	else
		{
		Assert( JET_wrnColumnNull == err );
		defragstat.SetTypeNull();
		}

	if ( poldstatDB->FAvailExtOnly() )
		{
		defragstat.SetTypeNull();
		defragstat.SetObjidCurrentTable( objidFDPMSO );
		}

	CallS( ErrDIRRelease( pfucb ) );
	
	Call( ErrOLDDefragTables( ppib, pfucb, defragstat ) );

	Call( ErrOLDUpdateDefragStatus( ppib, pfucb, defragstat ) );

	if( NULL != poldstatDB->Callback() )
		{
		Assert( ppibNil != ppib );
		(VOID)( poldstatDB->Callback() )(
					reinterpret_cast<JET_SESID>( ppib ),
					static_cast<JET_DBID>( ifmp ),
					JET_tableidNil,
					JET_cbtypOnlineDefragCompleted,
					NULL,
					NULL,
					NULL,
					0 );
		}
		
HandleError:
	Assert( ppibNil != ppib );
	
	if ( err < 0 )
		{
		CHAR		szErr[16];
		const CHAR	*rgszT[2];

		sprintf( szErr, "%d", err );

		rgszT[0] = rgfmp[ifmp].SzDatabaseName();
		rgszT[1] = szErr;

		//	even though an error was encountered, just report it as a warning
		//	because the next time OLD is invoked, it will simply resume from
		//	where it left off
		UtilReportEvent(
			eventWarning,
			ONLINE_DEFRAG_CATEGORY,
			OLD_ERROR_ID,
			2,
			rgszT );

		//	track errors to catch cases where we could actually
		//	have recovered from whatever error was encountered
		AssertTracking();
		}
	
	if ( pfucbNil != pfucb )
		{
		CallS( ErrFILECloseTable( ppib, pfucb ) );
		}
	if ( fOpenedDb )
		{
		CallS( ErrDBCloseAllDBs( ppib ) );
		}

	PIBEndSession( ppib );

	critOLD.Enter();
	if ( !poldstatDB->FTermRequested() )
		{
		
		//	we're terminating before the client asked
		poldstatDB->ThreadEnd();
#ifdef OLD_SCRUB_DB				
		poldstatDB->ScrubThreadEnd();
#endif
		poldstatDB->Reset();
		}
	critOLD.Leave();
	

	return 0;
	}


//  ================================================================
DWORD OLDDefragDbSLV( DWORD_PTR dw )
//  ================================================================
	{
	ERR						err;
	IFMP					ifmp			= (IFMP)dw;
	INST * const			pinst			= PinstFromIfmp( ifmp );
	OLDSLV_STATUS * const	poldstatSLV		= pinst->m_rgoldstatSLV + rgfmp[ifmp].Dbid();
	const CHAR *			rgszT[2];

	Assert( 0 == poldstatSLV->CPasses() );

	rgszT[0] = rgfmp[ifmp].SzSLVName();

	UtilReportEvent(
			eventInformation,
			ONLINE_DEFRAG_CATEGORY,
			OLDSLV_BEGIN_FULL_PASS_ID,
			1,
			rgszT );

	Call( ErrOLDSLVDefragDB( ifmp ) );
	
	if( NULL != poldstatSLV->Callback() )
		{
		(VOID)( poldstatSLV->Callback() )(
					JET_sesidNil,
					JET_dbidNil,
					JET_tableidNil,
					JET_cbtypOnlineDefragCompleted,
					NULL,
					NULL,
					NULL,
					0 );
		}
		
HandleError:	
	if ( err < 0 )
		{
		CHAR		szErr[16];

		sprintf( szErr, "%d", err );
		rgszT[1] = szErr;

		UtilReportEvent(
			eventWarning,
			ONLINE_DEFRAG_CATEGORY,
			OLDSLV_ERROR_ID,
			2,
			rgszT );

		AssertTracking();
		}
	else
		{
		UtilReportEvent(
			eventInformation,
			ONLINE_DEFRAG_CATEGORY,
			OLDSLV_STOP_ID,
			1,
			rgszT );
		}
		
	
	critOLD.Enter();
	if ( !poldstatSLV->FTermRequested() )
		{
		//	we're terminating before the client asked
		poldstatSLV->ThreadEnd();
		poldstatSLV->Reset();
		}
	critOLD.Leave();

	return 0;
	}


#ifdef OLD_SCRUB_DB


//  ================================================================
ULONG OLDScrubDb( DWORD_PTR dw )
//  ================================================================
	{
	const CPG cpgPreread = 256;

	ERR				err;
	IFMP			ifmp			= (IFMP)dw;
	PIB				*ppib			= ppibNil;
	BOOL			fOpenedDb		= fFalse;
	SCRUBDB 		* pscrubdb 		= NULL;

	const ULONG_PTR ulSecStart = UlUtilGetSeconds();
	
	CallR( ErrPIBBeginSession( PinstFromIfmp( ifmp ), &ppib, procidNil, fFalse ) );
	Assert( ppibNil != ppib );

	ppib->SetFUserSession();					//	we steal a user session in order to do OLD
///	ppib->SetSessionOLD();
	ppib->grbitsCommitDefault = JET_bitCommitLazyFlush;

	Call( ErrDBOpenDatabaseByIfmp( ppib, ifmp ) );
	fOpenedDb = fTrue;

	pscrubdb = new SCRUBDB( ifmp );
	if( NULL == pscrubdb )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	while( FOLDContinue( ifmp ) )
		{
		const CHAR * rgszT[1];
		INT isz = 0;
		
		rgszT[isz++] = rgfmp[ifmp].SzDatabaseName();
		Assert( isz <= sizeof( rgszT ) / sizeof( rgszT[0] ) );
		
		UtilReportEvent( eventInformation, DATABASE_ZEROING_CATEGORY, DATABASE_ZEROING_STARTED_ID, isz, rgszT );				
		
		PGNO pgnoLast;
		pgnoLast = rgfmp[ifmp].PgnoLast();

		DBTIME dbtimeLastScrubNew;
		dbtimeLastScrubNew = rgfmp[ifmp].Dbtime();
		
		Call( pscrubdb->ErrInit( ppib, CUtilProcessProcessor() ) );

		PGNO pgno;
		pgno = 1;

		while( pgnoLast	>= pgno && FOLDContinue( ifmp ) )
			{
			ULONG_PTR cpgCache;
			CallS( ErrBFGetCacheSize( &cpgCache ) );
			
			const CPG cpgChunk 		= 256;
			const CPG cpgPreread 	= min( cpgChunk, pgnoLast - pgno + 1 );
			BFPrereadPageRange( ifmp, pgno, cpgPreread );

			Call( pscrubdb->ErrScrubPages( pgno, cpgPreread ) );
			pgno += cpgPreread;

			while( ( pscrubdb->Scrubstats().cpgSeen + ( cpgCache / 4 ) ) < pgno
				   && FOLDContinue( ifmp ) )
				{
				UtilSleep( cmsecWaitGeneric );
				}
			}

		Call( pscrubdb->ErrTerm() );

		if( pgno > pgnoLast )
			{
			//  we completed a pass
			rgfmp[ifmp].Pdbfilehdr()->dbtimeLastScrub = dbtimeLastScrubNew;
			LGIGetDateTime( &rgfmp[ifmp].Pdbfilehdr()->logtimeScrub );
			}

			{
			const ULONG_PTR ulSecFinished 	= UlUtilGetSeconds();
			const ULONG_PTR ulSecs 			= ulSecFinished - ulSecStart;

			const CHAR * rgszT[16];
			INT isz = 0;

			CHAR	szSeconds[16];
			CHAR	szErr[16];
			CHAR	szPages[16];
			CHAR	szBlankPages[16];
			CHAR	szUnchangedPages[16];
			CHAR	szUnusedPages[16];
			CHAR	szUsedPages[16];
			CHAR	szDeletedRecordsZeroed[16];
			CHAR	szOrphanedLV[16];

			sprintf( szSeconds, "%d", ulSecs );
			sprintf( szErr, "%d", err );
			sprintf( szPages, "%d", pscrubdb->Scrubstats().cpgSeen );
			sprintf( szBlankPages, "%d", pscrubdb->Scrubstats().cpgUnused );
			sprintf( szUnchangedPages, "%d", pscrubdb->Scrubstats().cpgUnchanged );
			sprintf( szUnusedPages, "%d", pscrubdb->Scrubstats().cpgZeroed );
			sprintf( szUsedPages, "%d", pscrubdb->Scrubstats().cpgUsed );
			sprintf( szDeletedRecordsZeroed, "%d", pscrubdb->Scrubstats().cFlagDeletedNodesZeroed );
			sprintf( szOrphanedLV, "%d", pscrubdb->Scrubstats().cOrphanedLV );

			rgszT[isz++] = rgfmp[ifmp].SzDatabaseName();
			rgszT[isz++] = szSeconds;
			rgszT[isz++] = szErr;
			rgszT[isz++] = szPages;
			rgszT[isz++] = szBlankPages;
			rgszT[isz++] = szUnchangedPages;
			rgszT[isz++] = szUnusedPages;
			rgszT[isz++] = szUsedPages;
			rgszT[isz++] = szDeletedRecordsZeroed;
			rgszT[isz++] = szOrphanedLV;
			
			Assert( isz <= sizeof( rgszT ) / sizeof( rgszT[0] ) );
			UtilReportEvent( eventInformation, DATABASE_ZEROING_CATEGORY, DATABASE_ZEROING_STOPPED_ID, isz, rgszT );		
			}

		//  wait one minute before starting again
		pinst->m_rgoldstatDB[ rgfmp[ifmp].Dbid() ].FWaitOnSignal( 60 * 1000 );
		}

HandleError:
	if ( fOpenedDb )
		{
		CallS( ErrDBCloseAllDBs( ppib ) );
		}

	if( NULL != pscrubdb )
		{
		(VOID)pscrubdb->ErrTerm();
		delete pscrubdb;
		}

	Assert( ppibNil != ppib );	
	PIBEndSession( ppib );	
	
	return 0;
	}


#endif	//	OLD_SCRUB_DB


//  ================================================================
ERR ErrOLDSLVStart(
	const IFMP ifmp,
	ULONG * const pcPasses,
	ULONG * const pcsec,
	const JET_CALLBACK callback,
	const JET_GRBIT grbit )
//  ================================================================
	{
	Assert( JET_bitDefragmentSLVBatchStart == grbit );
	Assert( critOLD.FOwner() );

	JET_ERR					err				= JET_errSuccess;
	INST * const			pinst			= PinstFromIfmp( ifmp );
	OLDSLV_STATUS * const	poldstatSLV		= pinst->m_rgoldstatSLV + rgfmp[ifmp].Dbid();
	const LONG				fOLDLevel		= ( pinst->m_fOLDLevel & JET_OnlineDefragAllOBSOLETE ?
														JET_OnlineDefragAll :
														pinst->m_fOLDLevel );

	if ( poldstatSLV->FRunning() )
		{
		err = ErrERRCheck( JET_wrnDefragAlreadyRunning );
		}
	else if ( !( fOLDLevel & JET_OnlineDefragStreamingFiles ) )
		{
		err = JET_errSuccess;
		}
	else
		{
		poldstatSLV->Reset();
		
		if ( NULL != pcPasses )
			poldstatSLV->SetCPassesMax( *pcPasses );

		poldstatSLV->SetCsecStart( UlUtilGetSeconds() );
		if ( NULL != pcsec && *pcsec > 0 )
			poldstatSLV->SetCsecMax( poldstatSLV->CsecStart() + *pcsec );

		if ( NULL != callback )
			poldstatSLV->SetCallback( callback );

		err = poldstatSLV->ErrThreadCreate( ifmp, OLDDefragDbSLV );
		}

	Assert( critOLD.FOwner() );
	return err;
	}


//  ================================================================
ERR ErrOLDSLVStop(
	const IFMP ifmp,
	ULONG * const pcPasses,
	ULONG * const pcsec,
	const JET_CALLBACK callback,
	const JET_GRBIT grbit )
//  ================================================================
	{
	Assert( JET_bitDefragmentSLVBatchStop == grbit );
	Assert( critOLD.FOwner() );
	
	ERR						err					= JET_errSuccess;
	INST * const			pinst				= PinstFromIfmp( ifmp );
	OLDSLV_STATUS * const	poldstatSLV			= pinst->m_rgoldstatSLV + rgfmp[ifmp].Dbid();
	const BOOL				fReturnPassCount	= ( NULL != pcPasses );
	const BOOL				fReturnElapsedTime	= ( NULL != pcsec );
	const LONG				fOLDLevel			= ( pinst->m_fOLDLevel & JET_OnlineDefragAllOBSOLETE ?
															JET_OnlineDefragAll :
															pinst->m_fOLDLevel );

	if ( !poldstatSLV->FRunning()
		|| poldstatSLV->FTermRequested() )		//	someone else beat us to it, or the thread is terminating itself
		{
		//	if OLDSLV was force-disabled, then just
		//	report success instead of a warning
		err = ( fOLDLevel & JET_OnlineDefragStreamingFiles ?
					ErrERRCheck( JET_wrnDefragNotRunning ) :
					JET_errSuccess );
		}
	else
		{
		poldstatSLV->SetFTermRequested();
		critOLD.Leave();
				
		poldstatSLV->SetSignal();
		if ( poldstatSLV->FRunning() )
			{
			poldstatSLV->ThreadEnd();
			}

		critOLD.Enter();

		if ( fReturnPassCount )
			*pcPasses = poldstatSLV->CPasses();
		if ( fReturnElapsedTime )
			*pcsec = (ULONG)( UlUtilGetSeconds() - poldstatSLV->CsecStart() );

		poldstatSLV->Reset();

		err = JET_errSuccess;	
		}

	Assert( critOLD.FOwner() );
	return err;
	}


ERR ErrOLDDefragment(
	const IFMP		ifmp,
	const CHAR *	szTableName,
	ULONG *			pcPasses,
	ULONG *			pcsec,
	JET_CALLBACK	callback,
	JET_GRBIT		grbit )
	{
	ERR				err;
	const BOOL		fReturnPassCount	= ( NULL != pcPasses && ( grbit & JET_bitDefragmentBatchStop ) );
	const BOOL		fReturnElapsedTime	= ( NULL != pcsec && ( grbit & JET_bitDefragmentBatchStop ) );
	INST * const	pinst				= PinstFromIfmp( ifmp );
	const LONG		fOLDLevel			= ( pinst->m_fOLDLevel & JET_OnlineDefragAllOBSOLETE ?
												JET_OnlineDefragAll :
												pinst->m_fOLDLevel );
	const BOOL		fAvailExtOnly		= ( ( grbit & JET_bitDefragmentAvailSpaceTreesOnly ) ?
												( fOLDLevel & (JET_OnlineDefragDatabases|JET_OnlineDefragSpaceTrees) ) :
												( ( fOLDLevel & JET_OnlineDefragSpaceTrees )
													&& !( fOLDLevel & JET_OnlineDefragDatabases ) ) );
	grbit &= ~JET_bitDefragmentAvailSpaceTreesOnly;

	Assert( !pinst->FRecovering() );
	Assert( !fGlobalRepair );

	if ( fReturnPassCount )
		*pcPasses = 0;

	if ( fReturnElapsedTime )
		*pcsec = 0;

	if ( 0 == fOLDLevel )
		return JET_errSuccess;
	
	if ( dbidTemp == rgfmp[ ifmp ].Dbid() )
		{
		err = ErrERRCheck( JET_errInvalidDatabaseId );
		return err;
		}
	Assert( rgfmp[ ifmp ].Dbid() > 0 );

	if ( rgfmp[ ifmp ].FReadOnlyAttach() )
		{
		err = ErrERRCheck( JET_errDatabaseFileReadOnly );
		return err;
		}

	critOLD.Enter();

	OLDDB_STATUS * const		poldstatDB	= pinst->m_rgoldstatDB + rgfmp[ifmp].Dbid();

	switch( grbit )
		{			
		case JET_bitDefragmentSLVBatchStart:
			err = ErrOLDSLVStart( ifmp, pcPasses, pcsec, callback, grbit );
			break;
		case JET_bitDefragmentSLVBatchStop:
			err = ErrOLDSLVStop( ifmp, pcPasses, pcsec, callback, grbit );
			break;
			
		case JET_bitDefragmentBatchStart:
			if ( poldstatDB->FRunning() )
				{
				err = ErrERRCheck( JET_wrnDefragAlreadyRunning );
				}
			else if ( !( fOLDLevel & JET_OnlineDefragDatabases )
					&& !fAvailExtOnly )
				{
				err = JET_errSuccess;
				}
			else
				{
				poldstatDB->Reset();
				
				if ( fAvailExtOnly )
					poldstatDB->SetFAvailExtOnly();

				if ( NULL != pcPasses )
					poldstatDB->SetCPassesMax( *pcPasses );

				poldstatDB->SetCsecStart( UlUtilGetSeconds() );
				if ( NULL != pcsec && *pcsec > 0 )
					poldstatDB->SetCsecMax( poldstatDB->CsecStart() + *pcsec );

				if ( NULL != callback )
					poldstatDB->SetCallback( callback );

				err = poldstatDB->ErrThreadCreate( ifmp, OLDDefragDb );

#ifdef OLD_SCRUB_DB
				if( err >= 0 )
					{
					//	UNDONE: We currently don't clean up the thread handle correctly.  Must
					//	fix the code if this ever gets enabled.
					EnforceSz( fFalse, "Scrubbing during OLD not yet supported" );

					err = poldstatDB->ErrScrubThreadCreate( ifmp );
					}
#endif	//	OLD_SCRUB_DB
				}
			break;
		case JET_bitDefragmentBatchStop:
			if ( !poldstatDB->FRunning()
				|| poldstatDB->FTermRequested() )		//	someone else beat us to it, or the thread is terminating itself
				{
				if ( fOLDLevel & (JET_OnlineDefragDatabases|JET_OnlineDefragSpaceTrees) )
					{
					err = ErrERRCheck( JET_wrnDefragNotRunning );
					}
				else
					{
					//	OLD was force-disabled for this database,
					//	so just report success instead of a warning
					err = JET_errSuccess;
					}
				}
			else
				{
				poldstatDB->SetFTermRequested();
				critOLD.Leave();
				
				poldstatDB->SetSignal();
				if ( poldstatDB->FRunning() )
					{
					poldstatDB->ThreadEnd();
#ifdef OLD_SCRUB_DB				
					poldstatDB->ScrubThreadEnd();
#endif					
					}

				critOLD.Enter();

				if ( fReturnPassCount )
					*pcPasses = poldstatDB->CPasses();
				if ( fReturnElapsedTime )
					*pcsec = (ULONG)( UlUtilGetSeconds() - poldstatDB->CsecStart() );

				poldstatDB->Reset();

				err = JET_errSuccess;	
				}
			break;
			
		default:
			err = ErrERRCheck( JET_errInvalidGrbit );
		}

	critOLD.Leave();

	return err;
	}


ERR ErrIsamDefragment(
	JET_SESID	vsesid,
	JET_DBID	vdbid,
	const CHAR	*szTableName,
	ULONG		*pcPasses,
	ULONG		*pcsec,
	JET_CALLBACK callback,
	JET_GRBIT	grbit )
	{
	ERR			err;
	PIB			*ppib		= (PIB *)vsesid;
	const IFMP	ifmp		= IFMP( DBID( vdbid ) );
	INST* const	pinst		= PinstFromPpib( ppib );

	CallR( ErrPIBCheck( ppib ) );
	CallR( ErrPIBCheckIfmp( ppib, ifmp ) );
	CallR( ErrPIBCheckUpdatable( ppib ) );

	if ( JET_bitDefragmentScrubSLV == grbit )
		{
		CPG cpgSeen;
		CPG cpgScrubbed;
		CallR( ErrSCRUBScrubStreamingFile( ppib, ifmp, &cpgSeen, &cpgScrubbed, NULL ) );
		}
	else
		{
		CallR( ErrOLDDefragment( ifmp, szTableName, pcPasses, pcsec, callback, grbit ) );
		}

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\oldslv.cxx ===
#include "std.hxx"

PM_CEF_PROC LOLDSLVChunkSizeCEFLPv;

long LOLDSLVChunkSizeCEFLPv( long iInstance, void* pvBuf )
	{
	Unused( iInstance );
	
	if ( pvBuf )
		{		
		*( (unsigned long*) pvBuf ) = SLVSPACENODE::cpageMap * g_cbPage;
		}
		
	return 0;
	}

//  ================================================================
ERR ErrOLDSLVChecksumSLVUsingFiles(
		PIB * const ppib,
		FUCB * const pfucb,
		const JET_COLUMNID columnid,
		ULONG * const pulChecksum )
//  ================================================================
	{
	*pulChecksum = 0;
	return JET_errSuccess;
	}


//  ================================================================
ERR ErrOLDSLVChecksumSLVUsingData(
		PIB * const ppib,
		FUCB * const pfucb,
		const JET_COLUMNID columnid,
		ULONG * const pulChecksum )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	VOID * pv = NULL;
	BFAlloc( &pv );

	*pulChecksum = 0;

	ULONG cbActual 		= 0;
	ULONG cbRetrieved 	= 0;
	
	err = ErrIsamRetrieveColumn(
		ppib,
		pfucb,
		columnid,
		NULL,
		0,
		&cbActual,
		NO_GRBIT,
		NULL );
	if( JET_errColumnNotFound == err )
		{
		//	this column must have been deleted
		err = JET_errSuccess;
		goto HandleError;
		}

	//  now retrieve the entire SLV and checksum it

	while( cbRetrieved < cbActual )
		{
		JET_RETINFO retinfo;
		retinfo.cbStruct 		= sizeof( retinfo );
		retinfo.ibLongValue 	= cbRetrieved;
		retinfo.itagSequence 	= 1;

		ULONG cbActualT = 0;
		Call( ErrIsamRetrieveColumn(
				ppib,
				pfucb,
				columnid,
				pv,
				g_cbPage,
				&cbActualT,
				NO_GRBIT,
				&retinfo ) );

		ULONG cbRetrievedThisCall;
		cbRetrievedThisCall = min( cbActualT, g_cbPage );
		ULONG cbToZero;
		cbToZero = g_cbPage - cbRetrievedThisCall;
		memset( (BYTE *)pv + cbRetrievedThisCall, 0, cbToZero );

		*pulChecksum = LOG::UlChecksumBytes( (BYTE *)pv, (BYTE *)pv + g_cbPage, *pulChecksum );

		//  on the last iteration we will retrieve less than g_cbPage, but we are about
		//  to fall out of the loop
		
		cbRetrieved += g_cbPage;				
		}

	
HandleError:
	Assert( NULL != pv );
	BFFree( pv );
	return err;
	}
	

//  ================================================================
ERR ErrOLDSLVChecksumSLV(
		PIB * const ppib,
		FUCB * const pfucb,
		const JET_COLUMNID columnid,
		ULONG * const pulChecksum )
//  ================================================================
//
//  retrive the SLV column and checksum it
//
//-
	{
	if( PinstFromPpib( ppib )->FSLVProviderEnabled() )
		{
		return ErrOLDSLVChecksumSLVUsingFiles( ppib, pfucb, columnid, pulChecksum );
		}
	return ErrOLDSLVChecksumSLVUsingData( ppib, pfucb, columnid, pulChecksum );
	}


class OLDSLVCTRL
	{

private:
	IFMP 		m_ifmp;
	PIB * 		m_ppib;
	
public:
	OLDSLVCTRL ( IFMP ifmp );
	~OLDSLVCTRL(  );

	ERR 	ErrInit(  );
	ERR 	ErrTerm(  );
	
	IFMP 	Ifmp() const { return m_ifmp; }
	INST *	Pinst() const { return PinstFromIfmp ( Ifmp() ); }
	PIB *	Ppib() const { return m_ppib; }

	ERR 	ErrMoveRun ( PGNO pgno );

private:
	ERR 	ErrGetSpaceMapNode( const PGNO pgno, SLVOWNERMAP * const pslvownermap );
	ERR 	ErrAllocateNewRun ( CSLVInfo::RUN & oldRun, CSLVInfo::RUN & newRun );
	ERR 	ErrDeallocateRun ( CSLVInfo::RUN & run, CSLVInfo & slvinfo );

	ERR 	ErrCopyRunToRun ( CSLVInfo::RUN & runSrc, CSLVInfo::RUN & runDest );

	ERR 	ErrAllocateNewSLVInfoDeallocateOldSLVInfo_(
				const OBJID		objid,
				const COLUMNID	columnid,
				const BOOKMARK&	bm,
				CSLVInfo		* const pslvinfo,
				CSLVInfo		* const pslvinfoNew );
	};

INLINE OLDSLVCTRL::OLDSLVCTRL(IFMP ifmp) : m_ifmp ( ifmp ), m_ppib ( ppibNil )
	{
	FMP::AssertVALIDIFMP( ifmp );
	}
	
INLINE OLDSLVCTRL::~OLDSLVCTRL()
	{	
	Assert ( ppibNil == m_ppib );
	}

ERR OLDSLVCTRL::ErrInit()
	{
	ERR 	err 	= JET_errSuccess;
	FMP * 	pfmp 	= &rgfmp[ Ifmp() ];

	Assert ( NULL != Pinst() );
	
	CallR( ErrPIBBeginSession( Pinst(), &m_ppib, procidNil, fFalse ) );

	m_ppib->SetFUserSession();				//	we steal a user session in order to do OLD
	m_ppib->SetFSessionOLDSLV();
	m_ppib->grbitsCommitDefault = JET_bitCommitLazyFlush;

	Call ( ErrDBOpenDatabaseByIfmp( m_ppib, m_ifmp ) );

	return err;
	
HandleError:
	Assert ( ppibNil != m_ppib );
	PIBEndSession( m_ppib );
	m_ppib = ppibNil;
	
	return err;
	}
	
ERR OLDSLVCTRL::ErrTerm()
	{
	ERR 	err 	= JET_errSuccess;

	Call ( ErrDBCloseDatabase( m_ppib, Ifmp(), 0) );

	if ( ppibNil != m_ppib )
		{
		PIBEndSession( m_ppib );
		m_ppib = ppibNil;
		}
	
HandleError:
	return err;
	}

INLINE ERR OLDSLVCTRL::ErrDeallocateRun ( CSLVInfo::RUN & run, CSLVInfo & slvinfo )
	{
	ERR 				err 			= JET_errSuccess; 
	CSLVInfo::FILEID 	fileid;
	QWORD				cbAlloc;
	wchar_t 			wszFileName[ IFileSystemAPI::cchPathMax ];
	
	Call( slvinfo.ErrGetFileID( &fileid ) );
	Call( slvinfo.ErrGetFileAlloc( &cbAlloc ) );
	Call( slvinfo.ErrGetFileName( wszFileName ) );
		
	Call( ErrSLVDeleteCommittedRange(	m_ppib,
										Ifmp(),
										run.ibLogical,
										run.cbSize,
										fileid,
										cbAlloc,
										wszFileName ) );
HandleError:
	return err;
	}

INLINE ERR OLDSLVCTRL::ErrGetSpaceMapNode( const PGNO pgno, SLVOWNERMAP * const pslvownermap )
	{
	ERR 	err 	= JET_errSuccess;

	Call( ErrSLVOwnerMapGet( m_ppib, Ifmp(), pgno, pslvownermap ) );
	
HandleError:
	return err;	
	}


//  ================================================================
ERR OLDSLVCTRL::ErrAllocateNewSLVInfoDeallocateOldSLVInfo_(
	const OBJID		objid,
	const COLUMNID	columnid,
	const BOOKMARK&	bm,
	CSLVInfo		* const pslvinfoOld,
	CSLVInfo		* const pslvinfoNew )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	CSLVInfo::HEADER 	headerOld;
	CSLVInfo::HEADER 	headerNew;
	
	CSLVInfo::FILEID	fileid;
	QWORD				cbAlloc;
	
	QWORD			cbTotal 				= 0;
	__int64			cbRemainingToAllocate 	= 0;
	QWORD 			ibVirtual 				= 0;

	size_t 			cwchFileName;
	wchar_t 		wszFileName[ IFileSystemAPI::cchPathMax ];

	Call( pslvinfoOld->ErrGetHeader( &headerOld ) );
	cbTotal 				= headerOld.cbSize;
	cbRemainingToAllocate 	= cbTotal;

	Call( pslvinfoNew->ErrGetHeader( &headerNew ) );
	headerNew.cbSize			= headerOld.cbSize;
	headerNew.cRun				= 0;
	headerNew.fDataRecoverable	= headerOld.fDataRecoverable;
	headerNew.rgbitReserved_31	= headerOld.rgbitReserved_31;
	headerNew.rgbitReserved_32	= headerOld.rgbitReserved_32;
			
	//  allocate enough pages in the copy-buffer SLVINFO
	//  update the space map with all the pages in the new SLVINFO
	//  a zero-length SLV needs at least one run so go through
	//  the loop at least once

	do
		{
		
		CSLVInfo::RUN runNew;

		QWORD ibLogical = 0;
		QWORD cbSize 	= 0;
		
		Call( ErrSLVGetRange(
					m_ppib,
					Ifmp(),
					max( cbRemainingToAllocate, 1 ),
					&ibLogical,
					&cbSize,
					fFalse,
					fTrue ) );

		runNew.ibVirtualNext	= ibVirtual + cbSize;
		runNew.ibLogical		= ibLogical;
		runNew.qwReserved		= 0;
		runNew.ibVirtual		= ibVirtual;
		runNew.cbSize			= cbSize;
		runNew.ibLogicalNext	= ibLogical + cbSize;
		
		Call( ErrSLVOwnerMapSetUsageRange(
				m_ppib,
				Ifmp(),
				runNew.PgnoFirst(),
				runNew.Cpg(),
				objid,
				columnid,
				const_cast<BOOKMARK *>( &bm ),
				fSLVOWNERMAPSetSLVCopyOLD ) );

		Call( pslvinfoNew->ErrMoveAfterLast() );
		Call( pslvinfoNew->ErrSetCurrentRun( runNew ) );
		++(headerNew.cRun);

		cbRemainingToAllocate -= cbSize;
		ibVirtual += cbSize;
		
		} while( cbRemainingToAllocate > 0 );
		
	// update the column in the copy buffer

	Call( pslvinfoNew->ErrSetHeader( headerNew ) );
	Call( pslvinfoNew->ErrSave() );
	
	//  release all the space in the old SLVINFO
	
	Call( pslvinfoOld->ErrGetFileID( &fileid ) );
	Call( pslvinfoOld->ErrGetFileAlloc( &cbAlloc ) );
	Call( pslvinfoOld->ErrGetFileNameLength( &cwchFileName ) );
	Call( pslvinfoOld->ErrGetFileName( wszFileName ) );

	Call( pslvinfoOld->ErrMoveBeforeFirst() );
	while( ( err = pslvinfoOld->ErrMoveNext() ) >= JET_errSuccess )
		{		
		CSLVInfo::RUN run;
		
		Call( pslvinfoOld->ErrGetCurrentRun( &run ) );

		Call( ErrSLVDeleteCommittedRange(
					m_ppib,
					Ifmp(),
					run.ibLogical,
					run.cbSize,
					fileid,
					cbAlloc,
					wszFileName ) );
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
		
HandleError:
	return err;
	}
	

//  ================================================================
ERR OLDSLVCTRL::ErrMoveRun( PGNO pgno )
//  ================================================================
	{
	ERR 						err 			= JET_errSuccess;
	OBJID 						objidTable		= objidNil;
	FUCB * 						pfucbTable 		= pfucbNil;
	
	const ULONG 				itagSequence 	= 1;	//  UNDONE:  support multi-valued SLVs

	BOOL						fInTransaction	= fFalse;

	SLVOWNERMAP 				record;
	CSLVInfo 					slvinfo;
	CSLVInfo 					slvinfoNew;

	char						szTableName[JET_cbNameMost + 1];

	BOOKMARK 					bm;
	DATA 						dataNull;

	Assert( 0 == m_ppib->level );
	Call( ErrDIRBeginTransaction( m_ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

	Call( ErrGetSpaceMapNode( pgno, &record ) );
	if( objidNil == record.Objid() )
		{
		//	the record that now owns this data has not committed yet
		
		Call( ErrERRCheck( errOLDSLVUnableToMove ) );
		
		}
	Call( ErrCATGetObjectNameFromObjid( m_ppib, Ifmp(), record.Objid(), sysobjTable, record.Objid(), szTableName, sizeof(szTableName) ) );
	err = ErrFILEOpenTable( m_ppib, Ifmp(), &pfucbTable, szTableName, 0 );
	if( JET_errTableLocked == err )
		{
		Call( ErrERRCheck( errOLDSLVUnableToMove ) );
		}
	Call( err );
	
	//  force the LV FCB into memory into memory if it isn't there already
	//  if the table doesn't actually have a separate LV tree this will be a perf problem
	
	if( pfcbNil == pfucbTable->u.pfcb->Ptdb()->PfcbLV() )
		{
		FUCB * pfucbLV = pfucbNil;
		
		Call( ErrFILEOpenLVRoot( pfucbTable, &pfucbLV, fFalse ) );
		if( wrnLVNoLongValues != err )
			{
			DIRClose( pfucbLV );
			}
		}
	
	bm.Nullify();
	bm.key.suffix.SetCb( record.CbKey() );
	bm.key.suffix.SetPv( record.PvKey() );

	//  goto the record 
	
	Call( ErrIsamGotoBookmark( m_ppib, pfucbTable, record.PvKey(), record.CbKey() ) );

	//  get a waitlock on the record. if we get a write-conflict back off and retry
	//  if anyone else sees the waitlock they will block until we commit to level 0

	Assert( 1 == m_ppib->level );
	err = ErrDIRGetLock( pfucbTable, waitLock );
	if( JET_errWriteConflict == err )
		{
		Call( ErrERRCheck( errOLDSLVUnableToMove ) );
		}
	Call( err );

#ifdef DEBUG
	ULONG ulChecksumBefore;
	Call( ErrOLDSLVChecksumSLV( m_ppib, pfucbTable, record.Columnid(), &ulChecksumBefore ) );
#endif	//	DEBUG		
			
	Call( ErrIsamPrepareUpdate( m_ppib, pfucbTable, JET_prepReplace ) );
	PIBSetUpdateid( m_ppib, pfucbTable->updateid );
	
	//  load the source slvinfo from the record
	
	Call( slvinfo.ErrLoad( pfucbTable, record.Columnid(), itagSequence, fFalse ) );

	//  set the SLVINFO in the copy-buffer to NULL
	
	dataNull.Nullify();
	Call( ErrRECSetLongField(	pfucbTable,
								record.Columnid(),
								itagSequence,
								&dataNull,
								NO_GRBIT,
								0,
								0 ) );
	
	//  create the destination SLV in the copy buffer
	
	Call( slvinfoNew.ErrCreate(	pfucbTable, record.Columnid(), itagSequence, fTrue ) );

	//  allocate space in the new SLVINFO, free the space in the old
	
	Call( ErrAllocateNewSLVInfoDeallocateOldSLVInfo_( record.Objid(), record.Columnid(), bm, &slvinfo, &slvinfoNew ) );

	//  copy data between the two SLVInfo's. This will set buffer dependencies and log the move
	
	Call( ErrSLVMove( m_ppib, Ifmp(), slvinfo, slvinfoNew ) );

	//  replace the record
	
	Call( ErrIsamUpdate( m_ppib, pfucbTable, NULL, 0, NULL, fDIRLogColumnDiffs ) );

#ifdef DEBUG

	//  check that the move didn't corrupt the SLV
	
	ULONG ulChecksumAfter;
	Call( ErrOLDSLVChecksumSLV( m_ppib, pfucbTable, record.Columnid(), &ulChecksumAfter ) );
	Assert( ulChecksumAfter == ulChecksumBefore );
	
#endif	//	DEBUG		

	//  commit the transaction and purge all the versions
	
	Call( ErrDIRCommitTransaction( m_ppib, JET_bitCommitLazyFlush ) );
	fInTransaction 			= fFalse;

HandleError:

	slvinfo.Unload();
	slvinfoNew.Unload();

	if( pfucbNil != pfucbTable )
		{
		CallS( ErrFILECloseTable( m_ppib, pfucbTable ) );
		pfucbTable = pfucbNil;
		}
		
	if( fInTransaction )
		{
		Assert( 1 == m_ppib->level );
		CallSx( ErrDIRRollback( m_ppib ), JET_errRollbackError );
		}

	Assert( JET_errWriteConflict != err );		
	return err;	
	}

//  BUGBUG:  there is no error handling in OLDSLVCTRL::ErrCopyRunToRun()

ERR OLDSLVCTRL::ErrCopyRunToRun ( CSLVInfo::RUN & runSrc, CSLVInfo::RUN & runDest )
	{
	ERR 	err 			= JET_errSuccess; 
	CPG 	cpg 			= 0;
	IFMP	ifmp			= Ifmp() | ifmpSLV;
	BFLatch	bflSrc;
	BFLatch	bflDest;

	Assert( runSrc.Cpg() == runDest.Cpg() );

	for (int i = 0; i < runSrc.Cpg(); i++)
		{
		Call( ErrBFReadLatchPage( &bflSrc, ifmp, runSrc.PgnoFirst() + i, bflfNoTouch) );
		
		Call( ErrBFWriteLatchPage( &bflDest, ifmp, runDest.PgnoFirst() + i, bflfNew ) );

		BFDirty( &bflDest );

		UtilMemCpy( (BYTE*)bflDest.pv, (BYTE*)bflSrc.pv, g_cbPage );

		BFWriteUnlatch( &bflDest );
		BFReadUnlatch( &bflSrc );
		}
		
HandleError:
	return err;
	}


ERR OLDSLVCTRL::ErrAllocateNewRun ( CSLVInfo::RUN & oldRun, CSLVInfo::RUN & newRun )
	{
	PGNO 	pgnoFirstNew 	= pgnoNull;
	ERR 	err 			= JET_errSuccess; 
	CPG 	cpg 			= oldRun.Cpg();
	
	// UNDONE SLVOWNERMAP must be a new alloc function, this will return always all space requested on success
	err = ErrSLVGetPages( m_ppib, Ifmp(), &pgnoFirstNew, &cpg, fFalse, fTrue );

	if (err >= JET_errSuccess)
		{
		newRun = oldRun;
		Assert ( cpg == oldRun.Cpg() );
		Assert ( cpg == newRun.Cpg() );
		newRun.ibLogical = OffsetOfPgno( pgnoFirstNew );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVDeleteTrailingNodes( PIB * const ppib, FUCB * const pfucb, const INT cNodesToDelete )
//  ================================================================
//
//  Deletes the given number of nodes from the end of the table
//
//-
	{
	ERR err = JET_errSuccess;

	//  Move to the end of the table
	
	DIB dib;
	dib.pos 	= posLast;
	dib.dirflag = fDIRNull;
	dib.pbm		= NULL;
	Call( ErrBTDown( pfucb, &dib, latchReadTouch ) );

	//  Delete a record and move prev

	INT cNodesDeleted;
	cNodesDeleted = 0;

	for( cNodesDeleted = 0; cNodesDeleted < cNodesToDelete; ++cNodesDeleted )
		{
		Call( ErrBTFlagDelete( pfucb, fDIRNull, NULL ) );
		Pcsr( pfucb )->Downgrade( latchReadTouch );
		Call( ErrBTPrev( pfucb, fDIRNull ) );
		}
	
HandleError:
	return err;
	}

	
//  ================================================================
LOCAL ERR ErrOLDSLVShrinkSLVAvail( PIB * const ppib, const IFMP ifmp, const QWORD cbShrink )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbSLVAvail = pfucbNil;
	FCB * const pfcbSLVAvail = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcbSLVAvail );
	Assert( rgfmp[ifmp].RwlSLVSpace().FWriter() );

	Assert( 0 == ( cbShrink % ( SLVSPACENODE::cpageMap * g_cbPage ) ) ); 
	const INT cNodesToDelete = INT( cbShrink / ( SLVSPACENODE::cpageMap * g_cbPage ) );
	
	//  Open the SVLAvail Tree
	
	Call( ErrBTOpen( ppib, pfcbSLVAvail, &pfucbSLVAvail, fFalse ) );

	//  Delete the nodes

	Call( ErrOLDSLVDeleteTrailingNodes( ppib, pfucbSLVAvail, cNodesToDelete ) );
		
HandleError:
	if( pfucbNil != pfucbSLVAvail )
		{
		BTClose( pfucbSLVAvail );
		pfucbSLVAvail = pfucbNil;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVShrinkSLVOwnerMap( PIB * const ppib, const IFMP ifmp, const QWORD cbShrink )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbSLVOwnerMap = pfucbNil;
	FCB * const pfcbSLVOwnerMap = rgfmp[ifmp].PfcbSLVOwnerMap();
	Assert( pfcbNil != pfcbSLVOwnerMap );

	Assert( 0 == ( cbShrink % g_cbPage ) ); 
	const INT cNodesToDelete = INT( cbShrink / g_cbPage );

	//  Open the SVLSpaceMap Tree
	
	Call( ErrBTOpen( ppib, pfcbSLVOwnerMap, &pfucbSLVOwnerMap, fFalse ) );

	//  Delete the nodes

	Call( ErrOLDSLVDeleteTrailingNodes( ppib, pfucbSLVOwnerMap, cNodesToDelete ) );
	
HandleError:
	if( pfucbNil != pfucbSLVOwnerMap )
		{
		BTClose( pfucbSLVOwnerMap );
		pfucbSLVOwnerMap = pfucbNil;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVShrinkSLVFile( PIB * const ppib, const IFMP ifmp, const QWORD cbShrink )
//  ================================================================
// 
//  If this fails the file should still be the old size
//
//-
	{
	ERR err = JET_errSuccess;

	const QWORD cbTrueSizeOld 	= rgfmp[ifmp].CbTrueSLVFileSize();
	const QWORD cbTrueSizeNew	= cbTrueSizeOld - cbShrink;

	const CPG cpgOld		= rgfmp[ifmp].PgnoSLVLast();
	const QWORD cbSizeOld	= QWORD( cpgOld ) * g_cbPage;
	const QWORD cbSizeNew 	= cbSizeOld - cbShrink;

	Assert( cbTrueSizeNew < cbTrueSizeOld );
	Assert( cbSizeNew < cbSizeOld );
	
	//  truncate the file
	
	Call( rgfmp[ifmp].PfapiSLV()->ErrSetSize( cbTrueSizeNew ) );
	CallS( err );

	//	set database size in FMP -- this value should NOT include the reserved pages
	
	Assert( cbSizeNew < cbTrueSizeNew );
	Assert( cbSizeNew == cbTrueSizeNew - ( cpgDBReserved * g_cbPage ) );
	rgfmp[ifmp].SetSLVFileSize( cbSizeNew );

	rgfmp[ ifmp ].IncSLVSpaceCount( SLVSPACENODE::sFree, -CPG( cbShrink / SLVPAGE_SIZE ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVShrinkSLVSpaceCache( PIB * const ppib, const IFMP ifmp, const QWORD cbShrink )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	SLVSPACENODECACHE * const pspacenodecache = rgfmp[ifmp].Pslvspacenodecache();
	Assert( pspacenodecache );

	if( pspacenodecache )
		{
		const CPG cpgOld	= pspacenodecache->CpgCacheSize();
		const CPG cpgShrink = CPG( cbShrink >> g_shfCbPage );
		const CPG cpgNew	= cpgOld - cpgShrink;
	
		Call( pspacenodecache->ErrShrinkCache( cpgNew ) );
		}
	
HandleError:
	CallS( err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVShrinkSLV( PIB * const ppib, const IFMP ifmp, const QWORD cbShrink )
//  ================================================================
//
//  Shrink the SLV file by the given number of bytes
//  We should have the writer portion of the reader/writer SLV Space lock
//
//-
	{
	ERR err = JET_errSuccess;
	BOOL fInTransaction = fFalse;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

	//  Shrink all the structures that depend on the size of the SLV file
	//  We can't rollback the shrinking of the SLV so do it once the other logged
	//  operations are done
	
	Call( ErrOLDSLVShrinkSLVAvail( ppib, ifmp, cbShrink ) );
	Call( ErrOLDSLVShrinkSLVOwnerMap( ppib, ifmp, cbShrink ) );

	Assert( fInTransaction );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fFalse;
	Assert( 0 == ppib->level );	

	//  If this fails, the SLV will be too large
	//  We cannot do it before the commit because the commit might fail, leaving the SLV too small
	//  If we fail, the size of the file will be reset on the next startup
	
	Call( ErrOLDSLVShrinkSLVFile( ppib, ifmp, cbShrink ) );

	//  Shrink the in-memory cache last -- it can't fail and can't rollback
	
	CallS( ErrOLDSLVShrinkSLVSpaceCache( ppib, ifmp, cbShrink ) );

	Assert( !fInTransaction );
HandleError:
	if( fInTransaction )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		fInTransaction = fFalse;
		}		
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVGetShrinkableSpace( PIB * const ppib, const IFMP ifmp, QWORD * const pcbShrinkable )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbSLVAvail = pfucbNil;
	FCB * const pfcbSLVAvail = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcbSLVAvail );
	Assert( rgfmp[ifmp].RwlSLVSpace().FWriter() );
	
	*pcbShrinkable = 0;

	//  Open the SVLAvail Tree
	
	Call( ErrBTOpen( ppib, pfcbSLVAvail, &pfucbSLVAvail, fFalse ) );
	Assert( pfucbNil != pfucbSLVAvail );

	//  Seek to the end of the tree
	
	DIB dib;
	dib.pos 	= posLast;
	dib.dirflag = fDIRNull;
	dib.pbm		= NULL;
	Call( ErrBTDown( pfucbSLVAvail, &dib, latchReadTouch ) );
	
	PGNO pgnoLastPrev;
	pgnoLastPrev = 0;

	//  Move backwards through the tree counting completely empty chunks
	
	while( 1 )
		{

		//  CONSIDER:  make these runtime checks to detect corruption
		
		Assert( sizeof( SLVSPACENODE ) == pfucbSLVAvail->kdfCurr.data.Cb() );
		Assert( sizeof( PGNO ) == pfucbSLVAvail->kdfCurr.key.Cb() );
		
		PGNO pgnoLast;
		LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
		const SLVSPACENODE * const pspacenode = reinterpret_cast<SLVSPACENODE *>( pfucbSLVAvail->kdfCurr.data.Pv() );
		const CPG cpgAvail = pspacenode->CpgAvail();

		if( 0 != pgnoLastPrev 
			&& pgnoLast != pgnoLastPrev - SLVSPACENODE::cpageMap )
			{
			AssertSz( fFalse, "Corruption in the SLVAvail tree" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		pgnoLastPrev = pgnoLast;
			
		if( SLVSPACENODE::cpageMap != cpgAvail )
			{
			break;
			}
		
		err = ErrBTPrev( pfucbSLVAvail, fDIRNull );			
		if( JET_errNoCurrentRecord == err )
			{
			//  the entire tree is empty
			//  break out of the loop before we record this space
			//  we don't want to shrink the SLV file to 0 bytes!
			err = JET_errSuccess;
			break;
			}
		Call( err );

		//  This node is completely empty. Record its space

		*pcbShrinkable += SLVSPACENODE::cpageMap * g_cbPage;			
		}

HandleError:
	if( pfucbNil != pfucbSLVAvail )
		{
		BTClose( pfucbSLVAvail );
		pfucbSLVAvail = pfucbNil;
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVTryShrinkSLV( PIB * const ppib, const IFMP ifmp, QWORD * const pcbShrink )
//  ================================================================
//
//  See if the SLV file is shrinkable, if so shrink it
//
//-
	{
	ERR err = JET_errSuccess;

	BOOL fInLock = fFalse;

	*pcbShrink = 0;
	
	FCB * const pfcbSLVAvail = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcbSLVAvail );
	
	//  Get the writer lock. No-one can allocate space or extend the database while we have this lock

	rgfmp[ifmp].RwlSLVSpace().EnterAsWriter();
	fInLock = fTrue;
	
	//  See if we can shrink the database

	QWORD cbToShrink;
	Call( ErrOLDSLVGetShrinkableSpace( ppib, ifmp, &cbToShrink ) );
	Assert( 0 == ( cbToShrink % SLVSPACENODE::cpageMap ) );

	if( cbToShrink >= ( 1 * SLVSPACENODE::cpageMap * g_cbPage ) )
		{
		
		//  Enough free space to shrink the database by at least one chunk
		
		Call( ErrOLDSLVShrinkSLV( ppib, ifmp, cbToShrink ) );
		*pcbShrink = cbToShrink;
		}
	else
		{
		
		//  Not enough free space to shrink the database

		Assert( 0 == cbToShrink );
		Assert( 0 == *pcbShrink );
		}		

HandleError:
	if( fInLock )
		{
		rgfmp[ifmp].RwlSLVSpace().LeaveAsWriter();
		fInLock = fFalse;
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrOLDSLVMoveOneRun(
			FUCB * const pfucbSLVAvail,
			SLVSPACENODECACHE * const pslvspacenodecache,
			OLDSLVCTRL * const poldslvctrl )
//  ================================================================
	{
	ERR				err					= JET_errSuccess;
	const IFMP		ifmp				= pfucbSLVAvail->ifmp;
	INST * const	pinst		 		= PinstFromPfucb( pfucbSLVAvail );
	const INT		cpgFreeThreshold 	= ( SLVSPACENODE::cpageMap * pinst->m_lSLVDefragFreeThreshold ) / 100;

	//  Get the block that contains the next run to move

	PGNO pgnoAvailBlock;
	if( !pslvspacenodecache->FGetCachedLocationForDefragMove( &pgnoAvailBlock ) )
		{		
	
		//  no blocks to move
		
		return wrnOLDSLVNothingToMove;
		}
	
	//	Seek to the block with the given pgno

	BYTE rgbKey[sizeof(PGNO)];
	KeyFromLong( rgbKey, pgnoAvailBlock );

	BOOKMARK bookmark;
	bookmark.key.prefix.Nullify();
	bookmark.key.suffix.SetPv( rgbKey );
	bookmark.key.suffix.SetCb( sizeof( rgbKey ) );
	bookmark.data.Nullify();
	
	DIB dib;
	dib.pos 	= posDown;
	dib.pbm 	= &bookmark;
	dib.dirflag = fDIRNull;

	Call( ErrDIRDown( pfucbSLVAvail, &dib ) );

	//  Extract the SLVSPACENODE
	
	const SLVSPACENODE * pslvspacenode;
	Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof( SLVSPACENODE ) );
	pslvspacenode = reinterpret_cast< SLVSPACENODE* >( pfucbSLVAvail->kdfCurr.data.Pv() );
	
	//  Find the first committed page

	LONG ipage;
	ipage = pslvspacenode->IpageFirstCommitted();

	//  Release the page latch

	DIRUp( pfucbSLVAvail );

	//  Did we find a committed page?
	
	if( SLVSPACENODE::cpageMap == ipage )
		{
		//  try to shrink the database

		QWORD cbShrink;
		Call( ErrOLDSLVTryShrinkSLV( pfucbSLVAvail->ppib, ifmp, &cbShrink ) );

		if( cbShrink > 0 )
			{
			const CHAR	*rgszT[2];
			CHAR szCbShrink[16];
			rgszT[0] = rgfmp[ifmp].SzSLVName();
			rgszT[1] = szCbShrink;
			
			sprintf( szCbShrink, "%d", cbShrink );

			UtilReportEvent(
					eventInformation,
					ONLINE_DEFRAG_CATEGORY,
					OLDSLV_SHRANK_DATABASE_ID,
					2,
					rgszT );
			}					
		
		//  There are no committed pages in this block. Advance the SLVSPACENODE cache to the next block

		if( !pslvspacenodecache->FGetNextCachedLocationForDefragMove() )
			{
	
			//  no blocks to move

			err = wrnOLDSLVNothingToMove;
			}
		goto HandleError;
		}
	
	//  Translate into a pgno. Remember:
	//		SLVSPACENODE's are indexed by the last page in the chunk
	//		the first page is 1, not zero

	PGNO pgno;
	pgno = pgnoAvailBlock + ipage - SLVSPACENODE::cpageMap + 1;

	//  Call move run on the page

///	printf( "moving run at %d\n", pgno );
	Call( poldslvctrl->ErrMoveRun( pgno ) );
	
	pinst->m_rgoldstatSLV[ rgfmp[ifmp].Dbid() ].IncCChunksMoved();

HandleError:

	DIRUp( pfucbSLVAvail );
	return err;
	}


//  ================================================================
ERR ErrOLDSLVDefragDB( const IFMP ifmp )
//  ================================================================
	{
	ERR			err				= JET_errSuccess;
	CHAR		szTrace[64];

	OLDSLVCTRL	oldslv( ifmp );

	INST * const pinst 								= PinstFromIfmp( ifmp );
	SLVSPACENODECACHE * const pslvspacenodecache 	= rgfmp[ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache );

	//  Reset the performance counters

	CallR( oldslv.ErrInit() );
	PIB * const ppib = oldslv.Ppib();
	Assert( NULL != ppib );

	FUCB * pfucbSLVAvail = pfucbNil;

	sprintf( szTrace, "OLDSLV BEGIN (ifmp %d)", ifmp );
	Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );
		
	Call( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVAvail(), &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	while( FOLDSLVContinue( ifmp ) )
		{
		err = ErrOLDSLVMoveOneRun( pfucbSLVAvail, pslvspacenodecache, &oldslv );

		if( errOLDSLVMoveStopped == err )
			{
			//	The move was cancelled (and rolled-back) because OLDSLV was stopped
			//	Terminate without an error
			
			Assert( !FOLDSLVContinue( ifmp ) );
			err = JET_errSuccess;
			}
		else if( wrnOLDSLVNothingToMove == err )
			{
			//  nothing in the SLV file was movable. sleep and retry

			OLDSLVCompletedPass( ifmp );

			const CHAR	*rgszT[2];
			rgszT[0] = rgfmp[ifmp].SzSLVName();

			//  try to shrink the database

			QWORD cbShrink;
			Call( ErrOLDSLVTryShrinkSLV( ppib, ifmp, &cbShrink ) );

			if( cbShrink > 0 )
				{
				CHAR szCbShrink[16];
				rgszT[1] = szCbShrink;
			
				sprintf( szCbShrink, "%d", cbShrink );

				UtilReportEvent(
						eventInformation,
						ONLINE_DEFRAG_CATEGORY,
						OLDSLV_SHRANK_DATABASE_ID,
						2,
						rgszT );
				}

			UtilReportEvent(
					eventInformation,
					ONLINE_DEFRAG_CATEGORY,
					OLDSLV_COMPLETE_FULL_PASS_ID,
					1,
					rgszT );

			sprintf( szTrace, "OLDSLV COMPLETED PASS (ifmp %d)", ifmp );
			Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );
			
			OLDSLVSleep( ifmp, cmsecAsyncBackgroundCleanup );

			if ( FOLDSLVContinue( ifmp ) )
				{
				sprintf( szTrace, "OLDSLV RESTART FULL PASS (ifmp %d)", ifmp );
				Call ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );
				UtilReportEvent(
						eventInformation,
						ONLINE_DEFRAG_CATEGORY,
						OLDSLV_BEGIN_FULL_PASS_ID,
						1,
						rgszT );
				}
			}
		else if( errOLDSLVUnableToMove == err )
			{
			Assert( 0 == ppib->level );
			if ( FOLDSLVContinue( ifmp ) )
				{
				//  some thing in the SLV file couldn't be moved
				//  (another transaction session is using the record)
				//  sleep and retry
				OLDSLVSleep( ifmp, cmsecWaitOLDSLVConflict );
				}
			err = JET_errSuccess;
			}
		else if( JET_errSuccess == err )
			{
			while ( pinst->m_plog->m_fBackupInProgress && FOLDSLVContinue( ifmp ) )
				{								
				//	suspend OLDSLV if this process is performing online backup
				
				OLDSLVSleep( ifmp, cmsecWaitForBackup );
				}
			}
		else
			{
			Call( err );
			}
		}	//	while ( FOLDSLVContinue( ifmp )

HandleError:

	if( pfucbNil != pfucbSLVAvail )
		{
		DIRClose( pfucbSLVAvail );
		}

	sprintf( szTrace, "OLDSLV END (ifmp %d, err %d)", ifmp, err );
	(void) ( pinst->m_plog->ErrLGTrace( ppib, szTrace ) );
		
	CallS( oldslv.ErrTerm() );

	return err;		
	}

	
#ifdef DEBUG

// main function that dumps the nodes (it's called from the eseutil.cxx)
// the dump parameters are set according to the command line in pdbutil 
// Current options:	- dump only one table /{x|X}TableName
//					- dump visible nodes or all nodes 
//					  (including those marked as deleted)
ERR ErrOLDSLVTest( JET_SESID sesid, JET_DBUTIL *pdbutil )
	{

	ERR				err = JET_errSuccess;
	JET_DBID		ifmp				= JET_dbidNil;
	OLDSLVCTRL * 	poldslv 			= NULL;
	BOOL 			fOLDSLVInit 		= fFalse;
	
	Assert( NULL != pdbutil);
	Assert( sizeof(JET_DBUTIL) == pdbutil->cbStruct );
	Assert( NULL != pdbutil->szDatabase );

	// attach to the database
	CallR( ErrIsamAttachDatabase(
				sesid,
				pdbutil->szDatabase,
				NULL,
				NULL,
				0,
				0
				) );
	Call( ErrIsamOpenDatabase(
				sesid,
				pdbutil->szDatabase,
				NULL,
				&ifmp,
				0
				) );
	Assert( JET_instanceNil != ifmp );
	
	poldslv = new OLDSLVCTRL(ifmp);
	Assert ( poldslv );

	Call ( poldslv->ErrInit() );
	fOLDSLVInit = fTrue;

	Call ( poldslv->ErrMoveRun( pdbutil->pgno ) );
	
	CallS ( poldslv->ErrTerm() );
	fOLDSLVInit = fFalse;
	
HandleError:

	if (fOLDSLVInit)
		{
		Assert (poldslv);
		(void) poldslv->ErrTerm();
		}

	if (poldslv)
		{
		delete poldslv;
		}
		
	if ( JET_dbidNil != ifmp )
		{
		(VOID)ErrIsamCloseDatabase( sesid, ifmp, NO_GRBIT );
		}
	
	(VOID)ErrIsamDetachDatabase( sesid, NULL, pdbutil->szDatabase );

	return err;		
	}
#endif // DEBUG
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\slv.cxx ===
#include "std.hxx"

#define SLV_USE_VIEWS			//  use views of an SLV File to log data when
								//  the SLV Provider is enabled.  views allow
								//  us to avoid a memory copy but risk page
								//  faults on large files that have pages that
								//  have fallen out of memory.  views also save
								//  us from reopening an SLV File for I/O

//#define SLV_USE_RAW_FILE		//  use the backing file to log data when the SLV
								//  Provider is enabled.  use if and only if the
								//  streaming file is cached and not each SLV
								//  File.  reading from the backing file in this
								//  way saves us from opening or reopening the
								//  SLV File for I/O



//  ================================================================
CGROWABLEARRAY::CGROWABLEARRAY() :
	m_culEntries( 0 )
//  ================================================================
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		m_rgpul[ipul] = NULL;
		}
	}

	
//  ================================================================
CGROWABLEARRAY::~CGROWABLEARRAY()
//  ================================================================
//
//  Free all the memory allocated for the array. We can stop at the
//  first NULL entry
//
//-
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		delete [] m_rgpul[ipul];
		}
	}


//  ================================================================
ERR	CGROWABLEARRAY::ErrGrow( const INT culEntriesNew )
//  ================================================================
	{
	const INT ipul 		= IpulEntry_( culEntriesNew + 1 );
	INT ipulT;
	for( ipulT = ipul; ipulT >= 0; --ipulT )
		{
		if( NULL != m_rgpul[ipulT] )
			{
			//  everything below this will be filled in
			break;
			}
		else
			{
			const INT cul = CulPul_( ipulT );
			ULONG * const pul = new ULONG[ cul ];
			if( NULL == pul )
				{
				//	delete currently allocated chunks
				for ( ipulT++; ipul >= ipulT; ipulT++ )
					{
					delete m_rgpul[ipulT];
					m_rgpul[ipulT] = NULL;
					}
				return ErrERRCheck( JET_errOutOfMemory );
				}
			INT iul;
			for( iul = 0; iul < cul; ++iul )
				{
				pul[iul] = m_ulDefault;
				}
			//  to make growing multi-threaded, using AtomicCompareExchange here
			//  to make sure that the pointer is still NULL
			m_rgpul[ipulT] = pul;
			}
		}
	m_culEntries = culEntriesNew;
	return JET_errSuccess;
	}


//  ================================================================
ERR	CGROWABLEARRAY::ErrShrink( const INT culEntriesNew )
//  ================================================================
	{
	Assert( culEntriesNew > 0 );

	//  Reset all the entries we are removing
	
	INT iul;
	for( iul = culEntriesNew; iul < m_culEntries; ++iul )
		{
		ULONG * const pul = PulEntry( iul );
		*pul = m_ulDefault;
		}
	
	m_culEntries = culEntriesNew;
	return JET_errSuccess;
	}


//  ================================================================
ULONG * CGROWABLEARRAY::PulEntry( const INT iul )
//  ================================================================
	{
	Assert( iul < m_culEntries );
	return PulEntry_( iul );
	}


//  ================================================================
const ULONG * CGROWABLEARRAY::PulEntry( const INT iul ) const
//  ================================================================
	{
	Assert( iul < m_culEntries );
	return PulEntry_( iul );
	}


//  ================================================================
INT CGROWABLEARRAY::CulEntries() const
//  ================================================================
	{
	return m_culEntries;
	}


//  ================================================================
BOOL CGROWABLEARRAY::FFindBest(
		const ULONG ulMin,
		const ULONG ulMax,
		const INT iulStart,
		INT * const piul,
		const INT * const rgiulIgnore,
		const INT ciulIgnore ) const
//  ================================================================
	{
	const INT ipulStart	= IpulEntry_( iulStart );
	INT iulActual 		= iulStart;	
	INT ipul;

	ULONG	ulBest		= m_ulDefault;
	BOOL	fFoundMatch	= fFalse;

	for( ipul = ipulStart; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		const ULONG * const pul = m_rgpul[ipul];
		const INT cul = CulPul_( ipul );
		INT iul;
		if( ipulStart == ipul )
			{
			iul = iulStart - CulPulTotal_( ipul );
			}
		else
			{
			iul = 0;
			}
		for( ; iul < cul; ++iul )
			{
			if( pul[iul] > ulMin
				&& pul[iul] > ulBest
				&& pul[iul] < ulMax )
				{				
				INT iulT;
				for( iulT = 0; iulT < ciulIgnore; ++iulT )
					{
					if( iulActual == rgiulIgnore[iulT] )
						{
						break;
						}
					}

				if( ciulIgnore == iulT )
					{
					*piul 		= iulActual;
					ulBest		= pul[iul];
					fFoundMatch = fTrue;
					}
				}
			if( ++iulActual >= m_culEntries )
				{
				break;
				}
			}
		}
	return fFoundMatch;
	}


//  ================================================================
BOOL CGROWABLEARRAY::FFindFirst( const ULONG ulMic, const ULONG ulMax, const INT iulStart, INT * const piul ) const
//  ================================================================
	{
	const INT ipulStart	= IpulEntry_( iulStart );
	INT iulActual 		= iulStart;	
	INT ipul;

	BOOL	fFoundMatch	= fFalse;

	for( ipul = ipulStart; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		const ULONG * const pul = m_rgpul[ipul];
		const INT cul = CulPul_( ipul );
		INT iul;
		if( ipulStart == ipul )
			{
			iul = iulStart - CulPulTotal_( ipul );
			}
		else
			{
			iul = 0;
			}
		for( ; iul < cul; ++iul )
			{
			if( pul[iul] >= ulMic
				&& pul[iul] < ulMax )
				{
				*piul 		= iulActual;
				fFoundMatch = fTrue;
				break;
				}
			if( ++iulActual >= m_culEntries )
				{
				break;
				}
			}
		}
	return fFoundMatch;
	}
	
		
#ifndef RTM


//  ================================================================
VOID CGROWABLEARRAY::AssertValid() const
//  ================================================================
	{
	//  check that NULL and non-NULL entries are not intermingled
	BOOL fSeenNull = fFalse;
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		if( fSeenNull )
			{
			AssertRTL( NULL == m_rgpul[ipul] );
			}
		else
			{
			fSeenNull = ( NULL == m_rgpul[ipul] );
			}
		}
	}

	
#endif	//	RTM

#ifndef RTM


//  ================================================================
ERR CGROWABLEARRAY::ErrUnitTest()
//  ================================================================
//
//  this is a STATIC function
//
//-
	{	
	ERR err;
	CGROWABLEARRAY cga;

	AssertRTL( 1 * m_culInitial == cga.CulPul_( 0 ) );
	AssertRTL( 2 * m_culInitial == cga.CulPul_( 1 ) );
	AssertRTL( 4 * m_culInitial == cga.CulPul_( 2 ) );
	AssertRTL( 8 * m_culInitial == cga.CulPul_( 3 ) );
	AssertRTL( 16 * m_culInitial == cga.CulPul_( 4 ) );
	AssertRTL( 32 * m_culInitial == cga.CulPul_( 5 ) );

	AssertRTL( 0 * m_culInitial == cga.CulPulTotal_( 0 ) );
	AssertRTL( 1 * m_culInitial == cga.CulPulTotal_( 1 ) );
	AssertRTL( 3 * m_culInitial == cga.CulPulTotal_( 2 ) );
	AssertRTL( 7 * m_culInitial == cga.CulPulTotal_( 3 ) );
	AssertRTL( 15 * m_culInitial == cga.CulPulTotal_( 4 ) );
	AssertRTL( 31 * m_culInitial == cga.CulPulTotal_( 5 ) );

	AssertRTL( 0 == cga.IpulEntry_( m_culInitial * 0 ) );
	AssertRTL( 1 == cga.IpulEntry_( m_culInitial * 1) );
	AssertRTL( 1 == cga.IpulEntry_( m_culInitial * 2 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 3 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 4 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 5 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 6 ) );
	AssertRTL( 3 == cga.IpulEntry_( m_culInitial * 7 ) );
	AssertRTL( 3 == cga.IpulEntry_( m_culInitial * 10 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 15 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 25 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 30 ) );
	AssertRTL( 5 == cga.IpulEntry_( m_culInitial * 31 ) );

	INT cul;
	for( cul = 1; cul <= 4097; ++cul )
		{
		Call( cga.ErrGrow( cul ) );
		ASSERT_VALID( &cga );
		}

	for( cul = 4098; cul <= 10000; ++cul )
		{
		Call( cga.ErrGrow( cul ) );
		ASSERT_VALID( &cga );
		}

	for( cul = 10000; cul > 4097; --cul )
		{
		CallS( cga.ErrShrink( cul ) );
		ASSERT_VALID( &cga );
		}

	CallS( cga.ErrGrow( 10000 ) );

	INT iul;
	for( iul = 4098; iul <= 9999; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		AssertRTL( m_ulDefault == *pul );
		}
		
	CallS( cga.ErrShrink( 4097 ) );

	for( iul = 0; iul < 4096; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		AssertRTL( m_ulDefault == *pul );
		}

	ULONG * pul;
	pul = cga.PulEntry( 2037 );
	*pul = 59;

	INT iulT;
	for( iulT = 0; iulT <= 2037; ++iulT )
		{
		AssertRTL( cga.FFindBest( ( iulT % 50 ) + 1, 2038+iulT, iulT, &iul, NULL, 0 ) );
		AssertRTL( 2037 == iul );
		}

	AssertRTL( cga.FFindBest( 57, 2039, 0, &iul, NULL, 0 ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 57, 2039, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindBest( 58, 60, 0, &iul, NULL, 0 ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 58, 60, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( !cga.FFindBest( 59, 2038, 1, &iul, NULL, 0 ) );	

	AssertRTL( cga.FFindFirst( 59, 60, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 0, 4000, 2038, &iul ) );
	AssertRTL( 2038 == iul );

	AssertRTL( !cga.FFindBest( 60, 10000, 2, &iul, NULL, 0 ) );
	AssertRTL( !cga.FFindBest( 0, 4000, 2038, &iul, NULL, 0 ) );
	AssertRTL( !cga.FFindBest( 0, 4000, 2039, &iul, NULL, 0 ) );

	AssertRTL( !cga.FFindFirst( 60, 10000, 2, &iul ) );
	AssertRTL( !cga.FFindFirst( 1, 4000, 2038, &iul ) );
	AssertRTL( !cga.FFindFirst( 1, 4000, 2039, &iul ) );

	pul = cga.PulEntry( 2038 );
	*pul = 60;
	pul = cga.PulEntry( 2039 );
	*pul = 61;
	pul = cga.PulEntry( 2040 );
	*pul = 62;
	pul = cga.PulEntry( 2041 );
	*pul = 63;

	AssertRTL( cga.FFindBest( 0, 0xffffffff, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindBest( 0, 64, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindBest( 0, 63, 0, &iul, NULL, 0 ) );
	AssertRTL( 2040 == iul );
	AssertRTL( cga.FFindBest( 62, 64, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindFirst( 62, 64, 0, &iul ) );
	AssertRTL( 2040 == iul );

	for( iul = 0; iul < 4096; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		*pul = 0xbaadf00d;
		}

	AssertRTL( JET_errSuccess == cga.ErrShrink( 2049 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 2047 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 255 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 1 ) );	

	Call( cga.ErrGrow( 100 ) );

	pul = cga.PulEntry( 1 );
	*pul = 10;
	pul = cga.PulEntry( 10 );
	*pul = 28;
	pul = cga.PulEntry( 20 );
	*pul = 14;
	pul = cga.PulEntry( 30 );
	*pul = 24;
	pul = cga.PulEntry( 40 );
	*pul = 18;
	pul = cga.PulEntry( 50 );
	*pul = 20;
	pul = cga.PulEntry( 60 );
	*pul = 20;
	pul = cga.PulEntry( 70 );
	*pul = 22;
	pul = cga.PulEntry( 80 );
	*pul = 26;
	pul = cga.PulEntry( 90 );
	*pul = 12;

	INT rgiulIgnore[5];

	rgiulIgnore[0] = 10;
	AssertRTL( !cga.FFindBest( 29, 31, 0, &iul, rgiulIgnore, 1 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 1 ) );
	AssertRTL( 80 == iul );

	rgiulIgnore[1] = 80;
	AssertRTL( !cga.FFindBest( 25, 31, 0, &iul, rgiulIgnore, 2 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 2 ) );
	AssertRTL( 30 == iul );

	rgiulIgnore[2] = 30;
	rgiulIgnore[3] = 70;
	AssertRTL( !cga.FFindBest( 21, 31, 0, &iul, rgiulIgnore, 4 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 4 ) );
	AssertRTL( 50 == iul || 60 == iul );

	rgiulIgnore[4] = 50;
	AssertRTL( !cga.FFindBest( 21, 31, 0, &iul, rgiulIgnore, 5 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 5 ) );
	AssertRTL( 60 == iul );
	
	return JET_errSuccess;

HandleError:
	return err;
	}

	
#endif	//	!RTM


//-
//
//  For these calulations, imagine the case where m_culInitial = 1
//  (See ErrUnitTest)
//
//  iul		cpul	cpulTotal	entries in this array
//	-------------------------------------------------
//	0		1		0			[0,1)
//	1		2		1			[1,3)
//	2		4		3			[3,7)
//	3		8		7			[7,15)
//	4		16		15			[15,31)
//	5		32		31			[31,63)
//
//  We calculate these numbers and scale by m_culInitial
//
//-


//  ================================================================
INT CGROWABLEARRAY::IpulEntry_( const INT iul ) const
//  ================================================================
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		if( CulPulTotal_( ipul ) > iul )
			{
			return ipul - 1;
			}
		}
	AssertRTL( fFalse );
	return 0xffffffff;
	}


//  ================================================================
INT CGROWABLEARRAY::CulPulTotal_( const INT ipul ) const
//  ================================================================
	{
	//  ( 2^ipul - 1 ) * inital size
	const culPulTotal = ( ( 1 << ipul ) - 1 ) * m_culInitial;
	return culPulTotal;
	}


//  ================================================================
INT CGROWABLEARRAY::CulPul_( const INT ipul ) const
//  ================================================================
	{
	//  2^ipul * initial size
	const INT culPul = ( 1 << ipul ) * m_culInitial;
	Assert( culPul > 0 );
	return culPul;
	}

		
//  ================================================================
ULONG * CGROWABLEARRAY::PulEntry_( const INT iul ) const
//  ================================================================
	{
	//  which array in m_rgpul do we want
	const INT ipul 		= IpulEntry_( iul );
	//  how many entries are in the arrays pointed to by entries
	//  less than this one in m_rgpul
	const INT culTotal	= CulPulTotal_( ipul );
	//  which offset in the array do we want
	const INT iulOffset	= iul - culTotal;
	
	return m_rgpul[ipul] + iulOffset;
	}


//  ================================================================
SLVSPACENODECACHE::SLVSPACENODECACHE( 
	const LONG lSLVDefragFreeThreshold,			
	const LONG lSLVDefragDefragDestThreshold,	
	const LONG lSLVDefragDefragMoveThreshold ) :	
	m_crit( CLockBasicInfo( CSyncBasicInfo( szCritSLVSPACENODECACHE ), rankCritSLVSPACENODECACHE, 0 ) ),
	m_cgrowablearray(),
	m_cpgFullPages( 0 ),
	m_cpgEmptyPages( 0 )
//  ================================================================
	{
	m_rgcpgThresholdMin[ipgnoNewInsertion]		= ( lSLVDefragFreeThreshold * SLVSPACENODE::cpageMap ) / 100;
	m_rgcpgThresholdMin[ipgnoDefragInsertion]	= ( lSLVDefragDefragDestThreshold * SLVSPACENODE::cpageMap ) / 100; 
	m_rgcpgThresholdMin[ipgnoDefragMove]		= ( lSLVDefragDefragMoveThreshold * SLVSPACENODE::cpageMap ) / 100; 

	m_rgcpgThresholdMax[ipgnoNewInsertion]		= SLVSPACENODE::cpageMap + 1;
	m_rgcpgThresholdMax[ipgnoDefragInsertion]	= SLVSPACENODE::cpageMap + 1; 
	m_rgcpgThresholdMax[ipgnoDefragMove]		= m_rgcpgThresholdMin[ipgnoNewInsertion]; 

	INT ipgnoCached;
	for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
		{
		m_rgpgnoCached[ipgnoCached]	= pgnoNull;
		}

	ASSERT_VALID( this );
	}


//  ================================================================
SLVSPACENODECACHE::~SLVSPACENODECACHE()
//  ================================================================
	{
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgCacheSize() const
//  ================================================================
	{
	return m_cgrowablearray.CulEntries() * SLVSPACENODE::cpageMap;
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgEmpty() const
//  ================================================================
	{
	return m_cpgEmptyPages;
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgFull() const
//  ================================================================
	{
	return m_cpgFullPages;
	}
	


		//	Prepare to increase the size of the cache to include this many pages
		//	in the SLV tree. This ensures that a subsequent call to ErrGrowCache
		//	for the same or fewer number of pages will not fail
//  ================================================================
ERR SLVSPACENODECACHE::ErrReserve( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );

	const ERR err = ErrReserve_( cpgNewSLV );

	ASSERT_VALID_RTL( this );
	return err;	
	}


//  ================================================================
ERR	SLVSPACENODECACHE::ErrGrowCache( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;

	Assert( culCacheEntriesOld <= culCacheEntriesNew );
	
	const ERR err = m_cgrowablearray.ErrGrow( culCacheEntriesNew );

	if( JET_errSuccess == err )
		{
		
		//	these chunks default to full
		
		m_cpgFullPages += ( culCacheEntriesNew - culCacheEntriesOld );
		}
	
	ASSERT_VALID_RTL( this );
	return err;
	}


//  ================================================================
ERR	SLVSPACENODECACHE::ErrShrinkCache( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( ( cpgNewSLV % SLVSPACENODE::cpageMap ) == 0 );
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;
	const INT culCacheEntriesShrink = culCacheEntriesOld - culCacheEntriesNew;

#ifdef DEBUG

	//	we should be shrinking only empty pages

	Assert( culCacheEntriesShrink > 0 );
	Assert( m_cpgEmptyPages >= culCacheEntriesShrink );

	INT iul;
	for( iul = culCacheEntriesOld; iul < culCacheEntriesNew; ++iul )
		{
		const ULONG * const pul = m_cgrowablearray.PulEntry( iul );	
		Assert( *pul == SLVSPACENODE::cpageMap );
		}

#endif	//	DEBUG

	//	we are removing only empty pages, decrease the number of empty pages
	
	m_cpgEmptyPages -= culCacheEntriesShrink;

	//	one or more of the cached locations may have been in the range that
	//	was shrunk. invalidate the cache
	//	CONSIDER: only invalidate cached entries that are in the shrinking region
	
	INT ipgnoCached;
	for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
		{
		m_rgpgnoCached[ipgnoCached]	= pgnoNull;
		}
	
	const ERR err = m_cgrowablearray.ErrShrink( culCacheEntriesNew );
	
	ASSERT_VALID_RTL( this );
	return err;
	}


//  ================================================================
CPG SLVSPACENODECACHE::SetCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg >= 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	
	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const CPG cpgOld	= *pul;
	*pul = cpg;

	if( 0 == cpgOld )
		{
		--m_cpgFullPages;
		}
	else if ( SLVSPACENODE::cpageMap == cpgOld )
		{
		--m_cpgEmptyPages;
		}

	if( 0 == cpg )
		{
		++m_cpgFullPages;
		}
	else if ( SLVSPACENODE::cpageMap == cpg )
		{
		++m_cpgEmptyPages;
		}
		
	ASSERT_VALID_RTL( this );
	return cpgOld;
	}


//  ================================================================
CPG SLVSPACENODECACHE::IncreaseCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );

	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const ULONG ulOld 	= *pul;
	*pul += cpg;

	if( 0 == ulOld )
		{
		--m_cpgFullPages;
		}
		
	if( SLVSPACENODE::cpageMap == *pul )
		{
		++m_cpgEmptyPages;

		//  we have completely emptied a chunk. the chunk may be the cached
		//  one defrag is moving things out of. recalculate a new location to
		//  move defragged things from

		//	UNDONE: get rid of this loop
		
		INT ipgnoCached;
		for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
			{
			
			//  a rollback could be re-emptying the point we are allocating space from
			//  check to see if this is the defrag point
			
			if( pgno == m_rgpgnoCached[ipgnoCached]
				&& ipgnoDefragMove == ipgnoCached )
				{
				(VOID)FFindNewCachedLocation_( ipgnoCached, IulMapPgnoToCache_( pgno ) );
				break;
				}
			}
		}

	ASSERT_VALID_RTL( this );
	return ulOld;
	}


//  ================================================================
CPG SLVSPACENODECACHE::DecreaseCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );

	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const ULONG ulOld 	= *pul;
	*pul -= cpg;

	if( SLVSPACENODE::cpageMap == ulOld )
		{
		--m_cpgEmptyPages;
		}

	if( 0 == *pul )
		{
		++m_cpgFullPages;
		
		//  we have completely fulled a chunk. the chunk should be one of the
		//  cached ones used for insertion. recalculate a new cached chunk

		INT ipgnoCached;
		for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
			{
			if( pgno == m_rgpgnoCached[ipgnoCached] )
				{
				AssertRTL( ipgnoDefragMove != ipgnoCached );
				(VOID)FFindNewCachedLocation_( ipgnoCached, IulMapPgnoToCache_( pgno ) );
				break;
				}
			}
		}
		
	ASSERT_VALID_RTL( this );
	return ulOld;
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForNewInsertion( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoNewInsertion );

	ASSERT_VALID_RTL( this );
	return fResult;	
	}

	
//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForDefragInsertion( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoDefragInsertion );

	ASSERT_VALID_RTL( this );
	return fResult;		
	}

	
//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForDefragMove( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoDefragMove );

	ASSERT_VALID_RTL( this );
	return fResult;		
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetNextCachedLocationForDefragMove( )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );

	const BOOL fResult = ( pgnoNull == m_rgpgnoCached[ipgnoDefragMove] ) ?
			FFindNewCachedLocation_( ipgnoDefragMove, 0 ) :
			FFindNewCachedLocation_( ipgnoDefragMove, IulMapPgnoToCache_( m_rgpgnoCached[ipgnoDefragMove] ) );

	ASSERT_VALID_RTL( this );
	return fResult;			
	}


#if defined( DEBUG ) || !defined( RTM )


//  ================================================================
VOID SLVSPACENODECACHE::AssertValid() const
//  ================================================================
	{
	ASSERT_VALID_RTL( &m_cgrowablearray );

	AssertRTL( m_cpgEmptyPages >= 0 );
	AssertRTL( m_cpgFullPages >= 0 );
	AssertRTL( m_cpgEmptyPages <= m_cgrowablearray.CulEntries() );
	AssertRTL( m_cpgFullPages <= m_cgrowablearray.CulEntries() );
	
	BOOL fPagesCached = fFalse;
	
	//  make sure no page is cached twice
	
	INT ipgno;
	for( ipgno = 0; ipgno < cpgnoCached; ++ipgno )
		{
		if( pgnoNull == m_rgpgnoCached[ipgno] )
			{
			continue;
			}
		else
			{
			fPagesCached = fTrue;
			}
			
		INT ipgnoT;
		for( ipgnoT = 0; ipgnoT < cpgnoCached; ++ipgnoT )
			{
			if( ipgnoT == ipgno )
				{
				continue;
				}
			AssertRTL( m_rgpgnoCached[ipgno] != m_rgpgnoCached[ipgnoT] );
			}
		}

	//

	if( fPagesCached )
		{
		AssertRTL( m_cgrowablearray.CulEntries() > 0 );
		}
		
	//  make sure no cached extent has too many pages

	INT cpgEmptyPages 	= 0;
	INT cpgFullPages	= 0;
	
	INT iul;
	for( iul = 0; iul < m_cgrowablearray.CulEntries(); ++iul )
		{
		const ULONG * const pul 	= m_cgrowablearray.PulEntry( iul );	
		AssertRTL( *pul <= SLVSPACENODE::cpageMap );

		if( 0 == *pul )
			{
			++cpgFullPages;
			}
		else if ( SLVSPACENODE::cpageMap == *pul )
			{
			++cpgEmptyPages;
			}
		}

	AssertRTL( m_cpgEmptyPages == cpgEmptyPages );
	AssertRTL( m_cpgFullPages == cpgFullPages );
	}

		
#endif	//	DEBUG || !RTM

#ifndef RTM


//  ================================================================
ERR SLVSPACENODECACHE::ErrValidate( FUCB * const pfucbSLVAvail ) const
//  ================================================================
	{
	ERR err;
	
	DIB dib;

	dib.pos = posFirst;
	dib.pbm = NULL;
	dib.dirflag = fDIRNull;

	err = ErrDIRDown( pfucbSLVAvail, &dib );
	if( JET_errRecordNotFound == err )
		{
		//  the SLV Avail tree is empty
		return JET_errSuccess;
		}

	do
		{
		PGNO pgno;
		LongFromKey( &pgno, pfucbSLVAvail->kdfCurr.key );
		Assert( 0 == ( pgno % SLVSPACENODE::cpageMap ) );

		Assert( sizeof( SLVSPACENODE ) == pfucbSLVAvail->kdfCurr.data.Cb() );
		const SLVSPACENODE * const pslvspacenode = (SLVSPACENODE *)pfucbSLVAvail->kdfCurr.data.Pv();

		const INT iulCache 		= IulMapPgnoToCache_( pgno );
		const ULONG * const pul = m_cgrowablearray.PulEntry( iulCache );

		const LONG cpgAvailReal 	= pslvspacenode->CpgAvail();
		const LONG cpgAvailCached	= *pul;

		AssertRTL( cpgAvailReal == cpgAvailCached );
		
		} while( JET_errSuccess == ( err = ErrDIRNext( pfucbSLVAvail, fDIRNull ) ) );

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
		
	DIRUp( pfucbSLVAvail );
	return err;
	}


//  ================================================================
ERR SLVSPACENODECACHE::ErrUnitTest()
//  ================================================================
// 
//  STATIC method
//
//-
	{
	return CGROWABLEARRAY::ErrUnitTest();
	}
	

#endif  //  !RTM



//  ================================================================
ERR	SLVSPACENODECACHE::ErrReserve_( const CPG cpgNewSLV )
//  ================================================================
	{
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;

	Assert( culCacheEntriesOld <= culCacheEntriesNew );
	
	const ERR err = m_cgrowablearray.ErrGrow( culCacheEntriesNew );
	if( JET_errSuccess == err )
		{
		CallS( m_cgrowablearray.ErrShrink( culCacheEntriesOld ) );
		}

	return err;		
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocation_( PGNO * const ppgno, const INT ipgnoCached )
//  ================================================================
	{		
	if( pgnoNull == m_rgpgnoCached[ipgnoCached]
		&& !FFindNewCachedLocation_( ipgnoCached, 0 ) )
		{
		return fFalse;	
		}

	*ppgno = m_rgpgnoCached[ipgnoCached];
	return fTrue;
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FFindNewCachedLocation_( const INT ipgnoCached, const INT iulStart )
//  ================================================================
//
//  UNDONE:  make this able to wrap-around
//
//-
	{
	CPG	cpgThresholdMin	= m_rgcpgThresholdMin[ipgnoCached];
	CPG	cpgThresholdMax	= m_rgcpgThresholdMax[ipgnoCached];

	INT rgiulIgnore[cpgnoCached];
	INT i;
	for( i = 0; i < cpgnoCached; ++i )
		{
		rgiulIgnore[i] = ( m_rgpgnoCached[i] / SLVSPACENODE::cpageMap ) - 1;
		}

	INT iul;
	const BOOL f = m_cgrowablearray.FFindBest(
			cpgThresholdMin,
			cpgThresholdMax,
			0,
			&iul,
			rgiulIgnore,
			cpgnoCached
			);
			
	if( !f )
		{
		//  no entries have enough space
		m_rgpgnoCached[ipgnoCached] = pgnoNull;		
		}
	else
		{
		const PGNO pgno = ( iul + 1 ) * SLVSPACENODE::cpageMap;		
		m_rgpgnoCached[ipgnoCached] = pgno;
		}
	return f;
	}


//  ================================================================
INT SLVSPACENODECACHE::IulMapPgnoToCache_( const PGNO pgno ) const
//  ================================================================
	{
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	return ( pgno / SLVSPACENODE::cpageMap ) - 1;
	}


//  ================================================================
ULONG * SLVSPACENODECACHE::PulMapPgnoToCache_( const PGNO pgno )
//  ================================================================
	{
	const INT iulCache 	= IulMapPgnoToCache_( pgno );
	ULONG * const pul 	= m_cgrowablearray.PulEntry( iulCache );	
	return pul;
	}


//  ================================================================
const ULONG * SLVSPACENODECACHE::PulMapPgnoToCache_( const PGNO pgno ) const
//  ================================================================
	{
	const INT iulCache 		= IulMapPgnoToCache_( pgno );
	const ULONG * const pul = m_cgrowablearray.PulEntry( iulCache );	
	return pul;
	}

VOID SLVSoftSyncHeaderSLVDB( SLVFILEHDR * pslvfilehdr, const DBFILEHDR * const pdbfilehdr )
	{
	Assert( pdbfilehdr != NULL );
	Assert( pslvfilehdr != NULL );
	Assert( pdbfilehdr->le_attrib == attribDb );
	Assert( pslvfilehdr->le_attrib == attribSLV );

	Assert( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof(SIGNATURE) ) == 0 );

	pslvfilehdr->signDb = pdbfilehdr->signDb;
	pslvfilehdr->signLog = pdbfilehdr->signLog;

	pslvfilehdr->le_lgposAttach 		= pdbfilehdr->le_lgposAttach;
	pslvfilehdr->le_lgposConsistent 	= pdbfilehdr->le_lgposConsistent;
	pslvfilehdr->le_dbstate 			= pdbfilehdr->le_dbstate;

	pslvfilehdr->le_ulVersion = pdbfilehdr->le_ulVersion;
	pslvfilehdr->le_ulUpdate = pdbfilehdr->le_ulUpdate;
	
	if ( pdbfilehdr->FDbFromRecovery() )
		{
		pslvfilehdr->SetDbFromRecovery();
		}
	else
		{
		pslvfilehdr->ResetDbFromRecovery();
		}
	}

//	Updates slv header
//	Requres signSLV match for DB and SLV header

ERR ErrSLVSyncHeader(	IFileSystemAPI* const	pfsapi, 
						const BOOL				fReadOnly,
						const CHAR* const		szSLVName,
						DBFILEHDR* const		pdbfilehdr,
						SLVFILEHDR*				pslvfilehdr )
	{
	ERR	err = JET_errSuccess;

	Assert( pdbfilehdr != NULL );
	Assert( pdbfilehdr->FSLVExists() );
	Assert( !fReadOnly );
	Assert( NULL != szSLVName );
	
	BOOL fAllocMem = fFalse;
	if ( pslvfilehdr == NULL )
		{
		err = ErrSLVAllocAndReadHeader(	pfsapi, 
										fReadOnly,
										szSLVName,
										pdbfilehdr,
										&pslvfilehdr );
		if ( pslvfilehdr != NULL )
			{
			Assert( err == JET_errSuccess || err == JET_errDatabaseStreamingFileMismatch );
			fAllocMem = fTrue;
			}
		else
			{
			Call( err );
			}
		}
	else
		{
		if ( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof( SIGNATURE ) ) != 0 )
			{
			Call( ErrERRCheck( JET_errDatabaseStreamingFileMismatch ) );
			}
		}
	SLVSoftSyncHeaderSLVDB( pslvfilehdr, pdbfilehdr );
	Call( ErrUtilWriteShadowedHeader(	pfsapi, 
										szSLVName,
										fFalse,
										(BYTE *)pslvfilehdr, 
										g_cbPage ) );
HandleError:
	AssertSz( err != 0 || ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr ) == JET_errSuccess,
		"Why did you forget to update SLV header?" );
	if ( fAllocMem )
		{
		Assert( pslvfilehdr != NULL );
		OSMemoryPageFree( (VOID *)pslvfilehdr );
		}

	return err;	
	}

//	Allocate memory for SLV header and read it.
//	RETURN: JET_errSuccess
//			JET_errDatabaseStreamingFileMismatch - DB and SLV headers are mismathing,
//				slv header will be preserved if signSLV matches.
//			On any other errors the slv header will be released and NULL will be returned

ERR ErrSLVAllocAndReadHeader(	IFileSystemAPI *const	pfsapi, 
								const BOOL				fReadOnly,
								const CHAR* const		szSLVName,
								DBFILEHDR* const		pdbfilehdr,
								SLVFILEHDR** const		ppslvfilehdr )
	{
	ERR err;

	Assert( ppslvfilehdr != NULL );
	Assert( *ppslvfilehdr == NULL );
	Assert( NULL != szSLVName );

	*ppslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	if ( NULL == *ppslvfilehdr )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ErrSLVReadHeader(	pfsapi, 
							fReadOnly,
							szSLVName,
							pdbfilehdr,
							*ppslvfilehdr,
							NULL );

	//	result must be successfull read of SLV header at least
	//	so it means that result is no error or database and SLV file mismatch,
	//	but at least signSLV must match
	if ( ( err < 0 && err != JET_errDatabaseStreamingFileMismatch ) ||
		 memcmp( &(*ppslvfilehdr)->signSLV, &pdbfilehdr->signSLV, sizeof( SIGNATURE ) ) != 0 )
		{
		OSMemoryPageFree( (VOID *)*ppslvfilehdr );
		*ppslvfilehdr = NULL;
		}
	return err;
	}


ERR ErrSLVCheckDBSLVMatch( const DBFILEHDR * const pdbfilehdr, const SLVFILEHDR * const pslvfilehdr )
	{ 
	Assert( pdbfilehdr != NULL );
	Assert( pslvfilehdr != NULL );
	Assert( pdbfilehdr->le_attrib == attribDb );
	Assert( pslvfilehdr->le_attrib == attribSLV );
	static const SIGNATURE signNil;
	
	if ( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof(SIGNATURE) ) == 0 
		&& memcmp( &pslvfilehdr->signDb, &pdbfilehdr->signDb, sizeof(SIGNATURE) ) == 0
		&& ( memcmp( &pslvfilehdr->signLog, &pdbfilehdr->signLog, sizeof(SIGNATURE) ) == 0
			// old slv file
			|| memcmp( &pslvfilehdr->signLog, &signNil, sizeof(SIGNATURE) ) == 0 ) )
		{
		if ( CmpLgpos( &pslvfilehdr->le_lgposAttach, &pdbfilehdr->le_lgposAttach ) == 0
			&& CmpLgpos( &pslvfilehdr->le_lgposConsistent, &pdbfilehdr->le_lgposConsistent ) == 0 )
			{
			return JET_errSuccess;
			}
		// old slv file
		if ( CmpLgpos( &pslvfilehdr->le_lgposAttach, &lgposMin ) == 0
			&& CmpLgpos( &pslvfilehdr->le_lgposConsistent, &lgposMin ) == 0 )
			{
			return JET_errSuccess;
			}
		}
	return ErrERRCheck( JET_errDatabaseStreamingFileMismatch );
	}

INLINE ERR ErrSLVCreate( PIB *ppib, const IFMP ifmp )
	{
	ERR			err;
	FUCB		*pfucbDb		= pfucbNil;

	Assert( ppibNil != ppib );
	Assert( rgfmp[ifmp].FCreatingDB() );
	Assert( 0 == ppib->level || ( 1 == ppib->level && rgfmp[ifmp].FLogOn() ) );
	
	//  open the parent directory (so we are on the pgnoFDP ) and create the LV tree
	CallR( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbDb ) );
	Assert( pfucbNil != pfucbDb );

	Call( ErrSLVCreateAvailMap( ppib, pfucbDb ) );

	Call( ErrSLVCreateOwnerMap( ppib, pfucbDb ) );

HandleError:
	DIRClose( pfucbDb );

	return err;
	}

//  ================================================================
ERR ErrFILECreateSLV( IFileSystemAPI *const pfsapi, PIB *ppib, const IFMP ifmp, const int createOptions )
//  ================================================================
//
//  Creates the LV tree for the given table. 
//	
//-
	{
	ERR				err				= JET_errSuccess;
	IFileAPI		*pfapiSLV		= NULL;
	const BOOL		fRecovering		= PinstFromIfmp( ifmp )->FRecovering();
	const QWORD		cbSize			= OffsetOfPgno( cpgSLVFileMin + 1 );
	const FMP		*pfmp			= &rgfmp[ ifmp & ifmpMask ];
	BOOL			fInTransaction	= fFalse;
	SLVFILEHDR		*pslvfilehdr	= NULL;

	Assert( NULL != rgfmp[ifmp].SzSLVName() );

	Assert( !rgfmp[ifmp].PfapiSLV() );

	Assert( rgfmp[ifmp].FCreatingDB() );

	if ( pfmp->FLogOn() )
		{
		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fTrue;
		}
		
	if ( createOptions & SLV_CREATESLV_CREATE )
		{
		pslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
		if ( NULL == pslvfilehdr )
			{
			err = ErrERRCheck( JET_errOutOfMemory );
			return err;
			}

		memset( pslvfilehdr, 0, sizeof(SLVFILEHDR) );

		pslvfilehdr->le_ulMagic = ulDAEMagic;
		pslvfilehdr->le_ulVersion = ulDAEVersion;
		pslvfilehdr->le_ulUpdate = ulDAEUpdate;
		pslvfilehdr->le_ulCreateVersion = ulDAEVersion;
		pslvfilehdr->le_ulCreateUpdate = ulDAEUpdate;
		pslvfilehdr->le_attrib = attribSLV;
		pslvfilehdr->le_cbPageSize = g_cbPage;
		pslvfilehdr->le_dwMajorVersion 		= dwGlobalMajorVersion;
		pslvfilehdr->le_dwMinorVersion 		= dwGlobalMinorVersion;
		pslvfilehdr->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pslvfilehdr->le_lSPNumber 			= lGlobalSPNumber;

		pslvfilehdr->le_filetype 			= filetypeSLV;

		memcpy( &pslvfilehdr->signSLV, &rgfmp[ifmp].Pdbfilehdr()->signSLV, sizeof(SIGNATURE) );
		Call( ErrSLVSyncHeader(	pfsapi, 
								rgfmp[ifmp].FReadOnlyAttach(),
								rgfmp[ifmp].SzSLVName(),
								rgfmp[ifmp].Pdbfilehdr(),
								pslvfilehdr ) );

		Call( pfsapi->ErrFileOpen( rgfmp[ifmp].SzSLVName(), &pfapiSLV ) );
		Assert( pfapiSLV );
		Call( pfapiSLV->ErrSetSize( cbSize ) );
		delete pfapiSLV;
		pfapiSLV = NULL;
		}

	if ( createOptions & SLV_CREATESLV_ATTACH )
		{
		Call( ErrSLVCreate( ppib, ifmp ) );
		}

	if ( createOptions & SLV_CREATESLV_OPEN )
		{
		Call( ErrFILEOpenSLV( pfsapi, fRecovering ? ppibNil : ppib, ifmp ) );
		Assert ( JET_errSuccess >= err );
		}

	if ( pfmp->FLogOn() )
		{
		Call( ErrDIRCommitTransaction( ppib, 0 ) );
		fInTransaction = fFalse;
		}
HandleError:	
	delete pfapiSLV;

	if ( fInTransaction )
		{
		(VOID)ErrDIRRollback( ppib );
		}

	if ( err < 0 )
		{
		// clear, if needed after the ErrSLVCreate call
		SLVOwnerMapTerm( ifmp, fFalse );
		
		//	on error, try to delete SLV file
		(VOID)pfsapi->ErrFileDelete( rgfmp[ifmp].SzSLVName() );
		}

	if ( pslvfilehdr != NULL )
		{
		OSMemoryPageFree( (VOID *)pslvfilehdr );
		}

	return err;
	}
	
//  ================================================================
ERR ErrSLVAvailMapInit( PIB *ppib, const IFMP ifmp, const PGNO pgnoSLV, FCB **ppfcbSLV )
//  ================================================================
	{
	ERR		err;
	FUCB	*pfucbSLV;
	FCB		*pfcbSLV;
	
	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !PinstFromIfmp( ifmp )->FRecovering()
		|| PinstFromIfmp( ifmp )->m_plog->m_fHardRestore );

	// Link LV FCB into table.
	CallR( ErrDIROpen( ppib, pgnoSLV, ifmp, &pfucbSLV ) );
	Assert( pfucbNil != pfucbSLV );
	Assert( !FFUCBVersioned( pfucbSLV ) );	// Verify won't be deferred closed.
	pfcbSLV = pfucbSLV->u.pfcb;
	Assert( !pfcbSLV->FInitialized() );

	Assert( pfcbSLV->Ifmp() == ifmp );
	Assert( pfcbSLV->PgnoFDP() == pgnoSLV );
	Assert( pfcbSLV->Ptdb() == ptdbNil );
	Assert( pfcbSLV->CbDensityFree() == 0 );

	Assert( pfcbSLV->FTypeNull() );
	pfcbSLV->SetTypeSLVAvail();

	Assert( pfcbSLV->PfcbTable() == pfcbNil );

	//	finish the initialization of this SLV FCB

	pfcbSLV->CreateComplete();

	DIRClose( pfucbSLV );

	rgfmp[ifmp].SetPfcbSLVAvail( pfcbSLV );
	*ppfcbSLV = pfcbSLV;

	return err;
	}


VOID SLVAvailMapTerm( const IFMP ifmp, const BOOL fTerminating )
	{
	FMP	* const pfmp				= rgfmp + ifmp;
	Assert( pfmp );

	FCB	* const pfcbSLVAvailMap 	= pfmp->PfcbSLVAvail();
	
	if ( pfcbNil != pfcbSLVAvailMap )
		{
		//	synchronously purge the FCB
		Assert( pfcbSLVAvailMap->FTypeSLVAvail() );
		pfcbSLVAvailMap->PrepareForPurge();

		//	there should only be stray cursors if we're in the middle of terminating,
		//	in all other cases, all cursors should have been properly closed first
		Assert( 0 == pfcbSLVAvailMap->WRefCount() || fTerminating );
		if ( fTerminating )
			pfcbSLVAvailMap->CloseAllCursors( fTrue );

		pfcbSLVAvailMap->Purge();
		pfmp->SetPfcbSLVAvail( pfcbNil );
		}
	}


LOCAL ERR ErrSLVNewSize( const IFMP ifmp, const CPG cpg )
	{
	ERR			err;
	CPG			cpgCacheSizeOld = 0;

	/*	set new EOF pointer
	/**/
	QWORD cbSize = OffsetOfPgno( cpg + 1 );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !PinstFromIfmp( ifmp )->FRecovering()
		|| PinstFromIfmp( ifmp )->m_plog->m_fHardRestore );

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache 
			|| PinstFromIfmp( ifmp )->FRecovering() );

	if( pslvspacenodecache )
		{
		CallR( pslvspacenodecache->ErrReserve( cpg ) );
		}
	rgfmp[ifmp].SemIOExtendDB().Acquire();
	err = rgfmp[ifmp].PfapiSLV()->ErrSetSize( cbSize );
	rgfmp[ifmp].SemIOExtendDB().Release();
	Assert( err < 0 || err == JET_errSuccess );
	if ( JET_errSuccess == err )
		{
		/*	set database size in FMP -- this value should NOT include the reserved pages
		/**/
		cbSize = QWORD( cpg ) * g_cbPage;
		QWORD cbGrow = cbSize - rgfmp[ ifmp ].CbSLVFileSize();
		rgfmp[ifmp].SetSLVFileSize( cbSize );

		rgfmp[ ifmp ].IncSLVSpaceCount( SLVSPACENODE::sFree, CPG( cbGrow / SLVPAGE_SIZE ) );

		CallS( pslvspacenodecache->ErrGrowCache( cpg ) );		
		}

	return err;
	}


LOCAL ERR ErrSLVInit( PIB *ppib, const IFMP ifmp )
	{
	ERR			err;
	FMP			*pfmp			= rgfmp + ifmp;
	INST		*pinst 			= PinstFromIfmp( ifmp );
	DIB			dib;
	PGNO		pgnoLast;
	PGNO		pgnoSLVAvail;
	OBJID		objidSLVAvail;
	FUCB		*pfucbSLVAvail	= pfucbNil;
	FCB			*pfcbSLVAvail	= pfcbNil;
	const BOOL	fTempDb			= ( dbidTemp == pfmp->Dbid() );

	BOOL 		fInTransaction 	= fFalse;
	
	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !pinst->FRecovering()
			|| pinst->m_plog->m_fHardRestore );

#ifndef RTM
	SLVSPACENODE::Test();
	Call( SLVSPACENODECACHE::ErrUnitTest() );
#endif	//	!RTM

	if ( fTempDb )
		{
		pgnoSLVAvail = pgnoTempDbSLVAvail;
		}
	else
		{
		Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );
		}

	if ( pfcbNil == pfmp->PfcbSLVOwnerMap() )
		{
		Call ( ErrSLVOwnerMapInit( ppib, ifmp ) );
		}
	Assert( pfcbNil != pfmp->PfcbSLVOwnerMap() );

	Assert( pfcbNil == pfmp->PfcbSLVAvail() );
	Assert( pgnoNull != pgnoSLVAvail );
	Call( ErrSLVAvailMapInit( ppib, ifmp, pgnoSLVAvail, &pfcbSLVAvail ) );
	Assert( pfcbNil != pfcbSLVAvail );
	Assert( pfmp->PfcbSLVAvail() == pfcbSLVAvail );

	Call( ErrBTOpen( ppib, pgnoSLVAvail, ifmp, &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Assert( pfucbSLVAvail->u.pfcb == pfcbSLVAvail );
	
	dib.dirflag = fDIRNull;
	dib.pos 	= posLast;
	Call( ErrBTDown( pfucbSLVAvail, &dib, latchReadTouch ) );

	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );

	BTUp( pfucbSLVAvail );

	Assert( 0 == pfmp->CbSLVFileSize() );
	if ( FFUCBUpdatable( pfucbSLVAvail )
		&& ppib->FSetAttachDB()
		// don't reclaim the SLV space at the end of recovery
		// it will be done at restart time
		&& !ppib->FRecoveringEndAllSessions() )
		{		
		//  allocate the SLVSPACENODECACHE in the FMP
		SLVSPACENODECACHE * const pslvspacenodecache = new SLVSPACENODECACHE(
															pinst->m_lSLVDefragFreeThreshold,
															pinst->m_lSLVDefragFreeThreshold,
															pinst->m_lSLVDefragMoveThreshold );
		if( NULL == pslvspacenodecache )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		Assert( NULL == pfmp->Pslvspacenodecache() );
		pfmp->SetPslvspacenodecache( pslvspacenodecache );

		Call( pslvspacenodecache->ErrGrowCache( pgnoLast ) );

		//	move all reserved/deleted SLV space to the Free state
		ULONG cpgSeen;
		ULONG cpgReset;
		ULONG cpgFree;
		Call( ErrSLVResetAllReservedOrDeleted( pfucbSLVAvail, &cpgSeen, &cpgReset, &cpgFree ) );
		Assert( cpgSeen == pgnoLast );

#ifndef RTM
		//  compare the cache against the actual CPG avails
		CallS( pslvspacenodecache->ErrValidate( pfucbSLVAvail ) );
#endif	//	!RTM		

		//	after recovery, SLV file size may not be the correct size
		//	so ensure that the file size matches what the SLV space tree
		//	thinks it should be
		Call( ErrSLVNewSize( ifmp, (CPG)pgnoLast ) );
		Assert( pfmp->CbSLVFileSize() == QWORD( pgnoLast ) * QWORD( g_cbPage ) );

		//  init SLV Space stats

		pfmp->ResetSLVSpaceOperCount();
		pfmp->ResetSLVSpaceCount();
		pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, CPG( cpgFree ) );
		pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, CPG( cpgSeen - cpgFree ) );
		}
	else
		{
		pfmp->SetSLVFileSize( QWORD( pgnoLast ) * QWORD( g_cbPage ) );
		}

	if ( FFUCBUpdatable( pfucbSLVAvail ) )
		{
		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fTrue;

		Call( ErrSLVOwnerMapNewSize( ppib, ifmp, pfmp->PgnoSLVLast(), fSLVOWNERMAPNewSLVInit ) );
		
		Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fFalse;
		}

	BTClose( pfucbSLVAvail );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	if ( fInTransaction )
		{
		(VOID) ErrDIRRollback( ppib );
		fInTransaction = fFalse;
		}

	if ( pfcbSLVAvail != pfcbNil )
		{

		//	we need to purge the FCB

		if ( pfucbSLVAvail != pfucbNil )
			{
			//	close the FUCB
			Assert( pfcbSLVAvail->WRefCount() == 1 );
			BTClose( pfucbSLVAvail );
			}

		SLVAvailMapTerm( ifmp, fFalse );
		}
	else
		{
		//	impossible to have an FUCB open without an underlying FCB
		Assert( pfucbSLVAvail == pfucbNil );
		Assert( pfcbNil == pfmp->PfcbSLVAvail() );
		}

	//	reset all SLV variables
	SLVOwnerMapTerm( ifmp, fFalse );
	
	pfmp->SetSLVFileSize( 0 );

	SLVSPACENODECACHE * const pslvspacenodecacheT = pfmp->Pslvspacenodecache();
	pfmp->SetPslvspacenodecache( NULL );

	//  free the SLVSPACENODECACHE
	delete pslvspacenodecacheT;

	return err;
	}


ERR ErrSLVReadHeader(	IFileSystemAPI * const	pfsapi, 
						const BOOL				fReadOnly,
						const CHAR * const		szSLVName,
						DBFILEHDR * const		pdbfilehdr,
						SLVFILEHDR * const		pslvfilehdr,
						IFileAPI * const		pfapi )
	{
	ERR err;

	Assert( NULL != pslvfilehdr );
	Assert( NULL != szSLVName );

	err = ( fReadOnly ? ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )(
								pfsapi, 
								szSLVName, 
								(BYTE *)pslvfilehdr, 
								g_cbPage, 
								OffsetOf( SLVFILEHDR, le_cbPageSize ),
								pfapi );
	if ( err < 0 )
		{
		if ( JET_errFileNotFound == err )
			{
			err = ErrERRCheck( JET_errSLVStreamingFileMissing );
			}
		else if ( JET_errDiskIO == err )
			{
			err = ErrERRCheck( JET_errSLVHeaderBadChecksum );
			}
		}
	else if ( attribSLV != pslvfilehdr->le_attrib )
		{
		err = ErrERRCheck( JET_errSLVHeaderCorrupted );
		}
	else
		{
		err = ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr );
		}

	return err;
	}


VOID SLVSpaceRequest( const IFMP ifmpDb, const QWORD cbRecommended );
ERR ErrSLVSpaceFree( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted );

//  ================================================================
ERR ErrFILEOpenSLV( IFileSystemAPI *const pfsapi, PIB *ppib, const IFMP ifmp )
//  ================================================================
	{
	ERR				err;
	FMP				*pfmp			= rgfmp + ifmp;
	INST			*pinst			= PinstFromIfmp( ifmp );
	IFileAPI		*pfapiSLV		= NULL;
	SLVFILEHDR		*pslvfilehdr	= NULL;
	SLVROOT			slvroot			= slvrootNil;
	BOOL			fInitSLV		= fTrue;

	Assert( NULL != pfmp->SzSLVName() );
	Assert( !pfmp->FSLVAttached() );
	Assert( 0 == pfmp->CbSLVFileSize() );
	Assert( pfcbNil == pfmp->PfcbSLVAvail() );

	if ( ppibNil != ppib )
		{
		if ( pinst->FRecovering() )
			{
			if ( ppib->FRecoveringEndAllSessions() )
				{
				//	at the end of hard restore where we re-attach
				//	to the db because it moved (in ErrLGRIEndAllSessions())
				Assert( pinst->m_plog->m_fHardRestore );
				}
			else
				{
				//	redoing a CreateDb
				Assert( pfmp->FCreatingDB() );
				fInitSLV = fFalse;
				}
			}

		Assert( pfmp->FAttachingDB()
			|| pfmp->FCreatingDB()
			|| ppib->FSetAttachDB() );
		}
	else
		{
		Assert( pinst->FRecovering() );
		fInitSLV = fFalse;
		}

	Assert( !pfmp->FSLVAttached() );

	pslvfilehdr	= (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	if ( NULL == pslvfilehdr )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	err = ErrSLVReadHeader(	pfsapi, 
							pfmp->FReadOnlyAttach(),
							pfmp->SzSLVName(),
							pfmp->Pdbfilehdr(),
							pslvfilehdr,
							NULL );

	OSMemoryPageFree( (VOID *)pslvfilehdr );
	pslvfilehdr = NULL;

	Call( err );
	CallS( err );

	//  open the SLV streaming file

	Call( pfsapi->ErrFileOpen( pfmp->SzSLVName(), &pfapiSLV, pfmp->FReadOnlyAttach() ) );
	Assert( !pfmp->PfapiSLV() );
	pfmp->SetPfapiSLV( pfapiSLV );
	pfapiSLV = NULL;

	if ( fInitSLV )
		{
		Assert( ppibNil != ppib );

		// For temp. tables, if initialisation fails here, the space
		// for the LV will be lost, because it's not persisted in
		// the catalog.
		Call( ErrSLVInit( ppib, ifmp ) );

		Assert( pfmp->PgnoSLVLast() >= cpgSLVFileMin );
		Assert( pfcbNil != pfmp->PfcbSLVAvail() );
		}
	else
		{
		Assert( !pinst->FSLVProviderEnabled() );

		Assert( 0 == pfmp->CbSLVFileSize() );
		Assert( pfcbNil == pfmp->PfcbSLVAvail() );
		}

	//  the SLV Provider is enabled

	if ( pinst->FSLVProviderEnabled() )
		{
		Assert( !pinst->FRecovering() );

		//  close the SLV Streaming File
		
		delete pfmp->PfapiSLV();
		pfmp->SetPfapiSLV( NULL );

		//  build the root path and backing file path for the SLV Root

		wchar_t wszRootPath[IFileSystemAPI::cchPathMax];
		Call( ErrOSSTRAsciiToUnicode(	rgfmp[ ifmp ].SzSLVRoot(),
										wszRootPath,
										IFileSystemAPI::cchPathMax ) );
		
		wchar_t wszBackingFile[IFileSystemAPI::cchPathMax];
		Call( ErrOSSTRAsciiToUnicode(	rgfmp[ ifmp ].SzSLVName(),
										wszBackingFile,
										IFileSystemAPI::cchPathMax ) );

		//  open the SLV Root and SLV streaming file via the SLV Provider
		//
		//  NOTE:  as soon as the root is created, the below callbacks can fire

		Call( ErrOSSLVRootCreate(	wszRootPath,
									pinst->m_pfsapi,
									wszBackingFile,
									PSLVROOT_SPACEREQ( SLVSpaceRequest ),
									DWORD_PTR( ifmp ),
									PSLVROOT_SPACEFREE( ErrSLVSpaceFree ),
									DWORD_PTR( ifmp ),
									fFalse,
									&slvroot,
									&pfapiSLV ) );

		Assert( pfmp->SlvrootSLV() == slvrootNil );
		pfmp->SetSlvrootSLV( slvroot );
		slvroot = NULL;

		Assert( !pfmp->PfapiSLV() );
		pfmp->SetPfapiSLV( pfapiSLV );
		pfapiSLV = NULL;
		}

	Assert( pfmp->FSLVAttached() );
	return JET_errSuccess;


HandleError:
	SLVClose( ifmp );
	return err;
	}


VOID SLVClose( const IFMP ifmp )
	{
	FMP::AssertVALIDIFMP( ifmp );
	FMP 	*pfmp	= rgfmp + ifmp;
	Assert( pfmp );

	//  close the SLV Root
	//
	//  NOTE:  as soon as the SLV Root is closed we no longer need to worry
	//  about our callbacks firing
		
	if ( pfmp->SlvrootSLV() != slvrootNil )
		{
		OSSLVRootClose( pfmp->SlvrootSLV() );
		pfmp->SetSlvrootSLV( slvrootNil );
		pfmp->SetPfapiSLV( NULL );  //  do not close this!
		}

	//  close the SLV Streaming File
	
	delete pfmp->PfapiSLV();
	pfmp->SetPfapiSLV( NULL );

	//	synchronously purge the FCB
	SLVAvailMapTerm( ifmp, fFalse );
	SLVOwnerMapTerm( ifmp, fFalse );
	
	pfmp->SetSLVFileSize( 0 );

	SLVSPACENODECACHE * const pslvspacenodecache = pfmp->Pslvspacenodecache();
	pfmp->SetPslvspacenodecache( NULL );

	//  free the SLVSPACENODECACHE
	delete pslvspacenodecache;
	}

LOCAL ERR ErrSLVExtendSLV( FUCB *pfucbSLVAvail, CPG cpgReq, PGNO *ppgnoAfterAllExtend )
	{
	ERR			err;
	const IFMP	ifmp			= pfucbSLVAvail->ifmp;
	PGNO		pgnoLast;
	DIB			dib;
	PGNO		cpgEDBFile;
	PGNO		cpgSTMFile;
	PGNO		cpgQuota;
	PGNO		cpgSTMFileRemaining;
	PGNO		cpgQuotaRemaining;
	PGNO		cpgRemaining;
	PGNO		cpgGrow;

	//  get the last pgno used in the streaming file

	dib.pos = posLast;
	dib.dirflag = fDIRNull;

	CallR( ErrBTDown( pfucbSLVAvail, &dib, latchReadTouch ) );
	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );
	BTUp( pfucbSLVAvail );
	
	//	verify pages allocated in fixed chunks
	
	Assert( 0 == pgnoLast % cpgSLVExtent );

	//  compute the remaining pages available for allocation taking into account
	//  the remaining space in the streaming file as well as any database size
	//  quotas that may exist

	cpgEDBFile			= rgfmp[ifmp].PgnoLast();
	cpgSTMFile			= pgnoLast;
	cpgQuota			= rgfmp[ifmp].CpgDatabaseSizeMax();

	cpgSTMFileRemaining	= 0 - cpgSTMFile;
	cpgQuotaRemaining	= (	cpgQuota
								? (	cpgEDBFile + cpgSTMFile > cpgQuota
									? 0
									: cpgQuota - cpgEDBFile - cpgSTMFile )
								: cpgSTMFileRemaining );
	cpgRemaining		= min( cpgSTMFileRemaining, cpgQuotaRemaining );
	cpgRemaining		= cpgRemaining - cpgRemaining % cpgSLVExtent;

	//  compute how much we can grow the file

	//	round up to system parameter
	
	cpgGrow				= cpgReq + PinstFromPfucb( pfucbSLVAvail )->m_cpgSESysMin - 1;
	cpgGrow				= cpgGrow - cpgGrow % PinstFromPfucb( pfucbSLVAvail )->m_cpgSESysMin;

	//	round up to required SLV Extent sized chunks
	
	cpgGrow				= cpgGrow + cpgSLVExtent - 1;
	cpgGrow				= cpgGrow - cpgGrow % cpgSLVExtent;
	
	cpgGrow				= min( cpgGrow, cpgRemaining );

	//  we should be growing the file in SLV Extent sized chunks

	Assert( !( cpgGrow % cpgSLVExtent ) );

	//  we cannot grow the file

	if ( !cpgGrow )
		{
		//  report that we are out of space and fail

		char		szT[16];
		const char	*rgszT[2]	= { rgfmp[ifmp].SzDatabaseName(), szT };

		sprintf( szT, "%d", (ULONG)( ( (QWORD)( cpgEDBFile + cpgSTMFile ) * (QWORD)g_cbPage ) >> 20 ) );
					
		UtilReportEvent(
				eventWarning,
				SPACE_MANAGER_CATEGORY,
				SPACE_MAX_DB_SIZE_REACHED_ID,
				2,
				rgszT );

		CallR( ErrERRCheck( JET_errOutOfDatabaseSpace ) );
		}

	//	try to allocate more space from the device.  if we can't get what we
	//  want, try growing by the minimum size before giving up

	err = ErrSLVNewSize( ifmp, pgnoLast + cpgGrow );
	if ( err < JET_errSuccess )
		{
		CallR( ErrSLVNewSize( ifmp, pgnoLast + cpgSLVExtent ) );
		cpgGrow = cpgSLVExtent;
		}

	//	insert the space map for the newly allocated space

	const PGNO	pgnoLastAfterOneExtend	= pgnoLast + cpgSLVExtent;
	const PGNO	pgnoLastAfterAllExtend	= pgnoLast + cpgGrow;

	Assert( 0 == cpgGrow % cpgSLVExtent );
	Assert( 0 == pgnoLast % cpgSLVExtent );
	Assert( 0 == pgnoLastAfterOneExtend % cpgSLVExtent );
	Assert( 0 == pgnoLastAfterAllExtend % cpgSLVExtent );
	Assert( pgnoLastAfterOneExtend <= pgnoLastAfterAllExtend );
	
	do
		{
		pgnoLast += cpgSLVExtent;
		CallR( ErrSLVInsertSpaceNode( pfucbSLVAvail, pgnoLast ) );
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		}
	while ( pgnoLast < pgnoLastAfterAllExtend );
	Assert( pgnoLastAfterAllExtend == pgnoLast );

	Assert ( NULL != ppgnoAfterAllExtend );
	*ppgnoAfterAllExtend = pgnoLastAfterAllExtend;
	if ( pgnoLastAfterOneExtend == pgnoLastAfterAllExtend )
		{
		//	the correct node is write latched -- just exit
		}
	else
		{
		BOOKMARK	bm;
		BYTE		rgbKey[sizeof(PGNO)];

		KeyFromLong( rgbKey, pgnoLastAfterOneExtend );
		bm.key.prefix.Nullify();
		bm.key.suffix.SetCb( sizeof(PGNO) );
		bm.key.suffix.SetPv( rgbKey );
		bm.data.Nullify();
		
		//	must go back to the first node inserted and write latch that node
		//	WriteLatch will ensure no one consumed it

		BTUp( pfucbSLVAvail );
		CallR( ErrBTGotoBookmark( pfucbSLVAvail, bm, latchRIW, fTrue  ) );
		CallS( err );

		CallS( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
		}

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	return err;
	}


LOCAL ERR ErrSLVFindNextExtentWithFreePage( FUCB *pfucbSLVAvail, const BOOL fForDefrag )
	{
	ERR			err;
	CPG			cpgAvail;
	BOOKMARK	bm;
	DIB		 	dib;
	BYTE		rgbKey[sizeof(PGNO)];
	const LATCH	latch			= latchRIW;//latchReadTouch;

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[pfucbSLVAvail->ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache );

	PGNO		pgnoCache;
	BOOL		f;

Start:
	pgnoCache = pgnoNull;
	if( !fForDefrag )
		{
		f = pslvspacenodecache->FGetCachedLocationForNewInsertion( &pgnoCache );
		if( f )
			{
///			printf( "Cached insertion: %d\n", pgnoCache );
			}
		else
			{
///			printf( "Out of cached insertion space\n" );
			}
		}
	else
		{
		f = pslvspacenodecache->FGetCachedLocationForDefragInsertion( &pgnoCache );
		if( f )
			{
///			printf( "Cached defrag: %d\n", pgnoCache );
			}
		else
			{
///			printf( "Out of cached defrag space\n" );
			}
		}
		
	if( !f )
		{
		err = ErrERRCheck( wrnSLVNoFreePages );
		return err;
		}
		
	KeyFromLong( rgbKey, pgnoCache );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	
	dib.pos 	= posDown;
	dib.pbm 	= &bm;
	dib.dirflag = fDIRNull;

	err = ErrBTDown( pfucbSLVAvail, &dib, latch );
	
	if ( err >= JET_errSuccess )
		{
		Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );
		
		cpgAvail = ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->CpgAvail();
		Assert( cpgAvail >= 0 );

		if ( cpgAvail > 0 )
			{
			//	upgrade to write latch so that we can use this node
			err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
			if ( errBFLatchConflict == err )
				{
				Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
				goto Start;
				}
			CallR( err );

			//	verify no change
			Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
			Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );
			Assert( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->CpgAvail() == cpgAvail );

			return JET_errSuccess;
			}
		else
			{
			BTUp( pfucbSLVAvail );
			goto Start;
			}
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVGetFreePageRange(
	FUCB		*pfucbSLVAvail,
	PGNO		*ppgnoFirst,			//	initially pass in first page of extent
	CPG			*pcpgReq,
	const BOOL	fReserveOnly )
//  ================================================================
//
//	first the range of free pages within this extent, and continue searching subsequent
//	extents if necessary
//
//-
	{
	ERR			err;
	CPG			cpgReq			= *pcpgReq;
	PGNO		pgnoLast		= *ppgnoFirst + cpgSLVExtent - 1;

	//	must be in transaction because caller will be required to rollback on failure
	Assert( pfucbSLVAvail->ppib->level > 0 );

	Assert( *ppgnoFirst > pgnoNull );
	Assert( cpgReq > 0 );

	Assert( 0 == pgnoLast % cpgSLVExtent );

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	Call( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->ErrGetFreePagesFromExtent(
				pfucbSLVAvail,
				ppgnoFirst,
				pcpgReq,
				fReserveOnly ) );
	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );

	Assert( *pcpgReq > 0 );
	Assert( *pcpgReq <= cpgReq );
	Assert( *ppgnoFirst + *pcpgReq - 1 <= pgnoLast );

	//	if we hit the end of this extent and we still want more space,
	//	keep going
	while ( *ppgnoFirst + *pcpgReq - 1 == pgnoLast
		&& *pcpgReq < cpgReq )
		{
		PGNO	pgnoLastT;
		CPG		cpgT;

		Pcsr( pfucbSLVAvail )->Downgrade( latchReadTouch );

		err = ErrBTNext( pfucbSLVAvail, fDIRNull );
		Assert( JET_errRecordNotFound != err );
		if ( JET_errNoCurrentRecord == err )
			{
			break;
			}
		Call( err );
		
		err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
		if ( errBFLatchConflict == err )
			{
			//	next page latched by someone else, just bail out now and be
			//	happy with what we have
			Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
			break;
			}
		Call( err );
		
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

		LongFromKey( &pgnoLastT, pfucbSLVAvail->kdfCurr.key );
		Assert( 0 == (CPG)pgnoLastT % cpgSLVExtent );
		Assert( pgnoLastT > pgnoLast );
		if ( pgnoLastT != pgnoLast + cpgSLVExtent )
			{
			Assert( fFalse );		//	UNDONE: currently impossible because we don't delete SLV space nodes
			break;
			}
		pgnoLast = pgnoLastT;

		Assert( *pcpgReq < cpgReq );
		cpgT = ( cpgReq - *pcpgReq );
		Assert( cpgT > 0 );

		Call( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->ErrGetFreePagesFromExtent(
					pfucbSLVAvail,
					NULL,
					&cpgT,
					fReserveOnly ) );
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );

		Assert( cpgT <= ( cpgReq - *pcpgReq ) );
		*pcpgReq += cpgT;

		Assert( *pcpgReq <= cpgReq );
		Assert( *ppgnoFirst + *pcpgReq - 1 <= pgnoLast );
		}

	err = JET_errSuccess;

HandleError:
	BTUp( pfucbSLVAvail );
	return err;
	}


LOCAL ERR ErrSLVIGetPages(
	FUCB		*pfucbSLVAvail,
	PGNO		*ppgnoFirst,
	CPG			*pcpgReq,
	const BOOL	fReserveOnly,		//	set to TRUE of Free->Reserved, else Free->Committed
	const BOOL	fForDefrag )		//  set to TRUE if we want space to move an existing run into
	{
	ERR			err;
	FCB			*pfcbSLVAvail		= pfucbSLVAvail->u.pfcb;
	BOOL		fMayExtend			= fFalse;

	PGNO 		pgnoLastAfterAllExtend 	= pgnoNull;

	FMP * 		pfmp 				= &rgfmp[ pfucbSLVAvail->ifmp ];
	BOOL 		fExtended 			= fFalse;

	Assert( pfucbNil != pfucbSLVAvail );
	Assert( pfcbNil != pfucbSLVAvail->u.pfcb );

	//	must ask for at least one page
	Assert( *pcpgReq > 0 );
	
Start:
	if ( fMayExtend )
		{
		pfmp->RwlSLVSpace().EnterAsWriter();
		}
	else
		{
		pfmp->RwlSLVSpace().EnterAsWriter();
		}

	Call( ErrSLVFindNextExtentWithFreePage( pfucbSLVAvail, fForDefrag ) );

	if ( wrnSLVNoFreePages == err )
		{
		BTUp( pfucbSLVAvail );

		if ( !fMayExtend )
			{
			pfmp->RwlSLVSpace().LeaveAsWriter();
			fMayExtend = fTrue;
			goto Start;
			}

		Call( ErrSLVExtendSLV( pfucbSLVAvail, *pcpgReq, &pgnoLastAfterAllExtend ) );
		fExtended = fTrue;
		BTUp( pfucbSLVAvail );
		Call( ErrSLVFindNextExtentWithFreePage( pfucbSLVAvail, fForDefrag ) );			
		Enforce( wrnSLVNoFreePages != err );
		}

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
	Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

	PGNO	pgnoLast;
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );
	*ppgnoFirst = pgnoLast - cpgSLVExtent + 1;
	Call( ErrSLVGetFreePageRange(
				pfucbSLVAvail,
				ppgnoFirst,
				pcpgReq,
				fReserveOnly ) );

	Assert( *pcpgReq > 0 );
	Assert( pgnoNull != *ppgnoFirst );
	Assert( *ppgnoFirst >= pgnoLast - cpgSLVExtent + 1 );
	Assert( *ppgnoFirst <= pgnoLast );

	// check if we have to extend the SLVSpace Map if new space is allocated
	if ( fExtended )
		{
		Assert ( fMayExtend );
		Assert ( pgnoLastAfterAllExtend != pgnoNull );
		Call( ErrSLVOwnerMapNewSize(
					pfucbSLVAvail->ppib,
					pfucbSLVAvail->ifmp,
					pgnoLastAfterAllExtend,
					fSLVOWNERMAPNewSLVGetPages ) );
		}

HandleError:
	Assert( pfcbNil != pfcbSLVAvail );

	if ( fMayExtend )
		{
		pfmp->RwlSLVSpace().LeaveAsWriter();
		}
	else
		{
		pfmp->RwlSLVSpace().LeaveAsWriter();
		}

	return err;
	}
	
ERR ErrSLVGetPages(
	PIB			*ppib,
	const IFMP	ifmp,
	PGNO		*ppgnoFirst,
	CPG			*pcpgReq,
	const BOOL	fReserveOnly,		//	set to TRUE of Free->Reserved, else Free->Committed
	const BOOL  fForDefrag )		//  set to TRUE if we want space to move an existing run into
	{
	ERR			err;
	FUCB		*pfucbSLVAvail			= pfucbNil;
	BOOL		fInTransaction			= fFalse;

	//	must ask for at least one page
	Assert( *pcpgReq > 0 );
	
	Assert( rgfmp[ifmp].FSLVAttached() );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVAvail() );
	
	CallR( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVAvail(), &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

	Assert( rgfmp[ifmp].PfcbSLVAvail() == pfucbSLVAvail->u.pfcb );
	Call( ErrSLVIGetPages( pfucbSLVAvail, ppgnoFirst, pcpgReq, fReserveOnly, fForDefrag ) );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fFalse;

	//	must return at least one page
	Assert( *pcpgReq > 0 );
	Assert( pgnoNull != *ppgnoFirst );

HandleError:
	if ( fInTransaction )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	Assert( pfucbNil != pfucbSLVAvail );
	DIRClose( pfucbSLVAvail );
		
	return err;
	}


LOCAL ERR ErrSLVMapUnexpectedStateError( const SLVSPACEOPER slvspaceoper )
	{
	ERR		err;
	switch ( slvspaceoper )
		{
		case slvspaceoperReservedToCommitted:
		case slvspaceoperFreeReserved:
			err = ErrERRCheck( JET_errSLVPagesNotReserved );
			break;
		case slvspaceoperCommittedToDeleted:
			err = ErrERRCheck( JET_errSLVPagesNotCommitted );
			break;
		case slvspaceoperDeletedToFree:
			err = ErrERRCheck( JET_errSLVPagesNotDeleted );
			break;
		default:
			Assert( fFalse );	//	unexpected case
			err = ErrERRCheck( JET_errSLVSpaceCorrupted );
			break;
		}

	return err;
	}


LOCAL ERR ErrSLVChangePageState(
	PIB						*ppib,
	const IFMP				ifmp,
	const SLVSPACEOPER		slvspaceoper,
	PGNO					pgnoFirst,
	CPG						cpgToChange,
	CSLVInfo::FILEID		fileid			= CSLVInfo::fileidNil,
	QWORD					cbAlloc			= 0,
	const wchar_t* const	wszFileName		= L"" )
	{
	ERR					err;
	FCB					*pfcbSLVAvail		= rgfmp[ifmp].PfcbSLVAvail();
	FUCB				*pfucbSLVAvail		= pfucbNil;
	PGNO				pgnoLastInExtent;
	BOOKMARK			bm;
	DIB		 			dib;
	BYTE				rgbKey[sizeof(PGNO)];
	BOOL				fInTransaction		= fFalse;
	CSLVInfo			slvinfo;
	LATCH				latch				= latchReadTouch;
	DIRFLAG				fDIR;

	PGNO				pgnoFirstCopy 		= pgnoFirst;
	CPG					cpgToChangeCopy 	= cpgToChange;

	Assert( rgfmp[ifmp].FSLVAttached() );
	Assert( pfcbNil != pfcbSLVAvail );

	//	this function only supports certain transitions
	
	switch ( slvspaceoper )
		{
		case slvspaceoperReservedToCommitted:
		case slvspaceoperCommittedToDeleted:
			fDIR = fDIRNull;
			break;
		case slvspaceoperFreeReserved:
		case slvspaceoperDeletedToFree:
			fDIR = fDIRNoVersion;
			break;
		default:
			Assert( fFalse );
			break;
		}

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;

	CallR( ErrDIROpen( ppib, pfcbSLVAvail, &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

Start:
	//	find the chunk containing the first page of our range
	KeyFromLong( rgbKey, ( ( pgnoFirst + cpgSLVExtent - 1 ) / cpgSLVExtent ) * cpgSLVExtent );
	Assert( bm.key.prefix.FNull() );
	Assert( bm.key.suffix.Pv() == rgbKey );
	Assert( bm.key.Cb() == sizeof(PGNO) );
	Assert( posDown == dib.pos );
	Assert( &bm == dib.pbm );
	Assert( fDIRFavourNext == dib.dirflag );
	
	err = ErrBTDown( pfucbSLVAvail, &dib, latch );
	if ( JET_errSuccess != err )
		{
		//	shouldn't get warnings, because we fixed up the seek key to seek exactly to a chunk
		Assert( err < 0 );
		if ( JET_errRecordNotFound == err || err > 0 )
			{
			err = ErrSLVMapUnexpectedStateError( slvspaceoper );
			}

		goto HandleError;
		}

	Assert( Pcsr( pfucbSLVAvail )->FLatched() );
	LongFromKey( &pgnoLastInExtent, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLastInExtent % cpgSLVExtent );

	if ( pgnoFirst > pgnoLastInExtent
		|| pgnoFirst <= pgnoLastInExtent - cpgSLVExtent )
		{
		//	should be impossible
		Assert( fFalse );
		Call( ErrERRCheck( JET_errSLVSpaceCorrupted ) );
		}

	LONG	ipage;
	LONG	cpages;

	Assert( pgnoFirst >= ( pgnoLastInExtent - cpgSLVExtent + 1 ) );
	ipage = pgnoFirst - ( pgnoLastInExtent - cpgSLVExtent + 1 );

	Assert( ipage >= 0 );
	Assert( ipage < cpgSLVExtent );

	forever
		{
		//	upgrade to write latch so that we can use this node
		err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
		if ( errBFLatchConflict == err )
			{
			Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
			latch = latchRIW;
			goto Start;
			}
		Call( err );

		//	verify no change
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

		Assert( cpgToChange > 0 );
		Assert( ipage >= 0 );
		Assert( ipage < cpgSLVExtent );
		cpages = min( cpgToChange, cpgSLVExtent - ipage );
		
		Call( ErrBTMungeSLVSpace(
					pfucbSLVAvail,
					slvspaceoper,
					ipage,
					cpages,
					fDIR,
					fileid,
					cbAlloc,
					wszFileName ) );

		//  if we are committing reserved space, mark the space as committed to
		//  the SLV Provider.  we do this if and only if the space operation
		//  succeeds because if it does, we are guaranteed to either commit or
		//  free the space via the version store.  if the space operation fails,
		//  it is guaranteed that the SLV Provider will free the space
		//
		//  NOTE:  we had better be able to mark this space as committed or a
		//  double free of this space can occur later!!!!

		if (	slvspaceoperReservedToCommitted == slvspaceoper &&
				PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
			{
			BOOKMARK bm = pfucbSLVAvail->bmCurr;
			BTUp( pfucbSLVAvail );
			
			QWORD ibLogical = OffsetOfPgno( pgnoFirst );
			QWORD cbSize = QWORD( cpages ) * SLVPAGE_SIZE;
			
			CallS( slvinfo.ErrCreateVolatile() );

			CallS( slvinfo.ErrSetFileID( fileid ) );
			CallS( slvinfo.ErrSetFileAlloc( cbAlloc ) );
			CallS( slvinfo.ErrSetFileName( wszFileName ) );

			CSLVInfo::HEADER header;
			header.cbSize			= cbSize;
			header.cRun				= 1;
			header.fDataRecoverable	= fFalse;
			header.rgbitReserved_31	= 0;
			header.rgbitReserved_32	= 0;
			CallS( slvinfo.ErrSetHeader( header ) );

			CSLVInfo::RUN run;
			run.ibVirtualNext	= cbSize;
			run.ibLogical		= ibLogical;
			run.qwReserved		= 0;
			run.ibVirtual		= 0;
			run.cbSize			= cbSize;
			run.ibLogicalNext	= ibLogical + cbSize;
			CallS( slvinfo.ErrMoveAfterLast() );
			CallS( slvinfo.ErrSetCurrentRun( run ) );

			err = ErrOSSLVRootSpaceCommit( rgfmp[ ifmp ].SlvrootSLV(), slvinfo );
			Enforce( err >= JET_errSuccess );

			slvinfo.Unload();

			Call( ErrBTGotoBookmark( pfucbSLVAvail, bm, latchRIW, fTrue ) );
			CallS( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
			}

		//	subsequent iterations will always start with first page in extent
		ipage = 0;
		cpgToChange -= cpages;
		pgnoFirst += cpages;

		if ( cpgToChange > 0 )
			{
			Pcsr( pfucbSLVAvail )->Downgrade( latchReadTouch );
			err = ErrBTNext( pfucbSLVAvail, fDIRNull );
			Assert( JET_errRecordNotFound != err );
			if ( JET_errNoCurrentRecord == err )
				{
				err = ErrSLVMapUnexpectedStateError( slvspaceoper );
				}
			Call( err );
			}
		else
			{
			break;
			}
		}
		
	// We need to mark the pages as unused in the OwnerMap
	// if the operation is moving pages out from the commited state
	// (transition into commited state are not marked in OwnerMap at this level)
	Assert ( 	slvspaceoperReservedToCommitted == slvspaceoper ||
				slvspaceoperCommittedToDeleted == slvspaceoper ||
				slvspaceoperFreeReserved == slvspaceoper ||
				slvspaceoperDeletedToFree == slvspaceoper );
				
	if ( slvspaceoperCommittedToDeleted == slvspaceoper )
		{
		Call ( ErrSLVOwnerMapResetUsageRange(
					ppib,
					ifmp,
					pgnoFirstCopy,
					cpgToChangeCopy,
					fSLVOWNERMAPResetChgPgStatus ) );
		}

	BTUp( pfucbSLVAvail );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fFalse;

HandleError:
	if ( fInTransaction )
		{
		BTUp( pfucbSLVAvail );
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	Assert( pfucbNil != pfucbSLVAvail );
	DIRClose( pfucbSLVAvail );
	slvinfo.Unload();

	return err;
	}


// ErrSLVGetRange				- reserves/commits a range of free space from the SLV file
//								  using the requested size as a hint.  more or
//								  less space may be returned
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		cbRequested				- requested range size
//		pibLogical				- buffer for receiving the range's starting offset
//		pcb						- buffer for receiving the range's size
//		fReserveOnly			- if TRUE, move space from Free to Rserved state,
//								  else move from Free to Committed state
//
// RESULT:						ERR
//
// OUT:	
//		pibLogical				- starting offset of the reserved range
//		pcb						- size of the reserved range

ERR ErrSLVGetRange(
	PIB			*ppib,
	const IFMP	ifmp,
	const QWORD	cbRequested,
	QWORD		*pibLogical,
	QWORD		*pcb,
	const BOOL	fReserveOnly,
	const BOOL	fForDefrag )
	{
	CPG			cpgReq		= CPG( ( cbRequested + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
	PGNO		pgnoFirst	= pgnoNull;

	const ERR	err			= ErrSLVGetPages( ppib, ifmp, &pgnoFirst, &cpgReq, fReserveOnly, fForDefrag );

	*pibLogical	= OffsetOfPgno( pgnoFirst );
	*pcb		= QWORD( cpgReq ) * SLVPAGE_SIZE;

	return err;
	}

// ErrSLVCommitReservedRange	- changes the state of the specified range of space
//								  in the SLV file from Reserved to Committed
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//		fileid					- associated SLV File ID
//		cbAlloc					- associated SLV File allocation size
//		wszFileName				- associated SLV File Name
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVCommitReservedRange(	PIB* const				ppib,
											const IFMP				ifmp,
											const QWORD				ibLogical,
											const QWORD				cbSize,
											const CSLVInfo::FILEID	fileid,
											const QWORD				cbAlloc,
											const wchar_t* const	wszFileName )
	{
	const CPG	cpgToCommit		= CPG( ( cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperReservedToCommitted,
															pgnoFirst,
															cpgToCommit,
															fileid,
															cbAlloc,
															wszFileName );

	return err;
	}

// ErrSLVDeleteCommittedRange	- changes the state of the specified range of space
//								  in the SLV file from Committed to Deleted
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//		fileid					- associated SLV File ID
//		cbAlloc					- associated SLV File allocation size
//		wszFileName				- associated SLV File Name
//
// RESULT:						ERR

ERR ErrSLVDeleteCommittedRange(	PIB* const				ppib,
								const IFMP				ifmp,
								const QWORD				ibLogical,
								const QWORD				cbSize,
								const CSLVInfo::FILEID	fileid,
								const QWORD				cbAlloc,
								const wchar_t* const	wszFileName )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

///	printf( "freeing %d pages at %d\n", cpgToRelease, pgnoFirst );
	
	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperCommittedToDeleted,
															pgnoFirst,
															cpgToRelease,
															fileid,
															cbAlloc,
															wszFileName );

	return err;
	}

// ErrSLVFreeReservedRange		- changes the state of the specified range of space
//								  in the SLV file from Reserved to Free
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVFreeReservedRange(	PIB* const	ppib,
											const IFMP	ifmp,
											const QWORD	ibLogical,
											const QWORD	cbSize )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperFreeReserved,
															pgnoFirst,
															cpgToRelease );

	return err;
	}

// ErrSLVFreeDeletedRange		- changes the state of the specified range of space
//								  in the SLV file from Deleted to Free
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVFreeDeletedRange(	PIB* const	ppib,
											const IFMP	ifmp,
											const QWORD	ibLogical,
											const QWORD	cbSize )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperDeletedToFree,
															pgnoFirst,
															cpgToRelease );

	return err;
	}


//  ================================================================
class SLVRESERVETASK : public DBTASK
//  ================================================================	
//
//  Reserves space for the SLV Provider when it is enabled
//
//-
	{
	public:
		SLVRESERVETASK( const IFMP ifmp, const QWORD cbRecommended );
		
		ERR ErrExecute( PIB * const ppib );

	private:
		SLVRESERVETASK( const SLVRESERVETASK& );
		SLVRESERVETASK& operator=( const SLVRESERVETASK& );

		QWORD m_cbRecommended;		//  recommended amount of space to reserve
	};


//  ****************************************************************
//	SLVRESERVETASK
//  ****************************************************************

SLVRESERVETASK::SLVRESERVETASK( const IFMP ifmp, const QWORD cbRecommended )
	:	DBTASK( ifmp ),
		m_cbRecommended( cbRecommended )
	{
	}
		
ERR SLVRESERVETASK::ErrExecute( PIB * const ppib )
	{
	ERR			err					= JET_errSuccess;
	BOOL		fInTrx				= fFalse;
	BOOL		fCalledSLVProvider	= fFalse;
	CSLVInfo	slvinfo;
	QWORD		ibVirtual			= 0;

	CSLVInfo::HEADER header;
	CSLVInfo::RUN run;

	//  if we are detaching from the database then do not reserve space

	if ( rgfmp[ m_ifmp ].FDetachingDB() )
		{
		Call( ErrERRCheck( JET_errInvalidDatabase ) );
		}
	
	//  reserve our space in a transaction so that we can rollback on an
	//  error

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	//  reserve up to m_cbRecommended bytes of space for use by
	//  the SLV Provider, allowing failure for JET_errDiskFull

	CallS( slvinfo.ErrCreateVolatile() );

	header.cbSize			= 0;
	header.cRun				= 0;
	header.fDataRecoverable	= fFalse;
	header.rgbitReserved_31	= 0;
	header.rgbitReserved_32	= 0;

	run.ibVirtualNext		= 0;
	run.ibLogical			= 0;
	run.qwReserved			= 0;
	run.ibVirtual			= 0;
	run.cbSize				= 0;
	run.ibLogicalNext		= 0;

	while ( ibVirtual < m_cbRecommended )
		{
		//  reserve this chunk of space in a transaction so that if we run out
		//  of space in the SLV Info, we can undo the space reserve

		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		
		//  try to reserve a chunk of space large enough to cover the remaining
		//  space that we need

		QWORD ibLogical;
		QWORD cbSize;
		CallJ(	ErrSLVGetRange(	ppib,
								m_ifmp,
								m_cbRecommended - ibVirtual,
								&ibLogical,
								&cbSize,
								fTrue,
								fFalse ),
				HandleError2 );

		//  this space can be appended to the current run

		if ( ibLogical == run.ibLogicalNext )
			{
			//  increase the ibVirtualNext of this run to reflect the newly
			//  allocated space

			run.ibVirtualNext	+= cbSize;
			run.cbSize			+= cbSize;
			run.ibLogicalNext	+= cbSize;

			//  increase the size of the SLV File

			header.cbSize += cbSize;
			}

		//  this space cannot be appended to the current run

		else
			{
			//  move to after the last run to append a new run

			CallS( slvinfo.ErrMoveAfterLast() );

			//  set the run to include this data

			run.ibVirtualNext	= ibVirtual + cbSize;
			run.ibLogical		= ibLogical;
			run.qwReserved		= 0;
			run.ibVirtual		= ibVirtual;
			run.cbSize			= cbSize;
			run.ibLogicalNext	= ibLogical + cbSize;

			//  add a run to the header

			header.cbSize += cbSize;
			header.cRun++;
			}

		//  save our changes to the run

		CallJ( slvinfo.ErrSetCurrentRun( run ), HandleError2 );

		//  save our changes to the header
		
		CallS( slvinfo.ErrSetHeader( header ) );

		//  commit our transaction on success

		CallJ( ErrDIRCommitTransaction( ppib, NO_GRBIT ), HandleError2 );

		//  advance our reserve pointer

		ibVirtual += cbSize;

		//  handle errors for this space allocation

HandleError2:
		if ( err < JET_errSuccess )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

			if ( err == JET_errDiskFull )
				{
				err = JET_errSuccess;
				break;
				}

			Call( err );
			}
		}

	//  commit our transaction on success, but use lazy flush as we will
	//  recover this space either way on a crash

	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	fInTrx = fFalse;

	//  try to grant the reserved space to the SLV Provider

	err = ErrOSSLVRootSpaceReserve( rgfmp[ m_ifmp ].SlvrootSLV(), slvinfo );
	fCalledSLVProvider = fTrue;

	//  there was an error granting the space

	if ( err < JET_errSuccess )
		{
		//  try to free the space seen in all runs of this SLV Info, but don't
		//  worry if some of the space is not freed

		CallS( slvinfo.ErrMoveBeforeFirst() );
		while ( slvinfo.ErrMoveNext() != JET_errNoCurrentRecord )
			{
			CSLVInfo::RUN run;
			CallS( slvinfo.ErrGetCurrentRun( &run ) );

			(void)ErrSLVSpaceFree( m_ifmp, run.ibLogical, run.cbSize, fFalse );
			}
		
		//  fail with the original error

		Call( err );
		}

	//  rollback on an error

HandleError:
	if ( err < JET_errSuccess )
		{
		if ( fInTrx )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		if ( !fCalledSLVProvider )
			{
			slvinfo.Unload();
			CallS( slvinfo.ErrCreateVolatile() );
			CallS( ErrOSSLVRootSpaceReserve( rgfmp[ m_ifmp ].SlvrootSLV(), slvinfo ) );
			}
		}
	slvinfo.Unload();
	return err;
	}


// SLVSpaceRequest				- satisfies a synchronous request by the SLV Provider
//								  for more reserved space for an SLV Root.  we satisfy
//								  this request by posting a space request task to the
//								  task pool
//
// IN:
//		ifmpDb					- database ifmp that needs more space
//		cbRecommended			- recommended amount of space to reserve

void SLVSpaceRequest( const IFMP ifmpDb, const QWORD cbRecommended )
	{
	//  attempt to issue a task to reserve SLV space

	SLVRESERVETASK * const ptask = new SLVRESERVETASK( ifmpDb, cbRecommended );
	if ( ptask )
		{
		if ( PinstFromIfmp( ifmpDb )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) >= JET_errSuccess )
			{
			return;
			}
		delete ptask;
		}

	//  we failed to issue the task, so we will tell the SLV Provider that we
	//  have no free space

	CSLVInfo slvinfo;
	CallS( slvinfo.ErrCreateVolatile() );
	CallS( ErrOSSLVRootSpaceReserve( rgfmp[ ifmpDb ].SlvrootSLV(), slvinfo ) );
	slvinfo.Unload();
	}


//  ================================================================
class SLVFREETASK : public DBTASK
//  ================================================================	
//
//  Frees space for the SLV Provider when it is enabled
//
//-
	{
	public:
		SLVFREETASK( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted );
		
		ERR ErrExecute( PIB * const ppib );

	private:
		SLVFREETASK( const SLVFREETASK& );
		SLVFREETASK& operator=( const SLVFREETASK& );

		const QWORD	m_ibLogical;
		const QWORD	m_cbSize;
		const BOOL	m_fDeleted;
	};


//  ****************************************************************
//	SLVFREETASK
//  ****************************************************************

SLVFREETASK::SLVFREETASK( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted )
	:	DBTASK( ifmpDb ),
		m_ibLogical( ibLogical ),
		m_cbSize( cbSize ),
		m_fDeleted( fDeleted )
	{
	}

ERR SLVFREETASK::ErrExecute( PIB * const ppib )
	{
	ERR		err			= JET_errSuccess;
	BOOL	fInTrx		= fFalse;

	//  free our space in a transaction so that we can rollback on an
	//  error

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	//  free the specified range

	if ( m_fDeleted )
		{
		Call( ErrSLVFreeDeletedRange( ppib, m_ifmp, m_ibLogical, m_cbSize ) );
		}
	else
		{
		Call( ErrSLVFreeReservedRange( ppib, m_ifmp, m_ibLogical, m_cbSize ) );
		}

	//  commit our transaction on success, but use lazy flush as we will
	//  recover this space either way on a crash

	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );

	//  rollback on an error

HandleError:
	if ( err < JET_errSuccess )
		{
		AssertTracking();
		if ( fInTrx )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		}
	return err;
	}


// ErrSLVSpaceFree				- satisfies a synchronous request by the SLV Provider
//								  to free space for an SLV Root.  we satisfy this
//								  request by posting a space free task to the task
//								  pool
//
// IN:
//		ifmpDb					- database ifmp that contains the space to free
//		ibLogical				- starting offset of range to free
//		cbSize					- size of range to free
//		fDeleted				- indicates that the space to be freed is Deleted
//
// RESULT:						ERR

ERR ErrSLVSpaceFree( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted )
	{
	ERR				err			= JET_errSuccess;
	SLVFREETASK*	ptask		= NULL;

	//  issue a task to free SLV space

	if ( !( ptask = new SLVFREETASK( ifmpDb, ibLogical, cbSize, fDeleted ) ) )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	Call( PinstFromIfmp( ifmpDb )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) );

	return JET_errSuccess;

HandleError:
	if ( ptask )
		{
		delete ptask;
		}
	return err;
	}


// ErrSLVDelete					- changes the state of the space owned by the given SLV
//								  from Committed to Deleted.  the space will later be
//								  moved from Deleted to Free when it is known that
//								  no one can possibly see the SLV any longer or when the
//								  system restarts
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record in copy buffer
//
// RESULT:						ERR

ERR ErrSLVDelete(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer )
	{
	ERR 			err;
	CSLVInfo		slvinfo;
	
	//  validate IN args

	ASSERT_VALID( pfucb );

	//  walk all runs in the SLVInfo and decommit their space
	//
	//  NOTE:  we do this backwards so that the space is cleaned up in reverse
	//  order.  this is so that if the SLV Provider is enabled, cleanup will
	//  work properly.  we must cleanup the first run last because it determines
	//  the File ID.  if this space is freed before the rest of the space for
	//  a given file, it is possible that another file could use this space and
	//  thereby cause a File ID collision.  this, of course, would be bad

	Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fCopyBuffer ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfo.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfo.ErrGetFileAlloc( &cbAlloc ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	Call( slvinfo.ErrGetFileName( wszFileName ) );

	CallS( slvinfo.ErrMoveAfterLast() );
	while ( ( err = slvinfo.ErrMovePrev() ) >= JET_errSuccess )
		{
		CSLVInfo::RUN run;
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		
		Call( ErrSLVDeleteCommittedRange(	pfucb->ppib,
											pfucb->ifmp,
											run.ibLogical,
											run.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}
	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVLogDataFromFile		- logs the fact that the an SLV File with the
//								  given SLV Info has been inserted into the SLV
//								  streaming file, logging the actual data if
//								  requested
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		slvinfo					- SLV Info
//		pfapiSrc				- SLV File (optional)
//
// RESULT:						ERR

LOCAL ERR ErrSLVLogDataFromFile(	PIB*			ppib,
									IFMP			ifmpDb,
									CSLVInfo&		slvinfo,
									IFileAPI* const	pfapiSrc	= NULL )
	{
	ERR			err				= JET_errSuccess;
	IFileAPI*	pfapi			= pfapiSrc;
	void*		pvView			= NULL;
	void*		pvTempBuffer	= NULL;
	DATA		dataTemp;

	SLVOWNERMAPNODE	slvownermapNode;

#ifdef SLV_USE_VIEWS
	QWORD	cbLog			= SLVPAGE_SIZE;
#else  //  !SLV_USE_VIEWS
	QWORD	cbLog			= g_cbPage;
#endif  //  SLV_USE_VIEWS
	QWORD	cbView			= OSMemoryPageReserveGranularity();

	CSLVInfo::RUN run;
	
	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );

	//  get the header, file id, file allocation, and file name for the SLV

	CSLVInfo::HEADER header;
	CallS( slvinfo.ErrGetHeader( &header ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfo.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfo.ErrGetFileAlloc( &cbAlloc ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	Call( slvinfo.ErrGetFileName( wszFileName ) );

	//  we will be logging the actual data for this SLV

// with checksum enabled, we always read the data as we need to checksum it
	if ( header.cbSize )
		{
#ifdef SLV_USE_VIEWS

		//  init the temp buffer such that we will log only one SLVPAGE_SIZE
		//  chunk of data at a time and so that we have no data mapped into a
		//  view initially

		dataTemp.SetPv( (BYTE*)pvView + cbView );
		dataTemp.SetCb( ULONG( cbLog ) );

		//  create a view of the source file, opening it if necessary

		if ( !pfapi )
			{
			Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
									slvinfo,
									&pfapi,
									fTrue,
									fTrue ) );
			}
		
#else  //  !SLV_USE_VIEWS

		//  allocate a temporary buffer for use in reading the data
		
		BFAlloc( &pvTempBuffer );
		dataTemp.SetPv( pvTempBuffer );
		dataTemp.SetCb( cbLog );

#ifdef SLV_USE_RAW_FILE

		//  use the backing file handle for our reads

		pfapi = rgfmp[ ifmpDb ].PfapiSLV();

#else  //  !SLV_USE_RAW_FILE

		//  open the SLV as a file

		if ( !pfapi )
			{
			Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
									slvinfo,
									&pfapi,
									fTrue,
									fTrue ) );
			}

#endif  //  SLV_USE_RAW_FILE
#endif  //  SLV_USE_VIEWS
		}
		
	//  log all data from the SLV File

	QWORD ibVirtual;
	ibVirtual = 0;
	
	memset( &run, 0, sizeof( run ) );

	CallS( slvinfo.ErrMoveBeforeFirst() );

	while ( ibVirtual < header.cbSize )
		{
		//  we need to retrieve another run from the SLV

		Assert( ibVirtual <= run.ibVirtualNext );
		if ( ibVirtual == run.ibVirtualNext )
			{
			//  retrieve the next run from the SLV

			Call( slvinfo.ErrMoveNext() );
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			//  mark the space used for this SLV

			Call( ErrSLVCommitReservedRange(	ppib,
												ifmpDb,
												run.ibLogical,
												run.cbSize,
												fileid,
												cbAlloc,
												wszFileName ) );
			Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( ibVirtual - run.ibVirtual + run.ibLogical ) ) );
			}

		//  make sure that we don't read past the end of the valid data region

		dataTemp.SetCb( ULONG( min( cbLog, header.cbSize - ibVirtual ) ) );

		//  read data from the SLV if we are going to log it

// with checksum enabled, we always read the data as we need to checksum it
#ifdef SLV_USE_VIEWS

		//  advance to the next chunk of data that we need from this view

		dataTemp.SetPv( (BYTE*)dataTemp.Pv() + cbLog );

		//  the next chunk of data that we need is not in the this view

		if ( (BYTE*)dataTemp.Pv() >= (BYTE*)pvView + cbView )
			{
			//  unmap this view

			Call( pfapi->ErrMMFree( pvView ) );

			//  map a new view containing this offset

			Call( pfapi->ErrMMRead(	ibVirtual,
									min( cbView, header.cbSize - ibVirtual ),
									&pvView ) );

			//  set our buffer to point to this new view

			dataTemp.SetPv( pvView );
			}

#else  //  !SLV_USE_VIEWS
#ifdef SLV_USE_RAW_FILE

		//  read the next chunk of data to log from the backing file
		
		Call( pfapi->ErrIORead(	ibVirtual - run.ibVirtual + run.ibLogical,
								dataTemp.Cb(),
								(BYTE*)dataTemp.Pv() ) );

#else  //  !SLV_USE_RAW_FILE

		//  read the next chunk of data to log from the SLV File

		Call( pfapi->ErrIORead(	ibVirtual,
								dataTemp.Cb(),
								(BYTE*)dataTemp.Pv() ) );

#endif  //  SLV_USE_RAW_FILE
#endif  //  SLV_USE_VIEWS

		//  log an append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		Call( ErrLGSLVPageAppend(	ppib,
									ifmpDb | ifmpSLV,
									ibVirtual - run.ibVirtual + run.ibLogical,
									dataTemp.Cb(),
									dataTemp.Pv(),
									header.fDataRecoverable,
									fFalse,
									&slvownermapNode,
									&lgposAppend ) );
		slvownermapNode.NextPage();
		//  advance our read pointer

		ibVirtual += dataTemp.Cb();
		}

	//  if there is any space in this file beyond the valid data, commit it
	//
	//  NOTE:  if this is a zero length file, we must have at least one
	//  block of space to uniquely identify this file

	while ( slvinfo.ErrMoveNext() != JET_errNoCurrentRecord )
		{
		//  retrieve the next run from the SLV

		Call( slvinfo.ErrGetCurrentRun( &run ) );

		//  mark the space used for this SLV

		Call( ErrSLVCommitReservedRange(	ppib,
											ifmpDb,
											run.ibLogical,
											run.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}

HandleError:

#ifdef SLV_USE_VIEWS

	if ( err == errLGRecordDataInaccessible )
		{
		err = ErrERRCheck( JET_errSLVFileIO );
		}

	(void)pfapi->ErrMMFree( pvView );

	if ( pfapi != pfapiSrc )
		{
		delete pfapi;
		}

#else  //  !SLV_USE_VIEWS
#ifdef SLV_USE_RAW_FILE
#else  //  !SLV_USE_RAW_FILE

	if ( pfapi != pfapiSrc )
		{
		delete pfapi;
		}

#endif  //  SLV_USE_RAW_FILE

	if ( pvTempBuffer )
		{
		BFFree( pvTempBuffer );
		}

#endif  //  SLV_USE_VIEWS

	return err;
	}

class SLVCHUNK
	{
	public:
		SLVCHUNK()
			:	m_msigReadComplete( CSyncBasicInfo( _T( "SLVCHUNK::m_msigReadComplete" ) ) ),
				m_msigWriteComplete( CSyncBasicInfo( _T( "SLVCHUNK::m_msigWriteComplete" ) ) )
			{
			m_pfapiDest					= NULL;
			m_msigReadComplete.Set();
			m_msigWriteComplete.Set();
			m_err						= JET_errSuccess;
			}

		~SLVCHUNK()
			{
			Assert( m_msigReadComplete.FTryWait() );
			Assert( m_msigWriteComplete.FTryWait() );
			}
	
	public:
		IFileAPI*			m_pfapiDest;
		CManualResetSignal	m_msigReadComplete;
		CManualResetSignal	m_msigWriteComplete;
		ERR					m_err;
	};

void SLVICopyFileIWriteComplete(	const ERR			err,
									IFileAPI* const		pfapiDest,
									const QWORD			ibVirtual,
									const DWORD			cbData,
									const BYTE* const	pbData,
									SLVCHUNK* const		pslvchunk )
	{
	//  save the error code for the write

	pslvchunk->m_err = err;

	//  signal that the write has completed

	pslvchunk->m_msigWriteComplete.Set();
	}

void SLVICopyFileIReadComplete(	const ERR			err,
								IFileAPI* const		pfapiSrc,
								const QWORD			ibVirtual,
								const DWORD			cbData,
								const BYTE* const	pbData,
								SLVCHUNK* const		pslvchunk )
	{
	//  save the error code for the read

	pslvchunk->m_err = err;

	//  signal that the read has completed

	pslvchunk->m_msigReadComplete.Set();

	//  the read was successful

	if ( err >= JET_errSuccess )
		{		
		//  write the data to the destination file immediately
		//
		//  NOTE:  round up the write size to the nearest page boundary so that
		//  we can do uncached writes to files of odd sizes.  we know that the
		//  source buffer will be large enough for this

		CallS( pslvchunk->m_pfapiDest->ErrIOWrite(	ibVirtual,
													( ( cbData + g_cbPage - 1 ) / g_cbPage ) * g_cbPage, 
													pbData,
													IFileAPI::PfnIOComplete( SLVICopyFileIWriteComplete ),
													DWORD_PTR( pslvchunk ) ) );
		CallS( pslvchunk->m_pfapiDest->ErrIOIssue() );
		}

	//  the read was not successful

	else
		{
		//  complete the write callback with the same error

		SLVICopyFileIWriteComplete(	err,
									pslvchunk->m_pfapiDest,
									ibVirtual,
									( ( cbData + g_cbPage - 1 ) / g_cbPage ) * g_cbPage,
									pbData,
									pslvchunk );
		}
	}

// ErrSLVCopyFile				- copies the data from the given SLV File into the
//								  specifed record and field.  the SLV is copied via
//								  the SLV Provider
//
// IN:
//		slvinfo					- source SLV Info (empty indicates unknown)
//		pfapi					- source SLV File
//		cbSize					- source SLV File valid data length
//		fDataRecoverable		- data is logged for recovery
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyFile(
	CSLVInfo&		slvinfo,
	IFileAPI* const	pfapi,
	const QWORD		cbSize,
	const BOOL		fDataRecoverable,
	FUCB*			pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence)
	{
	ERR				err				= JET_errSuccess;
	IFileAPI*		pfapiDestSize	= NULL;
	IFileAPI*		pfapiDest		= NULL;
	CSLVInfo		slvinfoDest;
	size_t			cChunk			= 0;
	size_t			cbChunk			= 0;
	SLVCHUNK*		rgslvchunk		= NULL;
	BYTE*			rgbChunk		= NULL;
	
	CSLVInfo::RUN runDest;
	CSLVInfo::HEADER headerDest;
	
	SLVOWNERMAPNODE	slvownermapNode;
	
	//  validate IN args

	Assert( pfapi );
	ASSERT_VALID( pfucb );

	//  create the destination SLV File with a temporary name based on the
	//  number of copies done so far.  we will never persist this name so there
	//  is no worry of a name collision in the future
	//
	//  set the size of the destination SLV File first to force the SLV Provider
	//  to allocate its space optimally, to ensure we have enough space before
	//  we begin the copy, and to enable us to determine the physical location
	//  of the SLV File before hand so that we can copy it and log it in one pass

	QWORD cbSizeAlign = ( ( cbSize + g_cbPage - 1 ) / g_cbPage ) * g_cbPage;

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	swprintf( wszFileName, L"$CopyDest%016I64X$", rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() );
	Call( ErrOSSLVFileCreate(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
								wszFileName,
								cbSizeAlign,
								&pfapiDestSize ) );

	//  get the SLVInfo for the destination SLV we just created, committing its
	//  space in the SLV Provider

	Call( slvinfoDest.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	CallS( slvinfoDest.ErrSetFileNameVolatile() );
	Call( ErrOSSLVFileGetSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
									pfapiDestSize,
									&slvinfoDest ) );

	//  get the header, file id, and file allocation for the destination SLV

	CallS( slvinfoDest.ErrGetHeader( &headerDest ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfoDest.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfoDest.ErrGetFileAlloc( &cbAlloc ) );

	//  the destination SLV had better be setup properly

	if (	headerDest.cbSize < cbSizeAlign ||
			cbAlloc < cbSizeAlign )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	//  open the destination file with copy-on-write disabled so that we can copy
	//  and log the file data at the same time using the SLV Info acquired above,
	//  the acquisition of which turns on copy-on-write for the file

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							slvinfoDest,
							&pfapiDest,
							fFalse,
							fFalse ) );

	//  close the old destination file handle.  this must be done after opening
	//  the second handle so that we always have a reference on the file so its
	//  uncommitted space doesn't get freed by the SLV File Table

	delete pfapiDestSize;
	pfapiDestSize = NULL;

	//  set the true size and data recoverability for the destination SLV

	headerDest.cbSize			= cbSize;
	headerDest.fDataRecoverable	= fDataRecoverable;
	CallS( slvinfoDest.ErrSetHeader( headerDest ) );

	//  allocate resources for the copy
	//  CONSIDER:  expose these settings

	cbChunk	= size_t( min( 64 * 1024, cbSizeAlign ) );

	if ( cbChunk == 0 )
		{
		Assert( 0 == cbSize );
		Assert( 0 == cChunk);	
		Assert( NULL == rgslvchunk);	
		Assert( NULL == rgbChunk);	
		}
	else
		{
		Assert( cbChunk );	
		Assert( cbSize );
		
		cChunk = size_t( min( 16, cbSizeAlign / cbChunk ) );

		if ( !( rgslvchunk = new SLVCHUNK[ cChunk ] ) )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		//	this memory must be page aligned for I/O
		
		if ( !( rgbChunk = (BYTE*)PvOSMemoryPageAlloc( cChunk * cbChunk, NULL ) ) )	
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		}

	//  pre-issue reads to the source SLV File for the first n chunks of the data
	//  to be copied to prime the pump for the copy operation

	QWORD ibVirtualMac;
	ibVirtualMac = min( headerDest.cbSize, cChunk * cbChunk );
	
	QWORD ibVirtual;
	ibVirtual = 0;

	// the function that writes the destination data will zero out the unused part of the last page
	// this will allow to compute the checksum only over the valid data
	// Assert( buffer that will be used to zero out will be large enough );

	while ( ibVirtual < ibVirtualMac )
		{
		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	
		
		const size_t iChunk = size_t( ( ibVirtual / cbChunk ) % cChunk );

		rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
		rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
		rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
		rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

		CallS( pfapi->ErrIORead(	ibVirtual,
									DWORD( min( cbChunk, ibVirtualMac - ibVirtual ) ),
									rgbChunk + iChunk * cbChunk,
									IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
									DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );

		ibVirtual += cbChunk;
		}
	CallS( pfapi->ErrIOIssue() );

	//  copy all data from the source SLV File to the destination SLV File,
	//  logging that data and committing that space as we go

	memset( &runDest, 0, sizeof( runDest ) );
	CallS( slvinfoDest.ErrMoveBeforeFirst() );

	ibVirtual = 0;

	while ( ibVirtual < headerDest.cbSize )
		{

		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	

		//  we need to retrieve another run from the destination SLV

		Assert( ibVirtual <= runDest.ibVirtualNext );
		if ( ibVirtual == runDest.ibVirtualNext )
			{
			//  retrieve the next run from the destination SLV

			Call( slvinfoDest.ErrMoveNext() );
			Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );

			//  mark the space used by this run in the destination SLV

			Call( ErrSLVCommitReservedRange(	pfucb->ppib,
												pfucb->ifmp,
												runDest.ibLogical,
												runDest.cbSize,
												fileid,
												cbAlloc,
												wszFileName ) );
			Call( slvownermapNode.ErrCreateForSearch( pfucb->ifmp, PgnoOfOffset( ibVirtual - runDest.ibVirtual + runDest.ibLogical ) ) );
			}

		//  wait for the read for the current chunk to complete

		const size_t iChunk = size_t( ( ibVirtual / cbChunk ) % cChunk );
		const size_t cbOffsetInChunk = size_t( ibVirtual % cbChunk );

		rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
		Call( rgslvchunk[ iChunk ].m_err );

		//  log an append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		Call( ErrLGSLVPageAppend(	pfucb->ppib,
									pfucb->ifmp | ifmpSLV,
									ibVirtual - runDest.ibVirtual + runDest.ibLogical,
									ULONG( min( SLVPAGE_SIZE, headerDest.cbSize - ibVirtual ) ),
									rgbChunk + ( iChunk * cbChunk ) + cbOffsetInChunk,
									headerDest.fDataRecoverable,
									fFalse,
									&slvownermapNode, 
									&lgposAppend ) );
		slvownermapNode.NextPage();

		//  advance our copy pointer

		ibVirtual += SLVPAGE_SIZE;

		//  we are done logging this chunk of the destination SLV File

		if ( ibVirtual % cbChunk == 0 )
			{
			//  wait for the write for the current chunk to complete

			rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
			Call( rgslvchunk[ iChunk ].m_err );

			//  there is more data to read from the source SLV File

			QWORD ibVirtualChunkNext;
			ibVirtualChunkNext = ibVirtual + ( cChunk - 1 ) * cbChunk;
			
			if ( ibVirtualChunkNext < headerDest.cbSize )
				{
				//  issue a read to the source SLV File for the next chunk
				
				rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
				rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
				rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
				rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

				CallS( pfapi->ErrIORead(	ibVirtualChunkNext,
											DWORD( min( cbChunk, headerDest.cbSize - ibVirtualChunkNext ) ),
											rgbChunk + iChunk * cbChunk,
											IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
											DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );
				CallS( pfapi->ErrIOIssue() );
				}
			}
		}

	//  if there is any space in this file beyond the valid data, commit it
	//
	//  NOTE:  if this is a zero length file, we must have at least one
	//  block of space to uniquely identify this file

	while ( slvinfoDest.ErrMoveNext() != JET_errNoCurrentRecord )
		{
		//  retrieve the next run from the SLV

		Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );

		//  mark the space used for this SLV

		Call( ErrSLVCommitReservedRange(	pfucb->ppib,
											pfucb->ifmp,
											runDest.ibLogical,
											runDest.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}

	if ( FFUCBReplacePrepared(pfucb) )
		{
		ERR errT = slvinfoDest.ErrMoveBeforeFirst();
				
		Assert ( JET_errSuccess == errT );
		
		errT = slvinfoDest.ErrMoveNext();
		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
		
			Call( slvinfoDest.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVCopyFile ) );

			errT = slvinfoDest.ErrMoveNext();
			}
		Assert ( JET_errNoCurrentRecord == errT );

		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucb ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucb );
		}		

	//  save our changes to the destination SLV
	Call( slvinfoDest.ErrSave() );

	//  register the copy with the SLV Provider
	
	Call( ErrOSSLVRootCopyFile(		rgfmp[ pfucb->ifmp ].SlvrootSLV(),
															pfapi,
															slvinfo,
															pfapiDest,
															slvinfoDest,
															QWORD( rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() ) ) );


HandleError:
	if ( rgslvchunk )
		{
		for ( DWORD iChunk = 0; iChunk < cChunk; iChunk++ )
			{
			rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
			rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
			}
		}
	OSMemoryPageFree( rgbChunk );
	delete [] rgslvchunk;
	delete pfapiDest;
	slvinfoDest.Unload();
	delete pfapiDestSize;
	return err;
	}

// ErrSLVSetColumnFromEA		- set a SLV column from an SLV Provider EA List
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromEA(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	DATA			* pdata,
	const ULONG		grbit )
	{
	ERR				err		= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetSLVFromSLVEA ) ) );
	
	//  create a new iterator to contain the SLV Info for this SLV

	CSLVInfo slvinfo;
	Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	
	//  if this SLV's data is recoverable, mark the SLV's header as such

	BOOL fDataRecoverable;
	fDataRecoverable = fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	if ( fDataRecoverable )
		{
		CSLVInfo::HEADER header;
		CallS( slvinfo.ErrGetHeader( &header ) );
		header.fDataRecoverable = fTrue;
		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  retrieve the SLV Info for this SLV EA List

	Call( ErrOSSLVFileConvertEAToSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
											pdata->Pv(),
											pdata->Cb(),
											&slvinfo ) );

	// mark the space usage in SLVOWNERMAP on update operations
	if ( FFUCBReplacePrepared( pfucb ) )
		{
		ERR errT = slvinfo.ErrMoveBeforeFirst();
		
		if ( JET_errSuccess <= errT )
			errT = slvinfo.ErrMoveNext();

		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
			
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVFromEA ) );

			errT = slvinfo.ErrMoveNext();
			}
		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucb ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucb );
		}
	
	//  log this SLV for recovery

	Call( ErrSLVLogDataFromFile(	pfucb->ppib,
									pfucb->ifmp,
									slvinfo ) );

	//  save our changes to the SLV Info

	Call( slvinfo.ErrSave() );

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVSetColumnFromFile		- set a SLV column from an SLV Provider File Handle
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromFile(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	DATA			* pdata,
	const ULONG		grbit )
	{
	ERR				err			= JET_errSuccess;
	IFileAPI*		pfapiSrc	= NULL;
	CSLVInfo		slvinfo;

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetSLVFromSLVFile ) ) );

	//  open the SLV File

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							pdata->Pv(),
							pdata->Cb(),
							&pfapiSrc,
							fTrue ) );

	//  create a new iterator to contain the SLV Info for this SLV

	Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	
	//  if this SLV's data is recoverable, mark the SLV's header as such

	BOOL fDataRecoverable;
	fDataRecoverable = fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	if ( fDataRecoverable )
		{
		CSLVInfo::HEADER header;
		CallS( slvinfo.ErrGetHeader( &header ) );
		header.fDataRecoverable = fTrue;
		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  retrieve the SLV Info for this SLV File

	err = ErrOSSLVFileGetSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
									pfapiSrc,
									&slvinfo );
	if ( err != JET_errSLVEAListCorrupt )
		{
		Call( err );
		}

	//  the SLV EA List was corrupt

	if ( err == JET_errSLVEAListCorrupt )
		{
		//  if we are here, we assume that this SLV File is not from the local
		//  store or there is some other reason why this SLV File cannot be
		//  committed.  to resolve this, we will make a new copy of the file

		//  get the SLV File's size

		QWORD cbSizeSrc;
		Call( pfapiSrc->ErrSize( &cbSizeSrc ) );

		//  copy the SLV File into the local store

		CSLVInfo slvinfoSrc;
		CallS( slvinfo.ErrCreateVolatile() );
		Call( ErrSLVCopyFile(	slvinfoSrc,
								pfapiSrc,
								cbSizeSrc,
								fDataRecoverable,
								pfucb,
								columnid,
								itagSequence) );
		}

	//  the SLV EA List was not corrupt

	else
		{
		// mark the space usage in SLVOWNERMAP on update operations
		if ( FFUCBReplacePrepared( pfucb ) )
			{
			ERR errT = slvinfo.ErrMoveBeforeFirst();
			
			if ( JET_errSuccess <= errT )
				errT = slvinfo.ErrMoveNext();

			while ( JET_errSuccess <= errT )
				{
				CSLVInfo::RUN run;
				
				Call( slvinfo.ErrGetCurrentRun( &run ) );

				const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
				const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

				Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
				Call( ErrSLVOwnerMapSetUsageRange(
							pfucb->ppib,
							pfucb->ifmp,
							pgnoFirst,
							cpg,
							pfucb->u.pfcb->ObjidFDP(),
							columnid,
							&pfucb->bmCurr,
							fSLVOWNERMAPSetSLVFromFile));

				errT = slvinfo.ErrMoveNext();
				}
			}
		else
			{
			Assert ( FFUCBInsertPrepared( pfucb ) );
			FUCBSetSLVOwnerMapNeedUpdate( pfucb );
			}
		
		//  log this SLV for recovery

		Call( ErrSLVLogDataFromFile(	pfucb->ppib,
										pfucb->ifmp,
										slvinfo,
										pfapiSrc ) );

		//  save our changes to the SLV Info

		Call( slvinfo.ErrSave() );
		}

HandleError:
	delete pfapiSrc;
	slvinfo.Unload();
	return err;
	}


//  ================================================================
ULONG32 UlChecksumSLV( const BYTE * const pbMin, const BYTE * const pbMax )
//  ================================================================
	{	
	const ULONG32 		ulSeed 					= 0x89abcdef;
	const ULONG32 		cBitsInByte 			= 8;
	const ULONG32		ulResult				= LOG::UlChecksumBytes( pbMin, pbMax, ulSeed );

	return ulResult;
	}



// ErrSLVWriteRun				- writes SLV Data to an SLV Run
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		run						- SLV Run
//		ibOffset				- offset to start writing
//		pdata					- source buffer
//		pcbRead					- ULONG where to return the written data size
//		fDataRecoverable		- data is logged for recovery
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbWritten				- SLV Data bytes written
LOCAL ERR ErrSLVWriteRun(	FUCB * 			pfucb,
							PIB*			ppib,
							IFMP			ifmpDb,
							CSLVInfo::RUN&	run,
							QWORD			ibVirtualStart,
							DATA*			pdata,
							QWORD*			pcbWritten,
							BOOL			fDataRecoverable )
	{

	ERR				err		= JET_errSuccess;
	INST			*pinst	= PinstFromIfmp( ifmpDb );
	QWORD			ibData;
	QWORD			ibLogical;
	SLVOWNERMAPNODE	slvownermapNode;

	
	//  validate IN args
	Assert ( pinst );
	Assert ( pfucbNil == pfucb || (pfucb->ifmp == ifmpDb && pfucb->ppib == ppib) );

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( pcbWritten );

	//  write data from pages in run until we have run out of data or space

	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;

	const BOOL  fUpdateOwnerMap		= (BOOL) ( NULL != rgfmp[ifmpDb].PfcbSLVOwnerMap() );

#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	BOOL		fInvalidateChecksum	= fFalse;
#endif

	Assert( ( fUpdateOwnerMap && !pinst->FRecovering() ) ||
			( !fUpdateOwnerMap && pinst->FRecovering() ) ); // = ( fUpdateOwnerMap ^ pinst->m_plog->m_fRecovering )

	Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( ibLogical ) ) );

#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	if ( fUpdateOwnerMap )
		{
		ULONG	dwT1, dwT2;
		err = slvownermapNode.ErrGetChecksum( ppib, &dwT1, &dwT2 );
		if ( err < 0 )
			{
			if ( errSLVInvalidOwnerMapChecksum != err )
				{
				goto HandleError;
				}
			err = JET_errSuccess;
			}
		else
			{
			fInvalidateChecksum = fTrue;
			}
		}
#endif // SLV_USE_CHECKSUM_FRONTDOOR
	
	while ( ibData < pdata->Cb() && ibLogical < run.ibLogicalNext )
		{
		//  determine the ifmp / pgno / ib / cb where we will write data

		IFMP	ifmp	= ifmpDb | ifmpSLV;
		PGNO	pgno	= PgnoOfOffset( ibLogical );
		QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		QWORD	cb		= min( pdata->Cb() - ibData, SLVPAGE_SIZE - ib );

		Assert( fUpdateOwnerMap == (BOOL) ( NULL != rgfmp[ifmpDb].PfcbSLVOwnerMap() ) );

		Assert ( ( fUpdateOwnerMap && !pinst->FRecovering() )
			  || ( !fUpdateOwnerMap && pinst->FRecovering() ) ); // = ( fUpdateOwnerMap ^ pinst->m_plog->m_fRecovering )

		Assert( !pinst->FSLVProviderEnabled() );
		
		DWORD ulChecksum;
		
		if ( fUpdateOwnerMap )
			{
			Assert ( pfucbNil != pfucb);
			RCE::SLVPAGEAPPEND data;

			data.ibLogical	= OffsetOfPgno( pgno ) + ib;
			data.cbData		= DWORD( cb );

			Call ( pinst->m_pver->ErrVERFlag( pfucb, operSLVPageAppend, &data, sizeof(RCE::SLVPAGEAPPEND) ) );
			}

		//  log the append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		// We never update space map here because SLVProvider is disabled.
		Call( ErrLGSLVPageAppend(	ppib,
									ifmp,
									ibLogical,
									ULONG( cb ),
									(BYTE*)pdata->Pv() + ibData,
									fDataRecoverable,
									fFalse,
									NULL,
									&lgposAppend ) );

		//  write latch the current ifmp / pgno.  we will write latch a new page
		//  when the starting offset is on a page boundary because we are only
		//  appending data to the SLV and no valid data could already exist on
		//  that page anyway

		BFLatch bfl;
		Call( ErrBFWriteLatchPage( &bfl, ifmp, pgno, ib ? bflfDefault : bflfNew ) );

		//  dirty and setup log dependencies on the page

		BFDirty( &bfl );
		Assert( !rgfmp[ ifmpDb ].FLogOn() || !PinstFromIfmp( ifmpDb )->m_plog->m_fLogDisabled );
		if ( rgfmp[ ifmpDb ].FLogOn() )
			{
			BFSetLgposBegin0( &bfl, ppib->lgposStart );
			BFSetLgposModify( &bfl, lgposAppend );
			}

		//  copy the desired data from the buffer onto the page

		UtilMemCpy( (BYTE*)bfl.pv + ib, (BYTE*)pdata->Pv() + ibData, size_t( cb ) );

		// we are not overwriting data just appending 
		// so we set to 0 all not used data into the page
		// we need to do this before the checksum calculation
		//	==> we now store cbChecksum, so no need to zero-extend page
		Assert( SLVPAGE_SIZE >= ib + cb );
//		memset( (BYTE*)bfl.pv + ib + cb , 0, size_t( SLVPAGE_SIZE - (ib + cb) ) );

		if ( fUpdateOwnerMap )
			{
			ulChecksum = UlChecksumSLV( (BYTE*)bfl.pv, (BYTE*)bfl.pv + ib + cb );
			}

			//  unlatch the current ifmp / pgno

		BFWriteUnlatch( &bfl );

		if ( fUpdateOwnerMap )
			{
			// if we DON'T set frontdoor checksum and frontdoor is enabled
			// (ValidChecksum flag will remaing unset)
#ifdef SLV_USE_CHECKSUM_FRONTDOOR
			slvownermapNode.AssertOnPage( pgno );
			Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, ULONG( cb ) ) );
			slvownermapNode.NextPage();
#else // SLV_USE_CHECKSUM_FRONTDOOR
			if ( rgfmp[ ifmpDb ].FDefragSLVCopy() )
				{
				slvownermapNode.AssertOnPage( pgno );
				Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, ULONG( cb ) ) );
				slvownermapNode.NextPage();
				}
			else
				{
				if ( fInvalidateChecksum )
					{
					slvownermapNode.AssertOnPage( pgno );
					Call( slvownermapNode.ErrResetChecksum( ppib ) );
					slvownermapNode.NextPage();
					}
#ifdef DEBUG
				else
					{
					slvownermapNode.AssertOnPage( pgno );
					ERR errT;
					ULONG dwT1, dwT2;
					errT = slvownermapNode.ErrGetChecksum( ppib, &dwT1, &dwT2 );
					Assert( errT < 0 );
					slvownermapNode.NextPage();
					}
#endif // DEBUG
				}
#endif // SLV_USE_CHECKSUM_FRONTDOOR
			}
		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	//  set written bytes for return

	*pcbWritten = ibData;

HandleError:
	return err;
	}

// ErrSLVSetColumnFromData		- set a SLV column from actual data
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromData(
	FUCB				* pfucb,
	const COLUMNID		columnid,
	const ULONG			itagSequence,
	DATA				* pdata,
	const ULONG			grbit )
	{
	ERR					err			= JET_errSuccess;
	DATA				dataAppend;
	CSLVInfo			slvinfo;
	CSLVInfo::HEADER	header;
		

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( !PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetAppendLV ) ) );
	
	//  get parameters for setting the SLV

	BOOL fAppend			= ( grbit & JET_bitSetAppendLV ) != 0;
	BOOL fDataRecoverable	= fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	//  we are appending

	if ( fAppend )
		{
		//  get the SLVInfo for this SLV

		Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fTrue ) );
		CallS( slvinfo.ErrGetHeader( &header ) );

		//  retrieve the data recoverability state for this SLV, ignoring
		//  whatever the user is currently requesting.  we do this to ensure
		//  consistent behavior per SLV so that we don't see half the data
		//  logged and the other half not logged, for example

		fDataRecoverable = header.fDataRecoverable;
		}

	//  we are not appending

	else
		{
		//  create the SLVInfo for this SLV

		Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
		CallS( slvinfo.ErrGetHeader( &header ) );

		//  this SLV had better be empty

		Assert( !header.cbSize );
		Assert( !header.cRun );

		//  allocate an initial run for the SLV.  we do this even if there is
		//  no data to append because all SLVs must have at least one run.  we
		//  also set the data recoverability state for this SLV

		QWORD ibLogical;
		QWORD cbSize;
		Call( ErrSLVGetRange(	pfucb->ppib,
								pfucb->ifmp,
								max( pdata->Cb(), 1 ),
								&ibLogical,
								&cbSize,
								fFalse,
								fFalse ) );
								
		CSLVInfo::RUN run;
		run.ibVirtualNext	= cbSize;
		run.ibLogical		= ibLogical;
		run.qwReserved		= 0;
		run.ibVirtual		= 0;
		run.cbSize			= cbSize;
		run.ibLogicalNext	= ibLogical + cbSize;

		CallS( slvinfo.ErrMoveAfterLast() );
		Call( slvinfo.ErrSetCurrentRun( run ) );

		// mark the space usage in SLVOWNERMAP on update operations
		if ( FFUCBReplacePrepared( pfucb ) )
			{
			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );
			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVFromData ) );
			}
		else
			{
			Assert ( FFUCBInsertPrepared( pfucb ) );
			FUCBSetSLVOwnerMapNeedUpdate( pfucb );
			}
		

		header.cRun++;
		header.fDataRecoverable = fDataRecoverable;

		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  move to the last run in this SLV

	CallS( slvinfo.ErrMoveAfterLast() );
	CallS( slvinfo.ErrMovePrev() );

	//  append the given data to the SLV

	dataAppend = *pdata;

	while ( dataAppend.Cb() )
		{
		//  there is space available in the current run for appending data

		CSLVInfo::RUN run;
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		
		if ( (QWORD) run.ibVirtualNext > (QWORD) header.cbSize )
			{
			//  write as many bytes as we can into this run

			QWORD cbWritten;
			Call( ErrSLVWriteRun(	pfucb,
									pfucb->ppib,
									pfucb->ifmp,
									run,
									header.cbSize,
									&dataAppend,
									&cbWritten,
									fDataRecoverable ) );

			//  adjust header and data to reflect bytes written

			header.cbSize += cbWritten;
			CallS( slvinfo.ErrSetHeader( header ) );

			dataAppend.DeltaCb( INT( -cbWritten ) );
			dataAppend.DeltaPv( INT_PTR( cbWritten ) );
			}

		//  there is still data to be appended to the SLV

		if ( dataAppend.Cb() )
			{
			//  try to allocate a chunk of space large enough to hold this data

			QWORD ibLogical;
			QWORD cbSize;
			Call( ErrSLVGetRange(	pfucb->ppib,
									pfucb->ifmp,
									dataAppend.Cb(),
									&ibLogical,
									&cbSize,
									fFalse,
									fFalse ) );

			//  this space can be appended to the current run

			if ( ibLogical == run.ibLogicalNext )
				{
				//  increase the ibVirtualNext of this run to reflect the newly
				//  allocated space

				run.ibVirtualNext	+= cbSize;
				run.cbSize			+= cbSize;
				run.ibLogicalNext	+= cbSize;
				}

			//  this space cannot be appended to the current run

			else
				{
				//  remember the current run's ibVirtualNext for the new run's
				//  ibVirtual

				const QWORD ibVirtual = run.ibVirtualNext;

				//  move to after the last run to append a new run

				CallS( slvinfo.ErrMoveAfterLast() );

				//  set the run to include this data

				run.ibVirtualNext	= ibVirtual + cbSize;
				run.ibLogical		= ibLogical;
				run.qwReserved		= 0;
				run.ibVirtual		= ibVirtual;
				run.cbSize			= cbSize;
				run.ibLogicalNext	= ibLogical + cbSize;

				//  add a run to the header

				header.cRun++;
				CallS( slvinfo.ErrSetHeader( header ) );
				}

			// mark the space usage in SLVOWNERMAP on update operations
			if ( FFUCBReplacePrepared( pfucb ) )
				{
				const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
				const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );
				Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
				Call( ErrSLVOwnerMapSetUsageRange(
							pfucb->ppib,
							pfucb->ifmp,
							pgnoFirst,
							cpg,
							pfucb->u.pfcb->ObjidFDP(),
							columnid,
							&pfucb->bmCurr,
							fSLVOWNERMAPSetSLVFromData ) );
				}
			else
				{
				Assert ( FFUCBInsertPrepared( pfucb ) );
				FUCBSetSLVOwnerMapNeedUpdate( pfucb );
				}

			//  save our changes to this run

			Call( slvinfo.ErrSetCurrentRun( run ) );
			}
		}

	//  save our changes to the SLVInfo for this SLV

	Call( slvinfo.ErrSave() );

HandleError:
	slvinfo.Unload();
	return err;
	}


// ErrSLVSetColumn 				- set a SLV column
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		ibOffset				- offset
//		grbit					- grbit
//		pdata					- source buffer
//
// RESULT:						ERR

ERR ErrSLVSetColumn(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const ULONG		ibOffset,
	ULONG			grbit,
	DATA			* pdata )
	{
	ERR				err		= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	ASSERT_VALID( pfucb->ppib );
	if ( !pfucb->ppib->level )
		{
		CallR( ErrERRCheck( JET_errNotInTransaction ) );
		}

	if ( FFUCBReplaceNoLockPrepared( pfucb ) )
		{
		CallR( ErrRECUpgradeReplaceNoLock( pfucb ) );
		}
		
	Assert( FCOLUMNIDTagged( columnid ) );
	if ( ibOffset )
		{
		CallR( ErrERRCheck( JET_errInvalidParameter ) );
		}
	Assert( pdata );
	ASSERT_VALID( pdata );

	//  normalize the sequence number we are going to use to set the field so
	//  that if a non-existent sequence number was specified we use the next
	//  available sequence number

	TAGFIELDS	tagfields( pfucb->dataWorkBuf );
	FCB			* const pfcb		= pfucb->u.pfcb;
	const ULONG	itagSequenceMost	= tagfields.UlColumnInstances(
											pfcb,
											columnid,
											FRECUseDerivedBit( columnid, pfcb->Ptdb() ) );
	ULONG itagSequenceSet			= itagSequence && itagSequence <= itagSequenceMost ?
											itagSequence :
											itagSequenceMost + 1;

	//  get parameters for setting the SLV

	const BOOL	fExisting	= ( itagSequenceSet <= itagSequenceMost );

	if ( !fExisting )
		{
		//	strip off AppendLV flag if SLV doesn't exist
		grbit &= ~( grbit & JET_bitSetAppendLV );
		}
		
	const BOOL	fAppend		= ( grbit & JET_bitSetAppendLV );

	//  perform the set column in a transaction to allow rollback on an error
	
	CallR( ErrDIRBeginTransaction( pfucb->ppib, NO_GRBIT ) );

	//  we are replacing an existing SLV

	if ( fExisting && !fAppend )
		{
		//  decommit any space owned by the existing SLV

		Call( ErrSLVDelete( pfucb, columnid, itagSequenceSet, fTrue ) );

		//  delete all SLVInfo for this field
		
		DATA dataNull;
		dataNull.Nullify();
		Call( ErrRECSetLongField(	pfucb,
									columnid,
									itagSequenceSet,
									&dataNull,
									NO_GRBIT,
									0,
									0 ) );
		}

	//  we have data to set for this SLV or we are appending

	if ( pdata->Cb() || fAppend )
		{
		//  we are being asked to set the SLV from an EA list

		if ( grbit & JET_bitSetSLVFromSLVEA )
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetSLVFromSLVEA and the SLV Provider must be enabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetSLVFromSLVEA ) ) ||
					!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}

			//  set the SLV from an EA list

			Call( ErrSLVSetColumnFromEA(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}

		//  we are being asked to set the SLV from a file handle

		else if ( grbit & JET_bitSetSLVFromSLVFile )
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetSLVFromSLVFile and the SLV Provider must be enabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetSLVFromSLVFile ) ) ||
					!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}
			
			//  set the SLV from a file handle

			Call( ErrSLVSetColumnFromFile(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}

		//  we are being asked to return the actual SLV data

		else
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetAppendLV and the SLV Provider must be disabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetAppendLV ) ) ||
					PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}

			//  set the SLV from actual data

			Call( ErrSLVSetColumnFromData(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}
		}

	//  commit on success

	Call( ErrDIRCommitTransaction( pfucb->ppib, NO_GRBIT ) );

	return JET_errSuccess;

	//  rollback on an error

HandleError:
	CallSx( ErrDIRRollback( pfucb->ppib ), JET_errRollbackError );
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVPrereadRun(	PIB const *	ppib,
							const IFMP ifmpDb,
							const CSLVInfo::RUN& run,
							const QWORD ibVirtualStart,
							const QWORD cbData )
//  ================================================================
//
//  Preread the pages in the run. All pages will be preread in 64K chunks
//
//-
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );

	const IFMP	ifmp	= ifmpDb | ifmpSLV;

	QWORD ibData;
	QWORD ibLogical;
	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;

	const INT 	cpgnoMax 	= 16;	//	64K I/Os
	INT			ipgno 		= 0;
	PGNO		rgpgno[cpgnoMax+1];	//	NULL terminated
	
	//  preread pages in run until we have run out of data or read as much as we want

	while ( ibData < cbData
			&& ibLogical < run.ibLogicalNext )
		{
		//  determine the pgno / ib / cb where we will read data

		const PGNO	pgno	= PgnoOfOffset( ibLogical );
		const QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		const QWORD	cb		= min( cbData - ibData, SLVPAGE_SIZE - ib );

		rgpgno[ipgno++] = pgno;

		if( cpgnoMax == ipgno )
			{
			rgpgno[ipgno] = pgnoNull;
			BFPrereadPageList( ifmp, rgpgno );

			ipgno = 0;
			}

		
		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	rgpgno[ipgno] = pgnoNull;
	BFPrereadPageList( ifmp, rgpgno );
	
	return err;
	}


// ErrSLVReadRun				- reads SLV Data from an SLV Run
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		run						- SLV Run
//		ibOffset				- offset to start reading
//		pdata					- destination buffer
//		pcbRead					- ULONG where to return the read data size
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbRead					- SLV Data bytes read

LOCAL ERR ErrSLVReadRun(	PIB*			ppib,
							IFMP			ifmpDb,
							CSLVInfo::RUN&	run,
							QWORD			ibVirtualStart,
							DATA*			pdata,
							QWORD*			pcbRead )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( pcbRead );

	//  read data from pages in run until we have run out of data or buffer

	QWORD ibData;
	QWORD ibLogical;
	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;
	
	while ( ibData < pdata->Cb() && ibLogical < run.ibLogicalNext )
		{
		//  determine the ifmp / pgno / ib / cb where we will read data

		IFMP	ifmp	= ifmpDb | ifmpSLV;
		PGNO	pgno	= PgnoOfOffset( ibLogical );
		QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		QWORD	cb		= min( pdata->Cb() - ibData, SLVPAGE_SIZE - ib );
		
		//  latch the current ifmp / pgno

		BFLatch bfl;
		Call( ErrBFReadLatchPage( &bfl, ifmp, pgno ) );

		//  copy the desired data from the page into the buffer

		UtilMemCpy( (BYTE*)pdata->Pv() + ibData, (BYTE*)bfl.pv + ib, size_t( cb ) );

		//  unlatch the current ifmp / pgno

		BFReadUnlatch( &bfl );

		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	// we expect only this warning from BF and it must not be propagated
	Assert(	JET_errSuccess == err ||
			wrnBFPageFault == err ||
			wrnBFPageFlushPending == err );
	err = JET_errSuccess;
	
	//  set read bytes for return

	*pcbRead = ibData;

HandleError:
	return err;
	}


// ErrSLVRetrieveColumnAsData	- retrieve a SLV column as SLV Data
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		slvinfo					- SLV Info
//		ibOffset				- offset to start reading
//		pdata					- destination buffer
//		pcbActual				- ULONG where to return the data size
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbActual				- SLV Data bytes remaining after ibOffset
//
	
LOCAL ERR ErrSLVRetrieveColumnAsData(	PIB*		ppib,
										IFMP		ifmpDb,
										CSLVInfo&	slvinfo,
										ULONG		ibOffset,
										DATA*		pdata,
										ULONG*		pcbActual )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	Assert( !PinstFromIfmp( ifmpDb )->FSLVProviderEnabled() );
	Assert( pdata );
	ASSERT_VALID( pdata );
	
	//  get the header for this SLV

	CSLVInfo::HEADER header;
	CallS( slvinfo.ErrGetHeader( &header ) );

	//  we are being asked to read beyond the end of the SLV

	if ( ibOffset >= header.cbSize )
		{
		//  don't read data and return 0 bytes read

		if ( pcbActual )
			{
			*pcbActual = 0;
			}
		}

	//  we are not being asked to read beyond the end of the SLV

	else
		{
		//  the provided buffer is not zero-length (fast path getting the SLV size)

		if ( pdata->Cb() )
			{
			//  seek to the run before the run that contains the requested offset

			Call( slvinfo.ErrSeek( ibOffset ) );
			CallSx( slvinfo.ErrMovePrev(), JET_errNoCurrentRecord );
			
			//  retrieve data from the SLV until we run out of data or buffer
			
			QWORD ibVirtual	= ibOffset;
			QWORD ibData	= 0;

			while (	ibData < pdata->Cb() &&
					( err = slvinfo.ErrMoveNext() ) >= JET_errSuccess )
				{
				//  get the current run

				CSLVInfo::RUN run;
				Call( slvinfo.ErrGetCurrentRun( &run ) );
				
				//  read data from this run into the SLV

				DATA dataRun;
				dataRun.SetPv( (BYTE*)pdata->Pv() + ibData );
				dataRun.SetCb( ULONG( pdata->Cb() - ibData ) );

				Call( ErrSLVPrereadRun(	ppib,
										ifmpDb,
										run,
										ibVirtual,
										dataRun.Cb() ) );
				
				QWORD cbRead;
				Call( ErrSLVReadRun(	ppib,
										ifmpDb,
										run,
										ibVirtual,
										&dataRun,
										&cbRead ) );
				ibData 		+= cbRead;
				ibVirtual 	+= cbRead;
				}
			if ( err == JET_errNoCurrentRecord )
				{
				err = JET_errSuccess;
				}
			}

		//  return the amount of data in the SLV after the given offset

		if ( pcbActual )
			{
			*pcbActual = ULONG( header.cbSize - ibOffset );
			}
		if ( err >= JET_errSuccess && header.cbSize - ibOffset > pdata->Cb() )
			{
			err = JET_wrnBufferTruncated;
			}
		}

HandleError:
	return err;
	}

// ErrSLVRetrieveColumn			- retrieve a SLV column
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		ibOffset				- offset to start reading
//		grbit					- grbit
//		dataSLVInfo				- field from record
//		pdata					- destination buffer
//		pcbActual				- ULONG where to return the data size
//		pfLatched				- BOOL where to return record latch status 
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- retrieved data
//		pcbActual				- SLV bytes remaining after ibOffset
//		pfLatched				- record latch status

ERR ErrSLVRetrieveColumn(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fSeparatedSLV,
	const ULONG		ibOffset,
	ULONG			grbit,
	DATA&			dataSLVInfo,
	DATA			* pdata,
	ULONG			* pcbActual )
	{
	ERR 			err			= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	ASSERT_VALID( &dataSLVInfo );
	Assert( pdata );
	ASSERT_VALID( pdata );

	//  strip off copy buffer flag if present

	const BOOL	fCopyBuffer = ( grbit & JET_bitRetrieveCopy ) && FFUCBUpdatePrepared( pfucb );
	grbit = grbit & ~JET_bitRetrieveCopy;

	//  ignore JET_bitRetrieveNull as default values are not supported for SLVs

	grbit = grbit & ~JET_bitRetrieveNull;

	//  get the SLV Info for this SLV
	
	CSLVInfo slvinfo;
	Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fCopyBuffer, &dataSLVInfo, fSeparatedSLV ) );

// OwnerMap checking on SLV retrieve will fail
// if OLDSLV is running. Just disabled until a proper 
// check is found or ... 
#ifdef NEVER 

#ifdef DEBUG
	// the space map is not updated if the retrieve column is done
	// from inside an on going transaction and the record is prep insert
	// so don't check in this case
	if ( !FFUCBSLVOwnerMapNeedUpdate( pfucb ) )
		{
		ERR errT = slvinfo.ErrMoveBeforeFirst();
		
		if ( JET_errSuccess <= errT )
			errT = slvinfo.ErrMoveNext();

		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
			
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE;
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			
			Call( ErrSLVOwnerMapCheckUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr));
			
			errT = slvinfo.ErrMoveNext();
			}
		}
	
#endif // DEBUG
#endif // NEVER

	//  we are being asked to return an EA list

	if ( grbit & JET_bitRetrieveSLVAsSLVEA )
		{
		//  no other grbits may be selected and the SLV Provider must be enabled

		if (	grbit != JET_bitRetrieveSLVAsSLVEA ||
				!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  convert the SLV Info into an EA List

		BTUp( pfucb );

		Call( ErrOSSLVFileConvertSLVInfoToEA(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
												slvinfo,
												ibOffset,
												pdata->Pv(),
												pdata->Cb(),
												pcbActual ) );
		}

	//  we are being asked to return a file handle

	else if ( grbit & JET_bitRetrieveSLVAsSLVFile )
		{
		//  no other grbits may be selected and the SLV Provider must be enabled

		if (	grbit != JET_bitRetrieveSLVAsSLVFile ||
				!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  convert the SLV Info into an SLV File

		BTUp( pfucb );

		Call( ErrOSSLVFileConvertSLVInfoToFile(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
												slvinfo,
												ibOffset,
												pdata->Pv(),
												pdata->Cb(),
												pcbActual ) );
		}

	//  we are being asked to return the actual SLV data

	else
		{
		//  no other grbits may be selected and the SLV Provider must be disabled

		if (	grbit ||
				PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  return the actual data

		Call( ErrSLVRetrieveColumnAsData(	pfucb->ppib,
											pfucb->ifmp,
											slvinfo,
											ibOffset,
											pdata,
											pcbActual ) );
		}

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVCopyUsingData			- copies an SLV from the given source field to
//								  the copy buffer containing the given destination
//								  field.  the destination must not already exist.
//								  the SLV is copied via the Buffer Manager
//
// IN:
//		pfucbSrc				- source cursor
//		columnidSrc				- source field ID
//		itagSequenceSrc			- source sequence
//		pfucbDest				- destination cursor
//		columnidDest			- destination field ID
//		itagSequenceDest		- destination sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyUsingData(
	FUCB			 * pfucbSrc,
	const COLUMNID	columnidSrc,
	const ULONG		itagSequenceSrc,
	FUCB			* pfucbDest, 
	COLUMNID		columnidDest,
	ULONG			itagSequenceDest )
	{
	ERR				err				= JET_errSuccess;
	CSLVInfo		slvinfoSrc;
	CSLVInfo		slvinfoDest;
	void*			pvTempBuffer	= NULL;
	DATA			dataTemp;
	
	CSLVInfo::RUN runSrc;
	CSLVInfo::RUN runDest;
	CSLVInfo::HEADER headerSrc;
	CSLVInfo::HEADER headerDest;
	
	//  validate IN args

	ASSERT_VALID( pfucbSrc );
	Assert( columnidSrc > 0 );
	Assert( itagSequenceSrc > 0 );

	ASSERT_VALID( pfucbDest );
	Assert( columnidDest > 0 );
	Assert( itagSequenceDest > 0 );
	
	//  load the source and destination SLVs

	Call( slvinfoSrc.ErrLoad( pfucbSrc, columnidSrc, itagSequenceSrc, fFalse ) );

	Call( slvinfoDest.ErrCreate( pfucbDest, columnidDest, itagSequenceDest, fTrue ) );

	//  get the header for the source and destination SLVs

	CallS( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	CallS( slvinfoDest.ErrGetHeader( &headerDest ) );

	//  the destination SLV had better be empty

	Assert( !headerDest.cbSize );
	Assert( !headerDest.cRun );

	//  allocate an initial run for the destination SLV.  we do this even if
	//  there is no data to copy because all SLVs must have at least one run.
	//  we also set the data recoverability state for the destination SLV to
	//  be the same as for the source SLV

	QWORD ibLogical;
	QWORD cbSize;
	Call( ErrSLVGetRange(	pfucbDest->ppib,
							pfucbDest->ifmp,
							max( (ULONG) headerSrc.cbSize, 1 ),
							&ibLogical,
							&cbSize,
							fFalse,
							fFalse ) );
							
	runDest.ibVirtualNext	= cbSize;
	runDest.ibLogical		= ibLogical;
	runDest.qwReserved		= 0;
	runDest.ibVirtual		= 0;
	runDest.cbSize			= cbSize;
	runDest.ibLogicalNext	= ibLogical + cbSize;

	CallS( slvinfoDest.ErrMoveAfterLast() );
	Call( slvinfoDest.ErrSetCurrentRun( runDest ) );

	headerDest.cRun++;
	headerDest.fDataRecoverable = headerSrc.fDataRecoverable;

	CallS( slvinfoDest.ErrSetHeader( headerDest ) );
	
	//  copy all data from the source SLV to the destination SLV

	QWORD ibVirtual;
	ibVirtual = 0;
	
	memset( &runSrc, 0, sizeof( runSrc ) );
	
	BFAlloc( &pvTempBuffer );
	dataTemp.SetPv( pvTempBuffer );
	dataTemp.SetCb( g_cbPage );

	while ( ibVirtual < headerSrc.cbSize )
		{
		//  we need to retrieve another run from the source SLV

		Assert( ibVirtual <= runSrc.ibVirtualNext );
		if ( ibVirtual == runSrc.ibVirtualNext )
			{
			//  retrieve the next run from the source SLV

			Call( slvinfoSrc.ErrMoveNext() );
			Call( slvinfoSrc.ErrGetCurrentRun( &runSrc ) );
			}

		//  we need to allocate more space for the destination SLV

		Assert( ibVirtual <= runDest.ibVirtualNext );
		if ( ibVirtual == runDest.ibVirtualNext )
			{
			//  try to allocate a chunk of space large enough to hold this data

			Call( ErrSLVGetRange(	pfucbDest->ppib,
									pfucbDest->ifmp,
									headerSrc.cbSize - ibVirtual,
									&ibLogical,
									&cbSize,
									fFalse,
									fFalse ) );

			//  this space can be appended to the current run

			if ( headerDest.cRun && ibLogical == runDest.ibLogicalNext )
				{
				//  change the run to reflect the newly allocated space

				runDest.ibVirtualNext	+= cbSize;
				runDest.cbSize			+= cbSize;
				runDest.ibLogicalNext	+= cbSize;
				}

			//  this space cannot be appended to the current run

			else
				{
				//  move to after the last run to append a new run

				CallS( slvinfoDest.ErrMoveAfterLast() );

				//  set the run to include this data

				runDest.ibVirtualNext	= ibVirtual + cbSize;
				runDest.ibLogical		= ibLogical;
				runDest.qwReserved		= 0;
				runDest.ibVirtual		= ibVirtual;
				runDest.cbSize			= cbSize;
				runDest.ibLogicalNext	= ibLogical + cbSize;

				//  add a run to the header

				headerDest.cRun++;
				CallS( slvinfoDest.ErrSetHeader( headerDest ) );
				}

			//  save our changes to this run

			Call( slvinfoDest.ErrSetCurrentRun( runDest ) );
			}

		//  make sure that we don't read past the end of the valid data region

		dataTemp.SetCb( ULONG( min( dataTemp.Cb(), headerSrc.cbSize - ibVirtual ) ) );

		//  read data from the source SLV

		Call( ErrSLVPrereadRun(	pfucbSrc->ppib,
								pfucbSrc->ifmp,
								runSrc,
								ibVirtual,
								dataTemp.Cb() ) );

		QWORD cbRead;
		Call( ErrSLVReadRun(	pfucbSrc->ppib,
								pfucbSrc->ifmp,
								runSrc,
								ibVirtual,
								&dataTemp,
								&cbRead ) );
		Assert( cbRead == dataTemp.Cb() );

		//  write data to the destination SLV, logging the data if the source
		//  SLV's data is recoverable

		QWORD cbWritten;
		Call( ErrSLVWriteRun(	pfucbDest,
								pfucbDest->ppib,
								pfucbDest->ifmp,
								runDest,
								ibVirtual,
								&dataTemp,
								&cbWritten,
								headerDest.fDataRecoverable ) );
		Assert( cbWritten == dataTemp.Cb() );

		//  adjust header to reflect bytes copied

		headerDest.cbSize = headerDest.cbSize + dataTemp.Cb();
		CallS( slvinfoDest.ErrSetHeader( headerDest ) );

		//  advance our copy pointer

		ibVirtual += dataTemp.Cb();
		}

	if ( FFUCBReplacePrepared( pfucbDest ) )
		{
		ERR errT = slvinfoDest.ErrMoveBeforeFirst();
				
		Assert ( JET_errSuccess == errT );
		
		errT = slvinfoDest.ErrMoveNext();
		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
		
			Call( slvinfoDest.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucbDest->u.pfcb->FPrimaryIndex() && pfucbDest->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucbDest->ppib,
						pfucbDest->ifmp,
						pgnoFirst,
						cpg,
						pfucbDest->u.pfcb->ObjidFDP(),
						columnidDest,
						&pfucbDest->bmCurr,
						fSLVOWNERMAPSetSLVCopyData ) );

			errT = slvinfoDest.ErrMoveNext();
			}
		Assert ( JET_errNoCurrentRecord == errT );

		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucbDest ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucbDest );
		}		

	//  save our changes to the destination SLV

	Call( slvinfoDest.ErrSave() );

HandleError:
	if ( pvTempBuffer )
		{
		BFFree( pvTempBuffer );
		}
	slvinfoDest.Unload();
	slvinfoSrc.Unload();
	return err;
	}

// ErrSLVCopyUsingFiles			- copies an SLV from the field in the record to
//								  the same field in the record in the copy buffer.
//								  that SLV must not already exist.  the SLV is
//								  copied via the SLV Provider
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyUsingFiles(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence )
	{
	ERR				err			= JET_errSuccess;
	CSLVInfo		slvinfoSrc;
	IFileAPI*		pfapiSrc	= NULL;
	
	CSLVInfo::HEADER headerSrc;
	
	//  validate IN args

	ASSERT_VALID( pfucb );


	//  open the source SLV File and read its header.  use a temporary file
	//  name so that we are sure we copy the original data and not whatever
	//  the file currently contains

	Call( slvinfoSrc.ErrLoad( pfucb, columnid, itagSequence, fFalse ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	swprintf( wszFileName, L"$CopySrc%016I64X$", rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() );
	Call( slvinfoSrc.ErrSetFileNameVolatile() );
	Call( slvinfoSrc.ErrSetFileName( wszFileName ) );

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							slvinfoSrc,
							&pfapiSrc,
							fTrue,
							fTrue ) );

	CallS( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	//  copy the source SLV File into a new destination SLV File at the same
	//  field in the copy buffer with the same data recoverability as the source

	Call( ErrSLVCopyFile(	slvinfoSrc,
							pfapiSrc,
							headerSrc.cbSize,
							headerSrc.fDataRecoverable,
							pfucb,
							columnid,
							itagSequence ) );
 
HandleError:
	delete pfapiSrc;
	slvinfoSrc.Unload();
	return err;
	}

// ErrSLVCopy					- copies an SLV from the field in the record to
//								  the same field in the record in the copy buffer.
//								  that SLV must not already exist
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

ERR ErrSLVCopy(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence )
	{
	ERR				err			= JET_errSuccess;
	
	//  validate IN args

	ASSERT_VALID( pfucb );

	//  the SLV Provider is enabled

	if ( PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
		{
		//  copy the SLV using the SLV Provider

		Call( ErrSLVCopyUsingFiles( pfucb, columnid, itagSequence ) );
		}

	//  the SLV Provider is not enabled

	else
		{
		//  copy the SLV using data
		Call( ErrSLVCopyUsingData(	pfucb,
									columnid,
									itagSequence,
									pfucb,
									columnid,
									itagSequence ) );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVMoveUsingData(	PIB * const ppib,
								const IFMP ifmpDb,
								CSLVInfo& slvinfoSrc,
								CSLVInfo& slvinfoDest )
//  ================================================================
	{
	ERR			err				= JET_errSuccess;

	CSLVInfo::RUN runSrc;
	CSLVInfo::RUN runDest;

	const IFMP	ifmp	= ifmpDb | ifmpSLV;

	BFLatch	bflPageSrc;
	BFLatch	bflPageDest;

	bflPageSrc.pv	= NULL;
	bflPageDest.pv	= NULL;

	CSLVInfo::HEADER 	headerSrc;
	
	SLVOWNERMAPNODE		slvownermapNode;

	Call( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	Call( slvinfoSrc.ErrMoveBeforeFirst() );
	Call( slvinfoDest.ErrMoveBeforeFirst() );
	
	//  copy all data from the source SLV to the destination SLV

	QWORD ibVirtual;
	ibVirtual = 0;

	//	these need to be zeroed out for the loop below (ibVirtualNext specifically)
	
	memset( &runSrc, 0, sizeof( runSrc ) );
	
	memset( &runDest, 0, sizeof( runDest ) );

	while ( ibVirtual < headerSrc.cbSize && FOLDSLVContinue( ifmpDb ) )
		{
		const QWORD cbToCopy = min( g_cbPage, headerSrc.cbSize - ibVirtual );
		const QWORD cbToZero = g_cbPage - cbToCopy;

		Assert( ibVirtual <= runSrc.ibVirtualNext );
		if ( ibVirtual == runSrc.ibVirtualNext )
			{
			//  retrieve the next run from the source SLV

			Call( slvinfoSrc.ErrMoveNext() );
			Call( slvinfoSrc.ErrGetCurrentRun( &runSrc ) );

			//	preread the pages in the run that we will touch

			QWORD cbToPreread;
			cbToPreread = min( runSrc.cbSize, headerSrc.cbSize - ibVirtual );
			
			Call( ErrSLVPrereadRun(	ppib,
									ifmpDb,
									runSrc,
									ibVirtual,
									cbToPreread ) );
			}

		Assert( ibVirtual <= runDest.ibVirtualNext );
		if ( ibVirtual == runDest.ibVirtualNext )
			{
			//  retrieve the next run from the destination SLV

			Call( slvinfoDest.ErrMoveNext() );
			Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );
			Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( runDest.ibLogical + ibVirtual - runDest.ibVirtual ) ) );
			}

		//  determine the pgno to read from

		const QWORD 	ibLogicalSrc	= runSrc.ibLogical + ibVirtual - runSrc.ibVirtual;
		const PGNO		pgnoSrc			= PgnoOfOffset( ibLogicalSrc );
		
		//  RDW Latch the current ifmp / pgno.  this allows others to read the
		//  page but also allows us to set a dependency

		Call( ErrBFRDWLatchPage( &bflPageSrc, ifmp, pgnoSrc ) );

		const BOOL dwChecksum			= LOG::UlChecksumBytes( (BYTE *)bflPageSrc.pv, (BYTE *)bflPageSrc.pv+cbToCopy, 0 );
		
		//  determine the pgno that we will copy the data to

		const QWORD 	ibLogicalDest	= runDest.ibLogical + ibVirtual - runDest.ibVirtual;
		const PGNO		pgnoDest		= PgnoOfOffset( ibLogicalDest );

		//  Write Latch the new page

		Call( ErrBFWriteLatchPage( &bflPageDest, ifmp, pgnoDest, bflfNew ) );

		//  set buffer dependency

		Call( ErrBFDepend( &bflPageSrc, &bflPageDest ) );

		//  copy the data from the old page to the new

		memcpy( bflPageDest.pv, bflPageSrc.pv, size_t( cbToCopy ) );
		memset( (BYTE *)bflPageDest.pv + cbToCopy, 0, size_t( cbToZero ) ); 

		//	log the operation and obtain an lgpos
		//	logging this as a SLVPageMove wasn't good because the pages
		//	weren't flushed to the streaming file immediately and 
		//	we didn't know when to replay operations
		
		LGPOS lgpos;
		Call( ErrLGSLVPageAppend(	ppib,
									ifmp,
									ibLogicalDest,
									ULONG( cbToCopy ),
									bflPageSrc.pv,
									fTrue,
									fTrue,
									&slvownermapNode,
									&lgpos ) );
		slvownermapNode.NextPage();

		//  dirty the destination page
		
		BFDirty( &bflPageDest );

		//	set log dependency

		Assert( !rgfmp[ ifmpDb ].FLogOn() || !PinstFromIfmp( ifmpDb )->m_plog->m_fLogDisabled );
		if ( rgfmp[ ifmpDb ].FLogOn() )
			{
			BFSetLgposBegin0( &bflPageDest, ppib->lgposStart );
			BFSetLgposModify( &bflPageDest, lgpos );
			}
				
		//  release the latches

		BFRDWUnlatch( &bflPageSrc );
		bflPageSrc.pv = NULL;
		
		BFWriteUnlatch( &bflPageDest );
		bflPageDest.pv = NULL;

		//  advance our copy pointer

		ibVirtual += cbToCopy;
		}

HandleError:

	if ( NULL != bflPageSrc.pv )
		{
		Assert( err < 0 );
		Assert( NULL != bflPageDest.pv );
		BFRDWUnlatch( &bflPageSrc );
		bflPageSrc.pv = NULL;		
		}

	if ( NULL != bflPageDest.pv )
		{		
		Assert( err < 0 );
		BFWriteUnlatch( &bflPageDest );
		bflPageDest.pv = NULL;
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVMoveUsingFiles(	PIB * const ppib,
								const IFMP ifmpDb,
								CSLVInfo& slvinfoSrc,
								CSLVInfo& slvinfoDest )
//  ================================================================
	{
	ERR			err				= JET_errSuccess;
	
	IFileAPI*	pfapiDest		= NULL;
	IFileAPI*	pfapiSrc		= NULL;
	
	size_t		cChunk			= 0;
	size_t		cbChunk			= 0;
	SLVCHUNK*	rgslvchunk		= NULL;
	BYTE*		rgbChunk		= NULL;

	CSLVInfo::HEADER	headerSrc;
	CSLVInfo::RUN		runSrc;

	CSLVInfo::HEADER	headerDest;
	CSLVInfo::RUN		runDest;

	SLVOWNERMAPNODE		slvownermapNode;

	//  these will be needed for logging 
	
	memset( &runSrc, 0, sizeof( runSrc ) );
	memset( &runDest, 0, sizeof( runDest ) );

	//  open the files.  use a temporary file name for the source SLV File
	//  so that we are sure we copy the original data and not whatever the
	//  file currently contains.  also round the destination SLV File size
	//  to permit uncached writes

	CallS( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	swprintf( wszFileName, L"$MoveSrc%016I64X$", rgfmp[ ifmpDb ].DbtimeIncrementAndGet() );
	Call( slvinfoSrc.ErrSetFileNameVolatile() );
	Call( slvinfoSrc.ErrSetFileName( wszFileName ) );

	Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
							slvinfoSrc,
							&pfapiSrc,
							fTrue,
							fTrue ) );

	CallS( slvinfoDest.ErrGetHeader( &headerDest ) );
	if ( headerDest.cbSize != headerSrc.cbSize )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}
	headerDest.cbSize = ( ( headerDest.cbSize + g_cbPage - 1 ) / g_cbPage ) * g_cbPage;
	CallS( slvinfoDest.ErrSetHeader( headerDest ) );

	swprintf( wszFileName, L"$MoveDest%016I64X$", rgfmp[ ifmpDb ].DbtimeIncrementAndGet() );
	Call( slvinfoDest.ErrSetFileNameVolatile() );
	Call( slvinfoDest.ErrSetFileName( wszFileName ) );

	Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
							slvinfoDest,
							&pfapiDest,
							fFalse,
							fFalse ) );

	//  allocate resources for the copy
	//  CONSIDER:  expose these settings

	cbChunk	= size_t( min( 64 * 1024, headerDest.cbSize ) );

	if ( cbChunk == 0 )
		{
		Assert( 0 == headerSrc.cbSize );
		Assert( 0 == cChunk);	
		Assert( NULL == rgslvchunk);	
		Assert( NULL == rgbChunk);	
		}
	else
		{
		Assert( cbChunk);	
		Assert( headerSrc.cbSize );
		cChunk = size_t( min( 16, headerDest.cbSize / cbChunk ) );

		if ( !( rgslvchunk = new SLVCHUNK[ cChunk ] ) )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		//	this memory must be page-aligned for I/O
		
		if ( !( rgbChunk = (BYTE*)PvOSMemoryPageAlloc( cChunk * cbChunk, NULL ) ) )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		}

	//  pre-issue reads to the source SLV File for the first n chunks of the data
	//  to be copied to prime the pump for the copy operation

	QWORD ibVirtualMac;
	ibVirtualMac = min( headerSrc.cbSize, cChunk * cbChunk );
	
	QWORD ibVirtual;
	ibVirtual = 0;

	while ( ibVirtual < ibVirtualMac )
		{
		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	
		
		const size_t	iChunk 		= size_t( ( ibVirtual / cbChunk ) % cChunk );
		const DWORD		cbToRead 	= DWORD( min( cbChunk, ibVirtualMac - ibVirtual ) );
		BYTE * const	pbRead	 	= rgbChunk + iChunk * cbChunk;

		rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
		rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
		rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
		rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

		CallS( pfapiSrc->ErrIORead(	ibVirtual,
									cbToRead,
									pbRead,
									IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
									DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );

		ibVirtual += cbChunk;
		}
	CallS( pfapiSrc->ErrIOIssue() );

	//  we need to log the move on a run-by-run basis
	
	Call( slvinfoDest.ErrMoveBeforeFirst() );

	//  copy all data from the source SLV File to the destination SLV File,

	ibVirtual = 0;

	while ( ibVirtual < headerSrc.cbSize && FOLDSLVContinue( ifmpDb ) )
		{
		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	
		
		//  which chunk are we on?
		
		const size_t	iChunk 		= size_t( ( ibVirtual / cbChunk ) % cChunk );
		BYTE * const	pbChunk 	= rgbChunk + ( iChunk * cbChunk );
		
		//  how much data did we read?
		
		const QWORD cbRead = min( cbChunk, headerSrc.cbSize - ibVirtual );

		//  how many pages is that?
		
		const QWORD cPagesRead = ( ( cbRead + g_cbPage - 1 ) / g_cbPage );

		//  zero out up to a page boundary

		const QWORD cbToZero = ( cPagesRead * g_cbPage ) - cbRead;

		//  wait for the read to complete
		
		rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
		Call( rgslvchunk[ iChunk ].m_err );

		//  zero out unused space
		
		memset( pbChunk + cbRead, 0, size_t( cbToZero ) );
		
		//  now we have to log in page-sized chunks

		QWORD cPagesLogged;
		for( cPagesLogged = 0; cPagesLogged < cPagesRead; ++cPagesLogged )
			{
			
			//  calculate the checksum

			const BYTE * const pbData 		= pbChunk + ( cPagesLogged * g_cbPage );
			const BYTE * const pbDataMax	= pbChunk + cbRead;
			const INT cbData				= (INT)min( g_cbPage, pbDataMax - pbData );

			Assert( ibVirtual <= runDest.ibVirtualNext );
			if ( ibVirtual == runDest.ibVirtualNext )
				{
				
				//  retrieve the next run from the destination SLV

				Call( slvinfoDest.ErrMoveNext() );
				Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );
				Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( runDest.ibLogical + ibVirtual - runDest.ibVirtual ) ) );
				}

			//	log the operation and obtain an lgpos
			//	logging this as a SLVPageMove wasn't good because the pages
			//	weren't flushed to the streaming file immediately and 
			//	we didn't know when to replay operations

			LGPOS lgpos;
			Call( ErrLGSLVPageAppend(	ppib,
										ifmpDb | ifmpSLV,
										ibVirtual - runDest.ibVirtual + runDest.ibLogical,
										cbData,
										const_cast<BYTE *>( pbData ),
										fTrue,
										fTrue,
										&slvownermapNode,
										&lgpos ) );
									
			//  advance our copy pointer

			ibVirtual += g_cbPage;
			slvownermapNode.NextPage();
			}

		//  we are done logging this chunk of the destination SLV File

		Assert( ( ibVirtual % g_cbPage ) == 0 );
		Assert( ( ibVirtual % cbChunk ) == 0 || ibVirtual >= headerSrc.cbSize );
			
		//  wait for the write for the current chunk to complete

		rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
		Call( rgslvchunk[ iChunk ].m_err );

		//  there is more data to read from the source SLV File

		QWORD ibVirtualChunkNext;
		ibVirtualChunkNext = ibVirtual + ( cChunk - 1 ) * cbChunk;

		if ( ibVirtualChunkNext < headerSrc.cbSize )
			{
			const QWORD cbToRead 	= min( cbChunk, headerSrc.cbSize - ibVirtualChunkNext  );
			BYTE * const pbRead 	= rgbChunk + iChunk * cbChunk;
			
			//  issue a read to the source SLV File for the next chunk
			
			rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
			rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
			rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
			rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

			CallS( pfapiSrc->ErrIORead(	ibVirtualChunkNext,
										DWORD( cbToRead ),
										pbRead,
										IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
										DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );
			CallS( pfapiSrc->ErrIOIssue() );
			}
		}

	//  register the move with the SLV Provider
	
	Call( ErrOSSLVRootMoveFile(	rgfmp[ ifmpDb ].SlvrootSLV(),
								pfapiSrc,
								slvinfoSrc,
								pfapiDest,
								slvinfoDest,
								QWORD( rgfmp[ ifmpDb ].DbtimeIncrementAndGet() ) ) );

	//  set the destination file size back to its correct size
	
	headerDest.cbSize = headerSrc.cbSize;
	CallS( slvinfoDest.ErrSetHeader( headerDest ) );
	
HandleError:
	if ( rgslvchunk )
		{
		for ( DWORD iChunk = 0; iChunk < cChunk; ++iChunk )
			{
			rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
			rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
			}
		}
	OSMemoryPageFree( rgbChunk );
	delete [] rgslvchunk;
	delete pfapiDest;
	delete pfapiSrc;
	return err;
	}


//  ================================================================
ERR ErrSLVMove(	PIB * const ppib,
				const IFMP ifmpDb,
				CSLVInfo& slvinfoSrc,
				CSLVInfo& slvinfoDest )
//  ================================================================
//
//	WARNING: this will cancel itself if FOLDSLVContinue is no longer
//	true. Don't call this from outside of OLDSLV for this reason
//
//-
	{
	ERR err = JET_errSuccess;
	
	//  the SLV Provider is enabled

	if ( PinstFromPpib( ppib )->FSLVProviderEnabled() )
		{
		//  copy the SLV using the SLV Provider

		Call( ErrSLVMoveUsingFiles( ppib, ifmpDb, slvinfoSrc, slvinfoDest ) );
		}

	//  the SLV Provider is not enabled

	else
		{
		//  copy the SLV using data

		Call( ErrSLVMoveUsingData( ppib, ifmpDb, slvinfoSrc, slvinfoDest ) );
		}

HandleError:

	if( JET_errSuccess == err && !FOLDSLVContinue( ifmpDb ) )
		{
		err = ErrERRCheck( errOLDSLVMoveStopped );
		}
	return err;
	}


// ErrRECCopySLVsInRecord		- given a record and a copy of that record with
//								  all SLVs removed, this function will copy the
//								  SLVs from the record to the record in the copy
//								  buffer.  the effect of this is to make a new
//								  instance of each SLV for the record in the copy
//								  buffer.  this call is intended for use by
//								  JET_prepInsertCopy
//
// IN:
//		pfucb					- cursor pointing at a record to be copied
//
// RESULT:						ERR

ERR ErrRECCopySLVsInRecord( FUCB *pfucb )
	{
	ERR		err;
	
	//  validate IN args
	
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pfucb->ppib );
	if ( !pfucb->ppib->level )
		{
		CallR( ErrERRCheck( JET_errNotInTransaction ) );
		}
	AssertDIRNoLatch( pfucb->ppib );
	Assert( !pfucb->kdfCurr.data.FNull() );
	Assert( pfcbNil != pfucb->u.pfcb );


	//  perform the copies in a transaction to allow rollback on an error
	
	CallR( ErrDIRBeginTransaction( pfucb->ppib, NO_GRBIT ) );

	//  get the record to copy

	Call( ErrDIRGet( pfucb ) );

	//  copy all SLVs from original record to the record in the copy buffer
	//	(all the SLVs in the copy buffer should already have been removed
	//	by ErrRECAffectLongFieldsInWorkBuf())
	{
	TAGFIELDS	tagfields( pfucb->kdfCurr.data );
	Call( tagfields.ErrCopySLVColumns( pfucb ) );
	}
	
	//  release our latch

	if ( Pcsr( pfucb )->FLatched() )
		{
		CallS( ErrDIRRelease( pfucb ) )
		}
	
	//  commit on success

	Call( ErrDIRCommitTransaction( pfucb->ppib, NO_GRBIT ) );

	return JET_errSuccess;

	//  rollback on an error

HandleError:
	if ( Pcsr( pfucb )->FLatched() )
		{
		CallS( ErrDIRRelease( pfucb ) );
		}

	CallSx( ErrDIRRollback( pfucb->ppib ), JET_errRollbackError );
	AssertDIRNoLatch( pfucb->ppib );	// Guaranteed not to fail while we have a latch.
	return err;
	}


// ErrLGRIRedoSLVPageAppend		- redoes a physical SLV append
//
// IN:
//		ppib					- session
//		plrSLVPageAppend		- log record detailing the physical operation
//
// RESULT:						ERR

ERR LOG::ErrLGRIRedoSLVPageAppend( PIB* ppib, LRSLVPAGEAPPEND* plrSLVPageAppend )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	Assert( plrSLVPageAppend );
	Assert( plrSLVPageAppend->FDataLogged() );

	//  extract parameters from the log record

	INST*			pinst		= PinstFromPpib( ppib );
	DBID			dbid		= DBID( plrSLVPageAppend->dbid & dbidMask );
	IFMP			ifmp		= pinst->m_mpdbidifmp[ dbid ];
	QWORD			ibOffset	= plrSLVPageAppend->le_ibLogical % SLVPAGE_SIZE;

	CSLVInfo::RUN	run;
	run.ibVirtualNext	= SLVPAGE_SIZE;
	run.ibLogical		= plrSLVPageAppend->le_ibLogical - ibOffset;
	run.qwReserved		= 0;
	run.ibVirtual		= 0;
	run.cbSize			= SLVPAGE_SIZE;
	run.ibLogicalNext	= plrSLVPageAppend->le_ibLogical - ibOffset + SLVPAGE_SIZE;

	DATA			dataAppend;
	dataAppend.SetPv( (void*)plrSLVPageAppend->szData );
	dataAppend.SetCb( plrSLVPageAppend->le_cbData );

	//	UNDONE: if we ever support JET_bitSetSLVDataNotRecoverable, we'll need some
	//	smart mechanism to zero-out out-of-date data

	//  write the data to the SLV file

	QWORD cbWritten;
	Call( ErrSLVWriteRun(	pfucbNil,
							ppib,
							ifmp,
							run,
							ibOffset,
							&dataAppend,
							&cbWritten,
							fTrue ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVPrereadOffset(	PIB * const ppib,
								const IFMP ifmpDb,
								const QWORD ibLogical,
								const QWORD	cbPreread
								)
//  ================================================================
	{
	const IFMP	ifmp	= ifmpDb | ifmpSLV;
	
	QWORD 			cbPrereadRemaining	= cbPreread;

	const INT		cpgnoMax		= 16;	//	64K I/O's
	PGNO			rgpgno[cpgnoMax+1];
	INT				ipgno			= 0;
	
	//  determine the pgno to read from

	PGNO		pgno				= PgnoOfOffset( ibLogical );
	
	while( cbPrereadRemaining > 0 )
		{
		const QWORD cbPrereadThisPage	= min( cbPrereadRemaining, g_cbPage );
		
		rgpgno[ipgno++] = pgno;

		if( cpgnoMax == ipgno )
			{
			rgpgno[ipgno] = pgnoNull;
			BFPrereadPageList( ifmp, rgpgno );

			ipgno = 0;
			}
		
		cbPrereadRemaining -= cbPrereadThisPage;			
		++pgno;
		}

	rgpgno[ipgno] = pgnoNull;
	BFPrereadPageList( ifmp, rgpgno );
		
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrSLVChecksumOffset(	PIB * const ppib,
								const IFMP ifmpDb,
								const QWORD ibLogical,
								const QWORD	cb,
								DWORD * const pdwChecksum		
								)
//  ================================================================
	{
	ERR			err				= JET_errSuccess;

	const IFMP	ifmp	= ifmpDb | ifmpSLV;
	
	QWORD 			cbRemaining		= cb;
	
	//  determine the pgno to read from

	PGNO		pgno				= PgnoOfOffset( ibLogical );

	*pdwChecksum = 0;
	
	while( cbRemaining > 0 )
		{
		const QWORD cbThisPage	= min( cbRemaining, g_cbPage );

		BFLatch	bfl;
		Call( ErrBFReadLatchPage( &bfl, ifmp, pgno, bflfNoTouch ) );

		*pdwChecksum = LOG::UlChecksumBytes( (BYTE *)bfl.pv, (BYTE *)bfl.pv+cbThisPage, *pdwChecksum );

		BFReadUnlatch( &bfl );
							
		cbRemaining -= cbThisPage;			
		++pgno;
		}

HandleError:
	return err;
	}


//  SLV Information				- manages an LV containing an SLV scatter list

//  fileidNil

const CSLVInfo::FILEID CSLVInfo::fileidNil = -1;

//  constructor

CSLVInfo::CSLVInfo()
	{
	//  init object

	m_pvCache	= m_rgbSmallCache;
	m_cbCache	= sizeof( m_rgbSmallCache );
	}

//  destructor

CSLVInfo::~CSLVInfo()
	{
	//  we should not be holding resources

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	}

// ErrCreate					- creates SLV Information in the specified NULL
//								  record and field
// IN:
//		pfucb					- cursor
//      columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//
// RESULT:						ERR

ERR CSLVInfo::ErrCreate(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer )
	{
	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	ASSERT_VALID( pfucb );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	
	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= columnid;
	m_itagSequence	= itagSequence;
	m_fCopyBuffer	= fCopyBuffer;

	//  init our SLVInfo to appear to be a zero length SLV

	m_ibOffsetChunkMic			= 0;
	m_ibOffsetChunkMac			= sizeof( HEADER );
	m_ibOffsetRunMic			= sizeof( HEADER );
	m_ibOffsetRunMac			= sizeof( HEADER );
	m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
	m_fCacheDirty				= fFalse;
	m_fHeaderDirty				= fTrue;
	m_header.cbSize				= 0;
	m_header.cRun				= 0;
	m_header.fDataRecoverable	= fFalse;
	m_header.rgbitReserved_31	= 0;
	m_header.rgbitReserved_32	= 0;
	m_fFileNameVolatile			= fFalse;
	m_fileid					= fileidNil;
	m_cbAlloc					= 0;
		
	return JET_errSuccess;
	}

// ErrLoad						- loads SLV Information from the specifed record
//								  and field
//
// IN:
//		pfucb					- cursor
//      columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//		pdataSLVInfo			- optional buffer containing raw SLV Info data
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoad(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer,
	DATA			* pdataSLVInfo,
	const BOOL		fSeparatedSLV )
	{
	ERR				err			= JET_errSuccess;
	DWORD			cbSLVInfo	= 0;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	ASSERT_VALID( pfucb );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );

	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= columnid;
	m_itagSequence	= itagSequence;
	m_fCopyBuffer	= fCopyBuffer;

	//  load the cache with as much data as possible

	if ( pdataSLVInfo )
		{
		//	if retrieving from copy buffer, should not have anything latched
		//	if not retrieving from copy buffer, should have page latched
		Assert( ( fCopyBuffer && !Pcsr( m_pfucb )->FLatched() )
			|| ( !fCopyBuffer && Pcsr( m_pfucb )->FLatched() ) );

		if ( fSeparatedSLV )
			{
			Call( ErrRECIRetrieveSeparatedLongValue(
						m_pfucb,
						*pdataSLVInfo,
						fTrue,
						0,
						(BYTE *)m_pvCache,
						m_cbCache,
						&cbSLVInfo,
						NO_GRBIT ) );
			Assert( !Pcsr( m_pfucb )->FLatched() );
			}
		else
			{
			UtilMemCpy(	m_pvCache,
						pdataSLVInfo->Pv(),
						min( m_cbCache, pdataSLVInfo->Cb() ) );
			cbSLVInfo = pdataSLVInfo->Cb();
			}
		}
	else
		{
		Call( ErrReadSLVInfo( 0, (BYTE*)m_pvCache, m_cbCache, &cbSLVInfo ) );
		}

	//  there is no SLVInfo for this field

	if ( !cbSLVInfo )
		{
		//  init our SLVInfo to appear to be a zero length SLV

		m_ibOffsetChunkMic			= 0;
		m_ibOffsetChunkMac			= sizeof( HEADER );
		m_ibOffsetRunMic			= sizeof( HEADER );
		m_ibOffsetRunMac			= sizeof( HEADER );
		m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty				= fFalse;
		m_fHeaderDirty				= fTrue;
		m_header.cbSize				= 0;
		m_header.cRun				= 0;
		m_header.fDataRecoverable	= fFalse;
		m_header.rgbitReserved_31	= 0;
		m_header.rgbitReserved_32	= 0;
		m_fFileNameVolatile			= fFalse;
		m_fileid					= fileidNil;
		m_cbAlloc					= 0;
		}

	//  there is SLVInfo for this field

	else
		{
		//  retrieve the header from the SLVInfo

		UtilMemCpy( &m_header, m_pvCache, sizeof( HEADER ) );
		
		//  validate SLVInfo header

		if ( cbSLVInfo < sizeof( HEADER ) )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.cbSize && !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.rgbitReserved_31 || m_header.rgbitReserved_32 )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoBasic;
		cbSLVInfoBasic = DWORD( sizeof( HEADER ) + m_header.cRun * sizeof( _RUN ) );
		if ( cbSLVInfo < cbSLVInfoBasic )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoFileName;
		cbSLVInfoFileName = cbSLVInfo - cbSLVInfoBasic;
		if (	cbSLVInfoFileName % sizeof( wchar_t ) ||
				cbSLVInfoFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  init our SLVInfo from the retrieved data
		
		m_ibOffsetChunkMic	= 0;
		m_ibOffsetChunkMac	= min( m_cbCache, cbSLVInfo );
		m_ibOffsetRunMic	= DWORD( cbSLVInfo - m_header.cRun * sizeof( _RUN ) );
		m_ibOffsetRunMac	= cbSLVInfo;
		m_ibOffsetRun		= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty		= fFalse;
		m_fHeaderDirty		= fFalse;
		m_fFileNameVolatile	= fFalse;
		m_fileid			= fileidNil;
		m_cbAlloc			= 0;
		}

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}


// ErrLoadFromData				- loads SLV Information from the specifed record
//								  and field
//
// IN:
//		pfucb					- cursor
//      fid						- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//		pdataSLVInfo			- optional buffer containing raw SLV Info data
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoadFromData(
	FUCB		* const pfucb,
	const DATA&	dataSLVInfo,
	const BOOL	fSeparatedSLV )
	{
	ERR		err			= JET_errSuccess;
	DWORD	cbSLVInfo	= 0;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );

	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= 0;
	m_itagSequence	= 1;
	m_fCopyBuffer	= fFalse;

	//  load the cache with as much data as possible

	if ( fSeparatedSLV )
		{
		
		//	get the size
		
		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataSLVInfo,
					fTrue,
					0,
					NULL,
					0,
					&cbSLVInfo,
					NO_GRBIT ) );

		//	

		VOID * const pvT = PvOSMemoryHeapAlloc( cbSLVInfo );

		if( NULL == pvT )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_pvCache = pvT;
		m_cbCache = cbSLVInfo;

		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataSLVInfo,
					fTrue,
					0,
					(BYTE *)m_pvCache,
					m_cbCache,
					&cbSLVInfo,
					NO_GRBIT ) );
					
		}
	else
		{		
		//  An entire byte too big, but better safe than sorry!
		
		VOID * const pvT = PvOSMemoryHeapAlloc( dataSLVInfo.Cb() );

		if( NULL == pvT )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_pvCache = pvT;
		m_cbCache = dataSLVInfo.Cb();
	
		UtilMemCpy(
			m_pvCache,
			dataSLVInfo.Pv(),
			dataSLVInfo.Cb() );
		cbSLVInfo = dataSLVInfo.Cb();
		}

	//  there is no SLVInfo for this field

	if ( !cbSLVInfo )
		{
		//  init our SLVInfo to appear to be a zero length SLV

		m_ibOffsetChunkMic			= 0;
		m_ibOffsetChunkMac			= sizeof( HEADER );
		m_ibOffsetRunMic			= sizeof( HEADER );
		m_ibOffsetRunMac			= sizeof( HEADER );
		m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty				= fFalse;
		m_fHeaderDirty				= fTrue;
		m_header.cbSize				= 0;
		m_header.cRun				= 0;
		m_header.fDataRecoverable	= fFalse;
		m_header.rgbitReserved_31	= 0;
		m_header.rgbitReserved_32	= 0;
		m_fFileNameVolatile			= fFalse;
		m_fileid					= fileidNil;
		m_cbAlloc					= 0;
		}

	//  there is SLVInfo for this field

	else
		{
		//  retrieve the header from the SLVInfo

		UtilMemCpy( &m_header, m_pvCache, sizeof( HEADER ) );
		
		//  validate SLVInfo header

		if ( cbSLVInfo < sizeof( HEADER ) )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.cbSize && !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.rgbitReserved_31 || m_header.rgbitReserved_32 )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoBasic;
		cbSLVInfoBasic = DWORD( sizeof( HEADER ) + m_header.cRun * sizeof( _RUN ) );
		if ( cbSLVInfo < cbSLVInfoBasic )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoFileName;
		cbSLVInfoFileName = cbSLVInfo - cbSLVInfoBasic;
		if (	cbSLVInfoFileName % sizeof( wchar_t ) ||
				cbSLVInfoFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  init our SLVInfo from the retrieved data
		
		m_ibOffsetChunkMic	= 0;
		m_ibOffsetChunkMac	= min( m_cbCache, cbSLVInfo );
		m_ibOffsetRunMic	= DWORD( cbSLVInfo - m_header.cRun * sizeof( _RUN ) );
		m_ibOffsetRunMac	= cbSLVInfo;
		m_ibOffsetRun		= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty		= fFalse;
		m_fHeaderDirty		= fFalse;
		m_fFileNameVolatile	= fFalse;
		m_fileid			= fileidNil;
		m_cbAlloc					= 0;
		}

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}

// ErrCopy						- loads a duplicate of existing SLV Information
//
// IN:
//		slvinfo					- existing SLV Information
//
// RESULT:						ERR
//
// NOTE:  Double caching of persisted data is possible!  Use with caution!

ERR CSLVInfo::ErrCopy( CSLVInfo& slvinfo )
	{
	ERR		err			= JET_errSuccess;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );

	//  the source has a large cache

	if ( slvinfo.m_pvCache != slvinfo.m_rgbSmallCache )
		{
		//  get a large cache of our own

		if ( !( m_pvCache = PvOSMemoryHeapAlloc( slvinfo.m_cbCache ) ) )
			{
			m_pvCache = m_rgbSmallCache;
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_cbCache = slvinfo.m_cbCache;
		}

	//  copy the state of the source

	m_pfucb				= slvinfo.m_pfucb;
	m_columnid			= slvinfo.m_columnid;
	m_itagSequence		= slvinfo.m_itagSequence;
	m_fCopyBuffer		= slvinfo.m_fCopyBuffer;

	m_ibOffsetChunkMic	= slvinfo.m_ibOffsetChunkMic;
	m_ibOffsetChunkMac	= slvinfo.m_ibOffsetChunkMac;

	m_ibOffsetRunMic	= slvinfo.m_ibOffsetRunMic;
	m_ibOffsetRunMac	= slvinfo.m_ibOffsetRunMac;

	m_ibOffsetRun		= slvinfo.m_ibOffsetRun;

	m_fCacheDirty		= slvinfo.m_fCacheDirty;
	UtilMemCpy( m_pvCache, slvinfo.m_pvCache, slvinfo.m_ibOffsetRunMac );

	m_fHeaderDirty		= slvinfo.m_fHeaderDirty;
	m_header			= slvinfo.m_header;

	m_fFileNameVolatile	= slvinfo.m_fFileNameVolatile;
	if ( slvinfo.m_fFileNameVolatile )
		{
		wcscpy( m_wszFileName, slvinfo.m_wszFileName );
		}

	m_fileid			= slvinfo.m_fileid;
	m_cbAlloc			= slvinfo.m_cbAlloc;

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}

// ErrSave						- saves any changes made
//
// RESULT:						ERR

ERR CSLVInfo::ErrSave()
	{
	ERR err = JET_errSuccess;

	//  save any changes we may have made to the SLVInfo cache

	Call( ErrFlushCache() );

	//  the header is still dirty

	if ( m_fHeaderDirty )
		{
		//  write the header to the SLVInfo

		Call( ErrWriteSLVInfo( 0, (BYTE*)&m_header, sizeof( HEADER ) ) );
		m_fHeaderDirty = fFalse;
		}

HandleError:
	return err;
	}

// ErrCreateVolatile			- creates a container for volatile SLV Information.
//								  this information cannot be saved and any changes
//								  that would cause an overflow of the SLVInfo cache
//								  will return JET_errDiskFull
//
// RESULT:						ERR

ERR CSLVInfo::ErrCreateVolatile()
	{
	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	//  set our currency and field to indicate volatile SLVInfo

	m_pfucb			= pfucbNil;
	m_columnid		= 0;
	m_itagSequence	= 0;
	m_fCopyBuffer	= fFalse;

	//  init our SLVInfo to appear to be a zero length SLV

	m_ibOffsetChunkMic			= 0;
	m_ibOffsetChunkMac			= sizeof( HEADER );
	m_ibOffsetRunMic			= sizeof( HEADER );
	m_ibOffsetRunMac			= sizeof( HEADER );
	m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
	m_fCacheDirty				= fFalse;
	m_fHeaderDirty				= fTrue;
	m_header.cbSize				= 0;
	m_header.cRun				= 0;
	m_header.fDataRecoverable	= fFalse;
	m_header.rgbitReserved_31		= 0;
	m_header.rgbitReserved_32	= 0;
	m_fFileNameVolatile			= fTrue;
	m_wszFileName[0]			= L'\0';
	m_fileid					= fileidNil;
	m_cbAlloc					= 0;
		
	return JET_errSuccess;
	}

// Unload						- unloads all SLV Information throwing away any
//								  changes made
//
// RESULT:						ERR

void CSLVInfo::Unload()
	{
	//  blow cache if allocated

	if ( m_pvCache != m_rgbSmallCache )
		{
		OSMemoryHeapFree( m_pvCache );
		m_pvCache = m_rgbSmallCache;
		m_cbCache = sizeof( m_rgbSmallCache );
		}
	}

// ErrMoveBeforeFirst			- moves the run cursor before the first run in
//								  the scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrMoveBeforeFirst()
	{
	//  reset currency to be before the first run

	m_ibOffsetRun = m_ibOffsetRunMic - sizeof( _RUN );

	return JET_errSuccess;
	}

// ErrMoveNext					- moves the run cursor to the next run in the
//								  scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- there are no subsequent runs in the scatter
//								  list.  the run currency will be after the last
//								  run in the scatter list

ERR CSLVInfo::ErrMoveNext()
	{
	ERR err = JET_errSuccess;

	//  move to the next run

	m_ibOffsetRun += sizeof( _RUN );
	
	//  we are beyond the last run

	if ( m_ibOffsetRun == m_ibOffsetRunMac )
		{
		//  keep currency at the after last position

		m_ibOffsetRun = m_ibOffsetRunMac;
		
		//  return no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

HandleError:
	return err;
	}

// ErrMovePrev					- moves the run cursor to the previous run in the
//								  scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- there are no previous runs in the scatter list.
//								  the run currency will be before the first run in
//								  the scatter list

ERR CSLVInfo::ErrMovePrev()
	{
	ERR err = JET_errSuccess;

	//  move to the next run

	m_ibOffsetRun -= sizeof( _RUN );
	
	//  we are beyond the last run

	if ( m_ibOffsetRun == m_ibOffsetRunMic - sizeof( _RUN ) )
		{
		//  keep currency at the before first position

		m_ibOffsetRun = m_ibOffsetRunMic - sizeof( _RUN );
		
		//  return no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

HandleError:
	return err;
	}

// ErrMoveAfterLast				- moves the run cursor after the last run in
//								  the scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrMoveAfterLast()
	{
	//  reset currency to be after the last run

	m_ibOffsetRun = m_ibOffsetRunMac;

	return JET_errSuccess;
	}

// ErrSeek						- moves the run cursor to the run containing the
//								  specified SLV offset.  the specified offset is
//								  only valid if it is smaller than the SLV size
//								  as indicated by the current header information
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- this SLV offset is not contained by this SLV.
//								  the run currency will be after the last run in
//								  the scatter list

ERR CSLVInfo::ErrSeek( QWORD ibVirtual )
	{
	ERR err = JET_errSuccess;

	//  the specified offset is beyond the end of the valid data for the SLV or
	//  we have no runs

	if ( ibVirtual >= m_header.cbSize || !m_header.cRun )
		{
		//  move to after last and return no current record

		CallS( ErrMoveAfterLast() );
		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

	//  we are being asked to seek to the beginning of the SLV (a common case)

	if ( !ibVirtual )
		{
		//  move to the first run

		CallS( ErrMoveBeforeFirst() );
		Call( ErrMoveNext() );
		}

	//  we are not being asked to seek to the beginning of the SLV

	else
		{
		//  determine what runs are currently cached

		Assert( m_ibOffsetChunkMic <= m_ibOffsetChunkMac );
		Assert( m_ibOffsetRunMic <= m_ibOffsetChunkMac );

		QWORD ibOffsetMic;
		QWORD ibOffsetMac;

		ibOffsetMic = max( m_ibOffsetChunkMic, m_ibOffsetRunMic );
		ibOffsetMac = max( m_ibOffsetChunkMac, m_ibOffsetRunMic );

		ibOffsetMic = min( ibOffsetMic, m_ibOffsetRunMac );
		ibOffsetMac = min( ibOffsetMac, m_ibOffsetRunMac );

		ibOffsetMic = ibOffsetMic + sizeof( _RUN ) - 1;
		ibOffsetMic = ibOffsetMic - ( ibOffsetMic - m_ibOffsetRunMic ) % sizeof( _RUN );
		ibOffsetMac = ibOffsetMac - ( ibOffsetMac - m_ibOffsetRunMic ) % sizeof( _RUN );

		//  determine the limits for the binary search based on what we see in the
		//  cached runs, starting with all the runs.  we go through all this pain
		//  and suffering because loading the cache is VERY expensive and seeks
		//  usually are somewhat near each other with respect to the cache size giving
		//  a high probability that the run we want is already cached

		QWORD ibOffsetFirst;
		QWORD ibOffsetLast;

		ibOffsetFirst	= m_ibOffsetRunMic;
		ibOffsetLast	= m_ibOffsetRunMac - sizeof( _RUN );

		if ( ibOffsetMic + sizeof( _RUN ) <= ibOffsetMac )
			{
			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMic - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			if ( run.ibVirtualNext < ibVirtual )
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMic + sizeof( _RUN ) );
				}
			else if ( run.ibVirtualNext > ibVirtual )
				{
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMic );
				}
			else
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMic + sizeof( _RUN ) );
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMic + sizeof( _RUN ) );
				}
			}

		if ( ibOffsetMic < ibOffsetMac - sizeof( _RUN ) )
			{
			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMac - sizeof( _RUN ) - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			if ( run.ibVirtualNext < ibVirtual )
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMac );
				}
			else if ( run.ibVirtualNext > ibVirtual )
				{
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMac - sizeof( _RUN ) );
				}
			else
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMac );
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMac );
				}
			}

		Assert( ibOffsetFirst <= ibOffsetLast );

		//  binary search the remaining range of the scatter list for the run whose
		//  ibVirtualLast is greater than or equal to the one we desire

		while ( ibOffsetFirst < ibOffsetLast )
			{
			//  compute a new midpoint

			QWORD ibOffsetMid;
			ibOffsetMid = ( ibOffsetFirst + ibOffsetLast ) / 2;
			ibOffsetMid = ibOffsetMid - ( ibOffsetMid - m_ibOffsetRunMic ) % sizeof( _RUN );

			//  load the new data into the SLVInfo cache

			Assert( ibOffsetMid == ULONG( ibOffsetMid ) );
			Call( ErrLoadCache( ULONG( ibOffsetMid ), sizeof( _RUN ) ) );

			//  copy the data from the SLVInfo cache

			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMid - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			//  the midpoint is less than or equal to our target.  look in the top half

			if ( run.ibVirtualNext <= ibVirtual )
				{
				ibOffsetFirst = ibOffsetMid + sizeof( _RUN );
				}

			//  the midpoint is greater than our target.  look in the bottom half

			else
				{
				ibOffsetLast = ibOffsetMid;
				}
			}

		//  set the current run to the located run

		Assert( ibOffsetFirst == ULONG( ibOffsetFirst ) );
		m_ibOffsetRun = ULONG( ibOffsetFirst );
		}

#ifdef DEBUG

	//  verify that the run we found is the correct run
	{
	CSLVInfo::RUN run;
	Call( ErrGetCurrentRun( &run ) );

	Assert( run.ibVirtual <= ibVirtual );
	Assert( ibVirtual < run.ibVirtualNext );
	}
#endif  //  DEBUG

HandleError:
	return err;
	}

// ErrGetHeader					- retrieves the header of the SLV scatter list
//
// IN:
//		pheader					- buffer to receive the header
//
// RESULT:						ERR
//
// OUT:	
//		pheader					- header of the SLV scatter list

ERR CSLVInfo::ErrGetHeader( HEADER* pheader )
	{
	//  return a copy of the header

	UtilMemCpy( pheader, &m_header, sizeof( HEADER ) );

	return JET_errSuccess;
	}

// ErrSetHeader					- sets the header of the SLV scatter list
//
// IN:
//		header					- the new header for the SLV scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrSetHeader( HEADER& header )
	{
	//  modify our copy of the header

	UtilMemCpy( &m_header, &header, sizeof( HEADER ) );
	m_fHeaderDirty = fTrue;

	return JET_errSuccess;
	}

// ErrGetFileID					- retrieves the file ID of the SLV scatter list
//
// IN:
//		pfileid					- buffer to receive the file ID
//
// RESULT:						ERR
//
// OUT:	
//		pfileid					- file ID of the SLV scatter list

ERR CSLVInfo::ErrGetFileID( FILEID* pfileid )
	{
	ERR err = JET_errSuccess;

	//  the file ID is currently unknown

	if ( m_fileid == fileidNil )
		{
		//  there had better be at least one run in this SLV Info

		if ( !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}
			
		//  compute the file ID from the ibLogical of the first run

		Call( ErrLoadCache( m_ibOffsetRunMic, sizeof( _RUN ) ) );

		_RUN run;
		UtilMemCpy(	&run,
					(BYTE*)m_pvCache + m_ibOffsetRunMic - m_ibOffsetChunkMic,
					sizeof( _RUN ) );

		m_fileid = run.ibLogical;
		}

	//  return the file ID

	*pfileid = m_fileid;

HandleError:
	return err;
	}

// ErrSetFileID					- sets the file ID of the SLV scatter list
//
// IN:
//		fileid					- the new file ID for the SLV scatter list
//
// RESULT:						ERR
//

ERR CSLVInfo::ErrSetFileID( FILEID& fileid )
	{
	//  set the file ID

	m_fileid = fileid;

	return JET_errSuccess;
	}

// ErrGetFileAlloc				- retrieves the file allocation size of the
//								  SLV scatter list
//
// IN:
//		pcbAlloc				- buffer to receive the file allocation size
//
// RESULT:						ERR
//
// OUT:	
//		pcbAlloc				- file allocation size of the SLV scatter list

ERR CSLVInfo::ErrGetFileAlloc( QWORD* pcbAlloc )
	{
	ERR err = JET_errSuccess;

	//  the file allocation size is currently unknown

	if ( m_cbAlloc == 0 )
		{
		//  there had better be at least one run in this SLV Info

		if ( !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}
			
		//  compute the file allocation size from the ibVirtualNext of the last
		//  run

		Call( ErrLoadCache( m_ibOffsetRunMac - sizeof( _RUN ), sizeof( _RUN ) ) );

		_RUN run;
		UtilMemCpy(	&run,
					(BYTE*)m_pvCache + m_ibOffsetRunMac - sizeof( _RUN ) - m_ibOffsetChunkMic,
					sizeof( _RUN ) );

		m_cbAlloc = run.ibVirtualNext;
		}

	//  return the file allocation size

	*pcbAlloc = m_cbAlloc;

HandleError:
	return err;
	}

// ErrSetFileAlloc				- sets the file allocation size of the SLV
//								  scatter list
//
// IN:
//		cbAlloc					- the new file allocation size for the SLV
//								  scatter list
//
// RESULT:						ERR
//

ERR CSLVInfo::ErrSetFileAlloc( QWORD& cbAlloc )
	{
	//  set the file allocation size

	m_cbAlloc = cbAlloc;

	return JET_errSuccess;
	}

// ErrGetFileName				- retrieves the file name of the SLV scatter list
//
// IN:
//		wszFileName				- buffer to receive the file name (must be at least
//								  IFileSystemAPI::cchPathMax wchar_t's in size)
//
// RESULT:						ERR
//
// OUT:	
//		wszFileName				- file name of the SLV scatter list

ERR CSLVInfo::ErrGetFileName( wchar_t* wszFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	Assert( wszFileName );
	
	//  the file name is volatile

	if ( m_fFileNameVolatile )
		{
		//  copy the file name from the volatile file name cache

		wcscpy( wszFileName, m_wszFileName );
		}

	//  the file name is not volatile

	else
		{
		//  determine offset and size of file name in the SLV Info.  the file
		//  name is currently the entire extent of the SLV Info between the
		//  header and the runs

		QWORD ibFileName	= sizeof( HEADER );
		DWORD cbFileName	= m_ibOffsetRunMic - sizeof( HEADER );

		//  there is a stored file name

		if ( cbFileName > 0 )
			{
			//  load the new data into the SLVInfo cache

			Assert( ibFileName == ULONG( ibFileName ) );
			Call( ErrLoadCache( ULONG( ibFileName ), cbFileName ) );

			//  copy the data from the SLVInfo cache into the user buffer

			UtilMemCpy(	wszFileName,
						(BYTE*)m_pvCache + ibFileName - m_ibOffsetChunkMic,
						cbFileName );
			}

		//  null terminate the string (even if no data was copied)
		
		memset( (BYTE*)wszFileName + cbFileName, 0, sizeof( wchar_t ) );
		}

	//  there is no stored file name

	if ( wszFileName[0] == L'\0' )
		{
		//  name the SLV by its file ID

		FILEID fileid;
		Call( ErrGetFileID( &fileid ) );
		swprintf( wszFileName, L"$SLV%016I64X$", fileid );
		}

HandleError:
	return err;
	}

// ErrGetFileNameLength			- retrieves the length of the file name of the SLV
//								  scatter list
//
// IN:
//		pcwchFileName			- buffer to receive the length of the file name
//
// RESULT:						ERR
//
// OUT:	
//		pcwchFileName			- length of the file name of the SLV scatter list

ERR CSLVInfo::ErrGetFileNameLength( size_t* const pcwchFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	Assert( pcwchFileName );

	//  the file name is volatile

	if ( m_fFileNameVolatile )
		{
		//  return the length of the cached volatile file name

		*pcwchFileName = wcslen( m_wszFileName );
		}

	//  the file name is not volatile

	else
		{
		//  get the current length of the file name

		DWORD cbFileName = m_ibOffsetRunMic - sizeof( HEADER );

		//  validate SLV Info

		if (	( cbFileName % 2 ) != 0 ||
				cbFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  return the length of the file name

		*pcwchFileName = cbFileName / sizeof( wchar_t );
		}

	//  there is no stored file name

	if ( *pcwchFileName == 0 )
		{
		//  we will return a computed file name whose length is always 21 chars

		*pcwchFileName = 21;
		}

HandleError:
	return err;
	}

// ErrSetFileNameVolatile		- marks the file name of the SLV scatter list
//								  as volatile.  the file name will not be
//								  persisted but can still be manipulated
//								  normally
//
// RESULT:						ERR
//
// NOTES:
//
//   Currently, the file name must be marked as volatile before it is set
//   for the first time.  you will receive JET_errSLVCorrupted if you do not
//   observe this limitation

ERR CSLVInfo::ErrSetFileNameVolatile()
	{
	ERR err = JET_errSuccess;

	//  we currently do not support marking the file name as volatile after
	//  it has been set

	if ( m_ibOffsetRunMic - sizeof( HEADER ) > 0 )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	//  mark the file name as volatile

	m_fFileNameVolatile = fTrue;

	//  initialize the file name to be an empty string

	m_wszFileName[0] = L'\0';

HandleError:
	return err;
	}

// ErrSetFileName				- sets the file name of the SLV scatter list
//
// IN:
//		wszFileName				- the new file name for the SLV scatter list
//
// RESULT:						ERR
//
//		JET_errSLVCorrupted		- an attempt was made to set the file name of a
//								  scatter list that already has a file name or
//								  already has runs
//
// NOTES:
//
//   Currently, the file name length can only be changed if there are no runs in
//   the scatter list.  you will receive JET_errSLVCorrupted if you attempt to do
//   this.

ERR CSLVInfo::ErrSetFileName( const wchar_t* wszFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	Assert( wszFileName );
	Assert( wcslen( wszFileName ) );
	Assert( wcslen( wszFileName ) < IFileSystemAPI::cchPathMax );

	//  the file name is volatile

	if ( m_fFileNameVolatile )
		{
		//  copy the file name into the volatile file name cache

		wcscpy( m_wszFileName, wszFileName );
		}

	//  the file name is not volatile

	else
		{
		//  get the offset and size of the file name to insert

		QWORD ibFileName;
		DWORD cbFileName;
		ibFileName = sizeof( HEADER );
		cbFileName = (ULONG)wcslen( wszFileName ) * sizeof( wchar_t );

		//  we currently do not support changing the file name length if the SLV Info
		//  already has runs in its scatter list as it would require moving the rest
		//  of the scatter list in the LV

		if (	cbFileName != m_ibOffsetRunMic - sizeof( HEADER ) &&
				m_ibOffsetRunMic != m_ibOffsetRunMac )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  we also do not support shrinking the file name once it has been saved

		if (	cbFileName < m_ibOffsetRunMic - sizeof( HEADER ) &&
				!m_fCacheDirty )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  load the new data into the SLVInfo cache

		Assert( ibFileName == ULONG( ibFileName ) );
		Call( ErrLoadCache( ULONG( ibFileName ), cbFileName ) );

		//  ensure that the chunk and run limits account for the filename (simple append)

		Assert( ibFileName + cbFileName == ULONG( ibFileName + cbFileName ) );
		
		m_ibOffsetChunkMac	= max( m_ibOffsetChunkMac, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRunMic	= max( m_ibOffsetRunMic, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRunMac	= max( m_ibOffsetRunMac, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRun		= ULONG( ibFileName + cbFileName );

		//  modify the file name with the user supplied data
		
		UtilMemCpy(	(BYTE*)m_pvCache + ibFileName - m_ibOffsetChunkMic,
					(BYTE*)wszFileName,
					cbFileName );
		m_fCacheDirty = fTrue;
		}

HandleError:
	return err;
	}

// ErrGetCurrentRun				- retrieves the current run in the SLV scatter
//
// IN:
//		prun					- buffer to receive the run
//
// RESULT:						ERR
//
// OUT:	
//		prun					- run from the SLV scatter list

ERR CSLVInfo::ErrGetCurrentRun( RUN* prun )
	{
	ERR err = JET_errSuccess;
	_RUN rgrun[2];

	//  we are not currently on a run

	if ( m_ibOffsetRun < m_ibOffsetRunMic || m_ibOffsetRun >= m_ibOffsetRunMac )
		{
		//  fail with no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

	//  compute the amount of data to read.  normally, we need to read two runs,
	//  the run preceding the current run for its ibVirtualNext (to become the
	//  current run's ibVirtual) and the current run.  we do not need to read
	//  the preceding run if this is the first run

	BYTE* pbRead;
	DWORD cbRead;
	DWORD ibOffsetRead;

	if ( m_ibOffsetRun == m_ibOffsetRunMic )
		{
		rgrun[0].ibVirtualNext	= 0;
		rgrun[0].ibLogical		= -1;
		rgrun[0].qwReserved		= 0;
		
		pbRead			= (BYTE*)&rgrun[1];
		cbRead			= sizeof( rgrun[1] );
		ibOffsetRead	= m_ibOffsetRun;
		}
	else
		{
		pbRead			= (BYTE*)rgrun;
		cbRead			= sizeof( rgrun );
		ibOffsetRead	= m_ibOffsetRun - sizeof( _RUN );
		}

	//  load the new data into the SLVInfo cache

	Call( ErrLoadCache( ibOffsetRead, cbRead ) );

	//  copy the data from the SLVInfo cache

	UtilMemCpy(	pbRead,
				(BYTE*)m_pvCache + ibOffsetRead - m_ibOffsetChunkMic,
				cbRead );

	//  move the read data into the user buffer

	prun->ibVirtualNext	= rgrun[1].ibVirtualNext;
	prun->ibLogical		= rgrun[1].ibLogical;
	prun->qwReserved	= rgrun[1].qwReserved;
	prun->ibVirtual		= rgrun[0].ibVirtualNext;
	prun->cbSize		= prun->ibVirtualNext - prun->ibVirtual;
	prun->ibLogicalNext	= prun->ibLogical + prun->cbSize;

	//  validate SLV run

	if (	prun->ibVirtualNext % SLVPAGE_SIZE ||
			prun->ibLogical % SLVPAGE_SIZE ||
			prun->cbSize % SLVPAGE_SIZE )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	if (	m_pfucb != pfucbNil &&
			prun->ibLogicalNext > rgfmp[ m_pfucb->ifmp ].CbTrueSLVFileSize() )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	if ( prun->qwReserved )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

HandleError:
	return err;
	}

// ErrSetCurrentRun				- sets the current run in the SLV scatter list.
//								  new runs may be appended to the scatter list
//								  by setting the current run when we are after
//								  the last run
//
// IN:
//		run						- the new run for the SLV scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- the current run is before the first run

ERR CSLVInfo::ErrSetCurrentRun( RUN& run )
	{
	ERR err = JET_errSuccess;

	//  if we are before the first run, return no current record

	if ( m_ibOffsetRun < m_ibOffsetRunMic )
		{
		Call( ErrERRCheck( JET_errNoCurrentRecord ) )
		}

	//  load the new data into the SLVInfo cache

	Call( ErrLoadCache( m_ibOffsetRun, sizeof( _RUN ) ) );

	//  ensure that the chunk and run limits include this run (for simple append)

	m_ibOffsetChunkMac	= max( m_ibOffsetChunkMac, m_ibOffsetRun + sizeof( _RUN ) );
	m_ibOffsetRunMac	= max( m_ibOffsetRunMac, m_ibOffsetRun + sizeof( _RUN ) );

	//  modify this run with the user supplied data
	
	UtilMemCpy(	(BYTE*)m_pvCache + m_ibOffsetRun - m_ibOffsetChunkMic,
				(BYTE*)(_RUN*)&run,
				sizeof( _RUN ) );
	m_fCacheDirty = fTrue;

HandleError:
	return err;
	}

// ErrReadSLVInfo				- reads raw LV data from the SLV Information
//
// IN:
//		ibOffsetRead			- LV offset for the range to be retrieved
//		pbRead					- buffer to receive the data
//		cbRead					- size of the range to be retrieved
//		pcbActual				- buffer to receive the actual bytes read
//
// RESULT:						ERR
//
// OUT:	
//		pbRead					- retrieved data
//		pcbActual				- actual bytes beyond the given offset

ERR CSLVInfo::ErrReadSLVInfo(	ULONG ibOffsetRead,
								BYTE* pbRead,
								ULONG cbRead,
								DWORD* pcbActual )
	{
	ERR			err;
	BOOL		fNeedToReleaseLatch	= fFalse;

	//  we should never be here if this is volatile SLVInfo

	Assert( m_pfucb != pfucbNil );

	Assert( m_pfucb->ppib->level > 0 );
	const BOOL	fUseCopyBuffer		= ( ( m_fCopyBuffer && FFUCBUpdatePrepared( m_pfucb ) && !FFUCBNeverRetrieveCopy( m_pfucb ) )
										|| FFUCBAlwaysRetrieveCopy( m_pfucb ) );
	const DATA	* pdataRec;
	DATA		dataRetrieved;

	if ( fUseCopyBuffer )
		{
		pdataRec = &m_pfucb->dataWorkBuf;
		}
	else
		{
		if ( !Pcsr( m_pfucb )->FLatched() )
			{
			Call( ErrDIRGet( m_pfucb ) );
			fNeedToReleaseLatch = fTrue;
			}
		pdataRec = &m_pfucb->kdfCurr.data;
		}

	Assert( FCOLUMNIDTagged( m_columnid ) );
	Assert( 0 != m_itagSequence );
	Assert( pfcbNil != m_pfucb->u.pfcb );
	Assert( ptdbNil != m_pfucb->u.pfcb->Ptdb() );

	Call( ErrRECIRetrieveTaggedColumn(
			m_pfucb->u.pfcb,
			m_columnid,
			m_itagSequence,
			*pdataRec,
			&dataRetrieved,
			grbitRetrieveColumnReadSLVInfo | grbitRetrieveColumnDDLNotLocked | JET_bitRetrieveIgnoreDefault ) );
	if ( wrnRECIntrinsicSLV == err )
		{
		if ( ibOffsetRead >= dataRetrieved.Cb() )
			dataRetrieved.SetCb( 0 );
		else
			{
			dataRetrieved.DeltaPv( ibOffsetRead );
			dataRetrieved.DeltaCb( -ibOffsetRead );
			}

		*pcbActual = dataRetrieved.Cb();

		ULONG	cbCopy;
		if ( dataRetrieved.Cb() <= cbRead )
			{
			cbCopy = dataRetrieved.Cb();
			err = JET_errSuccess;
			}
		else
			{
			cbCopy = cbRead;
			err = ErrERRCheck( JET_wrnBufferTruncated );
			}

		if ( cbCopy > 0 )
			UtilMemCpy( pbRead, dataRetrieved.Pv(), cbCopy );
		}
	else if ( wrnRECSeparatedSLV == err )
		{
		//  If we are retrieving an after-image or
		//	haven't replaced a LV we can simply go
		//	to the LV tree. Otherwise we have to
		//	perform a more detailed consultation of
		//	the version store with ErrRECGetLVImage
		const BOOL fAfterImage = fUseCopyBuffer
									|| !FFUCBUpdateSeparateLV( m_pfucb )
									|| !FFUCBReplacePrepared( m_pfucb );
		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataRetrieved,
					fAfterImage,
					ibOffsetRead,
					pbRead,
					cbRead,
					pcbActual,
					NO_GRBIT ) );
		}
	else
		{
		Assert( fFalse );
		err = ErrERRCheck( JET_errSLVCorrupted );
		}

HandleError:
	if ( fNeedToReleaseLatch )
		{
		CallS( ErrDIRRelease( m_pfucb ) );
		}
	return err;
	}

// ErrWriteSLVInfo				- writes raw LV data to the SLV Information
//
// IN:
//		ibOffsetWrite			- LV offset for the range to be written
//		pbWrite					- buffer containing the data to write
//		cbWrite					- size of the range to be written
//
// RESULT:						ERR

ERR CSLVInfo::ErrWriteSLVInfo( ULONG ibOffsetWrite, BYTE* pbWrite, ULONG cbWrite )
	{
	ERR		err			= JET_errSuccess;
	DATA	dataWrite;

	//  if this is volatile SLVInfo, bail with JET_errDiskFull

	if ( m_pfucb == pfucbNil )
		{
		Call( ErrERRCheck( JET_errDiskFull ) );
		}

	//  setup to write the given data to the SLVInfo

	dataWrite.SetPv( pbWrite );
	dataWrite.SetCb( cbWrite );

	//  if we have a latch at this point, we must release it

	if ( m_pfucb->csr.FLatched() )
		{
		CallS( ErrDIRRelease( m_pfucb ) );
		}
			
	//  try to update the SLVInfo with the given data
	
	err = ErrRECSetLongField(	m_pfucb,
								m_columnid,
								m_itagSequence,
								&dataWrite,
								JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV,
									ibOffsetWrite,
								0 );

	//  this update would result in a record that is too big to fit on the page

	if ( err == JET_errRecordTooBig )
		{
		//  attempt to separate any LVs in the record to get more space
		
		Call( ErrRECAffectLongFieldsInWorkBuf( m_pfucb, lvaffectSeparateAll ) );

		//  try once more to update the SLVInfo with the given data.  if it fails
		//  this time because the record is too big, there's nothing we can do
		
		Call( ErrRECSetLongField(	m_pfucb,
									m_columnid,
									m_itagSequence,
									&dataWrite,
									JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV,
									ibOffsetWrite,
									0 ) );
		}

HandleError:
	Assert( err != JET_errColumnNoChunk );
	return err;
	}

// ErrLoadCache					- caches SLV Information from the LV containing
//								  at least the specified offset range.  if there
//								  is no SLVInfo for part of the specified byte
//								  range, sufficient buffer will be reserved to
//								  place new information in that sub-range
//
// IN:
//		ibOffsetRequired		- LV offset for the range to be cached
//		cbRequired				- size of the range to be cached
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoadCache( ULONG ibOffsetRequired, ULONG cbRequired )
	{
	ERR err = JET_errSuccess;

	//  this is the largest cache size we will ever use

	const DWORD cbCacheMax = g_cbPage;

	//  validate IN args

	Assert( cbRequired <= cbCacheMax );

	//  requested range is not already entirely cached

	if (	ibOffsetRequired < m_ibOffsetChunkMic ||
			ibOffsetRequired + cbRequired > m_ibOffsetChunkMac )
		{
		//  we cannot grow the SLVInfo cache's size for a simple append

		if (	ibOffsetRequired < m_ibOffsetChunkMic ||
				ibOffsetRequired > m_ibOffsetChunkMac ||
				ibOffsetRequired + cbRequired > m_ibOffsetChunkMic + cbCacheMax ||
				m_ibOffsetChunkMac < m_ibOffsetRunMac )
			{
			//  save any changes we may have made to the local cache

			Call( ErrFlushCache() );

			//  if we are still using the small cache, grow to the large cache

			if ( m_pvCache == m_rgbSmallCache )
				{
				if ( !( m_pvCache = PvOSMemoryHeapAlloc( cbCacheMax ) ) )
					{
					m_pvCache = m_rgbSmallCache;
					Call( ErrERRCheck( JET_errOutOfMemory ) );
					}
				m_cbCache = cbCacheMax;
				}

			//  choose to load the cache with either all data or as much data as
			//  possible including the current run biased for our traversal, depending
			//  on the current SLVInfo size

			DWORD ibOffsetChunk;
			if ( ibOffsetRequired + cbRequired < m_cbCache )
				{
				ibOffsetChunk = 0;
				}
			else
				{
				if ( ibOffsetRequired < m_ibOffsetChunkMic )
					{
					ibOffsetChunk = ibOffsetRequired + cbRequired - m_cbCache;
					}
				else
					{
					ibOffsetChunk = ibOffsetRequired;
					}
				}

			//  invalidate the current chunk in preparation for reading a new chunk

			m_ibOffsetChunkMic = m_ibOffsetChunkMac;
			
			//  load the cache with the decided data

			DWORD cbActual;
			Call( ErrReadSLVInfo( ibOffsetChunk, (BYTE*)m_pvCache, m_cbCache, &cbActual ) );

			//  reset the current chunk offsets to represent the cached data

			m_ibOffsetChunkMic	= ibOffsetChunk;
			m_ibOffsetChunkMac	= ibOffsetChunk + min( m_cbCache, cbActual );
			}

		//  we can grow the SLVInfo cache's size for a simple append but we are
		//  currently using the small cache

		else if ( ibOffsetRequired + cbRequired > m_ibOffsetChunkMic + m_cbCache )
			{
			//  grow to the large cache, copying the old data over

			Assert( m_pvCache == m_rgbSmallCache );
			
			if ( !( m_pvCache = PvOSMemoryHeapAlloc( cbCacheMax ) ) )
				{
				m_pvCache = m_rgbSmallCache;
				Call( ErrERRCheck( JET_errOutOfMemory ) );
				}
			m_cbCache = cbCacheMax;

			UtilMemCpy( m_pvCache, m_rgbSmallCache, sizeof( m_rgbSmallCache ) );
			}
		}

	//  we had better have cached or left room to create the byte range that was
	//  requested

	Assert( m_ibOffsetChunkMic <= ibOffsetRequired );
	Assert( ibOffsetRequired + cbRequired <= m_ibOffsetChunkMac ||
			m_ibOffsetChunkMac == m_ibOffsetRunMac );
	Assert( ibOffsetRequired + cbRequired <= m_ibOffsetChunkMic + m_cbCache );

	return JET_errSuccess;

HandleError:
	return err;
	}

// ErrFlushCache				- saves any changes made to any SLV Information
//								  from the LV that is currently cached
//
// RESULT:						ERR

ERR CSLVInfo::ErrFlushCache()
	{
	ERR err = JET_errSuccess;

	//  we have a SLVInfo cache and it is dirty

	if ( m_pvCache && m_fCacheDirty )
		{
		//  the header can fit in the current chunk

		if ( !m_ibOffsetChunkMic )
			{
			//  if the cache is dirty, it should include the space for the
			//  header already

			Assert( m_ibOffsetChunkMac >= sizeof( HEADER ) );
			
			//  copy the header into the cache

			UtilMemCpy( m_pvCache, &m_header, sizeof( HEADER ) );
			m_fHeaderDirty = fFalse;
			}

		//  write out the information in the local cache

		Call( ErrWriteSLVInfo(	m_ibOffsetChunkMic,
								(BYTE*)m_pvCache,
								m_ibOffsetChunkMac - m_ibOffsetChunkMic ) );
		m_fCacheDirty = fFalse;
		}

HandleError:
	return err;
	}


// UNDONE SLVOWNERMAP: duplicate code (except SetTypeSLVOwnerMap)  !
//  ================================================================
ERR ErrSLVFCBOwnerMap( PIB *ppib, const IFMP ifmp, const PGNO pgno, FCB **ppfcb )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	FUCB	*pfucb 		= pfucbNil;
	FCB		*pfcb 		= pfcbNil;
	
	Assert( ppibNil != ppib );

	CallR( ErrDIROpen( ppib, pgno, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( !FFUCBVersioned( pfucb ) );	// Verify won't be deferred closed.
	pfcb = pfucb->u.pfcb;
	
	Assert( pfcb->Ifmp() == ifmp );
	Assert( pfcb->PgnoFDP() == pgno );
	Assert( pfcb->Ptdb() == ptdbNil );
	Assert( pfcb->CbDensityFree() == 0 );
	
	Assert( pfcb->FTypeNull() );
	pfcb->SetTypeSLVOwnerMap();

	Assert( pfcb->PfcbTable() == pfcbNil );

	//	finish the initialization of this SLV FCB

	pfcb->CreateComplete();

	DIRClose( pfucb );
	*ppfcb = pfcb;
	return err;
	}


ERR ErrSLVOwnerMapInit( PIB *ppib, const IFMP ifmp, PGNO pgnoSLVOwnerMap )
	{
	ERR			err 				= JET_errSuccess;
	FMP			*pfmp				= rgfmp + ifmp;
	INST		*pinst 				= PinstFromIfmp( ifmp );
	FUCB		*pfucbSLVOwnerMap	= pfucbNil;
	FCB			*pfcbSLVOwnerMap	= pfcbNil;
	const BOOL	fTempDb				= ( dbidTemp == pfmp->Dbid() );

	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	// or we are during recovery in CreateDB when pgnoSLVOwnerMap is provided
	Assert( !pinst->FRecovering()
			|| pinst->m_plog->m_fHardRestore ||
			pgnoNull != pgnoSLVOwnerMap );

	if ( pgnoNull == pgnoSLVOwnerMap )
		{
		if ( fTempDb )
			{
			pgnoSLVOwnerMap = pgnoTempDbSLVOwnerMap;
			}
		else
			{		
			OBJID objid;
			Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objid ) );
			}
		}
	else
		{
		Assert( pfmp->FCreatingDB() || fGlobalRepair );
		Assert ( !fTempDb || pgnoSLVOwnerMap == pgnoTempDbSLVOwnerMap );
		}

	Assert( pgnoNull != pgnoSLVOwnerMap );

	Call( ErrSLVFCBOwnerMap( ppib, ifmp, pgnoSLVOwnerMap, &pfcbSLVOwnerMap ) );
	Assert( pfcbNil != pfcbSLVOwnerMap );
	Assert ( pfcbSLVOwnerMap->FTypeSLVOwnerMap() );
	
	pfmp->SetPfcbSLVOwnerMap( pfcbSLVOwnerMap );
	
	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	SLVOwnerMapTerm( ifmp, fFalse );
	return err;
	}


VOID SLVOwnerMapTerm( const IFMP ifmp, const BOOL fTerminating )
	{
	FMP	* const pfmp				= rgfmp + ifmp;
	Assert( pfmp );

	FCB	* const pfcbSLVOwnerMap 	= pfmp->PfcbSLVOwnerMap();

	if ( pfcbNil != pfcbSLVOwnerMap )
		{
		//	synchronously purge the FCB
		Assert( pfcbSLVOwnerMap->FTypeSLVOwnerMap() );
		pfcbSLVOwnerMap->PrepareForPurge();

		//	there should only be stray cursors if we're in the middle of terminating,
		//	in all other cases, all cursors should have been properly closed first
		Assert( 0 == pfcbSLVOwnerMap->WRefCount() || fTerminating );
		if ( fTerminating )
			pfcbSLVOwnerMap->CloseAllCursors( fTrue );

		pfcbSLVOwnerMap->Purge();
		pfmp->SetPfcbSLVOwnerMap( pfcbNil );
		}
	}


//	get the count of all space in the streaming file (owned and available)

ERR ErrSLVGetSpaceInformation( 
	PIB		*ppib,
	IFMP	ifmp,
	CPG		*pcpgOwned,
	CPG		*pcpgAvail )
	{
	ERR		err					= JET_errSuccess;
	FCB		*pfcb				= pfcbNil;
	FUCB	*pfucb				= pfucbNil;
	CPG		cpgAvail			= 0;
	CPG		cpgTotal			= 0;
	BOOL	fInTxn				= fFalse;

	//	make sure a streaming file is present

	if ( !rgfmp[ifmp].Pdbfilehdr()->FSLVExists() )
		{
		Call( ErrERRCheck( JET_errSLVStreamingFileNotCreated ) );
		}

	//	open the SLV avail tree

	pfcb = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcb );
	
	Call( ErrBTOpen( ppib, pfcb, &pfucb, fFalse ) );
	Assert( pfucbNil != pfucb );

	//	start a transaction

	Call( ErrDIRBeginTransaction( ppib, JET_bitTransactionReadOnly ) );
	fInTxn = fTrue;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;

	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	err = ErrBTDown( pfucb, &dib, latchReadTouch );
	if ( JET_errNoCurrentRecord == err )
		{

		//  the tree is empty

		err = JET_errSuccess;
		goto HandleError;
		}
	Call( err );

	do
		{

		//	check the current node

		if ( sizeof( SLVSPACENODE ) != pfucb->kdfCurr.data.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Data is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if ( sizeof( PGNO ) != pfucb->kdfCurr.key.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Key is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		//	add to the total number of pages seen

		cpgTotal += SLVSPACENODE::cpageMap;

		//	verify the total number of pages

		ULONG pgnoCurr;
		LongFromKey( &pgnoCurr, pfucb->kdfCurr.key );
		if ( cpgTotal != pgnoCurr )
			{
			AssertSz( fFalse, "SLV space tree corruption. Nodes out of order" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		const SLVSPACENODE * pspacenode = (SLVSPACENODE *)pfucb->kdfCurr.data.Pv();

#ifndef RTM
		Call( pspacenode->ErrCheckNode( CPRINTFDBGOUT::PcprintfInstance() ) );
#endif	//	RTM

		//	add to the number of available pages

		Assert( pspacenode->CpgAvail() <= SLVSPACENODE::cpageMap );
		cpgAvail += pspacenode->CpgAvail();
		}
	while ( JET_errSuccess == ( err = ErrBTNext( pfucb, fDIRNull ) ) );

	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTxn = fFalse;

HandleError:

	Assert( cpgAvail <= cpgTotal );
	if ( pcpgOwned )
		{
		*pcpgOwned = cpgTotal;
		}
	if ( pcpgAvail )
		{
		*pcpgAvail = cpgAvail;
		}

	if ( pfucb )
		{
		BTClose( pfucb );
		}

	if ( fInTxn )
		{
		Assert( err < JET_errSuccess );
		CallS( ErrDIRRollback( ppib ) );
		}

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\recupd.cxx ===
#include "std.hxx"

enum RECOPER
	{
	recoperInsert,
	recoperDelete,
	recoperReplace
	};

LOCAL ERR ErrRECIInsert( FUCB *pfucb, VOID *pv, ULONG cbMax, ULONG *pcbActual, DIRFLAG dirflag );
LOCAL ERR ErrRECIReplace( FUCB *pfucb, DIRFLAG dirflag = fDIRNull );


//  ================================================================
LOCAL ERR ErrRECICallback(
		PIB * const ppib,
		FUCB * const pfucb,
		TDB * const ptdb,
		const JET_CBTYP cbtyp,
		const ULONG ulId,
		void * const pvArg1,
		void * const pvArg2,
		const ULONG ulUnused )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const JET_SESID sesid 		= (JET_SESID)ppib;
	const JET_TABLEID tableid 	= (JET_TABLEID)pfucb;
	const JET_DBID ifmp 		= (JET_DBID)pfucb->ifmp;

	const VTFNDEF * const pvtfndefSaved = pfucb->pvtfndef;
	pfucb->pvtfndef = &vtfndefIsamCallback;
	
	const TRX trxSession = TrxVERISession( pfucb );
	const CBDESC * pcbdesc = ptdb->Pcbdesc();
	while( NULL != pcbdesc && err >= JET_errSuccess )
		{
		BOOL fVisible = fTrue;
#ifdef VERSIONED_CALLBACKS
		if( !( pcbdesc->fVersioned ) )
			{
			fVisible = fTrue;
			}
		else
			{
			if( trxMax == pcbdesc->trxRegisterCommit0 )
				{
				//  uncommitted register. only visible if we are the session that added it
				Assert( trxMax != pcbdesc->trxRegisterBegin0 );
				Assert( trxMax == pcbdesc->trxRegisterCommit0 );
				Assert( trxMax == pcbdesc->trxUnregisterBegin0 );
				Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
				fVisible = trxSession == pcbdesc->trxRegisterBegin0;
				}
			else if( trxMax == pcbdesc->trxUnregisterBegin0 )
				{
				//  committed register. visible if we began after the register committed
				Assert( trxMax != pcbdesc->trxRegisterBegin0 );
				Assert( trxMax != pcbdesc->trxRegisterCommit0 );
				Assert( trxMax == pcbdesc->trxUnregisterBegin0 );
				Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
				fVisible = TrxCmp( trxSession, pcbdesc->trxRegisterCommit0 ) > 0;
				}
			else if( trxMax == pcbdesc->trxUnregisterCommit0 )
				{
				//	uncommitted unregister. visible unless we are the session that unregistered it
				Assert( trxMax != pcbdesc->trxRegisterBegin0 );
				Assert( trxMax != pcbdesc->trxRegisterCommit0 );
				Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
				Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
				fVisible = trxSession != pcbdesc->trxUnregisterBegin0;
				}
			else
				{
				//  commited unregister. only visible if we began before the unregister committed
				Assert( trxMax != pcbdesc->trxRegisterBegin0 );
				Assert( trxMax != pcbdesc->trxRegisterCommit0 );
				Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
				Assert( trxMax != pcbdesc->trxUnregisterCommit0 );
				fVisible = TrxCmp( trxSession, pcbdesc->trxUnregisterCommit0 ) <= 0;
				}
			}
#endif	//	VERSIONED_CALLBACKS
		
		if( fVisible
			&& ( pcbdesc->cbtyp & cbtyp )
			&& ulId == pcbdesc->ulId )
			{
			++(Ptls()->ccallbacksNested);
			Ptls()->fInCallback = fTrue;

			TRY
				{
				err = (*pcbdesc->pcallback)(
						sesid,
						ifmp,
						tableid,
						cbtyp & pcbdesc->cbtyp,
						pvArg1,
						pvArg2,
						pcbdesc->pvContext,
						ulUnused );
				}
			EXCEPT( efaExecuteHandler )
				{
				err = JET_errCallbackFailed;
				}
			ENDEXCEPT;
				
			if ( JET_errSuccess != err )
				{
				ErrERRCheck( err );
				}
			if( 0 == ( --(Ptls()->ccallbacksNested) ) )
				{
				Ptls()->fInCallback = fFalse;
				}
			}
		pcbdesc = pcbdesc->pcbdescNext;
		}		

	pfucb->pvtfndef = pvtfndefSaved;

	return err;
	}


//  ================================================================
ERR ErrRECCallback( 
		PIB * const ppib,
		FUCB * const pfucb,
		const JET_CBTYP cbtyp,
		const ULONG ulId,
		void * const pvArg1,
		void * const pvArg2,
		const ULONG ulUnused )
//  ================================================================
//
//  Call the specified callback type for the specified id
//
//-
	{
	Assert( JET_cbtypNull != cbtyp );
	
	ERR err = JET_errSuccess;
	
	FCB * const pfcb = pfucb->u.pfcb;
	Assert( NULL != pfcb );
	TDB * const ptdb = pfcb->Ptdb();
	Assert( NULL != ptdb );

	Assert( err >= JET_errSuccess );

	if( NULL != ptdb->Pcbdesc() )
		{
///		pfcb->EnterDDL();
		err = ErrRECICallback( ppib, pfucb, ptdb, cbtyp, ulId, pvArg1, pvArg2, ulUnused );
///		pfcb->LeaveDDL();
		}

	if( err >= JET_errSuccess )
		{
		FCB * const pfcbTemplate = ptdb->PfcbTemplateTable();
		if( pfcbNil != pfcbTemplate )
			{
			TDB * const ptdbTemplate = pfcbTemplate->Ptdb();
			err = ErrRECICallback( ppib, pfucb, ptdbTemplate, cbtyp, ulId, pvArg1, pvArg2, ulUnused );
			}
		}

	return err;
	}


ERR VTAPI ErrIsamUpdate(
  	JET_SESID	sesid,
	JET_VTID	vtid,
	VOID 		*pv,
	ULONG 		cbMax,
	ULONG 		*pcbActual,
	JET_GRBIT	grbit )
	{
 	PIB * const ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB * const pfucb	= reinterpret_cast<FUCB *>( vtid );

	ERR			err;

	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	if ( FFUCBReplacePrepared( pfucb ) )
		{
		BOOKMARK *pbm;
		
		if ( cbMax > 0 )
			{
			CallR( ErrDIRGetBookmark( pfucb, &pbm ) );
			AssertDIRNoLatch( ppib );
			Assert( pbm->data.Cb() == 0 );
			Assert( pbm->key.Cb() > 0 );
			Assert( pbm->key.prefix.FNull() );

			if ( pcbActual != NULL )
				{
				*pcbActual = pbm->key.Cb();
				}

			pbm->key.CopyIntoBuffer( pv, min( cbMax, (ULONG)pbm->key.Cb() ) ); 
			}						    

		Assert( pfucb->ppib == ppib );
		err = ErrRECIReplace( pfucb, grbit | fDIRNull );
		}						   
	else if ( FFUCBInsertPrepared( pfucb ) )
		{
		//	get bookmark of inserted node in pv
		//
		err = ErrRECIInsert( pfucb, pv, cbMax, pcbActual, grbit | fDIRNull );
		}
	else
		err = ErrERRCheck( JET_errUpdateNotPrepared );

	//  free temp working buffer
	//
	if ( err >= 0 && !fGlobalRepair )		//	for fGlobalRepair we will cache these until we close the cursor
		{
		RECIFreeCopyBuffer( pfucb );
		}

	AssertDIRNoLatch( ppib );
	Assert( err != JET_errNoCurrentRecord );
	return err;
	}


LOCAL ERR ErrRECIUpdateIndex(
	FUCB			*pfucb,
	FCB				*pfcbIdx,
	const RECOPER	recoper,
	const DIB		*pdib = NULL )
	{
	ERR				err = JET_errSuccess;					// error code of various utility
	FUCB			*pfucbIdx;								//	cursor on secondary index
	
	Assert( pfcbIdx != pfcbNil );
	Assert( pfcbIdx->PfcbTable()->Ptdb() != ptdbNil );
	Assert( pfcbIdx->Pidb() != pidbNil );
	Assert( pfucb != pfucbNil );
	Assert( pfucb->ppib != ppibNil );
	Assert( pfucb->ppib->level < levelMax );
	Assert( !Pcsr( pfucb )->FLatched() );
	AssertDIRNoLatch( pfucb->ppib );

	Assert( pfcbIdx->FTypeSecondaryIndex() );
	Assert( pfcbIdx->PfcbTable() == pfucb->u.pfcb );

	//	open FUCB on this index
	//
	CallR( ErrDIROpen( pfucb->ppib, pfcbIdx, &pfucbIdx ) );
	Assert( pfucbIdx != pfucbNil );
	FUCBSetIndex( pfucbIdx );
	FUCBSetSecondary( pfucbIdx );
	
	//	get bookmark of primary index record replaced
	//
	Assert( FFUCBPrimary( pfucb ) );
	Assert( FFUCBUnique( pfucb ) );

#ifdef DEBUG
	const BOOKMARK		*pbmPrimary		= ( recoperInsert == recoper ? pdib->pbm : &pfucb->bmCurr );
	Assert( pbmPrimary->key.prefix.FNull() );
	Assert( pbmPrimary->key.Cb() > 0 );
	Assert( pbmPrimary->data.FNull() );
#endif

	if ( recoperInsert == recoper )
		{
		Assert( NULL != pdib );
		err = ErrRECIAddToIndex( pfucb, pfucbIdx, pdib->pbm, pdib->dirflag );
		}
	else
		{
		Assert( NULL == pdib );
		if ( recoperDelete == recoper )
			{
			err = ErrRECIDeleteFromIndex( pfucb, pfucbIdx, &pfucb->bmCurr );
			}
		else
			{
			Assert( recoperReplace == recoper);
			err = ErrRECIReplaceInIndex( pfucb, pfucbIdx, &pfucb->bmCurr );
			}
		}

	//	close the FUCB
	//
	DIRClose( pfucbIdx );

	AssertDIRNoLatch( pfucb->ppib );
	return err;
	}


LOCAL BOOL FRECIAllSparseIndexColumnsSet(
	const IDB * const		pidb,
	const FUCB * const		pfucbTable )
	{
	Assert( pidb->FSparseIndex() );

	if ( pidb->CidxsegConditional() > 0 )
		{
		//	can't use IDB's rgbitIdx because
		//	we must filter out conditional index
		//	columns

		const FCB * const		pfcbTable	= pfucbTable->u.pfcb;
		const TDB * const		ptdb		= pfcbTable->Ptdb();
		const IDXSEG *			pidxseg		= PidxsegIDBGetIdxSeg( pidb, ptdb );
		const IDXSEG * const	pidxsegMac	= pidxseg + pidb->Cidxseg();

		Assert( pidxseg < pidxsegMac );
		for ( ; pidxseg < pidxsegMac; pidxseg++ )
			{
			if ( !FFUCBColumnSet( pfucbTable, pidxseg->Fid() ) )
				{
				//	found a sparse index column that didn't get set
				return fFalse;
				}
			}
		}
	else
		{
		//	no conditional columns, so we can use
		//	the IDB bit array

		const LONG *		plIdx		= (LONG *)pidb->RgbitIdx();;
		const LONG * const	plIdxMax	= plIdx + ( 32 / sizeof(LONG) );;
		const LONG *		plSet		= (LONG *)pfucbTable->rgbitSet;
 
		for ( ; plIdx < plIdxMax; plIdx++, plSet++ )
			{
			if ( (*plIdx & *plSet) != *plIdx )
				{
				//	found a sparse index column that didn't get set
				return fFalse;
				}
			}
		}

	//	all sparse index columns were set
	return fTrue;
	}
	
LOCAL BOOL FRECIAnySparseIndexColumnSet(
	const IDB * const		pidb,
	const FUCB * const		pfucbTable )
	{
	Assert( pidb->FSparseIndex() );

	if ( pidb->CidxsegConditional() > 0 )
		{
		//	can't use IDB's rgbitIdx because
		//	we must filter out conditional index
		//	columns

		const FCB * const		pfcbTable	= pfucbTable->u.pfcb;
		const TDB * const		ptdb		= pfcbTable->Ptdb();
		const IDXSEG *			pidxseg		= PidxsegIDBGetIdxSeg( pidb, ptdb );
		const IDXSEG * const	pidxsegMac	= pidxseg + pidb->Cidxseg();

		Assert( pidxseg < pidxsegMac );
		for ( ; pidxseg < pidxsegMac; pidxseg++ )
			{
			if ( FFUCBColumnSet( pfucbTable, pidxseg->Fid() ) )
				{
				//	found a sparse index column that got set
				return fTrue;
				}
			}
		}

	else
		{
		//	no conditional columns, so we can use
		//	the IDB bit array

		const LONG *		plIdx		= (LONG *)pidb->RgbitIdx();;
		const LONG * const	plIdxMax	= plIdx + ( 32 / sizeof(LONG) );;
		const LONG *		plSet		= (LONG *)pfucbTable->rgbitSet;
 
		for ( ; plIdx < plIdxMax; plIdx++, plSet++ )
			{
			if ( *plIdx & *plSet )
				{
				//	found a sparse index column that got set
				return fTrue;
				}
			}
		}

	//	no sparse index columns were set
	return fFalse;
	}
	

INLINE BOOL FRECIPossiblyUpdateSparseIndex(
	const IDB * const		pidb,
	const FUCB * const		pfucbTable )
	{
	Assert( pidb->FSparseIndex() );

	if ( !pidb->FAllowSomeNulls() )
		{
		//	IgnoreAnyNull specified, so only need to
		//	update the index if all index columns
		//	were set
		return FRECIAllSparseIndexColumnsSet( pidb, pfucbTable );
		}
	else if ( !pidb->FAllowFirstNull() )
		{
		//	IgnoreFirstNull specified, so only need to
		//	update the index if the first column was set
		const FCB * const		pfcbTable	= pfucbTable->u.pfcb;
		const TDB * const		ptdb		= pfcbTable->Ptdb();
		const IDXSEG *			pidxseg		= PidxsegIDBGetIdxSeg( pidb, ptdb );
		return FFUCBColumnSet( pfucbTable, pidxseg->Fid() );
		}
	else
		{
		//	IgnoreNull specified, so need to update the
		//	index if any index column was set
		return FRECIAnySparseIndexColumnSet( pidb, pfucbTable );
		}
	}

LOCAL BOOL FRECIPossiblyUpdateSparseConditionalIndex(
	const IDB * const	pidb,
	const FUCB * const	pfucbTable )
	{
	Assert( pidb->FSparseConditionalIndex() );
	Assert( pidb->CidxsegConditional() > 0 );

	const FCB * const		pfcbTable	= pfucbTable->u.pfcb;
	const TDB * const		ptdb		= pfcbTable->Ptdb();
	const IDXSEG *			pidxseg		= PidxsegIDBGetIdxSegConditional( pidb, ptdb );
	const IDXSEG * const	pidxsegMac	= pidxseg + pidb->CidxsegConditional();

	//	check conditional columns and see if we can
	//	automatically deduce whether the record should
	//	be added to the index, regardless of whether any
	//	of the actual index columns were set or not
	for ( ; pidxseg < pidxsegMac; pidxseg++ )
		{
		//	check that the update didn't modify the
		//	conditional column
		if ( !FFUCBColumnSet( pfucbTable, pidxseg->Fid() ) )
			{
			const FIELD * const	pfield	= ptdb->Pfield( pidxseg->Columnid() );

			if ( !FFIELDUserDefinedDefault( pfield->ffield ) )
				{
				//	given that the update didn't modify the
				//	conditional column, see if the default
				//	value of the column would cause the
				//	record to be excluded from the index
				//	(NOTE: default values cannot be NULL)
				const BOOL	fHasDefault	= FFIELDDefault( pfield->ffield );
				const BOOL	fSparse		= ( pidxseg->FMustBeNull() ?
												fHasDefault :
												!fHasDefault );
				if ( fSparse )
					{
					//	this record will be excluded from
					//	the index
					return fFalse;
					}
				}
			}
		}

	//	could not exclude the record based on
	//	unset conditional columns
	return fTrue;
	}


//  ================================================================
LOCAL BOOL FRECIHasUserDefinedColumns( const IDXSEG * const pidxseg, const INT cidxseg, const TDB * const ptdb )
//  ================================================================
	{
	INT iidxseg;
	for( iidxseg = 0; iidxseg < cidxseg; ++iidxseg )
		{
		const FIELD * const pfield = ptdb->Pfield( pidxseg[iidxseg].Columnid() );
		if( FFIELDUserDefinedDefault( pfield->ffield ) )
			{
			return fTrue;
			}
		}
	return fFalse;
	}

//  ================================================================
LOCAL BOOL FRECIHasUserDefinedColumns( const IDB * const pidb, const TDB * const ptdb )
//  ================================================================
	{
	const INT cidxseg = pidb->Cidxseg();
	const IDXSEG * const pidxseg = PidxsegIDBGetIdxSeg( pidb, ptdb );
	if( FRECIHasUserDefinedColumns( pidxseg, cidxseg, ptdb ) )
		{
		return fTrue;
		}
	else if( pidb->CidxsegConditional() > 0 )
		{
		const INT cidxsegConditional = pidb->CidxsegConditional();
		const IDXSEG * const pidxsegConditional = PidxsegIDBGetIdxSegConditional( pidb, ptdb );
		return FRECIHasUserDefinedColumns( pidxsegConditional, cidxsegConditional, ptdb );
		}
	return fFalse;
	}


LOCAL VOID RECIReportIndexCorruption( const FCB * const pfcbIdx )
	{
	const IDB * const	pidb					= pfcbIdx->Pidb();
	INT					iszT					= 0;
	const CHAR *		rgszT[3];

	//	only for secondary indexes
	Assert( pidbNil != pidb );
	Assert( !pidb->FPrimary() );

	//	pfcbTable may not be linked up if we're in CreateIndex(),
	//	in which case we don't have access to the information
	//	we want to report
	if ( pfcbNil == pfcbIdx->PfcbTable() )
		return;

	//	WARNING! WARNING!  This code currently does not grab the DML latch,
	//	so there doesn't appear to be any guarantee that the table and index
	//	name won't be relocated from underneath us

	const BOOL			fHasUserDefinedColumns	= FRECIHasUserDefinedColumns(
															pidb,
															pfcbIdx->PfcbTable()->Ptdb() );

	rgszT[iszT++] = pfcbIdx->PfcbTable()->Ptdb()->SzTableName();
	rgszT[iszT++] = pfcbIdx->PfcbTable()->Ptdb()->SzIndexName(
														pidb->ItagIndexName(),
														pfcbIdx->FDerivedIndex() );
	rgszT[iszT++] = fHasUserDefinedColumns ? "1" : "0";
	Assert( iszT < ( sizeof( rgszT ) / sizeof( rgszT[0] ) ) );

	UtilReportEvent(
			eventError,
			DATABASE_CORRUPTION_CATEGORY,
			INDEX_CORRUPTED_ID,
			iszT,
			rgszT );
	}


//+local
// ErrRECIInsert
// ========================================================================
// ErrRECIInsert( FUCB *pfucb, VOID *pv, ULONG cbMax, ULONG *pcbActual, DIRFLAG dirflag )
//
// Adds a record to a data file.  All indexes on the data file are
// updated to reflect the addition.
//
// PARAMETERS	pfucb						FUCB for file
//				pv							pointer to bookmark buffer pv != NULL, bookmark is returned
//				cbMax						size of bookmark buffer
//				pcbActual					returned size of bookmark
//
// RETURNS		Error code, one of the following:
//					 JET_errSuccess		Everything went OK.
//					-KeyDuplicate		The record being added causes
//										an illegal duplicate entry in an index.
//					-NullKeyDisallowed	A key of the new record is NULL.
//					-RecordNoCopy		There is no working buffer to add from.
//					-NullInvalid		The record being added contains
//										at least one null-valued field
//										which is defined as NotNull.
// SIDE EFFECTS
//		After addition, file currency is left on the new record.
//		Index currency (if any) is left on the new index entry.
//		On failure, the currencies are returned to their initial states.
//
//	COMMENTS
//		No currency is needed to add a record.
//		A transaction is wrapped around this function.	Thus, any
//		work done will be undone if a failure occurs.
//-
LOCAL ERR ErrRECIInsert( FUCB *pfucb, VOID *pv, ULONG cbMax, ULONG *pcbActual, DIRFLAG dirflag )
	{
	ERR			err;  						 	// error code of various utility
	PIB			*ppib				= pfucb->ppib;
	KEY			keyToAdd;					 	// key of new data record
	BYTE		rgbKey[ JET_cbPrimaryKeyMost ];	// key buffer
	FCB			*pfcbTable;					 	// file's FCB
	FCB			*pfcbIdx;					 	// loop variable for each index on file
	FUCB		*pfucbT				= pfucbNil;
	BOOL		fUpdatingLatchSet	= fFalse;

	CheckPIB( ppib );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	//	should have been checked in PrepareUpdate
	//
	Assert( FFUCBUpdatable( pfucb ) );
	Assert( FFUCBInsertPrepared( pfucb ) );

	//	efficiency variables
	//
	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );

	//	if necessary, begin transaction
	//
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	//	delete the original copy if necessary
	if ( FFUCBInsertCopyDeleteOriginalPrepared( pfucb ) )
		{
		FUCBSetUpdateForInsertCopyDeleteOriginal( pfucb );
		Call( ErrIsamDelete( ppib, pfucb ) );
		}

	// Do the BeforeInsert callback
	Call( ErrRECCallback( ppib, pfucb, JET_cbtypBeforeInsert, 0, NULL, NULL, 0 ) );

	//	open temp FUCB on data file
	//
	Call( ErrDIROpen( ppib, pfcbTable, &pfucbT ) );
	Assert( pfucbT != pfucbNil );
	FUCBSetIndex( pfucbT );

	Assert( !pfucb->dataWorkBuf.FNull() );
	BOOL fIllegalNulls;
	Call( ErrRECIIllegalNulls( pfucb, pfucb->dataWorkBuf, &fIllegalNulls ) );
	if ( fIllegalNulls )
		{
		err = ErrERRCheck( JET_errNullInvalid );
		goto HandleError;
		}

	Call( pfcbTable->ErrSetUpdatingAndEnterDML( ppib ) );
	fUpdatingLatchSet = fTrue;
	
	//	set version and autoinc columns
	//
	pfcbTable->AssertDML();

	FID		fidVersion;
	fidVersion = pfcbTable->Ptdb()->FidVersion();		// UNDONE: Need to properly version these.
	if ( fidVersion != 0 && !( FFUCBColumnSet( pfucb, fidVersion ) ) )
		{
		//	set version column to zero
		//
		const BOOL		fTemplateColumn	= pfcbTable->Ptdb()->FFixedTemplateColumn( fidVersion );
		const COLUMNID	columnidT		= ColumnidOfFid( fidVersion, fTemplateColumn );
		ULONG			ul				= 0;
		DATA			dataField;

		dataField.SetPv( (BYTE *)&ul );
		dataField.SetCb( sizeof(ul) );
		err = ErrRECISetFixedColumn(
					pfucb,
					pfcbTable->Ptdb(),
					columnidT,
					&dataField );
		if ( err < 0 )
			{
			pfcbTable->LeaveDML();
			goto HandleError;
			}
		}

	pfcbTable->AssertDML();
	
#ifdef DEBUG	
	if ( pfcbTable->Ptdb()->FidAutoincrement() != 0 )
		{
		const TDB		* const ptdbT	= pfcbTable->Ptdb();
		const BOOL		fTemplateColumn	= ptdbT->FFixedTemplateColumn( ptdbT->FidAutoincrement() );
		const COLUMNID	columnidT		= ColumnidOfFid( ptdbT->FidAutoincrement(), fTemplateColumn );
		DATA			dataT;

		Assert( FFUCBColumnSet( pfucb, FidOfColumnid( columnidT ) ) );

		// Just retrieve column, even if we don't have versioned
		// access to it.
		CallS( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				ptdbT,
				columnidT,
				pfucb->dataWorkBuf,
				&dataT ) );
		
		Assert( !( pfcbTable->FTypeSort()
				|| pfcbTable->FTypeTemporaryTable() ) );	// Don't currently support autoinc with sorts/temp. tables

		Assert( ptdbT->QwAutoincrement() > 0 );
		if ( ptdbT->F8BytesAutoInc() )
			{
			Assert( dataT.Cb() == sizeof(QWORD) );
			Assert( *(UnalignedLittleEndian< QWORD > *)dataT.Pv() <= ptdbT->QwAutoincrement() );
			}
		else
			{
			Assert( dataT.Cb() == sizeof(ULONG) );
			Assert( *(UnalignedLittleEndian< ULONG > *)dataT.Pv() <= (ULONG)ptdbT->QwAutoincrement() );
			}
		}
#endif

	pfcbTable->LeaveDML();

	//	get key to add with new record
	//
	keyToAdd.prefix.Nullify();
	keyToAdd.suffix.SetPv( rgbKey );
	if ( pidbNil == pfcbTable->Pidb() )
		{
		DBK	dbk;

		//	file is sequential
		//

		//	dbk's are numbered starting at 1.  A dbk of 0 indicates that we must
		//	first retrieve the dbkMost.  In the pathological case where there are
		//	currently no dbk's, we'll go through here anyway, but only the first
		//	time (since there will be dbk's after that).
		//
		if ( pfcbTable->Ptdb()->DbkMost() == 0 )
			{
			DIB		dib;

			DIRGotoRoot( pfucbT );

			//	down to the last data record
			//
			dib.dirflag = fDIRNull;
			dib.pos = posLast;
			err = ErrDIRDown( pfucbT, &dib );
			Assert( err != JET_errNoCurrentRecord );
			switch( err )
				{
				case JET_errSuccess:
					{
					BYTE	rgbT[4];
					pfucbT->kdfCurr.key.CopyIntoBuffer( rgbT, sizeof( rgbT ) );
					dbk = ( rgbT[0] << 24 ) + ( rgbT[1] << 16 ) + ( rgbT[2] << 8 ) + rgbT[3];
					Assert( dbk > 0 );		// dbk's start numbering at 1
					DIRUp( pfucbT );
					Assert( !Pcsr( pfucbT )->FLatched( ) );
					break;
					}
				case JET_errRecordNotFound:
					Assert( !Pcsr( pfucbT )->FLatched( ) );
					dbk = 0;
					break;

				default:
					Assert( !Pcsr( pfucbT )->FLatched( ) );
					goto HandleError;
				}

			//	if there are no records in the table, then the first
			//	dbk value is 1.  Otherwise, set dbk to next value after
			//	maximum found.
			//
			dbk++;

			//	While retrieving the dbkMost, someone else may have been
			//	doing the same thing and beaten us to it.  When this happens,
			//	cede to the other guy.
			//
			pfcbTable->Ptdb()->InitDbkMost( dbk );
			}

		Call( pfcbTable->Ptdb()->ErrGetAndIncrDbkMost( &dbk ) );
		Assert( dbk > 0 );

		keyToAdd.suffix.SetCb( sizeof(DBK) );

		BYTE	*pb = (BYTE *) keyToAdd.suffix.Pv();
		pb[0] = (BYTE)((dbk >> 24) & 0xff);
		pb[1] = (BYTE)((dbk >> 16) & 0xff);
		pb[2] = (BYTE)((dbk >> 8) & 0xff);
		pb[3] = (BYTE)(dbk & 0xff);
		}

	else
		{
		//	file is primary
		//
		Assert( !pfcbTable->Pidb()->FMultivalued() );
		Assert( !pfcbTable->Pidb()->FTuples() );
		Call( ErrRECRetrieveKeyFromCopyBuffer(
			pfucb,
			pfcbTable->Pidb(),
			&keyToAdd, 
			1,
			0,
			prceNil ) );

		CallS( ErrRECValidIndexKeyWarning( err ) );
		Assert( wrnFLDNotPresentInIndex != err );
		Assert( wrnFLDOutOfKeys != err );
		Assert( wrnFLDOutOfTuples != err );

		if ( pfcbTable->Pidb()->FNoNullSeg()
			&& ( wrnFLDNullKey == err || wrnFLDNullFirstSeg == err || wrnFLDNullSeg == err ) )
			Error( ErrERRCheck( JET_errNullKeyDisallowed ), HandleError )
		}

	//	check if return buffer for bookmark is sufficient size
	//
	if ( pv != NULL && (ULONG)keyToAdd.Cb() > cbMax )
		{
		Error( ErrERRCheck( JET_errBufferTooSmall ), HandleError )
		}

	//	insert record.  Move to DATA root.
	//
	DIRGotoRoot( pfucbT );

	Call( ErrDIRInsert( pfucbT, keyToAdd, pfucb->dataWorkBuf, dirflag ) );

	
	//	return bookmark of inserted record
	//
	AssertDIRNoLatch( ppib );
	Assert( !pfucbT->bmCurr.key.FNull() && pfucbT->bmCurr.key.Cb() == keyToAdd.Cb() );
	Assert( pfucbT->bmCurr.data.Cb() == 0 );

	if ( pcbActual != NULL || pv != NULL )
		{
		BOOKMARK	*pbmPrimary;	//	bookmark of primary index node inserted

		CallS( ErrDIRGetBookmark( pfucbT, &pbmPrimary ) );
		
		//	set return values
		//
		if ( pcbActual != NULL )
			{
			Assert( pbmPrimary->key.Cb() == keyToAdd.Cb() );
			*pcbActual = pbmPrimary->key.Cb();
			}
		
		if ( pv != NULL )
			{
			Assert( cbMax >= (ULONG)pbmPrimary->key.Cb() );
			pbmPrimary->key.CopyIntoBuffer( pv, min( cbMax, (ULONG)pbmPrimary->key.Cb() ) ); 
			}
		}

	// UNDONE SLVOWNERMAP: performance double scan in loop and in slvInfo.ErrLoad !!!! To be changed
	if ( FFUCBSLVOwnerMapNeedUpdate( pfucb )
		|| FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) ) // we have to update the ownership of the SLV space
		{
		BOOKMARK	bmForOwnerMap;

		bmForOwnerMap.Nullify();

#ifdef DEBUG
		keyToAdd.AssertValid();
#endif // DEBUG		
	
		bmForOwnerMap.key.suffix.SetCb( keyToAdd.suffix.Cb() );
		bmForOwnerMap.key.suffix.SetPv( keyToAdd.suffix.Pv() );
		bmForOwnerMap.key.prefix.SetCb( keyToAdd.prefix.Cb() );
		bmForOwnerMap.key.prefix.SetPv( keyToAdd.prefix.Pv() );

		TAGFIELDS	tagfields( pfucb->dataWorkBuf );
		Call( tagfields.ErrUpdateSLVOwnerMapForRecordInsert( pfucb, bmForOwnerMap ) );
		
		FUCBResetSLVOwnerMapNeedUpdate( pfucb );
		}

	
	//	insert item in secondary indexes
	//
	// No critical section needed to guard index list because Updating latch
	// protects it.
	DIB		dib;
	BOOL fInsertCopy;
	fInsertCopy = FFUCBInsertCopyPrepared( pfucb );
	dib.pbm		= &pfucbT->bmCurr;
	dib.dirflag	= dirflag;
	for ( pfcbIdx = pfcbTable->PfcbNextIndex();
		pfcbIdx != pfcbNil;
		pfcbIdx = pfcbIdx->PfcbNextIndex() )
		{
		if ( FFILEIPotentialIndex( ppib, pfcbTable, pfcbIdx ) )
			{
			const IDB * const	pidb		= pfcbIdx->Pidb();
			BOOL				fUpdate		= fTrue;

			if ( !fInsertCopy )
				{
				//	see if the sparse conditional index can tell
				//	us to skip the update
				if ( pidb->FSparseConditionalIndex() )
					fUpdate = FRECIPossiblyUpdateSparseConditionalIndex( pidb, pfucb );

				//	if the sparse conditional index could not cause us to
				//	skip the index update, see if the sparse index can
				//	tell us to skip the update
				if ( fUpdate && pidb->FSparseIndex() )
					fUpdate = FRECIPossiblyUpdateSparseIndex( pidb, pfucb );
				}

			if ( fUpdate )
				{
				Call( ErrRECIUpdateIndex( pfucb, pfcbIdx, recoperInsert, &dib ) );
				}
			}
		}
	
	Assert( pfcbTable != pfcbNil );
	pfcbTable->ResetUpdating();
	fUpdatingLatchSet = fFalse;

	DIRClose( pfucbT );
	pfucbT = pfucbNil;

	//	if no error, commit transaction
	//
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
		
	FUCBResetUpdateFlags( pfucb );

	// Do the AfterInsert callback
	CallS( ErrRECCallback( ppib, pfucb, JET_cbtypAfterInsert, 0, NULL, NULL, 0 ) );

	AssertDIRNoLatch( ppib );

	return err;

HandleError:
	Assert( err < 0 );
	
	if ( fUpdatingLatchSet )
		{
		Assert( pfcbTable != pfcbNil );
		pfcbTable->ResetUpdating();
		}

	if ( pfucbNil != pfucbT )
		{
		DIRClose( pfucbT );
		}

	/*	rollback all changes on error
	/**/
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	AssertDIRNoLatch( ppib );
	return err;
	}


// Called by defrag to insert record but preserve copy buffer.
ERR ErrRECInsert( FUCB *pfucb, BOOKMARK * const pbmPrimary )
	{
	ERR		err;
	PIB		*ppib = pfucb->ppib;
	
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );
	Assert( pfucb->pfucbCurIndex == pfucbNil );
	Assert( !FFUCBSecondary( pfucb ) );

	if ( NULL != pbmPrimary )
		{
		ULONG cb = pbmPrimary->key.suffix.Cb();
		err = ErrRECIInsert(
					pfucb,
					pbmPrimary->key.suffix.Pv(),
					JET_cbBookmarkMost,
					&cb,
					fDIRNull );
		pbmPrimary->key.suffix.SetCb( cb );
		Assert( pbmPrimary->key.suffix.Cb() <= JET_cbBookmarkMost );
		}
	else
		{
		err = ErrRECIInsert( pfucb, NULL, 0, NULL, fDIRNull );
		}
	
	Assert( JET_errNoCurrentRecord != err );
	
	AssertDIRNoLatch( ppib );
	return err;
	}


//+local
// ErrRECIAddToIndex
// ========================================================================
// ERR ErrRECIAddToIndex( FCB *pfcbIdx, FUCB *pfucb )
//
// Extracts key from data record and adds that key with the given SRID to the index
//
// PARAMETERS	pfcbIdx 		  			FCB of index to insert into
//				pfucb						cursor pointing to primary index record
//
// RETURNS		JET_errSuccess, or error code from failing routine
//
// SIDE EFFECTS 
// SEE ALSO		Insert
//-
ERR	ErrRECIAddToIndex(
	FUCB		*pfucb,
	FUCB		*pfucbIdx,
	BOOKMARK	*pbmPrimary,
	DIRFLAG 	dirflag,
	RCE			*prcePrimary )
	{
	ERR			err;
	const FCB	* const pfcbIdx			= pfucbIdx->u.pfcb;
	const IDB	* const pidb			= pfcbIdx->Pidb();
	KEY			keyToAdd;
	BYTE		rgbKey[ JET_cbSecondaryKeyMost ];
	ULONG		itagSequence;
	BOOL		fNullKey				= fFalse;
	BOOL		fIndexUpdated			= fFalse;

	AssertDIRNoLatch( pfucb->ppib );
	
	Assert( pfcbIdx != pfcbNil );
	Assert( pfcbIdx->FTypeSecondaryIndex() );
	Assert( pidbNil != pidb );

	Assert( prceNil != prcePrimary  ?
				pfcbNil == pfcbIdx->PfcbTable() :		//	InsertByProxy: index not linked in yet
				pfcbIdx->PfcbTable() == pfucb->u.pfcb );

	keyToAdd.prefix.Nullify();
	keyToAdd.suffix.SetPv( rgbKey );
	for ( itagSequence = 1; ; itagSequence++ )
		{
		CallR( ErrRECRetrieveKeyFromCopyBuffer(
				pfucb,
				pidb,
				&keyToAdd,
				itagSequence,
				0,
				prcePrimary ) );

		CallS( ErrRECValidIndexKeyWarning( err ) );

		if ( wrnFLDOutOfKeys == err )
			{
			Assert( itagSequence > 1 );
			break;
			}

		else if ( wrnFLDOutOfTuples == err )
			{
			//	try next itagSequence
			Assert( pidb->FTuples() );
			if ( pidb->FMultivalued() )
				continue;
			else
				break;
			}

		else if ( wrnFLDNotPresentInIndex == err )
			{
			Assert( 1 == itagSequence );
///			AssertSz( fFalse, "[laurionb]: ErrRECIAddToIndex: new record is not in index" );
			break;
			}

		else if ( pidb->FNoNullSeg()
			&& ( wrnFLDNullKey == err || wrnFLDNullFirstSeg == err || wrnFLDNullSeg == err ) )
			{
			err = ErrERRCheck( JET_errNullKeyDisallowed );
			return err;
			}

		if ( wrnFLDNullKey == err )
			{
			Assert( 1 == itagSequence );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			Assert( !pidb->FTuples() );		//	tuple indexes would have generated wrnFLDOutOfTuples instead
			if ( !pidb->FAllowAllNulls() )
				break;
			fNullKey = fTrue;
			}
		else if ( ( wrnFLDNullFirstSeg  == err && !pidb->FAllowFirstNull() )
				|| ( wrnFLDNullSeg == err && !pidb->FAllowSomeNulls() ) )
			{
			break;
			}

		//	move to DATA root and insert index node
		//
		DIRGotoRoot( pfucbIdx );
		
		//  unversioned inserts into a secondary index are dangerous as if this fails the
		//  record will not be removed from the primary index
		Assert( !( dirflag & fDIRNoVersion ) );

		Assert( pbmPrimary->key.prefix.FNull() );
		Assert( pbmPrimary->key.Cb() > 0 );
		
		err = ErrDIRInsert( pfucbIdx, keyToAdd, pbmPrimary->key.suffix, dirflag, prcePrimary );
		if ( err < 0 )
			{
			if ( JET_errMultiValuedIndexViolation == err )
				{
				if ( itagSequence > 1 && !pidb->FUnique() )
					{
					Assert( pidb->FMultivalued() );

					//	must have been record with multi-value column
					//	or tuples with sufficiently similar values
					//	(ie. the indexed portion of the multi-values
					//	or tuples were identical) to produce redundant
					//	index entries.
					err = JET_errSuccess;
					}
				else
					{
					RECIReportIndexCorruption( pfcbIdx );
					AssertSz( fFalse, "JET_errSecondaryIndexCorrupted during an insert" );
					err = ErrERRCheck( JET_errSecondaryIndexCorrupted );
					}
				}
			CallR( err );
			}
		else
			{
			fIndexUpdated = fTrue;
			}

		if ( pidb->FTuples() )
			{
			//	at minimum, index entry with itagSequence==1 and ichOffset==0
			//	already got added
			Assert( fIndexUpdated );

			for ( ULONG ichOffset = 1; ichOffset < pidb->ChTuplesToIndexMax(); ichOffset++ )
				{
				CallR( ErrRECRetrieveKeyFromCopyBuffer(
							pfucb,
							pidb,
							&keyToAdd,
							itagSequence,
							ichOffset,
							prcePrimary ) );

				//	all other warnings should have been detected
				//	when we retrieved with ichOffset==0
				CallSx( err, wrnFLDOutOfTuples );
				if ( JET_errSuccess != err )
					break;

				//	move to DATA root and insert index node
				//
				DIRGotoRoot( pfucbIdx );
				
				//  unversioned inserts into a secondary index are dangerous as if this fails the
				//  record will not be removed from the primary index
				Assert( !( dirflag & fDIRNoVersion ) );

				Assert( pbmPrimary->key.prefix.FNull() );
				Assert( pbmPrimary->key.Cb() > 0 );
				
				err = ErrDIRInsert( pfucbIdx, keyToAdd, pbmPrimary->key.suffix, dirflag, prcePrimary );
				if ( err < 0 )
					{
					if ( JET_errMultiValuedIndexViolation == err )
						{
						//	must have been record with multi-value column
						//	or tuples with sufficiently similar values
						//	(ie. the indexed portion of the multi-values
						//	or tuples were identical) to produce redundant
						//	index entries.
						err = JET_errSuccess;
						}
					CallR( err );
					}
				}
			}

		//	dont keep extracting for keys with no tagged segments
		//
		if ( !pidb->FMultivalued() || fNullKey )
			{
			//	if no multivalues in this index, there's no point going beyond first itagSequence
			//	if key is null, this implies there are no further multivalues
			Assert( 1 == itagSequence );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			break;
			}
		}

	//	suppress warnings
	CallS( ErrRECValidIndexKeyWarning( err ) );
	err = ( fIndexUpdated ? ErrERRCheck( wrnFLDIndexUpdated ) : JET_errSuccess );
	return err;
	}
	

//+API
// ErrIsamDelete
// ========================================================================
// ErrIsamDelete( PIB *ppib, FCBU *pfucb )
//
// Deletes the current record from data file.  All indexes on the data
// file are updated to reflect the deletion.
//
// PARAMETERS
// 			ppib		PIB of this user
// 			pfucb		FUCB for file to delete from
// RETURNS
//		Error code, one of the following:
//			JET_errSuccess	 			Everything went OK.
//			JET_errNoCurrentRecord	   	There is no current record.
// SIDE EFFECTS 
//			After the deletion, file currency is left just before
//			the next record.  Index currency (if any) is left just
//			before the next index entry.  If the deleted record was
//			the last in the file, the currencies are left after the
//			new last record.  If the deleted record was the only record
//			in the entire file, the currencies are left in the
//			"beginning of file" state.	On failure, the currencies are
//			returned to their initial states.
//			If there is a working buffer for SetField commands,
//			it is discarded.
// COMMENTS		
//			If the currencies are not ON a record, the delete will fail.
//			A transaction is wrapped around this function.	Thus, any
//			work done will be undone if a failure occurs.
//			Index entries are not made for entirely-null keys.
//			For temporary files, transaction logging is deactivated
//			for the duration of the routine.
//-
ERR VTAPI ErrIsamDelete(
	JET_SESID	sesid,
	JET_VTID	vtid )
	{
	ERR			err;
 	PIB 		* ppib				= reinterpret_cast<PIB *>( sesid );
	FUCB		* pfucb				= reinterpret_cast<FUCB *>( vtid );
	FCB			* pfcbTable;					// table FCB
	FCB			* pfcbIdx;						// loop variable for each index on file
	BOOL		fUpdatingLatchSet	= fFalse;

	CallR( ErrPIBCheck( ppib ) );

	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );
	AssertDIRNoLatch( ppib );
	
	//	ensure that table is updatable
	//
	CallR( ErrFUCBCheckUpdatable( pfucb )  );
	CallR( ErrPIBCheckUpdatable( ppib ) );

#ifdef PREREAD_INDEXES_ON_DELETE
	if( pfucb->u.pfcb->FPreread() )
		{
		BTPrereadIndexesOfFCB( pfucb->u.pfcb );
		}
#endif	//	PREREAD_INDEXES_ON_DELETE

	if ( !FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) )
		{
		//	reset copy buffer status on record delete unless we are in
		//  insert-copy-delete-original mode (ie: copy buffer is in use)
		if ( FFUCBUpdatePrepared( pfucb ) )
			{
			if ( FFUCBInsertCopyDeleteOriginalPrepared( pfucb ) )
				{
				return ErrERRCheck( JET_errAlreadyPrepared );
				}
			else
				{
				CallR( ErrIsamPrepareUpdate( ppib, pfucb, JET_prepCancel ) );
				}
			}
		CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		}

	//	if InsertCopyDeleteOriginal, transaction is started in ErrRECIInsert()
	Assert( !FFUCBInsertCopyDeleteOriginalPrepared( pfucb ) );
	Assert( ppib->level > 0 );

#ifdef DEBUG
	const BOOL	fLogIsDone = !PinstFromPpib( ppib )->m_plog->m_fLogDisabled
							&& !PinstFromPpib( ppib )->m_plog->m_fRecovering
							&& rgfmp[pfucb->ifmp].FLogOn();
#endif

	// Do the BeforeDelete callback
	Call( ErrRECCallback( ppib, pfucb, JET_cbtypBeforeDelete, 0, NULL, NULL, 0 ) );

	// After ensuring that we're in a transaction, refresh
	// our cursor to ensure we still have access to the record.
	Call( ErrDIRGetLock( pfucb, writeLock ) );

	//	efficiency variables
	//
	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );

	Call( pfcbTable->ErrSetUpdatingAndEnterDML( ppib ) );
	fUpdatingLatchSet = fTrue;

	Assert(	fLogIsDone == ( !PinstFromPpib( ppib )->m_plog->m_fLogDisabled && !PinstFromPpib( ppib )->m_plog->m_fRecovering && rgfmp[pfucb->ifmp].FLogOn() ) );
	Assert( ppib->level < levelMax );

	//	delete from secondary indexes
	//
	// No critical section needed to guard index list because Updating latch
	// protects it.
	pfcbTable->LeaveDML();
	for( pfcbIdx = pfcbTable->PfcbNextIndex();
		pfcbIdx != pfcbNil;
		pfcbIdx = pfcbIdx->PfcbNextIndex() )
		{
		if ( FFILEIPotentialIndex( ppib, pfcbTable, pfcbIdx ) )
			{
			Call( ErrRECIUpdateIndex( pfucb, pfcbIdx, recoperDelete ) );
			}
		}

	//	do not touch LV/SLV data if we are doing an insert-copy-delete-original
	if ( !FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) )
		{
		//	SLVs now decommitted at the same time we deref separated LVs

		//	delete record long values
		Call( ErrRECDereferenceLongFieldsInRecord( pfucb ) );
		}

	Assert(	fLogIsDone == ( !PinstFromPpib( ppib )->m_plog->m_fLogDisabled && !PinstFromPpib( ppib )->m_plog->m_fRecovering && rgfmp[pfucb->ifmp].FLogOn() ) );

	//	delete record
	//
	Call( ErrDIRDelete( pfucb, fDIRNull ) );
	AssertDIRNoLatch( ppib );

	Assert(	fLogIsDone == ( !PinstFromPpib( ppib )->m_plog->m_fLogDisabled && !PinstFromPpib( ppib )->m_plog->m_fRecovering && rgfmp[pfucb->ifmp].FLogOn() ) );

	pfcbTable->ResetUpdating();
	fUpdatingLatchSet = fFalse;
	
	//	if no error, commit transaction
	//
	//	if InsertCopyDeleteOriginal, commit will be performed in ErrRECIInsert()
	if ( !FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) )
		{
		Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
		}

	AssertDIRNoLatch( ppib );

	// Do the AfterDelete callback
	CallS( ErrRECCallback( ppib, pfucb, JET_cbtypAfterDelete, 0, NULL, NULL, 0 ) );

	Assert(	fLogIsDone == ( !PinstFromPpib( ppib )->m_plog->m_fLogDisabled && !PinstFromPpib( ppib )->m_plog->m_fRecovering && rgfmp[pfucb->ifmp].FLogOn() ) );

	return err;

	
HandleError:
	Assert( err < 0 );
	AssertDIRNoLatch( ppib );
	Assert(	fLogIsDone == ( !PinstFromPpib( ppib )->m_plog->m_fLogDisabled && !PinstFromPpib( ppib )->m_plog->m_fRecovering && rgfmp[pfucb->ifmp].FLogOn() ) );

	if ( fUpdatingLatchSet )
		{
		Assert( pfcbTable != pfcbNil );
		pfcbTable->ResetUpdating();		//lint !e644
		}

	//	rollback all changes on error
	//	if InsertCopyDeleteOriginal, rollback will be performed in ErrRECIInsert()
	if ( !FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	return err;
	}


//+INTERNAL
//	ErrRECIDeleteFromIndex
//	========================================================================
//	ErrRECIDeleteFromIndex( FCB *pfcbIdx, FUCB *pfucb )
//	
//	Extracts key from data record and deletes the key with the given SRID
//
//	PARAMETERS	
//				pfucb							pointer to primary index record to delete
//				pfcbIdx							FCB of index to delete from
//	RETURNS		
//				JET_errSuccess, or error code from failing routine
//	SIDE EFFECTS 
//	SEE ALSO  	ErrRECDelete
//-

ERR	ErrRECIDeleteFromIndex(
	FUCB			*pfucb,
	FUCB			*pfucbIdx,
	BOOKMARK		*pbmPrimary,
	RCE				*prcePrimary )
	{
	ERR				err;
	const FCB		* const pfcbIdx	= pfucbIdx->u.pfcb;
	const IDB 		* const pidb	= pfcbIdx->Pidb();
	KEY				keyToDelete;
	BYTE			rgbKey[ JET_cbSecondaryKeyMost ];
	ULONG			itagSequence;
	BOOL			fNullKey		= fFalse;
	const BOOL		fDeleteByProxy	= ( prcePrimary != prceNil ); 
	const BOOL		fHasMultivalue	= pidb->FMultivalued();
	const BOOL		fAllowAllNulls	= pidb->FAllowAllNulls();
	const BOOL		fAllowFirstNull = pidb->FAllowFirstNull();
	const BOOL		fAllowSomeNulls = pidb->FAllowSomeNulls();
	const BOOL		fNoNullSeg		= pidb->FNoNullSeg();
	const BOOL		fUnique			= pidb->FUnique();
	BOOL			fIndexUpdated	= fFalse;

	AssertDIRNoLatch( pfucb->ppib );
	
	Assert( pfcbIdx != pfcbNil );
	Assert( pfcbIdx->FTypeSecondaryIndex() );
	Assert( pidbNil != pidb );
	
	//	delete all keys from this index for dying data record
	//
	keyToDelete.prefix.Nullify();
	keyToDelete.suffix.SetPv( rgbKey );
	for ( itagSequence = 1; ; itagSequence++ )
		{
		//	get key
		//
		if ( fDeleteByProxy )
			{
			Assert( pfcbIdx->PfcbTable() == pfcbNil );	// Index not linked in yet.
			CallR( ErrRECRetrieveKeyFromCopyBuffer(
				pfucb,
				pidb,
				&keyToDelete,
				itagSequence,
				0,
				prcePrimary ) );
			}
		else
			{
			Assert( pfcbIdx->PfcbTable() == pfucb->u.pfcb );		
			CallR( ErrRECRetrieveKeyFromRecord(
				pfucb,
				pidb,
				&keyToDelete,
				itagSequence,
				0,
				fFalse ) );
			}

		CallS( ErrRECValidIndexKeyWarning( err ) );

		if ( wrnFLDOutOfKeys == err )
			{
			Assert( itagSequence > 1 );
			break;
			}

		else if ( wrnFLDOutOfTuples == err )
			{
			//	try next itagSequence
			Assert( pidb->FTuples() );
			if ( fHasMultivalue )
				continue;
			else
				break;
			}

		//	record must honor index no NULL segment requirements
		//
		if ( fNoNullSeg )
			{
			Assert( wrnFLDNullSeg != err );
			Assert( wrnFLDNullFirstSeg != err );
			Assert( wrnFLDNullKey != err );
			}

		if ( ( wrnFLDNullFirstSeg == err && !fAllowFirstNull )
			|| ( wrnFLDNullSeg == err && !fAllowSomeNulls )
			|| ( wrnFLDNullKey == err && !fAllowAllNulls )
			|| wrnFLDNotPresentInIndex == err )
			{
			Assert( wrnFLDNotPresentInIndex != err || 1 == itagSequence );
///			AssertSz( wrnFLDNotPresentInIndex != err, "[laurionb]: ErrRECIDeleteFromIndex: original record was not in index" );
			break;
			}

		fNullKey = ( wrnFLDNullKey == err );
		Assert( !fNullKey || 1 == itagSequence );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
		Assert( !fNullKey || !pidb->FTuples() );	//	tuple indexes would have generated wrnFLDOutOfTuples instead

		//	move to DATA root and seek to index entry and delete it
		//
		DIRGotoRoot( pfucbIdx );
		Assert( pbmPrimary->key.prefix.FNull() );
		Assert( pbmPrimary->key.Cb() > 0 );
		CallR( ErrDIRDownKeyData( pfucbIdx, keyToDelete, pbmPrimary->key.suffix ) );

		err = ErrDIRDelete( pfucbIdx, fDIRNull, prcePrimary );
		if ( err < 0 )
			{
			if ( JET_errRecordDeleted == err )
				{
				if ( itagSequence > 1 && !fUnique )
					{
					Assert( fHasMultivalue );

					//	must have been record with multi-value column
					//	or tuples with sufficiently similar values
					//	(ie. the indexed portion of the multi-values
					//	or tuples were identical) to produce redundant
					//	index entries.
					err = JET_errSuccess;
					}
				else
					{
					RECIReportIndexCorruption( pfcbIdx );
					AssertSz( fFalse, "JET_errSecondaryIndexCorrupted during a delete" );
					err = ErrERRCheck( JET_errSecondaryIndexCorrupted );
					}
				}
			CallR( err );	
			}
		else
			{
			fIndexUpdated = fTrue;
			}

		if ( pidb->FTuples() )
			{
			//	at minimum, index entry with itagSequence==1 and ichOffset==0
			//	already got deleted
			Assert( fIndexUpdated );

			for ( ULONG ichOffset = 1; ichOffset < pidb->ChTuplesToIndexMax(); ichOffset++ )
				{
				if ( fDeleteByProxy )
					{
					Assert( pfcbIdx->PfcbTable() == pfcbNil );	// Index not linked in yet.
					CallR( ErrRECRetrieveKeyFromCopyBuffer(
						pfucb,
						pidb,
						&keyToDelete,
						itagSequence,
						ichOffset,
						prcePrimary ) );
					}
				else
					{
					Assert( pfcbIdx->PfcbTable() == pfucb->u.pfcb );		
					CallR( ErrRECRetrieveKeyFromRecord(
						pfucb,
						pidb,
						&keyToDelete,
						itagSequence,
						ichOffset,
						fFalse ) );
					}


				//	all other warnings should have been detected
				//	when we retrieved with ichOffset==0
				CallSx( err, wrnFLDOutOfTuples );
				if ( JET_errSuccess != err )
					break;

				//	move to DATA root and seek to index entry and delete it
				//
				DIRGotoRoot( pfucbIdx );
				Assert( pbmPrimary->key.prefix.FNull() );
				Assert( pbmPrimary->key.Cb() > 0 );
				CallR( ErrDIRDownKeyData( pfucbIdx, keyToDelete, pbmPrimary->key.suffix ) );

				err = ErrDIRDelete( pfucbIdx, fDIRNull, prcePrimary );
				if ( err < 0 )
					{
					if ( JET_errRecordDeleted == err )
						{
						//	must have been record with multi-value column
						//	or tuples with sufficiently similar values
						//	(ie. the indexed portion of the multi-values
						//	or tuples were identical) to produce redundant
						//	index entries.
						err = JET_errSuccess;
						}
					CallR( err );
					}
				}
			}

		//	dont keep extracting for keys with no tagged segments
		//
		if ( !fHasMultivalue || fNullKey )
			{
			//	if no multivalues in this index, there's no point going beyond first itagSequence
			//	if key is null, this implies there are no further multivalues
			Assert( 1 == itagSequence );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			break;
			}
		}

	//	suppress warnings
	CallS( ErrRECValidIndexKeyWarning( err ) );
	err = ( fIndexUpdated ? ErrERRCheck( wrnFLDIndexUpdated ) : JET_errSuccess );
	return err;
	}


//	determines whether an index may have changed using the hashed tags
//
LOCAL BOOL FRECIIndexPossiblyChanged( const BYTE *rgbitIdx, const BYTE *rgbitSet )
	{
	LONG	*plIdx;
	LONG	*plIdxMax;
	LONG	*plSet;
 
	plIdx = (LONG *)rgbitIdx;
	plSet = (LONG *)rgbitSet;
	plIdxMax = plIdx + (32 / sizeof( LONG ) );

	for ( ; plIdx < plIdxMax; plIdx++, plSet++ )
		{
		if ( *plIdx & *plSet )
			{
			return fTrue;
			}
		}
	return fFalse;
	}


//	determines whether an index may has changed by comparing the kets
//	UNDONE: only checks first multi-value in a multi-valued index and
//	first tuple in a tuple index
//
LOCAL ERR ErrRECFIndexChanged( FUCB *pfucb, FCB *pfcbIdx, BOOL *pfChanged )
	{
	KEY		keyOld;
	KEY		keyNew;
	BYTE	rgbOldKey[ KEY::cbKeyMax ];		//	this function is called on primary index to ensure it hasn't changed
	BYTE	rgbNewKey[ KEY::cbKeyMax ];		//	and on secondary index to cascade record updates
	DATA   	*plineNewData = &pfucb->dataWorkBuf;
	ERR		err;

	BOOL	fCopyBufferKeyIsPresentInIndex 	= fFalse;
	BOOL	fRecordKeyIsPresentInIndex		= fFalse;
	
	Assert( pfucb );
	Assert( !Pcsr( pfucb )->FLatched( ) );
	Assert( pfcbNil != pfucb->u.pfcb );
	Assert( pfcbNil != pfcbIdx );
	Assert( pfucb->dataWorkBuf.Cb() == plineNewData->Cb() );
	Assert( pfucb->dataWorkBuf.Pv() == plineNewData->Pv() );

	//	get new key from copy buffer
	//
	keyNew.prefix.Nullify();
	keyNew.suffix.SetCb( sizeof( rgbNewKey ) );
	keyNew.suffix.SetPv( rgbNewKey );
	CallR( ErrRECRetrieveKeyFromCopyBuffer(
		pfucb, 
		pfcbIdx->Pidb(),
		&keyNew, 
		1,
		0,
		prceNil ) );
	CallS( ErrRECValidIndexKeyWarning( err ) );
	Assert( wrnFLDOutOfKeys != err );
	Assert( wrnFLDOutOfTuples != err );

	fCopyBufferKeyIsPresentInIndex = ( wrnFLDNotPresentInIndex != err );
		

	//	get the old key from the node
	//
	keyOld.prefix.Nullify();
	keyOld.suffix.SetCb( sizeof( rgbNewKey ) );
	keyOld.suffix.SetPv( rgbOldKey );

	Call( ErrRECRetrieveKeyFromRecord( pfucb, pfcbIdx->Pidb(), &keyOld, 1, 0, fTrue ) );

	CallS( ErrRECValidIndexKeyWarning( err ) );
	Assert( wrnFLDOutOfKeys != err );
	Assert( wrnFLDOutOfTuples != err );

	fRecordKeyIsPresentInIndex = ( wrnFLDNotPresentInIndex != err );

	if( fCopyBufferKeyIsPresentInIndex && !fRecordKeyIsPresentInIndex 
		|| !fCopyBufferKeyIsPresentInIndex && fRecordKeyIsPresentInIndex )
		{
		//  one is in the index and the other isn't (even though an indexed column may not have changed!)
		*pfChanged = fTrue;
		Assert( !Pcsr( pfucb )->FLatched() );
		return JET_errSuccess;
		}
	else if( !fCopyBufferKeyIsPresentInIndex && !fRecordKeyIsPresentInIndex )
		{
		//  neither are in the index (nothing has changed)
		*pfChanged = fFalse;
		Assert( !Pcsr( pfucb )->FLatched() );
		return JET_errSuccess;		
		}
		
	//	record must honor index no NULL segment requirements
	if ( pfcbIdx->Pidb()->FNoNullSeg() )
		{
		Assert( wrnFLDNullSeg != err );
		Assert( wrnFLDNullFirstSeg != err );
		Assert( wrnFLDNullKey != err );
		}

	*pfChanged = !FKeysEqual( keyOld, keyNew );

	Assert( !Pcsr( pfucb )->FLatched() );
	return JET_errSuccess;
	
HandleError:
	Assert( !Pcsr( pfucb )->FLatched() );
	return err;
	}



//	upgrades a ReplaceNoLock to a regular Replace by write-locking the record
ERR ErrRECUpgradeReplaceNoLock( FUCB *pfucb )
	{
	ERR		err;

	Assert( FFUCBReplaceNoLockPrepared( pfucb ) );
	
	//	UNDONE:	compute checksum on commit to level 0
	//			in support of following sequence:
	// 				BeginTransaction
	// 				PrepareUpdate, defer checksum since in xact
	// 				SetColumns
	// 				Commit to level 0, other user may update it
	// 				Update
	Assert( !FFUCBDeferredChecksum( pfucb )
		|| pfucb->ppib->level > 0 );
			
	CallR( ErrDIRGetLock( pfucb, writeLock ) );

	CallR( ErrDIRGet( pfucb ));
	const BOOL	fWriteConflict = ( !FFUCBDeferredChecksum( pfucb ) && !FFUCBCheckChecksum( pfucb ) );
	CallS( ErrDIRRelease( pfucb ));

	if ( fWriteConflict )
		{
		err = ErrERRCheck( JET_errWriteConflict );
		}
	else
		{
		UpgradeReplaceNoLock( pfucb );
		}

	return err;
	}


//+local
//	ErrRECIReplace
//	========================================================================
//	ErrRECIReplace( FUCB *pfucb, DIRFLAG dirflag )
//
//	Updates a record in a data file.	 All indexes on the data file are
// 	pdated to reflect the updated data record.
//
//	PARAMETERS	pfucb		 FUCB for file
//	RETURNS		Error code, one of the following:
//					 JET_errSuccess	  			 Everything went OK.
//					-NoCurrentRecord			 There is no current record
//									  			 to update.
//					-RecordNoCopy				 There is no working buffer
//									  			 to update from.
//					-KeyDuplicate				 The new record data causes an
//									  			 illegal duplicate index entry
//									  			 to be generated.
//					-RecordPrimaryChanged		 The new data causes the primary
//									  			 key to change.
//	SIDE EFFECTS
//		After update, file currency is left on the updated record.
//		Similar for index currency.
//		The effect of a GetNext or GetPrevious operation will be
//		the same in either case.  On failure, the currencies are
//		returned to their initial states.
//		If there is a working buffer for SetField commands,
//		it is discarded.
//
//	COMMENTS
//		If currency is not ON a record, the update will fail.
//		A transaction is wrapped around this function.	Thus, any
//		work done will be undone if a failure occurs.
//		For temporary files, transaction logging is deactivated
//		for the duration of the routine.
//		Index entries are not made for entirely-null keys.
//-
LOCAL ERR ErrRECIReplace( FUCB *pfucb, DIRFLAG dirflag )
	{
	ERR		err;					// error code of various utility
	PIB		*ppib				= pfucb->ppib;
	FCB		*pfcbTable;				// file's FCB
	FCB		*pfcbIdx;				// loop variable for each index on file
	FID		fidFixedLast;
	FID		fidVarLast;
	BOOL   	fUpdateIndex;
	BOOL	fUpdatingLatchSet	= fFalse;

	CheckPIB( ppib );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );

	//	should have been checked in PrepareUpdate
	//
	Assert( FFUCBUpdatable( pfucb ) );
	Assert( FFUCBReplacePrepared( pfucb ) );

	//	efficiency variables
	//
	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );

	//	data to use for update is in workBuf
	//
	Assert( !pfucb->dataWorkBuf.FNull() );
	
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	//	optimistic locking, ensure that record has
	//	not changed since PrepareUpdate
	//
	if ( FFUCBReplaceNoLockPrepared( pfucb ) )
		{
		Call( ErrRECUpgradeReplaceNoLock( pfucb ) );
		}

	// Do the BeforeReplace callback
	Call( ErrRECCallback( ppib, pfucb, JET_cbtypBeforeReplace, 0, NULL, NULL, 0 ) );
	
	Call( pfcbTable->ErrSetUpdatingAndEnterDML( ppib ) );
	fUpdatingLatchSet = fTrue;

	//	Set these efficiency variables after FUCB read latch
	//
	pfcbTable->AssertDML();
	fidFixedLast = pfcbTable->Ptdb()->FidFixedLast();
	fidVarLast = pfcbTable->Ptdb()->FidVarLast();

	//	if need to update indexes, then cache old record
	//
	fUpdateIndex = FRECIIndexPossiblyChanged( pfcbTable->Ptdb()->RgbitAllIndex(), pfucb->rgbitSet );

	pfcbTable->LeaveDML();
	
   	Assert( !Pcsr( pfucb )->FLatched() );
	if ( fUpdateIndex )
		{
		//	ensure primary key did not change
		//
		if ( pfcbTable->Pidb() != pidbNil )
			{
			BOOL	fIndexChanged;

		   	Call( ErrRECFIndexChanged( pfucb, pfcbTable, &fIndexChanged ) );
		   	Assert( !Pcsr( pfucb )->FLatched() );

			if ( fIndexChanged )
				{
				Error( ErrERRCheck( JET_errRecordPrimaryChanged ), HandleError )
				}
			}
		}
		
#ifdef DEBUG
	else
		{
		if ( pfcbTable->Ptdb() != ptdbNil  &&  pfcbTable->Pidb() != pidbNil )
			{
			BOOL	fIndexChanged;

		   	Call( ErrRECFIndexChanged( pfucb, pfcbTable, &fIndexChanged ) );
			Assert( !fIndexChanged );
			}
		}
#endif

	//	set version column if present
	//
	FID		fidVersion;

	Assert( FFUCBIndex( pfucb ) );
	pfcbTable->EnterDML();
	fidVersion = pfcbTable->Ptdb()->FidVersion();
	pfcbTable->LeaveDML();

	if ( fidVersion != 0 )
		{
		FCB				* pfcbT			= pfcbTable;
		const BOOL		fTemplateColumn	= pfcbTable->Ptdb()->FFixedTemplateColumn( fidVersion );
		const COLUMNID	columnidT		= ColumnidOfFid( fidVersion, fTemplateColumn );
		ULONG			ulT;
		DATA			dataField;
		
		Assert( !Pcsr( pfucb )->FLatched() );

		err =  ErrRECIAccessColumn( pfucb, columnidT );
		if ( err < 0 )
			{
			if ( JET_errColumnNotFound != err )
				goto HandleError;
			}
		
		//	get current record
		//
		Call( ErrDIRGet( pfucb ) );

		
		if ( fTemplateColumn )
			{
			Assert( FCOLUMNIDTemplateColumn( columnidT ) );
			if ( !pfcbT->FTemplateTable() )
				{
				// Switch to template table.
				pfcbT->Ptdb()->AssertValidDerivedTable();
				pfcbT = pfcbT->Ptdb()->PfcbTemplateTable();
				}
			else
				{
				pfcbT->Ptdb()->AssertValidTemplateTable();
				}
			}
		else
			{
			Assert( !FCOLUMNIDTemplateColumn( columnidT ) );
			Assert( !pfcbT->FTemplateTable() );
			}

		//	increment field from value in current record
		//
		pfcbT->EnterDML();
		
		err = ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfcbT->Ptdb(),
				columnidT,
				pfucb->kdfCurr.data,
				&dataField );
		if ( err < 0 )
			{
			pfcbT->LeaveDML();
			CallS( ErrDIRRelease( pfucb ) );
			goto HandleError;
			}

		//	handle case where field is NULL when column added
		//	to table with records present
		//
		if ( dataField.Cb() == 0 )
			{
			ulT = 1;
			}
		else
			{
			Assert( dataField.Cb() == sizeof(ULONG) );
			ulT = *(UnalignedLittleEndian< ULONG > *)dataField.Pv();
			ulT++;
			}
			
		dataField.SetPv( (BYTE *)&ulT );
		dataField.SetCb( sizeof(ulT) );
		err = ErrRECISetFixedColumn( pfucb, pfcbT->Ptdb(), columnidT, &dataField );
		
		pfcbT->LeaveDML();

		CallS( ErrDIRRelease( pfucb ) );
		Call( err );
		}

	Assert( !Pcsr( pfucb )->FLatched( ) );
	
	//	update indexes
	//
	if ( fUpdateIndex )
		{
#ifdef PREREAD_INDEXES_ON_REPLACE
		if( pfucb->u.pfcb->FPreread() )
			{
			const INT cSecondaryIndexesToPreread = 16;
	
			PGNO rgpgno[cSecondaryIndexesToPreread + 1];	//  NULL-terminated
			INT	ipgno = 0;
	
			// No critical section needed to guard index list because Updating latch
			// protects it.
			for ( pfcbIdx = pfcbTable->PfcbNextIndex();
				  pfcbIdx != pfcbNil && ipgno < cSecondaryIndexesToPreread;
				  pfcbIdx = pfcbIdx->PfcbNextIndex() )
				{
				if ( pfcbIdx->Pidb() != pidbNil
					 && FFILEIPotentialIndex( ppib, pfcbTable, pfcbIdx )
					 && FRECIIndexPossiblyChanged( pfcbIdx->Pidb()->RgbitIdx(), pfucb->rgbitSet ) )
					{
					//  preread this index as we will probably update it
					rgpgno[ipgno++] = pfcbIdx->PgnoFDP();
					}
				}
			rgpgno[ipgno] = pgnoNull;
			
			BFPrereadPageList( pfucb->ifmp, rgpgno );
			}
#endif	//	PREREAD_INDEXES_ON_REPLACE

		// No critical section needed to guard index list because Updating latch
		// protects it.
		for ( pfcbIdx = pfcbTable->PfcbNextIndex();
			pfcbIdx != pfcbNil;
			pfcbIdx = pfcbIdx->PfcbNextIndex() )
			{
			if ( pfcbIdx->Pidb() != pidbNil )		// sequential indexes don't need updating
				{
				if ( FFILEIPotentialIndex( ppib, pfcbTable, pfcbIdx ) )
					{
					if ( FRECIIndexPossiblyChanged( pfcbIdx->Pidb()->RgbitIdx(), pfucb->rgbitSet ) )
						{
						Call( ErrRECIUpdateIndex( pfucb, pfcbIdx, recoperReplace ) );
						}
#ifdef DEBUG
					else
						{
						BOOL	fIndexChanged;

					   	Call( ErrRECFIndexChanged( pfucb, pfcbIdx, &fIndexChanged ) );
						Assert( !fIndexChanged );
						}
#endif
					}
				}
			}
		}

	//	do the replace
	//
	Call( ErrDIRReplace( pfucb, pfucb->dataWorkBuf, fDIRLogColumnDiffs ) );
	
	pfcbTable->ResetUpdating();
	fUpdatingLatchSet = fFalse;

	//	if no error, commit transaction
	//
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

	FUCBResetUpdateFlags( pfucb );

	// Do the AfterReplace callback
	CallS( ErrRECCallback( ppib, pfucb, JET_cbtypAfterReplace, 0, NULL, NULL, 0 ) );

	AssertDIRNoLatch( ppib );

	return err;


HandleError:
	Assert( err < 0 );
	AssertDIRNoLatch( ppib );

	if ( fUpdatingLatchSet )
		{
		Assert( pfcbTable != pfcbNil );
		pfcbTable->ResetUpdating();
		}

	//	rollback all changes on error
	//
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	return err;
	}


INLINE ERR ErrRECIRetrieveKeyForReplace(
	FUCB * const		pfucb,
	const FCB * const	pfcbIdx,
	KEY * const			pkey,
	const ULONG			itagSequence,
	const ULONG			ichOffset,
	RCE * const			prcePrimary )
	{
	ERR					err;
	const BOOL			fReplaceByProxy		= ( prceNil != prcePrimary );

	if ( fReplaceByProxy )
		{
		DATA	dataRec;

		Assert( pfcbNil == pfcbIdx->PfcbTable() );		// Index not linked in yet.

		// Before-image is stored in RCE.
		Assert( operReplace == prcePrimary->Oper() );
		dataRec.SetPv( const_cast<BYTE *>( prcePrimary->PbData() ) + cbReplaceRCEOverhead );
		dataRec.SetCb( prcePrimary->CbData() - cbReplaceRCEOverhead );
		CallR( ErrRECIRetrieveKey(
					pfucb,
					pfcbIdx->Pidb(),
					dataRec,
					pkey,
					itagSequence,
					ichOffset,
					fTrue,
					prcePrimary ) );
		}
	else
		{
		Assert( pfcbIdx->PfcbTable() == pfucb->u.pfcb );
		CallR( ErrRECRetrieveKeyFromRecord(
					pfucb,
					pfcbIdx->Pidb(),
					pkey,
					itagSequence,
					ichOffset,
					fTrue ) );
		}

	CallS( ErrRECValidIndexKeyWarning( err ) );
	return err;
	}
	

//+local
// ErrRECIReplaceInIndex
// ========================================================================
// ERR ErrRECIReplaceInIndex( FCB *pfcbIdx, FUCB *pfucb )
//
// Extracts keys from old and new data records, and if they are different,
// adds the new index entry and deletes the old index entry.
//
// PARAMETERS
//				pfcbIdx				  		FCB of index to insert into
//				pfucb						record FUCB pointing to primary record changed
//
// RETURNS		JET_errSuccess, or error code from failing routine
//
// SIDE EFFECTS 
// SEE ALSO		Replace
//-
ERR ErrRECIReplaceInIndex(
	FUCB			*pfucb,
	FUCB			*pfucbIdx,
	BOOKMARK		*pbmPrimary,
	RCE				*prcePrimary )
	{
	ERR				err;								// error code of various utility
	const FCB		* const pfcbIdx	= pfucbIdx->u.pfcb;
	const IDB		* const pidb	= pfcbIdx->Pidb();
	KEY				keyOld;				  				// key extracted from old record
	BYTE  	 		rgbOldKey[JET_cbSecondaryKeyMost];	// buffer for old key
	KEY				keyNew;				  				// key extracted from new record
	BYTE 	  		rgbNewKey[JET_cbSecondaryKeyMost];	// buffer for new key
	ULONG 	 		itagSequenceOld; 					// used to extract keys
	ULONG 	 		itagSequenceNew;					// used to extract keys
	BOOL  	 		fMustDelete;						// record no longer generates key
	BOOL 	 		fMustAdd;							// record now generates this key
	BOOL  	 		fDoOldNullKey;
	BOOL  	 		fDoNewNullKey;
	const BOOL		fHasMultivalue	= pidb->FMultivalued();
	const BOOL		fAllowAllNulls	= pidb->FAllowAllNulls();
	const BOOL		fAllowFirstNull = pidb->FAllowFirstNull();
	const BOOL		fAllowSomeNulls = pidb->FAllowSomeNulls();
	const BOOL		fNoNullSeg		= pidb->FNoNullSeg();
	const BOOL		fUnique			= pidb->FUnique();
	BOOL			fIndexUpdated	= fFalse;

	AssertDIRNoLatch( pfucb->ppib );
	
	Assert( pfcbIdx != pfcbNil );
	Assert( pfcbIdx->FTypeSecondaryIndex() );
	Assert( pidbNil != pidb );
	
	Assert( !( fNoNullSeg && ( fAllowAllNulls || fAllowSomeNulls ) ) );

	// if fAllowNulls, then fAllowSomeNulls needs to be true
	//
	Assert( !fAllowAllNulls || fAllowSomeNulls );

	keyOld.prefix.Nullify();
	keyOld.suffix.SetPv( rgbOldKey );
	keyNew.prefix.Nullify();
	keyNew.suffix.SetPv( rgbNewKey );

	//	delete the old key from the index 
	//
	fDoOldNullKey = fFalse;
	for ( itagSequenceOld = 1; ; itagSequenceOld++ )
		{
		CallR( ErrRECIRetrieveKeyForReplace(
					pfucb,
					pfcbIdx,
					&keyOld,
					itagSequenceOld,
					0,
					prcePrimary ) );

		if ( wrnFLDOutOfKeys == err )
			{
			Assert( itagSequenceOld > 1 );
			break;
			}

		else if ( wrnFLDOutOfTuples == err )
			{
			//	try next itagSequence
			Assert( pidb->FTuples() );
			if ( fHasMultivalue )
				continue;
			else
				break;
			}

		else if ( wrnFLDNotPresentInIndex == err )
			{
			//  original record was not in this index. no need to remove it
			Assert( 1 == itagSequenceOld );
///			AssertSz( fFalse, "[laurionb]: ErrRECIReplaceInIndex: original record was not in index" );
			break;
			}

		//	record must honor index no NULL segment requirements
		//
		if ( fNoNullSeg )
			{
			Assert( wrnFLDNullSeg != err );
			Assert( wrnFLDNullFirstSeg != err );
			Assert( wrnFLDNullKey != err );
			}

		if ( wrnFLDNullKey == err )
			{
			Assert( 1 == itagSequenceOld );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			Assert( !pidb->FTuples() );		//	tuple indexes would have generated wrnFLDOutOfTuples instead
			if ( !fAllowAllNulls )
				break;
			fDoOldNullKey = fTrue;
			}
		else if ( ( wrnFLDNullFirstSeg == err && !fAllowFirstNull )
				|| ( wrnFLDNullSeg == err && !fAllowSomeNulls ) )
			{
			break;
			}

		//	UNDONE: for tuple indexes, skip check to see if we have to delete
		//	(we will unconditionally delete)
		const BOOL	fPerformDeleteCheck		= !pidb->FTuples();
		fMustDelete = fTrue;	
		fDoNewNullKey = fFalse;
		for ( itagSequenceNew = 1; fPerformDeleteCheck; itagSequenceNew++ )
			{
			//	extract key from new data in copy buffer
			//
			Assert( prceNil != prcePrimary ?
					pfcbIdx->PfcbTable() == pfcbNil :		// Index not linked in yet.
					pfcbIdx->PfcbTable() == pfucb->u.pfcb );
			CallR( ErrRECRetrieveKeyFromCopyBuffer(
				pfucb, 
				pidb,
				&keyNew,
				itagSequenceNew,
				0,
				prcePrimary ) );

			CallS( ErrRECValidIndexKeyWarning( err ) );

			if ( wrnFLDOutOfKeys == err )
				{
				Assert( itagSequenceNew > 1 );
				break;
				}

			else if ( wrnFLDOutOfTuples == err )
				{
				//	try next itagSequence
				Assert( pidb->FTuples() );
				if ( fHasMultivalue )
					continue;
				else
					break;
				}

			else if ( wrnFLDNotPresentInIndex == err )
				{
				//  new entry not present in the index. always delete the old one
				Assert( fMustDelete );
				Assert( 1 == itagSequenceNew );
				break;
				}

			if ( wrnFLDNullKey == err )
				{
				Assert( 1 == itagSequenceNew );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
				Assert( !pidb->FTuples() );		//	tuple indexes would have generated wrnFLDOutOfTuples instead
				if ( !fAllowAllNulls )
					break;
				fDoNewNullKey = fTrue;
				}
			else if ( ( wrnFLDNullFirstSeg == err && !fAllowFirstNull )
					|| ( wrnFLDNullSeg == err && !fAllowSomeNulls ) )
				{
				break;
				}

			if ( FKeysEqual( keyOld, keyNew ) )
				{
				//	the existing key matches one of the new keys,
				//	so no need to delete it
				fMustDelete = fFalse;
				break;
				}
			else if ( !fHasMultivalue || fDoNewNullKey )
				{
				//	if no multivalues in this index, there's no point going beyond first itagSequence
				//	if key is null, this implies there are no further multivalues
				Assert( 1 == itagSequenceNew );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
				break;
				}
			}

		Assert( fMustDelete || !pidb->FTuples() );
		if ( fMustDelete )
			{
			//	move to DATA root.  Seek to index entry.
			//
			DIRGotoRoot( pfucbIdx );
			Assert( pbmPrimary->key.prefix.FNull() );
			Assert( pbmPrimary->key.Cb() > 0 );
			CallR( ErrDIRDownKeyData( pfucbIdx, keyOld, pbmPrimary->key.suffix ) );

			err = ErrDIRDelete( pfucbIdx, fDIRNull, prcePrimary );
			if ( err < 0 )
				{
				if ( JET_errRecordDeleted == err )
					{
					if ( itagSequenceOld > 1 && !fUnique )
						{
						Assert( fHasMultivalue );

						//	must have been record with multi-value column
						//	or tuples with sufficiently similar values
						//	(ie. the indexed portion of the multi-values
						//	or tuples were identical) to produce redundant
						//	index entries.
						err = JET_errSuccess;
						}
					else
						{
						RECIReportIndexCorruption( pfcbIdx );
						AssertSz( fFalse, "JET_errSecondaryIndexCorrupted during a replace" );
						err = ErrERRCheck( JET_errSecondaryIndexCorrupted );
						}
					}
				CallR( err );
				}
			else
				{
				fIndexUpdated = fTrue;
				}
			}

		if ( pidb->FTuples() )
			{
			//	at minimum, index with itagSequence==1 and ibSequence==0
			//	already got deleted, since we don't currently do
			//	FMustDelete optimisation for tuple indexes
			Assert( fIndexUpdated );

			for ( ULONG ichOffset = 1; ichOffset < pidb->ChTuplesToIndexMax(); ichOffset++ )
				{
				CallR( ErrRECIRetrieveKeyForReplace(
							pfucb,
							pfcbIdx,
							&keyOld,
							itagSequenceOld,
							ichOffset,
							prcePrimary ) );

				//	all other warnings should have been detected
				//	when we retrieved with ichOffset==0
				CallSx( err, wrnFLDOutOfTuples );
				if ( JET_errSuccess != err )
					break;

				//	move to DATA root.  Seek to index entry.
				//
				DIRGotoRoot( pfucbIdx );
				Assert( pbmPrimary->key.prefix.FNull() );
				Assert( pbmPrimary->key.Cb() > 0 );
				CallR( ErrDIRDownKeyData( pfucbIdx, keyOld, pbmPrimary->key.suffix ) );

				err = ErrDIRDelete( pfucbIdx, fDIRNull, prcePrimary );
				if ( JET_errRecordDeleted == err )
					{
					//	must have been record with multi-value column
					//	or tuples with sufficiently similar values
					//	(ie. the indexed portion of the multi-values
					//	or tuples were identical) to produce redundant
					//	index entries.
					err = JET_errSuccess;
					}
				CallR( err );
				}
			}

		if ( !fHasMultivalue || fDoOldNullKey )
			{
			//	if no multivalues in this index, there's no point going beyond first itagSequence
			//	if key is null, this implies there are no further multivalues
			Assert( 1 == itagSequenceOld );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			break;
			}
		}

	fDoNewNullKey = fFalse;
	for ( itagSequenceNew = 1; ; itagSequenceNew++ )
		{
		//	extract key from new data in copy buffer
		//
		Assert( prceNil != prcePrimary ?
				pfcbIdx->PfcbTable() == pfcbNil :		// Index not linked in yet.
				pfcbIdx->PfcbTable() == pfucb->u.pfcb );
		CallR( ErrRECRetrieveKeyFromCopyBuffer(
			pfucb, 
			pidb, 
			&keyNew, 
			itagSequenceNew,
			0,
			prcePrimary ) );

		CallS( ErrRECValidIndexKeyWarning( err ) );

		if ( wrnFLDOutOfKeys == err )
			{
			Assert( itagSequenceNew > 1 );
			break;
			}

		else if ( wrnFLDOutOfTuples == err )
			{
			//	try next itagSequence
			Assert( pidb->FTuples() );
			if ( fHasMultivalue )
				continue;
			else
				break;
			}

		else if ( wrnFLDNotPresentInIndex == err )
			{
			//  new record was not in this index
			Assert( 1 == itagSequenceNew );
			break;
			}

		if ( fNoNullSeg && ( wrnFLDNullSeg == err || wrnFLDNullFirstSeg == err || wrnFLDNullKey == err ) )
			{
			err = ErrERRCheck( JET_errNullKeyDisallowed );
			return err;
			}

		if ( wrnFLDNullKey == err )
			{
			Assert( 1 == itagSequenceNew );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			Assert( !pidb->FTuples() );		//	tuple indexes would have generated wrnFLDOutOfTuples instead
			if ( !fAllowAllNulls )
				break;
			fDoNewNullKey = fTrue;
			}
		else if ( ( wrnFLDNullFirstSeg == err && !fAllowFirstNull )
				|| ( wrnFLDNullSeg == err && !fAllowSomeNulls ) )
			{
			break;
			}
			

		//	UNDONE: for tuple indexes, skip check to see if we have to add
		//	(we will unconditionally add)
		const BOOL	fPerformAddCheck	= !pidb->FTuples();
		fMustAdd = fTrue;
		fDoOldNullKey = fFalse;
		for ( itagSequenceOld = 1; fPerformAddCheck; itagSequenceOld++ )
			{
			CallR( ErrRECIRetrieveKeyForReplace(
						pfucb,
						pfcbIdx,
						&keyOld,
						itagSequenceOld,
						0,
						prcePrimary ) );

			if ( wrnFLDOutOfKeys == err )
				{
				Assert( itagSequenceOld > 1 );
				break;
				}

			else if ( wrnFLDOutOfTuples == err )
				{
				//	try next itagSequence
				Assert( pidb->FTuples() );
				if ( fHasMultivalue )
					continue;
				else
					break;
				}

			else if ( wrnFLDNotPresentInIndex == err )
				{
				//  the old record was not present in the index. the new one is (or we would bail out above )
				Assert( fMustAdd );
				Assert( 1 == itagSequenceOld );
				break;
				}
				
			//	record must honor index no NULL segment requirements
			//
			if ( fNoNullSeg )
				{
				Assert( wrnFLDNullSeg != err );
				Assert( wrnFLDNullFirstSeg != err );
				Assert( wrnFLDNullKey != err );
				}

			if ( wrnFLDNullKey == err )
				{
				Assert( 1 == itagSequenceOld );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
				Assert( !pidb->FTuples() );		//	tuple indexes would have generated wrnFLDOutOfTuples instead
				if ( !fAllowAllNulls )
					break;
				fDoOldNullKey = fTrue;
				}
			else if ( ( wrnFLDNullFirstSeg == err && !fAllowFirstNull )
					|| ( wrnFLDNullSeg == err && !fAllowSomeNulls ) )
				{
				break;
				}

			if ( FKeysEqual( keyOld, keyNew ) )
				{
				//	the new key matches one of the existing old keys,
				//	so no need to add it
				fMustAdd = fFalse;
				break;
				}

			else if ( !fHasMultivalue || fDoOldNullKey )
				{
				//	if no multivalues in this index, there's no point going beyond first itagSequence
				//	if key is null, this implies there are no further multivalues
				Assert( 1 == itagSequenceOld );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
				break;
				}
			}

		Assert( fMustAdd || !pidb->FTuples() );
		if ( fMustAdd )
			{
			//	move to DATA root and insert new index entry.
			//
			DIRGotoRoot( pfucbIdx );
			Assert( pbmPrimary->key.prefix.FNull() );
			Assert( pbmPrimary->key.Cb() > 0 );

			err = ErrDIRInsert(
						pfucbIdx,
						keyNew,
						pbmPrimary->key.suffix,
						fDIRBackToFather,
						prcePrimary );
			if ( err < 0 )
				{
				if ( JET_errMultiValuedIndexViolation == err )
					{
					if ( itagSequenceNew > 1 && !fUnique )
						{
						Assert( fHasMultivalue );

						//	must have been record with multi-value column
						//	or tuples with sufficiently similar values
						//	(ie. the indexed portion of the multi-values
						//	or tuples were identical) to produce redundant
						//	index entries.
						err = JET_errSuccess;
						}
					else
						{
						RECIReportIndexCorruption( pfcbIdx );
						AssertSz( fFalse, "JET_errSecondaryIndexCorrupted during a replace" );
						err = ErrERRCheck( JET_errSecondaryIndexCorrupted );
						}
					}
				CallR( err );
				}
			else
				{
				fIndexUpdated = fTrue;
				}
			}

		if ( pidb->FTuples() )
			{
			//	at minimum, index entry with itagSequence==1 and ichOffset==0
			//	already got added, since we don't currently do
			//	FMustAdd optimisation for tuple indexes
			Assert( fIndexUpdated );

			for ( ULONG ichOffset = 1; ichOffset < pidb->ChTuplesToIndexMax(); ichOffset++ )
				{
				CallR( ErrRECRetrieveKeyFromCopyBuffer(
								pfucb, 
								pidb, 
								&keyNew, 
								itagSequenceNew,
								ichOffset,
								prcePrimary ) );

				//	all other warnings should have been detected
				//	when we retrieved with ichOffset==0
				CallSx( err, wrnFLDOutOfTuples );
				if ( JET_errSuccess != err )
					break;

				//	move to DATA root and insert new index entry.
				//
				DIRGotoRoot( pfucbIdx );
				Assert( pbmPrimary->key.prefix.FNull() );
				Assert( pbmPrimary->key.Cb() > 0 );

				err = ErrDIRInsert(
							pfucbIdx,
							keyNew,
							pbmPrimary->key.suffix,
							fDIRBackToFather,
							prcePrimary );
				if ( JET_errMultiValuedIndexViolation == err )
					{
					//	must have been record with multi-value column
					//	or tuples with sufficiently similar values
					//	(ie. the indexed portion of the multi-values
					//	or tuples were identical) to produce redundant
					//	index entries.
					err = JET_errSuccess;
					}
				CallR( err );
				}
			}

		if ( !fHasMultivalue || fDoNewNullKey )
			{
			//	if no multivalues in this index, there's no point going beyond first itagSequence
			//	if key is null, this implies there are no further multivalues
			Assert( 1 == itagSequenceNew );	//	nulls beyond itagSequence 1 would have generated wrnFLDOutOfKeys
			break;
			}
		}


	//	suppress warnings
	CallS( ErrRECValidIndexKeyWarning( err ) );
	err = ( fIndexUpdated ? ErrERRCheck( wrnFLDIndexUpdated ) : JET_errSuccess );
	return err;
	}


//  ================================================================
LOCAL ERR ErrRECIEscrowUpdate(
	PIB				*ppib,
	FUCB			*pfucb,
	const COLUMNID	columnid,
	const VOID		*pv,
	ULONG			cbMax,
	VOID			*pvOld,
	ULONG			cbOldMax,
	ULONG			*pcbOldActual,
	JET_GRBIT		grbit )
//  ================================================================
	{
	ERR			err			= JET_errSuccess;
	BOOL		fCommit		= fFalse;
	FIELD		fieldFixed;

	CallR( ErrRECIAccessColumn( pfucb, columnid, &fieldFixed ) );

	if ( FFUCBUpdatePrepared( pfucb ) )
		{
		return ErrERRCheck( JET_errAlreadyPrepared );
		}

	if ( sizeof( LONG ) != cbMax )
		{
		return ErrERRCheck( JET_errInvalidBufferSize );
		}

	if ( pvOld && 0 != cbOldMax && sizeof(LONG) > cbOldMax )
		{
		return ErrERRCheck( JET_errInvalidBufferSize );
		}

	if ( !FCOLUMNIDFixed( columnid ) )
		{
		return ErrERRCheck( JET_errInvalidOperation );
		}
	
	//	assert against client misbehaviour - EscrowUpdating a record while
	//	another cursor of the same session has an update pending on the record
	CallR( ErrRECSessionWriteConflict( pfucb ) );
	
	FCB	*pfcb = pfucb->u.pfcb;

	if ( FCOLUMNIDTemplateColumn( columnid ) )
		{
		if ( !pfcb->FTemplateTable() )
			{
			// Switch to template table.
			pfcb->Ptdb()->AssertValidDerivedTable();
			pfcb = pfcb->Ptdb()->PfcbTemplateTable();
			}
		else
			{
			pfcb->Ptdb()->AssertValidTemplateTable();
			}
		}

	if ( !FFIELDEscrowUpdate( fieldFixed.ffield ) )
		{
		return ErrERRCheck( JET_errInvalidOperation );
		}
	Assert( FCOLUMNIDFixed( columnid ) );

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

#ifdef DEBUG
	err = ErrDIRGet( pfucb );
	if ( err >= 0 )
		{
		const REC * const prec = (REC *)pfucb->kdfCurr.data.Pv();
		Assert( prec->FidFixedLastInRec() >= FidOfColumnid( columnid ) );
		CallS( ErrDIRRelease( pfucb ) );
		}
	Assert( !Pcsr( pfucb )->FLatched() );
#endif

	//  ASSERT: the offset is represented in the record

	DIRFLAG dirflag = fDIRNull;
	if ( JET_bitEscrowNoRollback & grbit )
		{
		dirflag |= fDIRNoVersion;
		}
	if( FFIELDFinalize( fieldFixed.ffield ) )
		{
		dirflag |= fDIRFinalize;
		}

	err = ErrDIRDelta(
			pfucb,
			fieldFixed.ibRecordOffset,
			pv,
			cbMax,
			pvOld,
			cbOldMax,
			pcbOldActual,
			dirflag );
	
	if ( err >= 0 )
		{
		err = ErrDIRCommitTransaction( ppib, NO_GRBIT );
		}
	if ( err < 0 )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	return err;
	}


//  ================================================================
ERR VTAPI ErrIsamEscrowUpdate(
  	JET_SESID		sesid,
	JET_VTID		vtid,
	JET_COLUMNID	columnid,
	VOID 			*pv,
	ULONG 			cbMax,
	VOID 			*pvOld,
	ULONG 			cbOldMax,
	ULONG			*pcbOldActual,
	JET_GRBIT		grbit )
//  ================================================================
	{
 	PIB * const ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB * const pfucb	= reinterpret_cast<FUCB *>( vtid );

	ERR			err = JET_errSuccess;

	if( ppib->level <= 0 )
		{
		return JET_errNotInTransaction;
		}
		
	CallR( ErrPIBCheck( ppib ) );
	AssertDIRNoLatch( ppib );
	CheckTable( ppib, pfucb );
	CheckSecondary( pfucb );
	
	//	ensure that table is updatable
	//
	CallR( ErrFUCBCheckUpdatable( pfucb )  );
	CallR( ErrPIBCheckUpdatable( ppib ) );

	err = ErrRECIEscrowUpdate(
			ppib,
			pfucb,
			columnid,
			pv,
			cbMax,
			pvOld,
			cbOldMax,
			pcbOldActual,
			grbit );

	AssertDIRNoLatch( ppib );
	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\sort.cxx ===
#include "std.hxx"

//  SORT internal functions

INLINE VOID SrecToKeydataflags( const SREC * psrec, FUCB * pfucb );
LOCAL LONG IspairSORTISeekByKey( const BYTE * rgbRec, const SPAIR * rgspair, LONG ispairMac, const KEY * pkey, BOOL fGT );
INLINE INT ISORTICmpKey( const SREC * psrec1, const SREC * psrec2 );
INLINE INT ISORTICmpKeyData( const SREC * psrec1, const SREC * psrec2 );
INLINE BOOL FSORTIDuplicate( SCB* const pscb, const SREC * psrec1, const SREC * psrec2 );
LOCAL LONG CspairSORTIUnique( SCB* pscb, BYTE * rgbRec, SPAIR * rgspair, const LONG ispairMac );
LOCAL ERR ErrSORTIOutputRun( SCB * pscb );
INLINE INT ISORTICmpPspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 );
LOCAL INT ISORTICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 );
INLINE VOID SWAPPspair( SPAIR **ppspair1, SPAIR **ppspair2 );
INLINE VOID SWAPSpair( SPAIR *pspair1, SPAIR *pspair2 );
INLINE VOID SWAPPsrec( SREC **ppsrec1, SREC **ppsrec2 );
INLINE VOID SWAPPmtnode( MTNODE **ppmtnode1, MTNODE **ppmtnode2 );
LOCAL VOID SORTIInsertionSort( SCB *pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn );
LOCAL VOID SORTIQuicksort( SCB * pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn );
LOCAL ERR ErrSORTIRunStart( SCB *pscb, LONG cb, LONG crec, RUNINFO *pruninfo );
LOCAL ERR ErrSORTIRunInsert( SCB *pscb, RUNINFO* pruninfo, SREC *psrec );
INLINE VOID SORTIRunEnd( SCB * pscb, RUNINFO* pruninfo );
INLINE VOID SORTIRunDelete( SCB * pscb, const RUNINFO * pruninfo );
LOCAL VOID	SORTIRunDeleteList( SCB *pscb, RUNLINK **pprunlink, LONG crun );
LOCAL VOID	SORTIRunDeleteListMem( SCB *pscb, RUNLINK **pprunlink, LONG crun );
LOCAL ERR ErrSORTIRunOpen( SCB *pscb, RUNINFO *pruninfo, RCB **pprcb );
LOCAL ERR ErrSORTIRunNext( RCB * prcb, SREC **ppsrec );
LOCAL VOID SORTIRunClose( RCB *prcb );
INLINE ERR ErrSORTIRunReadPage( RCB *prcb, PGNO pgno, LONG ipbf );
LOCAL ERR ErrSORTIMergeToRun( SCB *pscb, RUNLINK *prunlinkSrc, RUNLINK **pprunlinkDest );
LOCAL ERR ErrSORTIMergeStart( SCB *pscb, RUNLINK *prunlinkSrc );
LOCAL ERR ErrSORTIMergeFirst( SCB *pscb, SREC **ppsrec );
LOCAL ERR ErrSORTIMergeNext( SCB *pscb, SREC **ppsrec );
LOCAL VOID SORTIMergeEnd( SCB *pscb );
LOCAL ERR ErrSORTIMergeNextChamp( SCB *pscb, SREC **ppsrec );
INLINE VOID SORTIOptTreeInit( SCB *pscb );
LOCAL ERR ErrSORTIOptTreeAddRun( SCB *pscb, RUNINFO *pruninfo );
LOCAL ERR ErrSORTIOptTreeMerge( SCB *pscb );
INLINE VOID SORTIOptTreeTerm( SCB *pscb );
LOCAL ERR ErrSORTIOptTreeBuild( SCB *pscb, OTNODE **ppotnode );
LOCAL ERR ErrSORTIOptTreeMergeDF( SCB *pscb, OTNODE *potnode, RUNLINK **pprunlink );
LOCAL VOID SORTIOptTreeFree( SCB *pscb, OTNODE *potnode );

//----------------------------------------------------------
//  put the current sort record into the FUCB
//----------------------------------------------------------
INLINE VOID SrecToKeydataflags( const SREC * psrec, FUCB * pfucb )
	{ 
	pfucb->locLogical				= locOnCurBM;				//  CSR on record
	pfucb->kdfCurr.key.prefix.Nullify();
	pfucb->kdfCurr.key.suffix.SetCb( CbSRECKeyPsrec( psrec ) );	//  size of key
	pfucb->kdfCurr.key.suffix.SetPv( PbSRECKeyPsrec( psrec ) );	//  key
	pfucb->kdfCurr.data.SetCb( CbSRECDataPsrec( psrec ) );		//  size of data
	pfucb->kdfCurr.data.SetPv( PbSRECDataPsrec( psrec ) );		//  data
	pfucb->kdfCurr.fFlags			= 0;
	}

//----------------------------------------------------------
//	ErrSORTOpen( PIB *ppib, FUCB **pfucb, INT fFlags )
//
//	This function returns a pointer to an FUCB which can be 
//	use to add records to a collection of records to be sorted.  
//	Then the records can be retrieved in sorted order.  
//	
//	The fFlags fUnique flag indicates that records with duplicate
//	keys should be eliminated.
//----------------------------------------------------------

ERR ErrSORTOpen( PIB *ppib, FUCB **ppfucb, const BOOL fRemoveDuplicateKey, const BOOL fRemoveDuplicateKeyData )
	{
	ERR		err			= JET_errSuccess;
	FUCB   	* pfucb		= pfucbNil;
	SCB		* pscb		= pscbNil;
	SPAIR	* rgspair	= 0;
	BYTE	* rgbRec	= 0;

	/*	allocate a new SCB
	/**/
	INST *pinst = PinstFromPpib( ppib );
	IFMP ifmpTemp = pinst->m_mpdbidifmp[ dbidTemp ];
	CallR( ErrFUCBOpen( ppib, ifmpTemp, &pfucb ) );
	if ( ( pscb = PscbMEMAlloc( pinst ) ) == pscbNil )
		Error( ErrERRCheck( JET_errTooManySorts ), HandleError );

	Assert( FAlignedForAllPlatforms( pscb ) );

	/*	initialize sort context to insert mode
	/**/
	FCBInitFCB( &pscb->fcb );
	new( &pscb->fcb ) FCB( ifmpTemp, pgnoNull );
	pscb->fcb.SetTypeSort();
	pscb->fcb.SetFixedDDL();
	pscb->fcb.SetPrimaryIndex();
	pscb->fcb.SetSequentialIndex();

	//	finish the initialization of this FCB
	
	pscb->fcb.CreateComplete();

	FUCBSetSort( pfucb );
	SCBSetInsert( pscb );
	SCBResetRemoveDuplicateKey( pscb );
	SCBResetRemoveDuplicateKeyData( pscb );
	if ( fRemoveDuplicateKey )
		{
		SCBSetRemoveDuplicateKey( pscb );
		}
	if ( fRemoveDuplicateKeyData )
		{
		SCBSetRemoveDuplicateKeyData( pscb );
		}
	pscb->cRecords	= 0;

	/*	allocate sort pair buffer and record buffer
	/**/
	if ( !( rgspair = ( SPAIR * )( PvOSMemoryPageAlloc( cbSortMemFastUsed, NULL ) ) ) )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
	pscb->rgspair	= rgspair;
	if ( !( rgbRec = ( BYTE * )( PvOSMemoryPageAlloc( cbSortMemNormUsed, NULL ) ) ) )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
	pscb->rgbRec	= rgbRec;

	/*	initialize sort pair buffer
	/**/
	pscb->ispairMac	= 0;

	/*	initialize record buffer
	/**/
	pscb->irecMac	= 0;
	pscb->crecBuf	= 0;
	pscb->cbData	= 0;

	/*	reset run count to zero
	/**/
	pscb->crun = 0;

	/*	link FUCB to FCB in SCB
	/**/
	pscb->fcb.Link( pfucb );

	/*	defer allocating space for a disk merge as well as initializing for a
	/*	 merge until we are forced to perform one
	/**/
	Assert( pscb->fcb.PgnoFDP() == pgnoNull );

	/*	return initialized FUCB
	/**/
	*ppfucb = pfucb;

	err = JET_errSuccess;

HandleError:
	if ( JET_errSuccess != err )
		{
		if ( rgbRec != NULL )
			{
			OSMemoryPageFree( rgbRec );
			}
		if ( rgspair != NULL )
			{
			OSMemoryPageFree( rgspair );
			}
		if ( pscb != pscbNil )
			{
			MEMReleasePscb( pinst, pscb );
			}
		if ( pfucb != pfucbNil )
			{
			FUCBClose( pfucb );
			}
		}

	return err;
	}


//----------------------------------------------------------
//	ErrSORTInsert
//
//	Add the record represented by key-data 
//	to the collection of sort records.
//	Here, data is the primary key bookmark
//----------------------------------------------------------

ERR ErrSORTInsert( FUCB *pfucb, const KEY& key, const DATA& data )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	LONG	cbKey;
	LONG	cbData;
	LONG	irec;
	SREC	* psrec			= 0;
	SPAIR	* pspair		= 0;
	BYTE	* pbSrc			= 0;
	BYTE	* pbSrcMac		= 0;
	BYTE	* pbDest		= 0;
	BYTE	* pbDestMic		= 0;
	ERR		err				= JET_errSuccess;

	//  check input and input mode
	Assert( key.Cb() <= KEY::cbKeyMax );	// may be secondary key if called from BuildIndex
	Assert( FSCBInsert( pscb ) );

	//  check SCB
	
	Assert( pscb->crecBuf <= cspairSortMax );
	Assert( pscb->irecMac <= irecSortMax );

	//  calculate required normal memory/record indexes to store this record

	INT cbNormNeeded = CbSRECSizeCbCb( key.Cb(), data.Cb() );
	INT cirecNeeded = CirecToStoreCb( cbNormNeeded );

	//  if we are out of fast or normal memory, output a run
	
	if (	pscb->irecMac * cbIndexGran + cbNormNeeded > cbSortMemNormUsed ||
			pscb->crecBuf == cspairSortMax )
		{
		//  sort previously inserted records into a run

		SORTIQuicksort( pscb, pscb->rgspair, pscb->rgspair + pscb->ispairMac );

		//  move the new run to disk
		
		Call( ErrSORTIOutputRun( pscb ) );
		}

	//  create and add the sort record for this record

	irec = pscb->irecMac;
	psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
	pscb->irecMac += cirecNeeded;
	pscb->crecBuf++;
	pscb->cbData += cbNormNeeded;
	SRECSizePsrecCb( psrec, cbNormNeeded );
	SRECKeySizePsrecCb( psrec, key.Cb() );
	key.CopyIntoBuffer( PbSRECKeyPsrec( psrec ) ); 
	UtilMemCpy( PbSRECDataPsrec( psrec ), data.Pv(), data.Cb() );

	//  create and add the sort pair for this record

	//  get new SPAIR pointer and advance SPAIR counter

	pspair = pscb->rgspair + pscb->ispairMac++;

	//  copy key into prefix buffer BACKWARDS for fast compare

	cbKey = CbSRECKeyPsrec( psrec );
	pbSrc = PbSRECKeyPsrec( psrec );
	pbSrcMac = pbSrc + min( cbKey, cbKeyPrefix );
	pbDest = pspair->rgbKey + cbKeyPrefix - 1;

	while ( pbSrc < pbSrcMac )
		*( pbDest-- ) = *( pbSrc++ );

	//  do we have any unused buffer space?

	if ( pbDest >= pspair->rgbKey )
		{
		//  copy data into prefix buffer BACKWARDS for fast compare

		cbData = (LONG)min( pbDest - pspair->rgbKey + 1, CbSRECDataPsrec( psrec ) );
		pbSrc = PbSRECDataPsrec( psrec );
		pbDestMic = max( pspair->rgbKey, pbDest - cbData + 1 );

		while ( pbDest >= pbDestMic )
			*( pbDest-- ) = *( pbSrc++ );

		//  zero any remaining key space

		if ( pbDest >= pspair->rgbKey )
			memset( pspair->rgbKey, 0, pbDest - pspair->rgbKey + 1 );
		}


	//  set compressed pointer to full record
	
	pspair->irec = (USHORT) irec;

	//  keep track of record count

	pscb->cRecords++;

	//  check SCB
	
	Assert( pscb->crecBuf <= cspairSortMax );
	Assert( pscb->irecMac <= irecSortMax );

HandleError:
	return err;
	}


//----------------------------------------------------------
//	ErrSORTEndInsert
//
//	This function is called to indicate that no more records 
//	will be added to the sort.  It performs all work that needs 
//	to be done before the first record can be retrieved.  
//----------------------------------------------------------

ERR ErrSORTEndInsert( FUCB *pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err		= JET_errSuccess;

	//  verify insert mode

	Assert( FSCBInsert( pscb ) );

	//  deactivate insert mode
	
	SCBResetInsert( pscb );

	//  move CSR to before the first record (if any)
	SORTBeforeFirst( pfucb );	

	//  if we have no records, we're done

	if ( !pscb->cRecords )
		return JET_errSuccess;

	//  sort records in memory

	SORTIQuicksort( pscb, pscb->rgspair, pscb->rgspair + pscb->ispairMac );

	//  do we have any runs on disk?

	if ( pscb->crun )
		{
		//	empty sort buffer into final run

		Call( ErrSORTIOutputRun( pscb ) );

		//  free sort memory

		OSMemoryPageFree( pscb->rgspair );
		pscb->rgspair = NULL;
		OSMemoryPageFree( pscb->rgbRec );
		pscb->rgbRec = NULL;
		
		//	perform all but final merge

		Call( ErrSORTIOptTreeMerge( pscb ) );

		// initialize final merge
		
		Call( ErrSORTIMergeStart( pscb, pscb->runlist.prunlinkHead ) );
		}

	//  we have no runs on disk, so remove duplicates in sort buffer, if requested

	else
		{
		pscb->ispairMac = CspairSORTIUnique(	pscb,
												pscb->rgbRec,
												pscb->rgspair,
												pscb->ispairMac );
		pscb->cRecords = pscb->ispairMac;
		}

	//  return a warning if TT doesn't fit in memory, but success otherwise

	err = ( pscb->crun > 0 || pscb->irecMac * cbIndexGran > cbResidentTTMax ) ?
				ErrERRCheck( JET_wrnSortOverflow ) :
				JET_errSuccess;

HandleError:
	return err;
	}


//----------------------------------------------------------
//	ErrSORTFirst
//
//	Move to first record in sort or return an error if the sort
//  has no records.
//----------------------------------------------------------
ERR ErrSORTFirst( FUCB * pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	SREC	* psrec			= 0;
	LONG	irec;
	ERR		err				= JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//	reset index range

	FUCBResetLimstat( pfucb );

	//  if we have no records, error

	if ( !pscb->cRecords )
		{
		DIRBeforeFirst( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	Assert( pscb->crun > 0 || pscb->ispairMac > 0 );
		
	//  if we have runs, start last merge and get first record
	
	if ( pscb->crun )
		{
		CallR( ErrSORTIMergeFirst( pscb, &psrec ) );
		}

	//  we have no runs, so just get first record in memory
	
	else
		{
		pfucb->ispairCurr = 0L;
		irec = pscb->rgspair[pfucb->ispairCurr].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
		}

	//	get current record

	SrecToKeydataflags( psrec, pfucb );

	return JET_errSuccess;
	}
	

ERR ErrSORTLast( FUCB *pfucb )
	{
	SCB		*pscb	= pfucb->u.pscb;
	SREC	*psrec	= 0;
	LONG	irec;
	ERR		err = JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//	reset index range

	FUCBResetLimstat( pfucb );

	//  if we have no records, error

	if ( !pscb->cRecords )
		{
		DIRAfterLast( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	Assert( pscb->crun > 0 || pscb->ispairMac > 0 );
		
	//  if we have runs, get next record from last merge

	if ( pscb->crun )
		{
		err = ErrSORTIMergeNext( pscb, &psrec );
		while ( err >= 0 )
			{
			CallS( err );		// warnings not expected.

			// cache current record so we can revert to something
			// once we move past the end.
			SrecToKeydataflags( psrec, pfucb );

			err = ErrSORTIMergeNext( pscb, &psrec );
			}

		if ( JET_errNoCurrentRecord == err )
			{
			// Currency will be set to the last record cached
			// in the loop above.
			err = JET_errSuccess;
			}
		else
			{
			DIRAfterLast( pfucb );
			}
		}
		
	//  we have no runs, so just get last record in memory
	
	else
		{
		pfucb->ispairCurr = pscb->ispairMac - 1;
		irec = pscb->rgspair[pfucb->ispairCurr].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
		
		//	get current record
		SrecToKeydataflags( psrec, pfucb );
		}

	return err;
	}

//----------------------------------------------------------
//	ErrSORTNext
//
//	Return the next record, in sort order, after the previously 
//	returned record.  If no records have been returned yet, 
//	or the currency has been reset, this function returns 
//	the first record.
//----------------------------------------------------------

ERR ErrSORTNext( FUCB *pfucb )
	{
	SCB		*pscb	= pfucb->u.pscb;
	SREC	*psrec	= 0;
	LONG	irec;
	ERR		err = JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//  if we have runs, get next record from last merge

	if ( pscb->crun )
		{
		Call( ErrSORTIMergeNext( pscb, &psrec ) );
		}
	else
		{
		Assert( pfucb->ispairCurr <= pscb->ispairMac );	// may already be at AfterLast
		
		//  we have no runs, so get next record from memory

		if ( ++pfucb->ispairCurr < pscb->ispairMac )
			{
			irec = pscb->rgspair[pfucb->ispairCurr].irec;
			psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
			}

		//  we have no more records in memory, so return no current record
		
		else
			{
			pfucb->ispairCurr = pscb->ispairMac;
			Call( ErrERRCheck( JET_errNoCurrentRecord ) );
			}
		}

	//	get current record
	SrecToKeydataflags( psrec, pfucb );

	//  handle index range, if requested

	if ( FFUCBLimstat( pfucb ) && FFUCBUpper( pfucb ) )
		CallR( ErrSORTCheckIndexRange( pfucb ) );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );
	if ( JET_errNoCurrentRecord == err )
		DIRAfterLast( pfucb );

	return err;
	}


//----------------------------------------------------------
//	ErrSORTPrev
//
//	Return the previous record, in sort order, before the
//  previously returned record.  If no records have been
//  returned yet, the currency will be set to before the
//  first record.
//
//  NOTE:  This function supports in memory sorts only!
//  Larger sorts must be materialized for this functionality.
//----------------------------------------------------------

ERR ErrSORTPrev( FUCB *pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err				= JET_errSuccess;

	//  verify that we have an in memory sort

	Assert( !pscb->crun );
	
	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	Assert( pfucb->ispairCurr >= -1L );	// may already be at BeforeFirst
	if ( pfucb->ispairCurr <= 0L )
		{
		//  we have no more records in memory, so return no current record
		SORTBeforeFirst( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	//  get previous record from memory
	pfucb->ispairCurr--;
	Assert( pfucb->ispairCurr >= 0L );

	const LONG	irec = pscb->rgspair[pfucb->ispairCurr].irec;
	const SREC	*psrec = PsrecFromPbIrec( pscb->rgbRec, irec );

	//	get current record
	SrecToKeydataflags( psrec, pfucb );

	//  handle index range, if requested

	if ( FFUCBLimstat( pfucb ) && FFUCBUpper( pfucb ) )
		CallR( ErrSORTCheckIndexRange( pfucb ) );

	return JET_errSuccess;
	}


//----------------------------------------------------------
//	ErrSORTSeek
//
//	Return the first record with key >= pkey.  
//	If pkey == NULL then return the first record.
//
//  Return Value
//		JET_errSuccess				record with key == pkey is found
//		JET_wrnSeekNotEqual			record with key > pkey is found
//		JET_errNoCurrentRecord		no record with key >= pkey is found
//
//  NOTE:  This function supports in memory sorts only!
//  Larger sorts must be materialized for this functionality.
//----------------------------------------------------------

ERR ErrSORTSeek( FUCB *pfucb, const KEY * pkey, BOOL fGT )
	{
	SCB		* const pscb	= pfucb->u.pscb;

	//  verify that we have an in memory sort

	Assert( FFUCBSort( pfucb ) );
	Assert( !pscb->crun );
	
	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//  verify that we are scrollable or indexed or the key is NULL
	
	Assert( ( pfucb->u.pscb->grbit & JET_bitTTScrollable ) ||
		( pfucb->u.pscb->grbit & JET_bitTTIndexed ) ||
		( pkey == NULL ) );

	//  if we have no records, return error

	if ( !pscb->cRecords )
		return ErrERRCheck( JET_errRecordNotFound );

	//  verify that we have a valid key -- note that sorts and temp. tables
	//	don't currently support secondary indexes
	Assert( pkey->Cb() <= JET_cbPrimaryKeyMost );

	//  seek to key or next highest key
	
	pfucb->ispairCurr = IspairSORTISeekByKey(	pscb->rgbRec,
												pscb->rgspair,
												pscb->ispairMac,
												pkey,
												fGT );

	//  if we are after last pair, record not found
	
	if ( pfucb->ispairCurr == pscb->ispairMac )
		return ErrERRCheck( JET_errRecordNotFound );

	//	get current record

	const INT irec				= pscb->rgspair[pfucb->ispairCurr].irec;
	const SREC * const psrec	= PsrecFromPbIrec( pscb->rgbRec, irec );

	SrecToKeydataflags( psrec, pfucb );

	//  return warning if key not equal, success otherwise

	return ( FKeysEqual( pfucb->kdfCurr.key, *pkey ) ?
				JET_errSuccess :
				ErrERRCheck( JET_wrnSeekNotEqual ) );
	}


VOID SORTICloseRun( PIB * const ppib, SCB * const pscb )
	{
	Assert( pscb->fcb.WRefCount() == 1 );
	Assert( pscb->fcb.PgnoFDP() != pgnoNull );
	Assert( !pscb->fcb.FInList() );		// sorts don't go in global list
	
	/*	if we were merging, end merge
	/**/
	if ( pscb->crunMerge )
		SORTIMergeEnd( pscb );

	/*	free merge method resources
	/**/
	SORTIOptTreeTerm( pscb );

	/*	if our output buffer is still latched, unlatch it
	/**/
	if ( pscb->bflOut.pv != NULL )
		{
		BFWriteUnlatch( &pscb->bflOut );
		pscb->bflOut.pv			= NULL;
		pscb->bflOut.dwContext	= NULL;
		}

	// Versioning doesn't occur on sorts.
	Assert( pscb->fcb.PrceOldest() == prceNil );
	Assert( pscb->fcb.PrceNewest() == prceNil );

	// Remove from hash table, so FCB will be available for
	// another file using the FDP that is about to be freed.
	Assert( !pscb->fcb.FDeleteCommitted() );
	pscb->fcb.SetDeleteCommitted();
	SCBDeleteHashTable( pscb );
	
	// Free FDP and allocated sort space (including runs)
	Assert( rgfmp[ pscb->fcb.Ifmp() ].Dbid() == dbidTemp );
	(VOID)ErrSPFreeFDP( ppib, &pscb->fcb, pgnoSystemRoot );
	
	pscb->fcb.ResetSortSpace();
	}


//----------------------------------------------------------
//	SORTClose
//
//  Release sort FUCB and the sort itself if it is no longer
//  needed.
//----------------------------------------------------------

VOID SORTClose( FUCB *pfucb )
	{
	SCB	* const pscb	= pfucb->u.pscb;

	Assert( dbidTemp == rgfmp[ pfucb->ifmp ].Dbid() );
	
	//	if this is the last cursor on sort, then release sort resources
	Assert( !pscb->fcb.FInList() );		// sorts don't go in global list
	Assert( pscb->fcb.WRefCount() >= 1 );
	if ( pscb->fcb.WRefCount() == 1 )
		{
		//  if we have allocated sort space, free it and end all ongoing merge
		//  and output activities

		if ( pscb->crun > 0 )
			{
			SORTICloseRun( pfucb->ppib, pscb );
			}

		//	unlink the FUCB from the FCB without allowing the FCB to move into
		//		the avail LRU list because it will be disappearing via SORTClosePscb

	  	FCBUnlinkWithoutMoveToAvailList( pfucb );

	  	//	close the FUCB
	  	
		FUCBClose( pfucb );
			
		/*	since there are no more references to this sort, free its resources
		/**/
		Assert( pscb->fcb.WRefCount() == 0 );
		SORTClosePscb( pscb );
		}
	else
		{
		//	unlink the FUCB from the FCB without allowing the FCB to move into
		//		the avail LRU list because it will eventually be disappearing 
		//		via SORTClosePscb

	  	FCBUnlinkWithoutMoveToAvailList( pfucb );

		//	close the FUCB

		FUCBClose( pfucb );
		}
	}


//----------------------------------------------------------
//	SORTClosePscb
//
//  Release this SCB and all its resources.
//----------------------------------------------------------

VOID SORTClosePscb( SCB *pscb )
	{
	INST *pinst = PinstFromIfmp( pscb->fcb.Ifmp() );
	
	Assert( rgfmp[ pscb->fcb.Ifmp() ].Dbid() == dbidTemp );
	Assert( pscb->fcb.PgnoFDP() == pgnoNull );
	if ( pscb->rgspair != NULL )
		{
		OSMemoryPageFree( pscb->rgspair );
		}
	if ( pscb->rgbRec != NULL )
		{
		OSMemoryPageFree( pscb->rgbRec );
		}
	if ( pscb->fcb.Pidb() != pidbNil )
		{
		// Sort indexes don't have names.
		Assert( 0 == pscb->fcb.Pidb()->ItagIndexName() );

		// No need to free index name or idxseg array, since memory
		// pool will be freed when TDB is deleted below.
		pscb->fcb.ReleasePidb();
		}
	if ( pscb->fcb.Ptdb() != ptdbNil )
		{
		Assert( pscb->fcb.Ptdb()->PfcbLV() == pfcbNil );	// sorts don't have LV's (would have been materialized)
		pscb->fcb.Ptdb()->Delete( pinst );
		}

	MEMReleasePscb( pinst, pscb );
	}


//----------------------------------------------------------
//	ErrSORTCheckIndexRange
//
//  Restrain currency to a specific range.
//----------------------------------------------------------

ERR ErrSORTCheckIndexRange( FUCB *pfucb )
	{
	SCB		* const pscb = pfucb->u.pscb;

	//  range check FUCB

	ERR err =  ErrFUCBCheckIndexRange( pfucb, pfucb->kdfCurr.key );
	Assert( JET_errSuccess == err || JET_errNoCurrentRecord == err );

	//  if there is no current record, we must have wrapped around
	
	if ( err == JET_errNoCurrentRecord )
		{
		//  wrap around to bottom of sort
		DIRUp( pfucb );

		if ( FFUCBUpper( pfucb ) )
			{
			pfucb->ispairCurr = pscb->ispairMac;
			DIRAfterLast( pfucb );
			}

		//  wrap around to top of sort
		
		else
			{
			SORTBeforeFirst( pfucb );
			}
		}

	//  verify that currency is valid

	Assert( pfucb->locLogical == locBeforeFirst ||
			pfucb->locLogical == locOnCurBM ||
			pfucb->locLogical == locAfterLast );
	Assert( pfucb->ispairCurr >= -1 );
	Assert( pfucb->ispairCurr <= pscb->ispairMac );

	return err;
	}


//----------------------------------------------------------
//	Module internal functions
//----------------------------------------------------------


//	returns index of first entry >= pbKey, or the index past the end of the array

LOCAL LONG IspairSORTISeekByKey( const BYTE * rgbRec, const SPAIR * rgspair, LONG ispairMac, const KEY * pkey, BOOL fGT )
	{
	//  if there are no pairs, return end of array

	if ( !ispairMac )
		return 0;

	//  b-search array

	LONG	ispairBeg	= 0;
	LONG	ispairEnd	= ispairMac;

	do  {
		//  calculate midpoint of this partition
		
		const LONG ispairMid		= ispairBeg + ( ispairEnd - ispairBeg ) / 2;

		//  compare full keys
		
		const LONG irec				= rgspair[ispairMid].irec;
		const SREC * const psrec	= PsrecFromPbIrec( rgbRec, irec );

		KEY keyT;
		keyT.prefix.Nullify();
		keyT.suffix.SetPv( PbSRECKeyPsrec( psrec ) );
		keyT.suffix.SetCb( CbSRECKeyPsrec( psrec ) );
		const INT wCmp = CmpKey( keyT, *pkey);

		//  select partition containing destination

		if ( ( wCmp < 0 ) || ( fGT && wCmp == 0 ) )
			{
			ispairBeg = ispairMid + 1;
			}
		else
			{
			ispairEnd = ispairMid;
			}

		}
	while ( ispairBeg != ispairEnd );

	return ispairEnd;
	}


//  compares two SRECs by key

INLINE INT ISORTICmpKey( const SREC * psrec1, const SREC * psrec2 )
	{
	const BYTE*		stKey1	= StSRECKeyPsrec( psrec1 );
	const USHORT	cbKey1	= *( UnalignedLittleEndian< USHORT > *)stKey1;
	const BYTE*		stKey2	= StSRECKeyPsrec( psrec2 );
	const USHORT	cbKey2	= *( UnalignedLittleEndian< USHORT > *)stKey2;

	Assert( cbKey1 <= KEY::cbKeyMax );
	Assert( cbKey2 <= KEY::cbKeyMax );
	
	const INT w = memcmp(	stKey1 + cbKeyCount,
							stKey2 + cbKeyCount,
							min( cbKey1, cbKey2 ) );
	
	return w ? w : cbKey1 - cbKey2;
	}

//  compares two SRECs by key and data

INLINE INT ISORTICmpKeyData( const SREC * psrec1, const SREC * psrec2 )
	{
	//  compare the keys first, then the data if necessary

	INT cmp = ISORTICmpKey( psrec1, psrec2 );
	if ( 0 == cmp )
		{
		const LONG	cbData1	= CbSRECDataPsrec( psrec1 );
		const LONG	cbData2	= CbSRECDataPsrec( psrec2 );
		cmp = memcmp(	PbSRECDataPsrec( psrec1 ),
						PbSRECDataPsrec( psrec2 ),
						min( cbData1, cbData2 ) );
		if ( 0 == cmp )
			{
			cmp = cbData1 - cbData2;
			}
		}

	return cmp;
	}

//  returns fTrue if the two SRECs are considered duplicates according to the
//  current flags in the SCB

INLINE BOOL FSORTIDuplicate( SCB* const pscb, const SREC * psrec1, const SREC * psrec2 )
	{
	if ( FSCBRemoveDuplicateKey( pscb ) )
		{
		return !ISORTICmpKey( psrec1, psrec2 );
		}
	else if ( FSCBRemoveDuplicateKeyData( pscb ) )
		{
		return !ISORTICmpKeyData( psrec1, psrec2 );
		}
	else
		{
		return fFalse;
		}
	}
	

//  remove duplicates

LOCAL LONG CspairSORTIUnique( SCB* pscb, BYTE * rgbRec, SPAIR * rgspair, const LONG ispairMac )
	{
	//  if we don't need to perform duplicate removal then return immediately

	if (	!ispairMac ||
			!FSCBRemoveDuplicateKeyData( pscb ) && !FSCBRemoveDuplicateKey( pscb ) )
		{
		return ispairMac;
		}

	//  loop through records, moving unique records towards front of array

	LONG	ispairDest;
	LONG	ispairSrc;
	for ( ispairDest = 0, ispairSrc = 1; ispairSrc < ispairMac; ispairSrc++ )
		{
		//  get sort record pointers for src/dest
		
		const LONG		irecDest	= rgspair[ispairDest].irec;
		SREC * const	psrecDest	= PsrecFromPbIrec( rgbRec, irecDest );
		const LONG		irecSrc		= rgspair[ispairSrc].irec;
		SREC * const	psrecSrc	= PsrecFromPbIrec( rgbRec, irecSrc );

		//  if the keys are unequal, copy them forward
		
		if ( !FSORTIDuplicate( pscb, psrecSrc, psrecDest ) )
			{
			rgspair[++ispairDest] = rgspair[ispairSrc];
			}
		}
		
	Assert( ispairDest + 1 <= ispairMac );

	return ispairDest + 1;
	}


//  output current sort buffer to disk in a run
//
LOCAL ERR ErrSORTIOutputRun( SCB * pscb )
	{
	ERR		err;
	RUNINFO	runinfo;
	LONG	ispair;
	LONG	irec;
	SREC*	psrec		= NULL;
	SREC*	psrecLast;

	//  verify that there are records to put to disk
	//
	Assert( pscb->ispairMac );

	//  create sort space on disk now if not done already
	//
	if ( pscb->fcb.PgnoFDP() == pgnoNull )
		{
		FUCB 	* const pfucb	= pscb->fcb.Pfucb();
		PGNO	pgnoFDP			= pgnoNull;
		OBJID	objidFDP		= objidNil;
		
		//  allocate FDP and primary sort space
		//
		//  NOTE:  enough space is allocated to avoid file extension for a single
		//         level merge, based on the data size of the first run
		//	
		const CPG cpgMin = (PGNO) ( ( pscb->cbData + cbFreeSPAGE - 1 ) / cbFreeSPAGE * crunFanInMax );
		CPG cpgReq = cpgMin;

		CallR( ErrSPGetExt(	pfucb,
		 		pgnoSystemRoot,
				&cpgReq,
				cpgMin,
				&pgnoFDP,
				fSPNewFDP|fSPMultipleExtent|fSPUnversionedExtent,
				CPAGE::fPagePrimary,
				&objidFDP ) );

		//	place SCB in FCB hash table so BTOpen will find it instead
		//	of trying to allocate a new one.
		//
		Assert( pgnoNull != pgnoFDP );
		Assert( objidFDP > objidSystemRoot );
		Assert( !pscb->fcb.FSpaceInitialized() );
		pscb->fcb.SetSortSpace( pgnoFDP, objidFDP );
	 	SCBInsertHashTable( pscb );

		//  initialize merge process
		//
		SORTIOptTreeInit( pscb );

		//  reset sort/merge run output
		//
		pscb->bflOut.pv			= NULL;
		pscb->bflOut.dwContext	= NULL;

		//  reset merge run input
		//
		pscb->crunMerge = 0;
		}

	//  begin a new run big enough to store all our data
	//
	CallR( ErrSORTIRunStart( pscb, pscb->cbData, pscb->crecBuf, &runinfo ) );

	//  scatter-gather our sorted records into the run while performing
	//  duplicate removal
	//
	for ( ispair = 0; ispair < pscb->ispairMac; ispair++ )
		{
		psrecLast = psrec;
		
		irec = pscb->rgspair[ispair].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );

		if (	!psrecLast ||
				!FSORTIDuplicate( pscb, psrec, psrecLast ) )
			{
			CallJ( ErrSORTIRunInsert( pscb, &runinfo, psrec ), EndRun );
			}
		}

	//  end run and add to merge
	//
	SORTIRunEnd( pscb, &runinfo );
	CallJ( ErrSORTIOptTreeAddRun( pscb, &runinfo ), DeleteRun );
	
	//	reinitialize the SCB for another memory sort
	//
	pscb->ispairMac	= 0;
	pscb->irecMac	= 0;
	pscb->crecBuf	= 0;
	pscb->cbData	= 0;

	return JET_errSuccess;

EndRun:
	SORTIRunEnd( pscb, &runinfo );
DeleteRun:
	SORTIRunDelete( pscb, &runinfo );
	return err;
	}

#ifdef DEBUG
LOCAL INT IDBGICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	INT	cmp;
	INT	db;
	
	//  get the addresses of the sort records associated with these pairs
	
	SREC * const psrec1 = PsrecFromPbIrec( pscb->rgbRec, pspair1->irec );
	SREC * const psrec2 = PsrecFromPbIrec( pscb->rgbRec, pspair2->irec );

	//  calculate the length of full key remaining that we can compare
	//  we want to compare the key and the data (if the keys differ we
	//  won't compare the data) the data is stored after the key so it
	//  is O.K.
	const LONG cbKey1	= CbSRECKeyPsrec( psrec1 );
	const LONG cbKey2	= CbSRECKeyPsrec( psrec2 );
		
	if ( cbKey1 > cbKeyPrefix )
		{
		db = cbKey1 - cbKey2;
		cmp = memcmp( PbSRECKeyPsrec( psrec1 ) + cbKeyPrefix,
							PbSRECKeyPsrec( psrec2 ) + cbKeyPrefix,
							( db < 0 ? cbKey1 : cbKey2 ) - cbKeyPrefix );
		if ( 0 == cmp )
			cmp = db;
		if ( 0 != cmp )
			return cmp;
		}
	else
		{
		// If keys are less than or equal to prefix, then they must be equal
		// (otherwise this routine would not have been called).
		Assert( cbKey1 == cbKey2 );
		Assert( memcmp( PbSRECKeyPsrec( psrec1 ), PbSRECKeyPsrec( psrec2 ), cbKey1 ) == 0 );
		}


	// Full keys are identical.  Must resort to data comparison.
	const LONG cbData1 = CbSRECDataPsrec( psrec1 );
	const LONG cbData2 = CbSRECDataPsrec( psrec2 );

	db = cbData1 - cbData2;
	cmp = memcmp( PbSRECDataPsrec( psrec1 ),
					PbSRECDataPsrec( psrec2 ),
					db < 0 ? cbData1 : cbData2 );

	return ( 0 == cmp ? db : cmp );
	}
#endif	
	
//  ISORTICmpPspairPspair compares two SPAIRs for the cache optimized Quicksort.
//  Only the key prefixes are compared, unless there is a tie in which case we
//  are forced to go to the full record at the cost of several wait states.
//
INLINE INT ISORTICmpPspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	const BYTE	*rgb1	= (BYTE *) pspair1;
	const BYTE	*rgb2	= (BYTE *) pspair2;

	//  Compare prefixes first.  If they aren't equal, we're done.  Prefixes are
	//  stored in such a way as to allow very fast integer comparisons instead
	//  of byte by byte comparisons like memcmp.  Note that these comparisons are
	//  made scanning backwards.

	//  NOTE:  special case code:  cbKeyPrefix = 14, irec is first

	Assert( cbKeyPrefix == 14 );
	Assert( OffsetOf( SPAIR, irec ) == 0 );

#ifdef _X86_

	//  bytes 15 - 12
	if ( *( (DWORD *) ( rgb1 + 12 ) ) < *( (DWORD *) ( rgb2 + 12 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 12 ) ) > *( (DWORD *) ( rgb2 + 12 ) ) )
		return 1;

	//  bytes 11 - 8
	if ( *( (DWORD *) ( rgb1 + 8 ) ) < *( (DWORD *) ( rgb2 + 8 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 8 ) ) > *( (DWORD *) ( rgb2 + 8 ) ) )
		return 1;

	//  bytes 7 - 4
	if ( *( (DWORD *) ( rgb1 + 4 ) ) < *( (DWORD *) ( rgb2 + 4 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 4 ) ) > *( (DWORD *) ( rgb2 + 4 ) ) )
		return 1;

	//  bytes 3 - 2
	if ( *( (USHORT *) ( rgb1 + 2 ) ) < *( (USHORT *) ( rgb2 + 2 ) ) )
		return -1;
	if ( *( (USHORT *) ( rgb1 + 2 ) ) > *( (USHORT *) ( rgb2 + 2 ) ) )
		return 1;

#else  //  !_X86_

	//  bytes 15 - 8
	if ( *( (LittleEndian<QWORD> *) ( rgb1 + 8 ) ) < *( (LittleEndian<QWORD> *) ( rgb2 + 8 ) ) )
		return -1;
	if ( *( (LittleEndian<QWORD> *) ( rgb2 + 8 ) ) < *( (LittleEndian<QWORD> *) ( rgb1 + 8 ) ) )
		return 1;

	//  bytes 7 - 2
	if (	( *( (LittleEndian<QWORD> *) ( rgb1 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) <
			( *( (LittleEndian<QWORD> *) ( rgb2 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) )
		return -1;
	if (	( *( (LittleEndian<QWORD> *) ( rgb1 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) >
			( *( (LittleEndian<QWORD> *) ( rgb2 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) )
		return 1;

#endif  //  _X86_
	
	//  perform secondary comparison and return result if prefixes identical

	INT i = ISORTICmp2PspairPspair( pscb, pspair1, pspair2 );

#ifdef DEBUG
	// Verify two key/data pairs are key-equivalent, whether their key
	// and data parts are compared separately or concatenated together.
	INT j = IDBGICmp2PspairPspair( pscb, pspair1, pspair2 );

	Assert( ( i < 0 && j < 0 )
			|| ( i == 0 && j == 0 )
			|| ( i > 0 && j > 0 ) );
#endif			

	return i;
	}

		
LOCAL INT ISORTICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	//  get the addresses of the sort records associated with these pairs
	
	SREC * const psrec1 = PsrecFromPbIrec( pscb->rgbRec, pspair1->irec );
	SREC * const psrec2 = PsrecFromPbIrec( pscb->rgbRec, pspair2->irec );

	//  calculate the length of full key remaining that we can compare
	//  we want to compare the key and the data (if the keys differ we
	//  won't compare the data) the data is stored after the key so it
	//  is O.K.

	const LONG cbKey1 = CbSRECKeyDataPsrec( psrec1 );
	const LONG cbKey2 = CbSRECKeyDataPsrec( psrec2 );

	INT w = min( cbKey1, cbKey2 ) - cbKeyPrefix;

	//  compare the remainder of the full keys and then the data (if necessary)

	if ( w > 0 )
		{
		//	both keys are greater in length than cbKeyPrefix
		Assert( cbKey1 > cbKeyPrefix );
		Assert( cbKey2 > cbKeyPrefix );
		w = memcmp(	PbSRECKeyPsrec( psrec1 ) + cbKeyPrefix,
					PbSRECKeyPsrec( psrec2 ) + cbKeyPrefix,
					w );
		if ( 0 == w )
			{
			w = cbKey1 - cbKey2;
			}
		}
	else
		{
		//	prefix must be the same (as calculated by ISORTICmpPspairPspair()),
		//	so return comparison result based on key size
		w = cbKey1 - cbKey2;
		}

	return w;
	}


//  Swap functions

INLINE VOID SWAPPspair( SPAIR **ppspair1, SPAIR **ppspair2 )
	{
	SPAIR *pspairT;

	pspairT = *ppspair1;
	*ppspair1 = *ppspair2;
	*ppspair2 = pspairT;
	}


//  we do not use cache aligned memory for spairT (is this bad?)

INLINE VOID SWAPSpair( SPAIR *pspair1, SPAIR *pspair2 )
	{
	SPAIR spairT;

	spairT = *pspair1;
	*pspair1 = *pspair2;
	*pspair2 = spairT;
	}


INLINE VOID SWAPPsrec( SREC **ppsrec1, SREC **ppsrec2 )
	{
	SREC *psrecT;

	psrecT = *ppsrec1;
	*ppsrec1 = *ppsrec2;
	*ppsrec2 = psrecT;
	}


INLINE VOID SWAPPmtnode( MTNODE **ppmtnode1, MTNODE **ppmtnode2 )
	{
	MTNODE *pmtnodeT;

	pmtnodeT = *ppmtnode1;
	*ppmtnode1 = *ppmtnode2;
	*ppmtnode2 = pmtnodeT;
	}


//  SORTIInsertionSort is a cache optimized version of the standard Insertion
//  sort.  It is used to sort small partitions for SORTIQuicksort because it
//  provides a statistical speed advantage over a pure Quicksort.

LOCAL VOID SORTIInsertionSort( SCB *pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn )
	{
	SPAIR	*pspairLast;
	SPAIR	*pspairFirst;
	SPAIR	*pspairKey = pscb->rgspair + cspairSortMax;

	//  This loop is optimized so that we only scan for the current pair's new
	//  position if the previous pair in the list is greater than the current
	//  pair.  This avoids unnecessary pair copying for the key, which is
	//  expensive for sort pairs.

	for (	pspairFirst = pspairMinIn, pspairLast = pspairMinIn + 1;
			pspairLast < pspairMaxIn;
			pspairFirst = pspairLast++ )
		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairLast ) > 0 )
			{
			//  save current pair as the "key"

			*pspairKey = *pspairLast;

			//  move previous pair into this pair's position

			*pspairLast = *pspairFirst;
			
			//  insert key into the (sorted) first part of the array (MinIn through
			//  Last - 1), moving already sorted pairs out of the way

			while (	--pspairFirst >= pspairMinIn &&
					( ISORTICmpPspairPspair( pscb, pspairFirst, pspairKey ) ) > 0 )
				{
				*( pspairFirst + 1 ) = *pspairFirst;
				}
			*( pspairFirst + 1 ) = *pspairKey;
			}
	}


//  SORTIQuicksort is a cache optimized Quicksort that sorts sort pair arrays
//  generated by ErrSORTInsert.  It is designed to sort large arrays of data
//  without any CPU data cache misses.  To do this, it uses a special comparator
//  designed to work with the sort pairs (see ISORTICmpPspairPspair).

LOCAL VOID SORTIQuicksort( SCB * pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn )
	{
	//  partition stack
	struct _part
		{
		SPAIR	*pspairMin;
		SPAIR	*pspairMax;
		}	rgpart[cpartQSortMax];
	LONG	cpart		= 0;

	SPAIR	*pspairFirst;
	SPAIR	*pspairLast;

	//  current partition = partition passed in arguments

	SPAIR	*pspairMin	= pspairMinIn;
	SPAIR	*pspairMax	= pspairMaxIn;

	//  Quicksort current partition
	
	forever
		{
		//  if this partition is small enough, insertion sort it

		if ( pspairMax - pspairMin < cspairQSortMin )
			{
			SORTIInsertionSort( pscb, pspairMin, pspairMax );
			
			//  if there are no more partitions to sort, we're done

			if ( !cpart )
				break;

			//  pop a partition off the stack and make it the current partition

			pspairMin = rgpart[--cpart].pspairMin;	//lint !e530
			pspairMax = rgpart[cpart].pspairMax;	//lint !e530
			continue;
			}

		//  determine divisor by sorting the first, middle, and last pairs and
		//  taking the resulting middle pair as the divisor (stored in first place)

		pspairFirst	= pspairMin + ( ( pspairMax - pspairMin ) >> 1 );
		pspairLast	= pspairMax - 1;

		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairMin ) > 0 )
			SWAPSpair( pspairFirst, pspairMin );
		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairLast ) > 0 )
			SWAPSpair( pspairFirst, pspairLast );
		if ( ISORTICmpPspairPspair( pscb, pspairMin, pspairLast ) > 0 )
			SWAPSpair( pspairMin, pspairLast );

		//  sort large partition into two smaller partitions (<=, >)
		//
		//  NOTE:  we are not sorting the two end pairs as the first pair is the
		//  divisor and the last pair is already known to be > the divisor

		pspairFirst = pspairMin + 1;
		pspairLast--;

		Assert( pspairFirst <= pspairLast );
		
		forever
			{
			//  advance past all pairs <= the divisor
			
			while (	pspairFirst <= pspairLast &&
					ISORTICmpPspairPspair( pscb, pspairFirst, pspairMin ) <= 0 )
				pspairFirst++;

			//  advance past all pairs > the divisor
			
			while (	pspairFirst <= pspairLast &&
					ISORTICmpPspairPspair( pscb, pspairLast, pspairMin ) > 0 )
				pspairLast--;

			//  if we have found a pair to swap, swap them and continue

			Assert( pspairFirst != pspairLast );
			
			if ( pspairFirst < pspairLast )
				SWAPSpair( pspairFirst++, pspairLast-- );

			//  no more pairs to compare, partitioning complete
			
			else
				break;
			}

		//  place the divisor at the end of the <= partition

		if ( pspairLast != pspairMin )
			SWAPSpair( pspairMin, pspairLast );

		//  set first/last to delimit larger partition (as min/max) and set
		//  min/max to delimit smaller partition for next iteration

		if ( pspairMax - pspairLast - 1 > pspairLast - pspairMin )
			{
			pspairFirst	= pspairLast + 1;
			SWAPPspair( &pspairLast, &pspairMax );
			}
		else
			{
			pspairFirst	= pspairMin;
			pspairMin	= pspairLast + 1;
			}

		//  push the larger partition on the stack (recurse if there is no room)

		if ( cpart < cpartQSortMax )
			{
			rgpart[cpart].pspairMin		= pspairFirst;
			rgpart[cpart++].pspairMax	= pspairLast;
			}
		else
			SORTIQuicksort( pscb, pspairFirst, pspairLast );
		}
	}

//  Create a new run with the supplied parameters.  The new run's id and size
//  in pages is returned on success

LOCAL ERR ErrSORTIRunStart( SCB *pscb, LONG cb, LONG crec, RUNINFO *pruninfo )
	{
	ERR		err;

	//  allocate space for new run according to given info

	pruninfo->run		= runNull;
	pruninfo->cpg		= ( cb + cbFreeSPAGE - 1 ) / cbFreeSPAGE;
	pruninfo->cb		= 0;
	pruninfo->crec		= 0;
	pruninfo->cpgUsed	= pruninfo->cpg;

	CallR( ErrSPGetExt(
				pscb->fcb.Pfucb(),
				pscb->fcb.PgnoFDP(),
				&pruninfo->cpg,
				pruninfo->cpgUsed,
				&pruninfo->run ) );
	Assert( pruninfo->cpg >= pruninfo->cpgUsed );

	//  initialize output run data

	pscb->pgnoNext			= pruninfo->run;
	pscb->bflOut.pv			= NULL;
	pscb->bflOut.dwContext	= NULL;
	pscb->pbOutMac			= NULL;
	pscb->pbOutMax			= NULL;

	return JET_errSuccess;
	}


//  Inserts the given record into the run.  Records are stored compactly and
//  are permitted to cross page boundaries to avoid wasted space.

LOCAL ERR ErrSORTIRunInsert( SCB *pscb, RUNINFO* pruninfo, SREC *psrec )
	{
	ERR	  		err;
	LONG		cb;
	PGNO		pgnoNext;
	SPAGE_FIX	*pspage;
	LONG		cbToWrite;

	//  assumption:  record size < free sort page data size (and is valid)

	Assert(	CbSRECSizePsrec( psrec ) > CbSRECSizeCbCb( 0, 0 ) &&
			CbSRECSizePsrec( psrec ) < cbFreeSPAGE );

	//  calculate number of bytes that will fit on the current page

	cb = (LONG)min(	pscb->pbOutMax - pscb->pbOutMac, (LONG)CbSRECSizePsrec( psrec ) );

	//  if some data will fit, write it

	if ( cb )
		{
		UtilMemCpy( pscb->pbOutMac, psrec, cb );
		pscb->pbOutMac += cb;
		}

	//  all the data did not fit on this page

	if ( cb < (LONG) CbSRECSizePsrec( psrec ) )
		{
		//  page is full, so release it so it can be lazily-written to disk

		if ( pscb->bflOut.pv != NULL )
			{
			BFWriteUnlatch( &pscb->bflOut );
			pscb->bflOut.pv			= NULL;
			pscb->bflOut.dwContext	= NULL;
			}

		//  allocate a buffer for the next page in the run and latch it
		
		pgnoNext = pscb->pgnoNext++;
		CallR( ErrBFWriteLatchPage(	&pscb->bflOut,
									pscb->fcb.Pfucb()->ifmp,
									pgnoNext,
									bflfNew ) );
		BFDirty( &pscb->bflOut );

		//  initialize page

		pspage = (SPAGE_FIX *) pscb->bflOut.pv;

		pspage->pgnoThisPage = pgnoNext;

		//  initialize data pointers for this page

		pscb->pbOutMac = PbDataStartPspage( pspage );
		pscb->pbOutMax = PbDataEndPspage( pspage );

		//  write the remainder of the data to this page

		cbToWrite = CbSRECSizePsrec( psrec ) - cb;
		UtilMemCpy( pscb->pbOutMac, ( (BYTE *) psrec ) + cb, cbToWrite );
		pscb->pbOutMac += cbToWrite;
		}

	//  update this run's stats

	pruninfo->cb += CbSRECSizePsrec( psrec );
	pruninfo->crec++;
	return JET_errSuccess;
	}


//  ends current output run

INLINE VOID SORTIRunEnd( SCB * pscb, RUNINFO* pruninfo )
	{
	//  unlatch page so it can be lazily-written to disk
	
	BFWriteUnlatch( &pscb->bflOut );
	pscb->bflOut.pv			= NULL;
	pscb->bflOut.dwContext	= NULL;

	//  trim our space usage for this run

	pruninfo->cpgUsed = ( pruninfo->cb + cbFreeSPAGE - 1 ) / cbFreeSPAGE;

	if ( pruninfo->cpg - pruninfo->cpgUsed > 0 )
		{
		FUCB		* const pfucbT	= pscb->fcb.Pfucb();
		Assert( !FFUCBSpace( pfucbT ) );
		const ERR	errT			= ErrSPFreeExt(
											pfucbT,
											pruninfo->run + pruninfo->cpgUsed,
											pruninfo->cpg - pruninfo->cpgUsed );
#ifdef DEBUG
		switch( errT )
			{
			case JET_errSuccess:
			case JET_errOutOfCursors:
			case JET_errOutOfMemory:
			case JET_errOutOfBuffers:
				break;
			default:
				CallS( errT ); 
			}
#endif // DEBUG			
		}

	pruninfo->cpg = pruninfo->cpgUsed;
	}


//  Deletes a run from disk.  No error is returned because if delete fails,
//  it is not fatal (only wasted space in the temporary database).

INLINE VOID SORTIRunDelete( SCB * pscb, const RUNINFO * pruninfo )
	{
	//  delete run

	if ( pruninfo->cpg )
		{
		FUCB		* const pfucbT	= pscb->fcb.Pfucb();
		Assert( !FFUCBSpace( pfucbT ) );
		const ERR	errT			= ErrSPFreeExt(
											pfucbT,
											pruninfo->run,
											pruninfo->cpg );
#ifdef DEBUG
		switch( errT )
			{
			case JET_errSuccess:
			case JET_errOutOfCursors:
			case JET_errOutOfMemory:
			case JET_errOutOfBuffers:
				break;
			default:
				CallS( errT ); 
			}
#endif // DEBUG			
		}
	}


//  Deletes crun runs in the specified run list, if possible

LOCAL VOID	SORTIRunDeleteList( SCB *pscb, RUNLINK **pprunlink, LONG crun )
	{
	LONG	irun;

	//  walk list, deleting runs

	for ( irun = 0; *pprunlink != prunlinkNil && irun < crun; irun++ )
		{
		//  delete run
		
		SORTIRunDelete( pscb, &( *pprunlink )->runinfo );

		//  get next run to free

		RUNLINK	* prunlinkT = *pprunlink;
		*pprunlink = ( *pprunlink )->prunlinkNext;

		//  free RUNLINK

		RUNLINKReleasePrunlink( prunlinkT );
		}
	}


//  Deletes the memory for crun runs in the specified run list, but does not
//  bother to delete the runs from disk

LOCAL VOID	SORTIRunDeleteListMem( SCB *pscb, RUNLINK **pprunlink, LONG crun )
	{
	LONG	irun;

	//  walk list, deleting runs

	for ( irun = 0; *pprunlink != prunlinkNil && irun < crun; irun++ )
		{
		//  get next run to free

		RUNLINK	* prunlinkT = *pprunlink;
		*pprunlink = ( *pprunlink )->prunlinkNext;

		//  free RUNLINK

		RUNLINKReleasePrunlink( prunlinkT );
		}
	}


//  Opens the specified run for reading.

LOCAL ERR ErrSORTIRunOpen( SCB *pscb, RUNINFO *pruninfo, RCB **pprcb )
	{
	ERR		err;
	RCB		*prcb	= prcbNil;
	LONG	ipbf;
	CPG		cpgRead;
	
	//  allocate a new RCB

	if ( ( prcb = PrcbRCBAlloc() ) == prcbNil )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );

	//  initialize RCB

	prcb->pscb = pscb;
	prcb->runinfo = *pruninfo;
	
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		{
		prcb->rgbfl[ipbf].pv		= NULL;
		prcb->rgbfl[ipbf].dwContext	= NULL;
		}

	prcb->ipbf			= cpgClusterSize;
	prcb->pbInMac		= NULL;
	prcb->pbInMax		= NULL;
	prcb->cbRemaining	= prcb->runinfo.cb;
	prcb->pvAssy		= NULL;

	//  preread the first part of the run, to be access paged later as required

	cpgRead = min( prcb->runinfo.cpgUsed, 2 * cpgClusterSize );

	BFPrereadPageRange(	pscb->fcb.Pfucb()->ifmp,
						(PGNO) prcb->runinfo.run,
						cpgRead );

	//  return the initialized RCB

	*pprcb = prcb;
	return JET_errSuccess;

HandleError:
	*pprcb = prcbNil;
	return err;
	}


//  Returns next record in opened run (the first if the run was just opened).
//  Returns JET_errNoCurrentRecord if all records have been read.  The record
//  retrieved during the previous call is guaranteed to still be in memory
//  after this call for the purpose of duplicate removal comparisons.
//
//  Special care must be taken when reading the records because they could
//  be broken at arbitrary points across page boundaries.  If this happens,
//  the record is assembled in a temporary buffer, to which the pointer is
//  returned.  This memory is freed by this function or ErrSORTIRunClose.

LOCAL ERR ErrSORTIRunNext( RCB * prcb, SREC **ppsrec )
	{
	ERR		err;
	SCB		*pscb = prcb->pscb;
	SIZE_T	cbUnread;
	SHORT	cbRec;
	SPAGE_FIX	*pspage;
	LONG	ipbf;
	PGNO	pgnoNext;
	CPG		cpgRead;
	SIZE_T	cbToRead;

	//  free second to last assembly buffer, if present, and make last
	//  assembly buffer the second to last assembly buffer

	if ( pscb->pvAssyLast != NULL )
		{
		BFFree( pscb->pvAssyLast );
		}
	pscb->pvAssyLast = prcb->pvAssy;
	prcb->pvAssy = NULL;

	//  abandon last buffer, if present

	if ( pscb->bflLast.pv != NULL )
		{
		CLockDeadlockDetectionInfo::DisableOwnershipTracking();
		BFPurge( &pscb->bflLast, fTrue );
		CLockDeadlockDetectionInfo::EnableOwnershipTracking();
		pscb->bflLast.pv		= NULL;
		pscb->bflLast.dwContext	= NULL;
		}
	
	//  are there no more records to read?

	if ( !prcb->cbRemaining )
		{
		//  make sure we don't hold on to the last page of the run

		if ( prcb->rgbfl[prcb->ipbf].pv != NULL )
			{
			pscb->bflLast.pv		= prcb->rgbfl[prcb->ipbf].pv;
			pscb->bflLast.dwContext	= prcb->rgbfl[prcb->ipbf].dwContext;
			prcb->rgbfl[prcb->ipbf].pv			= NULL;
			prcb->rgbfl[prcb->ipbf].dwContext	= NULL;
			}
			
		//  return No Current Record
		
		Error( ErrERRCheck( JET_errNoCurrentRecord ), HandleError );
		}
	
	//  calculate size of unread data still in page

	cbUnread = prcb->pbInMax - prcb->pbInMac;

	//  is there any more data on this page?

	if ( cbUnread )
		{
		//  if the record is entirely on this page, return it

		if (	cbUnread > cbSRECReadMin &&
				(LONG) CbSRECSizePsrec( (SREC *) prcb->pbInMac ) <= cbUnread )
			{
			cbRec = (SHORT) CbSRECSizePsrec( (SREC *) prcb->pbInMac );
			*ppsrec = (SREC *) prcb->pbInMac;
			prcb->pbInMac += cbRec;
			prcb->cbRemaining -= cbRec;
			return JET_errSuccess;
			}

		//  allocate a new assembly buffer

		BFAlloc( &prcb->pvAssy );

		//  copy what there is of the record on this page into assembly buffer

		UtilMemCpy( prcb->pvAssy, prcb->pbInMac, cbUnread );
		prcb->cbRemaining -= cbUnread;
		}

	//  get next page number

	if ( prcb->ipbf < cpgClusterSize )
		{
		//  next page is sequentially after the used up buffer's page number
		
		pgnoNext = ( ( SPAGE_FIX * )prcb->rgbfl[prcb->ipbf].pv )->pgnoThisPage + 1;
		
		//  move the used up buffer to the last buffer
		//  to guarantee validity of record read last call

		pscb->bflLast.pv		= prcb->rgbfl[prcb->ipbf].pv;
		pscb->bflLast.dwContext	= prcb->rgbfl[prcb->ipbf].dwContext;
		prcb->rgbfl[prcb->ipbf].pv			= NULL;
		prcb->rgbfl[prcb->ipbf].dwContext	= NULL;
		}
	else
		{
		//  no pages are resident yet, so next page is the first page in the run
		
		pgnoNext = (PGNO) prcb->runinfo.run;
		}

	//  is there another pinned buffer available?

	if ( ++prcb->ipbf < cpgClusterSize )
		{
		//  yes, then this pbf should never be null

		Assert( prcb->rgbfl[prcb->ipbf].pv != NULL );
		Assert( prcb->rgbfl[prcb->ipbf].dwContext != NULL );
		
		//  set new page data pointers

		pspage = (SPAGE_FIX *) prcb->rgbfl[prcb->ipbf].pv;
		prcb->pbInMac = PbDataStartPspage( pspage );
		prcb->pbInMax = PbDataEndPspage( pspage );
		}
	else
		{
		//  no, get and pin all buffers that were read ahead last time

		cpgRead = min(	(LONG) ( prcb->runinfo.run + prcb->runinfo.cpgUsed - pgnoNext ),
						cpgClusterSize );
		Assert( cpgRead > 0 );
		
		for ( ipbf = 0; ipbf < cpgRead; ipbf++ )
			Call( ErrSORTIRunReadPage( prcb, pgnoNext + ipbf, ipbf ) );

		//  set new page data pointers

		prcb->ipbf		= 0;
		pspage			= (SPAGE_FIX *) prcb->rgbfl[prcb->ipbf].pv;
		prcb->pbInMac	= PbDataStartPspage( pspage );
		prcb->pbInMax	= PbDataEndPspage( pspage );
		
		//  issue prefetch for next cluster (if needed)

		pgnoNext += cpgClusterSize;
		cpgRead = min(	(LONG) ( prcb->runinfo.run + prcb->runinfo.cpgUsed - pgnoNext ),
						cpgClusterSize );
		if ( cpgRead > 0 )
			{
			Assert( pgnoNext >= prcb->runinfo.run );
			Assert(	pgnoNext + cpgRead - 1 <= prcb->runinfo.run + prcb->runinfo.cpgUsed - 1 );
			BFPrereadPageRange(	pscb->fcb.Pfucb()->ifmp,
								pgnoNext,
								cpgRead );
			}
		}

	//  if there was no data last time, entire record must be at the top of the
	//  page, so return it

	if ( !cbUnread )
		{
		cbRec = (SHORT) CbSRECSizePsrec( (SREC *) prcb->pbInMac );
		Assert( cbRec > (LONG) CbSRECSizeCbCb( 0, 0 ) && cbRec < cbFreeSPAGE );
		*ppsrec = (SREC *) prcb->pbInMac;
		prcb->pbInMac += cbRec;
		prcb->cbRemaining -= cbRec;
		return JET_errSuccess;
		}

	//  if we couldn't get the record size from the last page, copy enough data
	//  to the assembly buffer to get the record size

	if ( cbUnread < cbSRECReadMin )
		UtilMemCpy(	( (BYTE *) prcb->pvAssy ) + cbUnread,
				prcb->pbInMac,
				cbSRECReadMin - cbUnread );

	//  if not, copy remainder of record into assembly buffer

	cbToRead = CbSRECSizePsrec( (SREC *) prcb->pvAssy ) - cbUnread;
	UtilMemCpy( ( (BYTE *) prcb->pvAssy ) + cbUnread, prcb->pbInMac, cbToRead );
	prcb->pbInMac += cbToRead;
	prcb->cbRemaining -= cbToRead;

	//  return pointer to assembly buffer

	*ppsrec = (SREC *) prcb->pvAssy;
	return JET_errSuccess;

HandleError:
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		if ( prcb->rgbfl[ipbf].pv != NULL )
			{
			CLockDeadlockDetectionInfo::DisableOwnershipTracking();
			BFPurge( &prcb->rgbfl[ipbf], fTrue );
			CLockDeadlockDetectionInfo::EnableOwnershipTracking();
			prcb->rgbfl[ipbf].pv		= NULL;
			prcb->rgbfl[ipbf].dwContext	= NULL;
			}
	*ppsrec = NULL;
	return err;
	}


//  Closes an opened run

LOCAL VOID SORTIRunClose( RCB *prcb )
	{
	LONG	ipbf;
	
	//  free record assembly buffer

	if ( prcb->pvAssy != NULL )
		{
		BFFree( prcb->pvAssy );
		}

	//  unpin all read-ahead buffers
	
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		if ( prcb->rgbfl[ipbf].pv != NULL )
			{
			CLockDeadlockDetectionInfo::DisableOwnershipTracking();
			BFPurge( &prcb->rgbfl[ipbf], fTrue );
			CLockDeadlockDetectionInfo::EnableOwnershipTracking();
			prcb->rgbfl[ipbf].pv		= NULL;
			prcb->rgbfl[ipbf].dwContext	= NULL;
			}

	//  free RCB
	
	RCBReleasePrcb( prcb );
	}


//  get read access to a page in a run (buffer is pinned in memory)

INLINE ERR ErrSORTIRunReadPage( RCB *prcb, PGNO pgno, LONG ipbf )
	{
	ERR		err;

	//  verify that we are trying to read a page that is used in the run

	Assert( pgno >= prcb->runinfo.run );
	Assert( pgno < prcb->runinfo.run + prcb->runinfo.cpgUsed );
	
	//  read page

	CLockDeadlockDetectionInfo::DisableOwnershipTracking();
	err = ErrBFReadLatchPage(	&prcb->rgbfl[ipbf],
								prcb->pscb->fcb.Pfucb()->ifmp,
								pgno,
								bflfNoTouch );
	CLockDeadlockDetectionInfo::EnableOwnershipTracking();

	return err;
	}


//  Merges the specified number of runs from the source list into a new run in
//  the destination list

LOCAL ERR ErrSORTIMergeToRun( SCB *pscb, RUNLINK *prunlinkSrc, RUNLINK **pprunlinkDest )
	{
	ERR		err;
	LONG	irun;
	LONG	cbRun;
	LONG	crecRun;
	RUNLINK	*prunlink = prunlinkNil;
	SREC	*psrec;

	//  initialize merge
	
	CallR( ErrSORTIMergeStart( pscb, prunlinkSrc ) );

	//  calculate new run size

	for ( cbRun = 0, crecRun = 0, irun = 0; irun < pscb->crunMerge; irun++ )
		{
		cbRun += pscb->rgmtnode[irun].prcb->runinfo.cb;
		crecRun += pscb->rgmtnode[irun].prcb->runinfo.crec;
		}

	//  create a new run to receive merge data

	if ( ( prunlink = PrunlinkRUNLINKAlloc() ) == prunlinkNil )
		Error( ErrERRCheck( JET_errOutOfMemory ), EndMerge );

	CallJ( ErrSORTIRunStart( pscb, cbRun, crecRun, &prunlink->runinfo ), FreeRUNLINK );

	//  stream data from merge into run

	while ( ( err = ErrSORTIMergeNext( pscb, &psrec ) ) >= 0 )
		CallJ( ErrSORTIRunInsert( pscb, &prunlink->runinfo, psrec ), DeleteRun );

	if ( err < 0 && err != JET_errNoCurrentRecord )
		goto DeleteRun;

	SORTIRunEnd( pscb, &prunlink->runinfo );
	SORTIMergeEnd( pscb );

	//  add new run to destination run list

	prunlink->prunlinkNext = *pprunlinkDest;
	*pprunlinkDest = prunlink;

	return JET_errSuccess;

DeleteRun:
	SORTIRunEnd( pscb, &prunlink->runinfo );
	SORTIRunDelete( pscb, &prunlink->runinfo );
FreeRUNLINK:
	RUNLINKReleasePrunlink( prunlink );
EndMerge:
	SORTIMergeEnd( pscb );
	return err;
	}


/*	starts an n-way merge of the first n runs from the source run list.  The merge
/*	will remove duplicate values from the output if desired.
/**/
LOCAL ERR ErrSORTIMergeStart( SCB *pscb, RUNLINK *prunlinkSrc )
	{
	ERR		err;
	RUNLINK	*prunlink;
	LONG	crun;
	LONG	irun;
	MTNODE	*pmtnode;
#if defined( DEBUG ) || defined( PERFDUMP )
	char	szT[1024];
#endif

	/*	if termination in progress, then fail sort
	/**/
	if ( PinstFromIfmp( pscb->fcb.Ifmp() )->m_fTermInProgress )
		return ErrERRCheck( JET_errTermInProgress );

	/*	determine number of runs to merge
	/**/
	prunlink = prunlinkSrc;
	crun = 1;
	while ( prunlink->prunlinkNext != prunlinkNil )
		{
		prunlink = prunlink->prunlinkNext;
		crun++;
		}

	/*	we only support merging two or more runs
	/**/
	Assert( crun > 1 );

	/*	init merge data in SCB
	/**/
	pscb->crunMerge				= crun;
	pscb->bflLast.pv			= NULL;
	pscb->bflLast.dwContext		= NULL;
	pscb->pvAssyLast			= NULL;

#if defined( DEBUG ) || defined( PERFDUMP )
	sprintf( szT, "MERGE:  %ld runs -", crun );
#endif
	
	/*	initialize merge tree
	/**/
	prunlink = prunlinkSrc;
	for ( irun = 0; irun < crun; irun++ )
		{
		//  initialize external node

		pmtnode = pscb->rgmtnode + irun;
		Call( ErrSORTIRunOpen( pscb, &prunlink->runinfo, &pmtnode->prcb ) );
		pmtnode->pmtnodeExtUp = pscb->rgmtnode + ( irun + crun ) / 2;
		
		//  initialize internal node

		pmtnode->psrec = psrecNegInf;
		pmtnode->pmtnodeSrc = pmtnode;
		pmtnode->pmtnodeIntUp = pscb->rgmtnode + irun / 2;
		
#if defined( DEBUG ) || defined( PERFDUMP )
		sprintf(	szT + strlen( szT ),
					" %ld(%ld)",
					pmtnode->prcb->runinfo.run,
					pmtnode->prcb->runinfo.cpgUsed );
#endif

		//  get next run to open

		prunlink = prunlink->prunlinkNext;
		}

	return JET_errSuccess;

HandleError:
	pscb->crunMerge = 0;
	for ( irun--; irun >= 0; irun-- )
		SORTIRunClose( pscb->rgmtnode[irun].prcb );
	return err;
	}


//  Returns the first record of the current merge.  This function can be called
//  any number of times before ErrSORTIMergeNext is called to return the first
//  record, but it cannot be used to rewind to the first record after
//  ErrSORTIMergeNext is called.

LOCAL ERR ErrSORTIMergeFirst( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	
	//  if the tree still has init records, read past them to first record

	while ( pscb->rgmtnode[0].psrec == psrecNegInf )
		Call( ErrSORTIMergeNextChamp( pscb, ppsrec ) );

	//  return first record

	*ppsrec = pscb->rgmtnode[0].psrec;

	return JET_errSuccess;

HandleError:
	Assert( err != JET_errNoCurrentRecord );
	*ppsrec = NULL;
	return err;
	}


//  Returns the next record of the current merge, or JET_errNoCurrentRecord
//  if no more records are available.  You can call this function without
//  calling ErrSORTIMergeFirst to get the first record.

LOCAL ERR ErrSORTIMergeNext( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	SREC	*psrecLast;
	
	//  if the tree still has init records, return first record

	if ( pscb->rgmtnode[0].psrec == psrecNegInf )
		return ErrSORTIMergeFirst( pscb, ppsrec );

	//  get next record, performing duplicate removal

	do	{
		psrecLast = pscb->rgmtnode[0].psrec;
		CallR( ErrSORTIMergeNextChamp( pscb, ppsrec ) );
		}
	while ( FSORTIDuplicate( pscb, *ppsrec, psrecLast ) );

	return JET_errSuccess;
	}


//  Ends the current merge operation

LOCAL VOID SORTIMergeEnd( SCB *pscb )
	{
	LONG	irun;

	//  free / abandon BFs
	
	if ( pscb->bflLast.pv != NULL )
		{
		CLockDeadlockDetectionInfo::DisableOwnershipTracking();
		BFPurge( &pscb->bflLast, fTrue );
		CLockDeadlockDetectionInfo::EnableOwnershipTracking();
		pscb->bflLast.pv		= NULL;
		pscb->bflLast.dwContext	= NULL;
		}
	if ( pscb->pvAssyLast != NULL )
		{
		BFFree( pscb->pvAssyLast );
		pscb->pvAssyLast = NULL;
		}

	//  close all input runs
	
	for ( irun = 0; irun < pscb->crunMerge; irun++ )
		SORTIRunClose( pscb->rgmtnode[irun].prcb );
	pscb->crunMerge = 0;
	}


//  Returns next champion of the replacement-selection tournament on input
//  data.  If there is no more data, it will return JET_errNoCurrentRecord.
//  The tree is stored in losers' representation, meaning that the loser of
//  each tournament is stored at each node, not the winner.

LOCAL ERR ErrSORTIMergeNextChamp( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	MTNODE	*pmtnodeChamp;
	MTNODE	*pmtnodeLoser;

	//  goto exterior source node of last champ

	pmtnodeChamp = pscb->rgmtnode + 0;
	pmtnodeLoser = pmtnodeChamp->pmtnodeSrc;

	//  read next record (or lack thereof) from input run as the new
	//  contender for champ

	*ppsrec = NULL;
	err = ErrSORTIRunNext( pmtnodeLoser->prcb, &pmtnodeChamp->psrec );
	if ( err < 0 && err != JET_errNoCurrentRecord )
		return err;

	//  go up tree to first internal node

	pmtnodeLoser = pmtnodeLoser->pmtnodeExtUp;

	//  select the new champion by walking up the tree, swapping for lower
	//  and lower keys (or sentinel values)

	do	{
		//  if loser is psrecInf or champ is psrecNegInf, do not swap (if this
		//  is the case, we can't do better than we have already)

		if ( pmtnodeLoser->psrec == psrecInf || pmtnodeChamp->psrec == psrecNegInf )
			continue;

		//  if the loser is psrecNegInf or the current champ is psrecInf, or the
		//  loser is less than the champ, swap records

		if (	pmtnodeChamp->psrec == psrecInf ||
				pmtnodeLoser->psrec == psrecNegInf ||
				ISORTICmpKeyData( pmtnodeLoser->psrec, pmtnodeChamp->psrec ) < 0 )
			{
			SWAPPsrec( &pmtnodeLoser->psrec, &pmtnodeChamp->psrec );
			SWAPPmtnode( &pmtnodeLoser->pmtnodeSrc, &pmtnodeChamp->pmtnodeSrc );
			}
		}
	while ( ( pmtnodeLoser = pmtnodeLoser->pmtnodeIntUp ) != pmtnodeChamp );

	//  return the new champion

	if ( ( *ppsrec = pmtnodeChamp->psrec ) == NULL )
		return ErrERRCheck( JET_errNoCurrentRecord );

	return JET_errSuccess;
	}


//  initializes optimized tree merge

INLINE VOID SORTIOptTreeInit( SCB *pscb )
	{
	//  initialize runlist

	pscb->runlist.prunlinkHead		= prunlinkNil;
	pscb->runlist.crun				= 0;
	}


//  adds an initial run to be merged by optimized tree merge process

LOCAL ERR ErrSORTIOptTreeAddRun( SCB *pscb, RUNINFO *pruninfo )
	{
	RUNLINK	*prunlink;

	//  allocate and build a new RUNLINK for the new run

	if ( ( prunlink = PrunlinkRUNLINKAlloc() ) == prunlinkNil )
		return ErrERRCheck( JET_errOutOfMemory );
	prunlink->runinfo = *pruninfo;

	//  add the new run to the disk-resident runlist
	//
	//  NOTE:  by adding at the head of the list, we will guarantee that the
	//         list will be in ascending order by record count

	prunlink->prunlinkNext = pscb->runlist.prunlinkHead;
	pscb->runlist.prunlinkHead = prunlink;
	pscb->runlist.crun++;
	pscb->crun++;

	return JET_errSuccess;
	}


//  Performs an optimized tree merge of all runs previously added with
//  ErrSORTIOptTreeAddRun down to the last merge level (which is reserved
//  to be computed through the SORT iterators).  This algorithm is designed
//  to use the maximum fan-in as much as possible.

LOCAL ERR ErrSORTIOptTreeMerge( SCB *pscb )
	{
	ERR		err;
	OTNODE	*potnode = potnodeNil;
	
	//  If there are less than or equal to crunFanInMax runs, there is only
	//  one merge level -- the last one, which is to be done via the SORT
	//  iterators.  We are done.

	if ( pscb->runlist.crun <= crunFanInMax )
		return JET_errSuccess;

	//  build the optimized tree merge tree

	CallR( ErrSORTIOptTreeBuild( pscb, &potnode ) );

	//  perform all but the final merge

	Call( ErrSORTIOptTreeMergeDF( pscb, potnode, NULL ) );

	//  update the runlist information for the final merge

	Assert( pscb->runlist.crun == 0 );
	Assert( pscb->runlist.prunlinkHead == prunlinkNil );
	Assert( potnode->runlist.crun == crunFanInMax );
	Assert( potnode->runlist.prunlinkHead != prunlinkNil );
	pscb->runlist = potnode->runlist;

	//  free last node and return

	OTNODEReleasePotnode( potnode );
	return JET_errSuccess;

HandleError:
	if ( potnode != potnodeNil )
		{
		SORTIOptTreeFree( pscb, potnode );
		OTNODEReleasePotnode( potnode );
		}
	return err;
	}


//  free all optimized tree merge resources

INLINE VOID SORTIOptTreeTerm( SCB *pscb )
	{
	//  delete all runlists

	SORTIRunDeleteListMem( pscb, &pscb->runlist.prunlinkHead, crunAll );
	}


//  Builds the optimized tree merge tree by level in such a way that we use the
//  maximum fan-in as often as possible and the smallest merges (by length in
//  records) will be on the left side of the tree (smallest index in the array).
//  This will provide very high BF cache STATICity when the merge is performed
//  depth first, visiting subtrees left to right.

LOCAL ERR ErrSORTIOptTreeBuild( SCB *pscb, OTNODE **ppotnode )
	{
	ERR		err;
	OTNODE	*potnodeAlloc	= potnodeNil;
	OTNODE	*potnodeT;
	OTNODE	*potnodeLast2;
	LONG	crunLast2;
	OTNODE	*potnodeLast;
	LONG	crunLast;
	OTNODE	*potnodeThis;
	LONG	crunThis;
	LONG	crunFanInFirst;
	OTNODE	*potnodeFirst;
	LONG	ipotnode;
	LONG	irun;

	//  Set the original number of runs left for us to use.  If a last level
	//  pointer is potnodeLevel0, this means that we should use original runs for
	//  making the new merge level.  These runs come from this number.  We do
	//  not actually assign original runs to merge nodes until we actually
	//  perform the merge.

	potnodeLast2	= potnodeNil;
	crunLast2		= 0;
	potnodeLast		= potnodeLevel0;
	crunLast		= pscb->crun;
	potnodeThis		= potnodeNil;
	crunThis		= 0;

	//  create levels until the last level has only one node (the root node)

	do	{
		//  Create the first merge of this level, using a fan in that will result
		//  in the use of the maximum fan in as much as possible during the merge.
		//  We calculate this value every level, but it should only be less than
		//  the maximum fan in for the first merge level (but doesn't have to be).

		//  number of runs to merge

		if ( crunLast2 + crunLast <= crunFanInMax )
			crunFanInFirst = crunLast2 + crunLast;
		else
			crunFanInFirst = 2 + ( crunLast2 + crunLast - crunFanInMax - 1 ) % ( crunFanInMax - 1 );
		Assert( potnodeLast == potnodeLevel0 || crunFanInFirst == crunFanInMax );

		//  allocate and initialize merge node
		
		if ( ( potnodeT = PotnodeOTNODEAlloc() ) == potnodeNil )
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
		memset( potnodeT, 0, sizeof( OTNODE ) );
		potnodeT->potnodeAllocNext = potnodeAlloc;
		potnodeAlloc = potnodeT;
		ipotnode = 0;

		//  Add any leftover runs from the second to last level (the level before
		//  the last level) to the first merge of this level.

		Assert( crunLast2 < crunFanInMax );

		if ( potnodeLast2 == potnodeLevel0 )
			{
			Assert( potnodeT->runlist.crun == 0 );
			potnodeT->runlist.crun = crunLast2;
			}
		else
			{
			while ( potnodeLast2 != potnodeNil )
				{
				Assert( ipotnode < crunFanInMax );
				potnodeT->rgpotnode[ipotnode++] = potnodeLast2;
				potnodeLast2 = potnodeLast2->potnodeLevelNext;
				}
			}
		crunFanInFirst -= crunLast2;
		crunLast2 = 0;
			
		//  take runs from last level

		if ( potnodeLast == potnodeLevel0 )
			{
			Assert( potnodeT->runlist.crun == 0 );
			potnodeT->runlist.crun = crunFanInFirst;
			}
		else
			{
			for ( irun = 0; irun < crunFanInFirst; irun++ )
				{
				Assert( ipotnode < crunFanInMax );
				potnodeT->rgpotnode[ipotnode++] = potnodeLast;
				potnodeLast = potnodeLast->potnodeLevelNext;
				}
			}
		crunLast -= crunFanInFirst;

		//  save this node to add to this level later

		potnodeFirst = potnodeT;

		//  Create as many full merges for this level as possible, using the
		//  maximum fan in.
		
		while ( crunLast >= crunFanInMax )
			{
			//  allocate and initialize merge node

			if ( ( potnodeT = PotnodeOTNODEAlloc() ) == potnodeNil )
				Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			memset( potnodeT, 0, sizeof( OTNODE ) );
			potnodeT->potnodeAllocNext = potnodeAlloc;
			potnodeAlloc = potnodeT;
			ipotnode = 0;

			//  take runs from last level

			if ( potnodeLast == potnodeLevel0 )
				{
				Assert( potnodeT->runlist.crun == 0 );
				potnodeT->runlist.crun = crunFanInMax;
				}
			else
				{
				for ( irun = 0; irun < crunFanInMax; irun++ )
					{
					Assert( ipotnode < crunFanInMax );
					potnodeT->rgpotnode[ipotnode++] = potnodeLast;
					potnodeLast = potnodeLast->potnodeLevelNext;
					}
				}
			crunLast -= crunFanInMax;

			//  add this node to the current level

			potnodeT->potnodeLevelNext = potnodeThis;
			potnodeThis = potnodeT;
			crunThis++;
			}

		//  add the first merge to the current level

		potnodeFirst->potnodeLevelNext = potnodeThis;
		potnodeThis = potnodeFirst;
		crunThis++;

		//  Move level history back one level in preparation for next level.

		Assert( potnodeLast2 == potnodeNil || potnodeLast2 == potnodeLevel0 );
		Assert( crunLast2 == 0 );
		
		potnodeLast2	= potnodeLast;
		crunLast2		= crunLast;
		potnodeLast		= potnodeThis;
		crunLast		= crunThis;
		potnodeThis		= potnodeNil;
		crunThis		= 0;
		}
	while ( crunLast2 + crunLast > 1 );

	//  verify that all nodes / runs were used

	Assert( potnodeLast2 == potnodeNil || potnodeLast2 == potnodeLevel0 );
	Assert( crunLast2 == 0 );
	Assert(	potnodeLast != potnodeNil
			&& potnodeLast->potnodeLevelNext == potnodeNil );
	Assert( crunLast == 1 );

	//  return root node pointer

	*ppotnode = potnodeLast;
	return JET_errSuccess;
	
HandleError:
	while ( potnodeAlloc != potnodeNil )
		{
		SORTIRunDeleteListMem( pscb, &potnodeAlloc->runlist.prunlinkHead, crunAll );
		potnodeT = potnodeAlloc->potnodeAllocNext;
		OTNODEReleasePotnode( potnodeAlloc );
		potnodeAlloc = potnodeT;
		}
	*ppotnode = potnodeNil;
	return err;
	}

//  Performs an optimized tree merge depth first according to the provided
//  optimized tree.  When pprunlink is NULL, the current level is not
//  merged (this is used to save the final merge for the SORT iterator).

LOCAL ERR ErrSORTIOptTreeMergeDF( SCB *pscb, OTNODE *potnode, RUNLINK **pprunlink )
	{
	ERR		err;
	LONG	crunPhantom = 0;
	LONG	ipotnode;
	LONG	irun;
	RUNLINK	*prunlinkNext;

	//  if we have phantom runs, save how many so we can get them later

	if ( potnode->runlist.prunlinkHead == prunlinkNil )
		crunPhantom = potnode->runlist.crun;

	//  recursively merge all trees below this node

	for ( ipotnode = 0; ipotnode < crunFanInMax; ipotnode++ )
		{
		//  if this subtree pointer is potnodeNil, skip it

		if ( potnode->rgpotnode[ipotnode] == potnodeNil )
			continue;

		//  merge this subtree

		CallR( ErrSORTIOptTreeMergeDF(	pscb,
										potnode->rgpotnode[ipotnode],
										&potnode->runlist.prunlinkHead ) );
		OTNODEReleasePotnode( potnode->rgpotnode[ipotnode] );
		potnode->rgpotnode[ipotnode] = potnodeNil;
		potnode->runlist.crun++;
		}

	//  If this node has phantom (unbound) runs, we must grab the runs to merge
	//  from the list of original runs.  This is done to ensure that we use the
	//  original runs in the reverse order that they were generated to maximize
	//  the possibility of a BF cache hit.

	if ( crunPhantom > 0 )
		{
		for ( irun = 0; irun < crunPhantom; irun++ )
			{
			prunlinkNext = pscb->runlist.prunlinkHead->prunlinkNext;
			pscb->runlist.prunlinkHead->prunlinkNext = potnode->runlist.prunlinkHead;
			potnode->runlist.prunlinkHead = pscb->runlist.prunlinkHead;
			pscb->runlist.prunlinkHead = prunlinkNext;
			}
		pscb->runlist.crun -= crunPhantom;
		}

	//  merge all runs for this node

	if ( pprunlink != NULL )
		{
		//  merge the runs in the runlist
		
		CallR( ErrSORTIMergeToRun(	pscb,
									potnode->runlist.prunlinkHead, 
									pprunlink ) );
		SORTIRunDeleteList( pscb, &potnode->runlist.prunlinkHead, crunAll );
		potnode->runlist.crun = 0;
		}

	return JET_errSuccess;
	}


//  frees an optimized tree merge tree (except the given OTNODE's memory)

LOCAL VOID SORTIOptTreeFree( SCB *pscb, OTNODE *potnode )
	{
	LONG	ipotnode;

	//  recursively free all trees below this node

	for ( ipotnode = 0; ipotnode < crunFanInMax; ipotnode++ )
		{
		if ( potnode->rgpotnode[ipotnode] == potnodeNil )
			continue;

		SORTIOptTreeFree( pscb, potnode->rgpotnode[ipotnode] );
		OTNODEReleasePotnode( potnode->rgpotnode[ipotnode] );
		}

	//  free all runlists for this node

	SORTIRunDeleteListMem( pscb, &potnode->runlist.prunlinkHead, crunAll );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\slvsp.cxx ===
#include "std.hxx"
#include "_bt.hxx"


#ifndef RTM


//  ================================================================
LOCAL ERR ErrSLVISpaceTestInsert( PIB * const ppib, FUCB * const pfucb, const ULONG pgnoLast )
//  ================================================================
	{
	//  Insert the SLVSPACENODE
	SLVSPACENODE slvspacenode;
	slvspacenode.Init();

	ULONG ulKey;
	KeyFromLong( (BYTE *)&ulKey, pgnoLast );
	
	KEY key;
	key.prefix.Nullify();
	key.suffix.SetPv( (BYTE *)&ulKey );
	key.suffix.SetCb( sizeof( ulKey ) );

	DATA data;
	data.SetPv( &slvspacenode );
	data.SetCb( sizeof( slvspacenode ) );

	return ErrBTInsert( pfucb, key, data, fDIRNull );
	}


//  ================================================================
LOCAL ERR ErrSLVISpaceTestSeek( PIB * const ppib, FUCB * const pfucb, const ULONG pgnoLast )
//  ================================================================
	{
	ULONG ulKey;
	KeyFromLong( (BYTE *)&ulKey, pgnoLast );

	BOOKMARK bookmark;
	bookmark.key.prefix.Nullify();
	bookmark.key.suffix.SetPv( (BYTE *)&ulKey );
	bookmark.key.suffix.SetCb( sizeof( ulKey ) );
	bookmark.data.Nullify();

	//  Seek to the SLVSPACENODE
	DIB dib;
	dib.pos = posDown;
	dib.pbm = &bookmark;
	dib.dirflag = fDIRNull;
	return ErrBTDown( pfucb, &dib, latchRIW );
	}


//  ================================================================
LOCAL ERR ErrSLVISpaceTestMunge(
	PIB * const ppib,
	FUCB * const pfucb,
	const ULONG pgnoLast,
	const SLVSPACEOPER slvspaceoper,
	const LONG ipage,
	const LONG cpages,
	const DIRFLAG dirflag )
//  ================================================================
	{
	ERR err;
	Call( ErrSLVISpaceTestSeek( ppib, pfucb, pgnoLast ) );
	CallS( Pcsr( pfucb )->ErrUpgrade() );
	Call( ErrBTMungeSLVSpace(
				pfucb,
				slvspaceoper,
				ipage,
				cpages,
				dirflag ) );
	BTUp( pfucb );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVISpaceTestDelete( PIB * const ppib, FUCB * const pfucb, const ULONG pgnoLast )
//  ================================================================
	{
	ERR err;
	CallR( ErrSLVISpaceTestSeek( ppib, pfucb, pgnoLast ) );
	CallR( ErrBTRelease( pfucb ) );
	return ErrBTFlagDelete( pfucb, fDIRNull );
	}


//  ================================================================
LOCAL ERR ErrSLVISpaceTest( PIB * const ppib, FUCB * const pfucb, const ULONG pgnoLast )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	Call( ErrSLVISpaceTestSeek( ppib, pfucb, pgnoLast ) );
	CallS( Pcsr( pfucb )->ErrUpgrade() );

	//  FREE => RESERVED
	Call( ErrBTMungeSLVSpace(
				pfucb,
				slvspaceoperFreeToReserved,
				23,
				13,
				fDIRNull ) );

	//  RESERVED => COMMITTED
	Call( ErrBTMungeSLVSpace(
				pfucb,
				slvspaceoperReservedToCommitted,
				23,
				13,
				fDIRNull ) );

	//  FREE => COMMITTED
	Call( ErrBTMungeSLVSpace(
				pfucb,
				slvspaceoperFreeToCommitted,
				36,
				1,
				fDIRNull ) );

	//  COMMITTED => DELETED
	Call( ErrBTMungeSLVSpace(
				pfucb,
				slvspaceoperCommittedToDeleted,
				23,
				14,
				fDIRNull ) );

	//  After the commit RCEClean should pick up the deleted space and move it 
	//  to free
	
	BTUp( pfucb );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVISpaceTestRollback( PIB * const ppib, FUCB * const pfucb, const ULONG pgnoLast )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  FREE => RESERVED => ROLLBACK
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperFreeToReserved,
			47,
			16,
			fDIRNull ) );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	//  RESERVED => COMMITTED => ROLLBACK
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperFreeToReserved,
			100,
			16,
			fDIRNull ) );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperReservedToCommitted,
			100,
			16,
			fDIRNull ) );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	//  FREE => RESERVED => COMMITTED => ROLLBACK
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperFreeToReserved,
			100,
			16,
			fDIRNull ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperReservedToCommitted,
			100,
			16,
			fDIRNull ) );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
	
	//  FREE => COMMITTED => ROLLBACK
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperFreeToCommitted,
			300,
			16,
			fDIRNull ) );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

	//  COMMITTED => DELETED => ROLLBACK
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperFreeToCommitted,
			399,
			16,
			fDIRNull ) );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			pgnoLast,
			slvspaceoperCommittedToDeleted,
			399,
			16,
			fDIRNull ) );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

HandleError:
	return err;
	}

//  ================================================================
ERR ErrSLVISpaceTestSetupReservedAndDeleted( PIB * const ppib, FUCB * const pfucb )
//  ================================================================
	{
	ERR err;
	
	//  NODE 1: all pages reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			2 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			0,
			SLVSPACENODE::cpageMap,
			fDIRNoVersion ) );

	//  NODE 4: all pages deleted
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			5 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToCommitted,
			0,
			SLVSPACENODE::cpageMap,
			fDIRNoVersion ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			5 * SLVSPACENODE::cpageMap, 
			slvspaceoperCommittedToDeleted,
			0,
			SLVSPACENODE::cpageMap,
			fDIRNoVersion ) );

	//  NODE 27: first reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			28 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			0,
			1,
			fDIRNoVersion ) );

	//  NODE 28: last deleted
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			29 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToCommitted,
			SLVSPACENODE::cpageMap - 1,
			1,
			fDIRNoVersion ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			29 * SLVSPACENODE::cpageMap, 
			slvspaceoperCommittedToDeleted,
			SLVSPACENODE::cpageMap - 1,
			1,
			fDIRNoVersion ) );

	//  NODE 29: some reserved, some deleted
	//		10 :10  -- reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			10,
			10,
			fDIRNoVersion ) );
	//		20 :20  -- deleted
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToCommitted,
			20,
			60,	//	leave some committed nodes
			fDIRNoVersion ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperCommittedToDeleted,
			20,
			20,
			fDIRNoVersion ) );
	//		100:15  -- reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			100,
			15,
			fDIRNoVersion ) );
	//		115:3   -- deleted
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToCommitted,
			115,
			5,
			fDIRNoVersion ) );
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperCommittedToDeleted,
			115,
			5,
			fDIRNoVersion ) );
	//		150:1  -- reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			150,
			1,
			fDIRNoVersion ) );
	//		152:1  -- reserved
	Call( ErrSLVISpaceTestMunge(
			ppib,
			pfucb,
			30 * SLVSPACENODE::cpageMap, 
			slvspaceoperFreeToReserved,
			152,
			1,
			fDIRNoVersion ) );

HandleError:
	return err;
	}


//  ================================================================
ERR ErrSLVSpaceTest( PIB * const ppib, FUCB * const pfucb )
//  ================================================================
//
//  Insert a node and try the SLV space operations on it
//
//-
	{
	ERR err = JET_errSuccess;

	const INT cnodesMax = 32;
	INT cnodes;

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[pfucb->ifmp].Pslvspacenodecache();
	if( pslvspacenodecache )
		{
		const CPG cpg = cnodesMax * SLVSPACENODE::cpageMap;
		Call( pslvspacenodecache->ErrGrowCache( cpg ) );
		}

	//  Insert the nodes
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	for( cnodes = 0; cnodes < cnodesMax; ++cnodes )
		{
		const ULONG pgnoLast = (cnodes + 1) * SLVSPACENODE::cpageMap;
		Call( ErrSLVISpaceTestInsert( ppib, pfucb, pgnoLast ) );
		BTUp( pfucb );

		if( pslvspacenodecache )
			{
			pslvspacenodecache->SetCpgAvail( pgnoLast, SLVSPACENODE::cpageMap );
			}
		}
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

 	//  Tests
	Call( ErrSLVISpaceTest( ppib, pfucb, 7 * SLVSPACENODE::cpageMap ) );
	Call( ErrSLVISpaceTest( ppib, pfucb, 13 * SLVSPACENODE::cpageMap ) );
	Call( ErrSLVISpaceTestRollback( ppib, pfucb, 12 * SLVSPACENODE::cpageMap ) );
	Call( ErrSLVISpaceTestRollback( ppib, pfucb, 1 * SLVSPACENODE::cpageMap ) );

	Call( ErrSLVISpaceTestSetupReservedAndDeleted( ppib, pfucb ) );
	ULONG cpagesSeen;
	ULONG cpagesReset;
	ULONG cpagesFree;
	Call( ErrSLVResetAllReservedOrDeleted( pfucb, &cpagesSeen, &cpagesReset, &cpagesFree ) );

	//  make sure that all the nodes have been reset
	for( cnodes = 0; cnodes < cnodesMax; ++cnodes )
		{
		const ULONG pgnoLast = (cnodes + 1) * SLVSPACENODE::cpageMap;
		Call( ErrSLVISpaceTestSeek( ppib, pfucb, pgnoLast ) );
		const SLVSPACENODE * const pspacenode = (SLVSPACENODE *)pfucb->kdfCurr.data.Pv();
		const LONG ipageFirstReservedOrDeleted = pspacenode->IpageFirstReservedOrDeleted();
		Assert( ipageFirstReservedOrDeleted == SLVSPACENODE::cpageMap );
		BTUp( pfucb );
		}

	//  Let the version store cleanup
	Call( ErrIsamIdle( (JET_SESID)ppib, JET_bitIdleCompact ) );

	//  Remove the nodes
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	for( cnodes = 0; cnodes < cnodesMax; ++cnodes )
		{
		const ULONG pgnoLast = (cnodes + 1) * SLVSPACENODE::cpageMap;
		Call( ErrSLVISpaceTestDelete( ppib, pfucb, pgnoLast ) );
		BTUp( pfucb );
		}
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

HandleError:
	return err;
	}
#endif	//  !RTM


ERR ErrSLVInsertSpaceNode( FUCB *pfucbSLVAvail, const PGNO pgnoLast )
	{
	KEY				key;
	DATA			data;
	SLVSPACENODE	slvspnode;
	BYTE			rgbKey[sizeof(PGNO)];

	Assert( pfucbNil != pfucbSLVAvail );
	Assert( pgnoLast >= cpgSLVFileMin );

	KeyFromLong( rgbKey, pgnoLast );
	key.prefix.Nullify();
	key.suffix.SetPv( rgbKey );
	key.suffix.SetCb( sizeof(PGNO) );

	slvspnode.Init();
	Assert( cpgSLVExtent == slvspnode.CpgAvail() );

	data.SetPv( (BYTE *)&slvspnode );
	data.SetCb( sizeof(SLVSPACENODE) );

	BTUp( pfucbSLVAvail );
	const ERR	 err = ErrBTInsert( pfucbSLVAvail, key, data, fDIRNoVersion, NULL );
	Assert( err < 0 || Pcsr( pfucbSLVAvail )->FLatched() );

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[pfucbSLVAvail->ifmp].Pslvspacenodecache();
	if( err >= 0 && pslvspacenodecache )
		{
		pslvspacenodecache->SetCpgAvail( pgnoLast, cpgSLVExtent );
		}
		
	return err;
	}
	
ERR ErrSLVCreateAvailMap( PIB *ppib, FUCB *pfucbDb )
	{
	ERR				err;
	const IFMP		ifmp		= pfucbDb->ifmp;
	FUCB			*pfucbSLV	= pfucbNil;
	PGNO			pgnoSLVFDP;
	OBJID			objidSLV;
	const BOOL		fTempDb		= ( dbidTemp == rgfmp[ifmp].Dbid() );

	Assert( ppibNil != ppib );
	Assert( rgfmp[ifmp].FCreatingDB() || fGlobalRepair );
	Assert( 0 == ppib->level || ( 1 == ppib->level && rgfmp[ifmp].FLogOn() ) || fGlobalRepair );
	Assert( pfucbDb->u.pfcb->FTypeDatabase() );
	
	//  CONSIDER:  get at least enough pages to hold the LV we are about to insert
	//  we must open the directory with a different session.
	//	if this fails, rollback will free the extent, or at least, it will attempt
	//  to free the extent.
	CallR( ErrDIRCreateDirectory(
				pfucbDb,
				cpgSLVAvailTree,
				&pgnoSLVFDP,
				&objidSLV,
				CPAGE::fPageSLVAvail,
				fSPMultipleExtent | fSPUnversionedExtent ) );

	Assert( pgnoSLVFDP > pgnoSystemRoot );
	Assert( pgnoSLVFDP <= pgnoSysMax );

	Call( ErrBTOpen( ppib, pgnoSLVFDP, ifmp, &pfucbSLV ) );
	Assert( pfucbNil != pfucbSLV );

	Call( ErrSLVInsertSpaceNode( pfucbSLV, cpgSLVFileMin ) );
	Assert( Pcsr( pfucbSLV )->FLatched() );
	CallS( ErrBTRelease( pfucbSLV ) );

	//	ensure FUCB doesn't get defer-closed and FCB gets freed
	//	on subsequent BTClose() below
	Assert( !FFUCBVersioned( pfucbSLV ) );
	Assert( !pfucbSLV->u.pfcb->FInitialized() );

	if ( !fTempDb )
		{
		Assert( objidSLV > objidSystemRoot );
		Call( ErrCATAddDbSLVAvail(
					ppib,
					ifmp,
					szSLVAvail,
					pgnoSLVFDP,
					objidSLV ) );
		}
	else
		{
		Assert( pgnoTempDbSLVAvail == pgnoSLVFDP );
		Assert( objidTempDbSLVAvail == objidSLV );
		}

	BTClose( pfucbSLV );
	pfucbSLV = pfucbNil;
	
	if( fGlobalRepair )
		{
		FCB * pfcbSLVAvail = pfcbNil;
		Call( ErrSLVAvailMapInit( ppib, ifmp, pgnoSLVFDP, &pfcbSLVAvail ) );
		Assert( pfcbNil != pfcbSLVAvail );
		}

HandleError:
	if ( pfucbNil != pfucbSLV )
		{
		BTClose( pfucbSLV );
		}
	return err;
	}


//  ================================================================
ERR ErrSLVResetAllReservedOrDeleted( FUCB * const pfucb, ULONG * const pcpagesSeen, ULONG * const pcpagesReset, ULONG * const pcpagesFree )
//  ================================================================
//
//  Walks the SLV space tree pointed to by the pfucb and resets all
//  reserved or deleted pages to free. This is done unversioned (we
//  aren't interested in rollback and will have to re-examine the tree
//  if we crash).
//
//  The number of SLV pages seen and reset is returned
//  
//
//-
	{
	ERR err = JET_errSuccess;

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[pfucb->ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache );
	
	CallR( ErrDIRBeginTransaction( pfucb->ppib, NO_GRBIT ) );

	LONG cpagesReset 	= 0;
	LONG cpagesSeen		= 0;
	LONG cpagesFree 	= 0;
	
	DIB dib;

	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;

	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	err = ErrBTDown( pfucb, &dib, latchReadTouch );
	if( JET_errNoCurrentRecord == err )
		{
		//  the tree is empty
		err = JET_errSuccess;
		goto HandleError;
		}
	Call( err );

	do
		{
		if( sizeof( SLVSPACENODE ) != pfucb->kdfCurr.data.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Data is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( sizeof( PGNO ) != pfucb->kdfCurr.key.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Key is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
			
		cpagesSeen = cpagesSeen + SLVSPACENODE::cpageMap;

		ULONG pgnoCurr;
		LongFromKey( &pgnoCurr, pfucb->kdfCurr.key );
		if( cpagesSeen != pgnoCurr )
			{
			AssertSz( fFalse, "SLV space tree corruption. Nodes out of order" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		const SLVSPACENODE * pspacenode = (SLVSPACENODE *)pfucb->kdfCurr.data.Pv();

#ifndef RTM
		Call( pspacenode->ErrCheckNode( CPRINTFDBGOUT::PcprintfInstance() ) );
#endif	//	RTM

		if( pspacenode->CpgAvail() == SLVSPACENODE::cpageMap )
			{
			goto NextNode;
			}
			
		LONG ipageFirstReservedOrDeleted;
		ipageFirstReservedOrDeleted = pspacenode->IpageFirstReservedOrDeleted();
		Assert( ipageFirstReservedOrDeleted <= SLVSPACENODE::cpageMap );
		if( ipageFirstReservedOrDeleted >= SLVSPACENODE::cpageMap )
			{
			goto NextNode;
			}
		else
			{
			LATCH		latch		= latchReadTouch;
						
Refresh:
			//	upgrade latch on page
			//
			err = Pcsr( pfucb )->ErrUpgrade();
			if ( errBFLatchConflict == err )
				{
				Assert( !Pcsr( pfucb )->FLatched() );

				latch = latchRIW;
				Call ( ErrBTIRefresh( pfucb, latch ) );

				goto Refresh;
				}
			Call( err );
			
			Assert( latchWrite == Pcsr( pfucb )->Latch() );

			//  NOTE: the page may have moved while being upgraded (??)
			Assert( sizeof( SLVSPACENODE ) == pfucb->kdfCurr.data.Cb() );
			pspacenode = (SLVSPACENODE *)pfucb->kdfCurr.data.Pv();

			LONG 				ipageStartRun	= ipageFirstReservedOrDeleted;
			LONG 				cpagesRun		= 0;
			SLVSPACENODE::STATE stateRun		= SLVSPACENODE::sReserved;	//  guess that it is reserved
			
			LONG ipage;
			for( ipage = ipageFirstReservedOrDeleted; ipage < SLVSPACENODE::cpageMap; ++ipage )
				{
				const SLVSPACENODE::STATE state = pspacenode->GetState( ipage );
				if( state == stateRun )
					{
					//  this is part of the run
					++cpagesRun;
					}
				else
					{
					//  free the pages in the old run
					if( cpagesRun > 0 )
						{
						Assert( SLVSPACENODE::sReserved == stateRun
								|| SLVSPACENODE::sDeleted == stateRun );
						const SLVSPACEOPER oper = ( stateRun == SLVSPACENODE::sDeleted ) ? slvspaceoperFree : slvspaceoperFreeReserved;
						
						// No need to mark the pages as unused in the OwnerMap
						// because the operation isn't moving pages out from the commited state
						// (transition into commited state are not marked in OwnerMap at this level)
						Call( ErrBTMungeSLVSpace( pfucb, oper, ipageStartRun, cpagesRun, fDIRNoVersion ) );
						cpagesReset += cpagesRun;
						}

					//  start a new run if necessary
					if( SLVSPACENODE::sReserved == state
						|| SLVSPACENODE::sDeleted == state )
						{
						stateRun 		= state;
						ipageStartRun	= ipage;
						cpagesRun 		= 1;
						}
					else
						{
						stateRun 		= SLVSPACENODE::sInvalid;
						cpagesRun 		= 0;
#ifdef DEBUG
						ipageStartRun 	= 0xfefffeff;
#endif	//	DEBUG
						}
					}
				}
			//  if the run was at the end of the page we may not have finished
			if( cpagesRun > 0 )
				{
				Assert( SLVSPACENODE::sReserved == stateRun
						|| SLVSPACENODE::sDeleted == stateRun );
				const SLVSPACEOPER oper = ( stateRun == SLVSPACENODE::sDeleted ) ? slvspaceoperFree : slvspaceoperFreeReserved;
				
				// No need to mark the pages as unused in the OwnerMap
				// because the operation isn't moving pages out from the commited state
				// (transition into commited state are not marked in OwnerMap at this level)
				Call( ErrBTMungeSLVSpace( pfucb, oper, ipageStartRun, cpagesRun, fDIRNoVersion ) );
				cpagesReset += cpagesRun;
				}
				
			Assert( latchWrite == Pcsr( pfucb )->Latch() );
			Pcsr( pfucb )->Downgrade( latchReadTouch );			
			}

NextNode:

///		printf( "%d:%d\n", pgnoCurr, pspacenode->CpgAvail() );
		pslvspacenodecache->SetCpgAvail( pgnoCurr, pspacenode->CpgAvail() );
			
		cpagesFree = cpagesFree + pspacenode->CpgAvail();
		
		} while( JET_errSuccess == ( err = ErrBTNext( pfucb, fDIRNull ) ) );

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
		
	BTUp( pfucb );
	Call( ErrDIRCommitTransaction( pfucb->ppib, JET_bitCommitLazyFlush ) );
	
HandleError:
	if( pcpagesSeen )
		{
		*pcpagesSeen = cpagesSeen;
		}
	if( pcpagesReset )
		{
		*pcpagesReset = cpagesReset;
		}
	if( pcpagesFree )
		{
		*pcpagesFree = cpagesFree;
		}
		
	BTUp( pfucb );

	if ( err < JET_errSuccess )
		{
		CallSx( ErrDIRRollback( pfucb->ppib ), JET_errRollbackError );
		}

	return err;
	}


//  ================================================================
SLVSPACENODE::SLVSPACENODE()
//  ================================================================
	{
	//  do not initialize any member variables so that we can
	//  use placement new
	}

	
//  ================================================================
SLVSPACENODE::~SLVSPACENODE()
//  ================================================================
	{
	ASSERT_VALID( this );	
	}
	

//  ================================================================
VOID SLVSPACENODE::EnforcePageState( const LONG ipage, const LONG cpg, const STATE state ) const
//  ================================================================
	{
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const STATE stateReal = GetState_( ipageT );
		if( stateReal != state )
			{
			const CHAR * rgsz[2];
			INT irgsz = 0;

			CHAR szStateExpected[16];
			CHAR szStateActual[16];

			sprintf( szStateExpected, "%d", state );
			sprintf( szStateActual, "%d", stateReal );
			
			rgsz[irgsz++] = szStateExpected;
			rgsz[irgsz++] = szStateActual;
			
			UtilReportEvent(	eventError,
								DATABASE_CORRUPTION_CATEGORY,
								CORRUPT_SLV_SPACE_ID,
								irgsz,
								rgsz );
								
			EnforceSz( fFalse, "SLVSPACENODE::EnforcePageState" );
			}
		}
	}


#if defined( DEBUG ) || !defined( RTM )
//  ================================================================
VOID SLVSPACENODE::AssertPageState( const LONG ipage, const LONG cpg, const STATE state ) const
//  ================================================================
	{
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const STATE stateReal = GetState_( ipageT );
		Assert( state == stateReal );
		}
	}


//  ================================================================
VOID SLVSPACENODE::AssertValid() const
//  ================================================================
	{
	Assert( m_cpgAvail >= 0 );
	Assert( m_cpgAvail <= SLVSPACENODE::cpageMap );
	Assert( 0 == m_cpgAvailLargestContig );
	INT cpgAvail = 0;
	INT ipage;
	for( ipage = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		const STATE state = GetState_( ipage );
		Assert( sFree == state
				|| sReserved == state
				|| sDeleted == state
				|| sCommitted == state );
		if( sFree == state )
			{
			++cpgAvail;
			}
		}
	Assert( m_cpgAvail == cpgAvail );
	}
	

//  ================================================================
VOID SLVSPACENODE::Test()
//  ================================================================
	{
	Assert( 0x0 == sFree );
	Assert( 0x3 == sCommitted );
	
	SLVSPACENODE slvspacenode;
	slvspacenode.Init();
	INT ipage;
	INT ipageT;

	//  FREE => RESERVED => COMMITTED => DELETED
	for( ipage = SLVSPACENODE::cpageMap - 1; ipage >= 0; --ipage )
		{
		Assert( slvspacenode.CpgAvail() == ipage + 1 );
		Assert( slvspacenode.GetState( ipage ) == sFree );
		slvspacenode.Reserve( ipage, 1 );
		Assert( slvspacenode.GetState( ipage ) == sReserved );
		Assert( slvspacenode.CpgAvail() == ipage );
		slvspacenode.CommitFromReserved( ipage, 1 );
		Assert( slvspacenode.GetState( ipage ) == sCommitted );
		Assert( slvspacenode.CpgAvail() == ipage );
		slvspacenode.DeleteFromCommitted( ipage, 1 );
		Assert( slvspacenode.GetState( ipage ) == sDeleted );
		Assert( slvspacenode.CpgAvail() == ipage );
		}

	//  DELETED => FREE
	for( ipage = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		Assert( slvspacenode.CpgAvail() == ipage );
		slvspacenode.Free( ipage, 1 );
		Assert( slvspacenode.GetState( ipage ) == sFree );
		Assert( slvspacenode.CpgAvail() == ipage + 1 );
		}

	//  FREE => COMMITTED
	slvspacenode.CommitFromFree( 0, SLVSPACENODE::cpageMap );
	slvspacenode.AssertPageState( 0, SLVSPACENODE::cpageMap, sCommitted );
	Assert( 0 == slvspacenode.CpgAvail() );

	//  COMMITTED => FREE
	slvspacenode.Free( 0, SLVSPACENODE::cpageMap );
	slvspacenode.AssertPageState( 0, SLVSPACENODE::cpageMap, sFree );
	Assert( SLVSPACENODE::cpageMap == slvspacenode.CpgAvail() );

	//  IpageFirstFree
	//  move every node to the committed state and free each one separately
	slvspacenode.CommitFromFree( 0, SLVSPACENODE::cpageMap );
	slvspacenode.AssertPageState( 0, SLVSPACENODE::cpageMap, sCommitted );
	Assert( 0 == slvspacenode.CpgAvail() );

	for( ipage = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		slvspacenode.Free( ipage, 1 );
		const LONG ipageFree = slvspacenode.IpageFirstFree();
		Assert( ipageFree == ipage );
		slvspacenode.CommitFromFree( ipage, 1 );
		}

	//  IpageFirstReservedOrDeleted & IpageFirstCommitted
	//  move every node to the free state and commit
	ipageT = slvspacenode.IpageFirstReservedOrDeleted();
	Assert( SLVSPACENODE::cpageMap == ipageT );	//  all nodes are committed

	slvspacenode.Free( 0, SLVSPACENODE::cpageMap );
	slvspacenode.AssertPageState( 0, SLVSPACENODE::cpageMap, sFree );
	Assert( SLVSPACENODE::cpageMap == slvspacenode.CpgAvail() );

	ipageT = slvspacenode.IpageFirstReservedOrDeleted();
	Assert( SLVSPACENODE::cpageMap == ipageT );	//  all nodes are free

	for( ipage = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		LONG ipageFirstReservedOrDeleted;
		LONG ipageFirstCommitted;
		
		slvspacenode.Reserve( ipage, 1 );
		ipageFirstReservedOrDeleted = slvspacenode.IpageFirstReservedOrDeleted();
		Assert( ipageFirstReservedOrDeleted == ipage );
		
		slvspacenode.CommitFromReserved( ipage, 1 );
		ipageFirstCommitted = slvspacenode.IpageFirstCommitted();
		Assert( ipageFirstCommitted == ipage );		
		
		slvspacenode.DeleteFromCommitted( ipage, 1 );
		ipageFirstReservedOrDeleted = slvspacenode.IpageFirstReservedOrDeleted();
		Assert( ipageFirstReservedOrDeleted == ipage );		

		slvspacenode.Free( ipage, 1 );
		}	
	
	}
#endif //  DEBUG \\ !RTM


//  ================================================================
VOID SLVSPACENODE::Init()
//  ================================================================
	{
	m_cpgAvail 				= SLVSPACENODE::cpageMap;
	m_cpgAvailLargestContig = 0;	//	not currently maintained
	m_ulFlags				= 0;
	memset( m_rgbMap, 0x00, SLVSPACENODE::cbMap );
	ASSERT_VALID( this );
	}


//  ================================================================
LONG SLVSPACENODE::CpgAvail() const
//  ================================================================
	{
	ASSERT_VALID( this );	
	return m_cpgAvail;
	}


//  ================================================================
SLVSPACENODE::STATE SLVSPACENODE::GetState( const ULONG ipage ) const
//  ================================================================
	{
	Assert( ipage < SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	return GetState_( ipage );
	}


//  ================================================================
LONG SLVSPACENODE::IpageFirstFree() const
//  ================================================================
//
//  Returns the index of the first free page. Search by DWORDs for
//  speed
//
//  A DWORD contains 16 pairs of bits. We are looking for a pair where
//  both bits are zero. We shift the top bit of each pair down by one,
//  OR with the lower bits of each pair and set the unused bits to 0.
//  By comparing the results with the result if all pages were used
//  we can quickly determine if at least one page is free.
//
//  We then use a loop to find the first free page
//
//-
	{
	ASSERT_VALID( this );
	Assert( 0x00 == sFree );
	Assert( cbMap % sizeof( DWORD ) == 0 );
	Assert( 0 != m_cpgAvail );	//	don't call this if there aren't any free pages
	INT idw;
	for( idw = 0; idw < cbMap / sizeof( DWORD ); ++idw )
		{
		const DWORD dwMaskLowBits	= 0x55555555;	//	only the low bits of each pair
		const DWORD dw 				= ((UnalignedLittleEndian< DWORD > *)m_rgbMap)[idw];
		DWORD dwPairs 				= ( dw | ( dw >> 1 ) );
		if(  ( dwPairs & dwMaskLowBits ) != dwMaskLowBits )
			{
			INT ipageInDword;
			for( ipageInDword = 0; ipageInDword < cpagesInDword; ++ipageInDword )
				{
				if( !( dwPairs & 1 ) )
					{
					//  this page is free
					return ipageInDword + ( idw * cpagesInDword );
					}
				dwPairs >>= 2;
				}
			Assert( fFalse );
			}
		}
	Assert( fFalse );
	return idw * cpagesInDword;
	}


//  ================================================================
LONG SLVSPACENODE::IpageFirstCommitted() const
//  ================================================================
//
//  Returns the index of the first committed page. Search by DWORDs for
//  speed
//
//  A DWORD contains 16 pairs of bits. We are looking for a pair where
//  both bits are one. We shift the top bit of each pair down by one,
//  AND with the lower bits of each pair and set the unused bits to 0.
//  By comparing the result with 0 we can quickly determine if at least
//  one page is free committed
//
//  We then use a loop to find the first committed page
//
//-
	{
	ASSERT_VALID( this );
	Assert( 0x03 == sCommitted );
	Assert( cbMap % sizeof( DWORD ) == 0 );
	INT idw;
	for( idw = 0; idw < cbMap / sizeof( DWORD ); ++idw )
		{
		const DWORD dwMaskLowBits	= 0x55555555;	//	only the low bits of each pair
		const DWORD dw 				= ((UnalignedLittleEndian< DWORD > *)m_rgbMap)[idw];
		DWORD dwPairs 				= ( dw & ( dw >> 1 ) ) & dwMaskLowBits;
		if( dwPairs != 0 )
			{
			
			//  at least one of the pages in this DWORD is committed
			
			INT ipageInDword;
			for( ipageInDword = 0; ipageInDword < cpagesInDword; ++ipageInDword )
				{
				if( dwPairs & 0x1 )
					{
					//  this page is committed
					return ipageInDword + ( idw * cpagesInDword );
					}
				dwPairs >>= 2;
				}
			}
		}
	return idw * cpagesInDword;
	}


//  ================================================================
LONG SLVSPACENODE::IpageFirstReservedOrDeleted() const
//  ================================================================
//
//  Returns the index of the first reserved or deleted page.
//  Search by DWORDs for speed
//
//  A DWORD contains 16 pairs of bits. We are looking for a pair where
//  both bits are different. We shift the top bit of each pair down by one,
//  XOR with the lower bits of each pair and set the unused bits to 0.
//  By comparing the results with the result if all pairs had the same bits
//  (i.e. 0) we can quickly determine if at least one page is reserved/deleted.
//
//  We then use a loop to find the first free page
//
//-
	{
	ASSERT_VALID( this );
	Assert( 0x01 == sReserved );
	Assert( 0x02 == sDeleted );
	Assert( cbMap % sizeof( DWORD ) == 0 );
	INT idw;
	for( idw = 0; idw < cbMap / sizeof( DWORD ); ++idw )
		{
		const DWORD dwMaskLowBits	= 0x55555555;	//	only the low bits of each pair
		const DWORD dw 				= ((UnalignedLittleEndian< DWORD > *)m_rgbMap)[idw];
		DWORD dwPairs 				= ( dw ^ ( dw >> 1 ) );
		if( ( dwPairs & dwMaskLowBits ) != 0 )
			{
			INT ipageInDword;
			for( ipageInDword = 0; ipageInDword < cpagesInDword; ++ipageInDword )
				{
				if( dwPairs & 1 )
					{
					//  this page is free
					return ipageInDword + ( idw * cpagesInDword );
					}
				dwPairs >>= 2;
				}
			Assert( fFalse );
			}
		}
	return idw * cpagesInDword;
	}


//  ================================================================
ERR SLVSPACENODE::ErrCheckNode( CPRINTF * pcprintfError ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	LONG ipage;
	LONG cpgAvail;
	for( ipage = 0, cpgAvail = 0; ipage < SLVSPACENODE::cpageMap; ++ipage )
		{
		const SLVSPACENODE::STATE state = GetState( ipage );
		if( SLVSPACENODE::sFree == state )
			{
			++cpgAvail;
			}
		}

	if( CpgAvail() != cpgAvail )
		{
		(*pcprintfError)( "SLVSPACENODE: cpgAvail is wrong (%d, expected %d)\r\n", CpgAvail(), cpgAvail );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	return err;
	}


//  ================================================================
ERR SLVSPACENODE::ErrGetFreePagesFromExtent(
	FUCB		*pfucbSLVAvail,
	PGNO		*ppgnoFirst,			//	initially pass in first page of extent, pass NULL for subsequent extents
	CPG			*pcpgReq,
	const BOOL	fReserveOnly )
//  ================================================================
//
//	first the first range of free pages within this extent
//
//-
	{
	CPG			cpgReq			= *pcpgReq;
	UINT		ipageFirst;

	//	must be in transaction because caller will be required to rollback on failure
	Assert( pfucbSLVAvail->ppib->level > 0 );

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );

	Assert( NULL == ppgnoFirst || pgnoNull != *ppgnoFirst );

	Assert( cpgReq > 0 );
	*pcpgReq = 0;

	Assert( NULL == ppgnoFirst || CpgAvail() > 0 );

	if ( cpgSLVExtent == CpgAvail() )
		{
		//	special case: entire page free
		ipageFirst = 0;
		*pcpgReq = min( cpgReq, cpgSLVExtent );
		}
	else
		{
		UINT	ipage;

		if ( NULL != ppgnoFirst )
			{
			ipage	= IpageFirstFree();
			}
		else if ( sFree != GetState( 0 ) )
			{
			//	this is not the first extent, so can't do anything because first
			//	page in this extent is not free
			Assert( 0 == *pcpgReq );
			return JET_errSuccess;
			}
		else
			{
			ipage = 0;
			}
	
#ifdef DEBUG
		INT ipageT;
		for ( ipageT = 0; sFree != GetState( ipageT ); ipageT++ )
			{
			//	something drastically wrong if we fall off the
			//	end of the map without finding any free pages
			Assert( ipageT < cpageMap );
			}
		Assert( ipageT == ipage );
#endif	//	DEBUG

		for ( ipageFirst = ipage;
			ipage < cpageMap && sFree == GetState( ipage ) && (*pcpgReq) < cpgReq;
			ipage++ )
			{
			(*pcpgReq)++;
			}

		Assert( ipageFirst < cpageMap );
		Assert( *pcpgReq > 0 );

		if ( NULL != ppgnoFirst )
			{
			*ppgnoFirst += ipageFirst;
			}
		else
			{
			Assert( 0 == ipageFirst );
			}
		}


	Assert( ipageFirst < cpageMap );
	Assert( *pcpgReq > 0 );
	Assert( *pcpgReq <= cpgReq );

	// That's no need to update SLV OwnerMap as long as
	// the pages are not entering or exiting the sCommit status
	// The operation here is slvspaceoperFreeToReserved or slvspaceoperFreeToCommitted,

	const ERR	err		= ErrBTMungeSLVSpace(
								pfucbSLVAvail,
								fReserveOnly ? slvspaceoperFreeToReserved : slvspaceoperFreeToCommitted,
								ipageFirst,
								*pcpgReq,
								fDIRNull );

								
	return err;
	}


//  ================================================================
VOID SLVSPACENODE::CheckFree( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	//  we should not be freeing an already freed page
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const STATE sReal = GetState_( ipageT );
		if( sFree == sReal )
			{
			const CHAR * rgsz[2];
			INT irgsz = 0;

			CHAR szStateExpected[16];
			CHAR szStateActual[16];

			sprintf( szStateExpected, "%d", sFree );
			sprintf( szStateActual, "%d", sReal );
			
			rgsz[irgsz++] = szStateExpected;
			rgsz[irgsz++] = szStateActual;
			
			UtilReportEvent(	eventError,
								DATABASE_CORRUPTION_CATEGORY,
								CORRUPT_SLV_SPACE_ID,
								irgsz,
								rgsz );
								
			EnforceSz( fFalse, "SLV Space Map corrupted" );
			}
		}
	}


//  ================================================================
VOID SLVSPACENODE::CheckFreeReserved( const LONG ipage, const LONG cpg ) const
//  ================================================================
//
//  When freeing from the reserved state some of the pages in the 
//  SLVSPACENODE may have been freed already by a rollback from the
//  committed state.
//
//-
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

	//  we should not be freeing a deleted page
	//  committed pages should have been rolled back already
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const STATE sReal = GetState_( ipageT );
		if( sDeleted == sReal
			|| sCommitted == sReal )
			{
			const CHAR * rgsz[2];
			INT irgsz = 0;

			CHAR szStateExpected[16];
			CHAR szStateActual[16];

			sprintf( szStateExpected, "%d", sFree );
			sprintf( szStateActual, "%d", sReal );
			
			rgsz[irgsz++] = szStateExpected;
			rgsz[irgsz++] = szStateActual;
			
			UtilReportEvent(	eventError,
								DATABASE_CORRUPTION_CATEGORY,
								CORRUPT_SLV_SPACE_ID,
								irgsz,
								rgsz );
								
			EnforceSz( fFalse, "SLV Space Map corrupted" );
			}
		}

	}


//  ================================================================
VOID SLVSPACENODE::CheckReserve( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	EnforcePageState( ipage, cpg, sFree );
	}


//  ================================================================
VOID SLVSPACENODE::CheckCommitFromFree( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	EnforcePageState( ipage, cpg, sFree );
	}


//  ================================================================
VOID SLVSPACENODE::CheckCommitFromReserved( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	EnforcePageState( ipage, cpg, sReserved );
	}


//  ================================================================
VOID SLVSPACENODE::CheckCommitFromDeleted( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	EnforcePageState( ipage, cpg, sDeleted );
	}


//  ================================================================
VOID SLVSPACENODE::CheckDeleteFromCommitted( const LONG ipage, const LONG cpg ) const
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
	EnforcePageState( ipage, cpg, sCommitted );
	}


//  ================================================================
VOID SLVSPACENODE::Free( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

#ifdef DEBUG	//	checked already by CheckFree
	//  we should not be freeing an already freed page
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const STATE sReal = GetState_( ipageT );
		if( sFree == sReal )
			{
			const CHAR * rgsz[2];
			INT irgsz = 0;

			CHAR szStateExpected[16];
			CHAR szStateActual[16];

			sprintf( szStateExpected, "%d", sFree );
			sprintf( szStateActual, "%d", sReal );
			
			rgsz[irgsz++] = szStateExpected;
			rgsz[irgsz++] = szStateActual;
			
			UtilReportEvent(	eventError,
								DATABASE_CORRUPTION_CATEGORY,
								CORRUPT_SLV_SPACE_ID,
								irgsz,
								rgsz );
								
			EnforceSz( fFalse, "SLV Space Map corrupted" );
			}
		}
#endif	//	DEBUG

	SetState_( ipage, cpg, sFree );
	ChangeCpgAvail_( cpg );	

#ifdef DEBUG
	AssertPageState( ipage, cpg, sFree );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::FreeReserved( const LONG ipage, const LONG cpg, LONG * const pcpgFreed )
//  ================================================================
//
//  When freeing from the reserved state some of the pages in the 
//  SLVSPACENODE may have been freed already by a rollback from the
//  committed state.
//
//-
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

	LONG cpgActual = 0;
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		if( sFree != GetState_( ipageT ) )
			{
			++cpgActual;
			}
		}

	SetState_( ipage, cpg, sFree );
	ChangeCpgAvail_( cpgActual );	
	*pcpgFreed = cpgActual;

#ifdef DEBUG
	AssertPageState( ipage, cpg, sFree );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::Reserve( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sFree );
#endif	//	DEBUG
	
	SetState_( ipage, cpg, sReserved );
	ChangeCpgAvail_( -cpg );	
	
#ifdef DEBUG
	AssertPageState( ipage, cpg, sReserved );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromFree( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sFree );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );
	ChangeCpgAvail_( -cpg );	

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromReserved( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sReserved );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromDeleted( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sDeleted );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::DeleteFromCommitted( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sCommitted );
#endif	//	DEBUG

	SetState_( ipage, cpg, sDeleted );
	ASSERT_VALID( this );
	}

//  ================================================================
SLVSPACENODE::STATE SLVSPACENODE::GetState_( const LONG ipage ) const
//  ================================================================
	{
	const INT ib 			= ipage / cpagesPerByte;
	const INT shf 			= ( ipage % cpagesPerByte ) * cbitsPerPage;
	const BYTE b			= m_rgbMap[ib];
	const STATE state		= (STATE)( ( b >> shf ) & 0x3 );
	return state;
	}

	
//  ================================================================
VOID SLVSPACENODE::SetState_( const LONG ipage, const LONG cpg, const STATE state )
//  ================================================================
	{
	Assert( state >= 0 );
	Assert( state <= 3 );

	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const INT ib 			= ipageT / cpagesPerByte;
		const INT shf 			= ( ipageT % cpagesPerByte ) * cbitsPerPage;
		Assert( 0 <= shf );
		Assert( shf <= 6 );
		const BYTE b			= m_rgbMap[ib];
		const BYTE bmask		= (BYTE)( ~( 0x3 << shf ) );
		const BYTE bset			= (BYTE)( state << shf );
		const BYTE bnew			= (BYTE)( ( b & bmask ) | bset );

		m_rgbMap[ib] 			= bnew;
		}
		
	}


//  ================================================================	
VOID SLVSPACENODE::ChangeCpgAvail_( const LONG cpgDiff )
//  ================================================================
	{
	Assert( abs( cpgDiff ) <= cpageMap );
	if ( cpgDiff > 0 )
		{
		Assert( m_cpgAvail + cpgDiff <= cpageMap );
		}
	else
		{
		Assert( m_cpgAvail >= cpgDiff );
		}
	m_cpgAvail = USHORT( m_cpgAvail + cpgDiff );
	}


ERR ErrSLVCreateOwnerMap( PIB *ppib, FUCB *pfucbDb )
	{
	Assert ( ppibNil != ppib );
	Assert ( pfucbNil != pfucbDb );
	
	ERR				err 					= JET_errSuccess;
	const IFMP		ifmp					= pfucbDb->ifmp;
	FMP::AssertVALIDIFMP( ifmp );
	
	PGNO			pgnoSLVOwnerMapFDP 		= pgnoNull;
	OBJID			objidSLVOwnerMap 		= objidNil;
	const BOOL		fTempDb					= ( dbidTemp == rgfmp[ifmp].Dbid() );

	Assert( rgfmp[ifmp].FCreatingDB() || fGlobalRepair );
	Assert( 0 == ppib->level || ( 1 == ppib->level && rgfmp[ifmp].FLogOn() ) || fGlobalRepair );
	Assert( pfucbDb->u.pfcb->FTypeDatabase() );
	
	CallR( ErrDIRCreateDirectory(
				pfucbDb,
				cpgSLVOwnerMapTree,
				&pgnoSLVOwnerMapFDP,
				&objidSLVOwnerMap,
				CPAGE::fPageSLVOwnerMap,
				fSPMultipleExtent|fSPUnversionedExtent ) );

	Assert( pgnoSLVOwnerMapFDP > pgnoSystemRoot );
	Assert( pgnoSLVOwnerMapFDP <= pgnoSysMax );
	
	if ( !fTempDb )
		{
		Assert( objidSLVOwnerMap > objidSystemRoot );
		Call( ErrCATAddDbSLVOwnerMap(
					ppib,
					ifmp,
					szSLVOwnerMap,
					pgnoSLVOwnerMapFDP,
					objidSLVOwnerMap ) );
		}
	else
		{
		Assert( pgnoTempDbSLVOwnerMap == pgnoSLVOwnerMapFDP );
		Assert( objidTempDbSLVOwnerMap == objidSLVOwnerMap );
		}

	Assert( pfcbNil == rgfmp[ifmp].PfcbSLVOwnerMap() );
	Call ( ErrSLVOwnerMapInit( ppib, ifmp, pgnoSLVOwnerMapFDP ) );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVOwnerMap() );

	Call( ErrSLVOwnerMapNewSize( ppib, ifmp, cpgSLVFileMin, fSLVOWNERMAPNewSLVInit ) );

HandleError:
	return err;
	}


BOOL SLVOWNERMAP::FValidData( const DATA& data )
	{
#ifdef DEBUG	
	data.AssertValid();
#endif

	const SLVOWNERMAP	* const pslvownermap	= (SLVOWNERMAP *)data.Pv();
	const USHORT		cbKey					= pslvownermap->CbKey();
	const BOOL			fPartialPageSupport		= pslvownermap->FPartialPageSupport();
	const ULONG			cbAdjusted				= data.Cb() + ( fPartialPageSupport ? 0 : sizeof(USHORT) );

	if ( cbAdjusted > sizeof(SLVOWNERMAP)
		|| cbAdjusted < cbHeader + cbPreallocatedKeySize )
		{
		return fFalse;
		}

	if ( cbKey >= cbPreallocatedKeySize )
		{
		if ( cbAdjusted != cbHeader + cbKey )
			{
			return fFalse;
			}
		}
	else
		{
		if ( cbAdjusted != cbHeader + cbPreallocatedKeySize )
			{
			return fFalse;
			}		
		}

	return fTrue;			
	}


// always insert a entry for the page m_page and null (objid, columnid, key)
ERR SLVOWNERMAPNODE::ErrNew(PIB *ppib)
	{
	FMP::AssertVALIDIFMP( m_ifmp );
	Assert ( ppibNil != ppib );
	
	ERR 		err 	= JET_errSuccess;
	FMP * 		pfmp 	= &rgfmp[m_ifmp];
	
	Assert ( pfmp->FInUse() );

	// if new allowed we must have the cursor
	Assert ( !FTryNext() ||pfucbNil != Pfucb() );
	
	// we try to open fucb if not opened
	// we will close in the destructor
	CallR ( ErrOpenOwnerMap( ppib ) );
	Assert ( pfucbNil != Pfucb() );

	KEY 	key;
	DATA 	data;
	PGNO 	pageReversed;

	key.prefix.SetPv( NULL );
	key.prefix.SetCb( 0 );

	KeyFromLong( reinterpret_cast<BYTE *>( &pageReversed ), m_page );
	
	key.suffix.SetPv( (void *)&pageReversed );
	key.suffix.SetCb( sizeof(m_page) );

	Nullify();	
	data.SetPv( PvNodeData() );
	data.SetCb( CbNodeData() );


	if ( FTryNext() )
		{
		// don;t try next time unless NextPage is called
		m_fTryNext = fFalse;
		
		Assert( Pcsr( Pfucb() )->FLatched() );

		BTUp( Pfucb() );

		//	we can't call append because OLDSLV may have shrunk the table
		
		Call( ErrBTInsert( Pfucb(), key, data, fDIRNoVersion ) );
		Pfucb()->locLogical = locOnCurBM;

		//  keep the latch so that we can continue with append
		
		Assert( Pcsr( Pfucb() )->FLatched() );
		Assert( latchWrite == Pcsr( Pfucb() )->Latch() );
		}
	else
		{
		FUCBSetLevelNavigate( Pfucb(), Pfucb()->ppib->level );

		Assert( !Pcsr( Pfucb() )->FLatched() );

		Call( ErrBTInsert( Pfucb(), key, data, fDIRNoVersion ) );
		Pfucb()->locLogical = locOnCurBM;

		//  keep the latch so that we can continue with append
		
		Assert( Pcsr( Pfucb() )->FLatched() );
		Assert( latchWrite == Pcsr( Pfucb() )->Latch() );
		}

HandleError:
	return err;	
	}

ERR SLVOWNERMAPNODE::ErrResetData(PIB *ppib)
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI();
	if ( JET_errSuccess == err )
		{
		err = ErrReset( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}

ERR SLVOWNERMAPNODE::ErrDelete(PIB *ppib)
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI();
	if ( JET_errSuccess == err )
		{
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
		
		err = ErrDIRDelete( Pfucb(), fDIRNull  );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	
	}

ERR SLVOWNERMAPNODE::ErrGetChecksum(
	PIB			* ppib,
	ULONG		* const pulChecksum,
	ULONG		* const pcbChecksummed )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	Assert( NULL != pulChecksum );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
			
		const SLVOWNERMAP	* const pslvownermap	= (SLVOWNERMAP *)Pfucb()->kdfCurr.data.Pv();
		if ( pslvownermap->FValidChecksum() )
			{
			*pulChecksum = pslvownermap->UlChecksum();
			if ( pslvownermap->FPartialPageSupport() )
				{
				*pcbChecksummed = pslvownermap->CbDataChecksummed();
				}
			else
				{
				*pcbChecksummed = SLVPAGE_SIZE;
				}
			}
		else
			{
			err = ErrERRCheck ( errSLVInvalidOwnerMapChecksum );
			}
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;	
	}


ERR SLVOWNERMAPNODE::ErrSetChecksum(
	PIB				* ppib,
	const ULONG		ulChecksum,
	const ULONG		cbChecksummed )
	{
	ERR 			err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );
	Assert( cbChecksummed > 0 );
	Assert( cbChecksummed <= SLVPAGE_SIZE );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrUpdateChecksum( Pfucb(), ulChecksum, cbChecksummed );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}


ERR SLVOWNERMAPNODE::ErrResetChecksum(
	PIB				* ppib )
	{
	ERR 			err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrInvalidateChecksum( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}


ERR SLVOWNERMAP::ErrSet( FUCB * const pfucb, const BOOL fForceOwner )
	{
	ERR				err			= JET_errSuccess;
	SLVOWNERMAP 	slvownermapT;
	
	if ( !FValidData( pfucb->kdfCurr.data ) )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	Assert( CbKey() != 0 );
	Assert( Objid() != objidNil );
	Assert( Columnid() != 0 );

	slvownermapT.Retrieve( pfucb->kdfCurr.data );

	if ( objidNil == slvownermapT.Objid() )
		{
		Assert( 0 == slvownermapT.Columnid() );
		Assert( 0 == slvownermapT.CbKey() );
		}
	else if ( Objid() != slvownermapT.Objid() )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	if ( slvownermapT.Columnid() != 0
		&& slvownermapT.Columnid() != Columnid() )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	if ( 0 != slvownermapT.CbKey()
		&& !fForceOwner )
		{
		if ( slvownermapT.CbKey() != CbKey()
			|| 0 != memcmp( slvownermapT.PvKey(), PvKey(), min( CbKey(), cbPreallocatedKeySize ) ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
		}

	if ( slvownermapT.Objid() == objidNil || fForceOwner )
		{
		Assert( slvownermapT.Columnid() == 0 || fForceOwner );
		Assert( slvownermapT.CbKey() == 0 || fForceOwner );

		// we need to copy the checksum from the existing node
		Assert( FPartialPageSupport() );
		SetUlChecksum( slvownermapT.UlChecksum() );
		SetCbDataChecksummed( slvownermapT.CbDataChecksummed() );
		if ( slvownermapT.FValidChecksum() )
			{
			SetFValidChecksum();
			}
		else
			{
			ResetFValidChecksum();
			}
		
		DATA data;
		data.SetPv( this );
		data.SetCb( cbHeader + max( CbKey(), cbPreallocatedKeySize ) );
		Call ( ErrDIRReplace( pfucb, data, fDIRReplace ) );
		}
	else
		{
		// if page is already marked as belonging to this page, no replace is needed
		Assert( slvownermapT.Columnid() == Columnid() );
		Assert( slvownermapT.CbKey() == CbKey() );
		Assert( 0 == memcmp( slvownermapT.PvKey(), PvKey(), min( CbKey(), cbPreallocatedKeySize ) ) );
		}

HandleError:
	return err;
	}

ERR SLVOWNERMAPNODE::ErrSetData( PIB *ppib, const BOOL fForceOwner )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR ( ErrOpenOwnerMap( ppib ) );
	Assert ( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrSet( Pfucb(), fForceOwner );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}

ERR SLVOWNERMAPNODE::ErrSearchAndCheckInRange( PIB *ppib )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrCheckInRange( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	
	}

ERR SLVOWNERMAPNODE::ErrSearchI( )
	{
	FMP::AssertVALIDIFMP( m_ifmp );
	Assert ( pfucbNil != Pfucb() );
	
	ERR 		err 	= JET_errSuccess;
	BOOKMARK 	bm;
	DIB			dib;
	PGNO 		pageReversed;

	KeyFromLong( reinterpret_cast<BYTE *>( &pageReversed ), m_page );

	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( reinterpret_cast<BYTE *>( &pageReversed ) );
	bm.key.suffix.SetCb( sizeof( PGNO ) );
	bm.data.Nullify();


	if ( FTryNext() )
		{
		PGNO currentLatchPage;
		
		// don;t try next time unless NextPage is called
		m_fTryNext = fFalse;
		
		err = ErrDIRNext( Pfucb(), fDIRNull );

		if ( JET_errSuccess != err )
			{
			if ( err > 0 || JET_errNoCurrentRecord == err )
				err = ErrERRCheck( JET_errRecordNotFound );
			Call( err );
			}

		Assert ( Pcsr( Pfucb() )->FLatched() );					
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		LongFromKey( &currentLatchPage, Pfucb()->kdfCurr.key );

		// check if we are where we want to be
		if ( currentLatchPage != m_page )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
			}

		return JET_errSuccess;
		}

	if ( Pcsr( Pfucb() )->FLatched() )
		{
		PGNO	pgnoCurr;

		Assert ( Pcsr( Pfucb() )->FLatched() );					
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		LongFromKey( &pgnoCurr, Pfucb()->kdfCurr.key );
		
		// check if we are where we want to be
		if ( pgnoCurr == m_page )
			{
			return JET_errSuccess;
			}

		ResetCursor();
		}

	Assert ( !Pcsr( Pfucb() )->FLatched() );

	dib.dirflag = fDIRExact;
	dib.pos = posDown;
	dib.pbm = &bm;	
	err = ErrDIRDown( Pfucb(), &dib );

	// we care only if we found or not the record for the page
	if ( JET_errSuccess == err )
		{
		Assert ( JET_errSuccess == err );
		Assert ( Pcsr( Pfucb() )->FLatched() );
		
		PGNO foundPage;
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		if ( sizeof(PGNO) != Pfucb()->kdfCurr.key.Cb() )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
			}
		else
			{
			LongFromKey( &foundPage, Pfucb()->kdfCurr.key );
			Assert ( foundPage == m_page );
			if ( foundPage != m_page )
				{
				AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );				
				Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
				}
			}
		}
	else
		{
#ifdef DEBUG		
		switch ( err )
			{
			case wrnNDFoundGreater:
			case wrnNDFoundLess:
			case JET_errDiskIO:
			case JET_errReadVerifyFailure:
			case JET_errRecordNotFound:
			case JET_errOutOfBuffers:
				break;
			default:
				Assert( JET_errNoCurrentRecord != err );
				CallS( err );		//	force assert to report unexpected error
			}
#endif			

		if ( err > 0 )
			err = ErrERRCheck( JET_errRecordNotFound );
		Call( err );
		}

HandleError:
	return err;
	}


ERR SLVOWNERMAPNODE::ErrSearch( PIB *ppib )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		// copy into class fields the data from found node		
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}

		Retrieve( Pfucb()->kdfCurr.data );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}
		
	return err;
	}


ERR SLVOWNERMAPNODE::ErrCreate(
	const IFMP		ifmp,
	const PGNO		pgno,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm ) 
	{ 
	FMP::AssertVALIDIFMP( ifmp ); 
	FMP * pfmp = &rgfmp[ifmp];
	
	Assert ( pfmp->FInUse() );
	Assert( pgnoNull < pgno );

	m_ifmp = ifmp; 
	m_page = pgno;
	m_fTryNext = fFalse;

	SetObjid( objid );
	SetColumnid( columnid );
	
	if ( NULL != pbm )
		{
		Assert( JET_cbPrimaryKeyMost >= pbm->key.Cb() );
		SetCbKey( BYTE( pbm->key.Cb() ) );
		pbm->key.CopyIntoBuffer( PvKey() );
		}
	else
		{
		SetCbKey( 0 );
		}
		
	return JET_errSuccess;
	}

ERR SLVOWNERMAPNODE::ErrCreateForSearch(const IFMP ifmp, const PGNO pgno)
	{ 
	return ErrCreate( ifmp, pgno, objidNil, 0 /*columnidNil */ , NULL); 
	} 

ERR SLVOWNERMAPNODE::ErrCreateForSet(
	const IFMP		ifmp,
	const PGNO		pgno,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm )
	{ 
	FMP::AssertVALIDIFMP( ifmp );
	Assert( rgfmp[ifmp].FInUse() );

	Assert( FidOfColumnid( columnid ) >= fidTaggedLeast );
	Assert( FidOfColumnid( columnid ) <= fidTaggedMost );

#ifdef DEBUG
	Assert( pbm );
	pbm->AssertValid();
	pbm->key.AssertValid();
#endif

	Assert( pbm->key.Cb() <= JET_cbPrimaryKeyMost );

	return ErrCreate( ifmp, pgno, objid, columnid, pbm );
	}


INLINE ERR ErrSLVIOwnerMapDeleteRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrDelete( ppib ) );
		slvownermapNode.NextPage();
		}
	
HandleError:	
	return err;	
	}

INLINE ERR ErrSLVIOwnerMapNewRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrNew( ppib ) );
		slvownermapNode.NextPage(); // allow DIRAppend
		}
	
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapSetUsageRange(
	PIB						* ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const OBJID				objid,
	const COLUMNID			columnid,
	BOOKMARK				* pbm,
	const SLVOWNERMAP_FLAGS	fFlags,
	const BOOL				fForceOwner )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;
		
	Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objid, columnid, pbm ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++ )
		{
		Call( slvownermapNode.ErrSetData( ppib, fForceOwner ) );
		slvownermapNode.NextPage();
		}
		
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapSetChecksum(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const ULONG				ulChecksum,
	const ULONG				cbChecksummed )
	{
	ERR 					err 			= JET_errSuccess;

// if we DON'T set frontdoor checksum and frontdoor is enabled
// return (ValidChecksum flag will remaing unset)
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		return JET_errSuccess;
#endif // SLV_USE_CHECKSUM_FRONTDOOR

	SLVOWNERMAPNODE			slvownermapNode;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, cbChecksummed ) );

HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapGetChecksum(
	PIB				* ppib,
	const IFMP		ifmp,
	const PGNO		pgno,
	ULONG			* const pulChecksum,
	ULONG			* const pcbChecksummed )
	{
	ERR 			err 			= JET_errSuccess;
	SLVOWNERMAPNODE	slvownermapNode;
	
	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrGetChecksum( ppib, pulChecksum, pcbChecksummed ) );
		
HandleError:	
	return err;	
	}


ERR ErrSLVOwnerMapCheckChecksum(
	PIB *ppib,
	const IFMP ifmp,
	const PGNO pgno,
	const CPG cpg,
	const void * pv)
	{
	ERR 					err 			= JET_errSuccess;
	CPG 					ipg 			= 0;

// if we DON'T test frontdoor checksum and 
// frontdoor is enabled, return checksum OK
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		return JET_errSuccess;
#endif // SLV_USE_CHECKSUM_FRONTDOOR

	for( ipg = 0; ipg < cpg; ipg++)
		{
		ULONG	ulChecksum;
		ULONG	cbChecksummed;
		
		err = ErrSLVOwnerMapGetChecksum( ppib, ifmp, pgno + ipg, &ulChecksum, &cbChecksummed );
		if ( errSLVInvalidOwnerMapChecksum != err )
			{
			Call ( err );				

			ULONG ulPageChecksum;

			Assert( 0 == SLVPAGE_SIZE % sizeof( DWORD ) );
			ulPageChecksum = UlChecksumSLV(
								(BYTE*)pv + SLVPAGE_SIZE * ipg,
								(BYTE*)pv + SLVPAGE_SIZE * ipg + cbChecksummed );

			Assert( ulPageChecksum == ulChecksum );
			
			// at this point we got the checksum from the OwnerMap tree and from page
			if ( ulPageChecksum != ulChecksum )
				{
				Call ( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
				}
			}
		}		

	err = JET_errSuccess;
HandleError:	
	return err;	
	}


ERR ErrSLVOwnerMapGet(
	PIB						* ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	SLVOWNERMAP				* const pslvownermapRetrieved )
	{
	ERR 					err;
	SLVOWNERMAPNODE			slvownermapNode;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrSearch( ppib ) );
	slvownermapNode.CopyInto( pslvownermapRetrieved );

HandleError:
	return err;
	}


ERR ErrSLVOwnerMapCheckUsageRange(
	PIB				* ppib,
	const IFMP		ifmp,
	const PGNO		pgno,
	const CPG		cpg,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm )
	{
	ERR 			err 			= JET_errSuccess;
	SLVOWNERMAPNODE	slvownermapNode;
	CPG 			ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objid, columnid, pbm ) );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrSearchAndCheckInRange( ppib ) );
		slvownermapNode.NextPage();
		}
		
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapResetUsageRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );	
	slvownermapNode.SetDebugFlags( fFlags );

	for( ipg = 0; ipg < cpg; ipg++)
		{		
		Call( slvownermapNode.ErrResetData( ppib ) );
		slvownermapNode.NextPage();
		}

HandleError:	
	return err;		
	}

ERR ErrSLVOwnerMapNewSize(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgnoSLVLast,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 		= JET_errSuccess;
	FMP						* pfmp 		= &rgfmp[ifmp];
	PGNO 					pgnoLast 	= pgnoNull;
	FUCB					* pfucb 	= pfucbNil;
	DIB						dib;
		
	Assert ( NULL != pfmp->PfcbSLVOwnerMap() );

	CallR ( ErrDIROpen( ppib, pfmp->PfcbSLVOwnerMap(), &pfucb ) );
	Assert ( pfucbNil != pfucb );

	dib.dirflag = fDIRNull;
	dib.pos   = posLast;
	err = ErrDIRDown( pfucb, &dib );

	if ( JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		pgnoLast = 0;
		}
	else if (JET_errSuccess <= err )
		{
		Assert( pfucb->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
		}
	Call ( err );

	Assert (pfucbNil != pfucb);
	DIRClose(pfucb);
	pfucb = pfucbNil;

	if (pgnoLast < pgnoSLVLast)
		{
		Call ( ErrSLVIOwnerMapNewRange(ppib, ifmp, pgnoLast + 1, pgnoSLVLast - pgnoLast, fFlags ) );		
		}
	else if (pgnoLast > pgnoSLVLast)
		{
		Call ( ErrSLVIOwnerMapDeleteRange( ppib, ifmp, pgnoSLVLast + 1, pgnoLast - pgnoSLVLast , fFlags) );
		}
	
HandleError:	
	if (pfucbNil != pfucb)
		{
		DIRClose(pfucb);
		}
	return err;	
	}

#define SLVVERIFIER_CODE

#ifdef SLVVERIFIER_CODE

SLVVERIFIER::SLVVERIFIER( const IFMP ifmp, ERR* const pErr )
	: m_rgfValidChecksum( NULL ),
	m_rgulChecksums( NULL ),
	m_rgcbChecksummed( NULL ),
	m_cChecksums( 0 ),
	m_pgnoFirstChecksum( pgnoNull ),
	m_slvownermapNode( fTrue )
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	,
	m_fCheckChecksum( fTrue )
#endif
	{
	FMP::AssertVALIDIFMP( ifmp );
	Assert( pErr );

#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		m_fCheckChecksum = fFalse;
#endif
	
	//	verifier always goes from first SLV page to last
	const PGNO pgnoFirstSLVPage = 1;
	Assert( pgnoNull != pgnoFirstSLVPage );
	*pErr = m_slvownermapNode.ErrCreateForSearch( ifmp, pgnoFirstSLVPage );
	m_ifmp = ifmp;
	}

SLVVERIFIER::~SLVVERIFIER()
	{
	(VOID) ErrDropChecksums();
	}

//	Call before verifying chunks of pages to batch up a bunch of
//	checksums from OwnerMap all at once. Gets checksums for range of
//	file from byte ib for count of cb bytes.

ERR SLVVERIFIER::ErrGrabChecksums( const QWORD ib, const DWORD cb, PIB* const ppib ) 
	{
#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	if ( ! m_fCheckChecksum )
		{
		return JET_errSuccess;
		}
#endif
	ERR	err = JET_errSuccess;
	CPG	ipg = 0;

	const DWORD cbNonData = cpgDBReserved * SLVPAGE_SIZE;
	const QWORD	qwpg = ib / QWORD( SLVPAGE_SIZE );
	const CPG	dwpg = CPG( qwpg );
	Assert( qwpg == dwpg );
	PGNO	pgnoFirst = dwpg - cpgDBReserved + 1;
	CPG		cpg = cb / SLVPAGE_SIZE;
	// if the block is at the beginning of the file, skip over the header & shadow
	// of the SLV file.
	if ( ib < cbNonData )
		{
		// For goodness sakes, let's keep this simple...
		Assert( 0 == ib );
		Assert( cb >= cbNonData );

		// header and shadow are not recognized as pages
		pgnoFirst = 1;
		cpg -= cpgDBReserved;
		}

	Assert( pgnoNull != pgnoFirst );
	Assert( cpg > 0 );
	Assert( ppib );
	
	m_cChecksums = cpg;
	m_pgnoFirstChecksum = pgnoFirst;
	m_rgulChecksums = new ULONG[ cpg ];
	if ( ! m_rgulChecksums )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	m_rgcbChecksummed = new ULONG[ cpg ];
	if ( ! m_rgcbChecksummed )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	m_rgfValidChecksum = new BOOL[ cpg ];
	if ( ! m_rgfValidChecksum )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	for ( ipg = 0; ipg < cpg; ++ipg )
		{
		Assert( ipg < m_cChecksums );
		m_slvownermapNode.AssertOnPage( pgnoFirst + ipg );
		err = m_slvownermapNode.ErrGetChecksum( ppib, &m_rgulChecksums[ ipg ], &m_rgcbChecksummed[ ipg ] );
		if ( errSLVInvalidOwnerMapChecksum == err )
			{
			m_rgfValidChecksum[ ipg ] = fFalse;
			}
		else
			{
			Call( err );
			m_rgfValidChecksum[ ipg ] = fTrue;
			}
		m_slvownermapNode.NextPage();
		}

	//	release any latch that may exist on OwnerMap
	//	we retain currency on this record in case this function
	//	gets called again for the next chunk
	Assert( cpg > 0 );
	Assert( m_slvownermapNode.FTryNext() );
	Call( m_slvownermapNode.ErrReleaseCursor() );

	return JET_errSuccess;

HandleError:
	(VOID) ErrDropChecksums();

	m_slvownermapNode.ResetCursor();

	return err;
	}

// call after batch of checksums is no longer needed

ERR SLVVERIFIER::ErrDropChecksums()
	{
	delete[] m_rgulChecksums;
	m_rgulChecksums = NULL;
	delete[] m_rgcbChecksummed;
	m_rgcbChecksummed = NULL;
	delete[] m_rgfValidChecksum;
	m_rgfValidChecksum = NULL;
	m_cChecksums = 0;
	m_pgnoFirstChecksum = pgnoNull;

	return JET_errSuccess;
	}

//	Verify checksums of a bunch of pages starting at offset ib in the SLV file.
//	Buffer described by { pb, cb }

ERR	SLVVERIFIER::ErrVerify( const BYTE* const pb, const DWORD cb, const QWORD ib ) const
	{
	ERR		err					= JET_errSuccess;
	QWORD	ibOffset;
	DWORD	cbLength;
	ULONG	ulChecksumExpected;
	ULONG	ulChecksumActual;

	Assert( pb );
	Assert( cb > 0 );

	//	Header and shadow		
	const DWORD cbNonData = cpgDBReserved * SLVPAGE_SIZE;
	const BYTE* pbData = pb;
	DWORD cbData = cb;
	const QWORD qwpg = ib / QWORD( SLVPAGE_SIZE );
	const CPG dwpg = CPG( qwpg );
	Assert( dwpg == qwpg );
	PGNO pgnoFirst = dwpg - cpgDBReserved + 1;
	// if the block is at the beginning of the file, check the header & shadow
	if ( ib < cbNonData )
		{
		// For goodness sakes, let's keep this simple...
		Assert( 0 == ib );
		Assert( cb >= cbNonData );

		//	Checksum the header and the shadow

		const DWORD	cbHeader = g_cbPage;
		Assert( g_cbPage == SLVPAGE_SIZE );

		Assert( cb >= cbHeader * 2 );

		ulChecksumExpected	= *( (LittleEndian<DWORD>*) pb );
		ulChecksumActual	= UlUtilChecksum( pb, cbHeader );
		if ( ulChecksumExpected != ulChecksumActual )
			{
			ibOffset	= 0;
			cbLength	= cbHeader;
			Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
			}

		ulChecksumExpected	= *( (LittleEndian<DWORD>*) ( pb + cbHeader ) );
		ulChecksumActual	= UlUtilChecksum( pb + cbHeader, cbHeader );
		if ( ulChecksumExpected != ulChecksumActual )
			{
			ibOffset	= cbHeader;
			cbLength	= cbHeader;
			Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
			}

		// jump over header
		pbData += cbNonData;
		cbData -= cbNonData;

		pgnoFirst = 1;
		}

	// This check is placed down here, so we can at least check the SLV header & shadow if frontdoor checksumming is off.
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( ! m_fCheckChecksum )
		{
		return JET_errSuccess;
		}
#endif

	Assert( pgnoNull != pgnoFirst );
	
	Assert( ( pbData >= pb ) && ( pbData < pb + cb ) );
	Assert( cbData > 0 );

		{
		Assert( 0 == ( cbData % SLVPAGE_SIZE ) );
		const CPG cpg = cbData / SLVPAGE_SIZE;
		Assert( cpg > 0 );
		Assert( m_rgulChecksums );
		Assert( m_rgcbChecksummed );
		Assert( m_rgfValidChecksum );
		Assert( m_cChecksums > 0 );
		Assert( pgnoNull != m_pgnoFirstChecksum );
		
		for ( PGNO pgno = pgnoFirst; pgno < pgnoFirst + cpg; ++pgno )
			{
			Assert( pgno >= m_pgnoFirstChecksum );
			const UINT ichecksum = pgno - m_pgnoFirstChecksum;
			Assert( ichecksum < m_cChecksums );
			
			if ( m_rgfValidChecksum[ ichecksum ] )
				{
				ulChecksumExpected	= m_rgulChecksums[ ichecksum ];
				ulChecksumActual	= UlChecksumSLV(	pbData + SLVPAGE_SIZE * ( pgno - pgnoFirst ),
														pbData + SLVPAGE_SIZE * ( pgno - pgnoFirst ) + m_rgcbChecksummed[ ichecksum ] );
				if ( ulChecksumExpected != ulChecksumActual )
					{
					ibOffset	= OffsetOfPgno( pgno );
					cbLength	= m_rgcbChecksummed[ ichecksum ];
					Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
					}
				}
			}
		}

HandleError:
	if ( err == JET_errSLVReadVerifyFailure )
		{
		const _TCHAR*	rgpsz[ 6 ];
		DWORD			irgpsz		= 0;
		_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
		_TCHAR			szOffset[ 64 ];
		_TCHAR			szLength[ 64 ];
		_TCHAR			szError[ 64 ];
		_TCHAR			szChecksumExpected[ 64 ];
		_TCHAR			szChecksumActual[ 64 ];

		CallS( rgfmp[ m_ifmp ].PfapiSLV()->ErrPath( szAbsPath ) );
		_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
		_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
		_stprintf( szError, _T( "%i (0x%08x)" ), err, err );
		_stprintf( szChecksumExpected, _T( "%u (0x%08x)" ), ulChecksumExpected, ulChecksumExpected );
		_stprintf( szChecksumActual, _T( "%u (0x%08x)" ), ulChecksumActual, ulChecksumActual );

		rgpsz[ irgpsz++ ]	= szAbsPath;
		rgpsz[ irgpsz++ ]	= szOffset;
		rgpsz[ irgpsz++ ]	= szLength;
		rgpsz[ irgpsz++ ]	= szError;
		rgpsz[ irgpsz++ ]	= szChecksumExpected;
		rgpsz[ irgpsz++ ]	= szChecksumActual;

		UtilReportEvent(	eventError,
							LOGGING_RECOVERY_CATEGORY,
							SLV_PAGE_CHECKSUM_MISMATCH_ID,
							irgpsz,
							rgpsz );
		}
	return err;
	}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\repair.cxx ===
#include "std.hxx"


#ifdef RTM
#else
//	UNDONE: consider turning these off when repair stabilizes
#define REPAIR_DEBUG_VERBOSE_SPACE
#define REPAIR_DEBUG_VERBOSE_STREAMING
#define REPAIR_DEBUG_CALLS
#endif	//	!RTM

#ifdef REPAIR_DEBUG_CALLS

CPRINTF * pcprintfRepairDebugCalls = NULL;

void ReportErr( const long err, const unsigned long ulLine, const char * const szFileName )
	{
	if( pcprintfRepairDebugCalls )
		{
		(*pcprintfRepairDebugCalls)( "error %d at line %d of %s\r\n", err, ulLine, szFileName );
		}
	}

#undef CallJ
#undef Call

#define CallJ( fn, label )						\
	{											\
	if ( ( err = fn ) < 0 )						\
		{										\
		ReportErr( err, __LINE__, __FILE__ );	\
		goto label;								\
		}										\
	}

#define Call( fn )		CallJ( fn, HandleError )

#endif	//	REPAIR_DEBUG_CALLS

#define KeyLengthMax	16  // the max key length we currently use

//  ****************************************************************
//  STRUCTS/CLASSES
//  ****************************************************************

//  ================================================================
struct REPAIRTT
//  ================================================================
	{
	JET_TABLEID		tableidBadPages;
	INT				crecordsBadPages;
	JET_COLUMNID	rgcolumnidBadPages[1];
	
	JET_TABLEID		tableidAvailable;
	INT				crecordsAvailable;
	JET_COLUMNID	rgcolumnidAvailable[2];

	JET_TABLEID		tableidOwned;
	INT				crecordsOwned;
	JET_COLUMNID	rgcolumnidOwned[2];

	JET_TABLEID		tableidUsed;
	INT				crecordsUsed;
	JET_COLUMNID	rgcolumnidUsed[3];
	};


//  ================================================================
struct REPAIRTABLE
//  ================================================================
	{
	OBJID		objidFDP;
	OBJID		objidLV;

	PGNO		pgnoFDP;
	PGNO		pgnoLV;

	BOOL		fHasPrimaryIndex;
	
	OBJIDLIST	objidlistIndexes;
	REPAIRTABLE	*prepairtableNext;
	CHAR		szTableName[JET_cbNameMost+1];

	FID			fidFixedLast;
	FID			fidVarLast;
	FID			fidTaggedLast;

	USHORT		ibEndOfFixedData;

	BOOL		fRepairTable;
	BOOL		fRepairLV;
	BOOL		fRepairIndexes;
	BOOL		fRepairLVRefcounts;
	BOOL		fTableHasSLV;
	BOOL		fTemplateTable;
	BOOL		fDerivedTable;
	};


//  ================================================================
struct INTEGGLOBALS
//  ================================================================
//
//  Values shared between the different threads for multi-threaded
//  integrity
//
//-
	{
	INTEGGLOBALS() : crit( CLockBasicInfo( CSyncBasicInfo( szIntegGlobals ), rankIntegGlobals, 0 ) ) {}
	~INTEGGLOBALS() {}
	
	CCriticalSection crit;

	BOOL				fCorruptionSeen;	//	did we encounter a corrupted table?
	ERR					err;				//	used for runtime failures (i.e. not -1206)
	
	REPAIRTABLE 		** pprepairtable;
	TTARRAY 			* pttarrayOwnedSpace;
	TTARRAY 			* pttarrayAvailSpace;
	TTARRAY 			* pttarraySLVAvail;	
	TTARRAY 			* pttarraySLVOwnerMapColumnid;	
	TTARRAY 			* pttarraySLVOwnerMapKey;	
	TTARRAY				* pttarraySLVChecksumLengths;
	const REPAIROPTS 	* popts;
	};


//  ================================================================
struct CHECKTABLE
//  ================================================================
//
//  Tells a given task thread which table to check
//
//-
	{
	IFMP 				ifmp;
	char 				szTable[JET_cbNameMost+1];
	char 				szIndex[JET_cbNameMost+1];
	
	OBJID 				objidFDP;
	PGNO 				pgnoFDP;
	OBJID 				objidParent;
	PGNO				pgnoFDPParent;
	ULONG 				fPageFlags;
	BOOL 				fUnique;
	RECCHECK *  		preccheck;
	
	CPG					cpgPrimaryExtent;	//  used to preread the table
	INTEGGLOBALS		*pglobals;

	BOOL				fDeleteWhenDone;	//	if set to true the structure will be 'delete'd
	CManualResetSignal	signal;				//	set when the table has been checked if fDelete is not set

	//  need a constructor to initialize the signal
	
	CHECKTABLE() : signal( CSyncBasicInfo( _T( "CHECKTABLE::signal" ) ) ) {}
	};


//  ================================================================
struct CHECKSUMSLVCHUNK
//  ================================================================
//
//  Used to read and checksum information from a streaming files
//
//-
	{
	
	//	this signal will be set when I/O completes

	BOOL				fIOIssued;
	CManualResetSignal 	signal;

	//	errors from I/O will be returned in this
	
	ERR err;

	//	data, starting at pgnoFirst will be read into the given buffer
	
	VOID 	* pvBuffer;
	ULONG	cbBuffer;
	PGNO	pgnoFirst;
	ULONG	cbRead;

	//	verify checksums with non-zero lengths against the expected checksums
	//	store the real checksums in the checksums array
	
	ULONG 	* pulChecksumsExpected;
	ULONG	* pulChecksumLengths;
	ULONG 	* pulChecksums;
	OBJID 	* pobjid;
	COLUMNID* pcolumnid;
	USHORT	* pcbKey;
	BYTE   ** pprgbKey;

	//  need a constructor to initialize the signal
	
	CHECKSUMSLVCHUNK()
		:	signal( CSyncBasicInfo( _T( "CHECKSUMSLVCHUNK::signal" ) ) )
		{
		signal.Set();
		}
	};
	

//  ================================================================
struct CHECKSUMSLV
//  ================================================================
//
//  Tells a given task thread which table to check
//
//-
	{
	IFMP				ifmp;
	IFileAPI		*pfapiSLV;
	const CHAR 			*szSLV;
	CPG					cpgSLV;
	ERR 				*perr;
	const REPAIROPTS 	*popts;
	
	CHECKSUMSLVCHUNK	*rgchecksumchunk;	//  array of CHECKSUMSLVCHUNK's
	INT					cchecksumchunk;		//  number of chunks
	INT					cbchecksumchunk;	//  number of bytes per read

	TTARRAY * pttarraySLVChecksumsFromFile;
	TTARRAY * pttarraySLVChecksumLengthsFromSpaceMap;
	
	};


//  ================================================================
class CSLVAvailIterator
//  ================================================================
//
//  Walk through the SLVAvailTree
//
//-
	{
	public:
		CSLVAvailIterator();
		~CSLVAvailIterator();

		ERR ErrInit( PIB * const ppib, const IFMP ifmp );
		ERR ErrTerm();
		ERR ErrMoveFirst();
		ERR ErrMoveNext();

		BOOL FCommitted() const;

	private:
		FUCB * 	m_pfucb;
		PGNO	m_pgnoCurr;
		INT		m_ipage;
		const SLVSPACENODE * m_pspacenode;
		
	private:
		CSLVAvailIterator( const CSLVAvailIterator& );
		CSLVAvailIterator& operator= ( const CSLVAvailIterator& );
	};


//  ================================================================
class CSLVOwnerMapIterator
//  ================================================================
	{
	public:
		CSLVOwnerMapIterator();
		~CSLVOwnerMapIterator();

		ERR ErrInit( PIB * const ppib, const IFMP ifmp );
		ERR ErrTerm();
		ERR ErrMoveFirst();
		ERR ErrMoveNext();

		BOOL FNull() const;
		OBJID Objid() const;
		COLUMNID Columnid() const;
		const VOID * PvKey() const;
		INT CbKey() const;
		ULONG UlChecksum() const;
		USHORT CbDataChecksummed() const;
		BOOL FChecksumIsValid() const;

	private:
		FUCB * m_pfucb;
		SLVOWNERMAP m_slvownermapnode;

	private:
		CSLVOwnerMapIterator( const CSLVOwnerMapIterator& );
		CSLVOwnerMapIterator& operator= ( const CSLVOwnerMapIterator& );
	};


//  ================================================================
class RECCHECKMACRO : public RECCHECK
//  ================================================================
//
//  Used to string together multiple RECCHECKs
//
//-
	{
	public:
		RECCHECKMACRO();
		~RECCHECKMACRO();

		ERR operator()( const KEYDATAFLAGS& kdf );

		VOID Add( RECCHECK * const preccheck );

	private:
		INT	m_creccheck;
		RECCHECK * m_rgpreccheck[16];
	};


//  ================================================================
class RECCHECKNULL : public RECCHECK
//  ================================================================
//
//  No-op
//
//-
	{
	public:
		RECCHECKNULL() {}
		~RECCHECKNULL() {}

		ERR operator()( const KEYDATAFLAGS& kdf ) { return JET_errSuccess; }
	};


//  ================================================================
class RECCHECKSLVOWNERMAP : public RECCHECK
//  ================================================================
//
//  For the SLVOwnerMap
//
//-
	{
	public:
		RECCHECKSLVOWNERMAP(
			const REPAIROPTS * const popts );
		~RECCHECKSLVOWNERMAP();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const REPAIROPTS * const m_popts;
	};


//  ================================================================
class RECCHECKSLVSPACE : public RECCHECK
//  ================================================================
//
//  Checks the SLVSpace bitmap
//
//-
	{
	public:
		RECCHECKSLVSPACE( const IFMP ifmp, const REPAIROPTS * const popts );
		~RECCHECKSLVSPACE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const REPAIROPTS * const 	m_popts;
		const IFMP					m_ifmp;
		LONG						m_cpagesSeen;
	};


//  ================================================================
class RECCHECKSPACE : public RECCHECK
//  ================================================================
//
//  Checks OE/AE trees
//
//-
	{
	public:
		RECCHECKSPACE( PIB * const ppib, const REPAIROPTS * const popts );
		~RECCHECKSPACE();

	public:
		ERR operator()( const KEYDATAFLAGS& kdf );
		CPG CpgSeen() const { return m_cpgSeen; }
		CPG CpgLast() const { return m_cpgLast; }
		PGNO PgnoLast() const { return m_pgnoLast; }

	protected:
		PIB		* const m_ppib;
		const REPAIROPTS * const m_popts;

	private:	
		PGNO 	m_pgnoLast;
		CPG		m_cpgLast;
		CPG		m_cpgSeen;
	};


//  ================================================================
class RECCHECKSPACEOE : public RECCHECKSPACE
//  ================================================================
//
//-
	{
	public:
		RECCHECKSPACEOE(
			PIB * const ppib,
			TTARRAY * const pttarrayOE,
			const OBJID objid,
			const OBJID objidParent,
			const REPAIROPTS * const popts );
		~RECCHECKSPACEOE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const OBJID		m_objid;
		const OBJID		m_objidParent;
	
		TTARRAY 	* const m_pttarrayOE;	
	};


//  ================================================================
class RECCHECKSPACEAE : public RECCHECKSPACE
//  ================================================================
//
//-
	{
	public:
		RECCHECKSPACEAE(
			PIB * const ppib,
			TTARRAY * const pttarrayOE,
			TTARRAY * const pttarrayAE,
			const OBJID objid,
			const OBJID objidParent,
			const REPAIROPTS * const popts );
		~RECCHECKSPACEAE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const OBJID		m_objid;
		const OBJID		m_objidParent;
		
		TTARRAY * const m_pttarrayOE;
		TTARRAY * const m_pttarrayAE;
	};


//  ================================================================
struct ENTRYINFO
//  ================================================================
	{
	ULONG			objidTable;
	SHORT			objType;
	ULONG			objidFDP;
	CHAR			szName[JET_cbNameMost + 1];
	ULONG			pgnoFDPORColType;
	ULONG 			dwFlags;
	CHAR			szTemplateTblORCallback[JET_cbNameMost + 1];
	ULONG			ibRecordOffset;		//  offset of record in fixed column
	BYTE			rgbIdxseg[JET_ccolKeyMost*sizeof(IDXSEG)];
	};

//  ================================================================
struct ENTRYTOCHECK
//  ================================================================
	{
	ULONG			objidTable;
	SHORT			objType;
	ULONG			objidFDP;
	};

//  ================================================================
struct INFOLIST
//  ================================================================
	{
	ENTRYINFO		info;
	INFOLIST	*	pInfoListNext;
	};

//  ================================================================
struct TEMPLATEINFOLIST
//  ================================================================
	{
	CHAR					szTemplateTableName[JET_cbNameMost + 1];
	INFOLIST			*	pColInfoList;
	TEMPLATEINFOLIST	*	pTemplateInfoListNext;
	};


	
//  ****************************************************************
//  PROTOTYPES
//  ****************************************************************

extern VOID NDIGetKeydataflags( const CPAGE& cpage, INT iline, KEYDATAFLAGS * pkdf );

LOCAL VOID REPAIRIPrereadIndexesOfFCB( const FCB * const pfcb );

LOCAL PGNO PgnoLast( const IFMP ifmp );

LOCAL VOID REPAIRDumpHex( CHAR * const szDest, const INT cchDest, const BYTE * const pb, const INT cb );

LOCAL VOID REPAIRDumpStats(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const BTSTATS * const pbtstats,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRPrintSig( const SIGNATURE * const psig, CPRINTF * const pcprintf );

//

LOCAL JET_ERR __stdcall ErrREPAIRNullStatusFN( JET_SESID, JET_SNP, JET_SNT, void * );

//  

LOCAL VOID REPAIRSetCacheSize( const REPAIROPTS * const popts );
LOCAL VOID REPAIRResetCacheSize( const REPAIROPTS * const popts );
LOCAL VOID REPAIRPrintStartMessages( const CHAR * const szDatabase, const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRGetStreamingFileSize(
	const CHAR * const szSLV,
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	CPG * const pcpgSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckStreamingFileHeader(
	INST *pinst,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRCheckSLVAvailTreeTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL VOID REPAIRCheckSLVOwnerMapTreeTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL ERR ErrREPAIRStartCheckSLVTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const CHAR * const szSLV,
	IFileAPI *const pfapiSLV,
	const ULONG cpgSLV,
	TASKMGR * const ptaskmgr,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
	INTEGGLOBALS * const pintegglobals,		
	ERR * const perrSLVChecksum,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRStopCheckSLVTrees( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt );
LOCAL BOOL FREPAIRRepairtableHasSLV( const REPAIRTABLE * const prepairtable );
LOCAL VOID REPAIRFreeRepairtables( REPAIRTABLE * const prepairtable );
LOCAL VOID REPAIRPrintEndMessages(
	const CHAR * const szDatabase,
	const ULONG timerStart,
	const REPAIROPTS * const popts );
LOCAL VOID INTEGRITYPrintEndErrorMessages(
	const LOGTIME logtimeLastFullBackup,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRAttachForIntegrity(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	IFMP * const pifmp,
	const REPAIROPTS * const popts );

// Routines to check the database header, global space tree and catalogs

LOCAL ERR ErrREPAIRCheckHeader(
	INST * const pinst,
	const char * const szDatabase,
	const char * const szStreamingFile,
	LOGTIME * const plogtimeLastFullBackup,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSystemTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
	
// check the logical consistency of catalogs
LOCAL ERR ErrREPAIRRetrieveCatalogColumns(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * pfucbCatalog, 
	ENTRYINFO * const pentryinfo,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertIntoTemplateInfoList(
	TEMPLATEINFOLIST ** ppTemplateInfoList, 
	const CHAR * szTemplateTable,
	INFOLIST * const pInfoList,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRUtilCleanInfoList( INFOLIST **ppInfo );
LOCAL VOID REPAIRUtilCleanTemplateInfoList( TEMPLATEINFOLIST **ppTemplateInfoList ); 
LOCAL ERR ErrREPAIRCheckOneIndexLogical(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const ENTRYINFO entryinfo, 
	const ULONG objidTable,
	const ULONG pgnoFDPTable,
	const ULONG objidLV,
	const ULONG pgnoFDPLV,
	const INFOLIST * pColInfoList,
	const TEMPLATEINFOLIST * pTemplateInfoList, 
	const BOOL fDerivedTable,
	const CHAR * pszTemplateTableName,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckOneTableLogical( 
	INFOLIST **ppInfo, 
	const ENTRYINFO entryinfo, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckCatalogEntryPgnoFDPs(
	PIB * const ppib,
	const IFMP ifmp, 
	PGNO  *prgpgno, 
	const ENTRYTOCHECK * const prgentryToCheck,
	INFOLIST **ppEntriesToDelete,
	const BOOL	fFix,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckFixCatalogLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL  fShadow,
	const BOOL	fFix,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSystemTablesLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRCheckSpaceTree(
	PIB * const ppib,
	const IFMP ifmp, 
	BOOL * const pfSpaceTreeCorrupt,
	PGNO * const ppgnoLastOwned,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ErrREPAIRCheckRange(
	PIB * const ppib,
	TTARRAY * const pttarray,
	const ULONG ulFirst,
	const ULONG ulLast,
	const ULONG ulValue,
	BOOL * const pfMismatch,
	const REPAIROPTS * const popts );

//  Checksumming the streaming files

LOCAL VOID REPAIRStreamingFileReadComplete(
		const ERR			err,
		IFileAPI *const	pfapi,
		const QWORD			ibOffset,
		const DWORD			cbData,
		const BYTE* const	pbData,
		const DWORD_PTR		dwCompletionKey );


LOCAL ERR ErrREPAIRAllocChecksumslvchunk(
	CHECKSUMSLVCHUNK * const pchecksumslvchunk,
	const INT cbBuffer,
	const INT cpages );
LOCAL ERR ErrREPAIRAllocChecksumslv( CHECKSUMSLV * const pchecksumslv );
LOCAL VOID REPAIRFreeChecksumslvchunk( 
	CHECKSUMSLVCHUNK * const pchecksumslvchunk, 
	const INT cpages );		
LOCAL VOID REPAIRFreeChecksumslv( CHECKSUMSLV * const pchecksumslv );

LOCAL ERR ErrREPAIRChecksumSLVChunk(
	PIB * const ppib,
	const CHECKSUMSLV * const pchecksumslv,
	const INT ichunk,
	PGNO * const ppgno,
	const PGNO pgnoMax );


LOCAL ERR ErrREPAIRSetSequentialMoveFirst(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRISetupChecksumchunk(
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv,
	BOOL * const pfDone,
	BOOL * const pfChunkHasChecksums,
	const INT ichunk );
LOCAL ERR ErrREPAIRCheckSLVChecksums(
	PIB * const ppib,
	FUCB * const pfucbSLVOwnerMap,
	CHECKSUMSLV * const pchecksumslv );

LOCAL VOID REPAIRSLVChecksumTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL ERR ErrREPAIRChecksumSLV(
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TASKMGR * const ptaskmgr,
	ERR * const perr,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
	const REPAIROPTS * const popts );

//  Routines to check the SLV avail and space-map trees	

LOCAL ERR ErrREPAIRCheckSLVAvailTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSLVOwnerMapTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRVerifySLVTrees(
	PIB * const ppib, 
	const IFMP ifmp,
	BOOL * const pfSLVSpaceTreesCorrupt,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY	* const pttarraySLVChecksumLengths,		
	const REPAIROPTS * const popts );

//	Routines to check the space allocation of a table

LOCAL ERR ErrREPAIRGetPgnoOEAE( 
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	PGNO * const ppgnoOE,
	PGNO * const ppgnoAE,
	PGNO * const ppgnoParent,	
	const BOOL fUnique,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertSEInfo(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertOERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertAERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );

//  Routines to check tables

LOCAL ERR ErrREPAIRStartCheckAllTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumLengths,
	INTEGGLOBALS * const pintegglobals,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRStopCheckTables( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt );
LOCAL ERR ErrREPAIRPostTableTask(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const CHAR * const szTable,
	REPAIRTABLE ** const pprepairtable,
	INTEGGLOBALS * const pintegglobals,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRCheckOneTableTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL VOID REPAIRCheckTreeAndSpaceTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL BOOL FREPAIRTableHasSLVColumn( const FCB * const pfcb );
LOCAL ERR ErrREPAIRCheckOneTable(
	PIB * const ppib,
	const IFMP ifmp,
	const char * const szTable,
	const OBJID objidTable,
	const PGNO pgnoFDP,
	const CPG cpgPrimaryExtent,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumLengths,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCompareLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	TTMAP& ttmapLVRefcountsFromTable,
	TTMAP& ttmapLVRefcountsFromLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSplitBuf( 
	PIB * const ppib,
	const PGNO pgnoLastBuffer, 
	const CPG cpgBuffer,	
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSPLITBUFFERInSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );	
LOCAL ERR ErrREPAIRCheckTreeAndSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const CPG cpgDatabase,
	BOOL * const pfSpaceTreeCorrupt,
	TTARRAY ** const ppttarrayOwnedSpace,
	TTARRAY ** const ppttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoRoot,
	const OBJID objidFDP,
	const ULONG fPageFlags,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheck(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const ULONG fFlagsFDP,
	CSR& csr,
	const BOOL fPrereadSibling,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BTSTATS *const  btstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckNode(
	const PGNO pgno,
	const INT iline,
	const BYTE * pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckRecord(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckInternalLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev );
LOCAL ERR ErrREPAIRICheckLeafLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev,
	BOOL * const pfEmpty );
LOCAL ERR ErrREPAIRCheckInternal(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckLeaf(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );

//  Preparing to repair

LOCAL ERR ErrREPAIRCreateTempTables(
	PIB * const ppib,
	const BOOL fRepairGlobalSpace,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );

//  Scanning all the pages in the database

LOCAL ERR ErrREPAIRScanDB(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	DBTIME * const pdbtimeLast,
	OBJID  * const pobjidLast,
	PGNO   * const ppgnoLastOESeen,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertPageIntoTables(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertBadPageIntoTables(
	PIB * const ppib,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );

//	Attach the database to repair it, changing the header

LOCAL ERR ErrREPAIRAttachForRepair(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	IFMP * const pifmp,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeDBSignature(
	INST *pinst,
	const char * const szDatabase,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	SIGNATURE * const psignDb,
	SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeSLVSignature(
	INST *pinst,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const SIGNATURE * const psignDb,
	const SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeSignature(
	INST *pinst,
	const char * const szDatabase,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts );

//  Repair the global space tree

LOCAL ERR ErrREPAIRRepairGlobalSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );

//  Fix the catalogs, copying from the shadow if necessary

LOCAL ERR ErrREPAIRBuildCatalogEntryToDeleteList( 
	INFOLIST **ppDeleteList, 
	const ENTRYINFO entryinfo );
LOCAL ERR ErrREPAIRDeleteCorruptedEntriesFromCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const INFOLIST *pTablesToDelete,
	const INFOLIST *pEntriesToDelete,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertMSOEntriesToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL fCatalogCorrupt,
	const BOOL fShadowCatalogCorrupt, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRScanDBAndRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertCatalogRecordIntoTempTable(
	PIB * const ppib,
	const IFMP ifmp,
	const KEYDATAFLAGS& kdf,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCopyTempTableToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts );

//	Repair ordinary tables

LOCAL ERR ErrREPAIRRepairDatabase(
	PIB * const ppib,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	IFMP * const pifmp,
	const OBJID objidLast,
	const PGNO pgnoLastOE,
	REPAIRTABLE * const prepairtable,
	const BOOL fRepairedCatalog,
	BOOL fRepairGlobalSpace,
	const BOOL fRepairSLVSpace,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY * const pttarraySLVChecksumLengths,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,		
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRDeleteSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );	
LOCAL ERR ErrREPAIRCreateSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY * const pttarraySLVChecksumLengths,
	TTARRAY * const pttarraySLVOwnerMapColumnid,
	TTARRAY * const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,	
	const OBJIDLIST * const pobjidlist,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRepairTable(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildBT(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	FUCB * const pfucbTable,
	PGNO * const ppgnoFDP,
	const ULONG fPageFlags,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCreateEmptyFDP(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoParent,
	PGNO * const pgnoFDPNew,
	const ULONG fPageFlags,
	const BOOL fUnique,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildSpace(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoParent,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertRunIntoSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildInternalBT(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRFixLVs(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapLVTree,
	const BOOL fFixMissingLVROOT,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSLVInfoForUpdateTrees( 
	PIB * const ppib,
	CSLVInfo * const pslvinfo,
	const OBJID objidFDP,
	TTARRAY * const pttarraySLVAvail,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVAvailFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const CSLVInfo::RUN& slvRun,
	FUCB * const pfucbSLVAvail,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVOwnerMapFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const COLUMNID columnid,
	const BOOKMARK& bm,
	const CSLVInfo::RUN& slvRun,
	SLVOWNERMAPNODE * const pslvownermapNode,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVTablesFromSLVInfo( 
	PIB * const ppib,
	const IFMP ifmp,
	CSLVInfo * const pslvinfo,
	const BOOKMARK& bm,
	const OBJID objidFDP,
	const COLUMNID columnid,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,	
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	const TTMAP * const pttmapLVTree,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	TTMAP * const pttmapRecords,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const INT itagSequence,
	const TAGFLD * const ptagfld,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRAddOneCatalogRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const ULONG	objidTable,
	const COLUMNID	fidColumnLastInRec,
	const USHORT ibRecordOffset, 
	const ULONG	cbMaxLen,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertDummyRecordsToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixRecords(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapRecords,
	TTMAP * const pttmapLVTree,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixupTable(
	PIB * const ppib,
	const IFMP ifmp,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRBuildAllIndexes(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB ** const ppfucb,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );


//  ****************************************************************
//  GLOBALS
//  ****************************************************************

//  These are typically very large tables. Start integrity checking
//  them first for maximum overlap
//
//  THE TABLENAMES MUST BE IN ASCENDING ALPHABETICAL ORDER
//  if they are not FIsLargeTable will not work properly!
//

const CHAR * rgszLargeTables[] = {
	"1-24",
	"1-2F",
	"1-A",
	"1-D",
	"Folders",
	"Msg"
};
const INT cszLargeTables = sizeof( rgszLargeTables ) / sizeof( rgszLargeTables[0] );

//  To take advantage of sequential NT I/O round up sequential prereads to this many pages (64K)

const INT g_cpgMinRepairSequentialPreread = ( 64 * 1024 ) / g_cbPage;

//  When examing the SLV space maps we don't store the entire key. This determines
//  how many bytes to store (rounded up to the nearest ULONG
//  the first BYTE stores the length of the key

static const cbSLVKeyToStore = 15;
static const culSLVKeyToStore = cbSLVKeyToStore + sizeof( ULONG ) / sizeof( ULONG );

//  Checksumming the SLV files
//  These must come out to a multiple of the SLV file size

static const cbSLVChecksumChunk = 64 * 1024;
static const cSLVChecksumChunk	= 32;

//	Any SLV pages owned by this objid have invalid checksums

const OBJID objidInvalid = 0xfffffffe;


//  ================================================================
ERR ErrDBUTLRepair( JET_SESID sesid, const JET_DBUTIL *pdbutil, CPRINTF* const pcprintf )
//  ================================================================
	{
	Assert( NULL != pdbutil->szDatabase );
	
	ERR 		err 	= JET_errSuccess;
	PIB * const ppib 	= reinterpret_cast<PIB *>( sesid );
	INST * const pinst	= PinstFromPpib( ppib );

	const CHAR * const szDatabase		= pdbutil->szDatabase;
	const CHAR * const szSLV			= pdbutil->szSLV;

	_TCHAR szFolder[ IFileSystemAPI::cchPathMax ];
	_TCHAR szPrefix[ IFileSystemAPI::cchPathMax ];
	_TCHAR szFileExt[ IFileSystemAPI::cchPathMax ];
	_TCHAR szFile[ IFileSystemAPI::cchPathMax ];

	const	INT cThreads 			= ( CUtilProcessProcessor() * 4 );
		
	CPRINTFFN	cprintfStdout( printf );
	
	if ( pdbutil->szIntegPrefix )
		{
		_tcscpy( szPrefix, pdbutil->szIntegPrefix );
		}
	else
		{
		CallR( pinst->m_pfsapi->ErrPathParse(	pdbutil->szDatabase,
												szFolder,
												szPrefix,
												szFileExt ) );
		}
	_tcscpy( szFile, szPrefix );
	_tcscat( szFile, ".INTEG.RAW" );
	CPRINTFFILE cprintfFile( szFile );
	
	_tcscpy( szFile, szPrefix );
	_tcscat( szFile, ".INTGINFO.TXT" );
	CPRINTF * const pcprintfStatsInternal = ( pdbutil->grbitOptions & JET_bitDBUtilOptionStats ) ?
									new CPRINTFFILE( szFile ) :
									CPRINTFNULL::PcprintfInstance();
	if( NULL == pcprintfStatsInternal )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	CPRINTFTLSPREFIX cprintf( pcprintf );
	CPRINTFTLSPREFIX cprintfVerbose( &cprintfFile );
	CPRINTFTLSPREFIX cprintfDebug( &cprintfFile );
	CPRINTFTLSPREFIX cprintfError( &cprintfFile, "ERROR: " );
	CPRINTFTLSPREFIX cprintfWarning( &cprintfFile, "WARNING: " );
	CPRINTFTLSPREFIX cprintfStats( pcprintfStatsInternal );

#ifdef REPAIR_DEBUG_CALLS
	pcprintfRepairDebugCalls = &cprintfError;
#endif	//	REPAIR_DEBUG_CALLS

	JET_SNPROG	snprog;
	memset( &snprog, 0, sizeof( JET_SNPROG ) );
	snprog.cbStruct = sizeof( JET_SNPROG );

	REPAIROPTS	repairopts;	
	repairopts.grbit			= pdbutil->grbitOptions;
	repairopts.pcprintf			= &cprintf;
	repairopts.pcprintfVerbose 	= &cprintfVerbose;
	repairopts.pcprintfError 	= &cprintfError;
	repairopts.pcprintfWarning	= &cprintfWarning;
	repairopts.pcprintfDebug	= &cprintfDebug;
	repairopts.pcprintfStats	= &cprintfStats;
	repairopts.pfnStatus		= pdbutil->pfnCallback ? (JET_PFNSTATUS)(pdbutil->pfnCallback) : ErrREPAIRNullStatusFN;
	repairopts.psnprog			= &snprog;

	const REPAIROPTS * const popts = &repairopts;

	//	startup messages
	
	REPAIRPrintStartMessages( szDatabase, popts );

	//	first, set the cache size. the BF clean thread will see the change
	//	and grow the cache
	
	REPAIRSetCacheSize( popts );

	//
	
	ERR				errSLVChecksum				= JET_errSuccess;
	
	BOOL			fGlobalSpaceCorrupt			= fFalse;
	BOOL 			fCatalogCorrupt				= fFalse;
	BOOL 			fShadowCatalogCorrupt		= fFalse;
	BOOL			fSLVSpaceTreesCorrupt		= fFalse;
	BOOL			fTablesCorrupt				= fFalse;
	BOOL			fStreamingFileCorrupt		= fFalse;
	BOOL			fRepairedCatalog			= fFalse;

	TTARRAY * pttarrayOwnedSpace 			= NULL;
	TTARRAY * pttarrayAvailSpace 			= NULL;
	TTARRAY * pttarraySLVAvail 				= NULL;
	TTARRAY * pttarraySLVOwnerMapColumnid	= NULL;
	TTARRAY * pttarraySLVOwnerMapKey		= NULL;
	TTARRAY * pttarraySLVChecksumLengths	= NULL;

	TTARRAY * pttarraySLVChecksumsFromFile				= NULL;
	TTARRAY * pttarraySLVChecksumLengthsFromSpaceMap	= NULL;
	
	REPAIRTABLE * 	prepairtable 				= NULL;
	IFileAPI	*pfapiSLV 					= NULL;
	IFMP			ifmp						= 0xffffffff;	
	CPG 			cpgDatabase					= 0;
	CPG 			cpgSLV						= 0;
	OBJID 			objidLast					= objidNil;
	PGNO			pgnoLastOE					= pgnoNull;
	TASKMGR			taskmgr;
	
	INTEGGLOBALS	integglobalsTables;
	INTEGGLOBALS	integglobalsSLVSpaceTrees;

	LOGTIME			logtimeLastFullBackup;
	memset( &logtimeLastFullBackup, 0, sizeof( LOGTIME ) );

	
	const ULONG timerStart = TickOSTimeCurrent();

	BOOL			fGlobalRepairSave			= fGlobalRepair;

	//  unless fDontRepair is set this will set the consistency bit so we can attach
	
	Call( ErrREPAIRCheckHeader(
			pinst,
			szDatabase,
			szSLV,
			&logtimeLastFullBackup,
			popts ) );

	//  set the magic switch!
	
	fGlobalRepair = fTrue;

	//	make sure the SLV matches the database
	
	if( szSLV )
		{		
		Call( ErrREPAIRCheckStreamingFileHeader(
				pinst,
				szDatabase,
				szSLV,
				ifmp,
				popts ) );				
		}

	//  attach to the database
	
	Call( ErrREPAIRAttachForIntegrity( sesid, szDatabase, &ifmp, popts ) );
	cpgDatabase = PgnoLast( ifmp );

	//  preread the first 2048 pages (8/16 megs)
	
	BFPrereadPageRange( ifmp, pgnoSystemRoot, min( 2048, cpgDatabase ) );
	
	//	open the SLV file and get its size

	if( szSLV )
		{		
		Call( pinst->m_pfsapi->ErrFileOpen( szSLV, &pfapiSLV, fTrue ) );
		Call( ErrREPAIRGetStreamingFileSize(
				szSLV,
				ifmp,
				pfapiSLV,
				&cpgSLV,
				popts ) );
		Assert( rgfmp[ifmp].CbSLVFileSize() == ( (QWORD)cpgSLV << (QWORD)g_shfCbPage ) );		

		//	we can't have a file open read-only and read-write at the same time
		//	close the file in case we need to open it to update the header
		//	during a catalog repair
		
		delete pfapiSLV;
		pfapiSLV = NULL;
		}
		
	//	init the TTARRAY's
	
	pttarrayOwnedSpace 				= new TTARRAY( cpgDatabase + 1, objidSystemRoot );
	pttarrayAvailSpace 				= new TTARRAY( cpgDatabase + 1, objidNil );
	pttarraySLVAvail 				= new TTARRAY( cpgSLV + 1, objidNil );
	pttarraySLVOwnerMapColumnid 	= new TTARRAY( cpgSLV + 1, 0 );
	pttarraySLVOwnerMapKey 			= new TTARRAY( ( cpgSLV + 1 ) * culSLVKeyToStore, 0 );
	pttarraySLVChecksumLengths 		= new TTARRAY( cpgSLV + 1, 0xFFFFFFFF );
	pttarraySLVChecksumsFromFile			= new TTARRAY( cpgSLV + 1, 0xFFFFFFFF );
	pttarraySLVChecksumLengthsFromSpaceMap 	= new TTARRAY( cpgSLV + 1, 0x0 );	
	if( NULL == pttarrayOwnedSpace
		|| NULL == pttarrayAvailSpace
		|| NULL == pttarraySLVAvail
		|| NULL == pttarraySLVOwnerMapColumnid
		|| NULL == pttarraySLVChecksumLengths
		|| NULL == pttarraySLVChecksumsFromFile
		|| NULL == pttarraySLVChecksumLengthsFromSpaceMap
		)
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
		
	Call( pttarrayOwnedSpace->ErrInit( pinst ) );
	Call( pttarrayAvailSpace->ErrInit( pinst ) );
	Call( pttarraySLVAvail->ErrInit( pinst ) );
	Call( pttarraySLVOwnerMapColumnid->ErrInit( pinst ) );
	Call( pttarraySLVOwnerMapKey->ErrInit( pinst ) );
	Call( pttarraySLVChecksumLengths->ErrInit( pinst ) );
	Call( pttarraySLVChecksumsFromFile->ErrInit( pinst ) );
	Call( pttarraySLVChecksumLengthsFromSpaceMap->ErrInit( pinst ) );

	//	init the TASKMGR

	(*popts->pcprintfVerbose)( "Creating %d threads\r\n", cThreads );
	Call( taskmgr.ErrInit( pinst, cThreads ) );

	//	init the status bar

	snprog.cunitTotal 	= cpgDatabase;
	snprog.cunitDone 	= 0;
	(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );	

	//	Set the cache size to 0 to allow DBA to do its thing

	REPAIRResetCacheSize( popts );

	//	check the global space trees
	
	Call( ErrREPAIRCheckSpaceTree(
			ppib,
			ifmp,
			&fGlobalSpaceCorrupt,
			&pgnoLastOE,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );

	//	check the catalog and shadow catalog
	
	Call( ErrREPAIRCheckSystemTables(
			ppib,
			ifmp,
			&taskmgr,
			&fCatalogCorrupt,
			&fShadowCatalogCorrupt,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );

	// IF 	at least one of catalogs is physically consistent
	// THEN Check the logical consistency of catalogs
	if ( !fCatalogCorrupt || !fShadowCatalogCorrupt )
		{
		Call( ErrREPAIRCheckSystemTablesLogical( 
			ppib, 
			ifmp, 
			&objidLast,
			&fCatalogCorrupt, 
			&fShadowCatalogCorrupt,
			popts ) );
		}


	//  if needed we have to repair the catalog right now so that we can access the other tables
			
	if( fCatalogCorrupt || fShadowCatalogCorrupt )
		{
		if ( popts->grbit & JET_bitDBUtilOptionDontRepair )
			{
			(*popts->pcprintfVerbose)( "The catalog is corrupted. not all tables could be checked\r\n"  );
			(VOID)INTEGRITYPrintEndErrorMessages( logtimeLastFullBackup, popts );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );

			}
		else
			{
					
			//  We don't know the dbtimeLast yet. We'll find it when we scan the database
			//  and attach for repair again. we set the dbtimeLast to 1. This will be
			//  fine as we will set the dbtime in the header later and when these pages are updated
			//  later they will get a good dbtime
			
			Call( ErrREPAIRAttachForRepair( sesid, szDatabase, szSLV, &ifmp, 1, objidNil, popts ) );
			if( fGlobalSpaceCorrupt 
				|| cpgDatabase > pgnoLastOE )
				{
				Call( ErrREPAIRRepairGlobalSpace( ppib, ifmp, popts ) );
				fGlobalSpaceCorrupt 	= fFalse;
				}

			(*popts->pcprintfVerbose)( "rebuilding catalogs\r\n"  );
			(*popts->pcprintfVerbose).Indent();
			Call( ErrREPAIRRepairCatalogs( 
						ppib, 
						ifmp, 
						&objidLast, 
						fCatalogCorrupt, 
						fShadowCatalogCorrupt, 
						popts ) );
			(*popts->pcprintfVerbose).Unindent();

			//  Flush the entire database so that if we crash here we don't have to repair the catalogs again
			
			(VOID)ErrBFFlush( ifmp );

			//Space tree may change after repairing catalog
			//Re-build pttarrayOwnedSpace and pttarrayAvailSpace
			Call( ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
						ppib,
						ifmp,
						cpgDatabase,
						&fGlobalSpaceCorrupt,
						&pttarrayOwnedSpace,
						&pttarrayAvailSpace,
						popts ) );

			fCatalogCorrupt 		= fFalse;
			fShadowCatalogCorrupt 	= fFalse;
			fRepairedCatalog		= fTrue;

			// done repairing catalogs
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntComplete, NULL );

			// continue integrity check 
			(*popts->pcprintf)( "\r\nChecking the database.\r\n"  );
			popts->psnprog->cunitTotal 	= cpgDatabase;
			popts->psnprog->cunitDone	= 0;
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );
			}
		}

	//	start checking the SLV space trees

	if( szSLV )
		{		
		Call( pinst->m_pfsapi->ErrFileOpen( szSLV, &pfapiSLV, fTrue ) );		
		Call( ErrREPAIRStartCheckSLVTrees(
				ppib,
				ifmp,
				szSLV,
				pfapiSLV,
				cpgSLV,
				&taskmgr,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				pttarraySLVChecksumsFromFile,
				pttarraySLVChecksumLengthsFromSpaceMap,
				&integglobalsSLVSpaceTrees,
				&errSLVChecksum,
				popts ) );
		}

	//  check all the normal tables in the system 
	//  if this returns JET_errDatabaseCorrupted it means there is a corruption in the catalog
	
	Call( ErrREPAIRStartCheckAllTables(
			ppib,
			ifmp,
			&taskmgr,
			&prepairtable, 
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pttarraySLVAvail,
			pttarraySLVOwnerMapColumnid,
			pttarraySLVOwnerMapKey,
			pttarraySLVChecksumLengths,
			&integglobalsTables,
			popts ) );

	//  terminate the taskmgr. once all the threads have stopped all the checks will
	//  have been done and we can collect the results
	
	Call( taskmgr.ErrTerm() );

	//	are the SLV space trees corrupt?

	if( szSLV )
		{		
		Call( ErrREPAIRStopCheckSLVTrees(
					&integglobalsSLVSpaceTrees,
					&fSLVSpaceTreesCorrupt ) );
		delete pfapiSLV;
		pfapiSLV = NULL;					
		}

	//	was the streaming file corrupt
	if( !fSLVSpaceTreesCorrupt )
		{
		if( JET_errSLVReadVerifyFailure == errSLVChecksum )
			{
			fStreamingFileCorrupt = fTrue;
			}
		else
			{
			Call( errSLVChecksum );
			}
		}
	
	//	were any tables corrupt?
	
	Call( ErrREPAIRStopCheckTables(
				&integglobalsTables,
				&fTablesCorrupt ) );
	if( objidNil == objidLast )
		{
		(*popts->pcprintfWarning)( "objidLast is objidNil (%d)\r\n", objidNil );
		objidLast = objidNil + 1;
		}

	//	if we don't think the global space tree is corrupt, check to see
	//	if any of the pages beyond the end of the global OwnExt are actually
	//	being used by other tables

	if( !fGlobalSpaceCorrupt )
		{
		const PGNO pgnoLastPhysical 	= cpgDatabase;
		const PGNO pgnoLastLogical	 	= pgnoLastOE;
		
		if( pgnoLastPhysical < pgnoLastLogical )
			{
			(*popts->pcprintfError)( "Database file is too small (expected %d pages, file is %d pages)\r\n",
										pgnoLastLogical, pgnoLastPhysical );
			fGlobalSpaceCorrupt = fTrue;
			}
		else if( pgnoLastPhysical > pgnoLastLogical )
			{
			
			//	make sure that every page from the logical last to the 
			//	physical last is owned by the database

			(*popts->pcprintfWarning)( "Database file is too big (expected %d pages, file is %d pages)\r\n",
										pgnoLastLogical, pgnoLastPhysical );
			
			Call( ErrREPAIRCheckRange(
					ppib,
					pttarrayOwnedSpace,
					pgnoLastLogical,
					pgnoLastPhysical,
					objidSystemRoot,
					&fGlobalSpaceCorrupt,
					popts ) );

			if( fGlobalSpaceCorrupt )
				{
				(*popts->pcprintfError)( "Global space tree is too small (has %d pages, file is %d pages). "
										 "The space tree will be rebuilt\r\n",
										pgnoLastLogical, pgnoLastPhysical );
				}
					
			}
		}
		
	//  see if any of the corrupted tables had SLV columns
	//  if so, we will need to rebuild the SLV space maps
	
	if ( fTablesCorrupt
		&& !fSLVSpaceTreesCorrupt
		&& szSLV )
		{		
		fSLVSpaceTreesCorrupt = FREPAIRRepairtableHasSLV( prepairtable );
		}

	//  Now all the tables have been checked we can verify the SLV space structures
	
	if ( !fSLVSpaceTreesCorrupt
		&& szSLV )
		{
		err = ErrREPAIRVerifySLVTrees(
					ppib,
					ifmp,
					&fSLVSpaceTreesCorrupt,
					pttarraySLVAvail,
					pttarraySLVOwnerMapColumnid,
					pttarraySLVOwnerMapKey,
					pttarraySLVChecksumLengths,
					popts );	
		if( JET_errDatabaseCorrupted == err )
			{
			err = JET_errSuccess;
			}
		Call( err );
		}

	//  finished with the integrity checking phase
	
	(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntComplete, NULL );	
	(*popts->pcprintfVerbose).Unindent();
				
	//  repair the database, or exit if we are just doing an integrity check 

	Assert( !fCatalogCorrupt );
	Assert( !fShadowCatalogCorrupt );

	//	bugfix (X5:121062, X5:121266): need to scan the database if we repaired the catalog and
	//	no tables are corrupt. the dbtime, objid must be set and bad pages must be zeroed out
	
	if( fTablesCorrupt || 
		fGlobalSpaceCorrupt || 
		fSLVSpaceTreesCorrupt || 
		fRepairedCatalog || 
		fStreamingFileCorrupt )
		{				
		if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
			{
			Call( ErrREPAIRRepairDatabase( 
					ppib,
					szDatabase,
					szSLV,
					cpgSLV,
					&ifmp,
					objidLast,
					pgnoLastOE,
					prepairtable,
					fRepairedCatalog,
					fGlobalSpaceCorrupt,
					fSLVSpaceTreesCorrupt | fStreamingFileCorrupt,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					pttarraySLVAvail,
					pttarraySLVChecksumLengths,
					pttarraySLVOwnerMapColumnid,
					pttarraySLVOwnerMapKey,
					pttarraySLVChecksumsFromFile,
					pttarraySLVChecksumLengthsFromSpaceMap,
					popts ) );
			(*popts->pcprintfVerbose)( "\r\nRepair completed. Database corruption has been repaired!\r\n\n" );
			(*popts->pcprintf)( "\r\nRepair completed. Database corruption has been repaired!\r\n\n" );
			err = ErrERRCheck( JET_wrnDatabaseRepaired );
			}
		else
			{
			(*popts->pcprintfVerbose)( "\r\nIntegrity check completed. Database is CORRUPTED!\r\n" );
			(VOID)INTEGRITYPrintEndErrorMessages( logtimeLastFullBackup, popts );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		}
	else
		{
		(*popts->pcprintfVerbose)( "\r\nIntegrity check successful.\r\n\r\n" );
		(*popts->pcprintf)( "\r\nIntegrity check successful.\r\n\r\n" );
		}

	(VOID)ErrIsamCloseDatabase( sesid, (JET_DBID)ifmp, 0 );
	(VOID)ErrIsamDetachDatabase( sesid, NULL, szDatabase );

HandleError:

	CallS( taskmgr.ErrTerm() );
	
	(*popts->pcprintfVerbose).Unindent();

	REPAIRPrintEndMessages( szDatabase, timerStart, popts );
	
	REPAIRFreeRepairtables( prepairtable );

	delete pttarrayOwnedSpace;
	delete pttarrayAvailSpace;
	delete pttarraySLVAvail;
	delete pttarraySLVOwnerMapColumnid;
	delete pttarraySLVOwnerMapKey;
	delete pttarraySLVChecksumLengths;

	delete pttarraySLVChecksumsFromFile;
	delete pttarraySLVChecksumLengthsFromSpaceMap;
			
	if( pdbutil->grbitOptions & JET_bitDBUtilOptionStats )
		{
		delete pcprintfStatsInternal;
		}

	delete pfapiSLV;

	fGlobalRepair = fGlobalRepairSave;
	return err;
	}


//  ================================================================
BOOL FIsLargeTable( const CHAR * const szTable )
//  ================================================================
	{
#ifdef DEBUG

	//  check to make sure the rgszLargeTables is in order
	
	static INT fChecked = fFalse;
	if( !fChecked )
		{
		INT iszT;
		for( iszT = 0; iszT < cszLargeTables - 1; ++iszT )
			{
			AssertSz( _stricmp( rgszLargeTables[iszT], rgszLargeTables[iszT+1] ) < 0, "rgszLargeTables is out of order" );
			}
		fChecked = fTrue;
		}
#endif	//	DEBUG

	INT isz;
	for( isz = 0; isz < cszLargeTables; ++isz )
		{
		const INT cmp = _stricmp( szTable, rgszLargeTables[isz] );
		if( 0 == cmp )
			{
			return fTrue;
			}
		if( cmp < 0 )
			{
			//  all other table names in the array will be greater than this as
			//  well. we can stop checking here
			return fFalse;
			}
		}
	return fFalse;
	}


//  ================================================================
LOCAL PGNO PgnoLast( const IFMP ifmp )
//  ================================================================
	{
	return rgfmp[ifmp].PgnoLast();
	}


//  ================================================================
LOCAL VOID REPAIRIPrereadIndexesOfFCB( const FCB * const pfcb )
//  ================================================================
	{
	const INT cSecondaryIndexesToPreread = 64;
	
	PGNO rgpgno[cSecondaryIndexesToPreread + 1];	//  NULL-terminated
	INT	ipgno = 0;
	const FCB *	pfcbT = pfcb->PfcbNextIndex();

	while( pfcbNil != pfcbT && ipgno < cSecondaryIndexesToPreread )
		{
		rgpgno[ipgno++] = pfcbT->PgnoFDP();
		pfcbT = pfcbT->PfcbNextIndex();
		}
	rgpgno[ipgno] = pgnoNull;
	
	BFPrereadPageList( pfcb->Ifmp(), rgpgno );
	}


//  ================================================================
LOCAL VOID REPAIRDumpStats(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const BTSTATS * const pbtstats,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	if( !(popts->grbit & JET_bitDBUtilOptionStats ) )
		{
		return;
		}
		
	(*popts->pcprintfStats)( "total pages: %d\r\n", pbtstats->cpageInternal+pbtstats->cpageLeaf );
	(*popts->pcprintfStats).Indent();
	(*popts->pcprintfStats)( "internal pages: %d\r\n", pbtstats->cpageInternal );
	(*popts->pcprintfStats)( "leaf pages: %d\r\n", pbtstats->cpageLeaf );
	(*popts->pcprintfStats).Unindent();
	(*popts->pcprintfStats)( "tree height: %d\r\n", pbtstats->cpageDepth );
	(*popts->pcprintfStats)( "empty pages: %d\r\n", pbtstats->cpageEmpty );

	LONG cnodeInternal 	= 0;
	LONG cnodeLeaf		= 0;

	QWORD cbKeyTotal	= 0;
	QWORD cbKeySuffixes = 0;
	QWORD cbKeyPrefixes	= 0;
	
	INT ikey;
	for( ikey = 0; ikey < JET_cbKeyMost; ikey++ )
		{
		cnodeInternal += pbtstats->rgckeyInternal[ikey];
		cnodeLeaf	  += pbtstats->rgckeyLeaf[ikey];

		cbKeyTotal	+= pbtstats->rgckeyInternal[ikey] * ikey;
		cbKeyTotal	+= pbtstats->rgckeyLeaf[ikey] * ikey;

		cbKeySuffixes	+= pbtstats->rgckeySuffixInternal[ikey] * ikey;
		cbKeySuffixes	+= pbtstats->rgckeySuffixLeaf[ikey] * ikey;

		cbKeyPrefixes	+= pbtstats->rgckeyPrefixInternal[ikey] * ikey;
		cbKeyPrefixes	+= pbtstats->rgckeyPrefixLeaf[ikey] * ikey;
		}

	const QWORD cbSavings = cbKeyTotal - ( cbKeySuffixes + cbKeyPrefixes + ( pbtstats->cnodeCompressed * 2 ) );
	
	(*popts->pcprintfStats)( "total nodes: %d\r\n", cnodeInternal+cnodeLeaf );	
	(*popts->pcprintfStats)( "compressed nodes: %d\r\n", pbtstats->cnodeCompressed );
	(*popts->pcprintfStats)( "space saved: %d\r\n", cbSavings );
	(*popts->pcprintfStats)( "internal nodes: %d\r\n", cnodeInternal );	
	(*popts->pcprintfStats)( "leaf nodes: %d\r\n", cnodeLeaf );	
	(*popts->pcprintfStats).Indent();
	(*popts->pcprintfStats)( "versioned: %d\r\n", pbtstats->cnodeVersioned );
	(*popts->pcprintfStats)( "deleted: %d\r\n", pbtstats->cnodeDeleted );
	(*popts->pcprintfStats).Unindent();
	}


//  ================================================================
LOCAL VOID REPAIRDumpHex( CHAR * const szDest, const INT cchDest, const BYTE * const pb, const INT cb )
//  ================================================================
	{
	const BYTE * const pbMax = pb + cb;
	const BYTE * pbT = pb;
	
	CHAR * sz = szDest;
	
	while( pbT < pbMax )
		{
		sz += sprintf( sz, "%2.2x", *pbT++ );
		if( pbT != pbMax )
			{
			sz += sprintf( sz, " " );
			}
		}
	}


//  ================================================================
LOCAL VOID REPAIRPrintSig( const SIGNATURE * const psig, CPRINTF * const pcprintf )
//  ================================================================
	{
	const LOGTIME tm = psig->logtimeCreate;
	(*pcprintf)( "Create time:%02d/%02d/%04d %02d:%02d:%02d Rand:%lu Computer:%s\r\n",
						(short) tm.bMonth, (short) tm.bDay,	(short) tm.bYear + 1900,
						(short) tm.bHours, (short) tm.bMinutes, (short) tm.bSeconds,
						ULONG(psig->le_ulRandom),
						psig->szComputerName );
	}


//  ================================================================
LOCAL VOID REPAIRSetCacheSize( const REPAIROPTS * const popts )
//  ================================================================
//
//  try to maximize our cache. ignore any errors
//  because of some weirdness with DBA we cap the maximum cache size at 1.5GB
//
//-
	{
	const ULONG cpgBuffersT	= (ULONG)max( 1536, ( ( OSMemoryAvailable() - ( 16 * 1024 * 1024 ) ) / ( g_cbPage + 128 ) ) );
	const ULONG	cpgBuffers	= (ULONG)min( cpgBuffersT, ( 1536 * 1024 * 1024 ) / ( g_cbPage + 128 ) );
	(*popts->pcprintfVerbose)( "trying for %d buffers\r\n", cpgBuffers );
	CallS( ErrBFSetCacheSize( cpgBuffers ) );
	}


//  ================================================================
LOCAL VOID REPAIRResetCacheSize( const REPAIROPTS * const popts )
//  ================================================================
	{
	CallS( ErrBFSetCacheSize( 0 ) );
	}


//  ================================================================
LOCAL VOID REPAIRPrintStartMessages( const CHAR * const szDatabase, const REPAIROPTS * const popts )
//  ================================================================
	{
	(*popts->pcprintfStats)( "***** %s of database '%s' started [%s version %02d.%02d.%04d.%04d, (%s)]\r\n\r\n",
				( popts->grbit & JET_bitDBUtilOptionDontRepair ) ? "Integrity check" : "Repair",
				szDatabase,
				SzUtilImageVersionName(),
				DwUtilImageVersionMajor(),
				DwUtilImageVersionMinor(),
				DwUtilImageBuildNumberMajor(),
				DwUtilImageBuildNumberMinor(),
				SzUtilImageBuildClass()
				);
				
	(*popts->pcprintfVerbose)( "***** %s of database '%s' started [%s version %02d.%02d.%04d.%04d, (%s)]\r\n\r\n",
				( popts->grbit & JET_bitDBUtilOptionDontRepair ) ? "Integrity check" : "Repair",
				szDatabase,
				SzUtilImageVersionName(),
				DwUtilImageVersionMajor(),
				DwUtilImageVersionMinor(),
				DwUtilImageBuildNumberMajor(),
				DwUtilImageBuildNumberMinor(),
				SzUtilImageBuildClass()
				);

	(*popts->pcprintfVerbose)( "search for \'ERROR:\' to find errors\r\n" );
	(*popts->pcprintfVerbose)( "search for \'WARNING:\' to find warnings\r\n" );
				
	(*popts->pcprintfVerbose).Indent();				

	(*popts->pcprintf)( "\r\nChecking database integrity.\r\n"  );
	}


//  ================================================================
LOCAL ERR ErrREPAIRGetStreamingFileSize(
			const CHAR * const szSLV,
			const IFMP ifmp,
			IFileAPI *const pfapiSLV,
			CPG * const pcpgSLV,
			const REPAIROPTS * const popts )
//  ================================================================
	{			
	ERR err = JET_errSuccess;
	
	QWORD 	cbSLV;
		
	Call( pfapiSLV->ErrSize( &cbSLV ) );
	*pcpgSLV = (LONG)( cbSLV >> (QWORD)g_shfCbPage ) - cpgDBReserved;

	if( cbSLV < ( SLVSPACENODE::cpageMap << g_shfCbPage ) )
		{
		(*popts->pcprintfError)( "streaming file \"%s\" is too small (%I64d bytes, must be at least %d bytes)\r\n",
										szSLV, cbSLV, SLVSPACENODE::cpageMap << g_shfCbPage );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
#ifdef NEVER	

	//	this can be the case because the file is not extended at the end of recovery
	//	instead we wait until the next attach to fix the streaming file size
	//	(this way we avoid opening the catalog during recovery)
	
	else if( 0 != ( ( cbSLV - ( (QWORD)cpgDBReserved << g_shfCbPage ) ) % ( SLVSPACENODE::cpageMap << g_shfCbPage ) ) )
		{
		(*popts->pcprintfError)( "streaming file \"%s\" is a weird size (%I64d bytes, expected a multiple of %d bytes)\r\n",
										szSLV, cbSLV, SLVSPACENODE::cpageMap << g_shfCbPage );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}	
		
#endif	//	NEVER		

	else
		{
			
		(*popts->pcprintfVerbose)( "streaming file has %d pages\r\n", *pcpgSLV );

		//  several things depend on this being set
	
		rgfmp[ifmp].SetSLVFileSize( (QWORD)*pcpgSLV << (QWORD)g_shfCbPage );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckStreamingFileHeader(
	INST *pinst,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	DBFILEHDR * const pdbfilehdr = (DBFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	SLVFILEHDR * const pslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	
	if ( NULL == pslvfilehdr
		|| NULL == pdbfilehdr )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	Call( ErrUtilReadShadowedHeader( pinst->m_pfsapi, szDatabase, (BYTE *)pdbfilehdr, g_cbPage, OffsetOf( DBFILEHDR, le_cbPageSize ) ) );
	Call( ErrUtilReadShadowedHeader( pinst->m_pfsapi, szSLV, (BYTE *)pslvfilehdr, g_cbPage, OffsetOf( SLVFILEHDR, le_cbPageSize ) ) );
	
	if ( attribSLV != pslvfilehdr->le_attrib )
		{
		(*popts->pcprintfError)( "'%s' is not a streaming file (attrib is %d, expected %d)\r\n",
									szSLV, pslvfilehdr->le_attrib, attribSLV );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if ( ( err = ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr ) ) < 0 )
		{
		(*popts->pcprintfError)( "file mismatch between database '%s' and streaming file '%s'\r\n",
									szDatabase, szSLV );
		(*popts->pcprintfError)( "database signatures:\r\n" );
		REPAIRPrintSig( &pdbfilehdr->signDb, popts->pcprintfError );
		REPAIRPrintSig( &pdbfilehdr->signSLV, popts->pcprintfError );
		(*popts->pcprintfError)( "streaming file signatures:\r\n" );
		REPAIRPrintSig( &pslvfilehdr->signDb, popts->pcprintfError );		
		REPAIRPrintSig( &pslvfilehdr->signSLV, popts->pcprintfError );
		// UNDONE STRICT_DB_SLV_CHECK have to print lgpos and logtime
		}

HandleError:
	OSMemoryPageFree( pslvfilehdr );
	OSMemoryPageFree( pdbfilehdr );

	if( JET_errFileNotFound == err )
		{(*popts->pcprintfError)( "streaming file '%s' is missing\r\n", szSLV );
		err = JET_errSLVStreamingFileMissing;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStartCheckSLVTrees(
		PIB * const ppib,
		const IFMP ifmp,
		const CHAR * const szSLV,
		IFileAPI *const pfapiSLV,	
		const ULONG cpgSLV,
		TASKMGR * const ptaskmgr,
		TTARRAY * const pttarrayOwnedSpace,
		TTARRAY * const pttarrayAvailSpace,
		TTARRAY * const pttarraySLVChecksumsFromFile,
		TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,		
		INTEGGLOBALS * const pintegglobals,		
		ERR * const perrSLVChecksum,
		const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	pintegglobals->fCorruptionSeen 				= fFalse;
	pintegglobals->err 							= JET_errSuccess;
	pintegglobals->pprepairtable 				= NULL;
	pintegglobals->pttarrayOwnedSpace 			= pttarrayOwnedSpace;
	pintegglobals->pttarrayAvailSpace 			= pttarrayAvailSpace;
	pintegglobals->pttarraySLVAvail 			= NULL;
	pintegglobals->pttarraySLVOwnerMapColumnid 	= NULL;
	pintegglobals->pttarraySLVOwnerMapKey 		= NULL;
	pintegglobals->pttarraySLVChecksumLengths	= NULL;	
	pintegglobals->popts = popts;

	CHECKTABLE 			* pchecktableSLVAvail		= NULL;
	CHECKTABLE 			* pchecktableSLVOwnerMap	= NULL;

	//  SLVAvail (async)
	
	pchecktableSLVAvail = new CHECKTABLE;
	if( NULL == pchecktableSLVAvail )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecktableSLVAvail->ifmp 				= ifmp;
	strcpy(pchecktableSLVAvail->szTable, szSLVAvail );
	pchecktableSLVAvail->szIndex[0]			= 0;
	pchecktableSLVAvail->objidFDP 			= objidNil;
	pchecktableSLVAvail->pgnoFDP 			= pgnoNull;
	pchecktableSLVAvail->objidParent		= objidSystemRoot;
	pchecktableSLVAvail->pgnoFDPParent		= pgnoSystemRoot;
	pchecktableSLVAvail->fPageFlags			= 0;
	pchecktableSLVAvail->fUnique			= fTrue;
	pchecktableSLVAvail->preccheck			= NULL;
	pchecktableSLVAvail->cpgPrimaryExtent	= 0;
	pchecktableSLVAvail->pglobals			= pintegglobals;
	pchecktableSLVAvail->fDeleteWhenDone	= fTrue;

	err = ptaskmgr->ErrPostTask( REPAIRCheckSLVAvailTreeTask, (ULONG_PTR)pchecktableSLVAvail );
	if( err < 0 )
		{
		delete pchecktableSLVAvail;
		Call( err );
		}

	//	SLVOwnerMap (sync)
	
	pchecktableSLVOwnerMap = new CHECKTABLE;
	if( NULL == pchecktableSLVOwnerMap )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecktableSLVOwnerMap->ifmp 				= ifmp;
	strcpy(pchecktableSLVOwnerMap->szTable, szSLVOwnerMap );
	pchecktableSLVOwnerMap->szIndex[0]			= 0;
	pchecktableSLVOwnerMap->objidFDP 			= objidNil;
	pchecktableSLVOwnerMap->pgnoFDP 			= pgnoNull;
	pchecktableSLVOwnerMap->objidParent			= objidSystemRoot;
	pchecktableSLVOwnerMap->pgnoFDPParent		= pgnoSystemRoot;
	pchecktableSLVOwnerMap->fPageFlags			= 0;
	pchecktableSLVOwnerMap->fUnique				= fTrue;
	pchecktableSLVOwnerMap->preccheck			= NULL;
	pchecktableSLVOwnerMap->cpgPrimaryExtent	= 0;
	pchecktableSLVOwnerMap->pglobals			= pintegglobals;
	pchecktableSLVOwnerMap->fDeleteWhenDone		= fTrue;

	REPAIRCheckSLVOwnerMapTreeTask( ppib, (ULONG_PTR)pchecktableSLVOwnerMap );

	//	Checksum the streaming file (async)

	Call( ErrREPAIRChecksumSLV(
			ifmp,
			pfapiSLV,
			szSLV,
			cpgSLV,
			ptaskmgr,
			perrSLVChecksum,
			pttarraySLVChecksumsFromFile,
			pttarraySLVChecksumLengthsFromSpaceMap,				
			popts ) );	
	
HandleError:
	Ptls()->szCprintfPrefix = NULL;
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStopCheckSLVTrees( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt )
//  ================================================================
	{
	*pfCorrupt = pintegglobals->fCorruptionSeen;
	return pintegglobals->err;
	}


//  ================================================================
LOCAL VOID REPAIRCheckSLVAvailTreeTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckSLVAvailTreeTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;

	ERR err = ErrREPAIRCheckSLVAvailTree(
				ppib,
				pchecktable->ifmp,
				pchecktable->pglobals->pttarrayOwnedSpace,
				pchecktable->pglobals->pttarrayAvailSpace,
				pchecktable->pglobals->popts );

	switch( err )
		{
		
		//  we should never normally get these errors. morph them into corrupted database errors
		
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		
		//  we just need to set this, it will never be unset
		
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		
		}
	else if( err < 0 )
		{
		
		//  we'll just keep the last non-corrupting error
		
		pchecktable->pglobals->err = err;
		
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL VOID REPAIRCheckSLVOwnerMapTreeTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckSLVOwnerMapTreeTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;

	ERR err = ErrREPAIRCheckSLVOwnerMapTree(
				ppib,
				pchecktable->ifmp,
				pchecktable->pglobals->pttarrayOwnedSpace,
				pchecktable->pglobals->pttarrayAvailSpace,
				pchecktable->pglobals->popts );

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		
		//  we just need to set this, it will never be unset
		
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		
		}
	else if( err < 0 )
		{
		
		//  we'll just keep the last non-corrupting error
		
		pchecktable->pglobals->err = err;		
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );	
	}


//  ================================================================
LOCAL BOOL FREPAIRRepairtableHasSLV( const REPAIRTABLE * const prepairtable )
//  ================================================================
	{
	const REPAIRTABLE * prepairtableT = prepairtable;
	while( prepairtableT )
		{
		if( prepairtableT->fTableHasSLV )
			{
			return fTrue;
			}
		prepairtableT = prepairtableT->prepairtableNext;
		}
	return fFalse;
	}


//  ================================================================
LOCAL VOID REPAIRFreeRepairtables( REPAIRTABLE * const prepairtable )
//  ================================================================
	{
	REPAIRTABLE * prepairtableT = prepairtable;
	
	while( prepairtableT )
		{
		REPAIRTABLE * const prepairtableNext = prepairtableT->prepairtableNext;
		prepairtableT->~REPAIRTABLE();
		OSMemoryHeapFree( prepairtableT );
		prepairtableT = prepairtableNext;
		}
	}


//  ================================================================
LOCAL VOID REPAIRPrintEndMessages(
			const CHAR * const szDatabase,
			const ULONG timerStart,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	const ULONG timerEnd = TickOSTimeCurrent();
	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "***** Eseutil completed in %d seconds\r\n\r\n",
				( ( timerEnd - timerStart ) / 1000 ) );
	(*popts->pcprintfVerbose)( "\r\n\r\n" );
	(*popts->pcprintfVerbose)( "***** Eseutil completed in %d seconds\r\n\r\n",
				( ( timerEnd - timerStart ) / 1000 ) );
	}


//  ================================================================
LOCAL VOID INTEGRITYPrintEndErrorMessages(
	const LOGTIME logtimeLastFullBackup,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	(*popts->pcprintf)( "\r\nIntegrity check completed.  " );
	if( 0 == logtimeLastFullBackup.bYear && 0 == logtimeLastFullBackup.bMonth && 
		0 == logtimeLastFullBackup.bDay && 0 == logtimeLastFullBackup.bHours && 
		0 == logtimeLastFullBackup.bMinutes && 0 == logtimeLastFullBackup.bSeconds )
		{
		(*popts->pcprintf)( "Database is CORRUPTED!\r\n" );
		}
	else
		{
		(*popts->pcprintfVerbose)( "\r\nThe last full backup of this database was on %02d/%02d/%04d %02d:%02d:%02d\r\n",
						(short) logtimeLastFullBackup.bMonth, 
						(short) logtimeLastFullBackup.bDay,	
						(short) logtimeLastFullBackup.bYear + 1900, 
						(short) logtimeLastFullBackup.bHours, 
						(short) logtimeLastFullBackup.bMinutes, 
						(short) logtimeLastFullBackup.bSeconds );
		(*popts->pcprintf)( "\r\nDatabase is CORRUPTED, the last full backup of this database was on %02d/%02d/%04d %02d:%02d:%02d\r\n",
						(short) logtimeLastFullBackup.bMonth, 
						(short) logtimeLastFullBackup.bDay,	
						(short) logtimeLastFullBackup.bYear + 1900, 
						(short) logtimeLastFullBackup.bHours, 
						(short) logtimeLastFullBackup.bMinutes, 
						(short) logtimeLastFullBackup.bSeconds );
		}
	}


//  ================================================================
LOCAL JET_ERR __stdcall ErrREPAIRNullStatusFN( JET_SESID, JET_SNP, JET_SNT, void * )
//  ================================================================
	{
	JET_PFNSTATUS pfnStatus = ErrREPAIRNullStatusFN;
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckHeader(
	INST * const pinst,
	const char * const szDatabase,
	const char * const szStreamingFile,
	LOGTIME * const plogtimeLastFullBackup,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	DBFILEHDR_FIX * const pdfh = reinterpret_cast<DBFILEHDR_FIX * >( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pdfh )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	(*popts->pcprintfVerbose)( "checking database header\r\n" );
	(*popts->pcprintfVerbose).Unindent();

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szDatabase ),
				reinterpret_cast<BYTE*>( pdfh ),
				g_cbPage,
				OffsetOf( DBFILEHDR_FIX, le_cbPageSize ) );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read database header\r\n" );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( pdfh->m_ulDbFlags & fDbSLVExists )
		{
		if( NULL == szStreamingFile )
			{
			(*popts->pcprintfError)( "streaming file not specified, but fDBSLVExists set in database header\r\n" );
			err = ErrERRCheck( JET_errSLVStreamingFileMissing );
			goto HandleError;
			}
		}
	else
		{
		if( NULL != szStreamingFile )
			{
			(*popts->pcprintfError)( "streaming file specified, but fDBSLVExists not set in database header\r\n" );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			goto HandleError;
			}
		}
		
	if( JET_dbstateCleanShutdown != pdfh->le_dbstate )
		{
		const CHAR * szState;
		switch( pdfh->le_dbstate )
			{
			case JET_dbstateDirtyShutdown:
				szState = "dirty shutdown";
				break;
			case JET_dbstateForceDetach:
				szState = "force detech";
				break;
			case JET_dbstateJustCreated:
				szState = "just created";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			case JET_dbstateBeingConverted:
				szState = "being converted";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			default:
				szState = "unknown";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			}
		(*popts->pcprintfError)( "database was not shutdown cleanly (%s)\r\n", szState );

		if( JET_dbstateDirtyShutdown == pdfh->le_dbstate 	
			|| JET_dbstateForceDetach == pdfh->le_dbstate )
			{
			(*popts->pcprintf)( "\r\nThe database is not up-to-date. This operation may find that\n");
			(*popts->pcprintf)( "this database is corrupt because data from the log files has\n");
			(*popts->pcprintf)( "yet to be placed in the database.\r\n\n" ); 
			(*popts->pcprintf)( "To ensure the database is up-to-date please use the 'Recovery' operation.\r\n\n" );
			}			
		}


	// Get the last full backup info
	if ( NULL != plogtimeLastFullBackup )
		{
		*plogtimeLastFullBackup = pdfh->bkinfoFullPrev.logtimeMark;
		}

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	OSMemoryPageFree( pdfh );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSystemTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	FUCB * pfucbCatalog = pfucbNil;

	const FIDLASTINTDB fidLastInTDB = { fidMSO_FixedLast, fidMSO_VarLast, fidMSO_TaggedLast };

	(*popts->pcprintfVerbose)( "checking system tables\r\n" );
	(*popts->pcprintfVerbose).Indent();
	
	RECCHECKNULL 	recchecknull;
	RECCHECKTABLE 	recchecktable( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts );
	
	*pfCatalogCorrupt 		= fTrue;
	*pfShadowCatalogCorrupt = fTrue;

	INTEGGLOBALS	integglobalsCatalog;
	INTEGGLOBALS	integglobalsShadowCatalog;

	integglobalsCatalog.fCorruptionSeen 			= integglobalsShadowCatalog.fCorruptionSeen 			= fFalse;
	integglobalsCatalog.err 						= integglobalsShadowCatalog.err 						= JET_errSuccess;
	integglobalsCatalog.pprepairtable 				= integglobalsShadowCatalog.pprepairtable 				= NULL;
	integglobalsCatalog.pttarrayOwnedSpace 			= integglobalsShadowCatalog.pttarrayOwnedSpace 			= pttarrayOwnedSpace;
	integglobalsCatalog.pttarrayAvailSpace 			= integglobalsShadowCatalog.pttarrayAvailSpace 			= pttarrayAvailSpace;
	integglobalsCatalog.pttarraySLVAvail 			= integglobalsShadowCatalog.pttarraySLVAvail 			= NULL;
	integglobalsCatalog.pttarraySLVOwnerMapColumnid = integglobalsShadowCatalog.pttarraySLVOwnerMapColumnid	= NULL;
	integglobalsCatalog.pttarraySLVOwnerMapKey 		= integglobalsShadowCatalog.pttarraySLVOwnerMapKey 		= NULL;
	integglobalsCatalog.pttarraySLVChecksumLengths	= integglobalsShadowCatalog.pttarraySLVChecksumLengths	= NULL;
	integglobalsCatalog.popts 						= integglobalsShadowCatalog.popts 						= popts;

	CHECKTABLE checktableMSO;
	CHECKTABLE checktableMSOShadow;
	CHECKTABLE checktableMSO_NameIndex;
	CHECKTABLE checktableMSO_RootObjectIndex;	

	//  MSO
	
	checktableMSO.ifmp 							= ifmp;
	strcpy(checktableMSO.szTable, szMSO );
	checktableMSO.szIndex[0]					= 0;
	checktableMSO.objidFDP	 					= objidFDPMSO;
	checktableMSO.pgnoFDP 						= pgnoFDPMSO;
	checktableMSO.objidParent					= objidSystemRoot;
	checktableMSO.pgnoFDPParent					= pgnoSystemRoot;
	checktableMSO.fPageFlags					= CPAGE::fPagePrimary;
	checktableMSO.fUnique						= fTrue;
	checktableMSO.preccheck						= &recchecktable;
	checktableMSO.cpgPrimaryExtent				= 16;
	checktableMSO.pglobals						= &integglobalsCatalog;
	checktableMSO.fDeleteWhenDone				= fFalse;

	Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO ) );
	(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO.szTable, checktableMSO.szIndex  );

	//	MSOShadow
	
	checktableMSOShadow.ifmp 					= ifmp;
	strcpy(checktableMSOShadow.szTable, szMSOShadow );
	checktableMSOShadow.szIndex[0]				= 0;
	checktableMSOShadow.objidFDP 				= objidFDPMSOShadow;
	checktableMSOShadow.pgnoFDP 				= pgnoFDPMSOShadow;
	checktableMSOShadow.objidParent				= objidSystemRoot;
	checktableMSOShadow.pgnoFDPParent			= pgnoSystemRoot;
	checktableMSOShadow.fPageFlags				= CPAGE::fPagePrimary;
	checktableMSOShadow.fUnique					= fTrue;
	checktableMSOShadow.preccheck				= &recchecktable;
	checktableMSOShadow.cpgPrimaryExtent		= 16;
	checktableMSOShadow.pglobals				= &integglobalsShadowCatalog;
	checktableMSOShadow.fDeleteWhenDone			= fFalse;

	Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSOShadow ) );
	(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSOShadow.szTable, checktableMSOShadow.szIndex  );

	//	wait for the check of MSO to finish so the space tables are up to date

	checktableMSO.signal.Wait();
	Call( integglobalsCatalog.err );

	if( !integglobalsCatalog.fCorruptionSeen )
		{

		//	MSO ==> MSO_NameIndex
		
		checktableMSO_NameIndex.ifmp 				= ifmp;
		strcpy(checktableMSO_NameIndex.szTable, szMSO );
		strcpy(checktableMSO_NameIndex.szIndex, szMSONameIndex );
		checktableMSO_NameIndex.objidFDP 			= objidFDPMSO_NameIndex;
		checktableMSO_NameIndex.pgnoFDP 			= pgnoFDPMSO_NameIndex;
		checktableMSO_NameIndex.objidParent			= objidFDPMSO;
		checktableMSO_NameIndex.pgnoFDPParent		= pgnoFDPMSO;
		checktableMSO_NameIndex.fPageFlags			= CPAGE::fPageIndex;
		checktableMSO_NameIndex.fUnique				= fTrue;
		checktableMSO_NameIndex.preccheck			= &recchecknull;
		checktableMSO_NameIndex.cpgPrimaryExtent	= 16;
		checktableMSO_NameIndex.pglobals			= &integglobalsCatalog;
		checktableMSO_NameIndex.fDeleteWhenDone		= fFalse;

		Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO_NameIndex ) );
		(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO_NameIndex.szTable, checktableMSO_NameIndex.szIndex  );

		//	MSO ==> MSO_RootObjectIndex

		checktableMSO_RootObjectIndex.ifmp 				= ifmp;
		strcpy(checktableMSO_RootObjectIndex.szTable, szMSO );
		strcpy(checktableMSO_RootObjectIndex.szIndex, szMSORootObjectsIndex );
		checktableMSO_RootObjectIndex.objidFDP 			= objidFDPMSO_RootObjectIndex;
		checktableMSO_RootObjectIndex.pgnoFDP 			= pgnoFDPMSO_RootObjectIndex;
		checktableMSO_RootObjectIndex.objidParent		= objidFDPMSO;
		checktableMSO_RootObjectIndex.pgnoFDPParent		= pgnoFDPMSO;
		checktableMSO_RootObjectIndex.fPageFlags		= CPAGE::fPageIndex;
		checktableMSO_RootObjectIndex.fUnique			= fTrue;
		checktableMSO_RootObjectIndex.preccheck			= &recchecknull;
		checktableMSO_RootObjectIndex.cpgPrimaryExtent	= 16;
		checktableMSO_RootObjectIndex.pglobals			= &integglobalsCatalog;
		checktableMSO_RootObjectIndex.fDeleteWhenDone	= fFalse;

		Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO_RootObjectIndex ) );
		(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO_RootObjectIndex.szTable, checktableMSO_RootObjectIndex.szIndex  );

		//	wait for the index checks

		checktableMSO_NameIndex.signal.Wait();
		checktableMSO_RootObjectIndex.signal.Wait();
		
		Call( integglobalsCatalog.err );
		}

	//	wait for the shadow catalog 
	
	checktableMSOShadow.signal.Wait();
	Call( integglobalsShadowCatalog.err );

	//	were there any corruptions?

	*pfShadowCatalogCorrupt = integglobalsShadowCatalog.fCorruptionSeen;

	//  rebuild the indexes of the catalog

	if( !integglobalsCatalog.fCorruptionSeen )
		{
		(*popts->pcprintfVerbose)( "rebuilding and comparing indexes\r\n" );
		Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fFalse ) );
		Assert( pfucbCatalog->u.pfcb->PfcbNextIndex() != pfcbNil );	
		DIRUp( pfucbCatalog );
		err = ErrFILEBuildAllIndexes(
				ppib,
				pfucbCatalog,
				pfucbCatalog->u.pfcb->PfcbNextIndex(),
				NULL,
				cFILEIndexBatchMax,
				fTrue,
				popts->pcprintfError );

		if( JET_errSuccess != err )
			{
			*pfCatalogCorrupt = fTrue;
			err = JET_errSuccess;
			goto HandleError;
			}

		//	if we made it this far, the catalog must be fine
		
		*pfCatalogCorrupt 		= fFalse;
		}


HandleError:
	(*popts->pcprintfVerbose).Unindent();

	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}

	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRRetrieveCatalogColumns(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * pfucbCatalog, 
	ENTRYINFO * const pentryinfo,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const INT	cColumnsToRetrieve 		= 9;
	JET_RETRIEVECOLUMN	rgretrievecolumn[ cColumnsToRetrieve ];
	ERR					err				= JET_errSuccess;
	INT					iretrievecolumn	= 0;

	memset( rgretrievecolumn, 0, sizeof( rgretrievecolumn ) );

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_ObjidTable;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objidTable );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objidTable );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Type;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objType );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objType );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Id;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objidFDP );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objidFDP );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Name;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->szName );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->szName );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_PgnoFDP;  
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->pgnoFDPORColType );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->pgnoFDPORColType );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	


	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Flags;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->dwFlags );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->dwFlags );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_TemplateTable;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->szTemplateTblORCallback );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->szTemplateTblORCallback );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_RecordOffset;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->ibRecordOffset );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->ibRecordOffset );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;		

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_KeyFldIDs;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->rgbIdxseg );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->rgbIdxseg );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;		

	Call( ErrIsamRetrieveColumns(
				(JET_SESID)ppib,
				(JET_TABLEID)pfucbCatalog,
				rgretrievecolumn,
				iretrievecolumn ) );
				
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertIntoTemplateInfoList(
	TEMPLATEINFOLIST ** ppTemplateInfoList, 
	const CHAR * szTemplateTable,
	INFOLIST * const pInfoList,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR						err				= JET_errSuccess;
	TEMPLATEINFOLIST 	* 	pTemplateInfo;
	TEMPLATEINFOLIST 	*	pTemp 			= *ppTemplateInfoList;

	pTemplateInfo = new TEMPLATEINFOLIST;

	if ( NULL == pTemplateInfo )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	memset( pTemplateInfo, 0, sizeof(TEMPLATEINFOLIST) );
	strcpy( pTemplateInfo->szTemplateTableName, szTemplateTable );
	pTemplateInfo->pColInfoList = pInfoList;
	pTemplateInfo->pTemplateInfoListNext = NULL;

	if (NULL == pTemp ) // empty list
		{
		*ppTemplateInfoList = pTemplateInfo;
		}
	else 
		{
		while ( pTemp->pTemplateInfoListNext )
			{		
			pTemp = pTemp->pTemplateInfoListNext;
			} 
		pTemp->pTemplateInfoListNext = pTemplateInfo;
		}	

HandleError:
	return err;			
	}


//  ================================================================
LOCAL VOID REPAIRUtilCleanInfoList( INFOLIST **ppInfo )
//  ================================================================
	{
	INFOLIST *pTemp = *ppInfo;

	while ( pTemp )
		{
		pTemp = pTemp->pInfoListNext;
		delete *ppInfo;
		*ppInfo = pTemp;
		}
	}


//  ================================================================
LOCAL VOID REPAIRUtilCleanTemplateInfoList( TEMPLATEINFOLIST **ppTemplateInfoList )
//  ================================================================
	{
	TEMPLATEINFOLIST *pTemp = *ppTemplateInfoList;
	
	while ( pTemp )
		{
		pTemp = pTemp->pTemplateInfoListNext;
		REPAIRUtilCleanInfoList( &((*ppTemplateInfoList)->pColInfoList) );
		delete *ppTemplateInfoList;
		*ppTemplateInfoList = pTemp;
		}	
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckOneIndexLogical(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const ENTRYINFO entryinfo, 
	const ULONG objidTable,
	const ULONG pgnoFDPTable,
	const ULONG objidLV,
	const ULONG pgnoFDPLV,
	const INFOLIST * pColInfoList,
	const TEMPLATEINFOLIST * pTemplateInfoList, 
	const BOOL fDerivedTable,
	const CHAR * pszTemplateTableName,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err					= JET_errSuccess;
	ULONG 			fPrimaryIndex 		= fidbPrimary & entryinfo.dwFlags;

	INFOLIST		*	pTemplateColInfo 	= NULL;
	const INFOLIST	* 	pTemp 				= NULL;
	IDXSEG			rgbIdxseg[JET_ccolKeyMost];
	ULONG 			ulKey;

	// check index entry itself
	if( objidTable == entryinfo.objidFDP && !fPrimaryIndex )
		{
		(*popts->pcprintfError)( "objidFDP (%d) in a secondary index is the same as tableid (%d)\t\n", 
									entryinfo.objidFDP, objidTable );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( fPrimaryIndex && objidTable != entryinfo.objidFDP )
		{
		(*popts->pcprintfError)( "objidFDP (%d) in a primary index is not the same as tableid (%d)\t\n", 
									entryinfo.objidFDP, objidTable );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoFDPTable == entryinfo.pgnoFDPORColType && !fPrimaryIndex )
		{
		(*popts->pcprintfError)( "pgnoFDP (%d) in a secondary index is the same as pgnoFDP table (%d)\t\n", 
									entryinfo.pgnoFDPORColType, pgnoFDPTable );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( fPrimaryIndex && pgnoFDPTable != entryinfo.pgnoFDPORColType )
		{
		(*popts->pcprintfWarning)( "pgnoFDP (%d) in a primary index is not the same as pgnoFDP table (%d)\t\n", 
									entryinfo.pgnoFDPORColType, pgnoFDPTable );
		//Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
				
	if( objidLV == entryinfo.objidFDP )
		{
		(*popts->pcprintfError)( "objidFDP (%d) is the same as objid LV (%d)\t\n", 
									entryinfo.objidFDP, objidLV );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoFDPLV == entryinfo.pgnoFDPORColType )
		{
		(*popts->pcprintfError)( "pgnoFDP (%d) is the same as pgnoFDP LV (%d)\t\n", 
									entryinfo.pgnoFDPORColType, pgnoFDPLV );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}


	// Check if a column in an index is a table column 
	
	if( fDerivedTable )
		{
		while ( pTemplateInfoList )
			{
			if ( !strcmp(pszTemplateTableName, pTemplateInfoList->szTemplateTableName) )
				{
				pTemplateColInfo = pTemplateInfoList->pColInfoList; // find it 
				break; 
				}
			else 
				{
				pTemplateInfoList = pTemplateInfoList->pTemplateInfoListNext;
				}
			}
		}

	memset( rgbIdxseg, 0, sizeof(rgbIdxseg) );
	memcpy( rgbIdxseg, entryinfo.rgbIdxseg, sizeof(entryinfo.rgbIdxseg) );
	for( ulKey = 0; ulKey < JET_ccolKeyMost; ulKey++ )
		{
		if ( 0 != rgbIdxseg[ulKey].Fid() ) 
			{
			Assert( pColInfoList || pTemplateColInfo );
			pTemp = pColInfoList;	
			while( pTemp )
				{
				if( rgbIdxseg[ulKey].Fid() == pTemp->info.objidFDP ) 
					{
					break; // Find Columnid
					}
				pTemp = pTemp->pInfoListNext;
				}

			if( !pTemp && fDerivedTable ) // not find in its own table, go to template table
				{
				pTemp = pTemplateColInfo;
				while( pTemp )
					{
					if( rgbIdxseg[ulKey].Fid() == pTemp->info.objidFDP ) // Template Column
						{
						break; // Find Columnid
						} 
					pTemp = pTemp->pInfoListNext;
					}
				}
					
			if ( !pTemp // not find any table column that matches this column in an index
				 || JET_coltypNil == pTemp->info.pgnoFDPORColType )
				{
				(*popts->pcprintfError)( "Column (%d) from an index does not exists in the table (%d) \t\n", rgbIdxseg[ulKey].Fid(), objidTable );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		else // No More Key
			{
			break; // for loop
			}
		} // for
		
HandleError:
	return err;
	}


//  ================================================================ 
ERR ErrREPAIRCheckOneTableLogical(
	INFOLIST **ppInfo, 
	const ENTRYINFO entryinfo, 
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err			= JET_errSuccess;
	INFOLIST 	* 	pInfo;
	INFOLIST 	*	pTemp 		= *ppInfo;
	BOOL			fAddedIntoList 	= fFalse;

	pInfo = new INFOLIST;

	if ( NULL == pInfo )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	memset( pInfo, 0, sizeof(INFOLIST) );
	pInfo->info = entryinfo;
	pInfo->pInfoListNext = NULL;

	if (NULL == pTemp ) // empty list
		{
		*ppInfo = pInfo;
		fAddedIntoList = fTrue;
		}
	else 
		{
		// at least one element in the linked list
		do
			{
			// Check uniqueness of name
			if( !strcmp( pTemp->info.szName, pInfo->info.szName ) )
				{
				(*popts->pcprintfError)( "Dulicated name (%s) in a table\t\n", pInfo->info.szName );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			// Check uniqueness of pgnoFDP for Indexes
			if( sysobjIndex == pInfo->info.objType  && 
				pTemp->info.pgnoFDPORColType == pInfo->info.pgnoFDPORColType )
				{
				(*popts->pcprintfError)( "Dulicated pgnoFDP (%d) in a table\t\n", pInfo->info.pgnoFDPORColType );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			// Check uniqueness of objid
			if ( pTemp->info.objidFDP == pInfo->info.objidFDP )
				{
				(*popts->pcprintfError)( "Dulicated objid (%d) in a table\t\n", pInfo->info.objidFDP );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			Assert( pTemp->info.objidFDP < pInfo->info.objidFDP );

			if( NULL == pTemp->pInfoListNext ) 
				{
				pTemp->pInfoListNext = pInfo; // always inserted into the end of the list
				fAddedIntoList = fTrue;
				break;
				}
			else
				{
				pTemp = pTemp->pInfoListNext;
				}
			}
		while( pTemp );
		}	

HandleError:
	if ( !fAddedIntoList )
		{
		delete pInfo;
		}

	return err;	
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckCatalogEntryPgnoFDPs(
	PIB * const ppib,
	const IFMP ifmp, 
	PGNO  * prgpgno, 
	const ENTRYTOCHECK * const prgentryToCheck,
	INFOLIST **ppEntriesToDelete,
	const BOOL	fFix,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 			err = JET_errSuccess;
	
	ULONG	ulpgno = 0;

	//Preread 
	BFPrereadPageList( ifmp, prgpgno );

	// Check
	for(ulpgno = 0; pgnoNull != prgpgno[ulpgno]; ulpgno++ )
		{
		// check the root page of an index
		OBJID objidIndexFDP = 0;
		CSR	csr;
	
		err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					prgpgno[ulpgno],
					bflfNoTouch );
		if( err < 0 )
			{
			(*popts->pcprintfError)( "error %d trying to read page %d\r\n", err, prgpgno[ulpgno] );
			err = JET_errSuccess;
			}
		else
			{
			objidIndexFDP = csr.Cpage().ObjidFDP();
			}
		csr.ReleasePage( fTrue );

		// Error: the root page of a tree belongs to another objid
		if( prgentryToCheck[ulpgno].objidFDP != objidIndexFDP 
			&& sysobjTable != prgentryToCheck[ulpgno].objType )
			{
			ENTRYINFO entryinfo;
			memset( &entryinfo, 0, sizeof( entryinfo ) );
			entryinfo.objidTable = prgentryToCheck[ulpgno].objidTable;
			entryinfo.objType	 = prgentryToCheck[ulpgno].objType;
			entryinfo.objidFDP	 = prgentryToCheck[ulpgno].objidFDP;

			(*popts->pcprintfError)( "Catalog entry corruption: page %d belongs to %d (expected %d)\t\n", 
									prgpgno[ulpgno], objidIndexFDP, entryinfo.objidFDP );
									
			//Collect Corrupted catalog entry info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( ppEntriesToDelete, entryinfo ) );
				}
				else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckFixCatalogLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL  fShadow,
	const BOOL	fFix,
	const REPAIROPTS * const popts )
//  ================================================================
	{				
	ERR				err				= JET_errSuccess;
	FUCB		* 	pfucbCatalog 	= pfucbNil;

	BOOL			fObjidFDPSeen	= fFalse;
	
	BOOL			fTemplateTable	= fFalse;
	BOOL			fDerivedTable	= fFalse;
	BOOL			fSeenSLVAvail	= fFalse;
	BOOL			fSeenSLVOwnerMap= fFalse;

	ENTRYINFO		entryinfo;
	ULONG			pgnoFDPTableCurr= 0;
	ULONG 			objidTableCurr 	= 0;
	CHAR			szTableName[JET_cbNameMost + 1];
	CHAR			szTemplateTable[JET_cbNameMost + 1];
	ULONG			pgnoFDPLVCurr 	= 0;
	ULONG			objidLVCurr		= 0;
	BOOL 			fSeenLongValue 	= fFalse;
	BOOL 			fSeenCallback 	= fFalse;
	
	BOOL			fSeenCorruptedTable			= fFalse;
	ULONG			objidLastCorruptedTable 	= 0xffffffff;
	BOOL			fSeenCorruptedIndex 		= fFalse;

	INFOLIST	*	pColInfo 		= NULL;
	INFOLIST	*	pIdxInfo		= NULL;
	TEMPLATEINFOLIST	*	pTemplateInfoList = NULL;

	INFOLIST	*	pTablesToDelete = NULL;
	INFOLIST	*	pEntriesToDelete = NULL;

	const INT		cpgnoFDPToPreread		= 64; // max pgnoFDP to pre-read
	
	PGNO			rgpgno[cpgnoFDPToPreread + 1]; //  NULL-terminated
	ENTRYTOCHECK	rgentryToCheck[cpgnoFDPToPreread + 1];
	ULONG			ulCount = 0;
	
	ULONG 			ulpgno;

	//Initialize
	for(ulpgno = 0; cpgnoFDPToPreread >= ulpgno ; ulpgno++ )
		{
		rgpgno[ulpgno] = pgnoNull;
		}


	Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fShadow ) ); 
	Assert( pfucbNil != pfucbCatalog );
	Call( ErrIsamSetCurrentIndex( ppib, pfucbCatalog, szMSOIdIndex ) );
	Call( ErrIsamMove( ppib, pfucbCatalog, JET_MoveFirst, NO_GRBIT ) );

	while ( JET_errNoCurrentRecord != err )
		{
		Call( err );

		memset( &entryinfo, 0, sizeof( entryinfo ) );
		Call( ErrREPAIRRetrieveCatalogColumns( ppib, ifmp, pfucbCatalog, &entryinfo, popts) );

		if( objidSystemRoot == entryinfo.objidTable //special table
			&& !fSeenCorruptedTable ) 
			{
			objidTableCurr = entryinfo.objidTable;
			
			if( sysobjSLVAvail == entryinfo.objType )
				{
				//	special case
				if ( fSeenSLVAvail )
					{
					(*popts->pcprintfError)("Multiple SLVAvail trees in catalog\t\n" );
					fSeenCorruptedTable = fTrue;
					}
				else
					{
					fSeenSLVAvail = fTrue;
					fObjidFDPSeen = fTrue;
					}
				}
			else if( sysobjSLVOwnerMap == entryinfo.objType )
				{	
				//	special case
				if( fSeenSLVOwnerMap )
					{
					(*popts->pcprintfError)("Multiple SLVOwnerMap trees in catalog\t\n" );
					fSeenCorruptedTable = fTrue;
					}
				else
					{
					fSeenSLVOwnerMap = fTrue;
					fObjidFDPSeen = fTrue;
					}
				} 
			else 
				{
				// Error
				(*popts->pcprintfError)( "Invalid object type (%d) for table (%d)\t\n", entryinfo.objType, entryinfo.objidTable );
				fSeenCorruptedTable = fTrue;
				}
			}
		else if( objidTableCurr != entryinfo.objidTable )
			{
			//	we are on the first record of a different table

			// Clean the info of previous table 
			if ( fTemplateTable && !fSeenCorruptedTable )
				{
				// We need keep template table column info
				Call( ErrREPAIRInsertIntoTemplateInfoList( 
								&pTemplateInfoList, 
								szTemplateTable,
								pColInfo, 
								popts ) );
				}
			else
				{ 
				REPAIRUtilCleanInfoList ( &pColInfo );
				}
				REPAIRUtilCleanInfoList ( &pIdxInfo );

			// Start checking new table info
			objidTableCurr = entryinfo.objidTable;
			
			if( sysobjTable != entryinfo.objType )
				{
				//	ERROR: the first record must be sysobjTable
				(*popts->pcprintfError)( "Invalid object type (%d, expected %d)\n", entryinfo.objType, sysobjTable );
				fSeenCorruptedTable = fTrue;
				}
			else if( entryinfo.objidTable != entryinfo.objidFDP )
				{
				Assert( sysobjTable == entryinfo.objType );
				(*popts->pcprintfError)( "Invalid tableid (%d, expected %d)\n", entryinfo.objidFDP, objidTableCurr );
				fSeenCorruptedTable = fTrue;
				}
			else
				{
				fObjidFDPSeen = fTrue;

				// set up some info for new table
				fSeenCorruptedTable	= fFalse;
				pgnoFDPLVCurr		= 0;
				objidLVCurr			= 0;
				fSeenLongValue 		= fFalse;
				fSeenCallback 		= fFalse;
				pColInfo 			= NULL;
				pIdxInfo			= NULL;

				pgnoFDPTableCurr 	= entryinfo.pgnoFDPORColType;
				strcpy( szTableName, entryinfo.szName );
				if( JET_bitObjectTableTemplate & entryinfo.dwFlags ) 
					{
					fTemplateTable = fTrue;
					strcpy( szTemplateTable, entryinfo.szName );
					fDerivedTable = fFalse;
					}
				else if( JET_bitObjectTableDerived & entryinfo.dwFlags )
					{
					fDerivedTable = fTrue;
					strcpy( szTemplateTable, entryinfo.szTemplateTblORCallback );
					fTemplateTable = fFalse;

					TEMPLATEINFOLIST *pTempTemplateList = pTemplateInfoList;
					while( pTempTemplateList )
						{
						if ( !strcmp(szTemplateTable, pTempTemplateList->szTemplateTableName) )
							{
 							break; // find it
							}
						else 
							{
							pTempTemplateList = pTempTemplateList->pTemplateInfoListNext;
							}
						}
					if( NULL == pTempTemplateList )
						{
						//did not find the template table
						(*popts->pcprintfError)("Template table (%s) does not exist\n", szTemplateTable );
						fSeenCorruptedTable = fTrue;
						}
					}
				else 
					{
					fTemplateTable = fFalse;
					fDerivedTable = fFalse;
					}
				}
			}
		else if( !fSeenCorruptedTable )
			{
			// check the logical correctness of a table
			Assert( objidTableCurr == entryinfo.objidTable );

			switch ( entryinfo.objType )
				{
				case sysobjTable:
				//	ERROR: multiple table record;
					(*popts->pcprintfError)("Multiple table records for a table (%d) in a catalog\n", entryinfo.objidTable ); 
					fSeenCorruptedTable = fTrue;
					break;
				case sysobjColumn:
					//check column info and store it into column linked list
					err = ErrREPAIRCheckOneTableLogical( 
									&pColInfo, 
									entryinfo, 
									popts );
					
					if( JET_errDatabaseCorrupted == err )
						{
						fSeenCorruptedTable = fTrue;
						}
						
					break;
				case sysobjIndex:	
					err = ErrREPAIRCheckOneIndexLogical( 
								ppib, 
								ifmp, 
								pfucbCatalog,
								entryinfo, 
								objidTableCurr, 
								pgnoFDPTableCurr, 
								objidLVCurr, 
								pgnoFDPLVCurr, 
								pColInfo, 
								pTemplateInfoList, 
								fDerivedTable, 
								szTemplateTable, 
								popts );
					if( JET_errDatabaseCorrupted == err ) 
						{
						fSeenCorruptedIndex = fTrue;
						}
					else 
						{
						//	check index info and store it into index linked list
						err = ErrREPAIRCheckOneTableLogical( 
									&pIdxInfo, 
									entryinfo, 
									popts );
						if ( JET_errDatabaseCorrupted == err )
							{
							//fSeenCorruptedTable = fTrue;
							fSeenCorruptedIndex = fTrue;
							} 
						else
							{
							fObjidFDPSeen = fTrue;
							}
						}
					break;
				case sysobjLongValue:
					if( fSeenLongValue )
						{
						//	ERROR: multiple long-value records
						(*popts->pcprintfError)("Multiple long-value records for a table (%d) in a catalog\t\n", entryinfo.objidTable );
						fSeenCorruptedTable = fTrue;
						}
					else
						{
						fObjidFDPSeen = fTrue;
						
						pgnoFDPLVCurr = entryinfo.pgnoFDPORColType;
						objidLVCurr = entryinfo.objidFDP;
						fSeenLongValue = fTrue;
						}
					break;
				case sysobjCallback:
					if( fSeenCallback )
						{
						//	ERROR: multiple callbacks
						(*popts->pcprintfError)("Multiple callbacks for a table (%d) in a catalog\t\n", entryinfo.objidTable );
						fSeenCorruptedTable = fTrue;
						}
					else
						{
						fSeenCallback = fTrue;
						}
					break;
				case sysobjSLVAvail:
				case sysobjSLVOwnerMap:
				default:
					//	ERROR
					(*popts->pcprintfError)("Invalid Object Type (%d) for a table (%d) in a catalog\t\n", entryinfo.objType, entryinfo.objidTable );
					fSeenCorruptedTable = fTrue;
					break;
				} //switch
			}
		else
			{
			// Inside a table that has already been found corruption 
			Assert( fSeenCorruptedTable && objidTableCurr == entryinfo.objidTable );
			}
		
		if( fSeenCorruptedIndex )
			{
			//Collect Corrupted index info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( &pEntriesToDelete, entryinfo ) );
				}
			else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			fSeenCorruptedIndex = fFalse;
			}

		if( fSeenCorruptedTable && objidLastCorruptedTable != entryinfo.objidTable )
			{
			//Collect Corrupted table info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( &pTablesToDelete, entryinfo ) );
				}
			else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			objidLastCorruptedTable = entryinfo.objidTable;
			}

		if( fObjidFDPSeen )
			{
			if( entryinfo.objidFDP > *pobjidLast )
				{
				*pobjidLast = entryinfo.objidFDP;
				}
				
			rgpgno[ulCount] = entryinfo.pgnoFDPORColType;
			rgentryToCheck[ulCount].objidTable = entryinfo.objidTable; 
			rgentryToCheck[ulCount].objType	= entryinfo.objType;
			rgentryToCheck[ulCount].objidFDP = entryinfo.objidFDP;
			ulCount++;
			
			fObjidFDPSeen = fFalse;
			}
			
		err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveNext, NO_GRBIT );

		//Special case: 
		//test if the root page in an index belongs to objid
		if ( 0 == ( ulCount % ( cpgnoFDPToPreread - 1 ) ) 
			 || JET_errNoCurrentRecord == err )
			{
			ERR errCheck = JET_errSuccess;
			
			errCheck = ErrREPAIRICheckCatalogEntryPgnoFDPs(
						ppib,
						ifmp, 
						rgpgno, 
						rgentryToCheck,
						&pEntriesToDelete,
						fFix,
						popts );
			if( errCheck < 0 )
				{
				Call( errCheck );
				}

			if( JET_errNoCurrentRecord != err )
				{
				//reset the array for future use
				for( ULONG ulpgno = 0; pgnoNull != rgpgno[ulpgno]; ulpgno++ )
					{
					Assert( cpgnoFDPToPreread > ulpgno ); 
					rgpgno[ulpgno] = pgnoNull;
					}
				//reset the counter
				ulCount = 0;
				}
			}
		
		} // while ( JET_errNoCurrentRecord != err )

	if( JET_errNoCurrentRecord == err )
		{
		//	expected error: we are at the end of catalog
		err = JET_errSuccess;
		}

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	
	// clean		
	REPAIRUtilCleanInfoList( &pColInfo );
	REPAIRUtilCleanInfoList( &pIdxInfo );
	REPAIRUtilCleanTemplateInfoList( &pTemplateInfoList );

	if( pfucbNil != pfucbCatalog )
		{
		CallS( ErrCATClose( ppib, pfucbCatalog ) );
		}

		
	if( fFix && ( pTablesToDelete || pEntriesToDelete ) )
		{
		// Delete corrupted table entries in catalog
		(*popts->pcprintfVerbose)( "Deleting corrupted catalog entries\t\n" );
		err = ErrREPAIRDeleteCorruptedEntriesFromCatalog( 
					ppib, 
					ifmp, 
					pTablesToDelete, 
					pEntriesToDelete, 
					popts );
		if( JET_errSuccess == err )
			{
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		}
		
	// clean
	if( pTablesToDelete )
		{
		REPAIRUtilCleanInfoList( &pTablesToDelete );
		}
	if( pEntriesToDelete )
		{
		REPAIRUtilCleanInfoList( &pEntriesToDelete );
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSystemTablesLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err				= JET_errSuccess;
	const BOOL 		fFix			= fFalse;   // Don't fix 

	// Enter this function when we did not find corruption 
	// in at least one catalog from physical check
	Assert( ! ( *pfCatalogCorrupt ) || !( *pfShadowCatalogCorrupt ) );

	(*popts->pcprintfVerbose)( "checking logical consistency of system tables\t\n" );
	(*popts->pcprintfVerbose).Indent();


	if( !( *pfCatalogCorrupt ) )
		{
		(*popts->pcprintfVerbose)( "%s\t\n", szMSO );
		err = ErrREPAIRCheckFixCatalogLogical( ppib, ifmp, pobjidLast, fFalse, fFix, popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "%s is logically corrupted\t\n", szMSO );
			*pfCatalogCorrupt = fTrue;
			}
		else
			{
			Call( err );
			}
		}

	if( !( *pfShadowCatalogCorrupt ) )
		{
		(*popts->pcprintfVerbose)( "%s\t\n", szMSOShadow );
		err = ErrREPAIRCheckFixCatalogLogical( ppib, ifmp, pobjidLast, fTrue, fFix, popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "%s is logically corrupted\t\n", szMSOShadow );
			*pfShadowCatalogCorrupt = fTrue;
			}
		else
			{
			Call( err );
			}
		}

	Assert( JET_errDatabaseCorrupted == err ||
			JET_errSuccess	== err );
	err = JET_errSuccess;

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRCheckSpaceTree(
	PIB * const ppib,
	const IFMP ifmp, 
	BOOL * const pfSpaceTreeCorrupt,
	PGNO * const ppgnoLastOwned,	
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	RECCHECKNULL recchecknull;
	RECCHECKSPACE	reccheckSpace( ppib, popts );
	RECCHECKSPACEAE reccheckAE( ppib, pttarrayOwnedSpace, pttarrayAvailSpace, 1, objidNil, popts );

	BTSTATS btstats;
	memset( &btstats, 0, sizeof( btstats ) );

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	(*popts->pcprintfVerbose)( "checking SystemRoot\r\n" );
	(*popts->pcprintfVerbose).Indent();

	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "===== database root =====\r\n" );

	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot,
			1,
			CPAGE::fPagePrimary,
			&recchecknull,
			NULL,
			NULL,
			fFalse,
			&btstats,
			popts ) );
	(*popts->pcprintfVerbose)( "SystemRoot (OE)\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot+1,
			1,
			CPAGE::fPageSpaceTree,
			&reccheckSpace,
			NULL,
			NULL,
			fFalse,
			&btstats,
			popts ) );
	(*popts->pcprintfVerbose)( "SystemRoot (AE)\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot+2,
			1,
			CPAGE::fPageSpaceTree,
			&reccheckAE,
			NULL,
			NULL,
			fFalse,
			&btstats,
			popts ) );

	*ppgnoLastOwned = reccheckSpace.PgnoLast();

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVOwnerMapCorrupted:
		case JET_errDatabaseCorrupted:
			*pfSpaceTreeCorrupt = fTrue;
			err = JET_errSuccess;
			break;
		default:
			*pfSpaceTreeCorrupt = fFalse;
			break;
		}

	return err;
	}


//  ================================================================
LOCAL ErrREPAIRCheckRange(
	PIB * const ppib,
	TTARRAY * const pttarray,
	const ULONG ulFirst,
	const ULONG ulLast,
	const ULONG ulValue,
	BOOL * const pfMismatch,
	const REPAIROPTS * const popts )
//  ================================================================
//
//	Make sure all entries in the ttarray are equal to the given value
//
//-
	{
	ERR err = JET_errSuccess;
	TTARRAY::RUN run;

	pttarray->BeginRun( ppib, &run );

	ULONG ul;
	for( ul = ulFirst; ul < ulLast; ul++ )
		{
		ULONG ulActual;

		Call( pttarray->ErrGetValue( ppib, ul, &ulActual, &run ) );

		if( ulActual != ulValue )
			{
			*pfMismatch = fTrue;
			break;
			}
		}


HandleError:
	pttarray->EndRun( ppib, &run );	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVAvailTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	RECCHECKSLVSPACE reccheckslvspace( ifmp, popts );

	PGNO 	pgnoSLVAvail;
	OBJID	objidSLVAvail;
	Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );

	(*popts->pcprintfVerbose)( "checking SLV space (pgno %d, objid %d)\r\n", pgnoSLVAvail, objidSLVAvail );
	(*popts->pcprintfVerbose).Indent();

	Call( ErrREPAIRCheckTreeAndSpace(
			ppib,
			ifmp,
			objidSLVAvail,
			pgnoSLVAvail,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPageSLVAvail,
			fTrue,
			&reccheckslvspace,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );
	
HandleError:

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:		
		case JET_errSLVSpaceMapCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVOwnerMapTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;
	
	RECCHECKSLVOWNERMAP reccheckslvownermap( popts );

	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;
	Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );

	(*popts->pcprintfVerbose)( "checking SLV space map (pgno %d, objid %d)\r\n", pgnoSLVOwnerMap, objidSLVOwnerMap );
	(*popts->pcprintfVerbose).Indent();

	Call( ErrREPAIRCheckTreeAndSpace(
			ppib,
			ifmp,
			objidSLVOwnerMap,
			pgnoSLVOwnerMap,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPageSLVOwnerMap,
			fTrue,
			&reccheckslvownermap,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );
	
HandleError:

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:  		
		case JET_errSLVSpaceMapCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}
		
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRStreamingFileReadComplete(
		const ERR			err,
		IFileAPI *const	pfapi,
		const QWORD			ibOffset,
		const DWORD			cbData,
		const BYTE* const	pbData,
		const DWORD_PTR		dwCompletionKey )
//  ================================================================
//
//  Just set the signal, the checksumming will be done in the main thread
//
//-
	{
	IFileAPI::PfnIOComplete pfn = REPAIRStreamingFileReadComplete;	//	should only compile if signatures match
	
	CHECKSUMSLVCHUNK * pchecksumchunk = (CHECKSUMSLVCHUNK *)dwCompletionKey;
	pchecksumchunk->err = err;
	pchecksumchunk->signal.Set();
	}


//  ================================================================
LOCAL ERR ErrREPAIRAllocChecksumslvchunk( CHECKSUMSLVCHUNK * const pchecksumslvchunk, const INT cbBuffer, const INT cpages )
//  ================================================================
	{
	pchecksumslvchunk->pvBuffer 			= PvOSMemoryPageAlloc( cbBuffer, NULL );
	pchecksumslvchunk->pulChecksums 		= new ULONG[cpages];
	pchecksumslvchunk->pulChecksumsExpected = new ULONG[cpages];
	pchecksumslvchunk->pulChecksumLengths 	= new ULONG[cpages];
	pchecksumslvchunk->pobjid				= new OBJID[cpages];
	pchecksumslvchunk->pcolumnid			= new COLUMNID[cpages];
	pchecksumslvchunk->pcbKey				= new USHORT[cpages];
	pchecksumslvchunk->pprgbKey				= new BYTE*[cpages];

	if(	NULL == pchecksumslvchunk->pvBuffer
		|| NULL == pchecksumslvchunk->pulChecksums
		|| NULL == pchecksumslvchunk->pulChecksumsExpected
		|| NULL == pchecksumslvchunk->pulChecksumLengths
		|| NULL == pchecksumslvchunk->pobjid
		|| NULL == pchecksumslvchunk->pcolumnid
		|| NULL == pchecksumslvchunk->pcbKey
		|| NULL == pchecksumslvchunk->pprgbKey )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	for (int ipage = 0; ipage < cpages; ipage++) 
		{
		pchecksumslvchunk->pprgbKey[ipage] = new BYTE[KeyLengthMax];
		if ( NULL == pchecksumslvchunk->pprgbKey[ipage] )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
		}

	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAllocChecksumslv( CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	pchecksumslv->rgchecksumchunk 			= new CHECKSUMSLVCHUNK[pchecksumslv->cchecksumchunk];
	if( NULL == pchecksumslv->rgchecksumchunk )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	const INT cbBuffer 	= pchecksumslv->cbchecksumchunk;
	const INT cpages 	= cbBuffer / g_cbPage;

	INT ichunk;
	for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
		{
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		pchecksumslv->rgchecksumchunk[ichunk].err		= JET_errSuccess;
		
		const ERR err = ErrREPAIRAllocChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cbBuffer, cpages );

		if( err < 0 )
			{
			while( --ichunk > 0 )
				{
				REPAIRFreeChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cpages );
				}
			return err;
			}
		}
	return JET_errSuccess;
	}


//  ================================================================
LOCAL VOID REPAIRFreeChecksumslvchunk( CHECKSUMSLVCHUNK * const pchecksumslvchunk, const INT cpages  )
//  ================================================================
	{
	OSMemoryPageFree( pchecksumslvchunk->pvBuffer );
	delete [] pchecksumslvchunk->pulChecksums;
	delete [] pchecksumslvchunk->pulChecksumsExpected;
	delete [] pchecksumslvchunk->pulChecksumLengths;
	delete [] pchecksumslvchunk->pobjid;
	delete [] pchecksumslvchunk->pcolumnid;
	delete [] pchecksumslvchunk->pcbKey;
	for (int ipage = 0; ipage < cpages; ipage++)
		{
		if ( pchecksumslvchunk->pprgbKey[ipage] )
			delete [] pchecksumslvchunk->pprgbKey[ipage];
		pchecksumslvchunk->pprgbKey[ipage] = NULL;
		} 
	delete [] pchecksumslvchunk->pprgbKey;
	
	pchecksumslvchunk->pvBuffer 	= NULL;
	pchecksumslvchunk->pulChecksums	= NULL;
	pchecksumslvchunk->pulChecksumsExpected = NULL;
	pchecksumslvchunk->pulChecksumLengths	= NULL;
	pchecksumslvchunk->pobjid = NULL;
	pchecksumslvchunk->pcolumnid = NULL;
	pchecksumslvchunk->pcbKey = NULL;
	pchecksumslvchunk->pprgbKey = NULL;

	}


//  ================================================================
LOCAL VOID REPAIRFreeChecksumslv( CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	if( pchecksumslv )
		{
		if( pchecksumslv->rgchecksumchunk )
			{
			const INT cpages = pchecksumslv->cbchecksumchunk / g_cbPage;
			INT ichunk;
			for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
				{
				if( NULL != pchecksumslv->rgchecksumchunk[ichunk].pvBuffer )
					{
					REPAIRFreeChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cpages );
					}
				}
			delete [] pchecksumslv->rgchecksumchunk;
			}
		delete pchecksumslv;
		}
	}


//  ================================================================
LOCAL ERR ErrREPAIRChecksumSLVChunk(
	PIB * const ppib,
	const CHECKSUMSLV * const pchecksumslv,
	const INT ichunk,
	BOOL * const pfMismatchSeen )
//  ================================================================
	{
	ERR 	err 		= JET_errSuccess;

	const CPG cpgRead 	= pchecksumslv->rgchecksumchunk[ichunk].cbRead / g_cbPage;
	const BYTE * pb 	= (BYTE *)pchecksumslv->rgchecksumchunk[ichunk].pvBuffer;

	PGNO pgno = pchecksumslv->rgchecksumchunk[ichunk].pgnoFirst;

	*pfMismatchSeen 	= fFalse;

	//

	TTARRAY::RUN checksumsRun;
	TTARRAY::RUN checksumLengthsRun;

	pchecksumslv->pttarraySLVChecksumsFromFile->BeginRun( ppib, &checksumsRun );
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->BeginRun( ppib, &checksumLengthsRun );

	//
	
	INT ipage;
	for( ipage = 0; ipage < cpgRead; ++ipage )
		{
		const ULONG cbChecksum 			= pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage];
		Assert( cbChecksum <= g_cbPage );
		
		if( 0 != cbChecksum )
			{
			const ULONG ulChecksum 			= UlChecksumSLV( pb, pb + cbChecksum );
			const ULONG ulChecksumExpected = pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage];

			pchecksumslv->rgchecksumchunk[ichunk].pulChecksums[ipage] = ulChecksum;

			if( ulChecksum != ulChecksumExpected )
				{
				CHAR rgbKey[64];
				REPAIRDumpHex(
					rgbKey,
					sizeof( rgbKey ),
					pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage],
					pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] );
				
				(*pchecksumslv->popts->pcprintfError)(
					"SLV checksum mismatch: Page %d (owned by %d:%d:%s). Checksum length %d bytes. Expected checksum 0x%x. Actual checksum 0x%x.\r\n",
					pgno,   
					pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage], 
					pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage],
					rgbKey,
					cbChecksum, 
					ulChecksumExpected, 
					ulChecksum);
					
				*pfMismatchSeen = fTrue;
				}
			else
				{
				Call( pchecksumslv->pttarraySLVChecksumsFromFile->ErrSetValue(
					ppib,
					pgno,
					ulChecksum,
					&checksumsRun ) );
				Call( pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->ErrSetValue(
					ppib,
					pgno,
					cbChecksum,
					&checksumLengthsRun ) );
				}
			}
		pb += g_cbPage;
		++pgno;
		}

HandleError:

	pchecksumslv->pttarraySLVChecksumsFromFile->EndRun( ppib, &checksumsRun );
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->EndRun( ppib, &checksumLengthsRun );
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRISetupChecksumchunk(
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv,
	BOOL * const pfDone,
	BOOL * const pfChunkHasChecksums,
	const INT ichunk )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	INT ipage;

	const INT cpages 	= pchecksumslv->cbchecksumchunk / g_cbPage;	
	const INT cbToZero 	= sizeof( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[0] ) * cpages;
	
	memset( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected, 0, cbToZero );
	memset( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths, 0, cbToZero );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pobjid, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pobjid[0] ) * cpages );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pcolumnid, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[0] ) * cpages );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pcbKey, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pcbKey[0] ) * cpages );
	for(ipage =0; ipage < cpages; ++ipage)
		{
		memset(pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 0, KeyLengthMax );
		}
	
	*pfDone 				= fFalse;
	*pfChunkHasChecksums	= fFalse;

	pchecksumslv->rgchecksumchunk[ichunk].signal.Reset();
	pchecksumslv->rgchecksumchunk[ichunk].err 		= JET_errSuccess;
	pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
	pchecksumslv->rgchecksumchunk[ichunk].cbRead	= 0;
	
	//	are we at the end of slvspacemap?
	
	if( !Pcsr( pfucbSLVSpaceMap )->FLatched() )
		{
		*pfDone = fTrue;
		return JET_errSuccess;
		}

	//

	PGNO pgnoFirst;
	LongFromKey( (ULONG *)&pgnoFirst, pfucbSLVSpaceMap->kdfCurr.key );
	pchecksumslv->rgchecksumchunk[ichunk].pgnoFirst = pgnoFirst;
	
	//	propagate the checksum lengths into the CHECKSUMCHUNK

	INT ipageMaxWithChecksum = 0;
	for( ipage = 0; ipage < cpages; ++ipage )
		{
		SLVOWNERMAPNODE slvownermapnode;	
		
		slvownermapnode.Retrieve( pfucbSLVSpaceMap->kdfCurr.data );

		if( slvownermapnode.FValidChecksum() )
			{
			ipageMaxWithChecksum = ipage;
			
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage] = slvownermapnode.UlChecksum();
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage] 	= slvownermapnode.CbDataChecksummed();		
			pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage] = slvownermapnode.Objid();
			pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage] = slvownermapnode.Columnid();
			pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] = slvownermapnode.CbKey();

			//make sure the key Length is not greater than KeyLengthMax
			if (pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] > KeyLengthMax)
				{
				(*pchecksumslv->popts->pcprintfError)(
					"INTERNAL ERROR: key of SLV-owning record is too large (%d bytes, buffer is %d bytes)\r\n",
					pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage], KeyLengthMax );
				Call( ErrERRCheck( JET_errInternalError ) );				
				}

			memcpy(	pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 
			        (BYTE *)slvownermapnode.PvKey(), 
			        pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] );
					
			*pfChunkHasChecksums = fTrue;
			}
		else
			{
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage] 	= 0;
			pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] = 0;
			memset(pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 0, KeyLengthMax );
			}
			
		err = ErrDIRNext( pfucbSLVSpaceMap, fDIRNull );
		if( JET_errNoCurrentRecord == err )
			{
			err = JET_errSuccess;
			break;
			}
		Call( err );
		}

	pchecksumslv->rgchecksumchunk[ichunk].cbRead = ( ipageMaxWithChecksum + 1 ) * g_cbPage;
	
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRSetSequentialMoveFirst(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCBSetPrereadForward( pfucbSLVSpaceMap, cpgPrereadSequential );
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;

	Call( ErrDIRDown( pfucbSLVSpaceMap, &dib ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVChecksums(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	ERR errSav = JET_errSuccess;

	BOOL fNoMoreReads 		= fFalse;
	BOOL fChunkHasChecksums	= fFalse;

	BOOL fMismatchSeen		= fFalse;
	BOOL fMismatchSeenT		= fFalse;
	
	const QWORD ibOffsetMax = ( ( pchecksumslv->cpgSLV + cpgDBReserved ) << g_shfCbPage );
	QWORD ibOffset = (cpgDBReserved << g_shfCbPage);
	
	INT ichunk;

	//	move to the start of the SLVSpaceMap

	Call( ErrREPAIRSetSequentialMoveFirst( ppib, pfucbSLVSpaceMap, pchecksumslv->popts ) );
	
	//  setup initial reads

	ichunk = 0;
	while( ichunk < pchecksumslv->cchecksumchunk && !fNoMoreReads )
		{
		Call( ErrREPAIRISetupChecksumchunk(
				pfucbSLVSpaceMap,
				pchecksumslv,
				&fNoMoreReads,
				&fChunkHasChecksums,
				ichunk ) );
		if( fChunkHasChecksums )
			{
			Assert( 0 != pchecksumslv->rgchecksumchunk[ichunk].cbRead );
			
			Call( pchecksumslv->pfapiSLV->ErrIORead(
										ibOffset,
										pchecksumslv->rgchecksumchunk[ichunk].cbRead,
										(BYTE *)(pchecksumslv->rgchecksumchunk[ichunk].pvBuffer),
										REPAIRStreamingFileReadComplete,
										(DWORD_PTR)(pchecksumslv->rgchecksumchunk + ichunk) ) );
										
			pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fTrue;
						
			Assert( ibOffset <= ibOffsetMax );
			++ichunk;
			}
		ibOffset += pchecksumslv->cbchecksumchunk;			
		}

	CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );
		
	//

	ichunk = 0;
	while( !fNoMoreReads )
		{
		
		//	collect the I/O for this chunk

		Assert( pchecksumslv->rgchecksumchunk[ichunk].fIOIssued );
		pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		
		Call( pchecksumslv->rgchecksumchunk[ichunk].err );

		Call( ErrREPAIRChecksumSLVChunk(	
				ppib,
				pchecksumslv,
				ichunk,
				&fMismatchSeenT ) );
		fMismatchSeen = ( fMismatchSeen || fMismatchSeenT );

		//

		while (1)
			{
			Call( ErrREPAIRISetupChecksumchunk(
				pfucbSLVSpaceMap,
				pchecksumslv,
				&fNoMoreReads,
				&fChunkHasChecksums,
				ichunk ) );

			if( fChunkHasChecksums || fNoMoreReads )
				{
				break;
				}
			else
				{
				ibOffset += pchecksumslv->cbchecksumchunk;
				}
			}
			
		if( !fChunkHasChecksums )
			{
			Assert( fNoMoreReads );
			}
		else
			{
			Assert( 0 != pchecksumslv->rgchecksumchunk[ichunk].cbRead );
			
			Call( pchecksumslv->pfapiSLV->ErrIORead(
										ibOffset,
										pchecksumslv->rgchecksumchunk[ichunk].cbRead,
										(BYTE *)(pchecksumslv->rgchecksumchunk[ichunk].pvBuffer),
										REPAIRStreamingFileReadComplete,
										(DWORD_PTR)(pchecksumslv->rgchecksumchunk + ichunk) ) );
										
			pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fTrue;
			
			ichunk = ( ichunk + 1 ) % pchecksumslv->cchecksumchunk;
			ibOffset += pchecksumslv->cbchecksumchunk;
			
			Assert( ibOffset <= ibOffsetMax );
			}			

		CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );		
		}

	//  collect all outstanding reads

	for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
		{
		if( !pchecksumslv->rgchecksumchunk[ichunk].fIOIssued )
			{
			continue;
			}
			
		pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		
		Call( pchecksumslv->rgchecksumchunk[ichunk].err );

		Call( ErrREPAIRChecksumSLVChunk(	
				ppib,
				pchecksumslv,
				ichunk,
				&fMismatchSeenT ) );
		fMismatchSeen = ( fMismatchSeen || fMismatchSeenT );
		}
		
HandleError:

	BTUp( pfucbSLVSpaceMap );
	CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );

	for ( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ichunk++ )
		{
		if( pchecksumslv->rgchecksumchunk[ichunk].fIOIssued )
			{
			Assert( err < JET_errSuccess );
			pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
			}
		}

	if( JET_errSuccess == err && fMismatchSeen )
		{
		*(pchecksumslv->perr) = JET_errSLVReadVerifyFailure;
		}
	else
		{
		*(pchecksumslv->perr) = err;		
		}
		
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRSLVChecksumTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRSLVChecksumTask;	// should only compile if the signatures match

	ERR err = JET_errSuccess;

	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;

	FUCB * pfucbSLVOwnerMap = pfucbNil;
	
	CHECKSUMSLV * const pchecksumslv = (CHECKSUMSLV *)ul;

	Ptls()->szCprintfPrefix = pchecksumslv->szSLV;

	(*pchecksumslv->popts->pcprintfVerbose)( "Checksumming streaming file \"%s\" (%d pages)\r\n",
		pchecksumslv->szSLV,
		pchecksumslv->cpgSLV );

	Call( ErrCATAccessDbSLVOwnerMap( ppib, pchecksumslv->ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );
	Call( ErrBTOpen( ppib, pgnoSLVOwnerMap, pchecksumslv->ifmp, &pfucbSLVOwnerMap ) );	
	Call( ErrREPAIRCheckSLVChecksums( ppib, pfucbSLVOwnerMap, pchecksumslv ) );
		
HandleError:
	(*pchecksumslv->popts->pcprintfVerbose)( "Checksumming of streaming file \"%s\" finishes with error %d\r\n",
		pchecksumslv->szSLV,
		err );

	if( pfucbNil != pfucbSLVOwnerMap )
		{
		DIRClose( pfucbSLVOwnerMap );
		}
	REPAIRFreeChecksumslv( pchecksumslv );		
	}


//  ================================================================
LOCAL ERR ErrREPAIRChecksumSLV(
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TASKMGR * const ptaskmgr,
	ERR * const perr,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CHECKSUMSLV * const pchecksumslv = new CHECKSUMSLV;

	if( NULL == pchecksumslv )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecksumslv->ifmp						= ifmp;
	pchecksumslv->pfapiSLV 					= pfapiSLV;
	pchecksumslv->szSLV 					= szSLV;
	pchecksumslv->cpgSLV 					= cpgSLV;
	pchecksumslv->perr 						= perr;
	pchecksumslv->popts 					= popts;

	pchecksumslv->pttarraySLVChecksumsFromFile				= pttarraySLVChecksumsFromFile;
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap 	= pttarraySLVChecksumLengthsFromSpaceMap;

	pchecksumslv->cchecksumchunk 			= cSLVChecksumChunk;	
	pchecksumslv->cbchecksumchunk 			= cbSLVChecksumChunk;

	Call( ErrREPAIRAllocChecksumslv( pchecksumslv ) );
	
	Call( ptaskmgr->ErrPostTask( REPAIRSLVChecksumTask, (ULONG_PTR)pchecksumslv ) );
	
HandleError:
	if( err < 0 )
		{
		REPAIRFreeChecksumslv( pchecksumslv );
		}
	return err;
	}	

//  ================================================================
LOCAL ERR ErrREPAIRVerifySLVTrees(
	PIB * const ppib, 
	const IFMP ifmp,
	BOOL * const pfSLVSpaceTreesCorrupt,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY	* const pttarraySLVChecksumLengths,	
	const REPAIROPTS * const popts )
//  ================================================================
//
//  At this point all the tables in the database should be good.
//  Check logical consistency.
//
//-
	{
	ERR err = JET_errSuccess;

	BOOL fCorrupted = fFalse;
	
	(*popts->pcprintfVerbose)( "Verifying SLV space maps\r\n" );

	CSLVAvailIterator 		slvavailIterator;
	CSLVOwnerMapIterator 	slvownermapIterator;

	//  no need to worry about deadlock between these TTARRAYs because they are
	//  only being accessed by this thread for the duration of this function call
	TTARRAY::RUN availRun;
	TTARRAY::RUN columnidRun;
	TTARRAY::RUN keyRun;
	TTARRAY::RUN lengthRun;
	
	INT pgno = 1;	

	Call( slvavailIterator.ErrInit( ppib, ifmp ) );
	Call( slvownermapIterator.ErrInit( ppib, ifmp ) );

	Call( slvavailIterator.ErrMoveFirst() );
	Call( slvownermapIterator.ErrMoveFirst() );

	pttarraySLVAvail->BeginRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->BeginRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->BeginRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->BeginRun( ppib, &lengthRun );
	
	while(1)
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil == objidOwning )
			{
			
			//  unused page
			
			if( slvavailIterator.FCommitted() )
				{
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is unused but is marked as committed\r\n", pgno );
				fCorrupted = fTrue;
				}

			if( !slvownermapIterator.FNull() )
				{
				CHAR rgbKey[64];
				REPAIRDumpHex(
					rgbKey,
					sizeof( rgbKey ),
					(BYTE *)slvownermapIterator.PvKey(),
					slvownermapIterator.CbKey() );
				
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is unused but is marked as owned by %d:%d:%s\r\n",
					pgno, slvownermapIterator.Objid(), slvownermapIterator.Columnid(), rgbKey );

				fCorrupted = fTrue;
				}
				
			}
		else
			{
			
			//  used page

			COLUMNID columnid;
			ULONG cbDataChecksummed;
			ULONG rgulKey[culSLVKeyToStore];
			BYTE * const pbKey = (BYTE *)rgulKey;
			
			KEY keyOwning;
			keyOwning.Nullify();
			keyOwning.suffix.SetPv( pbKey + 1 );

			KEY keyExpected;
			keyExpected.Nullify();
			keyExpected.suffix.SetPv( const_cast<VOID *>( slvownermapIterator.PvKey() ) );
			keyExpected.suffix.SetCb( slvownermapIterator.CbKey() );
			
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			Call( pttarraySLVChecksumLengths->ErrGetValue( ppib, pgno, &cbDataChecksummed, &lengthRun ) );

			INT iul;
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			keyOwning.suffix.SetCb( *pbKey );

			if( !slvavailIterator.FCommitted() )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s but is not marked as committed\r\n",
					pgno,
					objidOwning,
					columnid,
					rgbKeyOwner );
				fCorrupted = fTrue;
				}

			if( slvownermapIterator.FNull() )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s but has no space map entry\r\n",
					pgno,
					objidOwning,
					columnid,
					rgbKeyOwner );
				fCorrupted = fTrue;
				}
			else if( slvownermapIterator.Objid() != objidOwning
					|| slvownermapIterator.Columnid() != columnid
					|| 0 != CmpKeyShortest( keyOwning, keyExpected ) )
				{
				CHAR rgbKeyExpected[64];
				REPAIRDumpHex(
					rgbKeyExpected,
					sizeof( rgbKeyExpected ),
					(BYTE *)slvownermapIterator.PvKey(),
					slvownermapIterator.CbKey() );
					
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s (expected %d:%d:%s)\r\n",
					pgno,
					//  the record actually referencing the page is
					objidOwning,
					columnid,
					rgbKeyOwner,
					//  but the SLVOwnerMap says
					slvownermapIterator.Objid(),
					slvownermapIterator.Columnid(),
					rgbKeyExpected
				 );

				fCorrupted = fTrue;
				}			
			else if( slvownermapIterator.FChecksumIsValid()
					&& slvownermapIterator.CbDataChecksummed() != cbDataChecksummed )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
				
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d (owned by %d:%d:%s) has a checksum length mismatch (%d, expected %d)\r\n",
					pgno,
					//  the record actually referencing the page is
					objidOwning,
					columnid,
					rgbKeyOwner,
					slvownermapIterator.CbDataChecksummed(),
					cbDataChecksummed );
				fCorrupted = fTrue;
				}
			}
			
		++pgno;
		if( pgno > rgfmp[ifmp].PgnoSLVLast() )
			{
			break;
			}

		err = slvavailIterator.ErrMoveNext();
		if( JET_errNoCurrentRecord == err )
			{
			//  the size may not match, but we are O.K. as long as there are no pages used
			//	beyond this point

			err = JET_errSuccess;
			break;
			}
		Call( err );
			
		err = slvownermapIterator.ErrMoveNext();					
		if( JET_errNoCurrentRecord == err )
			{
			//  the size may not match, but we are O.K. as long as there are no pages used
			//	beyond this point

			err = JET_errSuccess;
			break;			
			}
		Call( err );			
		}

	while( pgno < rgfmp[ifmp].PgnoSLVLast() )
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil != objidOwning )
			{
			(*popts->pcprintfError)( 
				"streaming file corruption: page %d is used by objid %d but has no entry\r\n",
				pgno, objidOwning );
			fCorrupted = fTrue;
			}		

		++pgno;			
		}
	
HandleError:
	pttarraySLVAvail->EndRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->EndRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->EndRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->EndRun( ppib, &lengthRun );

	CallS( slvavailIterator.ErrTerm() );
	CallS( slvownermapIterator.ErrTerm() );

	*pfSLVSpaceTreesCorrupt = fCorrupted;

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStartCheckAllTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,	
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY * const pttarraySLVChecksumLengths,	
	INTEGGLOBALS * const pintegglobals,
	const REPAIROPTS * const popts )
//  ================================================================
//
//-
	{
	ERR		err				= JET_errSuccess;
	FUCB	*pfucbCatalog 	= pfucbNil;
	DATA	data;
	BOOL	fCorruptionSeen	= fFalse;	//  true if we have seen corruption in the catalog itself

	pintegglobals->fCorruptionSeen 				= fFalse;
	pintegglobals->err							= JET_errSuccess;
	pintegglobals->pprepairtable				= pprepairtable;
	pintegglobals->pttarrayOwnedSpace			= pttarrayOwnedSpace;
	pintegglobals->pttarrayAvailSpace			= pttarrayAvailSpace;
	pintegglobals->pttarraySLVAvail				= pttarraySLVAvail;
	pintegglobals->pttarraySLVOwnerMapColumnid	= pttarraySLVOwnerMapColumnid;
	pintegglobals->pttarraySLVOwnerMapKey 		= pttarraySLVOwnerMapKey;	
	pintegglobals->pttarraySLVChecksumLengths	= pttarraySLVChecksumLengths;
	pintegglobals->popts						= popts;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog ) );
	Assert( pfucbNil != pfucbCatalog );
	FUCBSetSequential( pfucbCatalog );
	FUCBSetPrereadForward( pfucbCatalog, cpgPrereadSequential );

	Call( ErrIsamSetCurrentIndex( ppib, pfucbCatalog, szMSORootObjectsIndex ) );

	//  check the large tables first for maximum overlap
	
	INT isz;
	for( isz = 0; isz < cszLargeTables && !fCorruptionSeen; ++isz )
		{
		const BYTE	bTrue		= 0xff;	
		Call( ErrIsamMakeKey(
					ppib,
					pfucbCatalog,
					&bTrue,
					sizeof(bTrue),
					JET_bitNewKey ) );
		Call( ErrIsamMakeKey(
					ppib,
					pfucbCatalog,
					(BYTE *)rgszLargeTables[isz],
					(ULONG)strlen( rgszLargeTables[isz] ),
					NO_GRBIT ) );
		err = ErrIsamSeek( ppib, pfucbCatalog, JET_bitSeekEQ );
		if ( JET_errRecordNotFound == err )
			{
			//  This large table not present in this database
			continue;
			}
		Call( err );
		Call( ErrDIRGet( pfucbCatalog ) );
		Call( ErrREPAIRPostTableTask(
				ppib,
				ifmp,
				pfucbCatalog,
				rgszLargeTables[isz],
				pprepairtable,
				pintegglobals,
				ptaskmgr,
				&fCorruptionSeen,
				popts ) );
		}


	err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveFirst, 0 );
	while ( err >= 0 )
		{
		Call( ErrDIRGet( pfucbCatalog ) );

		CHAR szTable[JET_cbNameMost+1];
		Assert( FVarFid( fidMSO_Name ) );
		Call( ErrRECIRetrieveVarColumn(
					pfcbNil,
					pfucbCatalog->u.pfcb->Ptdb(),
					fidMSO_Name,
					pfucbCatalog->kdfCurr.data,
					&data ) );
		CallS( err );
		if( data.Cb() > JET_cbNameMost  )
			{
			(*popts->pcprintfError)( "catalog corruption (MSO_Name): name too long: expected no more than %d bytes, got %d\r\n",
				JET_cbNameMost, data.Cb() );	
			fCorruptionSeen = fTrue;
			Call( ErrDIRRelease( pfucbCatalog ) );
			}
		else
			{
			UtilMemCpy( szTable, data.Pv(), data.Cb() );
			szTable[data.Cb()] = 0;

			//  if this is a catalog or a large table it will
			//  already have been checked
			
			if( !FCATSystemTable( szTable )
				&& !FIsLargeTable( szTable ) )
				{			
				Call( ErrREPAIRPostTableTask(
						ppib,
						ifmp,
						pfucbCatalog,
						szTable,
						pprepairtable,
						pintegglobals,
						ptaskmgr,
						&fCorruptionSeen,
						popts ) );
				}
			else
				{
				Call( ErrDIRRelease( pfucbCatalog ) );
				}
			}

		err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveNext, 0 );
		}
	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}


HandleError:
	if( pfucbNil != pfucbCatalog )
		{
		CallS( ErrFILECloseTable( ppib, pfucbCatalog ) );
		}
	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

	if( JET_errSuccess == err
		&& fCorruptionSeen )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStopCheckTables( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt )
//  ================================================================
	{
	*pfCorrupt = pintegglobals->fCorruptionSeen;
	return pintegglobals->err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRPostTableTask(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const CHAR * const szTable,
	REPAIRTABLE ** const pprepairtable,
	INTEGGLOBALS * const pintegglobals,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Takes a latched FUCB, returns an unlatched FUCB
//
//-
	{
	ERR err = JET_errSuccess;
	DATA data;
	
	PGNO pgnoFDP;
	CPG  cpgExtent;
		
	Assert( FFixedFid( fidMSO_PgnoFDP ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_PgnoFDP,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( PGNO ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_PgnoFDP): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( PGNO ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	pgnoFDP = *( (UnalignedLittleEndian< PGNO > *)data.Pv() );
	if( pgnoNull == pgnoFDP )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_PgnoFDP): pgnoFDP is NULL\r\n" );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}

	Assert( FFixedFid( fidMSO_Pages ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Pages,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( CPG ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (fidMSO_Pages): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( CPG ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	cpgExtent = max( 1, (INT) *( (UnalignedLittleEndian< CPG > *)data.Pv() ) );
	
	Assert( FFixedFid( fidMSO_Type ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Type,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( SYSOBJ ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Type): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( SYSOBJ ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	if( sysobjTable != *( (UnalignedLittleEndian< SYSOBJ > *)data.Pv() ) )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Type): unexpected type: expected type %d, got type %d\r\n",
			sysobjTable, *( (UnalignedLittleEndian< SYSOBJ > *)data.Pv() ) );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
			
#ifdef DEBUG
	Assert( FVarFid( fidMSO_Name ) );
	Call( ErrRECIRetrieveVarColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Name,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	Assert( 0 == _strnicmp( szTable, reinterpret_cast<CHAR *>( data.Pv() ), data.Cb() ) );
#endif	//	DEBUG

	OBJID objidTable;
	Assert( FFixedFid( fidMSO_Id ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Id,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( objidTable ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Id): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( objidTable ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}

	objidTable = *(UnalignedLittleEndian< OBJID > *)data.Pv();

	if( objidInvalid == objidTable )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Id): objidInvalid (%d) reached.\r\n", objidInvalid );	
		*pfCorrupted = fTrue;
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	Call( ErrDIRRelease( pfucbCatalog ) );

		{
		CHECKTABLE * const pchecktable 	= new CHECKTABLE;
		if( NULL == pchecktable )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
			
		pchecktable->ifmp 				= ifmp;
		strcpy(pchecktable->szTable, szTable );
		pchecktable->szIndex[0]			= 0;
		pchecktable->objidFDP 			= objidTable;
		pchecktable->pgnoFDP 			= pgnoFDP;
		pchecktable->objidParent		= objidSystemRoot;
		pchecktable->pgnoFDPParent		= pgnoSystemRoot;
		pchecktable->fPageFlags			= 0;
		pchecktable->fUnique			= fTrue;
		pchecktable->preccheck			= NULL;
		pchecktable->cpgPrimaryExtent	= cpgExtent;
		pchecktable->pglobals			= pintegglobals;
		pchecktable->fDeleteWhenDone	= fTrue;

		err = ptaskmgr->ErrPostTask( REPAIRCheckOneTableTask, (ULONG_PTR)pchecktable );
		if( err < 0 )
			{
			Assert( JET_errOutOfMemory == err );
			//  we were not able to post this
			delete pchecktable;
			Call( err );
			}
		}

	return err;

HandleError:
	Call( ErrDIRRelease( pfucbCatalog ) );
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRCheckTreeAndSpaceTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckTreeAndSpaceTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;
	
	ERR err = ErrREPAIRCheckTreeAndSpace(
						ppib,
						pchecktable->ifmp,
						pchecktable->objidFDP,
						pchecktable->pgnoFDP,
						pchecktable->objidParent,
						pchecktable->pgnoFDPParent,
						pchecktable->fPageFlags,
						pchecktable->fUnique,
						pchecktable->preccheck,
						pchecktable->pglobals->pttarrayOwnedSpace,
						pchecktable->pglobals->pttarrayAvailSpace,
						pchecktable->pglobals->popts );

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		//  we just need to set this, it will never be unset
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		}
	else if( err < 0 )
		{
		//  we'll just keep the last non-corrupting error
		pchecktable->pglobals->err = err;
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL VOID REPAIRCheckOneTableTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckOneTableTask;	// should only compile if the signatures match

	REPAIRTABLE * prepairtable = NULL;
	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;
	
	const ERR err = ErrREPAIRCheckOneTable(
						ppib,
						pchecktable->ifmp,
						pchecktable->szTable,
						pchecktable->objidFDP,
						pchecktable->pgnoFDP,
						pchecktable->cpgPrimaryExtent,
						&prepairtable,
						pchecktable->pglobals->pttarrayOwnedSpace,
						pchecktable->pglobals->pttarrayAvailSpace,
						pchecktable->pglobals->pttarraySLVAvail,
						pchecktable->pglobals->pttarraySLVOwnerMapColumnid,
						pchecktable->pglobals->pttarraySLVOwnerMapKey,
						pchecktable->pglobals->pttarraySLVChecksumLengths,
						pchecktable->pglobals->popts );

	if( JET_errDatabaseCorrupted == err )
		{
		//  we just need to set this, it will never be unset
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		}
	else if( err < 0 )
		{
		//  we'll just keep the last non-corrupting error
		pchecktable->pglobals->err = err;
		}

	if( NULL != prepairtable )
		{
		pchecktable->pglobals->crit.Enter();
		prepairtable->prepairtableNext = *(pchecktable->pglobals->pprepairtable);
		*(pchecktable->pglobals->pprepairtable) = prepairtable;
		pchecktable->pglobals->crit.Leave();
		}


	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL BOOL FREPAIRTableHasSLVColumn( const FCB * const pfcb )
//  ================================================================
	{
	return pfcb->Ptdb()->FTableHasSLVColumn();
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckOneTable(
	PIB * const ppib,
	const IFMP ifmp,
	const char * const szTable,
	const OBJID objidTable,
	const PGNO pgnoFDP,
	const CPG cpgPrimaryExtent,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,	
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY * const pttarraySLVChecksumLengths,	
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err;
	
	FUCB	*pfucbTable = pfucbNil;
	
	FCB		*pfcbTable	= pfcbNil;
	FCB		*pfcbIndex	= pfcbNil;

	PGNO 	pgnoLV 		= pgnoNull;
	OBJID	objidLV		= objidNil;

	BOOL		fRepairTable		= fTrue;
	BOOL		fRepairLV			= fTrue;
	BOOL		fRepairIndexes		= fTrue;
	BOOL		fRepairLVRefcounts	= fTrue;

	const ULONG timerStart = TickOSTimeCurrent();

	//  preread the first extent
	
	const CPG cpgToPreread = min( rgfmp[ifmp].PgnoLast() - pgnoFDP + 1,
								max( cpgPrimaryExtent, g_cpgMinRepairSequentialPreread ) );
	BFPrereadPageRange( ifmp, pgnoFDP, cpgToPreread );

	//  do not pass in the pgnoFDP (forces lookup of the objid)
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucbTable, szTable, NO_GRBIT );	

	FIDLASTINTDB fidLastInTDB;
	memset( &fidLastInTDB, 0, sizeof(fidLastInTDB) );
	if( err >= 0)
		{
		Assert( pfucbNil != pfucbTable );
		pfcbTable 	= pfucbTable->u.pfcb;	

		fidLastInTDB.fidFixedLastInTDB = pfcbTable->Ptdb()->FidFixedLast(); 
		fidLastInTDB.fidVarLastInTDB = pfcbTable->Ptdb()->FidVarLast();
		fidLastInTDB.fidTaggedLastInTDB = pfcbTable->Ptdb()->FidTaggedLast();
		}

	TTMAP ttmapLVRefcountsFromTable( ppib );
	TTMAP ttmapLVRefcountsFromLV( ppib );
	
	RECCHECKTABLE 	recchecktable(
						objidTable,
						pfucbTable,
						fidLastInTDB,
						&ttmapLVRefcountsFromTable,
						pttarraySLVAvail,
						pttarraySLVOwnerMapColumnid,	
						pttarraySLVOwnerMapKey,	
						pttarraySLVChecksumLengths,
						popts );

	Call( err );
	

	//  preread the indexes of the table
	
	REPAIRIPrereadIndexesOfFCB( pfcbTable );

	//  check for a LV tree
	
	Call( ErrCATAccessTableLV( ppib, ifmp, objidTable, &pgnoLV, &objidLV ) );
	if( pgnoNull != pgnoLV )
		{
		const CPG cpgToPrereadLV = min( rgfmp[ifmp].PgnoLast() - pgnoLV + 1,
								max( cpgLVTree, g_cpgMinRepairSequentialPreread ) );
		BFPrereadPageRange( ifmp, pgnoLV, cpgToPrereadLV );
		Call( ttmapLVRefcountsFromLV.ErrInit( PinstFromPpib( ppib ) ) );
		}

	Call( ttmapLVRefcountsFromTable.ErrInit( PinstFromPpib( ppib ) ) );

	(*popts->pcprintfVerbose)( "checking table \"%s\" (%d)\r\n", szTable, objidTable );
	(*popts->pcprintfVerbose).Indent();

	(*popts->pcprintfStats).Indent();

	Call( ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidTable,
			pgnoFDP,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );

	//  check the long-value tree if it exists
	//	CONSIDER:  multi-thread this by checking the LV tree and data trees in separate threads
	
	if( pgnoNull != pgnoLV )
		{
		LVSTATS lvstats;
		memset( &lvstats, 0, sizeof( lvstats ) );
		lvstats.cbLVMin = LONG_MAX;
		RECCHECKLV 		recchecklv( ttmapLVRefcountsFromLV, popts );
		RECCHECKLVSTATS	recchecklvstats( &lvstats );
		RECCHECKMACRO	reccheckmacro;
		reccheckmacro.Add( &recchecklvstats );
		reccheckmacro.Add( &recchecklv );	// put this last so that warnings are returned
		(*popts->pcprintfVerbose)( "checking long value tree (%d)\r\n", objidLV );
		(*popts->pcprintfStats)( "\r\n" );
		(*popts->pcprintfStats)( "===== long value tree =====\r\n" );
		Call( ErrREPAIRCheckTreeAndSpace(
				ppib,
				ifmp,
				objidLV,
				pgnoLV,
				objidTable,
				pgnoFDP,
				CPAGE::fPageLongValue,
				fTrue,
				&reccheckmacro, 
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		}
	else
		{
		fRepairLV = fFalse;
		}
		
	//  check the main table
	
	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "===== table \"%s\" =====\r\n", szTable );

	(*popts->pcprintfVerbose)( "checking data\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			objidTable,
			pgnoFDP,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );
	fRepairTable = fFalse;

	// Until now, we can safely say that long-Value tree  
	// don't need to be rebuilt 
	fRepairLV = fFalse;

	//  compare LV refcounts found in the table to LV refcounts found in the LV tree

	if( pgnoNull != pgnoLV )
		{
		(*popts->pcprintfVerbose)( "checking LV refcounts\r\n" );
		Call( ErrREPAIRCompareLVRefcounts( ppib, ifmp, ttmapLVRefcountsFromTable, ttmapLVRefcountsFromLV, popts ) ); 
		}
	else
		{
		// LV tree does not exist, LV refcounts found in the table should be 0
		err = ttmapLVRefcountsFromTable.ErrMoveFirst();
		if( JET_errNoCurrentRecord != err ) 
			{
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		else
			{ // expected result from a good table
			err = JET_errSuccess;
			}
		}

	fRepairLVRefcounts	= fFalse;

	//	check all secondary indexes
	
	for(
		pfcbIndex = pfucbTable->u.pfcb->PfcbNextIndex();
		pfcbNil != pfcbIndex;
		pfcbIndex = pfcbIndex->PfcbNextIndex() )
		{
		RECCHECKNULL 	recchecknull;
		const CHAR * const szIndexName 	= pfucbTable->u.pfcb->Ptdb()->SzIndexName( pfcbIndex->Pidb()->ItagIndexName(), pfcbIndex->FDerivedIndex() );
		const OBJID objidIndex = pfcbIndex->ObjidFDP();
		(*popts->pcprintfVerbose)( "checking index \"%s\" (%d)\r\n", szIndexName, objidIndex );
		(*popts->pcprintfStats)( "\r\n" );
		(*popts->pcprintfStats)( "===== index \"%s\" =====\r\n", szIndexName );
			
		Call( ErrREPAIRCheckTreeAndSpace(
				ppib,
				ifmp,
				pfcbIndex->ObjidFDP(),
				pfcbIndex->PgnoFDP(),
				objidTable,
				pgnoFDP,
				CPAGE::fPageIndex,
				pfcbIndex->Pidb()->FUnique(),
				&recchecknull,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontBuildIndexes ) )
		{
		if ( pfucbTable->u.pfcb->PfcbNextIndex() != pfcbNil )
			{
			(*popts->pcprintfVerbose)( "rebuilding and comparing indexes\r\n" );
			DIRUp( pfucbTable );
			Call( ErrFILEBuildAllIndexes(
					ppib,
					pfucbTable,
					pfucbTable->u.pfcb->PfcbNextIndex(),
					NULL,
					cFILEIndexBatchMax,
					fTrue,
					popts->pcprintfError ) );
			}
		}

	fRepairIndexes = fFalse;

HandleError:		
	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	ERR errSaved = err;

	if( JET_errDatabaseCorrupted == err )
		{
		if( fRepairTable )
			{
			(*popts->pcprintfVerbose)( "\tData table will be rebuilt\r\n" );
			}
		if( fRepairLV	)
			{
			(*popts->pcprintfVerbose)( "\tLong-Value table will be rebuilt\r\n" );
			}
		if( fRepairIndexes )
			{
			(*popts->pcprintfVerbose)( "\tIndexes will be rebuilt\r\n" );
			}
		if( fRepairLVRefcounts )
			{
			(*popts->pcprintfVerbose)( "\tLong-Value refcounts will be rebuilt\r\n" );
			}
		}

	if( JET_errDatabaseCorrupted == err && !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		//  store all the pgnos that we are interested in
		//  the error handling here is sloppy (running out of memory will cause a leak)
		
		Assert( pfcbNil != pfcbTable );
		
		(*popts->pcprintfVerbose)( "table \"%s\" is corrupted\r\n", szTable );
		VOID * pv = PvOSMemoryHeapAlloc( sizeof( REPAIRTABLE ) );
		if( NULL == pv )
			{
			err = ErrERRCheck( JET_errOutOfMemory );
			goto Abort;
			}
		memset( pv, 0, sizeof( REPAIRTABLE ) );
		
		REPAIRTABLE * prepairtable = new(pv) REPAIRTABLE;

		strcpy( prepairtable->szTableName, szTable );

		prepairtable->objidFDP 	= objidTable;
		prepairtable->pgnoFDP	= pfcbTable->PgnoFDP();
		(*popts->pcprintfVerbose)( "\tdata: %d (%d)\r\n", prepairtable->objidFDP, prepairtable->pgnoFDP );

		Assert( fRepairTable || fRepairLV || fRepairLVRefcounts || fRepairIndexes );
		prepairtable->fRepairTable			= fRepairTable;
		prepairtable->fRepairLV				= fRepairLV;
		prepairtable->fRepairIndexes		= fRepairIndexes;
		prepairtable->fRepairLVRefcounts	= fRepairLVRefcounts;

		if( pgnoNull != pgnoLV )
			{
			Assert( objidNil != objidLV );
			prepairtable->objidLV 	= objidLV;
			prepairtable->pgnoLV	= pgnoLV;
			(*popts->pcprintfVerbose)( "\tlong values: %d (%d)\r\n", prepairtable->objidLV, prepairtable->pgnoLV );
			}
			
		for( pfcbIndex = pfucbTable->u.pfcb;
			pfcbNil != pfcbIndex;
			pfcbIndex = pfcbIndex->PfcbNextIndex() )
			{
			if( !pfcbIndex->FPrimaryIndex() )
				{
				const CHAR * const szIndexName 	= pfucbTable->u.pfcb->Ptdb()->SzIndexName( pfcbIndex->Pidb()->ItagIndexName(), pfcbIndex->FDerivedIndex() );
				CallJ( prepairtable->objidlistIndexes.ErrAddObjid( pfcbIndex->ObjidFDP() ), Abort );
				(*popts->pcprintfVerbose)( "\tindex \"%s\": %d (%d)\r\n", szIndexName, pfcbIndex->ObjidFDP(),pfcbIndex->PgnoFDP() );
				}
			else if( pfcbIndex->Pidb() != pidbNil )
				{
				prepairtable->fHasPrimaryIndex = fTrue;
				(*popts->pcprintfVerbose)( "\tprimary index\r\n" );
				}			
			}

		prepairtable->fTableHasSLV 		= FREPAIRTableHasSLVColumn( pfcbTable );

		if( prepairtable->fTableHasSLV )
			{
			(*popts->pcprintfVerbose)( "\tSLV column\r\n" );			
			}
			
		prepairtable->fTemplateTable 	= pfcbTable->FTemplateTable();
		prepairtable->fDerivedTable 	= pfcbTable->FDerivedTable();
		
		prepairtable->objidlistIndexes.Sort();
		prepairtable->prepairtableNext = *pprepairtable;
		*pprepairtable = prepairtable;
		}

Abort:
	if ( pfucbNil != pfucbTable )
		{
		FCB * const pfcbTable = pfucbTable->u.pfcb;
		
		CallS( ErrFILECloseTable( ppib, pfucbTable ) );

		//  BUGFIX: purge the FCB to avoid confusion with other tables/indexes with have
		//  the same pgnoFDP (corrupted catalog)

		//	BUGFIX: callbacks can open this table so we can no longer guarantee that 
		//	the FCB is not referenced.
		//	UNDONE: get a better way to avoid duplicate pgnoFDP problems
		
///		if( pfcbTable && !pfcbTable->FTemplateTable() )
///			{
///			pfcbTable->PrepareForPurge();
///			pfcbTable->Purge();
///			}
		}

	(*popts->pcprintfVerbose).Unindent();
	(*popts->pcprintfStats).Unindent();

	const ULONG timerEnd 	= TickOSTimeCurrent();
	const ULONG csecElapsed = ( ( timerEnd - timerStart ) / 1000 );

	err = ( JET_errSuccess == err ) ? errSaved : err;

	(*popts->pcprintfVerbose)( "integrity check of table \"%s\" (%d) finishes with error %d after %d seconds\r\n",
		szTable,
		objidTable,
		err,
		csecElapsed );

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCompareLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	TTMAP& ttmapLVRefcountsFromTable,
	TTMAP& ttmapLVRefcountsFromLV,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	BOOL fNoMoreRefcountsFromTable 	= fFalse;
	BOOL fNoMoreRefcountsFromLV 	= fFalse;
	BOOL fSawError					= fFalse;

	err = ttmapLVRefcountsFromTable.ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		fNoMoreRefcountsFromTable = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

	err = ttmapLVRefcountsFromLV.ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		fNoMoreRefcountsFromLV = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

	//  repeat while we have at least one set of refcounts
	
	while( 	!fNoMoreRefcountsFromTable
			|| !fNoMoreRefcountsFromLV )
		{
		ULONG lidFromTable;
		ULONG ulRefcountFromTable;
		if( !fNoMoreRefcountsFromTable )
			{
			Call( ttmapLVRefcountsFromTable.ErrGetCurrentKeyValue( &lidFromTable, &ulRefcountFromTable ) );
			}
		else
			{
			lidFromTable 		= 0xffffffff;
			ulRefcountFromTable = 0;
			}

		ULONG lidFromLV;
		ULONG ulRefcountFromLV;
		if( !fNoMoreRefcountsFromLV )
			{
			Call( ttmapLVRefcountsFromLV.ErrGetCurrentKeyValue( &lidFromLV, &ulRefcountFromLV ) );
			}
		else
			{
			lidFromLV 			= 0xffffffff;
			ulRefcountFromLV	= 0;
			}

		if( lidFromTable > lidFromLV )
			{
			
			//  we see a LID in the LV-tree that is not referenced in the table
			//  its an orphaned LV. WARNING

			(*popts->pcprintfWarning)( "orphaned LV (lid %d, refcount %d)\r\n", lidFromLV, ulRefcountFromLV );
			
			}
		else if( lidFromLV > lidFromTable )
			{
			
			//  we see a LID in the table that doesn't exist in the LV tree. ERROR

			(*popts->pcprintfError)( "record references non-existant LV (lid %d, refcount %d)\r\n",
									lidFromTable, ulRefcountFromTable );
			fSawError = fTrue;
			}
		else if( ulRefcountFromTable > ulRefcountFromLV )
			{
			Assert( lidFromTable == lidFromLV );
			
			//  the refcount in the LV tree is too low. ERROR

			(*popts->pcprintfError)( "LV refcount too low (lid %d, refcount %d, correct refcount %d)\r\n",
										lidFromLV, ulRefcountFromLV, ulRefcountFromTable );

			fSawError = fTrue;
			}
		else if( ulRefcountFromLV > ulRefcountFromTable )
			{
			Assert( lidFromTable == lidFromLV );

			//  the refcount in the LV tree is too high. WARNING

			(*popts->pcprintfWarning)( "LV refcount too high (lid %d, refcount %d, correct refcount %d)\r\n",
										lidFromLV, ulRefcountFromLV, ulRefcountFromTable );
			
			}
		else
			{

			//  perfect match. no error
			
			Assert( lidFromTable == lidFromLV );
			Assert( ulRefcountFromTable == ulRefcountFromLV );
			Assert( 0xffffffff != lidFromLV );
			}

		if( lidFromTable <= lidFromLV )
			{
			err = ttmapLVRefcountsFromTable.ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				fNoMoreRefcountsFromTable = fTrue;
				err = JET_errSuccess;
				}
			Call( err );
			}
		if( lidFromLV <= lidFromTable )
			{
			err = ttmapLVRefcountsFromLV.ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				fNoMoreRefcountsFromLV = fTrue;
				err = JET_errSuccess;
				}
			Call( err );
			
			}
		}

HandleError:		
	if( JET_errSuccess == err && fSawError )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateTempTables(
	PIB * const ppib,
	const BOOL fRepairGlobalSpace,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err = JET_errSuccess;
		
	JET_COLUMNDEF	rgcolumndef[3] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey }
		};	
	
	//  BadPages
	rgcolumndef[0].coltyp = JET_coltypLong;	// Pgno	
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		1,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&prepairtt->tableidBadPages,
		prepairtt->rgcolumnidBadPages ) );

	//  Owned
	rgcolumndef[0].coltyp = JET_coltypLong;	// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLong;	// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		2,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidOwned,
		prepairtt->rgcolumnidOwned ) );

	//  Used
	rgcolumndef[0].coltyp = JET_coltypLong;			// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLongBinary;	// Key
	rgcolumndef[2].grbit  = NO_GRBIT;				// allows us to catch duplicates (same objid/key)
	rgcolumndef[2].coltyp = JET_coltypLong;			// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		3,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidUsed,
		prepairtt->rgcolumnidUsed ) );
	rgcolumndef[2].grbit  = rgcolumndef[0].grbit;

	//  Available
	rgcolumndef[0].coltyp = JET_coltypLong;	// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLong;	// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		2,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidAvailable,
		prepairtt->rgcolumnidAvailable ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRScanDB(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	DBTIME * const pdbtimeLast,
	OBJID  * const pobjidLast,
	PGNO   * const ppgnoLastOESeen,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	ERR err = JET_errSuccess;

	const CPG cpgPreread = 256;
	CPG cpgRemaining;

	CPG	cpgUninitialized 	= 0;
	CPG	cpgBad 				= 0;
	
	const PGNO	pgnoFirst 	= 1;
	const PGNO	pgnoLast	= PgnoLast( ifmp );
	PGNO	pgno			= pgnoFirst;

	(*popts->pcprintfVerbose)( "scanning the database from page %d to page %d\r\n", pgnoFirst, pgnoLast );		

	popts->psnprog->cunitTotal = pgnoLast;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	
	
	BFPrereadPageRange( ifmp, pgnoFirst, min(cpgPreread * 2,pgnoLast-1) );
	cpgRemaining = cpgPreread;

	while( pgnoLast	>= pgno )
		{
		if( 0 == --cpgRemaining )
			{
			if( ( pgno + ( cpgPreread * 2 ) ) < pgnoLast )
				{
				BFPrereadPageRange( ifmp, pgno + cpgPreread, cpgPreread );
				}
			popts->psnprog->cunitDone = pgno;
			(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			cpgRemaining = cpgPreread;
			}
			
		CSR	csr;
		err = csr.ErrGetReadPage( 
						ppib, 
						ifmp,
						pgno,
						bflfNoTouch );
			
		if( JET_errPageNotInitialized == err )
			{
			//  unused page. ignore it
			++cpgUninitialized;
			err = JET_errSuccess;
			}
		else if( JET_errReadVerifyFailure == err || JET_errDiskIO == err )
			{
			++cpgBad;
			err = CPAGE::ErrResetHeader( ppib, ifmp, pgno );
			if( err < 0 )
				{
				(*popts->pcprintfVerbose)( "error %d resetting page %d (online backup may not work)\r\n", err, pgno );		
				}
			else
				{
				(*popts->pcprintfVerbose)( "error %d reading page %d. the page has been zeroed out so online backup will work\r\n", err, pgno );						
				}
			err = ErrREPAIRInsertBadPageIntoTables( ppib, pgno, prepairtt, prepairtable, popts );
			}
		else if( err >= 0 )
			{
			*ppgnoLastOESeen = max( pgno, *ppgnoLastOESeen );
			
			if( csr.Cpage().Dbtime() > *pdbtimeLast )
				{
				*pdbtimeLast = csr.Cpage().Dbtime();
				}
			if( csr.Cpage().ObjidFDP() > *pobjidLast )
				{
				*pobjidLast = csr.Cpage().ObjidFDP();
				}

			err = ErrREPAIRInsertPageIntoTables(
					ppib,
					ifmp,
					csr,
					prepairtt,
					prepairtable,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,					
					popts );
			}
		csr.ReleasePage( fTrue );
		csr.Reset();
		Call( err );
		++pgno;
		}

	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRIFixLeafPage(
	PIB * const	ppib,
	const IFMP ifmp,
	CSR& csr,
#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved,
#endif  //  SYNC_DEADLOCK_DETECTION
	const REPAIROPTS * const popts)
//  ================================================================
//
//  Called on leaf pages that are part of tables being repaired
//
//  Currently this just checks leaf pages to make sure that they are
//  usable.
//
//  UNDONE: write-latch the page and fix it
//
//-
	{
	ERR err = JET_errSuccess;
	KEYDATAFLAGS 	rgkdf[2];			
	
	Call( csr.Cpage().ErrCheckPage( popts->pcprintfError ) );

Restart:

	//  check to see if the nodes are in key order		
	
	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		const INT ikdfCurr = iline % 2;
		const INT ikdfPrev = ( iline + 1 ) % 2;

		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), rgkdf + ikdfCurr );
		Call( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				rgkdf[ikdfCurr],
				popts ) );

		if( !csr.Cpage().FLongValuePage()
			&& !csr.Cpage().FIndexPage()
			&& !csr.Cpage().FSLVOwnerMapPage()
			&& !csr.Cpage().FSLVAvailPage() )
			{
			err = ErrREPAIRICheckRecord(
					csr.Pgno(),
					csr.ILine(),
					(BYTE *)csr.Cpage().PvBuffer(),
					rgkdf[ikdfCurr],
					popts );
			if( JET_errDatabaseCorrupted == err )
				{
				
				//	delete this record
				
#ifdef SYNC_DEADLOCK_DETECTION
				if( pownerSaved )
					{
					Pcls()->pownerLockHead = pownerSaved;
					}
#endif  //  SYNC_DEADLOCK_DETECTION

				// the page should have been read/write latched
				Assert( FBFReadLatched( ifmp, csr.Pgno() )
						|| FBFWriteLatched( ifmp, csr.Pgno() ) );

				if( !FBFWriteLatched( ifmp, csr.Pgno() ) )
					{
					err = csr.ErrUpgrade( );
					//Only one thread should latch the page
					Assert( errBFLatchConflict != err );
					Call( err );
					csr.Dirty();
					}

				csr.Cpage().Delete( csr.ILine() );

				goto Restart;
				}
			}
			
		if( rgkdf[ikdfCurr].key.prefix.Cb() == 1 )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		if( iline > 0 )
			{
			//  this routine should only be called on the clustered index or LV tree
			//  just compare the keys
			
			const INT cmp = CmpKey( rgkdf[ikdfPrev].key, rgkdf[ikdfCurr].key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on leaf page\r\n", csr.Pgno(), csr.ILine() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on leaf page\r\n", csr.Pgno(), csr.ILine() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:

#ifdef SYNC_DEADLOCK_DETECTION
	if( pownerSaved )
		{
		Pcls()->pownerLockHead = NULL;
		}
#endif  //  SYNC_DEADLOCK_DETECTION

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertOwned(
	const JET_SESID sesid,
	const OBJID objidOwning,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

#ifdef REPAIR_DEBUG_VERBOSE_SPACE
	(*popts->pcprintfDebug)( "Inserting page %d (objidOwning: %d) into Owned table\r\n", pgno, objidOwning );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

	Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidOwned, JET_prepInsert ) );
	Call( ErrDispSetColumn(		//  ObjidFDP
		sesid, 
		prepairtt->tableidOwned, 
		prepairtt->rgcolumnidOwned[0],
		(BYTE *)&objidOwning, 
		sizeof( objidOwning ),
		0, 
		NULL ) );
	Call( ErrDispSetColumn(		//  Pgno
		sesid, 
		prepairtt->tableidOwned, 
		prepairtt->rgcolumnidOwned[1],
		(BYTE *)&pgno, 
		sizeof( pgno ),
		0, 
		NULL ) );
	Call( ErrDispUpdate( sesid, prepairtt->tableidOwned, NULL, 0, NULL, 0 ) );
	++(prepairtt->crecordsOwned);

HandleError:
	Assert( err != JET_errKeyDuplicate );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertPageIntoTables(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err			= JET_errSuccess;
	const PGNO	pgno		= csr.Cpage().Pgno();
	const OBJID	objidFDP	= csr.Cpage().ObjidFDP();
	
	const REPAIRTABLE * prepairtableT = NULL;

	BOOL fOwnedPage		= fFalse;
	BOOL fAvailPage 	= fFalse;
	BOOL fUsedPage		= fFalse;
	OBJID objidInsert		= objidNil;
	OBJID objidInsertParent	= objidNil;

	BOOL	fInTransaction 	= fFalse;

	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
								
		if( csr.Cpage().FLeafPage()
			&& !csr.Cpage().FSpaceTree()
			&& !csr.Cpage().FEmptyPage()
			&& !csr.Cpage().FRepairedPage()
			&& csr.Cpage().Clines() > 0
			&& objidFDP == prepairtableT->objidFDP
			&& csr.Cpage().FPrimaryPage()
			)
			{
			//  leaf page of main tree.  re-used			
			fOwnedPage 		= fTrue;
			fAvailPage		= fFalse;
			fUsedPage		= fTrue;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( csr.Cpage().FLeafPage()
			&& !csr.Cpage().FSpaceTree()
			&& !csr.Cpage().FEmptyPage()
			&& !csr.Cpage().FRepairedPage()
			&& csr.Cpage().Clines() > 0
			&& objidFDP == prepairtableT->objidLV
			&& csr.Cpage().FLongValuePage()
			)
			{
			//  leaf page of LV tree.  re-used			
			fOwnedPage 		= fTrue;
			fAvailPage		= fFalse;
			fUsedPage		= fTrue;
			
			objidInsert			= objidFDP;
			objidInsertParent	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( prepairtableT->objidlistIndexes.FObjidPresent( objidFDP )
			&& csr.Cpage().FIndexPage())
			{
			//  a secondary index page (non-FDP). discard as we will rebuild the indexes			
			fOwnedPage 		= fTrue;
			fAvailPage		= fTrue;
			fUsedPage		= fFalse;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( objidFDP == prepairtableT->objidFDP
				 || objidFDP == prepairtableT->objidLV )
			{
			//  a internal/space page from the main or LV tree. discard as we rebuild internal pages and the
			//  space tree
			Assert( !csr.Cpage().FLeafPage()
					|| csr.Cpage().FSpaceTree()
					|| csr.Cpage().FEmptyPage()
					|| csr.Cpage().FRepairedPage()
					|| csr.Cpage().Clines() == 0
					);
					
			fOwnedPage 		= fTrue;
			fAvailPage		= fTrue;
			fUsedPage		= fFalse;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		prepairtableT = prepairtableT->prepairtableNext;
		}

	//  Optimization: this is not a page we are interested in
	if( NULL == prepairtableT )
		{
		Assert( !fOwnedPage );
		Assert( !fAvailPage );
		Assert( !fUsedPage );
		return JET_errSuccess;
		}

	Assert( !fUsedPage || fOwnedPage );
	Assert( !fAvailPage || fOwnedPage );
	Assert( !( fUsedPage && fAvailPage ) );
	
#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved = Pcls()->pownerLockHead;
	Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION

	OBJID objidAvail;
	Call( pttarrayAvailSpace->ErrGetValue( ppib, pgno, &objidAvail ) );
	// getting either objidNil or objidInsert is OK
	if( objidNil != objidAvail 
		&& objidInsert != objidAvail
		&& objidInsertParent != objidAvail
		&& objidSystemRoot != objidAvail )
		{
		(*popts->pcprintfDebug)(
			"page %d (objidFDP: %d, objidInsert: %d) is available to objid %d. ignoring\r\n",
			pgno, objidFDP, objidInsert, objidAvail );			
			
		fOwnedPage	= fFalse;
		fAvailPage	= fFalse;
		fUsedPage	= fFalse;
		err = JET_errSuccess;
		goto HandleError;
		}

	OBJID objidOwning;
	Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning ) );
	if( objidInsert != objidOwning
			&& objidFDP != objidOwning )
		{
		(*popts->pcprintfDebug)(
			"page %d (objidFDP: %d, objidInsert: %d) is owned by objid %d. ignoring\r\n",
			pgno, objidFDP, objidInsert, objidOwning );
			
		fOwnedPage	= fFalse;
		fAvailPage	= fFalse;
		fUsedPage	= fFalse;
		err = JET_errSuccess;
		goto HandleError;
		}

	Call( ErrIsamBeginTransaction( sesid, NO_GRBIT ) );
	fInTransaction = fTrue;

	if( fOwnedPage )
		{
		Assert( objidNil != objidInsert );
		Assert( objidInsert != objidInsertParent );
		Assert( fAvailPage || fUsedPage );

		Call( ErrREPAIRInsertOwned( sesid, objidInsert, pgno, prepairtt, popts ) );
		if( objidNil != objidInsertParent )
			{
			Call( ErrREPAIRInsertOwned( sesid, objidInsertParent, pgno, prepairtt, popts ) );
			}
		}
	else
		{
		Assert( !fAvailPage );
		Assert( !fUsedPage );
		}

	Assert( !fAvailPage || objidNil == objidInsertParent );

	if( fAvailPage )
		{
		Assert( !fUsedPage );
		
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)(
			"Inserting page %d (objidFDP: %d, objidInsert: %d) into Available table\r\n",
			pgno, objidFDP, objidInsert );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Assert( JET_tableidNil != prepairtt->tableidAvailable );

		Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidAvailable, JET_prepInsert ) );
		Call( ErrDispSetColumn(		//  ObjidInsert
			sesid, 
			prepairtt->tableidAvailable, 
			prepairtt->rgcolumnidAvailable[0],
			(BYTE *)&objidInsert, 
			sizeof( objidInsert ),
			0, 
			NULL ) );
		Call( ErrDispSetColumn(		//  Pgno
			sesid, 
			prepairtt->tableidAvailable, 
			prepairtt->rgcolumnidAvailable[1],
			(BYTE *)&pgno, 
			sizeof( pgno ),
			0, 
			NULL ) );
		Call( ErrDispUpdate( sesid, prepairtt->tableidAvailable, NULL, 0, NULL, 0 ) );
		++(prepairtt->crecordsAvailable);
		}
		
	else if( fUsedPage )
		{
		Assert( csr.Cpage().FLeafPage() );
		Assert( csr.Cpage().Clines() > 0 );
		Assert( objidInsert == objidFDP );

		err = ErrREPAIRIFixLeafPage(
				ppib,
				ifmp,
				csr,
			#ifdef SYNC_DEADLOCK_DETECTION
				pownerSaved,
			#endif  //  SYNC_DEADLOCK_DETECTION
				popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "page %d: err %d. discarding page\r\n", pgno, err );

			UtilReportEvent(
					eventWarning,
					REPAIR_CATEGORY,
					REPAIR_BAD_PAGE_ID,
					0, NULL );

			//  this page is not usable. skip it
			
			err = JET_errSuccess;
			goto HandleError;
			}
		else if( 0 == csr.Cpage().Clines() )
			{
			(*popts->pcprintfError)( "page %d: all records were bad. discarding page\r\n", pgno );

			UtilReportEvent(
					eventWarning,
					REPAIR_CATEGORY,
					REPAIR_BAD_PAGE_ID,
					0, NULL );

			//  this page is now empty. skip it
			
			err = JET_errSuccess;
			goto HandleError;
			}
		
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "Inserting page %d (objidFDP: %d) into Used table\r\n", pgno, objidFDP );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE
		Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidUsed, JET_prepInsert ) );
		Call( ErrDispSetColumn(		//  ObjidFDP
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[0],
			(BYTE *)&objidFDP, 
			sizeof( objidFDP ),
			0, 
			NULL ) );

		//  extract the key of the last node on the page
		BYTE rgbKey[JET_cbKeyMost+1];
		csr.SetILine( csr.Cpage().Clines() - 1 );
		KEYDATAFLAGS kdf;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );
		kdf.key.CopyIntoBuffer( rgbKey, sizeof( rgbKey ) );
		
		Call( ErrDispSetColumn(		//  Key
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[1],
			rgbKey, 
			kdf.key.Cb(),
			0, 
			NULL ) );
		Call( ErrDispSetColumn(		//  Pgno
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[2],
			(BYTE *)&pgno, 
			sizeof( pgno ),
			0, 
			NULL ) );
		err = ErrDispUpdate( sesid, prepairtt->tableidUsed, NULL, 0, NULL, 0 );
		if( JET_errKeyDuplicate == err )
			{
			UtilReportEvent(
				eventWarning,
				REPAIR_CATEGORY,
				REPAIR_BAD_PAGE_ID,
				0, NULL );
			(*popts->pcprintfError)( "page %d: duplicate keys. discarding page\r\n", pgno );
			err = ErrDispPrepareUpdate( sesid, prepairtt->tableidUsed, JET_prepCancel );
			}
		Call( err );
		++(prepairtt->crecordsUsed);
		}

	Call( ErrIsamCommitTransaction( sesid, 0 ) );
	fInTransaction = fFalse;	

HandleError:
	if( fInTransaction )
		{
		CallS( ErrIsamRollback( sesid, 0 ) );
		}

#ifdef SYNC_DEADLOCK_DETECTION
	Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertBadPageIntoTables(
	PIB * const ppib,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	BOOL	fInTransaction 	= fFalse;

	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	
	(*popts->pcprintfDebug)( "Inserting page %d into BadPages table\r\n", pgno );

	UtilReportEvent(
			eventWarning,
			REPAIR_CATEGORY,
			REPAIR_BAD_PAGE_ID,
			0, NULL );

#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved = Pcls()->pownerLockHead;
	Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION
		
	Call( ErrIsamBeginTransaction( sesid, NO_GRBIT ) );
	fInTransaction = fTrue;
		
	Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidBadPages, JET_prepInsert ) );
	Call( ErrDispSetColumn(		//  pgno
		sesid, 
		prepairtt->tableidBadPages, 
		prepairtt->rgcolumnidBadPages[0],
		(BYTE *)&pgno, 
		sizeof( pgno ),
		0, 
		NULL ) );
	Call( ErrDispUpdate( sesid, prepairtt->tableidBadPages, NULL, 0, NULL, 0 ) );
	++(prepairtt->crecordsBadPages);
		
	Call( ErrIsamCommitTransaction( sesid, 0 ) );
	fInTransaction = fFalse;

HandleError:
	if( fInTransaction )
		{
		CallS( ErrIsamRollback( sesid, 0 ) );
		}

#ifdef SYNC_DEADLOCK_DETECTION
	Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION

	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRGetPgnoOEAE( 
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	PGNO * const ppgnoOE,
	PGNO * const ppgnoAE,
	PGNO * const ppgnoParent,
	const BOOL fUnique,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err = JET_errSuccess;

	*ppgnoOE 		= pgnoNull;
	*ppgnoAE 		= pgnoNull;
	*ppgnoParent	= pgnoNull;
	
	CSR	csr;
	CallR( csr.ErrGetReadPage(
			ppib,
			ifmp,
			pgnoFDP,
			bflfNoTouch ) );

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	if( sizeof( SPACE_HEADER ) != line.cb )
		{
		(*popts->pcprintfError)( "page %d: external header is unexpected size. got %d bytes, expected %d\r\n",
								 pgnoFDP, line.cb, sizeof( SPACE_HEADER ) );		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	const SPACE_HEADER * psph;
	psph = reinterpret_cast <SPACE_HEADER *> ( line.pv );

	if( fUnique != psph->FUnique() )
		{
		(*popts->pcprintfError)( "page %d: external header has wrong unique flag. got %s, expected %s\r\n",
									pgnoFDP,
									psph->FUnique() ? "UNIQUE" : "NON-UNIQUE",
									fUnique ? "UNIQUE" : "NON-UNIQUE"
									);		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	*ppgnoParent = psph->PgnoParent();
	
	if ( psph->FSingleExtent() )
		{
		*ppgnoOE = pgnoNull;
		*ppgnoAE = pgnoNull;
		}
	else
		{
		*ppgnoOE = psph->PgnoOE();
		*ppgnoAE = psph->PgnoAE();
		if( pgnoNull == *ppgnoOE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE is pgnoNull", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoNull == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoAE is pgnoNull", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( *ppgnoOE == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE and pgnoAE are the same (%d)", pgnoFDP, *ppgnoOE );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoFDP == *ppgnoOE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE and pgnoFDP are the same", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoFDP == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoAE and pgnoFDP are the same", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}			
		}

HandleError:
	csr.ReleasePage();
	csr.Reset();
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSplitBuf( 
	PIB * const ppib,
	const PGNO pgnoLastBuffer, 
	const CPG cpgBuffer,	
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;
	PGNO 	pgnoT;

	Assert( cpgBuffer >= 0 );
	Assert( pgnoLastBuffer <= pgnoSysMax );
	Assert( pgnoLastBuffer >= cpgBuffer );

	//  these TTARRAYs are always accessed in OwnedSpace, AvailSpace order
	TTARRAY::RUN runOwned;
	TTARRAY::RUN runAvail;
	
	pttarrayOwnedSpace->BeginRun( ppib, &runOwned );
	pttarrayAvailSpace->BeginRun( ppib, &runAvail );

	
	for ( pgnoT = pgnoLastBuffer - cpgBuffer + 1; pgnoT <= pgnoLastBuffer; pgnoT++ )
		{
		OBJID objid;
			
		if( pttarrayOwnedSpace )
			{
			Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgnoT, &objid, &runOwned ) );
			
			if( objidParent != objid && objidCurrent != objid )
				{
				(*popts->pcprintfError)( "space allocation error (OE): page %d is already owned by objid %d ( expected parent objid %d or objid %d)\r\n",
												pgnoT, objid, objidParent, objidCurrent );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			if( objidCurrent != objid )
				{
				Assert( objidParent == objid );
				Call( pttarrayOwnedSpace->ErrSetValue( ppib, pgnoT, objidCurrent, &runOwned ) );
				}
				
			}
		
		if( pttarrayAvailSpace )
			{
			Call( pttarrayAvailSpace->ErrGetValue( ppib, pgnoT, &objid, &runAvail ) );
			
			if( objidNil != objid )
				{
				(*popts->pcprintfError)( "space allocation error (AE): page %d is available to objid %d (expected 0)\r\n",
										pgnoT, objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}

			Call( pttarrayAvailSpace->ErrSetValue( ppib, pgnoT, objidCurrent, &runAvail ) );
			}
		}
HandleError:
	pttarrayOwnedSpace->EndRun( ppib, &runOwned );
	pttarrayAvailSpace->EndRun( ppib, &runAvail );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSPLITBUFFERInSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;
	
	Assert( pgnoNull != pgnoFDP );

	SPLIT_BUFFER 	spb;

	memset( &spb, 0, sizeof( SPLIT_BUFFER ) );


	CSR	csr;
	err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgnoFDP,
					bflfNoTouch );
	if( err < 0 )
		{
		(*popts->pcprintfError)( "page %d: error %d on read\r\n", pgnoFDP, err );
		Call( err );
		}
		
	// the spacetree and rootpage flags should be checked before this function
	Assert( csr.Cpage().FSpaceTree() );
	Assert( csr.Cpage().FRootPage() );
		
	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	if( 0 == line.cb )
		{
		// no SPLIT_BUFFER
		Call( JET_errSuccess );
		}
	else if(sizeof( SPLIT_BUFFER ) == line.cb)
		{
		UtilMemCpy( &spb, line.pv, sizeof( SPLIT_BUFFER ) );
		}
	else
		{
		(*popts->pcprintfError)( "page %d: split buffer is unexpected size. got %d bytes, expected %d\r\n",
								 pgnoFDP, line.cb, sizeof( SPLIT_BUFFER ) );		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( 0 != spb.CpgBuffer1() )
		{
		Call( ErrREPAIRCheckSplitBuf( 
					ppib,
					spb.PgnoLastBuffer1(), 
					spb.CpgBuffer1(),	
					objidCurrent,
					objidParent,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					popts ) );
		}
		
	if( 0 != spb.CpgBuffer2() )
		{
		Call( ErrREPAIRCheckSplitBuf( 
					ppib,
					spb.PgnoLastBuffer2(), 
					spb.CpgBuffer2(),	
					objidCurrent,
					objidParent,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					popts ) );
		}		
		
HandleError:
	csr.ReleasePage();
	csr.Reset();

	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRCheckSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	BTSTATS btstats;

	//  don't preread the root of the tree, it should have been preread already 
	
	PGNO pgnoOE;
	PGNO pgnoAE;
	PGNO pgnoParentActual;
	Call( ErrREPAIRGetPgnoOEAE(
			ppib,
			ifmp,
			pgnoFDP,
			&pgnoOE,
			&pgnoAE,
			&pgnoParentActual,
			fUnique,
			popts ) );

	if( pgnoNull != pgnoOE )
		{
		//  preread the roots of the space trees
		PGNO	rgpgno[3];
		rgpgno[0] = pgnoOE;
		rgpgno[1] = pgnoAE;
		rgpgno[2] = pgnoNull;
		BFPrereadPageList( ifmp, rgpgno );
		}

	if( pgnoFDPParent != pgnoParentActual )
		{
		(*popts->pcprintfError)( "page %d (objid %d): space corruption. pgno parent is %d, expected %d\r\n",
			pgnoFDP, objid, pgnoParentActual, pgnoFDPParent );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
				
	if( pgnoNull != pgnoOE )
		{
		RECCHECKSPACEOE reccheckOE( ppib, pttarrayOwnedSpace, objid, objidParent, popts );
		RECCHECKSPACEAE reccheckAE( ppib, pttarrayOwnedSpace, pttarrayAvailSpace, objid, objidParent, popts );

		memset( &btstats, 0, sizeof( BTSTATS ) );
		Call( ErrREPAIRCheckTree(
				ppib,
				ifmp,
				pgnoOE,
				objid,
				fPageFlags | CPAGE::fPageSpaceTree,
				&reccheckOE,
				NULL,
				pttarrayAvailSpace,	//  at least make sure we aren't available to anyone else
				fFalse,
				&btstats,
				popts ) );

		memset( &btstats, 0, sizeof( BTSTATS ) );
		Call( ErrREPAIRCheckTree(
				ppib,
				ifmp,
				pgnoAE,
				objid,
				fPageFlags | CPAGE::fPageSpaceTree,
				&reccheckAE,
				pttarrayOwnedSpace,	//  we now know which pages we own
				pttarrayAvailSpace,	//  at least make sure we aren't available to anyone else
				fFalse,
				&btstats,
				popts ) );

		// check SPLIT_BUFFER 
		Call( ErrREPAIRCheckSPLITBUFFERInSpaceTree(
				ppib,
				ifmp,
				pgnoOE,
				objid, 
				objidParent, 
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		Call( ErrREPAIRCheckSPLITBUFFERInSpaceTree(
				ppib,
				ifmp,
				pgnoAE,
				objid, 
				objidParent,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );

		}
	else
		{
		Call( ErrREPAIRInsertSEInfo(
				ppib,
				ifmp,
				pgnoFDP,
				objid,
				objidParent,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	BTSTATS btstats;

	memset( &btstats, 0, sizeof( BTSTATS ) );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoFDP,
			objid,
			fPageFlags,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			!fUnique,
			&btstats,
			popts ) );

	if( popts->grbit & JET_bitDBUtilOptionStats )
		{
		REPAIRDumpStats( ppib, ifmp, pgnoFDP, &btstats, popts );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTreeAndSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Call( ErrREPAIRCheckSpace( 
			ppib,
			ifmp,
			objid,
			pgnoFDP,
			objidParent,
			pgnoFDPParent,
			fPageFlags,
			fUnique,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );

	Call( ErrREPAIRCheckTree( 
			ppib,
			ifmp,
			objid,
			pgnoFDP,
			objidParent,
			pgnoFDPParent,
			fPageFlags,
			fUnique,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertSEInfo(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;
	CSR csr;
	
	CallR( csr.ErrGetReadPage(
			ppib,
			ifmp,
			pgnoFDP,
			bflfNoTouch ) );

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	Assert( sizeof( SPACE_HEADER ) == line.cb );	//  checked in ErrREPAIRGetPgnoOEAE
		
	const SPACE_HEADER * const psph = reinterpret_cast <SPACE_HEADER *> ( line.pv );
	Assert( psph->FSingleExtent() );	//	checked in ErrREPAIRGetPgnoOEAE

	const CPG cpgOE = psph->CpgPrimary();
	const PGNO pgnoOELast = pgnoFDP + cpgOE - 1;

	Call( ErrREPAIRInsertOERunIntoTT(
		ppib,
		pgnoOELast,
		cpgOE,
		objid,
		objidParent,
		pttarrayOwnedSpace,
		popts ) );

HandleError:
	csr.ReleasePage();
	csr.Reset();

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertOERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	TTARRAY::RUN run;
	pttarrayOwnedSpace->BeginRun( ppib, &run );
	
	for( PGNO pgno = pgnoLast; pgno > pgnoLast - cpgRun; --pgno )
		{
		if( objidNil != objidParent )
			{
			OBJID objidOwning;
			Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning, &run ) );
			
			if( objidParent != objidOwning )
				{
				(*popts->pcprintfError)( "space allocation error (OE): page %d is already owned by objid %d, (expected parent objid %d for objid %d)\r\n",
												pgno, objidOwning, objidParent, objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		Call( pttarrayOwnedSpace->ErrSetValue( ppib, pgno, objid, &run ) );
		}
		
HandleError:
	pttarrayOwnedSpace->EndRun( ppib, &run );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertAERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	//  these TTARRAYs are always accessed in OwnedSpace, AvailSpace order
	TTARRAY::RUN runOwned;
	TTARRAY::RUN runAvail;
	
	pttarrayOwnedSpace->BeginRun( ppib, &runOwned );
	pttarrayAvailSpace->BeginRun( ppib, &runAvail );
	
	for( PGNO pgno = pgnoLast; pgno > pgnoLast - cpgRun; --pgno )
		{		
		//  we must own this page
		OBJID objidOwning;
		Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning, &runOwned ) );
		
		if( objidOwning != objid )
			{
			(*popts->pcprintfError)( "space allocation error (AE): page %d is owned by objid %d, (expected objid %d)\r\n",
										pgno, objidOwning, objid );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}

		//  the page must not be available to any other table
		Call( pttarrayAvailSpace->ErrGetValue( ppib, pgno, &objidOwning, &runAvail ) );
		if( objidNil != objidOwning )
			{
			(*popts->pcprintfError)( "space allocation error (AE): page %d is available to objid %d\r\n",
										pgno, objidOwning );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		Call( pttarrayAvailSpace->ErrSetValue( ppib, pgno, objid, &runAvail ) );
		}

HandleError:
	pttarrayOwnedSpace->EndRun( ppib, &runOwned );
	pttarrayAvailSpace->EndRun( ppib, &runAvail );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const CPG cpgDatabase,
	BOOL * const pfSpaceTreeCorrupt,
	TTARRAY ** const ppttarrayOwnedSpace,
	TTARRAY ** const ppttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgnoLastOE	= pgnoNull;
	INST * const pinst	= PinstFromPpib( ppib );

	const FIDLASTINTDB fidLastInTDB = { fidMSO_FixedLast, fidMSO_VarLast, fidMSO_TaggedLast };
	
	RECCHECKNULL 	recchecknull;
	RECCHECKTABLE 	recchecktable( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts ); 

	delete *ppttarrayOwnedSpace;
	delete *ppttarrayAvailSpace;

	*ppttarrayOwnedSpace 	= new TTARRAY( cpgDatabase + 1, objidSystemRoot );
	*ppttarrayAvailSpace 	= new TTARRAY( cpgDatabase + 1, objidNil );
		
	if( NULL == *ppttarrayOwnedSpace
		|| NULL == *ppttarrayAvailSpace )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	Call( (*ppttarrayOwnedSpace)->ErrInit( pinst ) );
	Call( (*ppttarrayAvailSpace)->ErrInit( pinst ) );
			
	Call( ErrREPAIRCheckSpaceTree(
			ppib,
			ifmp,
			pfSpaceTreeCorrupt,
			&pgnoLastOE,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			popts ) );

	// ignore err 
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO,
			pgnoFDPMSO,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			popts );
			
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO_NameIndex,
			pgnoFDPMSO_NameIndex,
			objidFDPMSO,
			pgnoFDPMSO,
			CPAGE::fPageIndex,
			fTrue,
			&recchecknull,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			popts );
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO_RootObjectIndex,
			pgnoFDPMSO_RootObjectIndex,
			objidFDPMSO,
			pgnoFDPMSO,
			CPAGE::fPageIndex,
			fTrue,
			&recchecknull,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			popts );				
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSOShadow,
			pgnoFDPMSOShadow,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			popts );
							
	err = JET_errSuccess;

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoRoot,
	const OBJID objidFDP,
	const ULONG fPageFlags,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace, 
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err	= JET_errSuccess;

	const ULONG	fFlagsFDP = fPageFlags;

	pbtstats->pgnoLastSeen = pgnoNull;
	
	CSR	csr;
	err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgnoRoot,
					bflfNoTouch );
	if( err < 0 )
		{
		(*popts->pcprintfError)( "page %d: error %d on read\r\n", pgnoRoot, err );
		Call( err );
		}

	if( !csr.Cpage().FRootPage() )
		{
		(*popts->pcprintfError)( "page %d: pgnoRoot is not a root page\r\n", pgnoRoot );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	
	csr.SetILine( 0 );
	
	Call( ErrREPAIRICheck(
			ppib,
			ifmp,
			objidFDP,
			fFlagsFDP,
			csr,
			fFalse,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			fNonUnique,
			pbtstats,
			NULL,
			NULL,
			popts ) );

	if( pgnoNull != pbtstats->pgnoNextExpected )
		{
		(*popts->pcprintfError)( "page %d: corrupt leaf links\r\n", pbtstats->pgnoLastSeen );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
HandleError:
	csr.ReleasePage( fTrue );
	csr.Reset();
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheck(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const ULONG fFlagsFDP,
	CSR& csr,
	const BOOL fPrereadSibling,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,	//  can be null
	const TTARRAY * const pttarrayAvailSpace,	//	can be null
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	BOOL	fDbtimeTooLarge	= fFalse;

	if ( 0 == ( AtomicIncrement( (LONG *)(&popts->psnprog->cunitDone) ) % ( popts->psnprog->cunitTotal / 100 ) )
		&& popts->crit.FTryEnter() )
		{
		if ( 0 == ( popts->psnprog->cunitDone % ( popts->psnprog->cunitTotal / 100 ) ) )	// every 1%
			{
			popts->psnprog->cunitDone = min( popts->psnprog->cunitDone, popts->psnprog->cunitTotal );
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			}
		popts->crit.Leave();
		}

	Call( csr.Cpage().ErrCheckPage( popts->pcprintfError ) );
	
	if( csr.Cpage().ObjidFDP() != objidFDP )
		{
		(*popts->pcprintfError)( "page %d: page belongs to different tree (objid is %d, should be %d)\r\n",
									csr.Pgno(),
									csr.Cpage().ObjidFDP(),
									objidFDP );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( ( csr.Cpage().FFlags() | fFlagsFDP ) != csr.Cpage().FFlags() )
		{
		(*popts->pcprintfError)( "page %d: page flag mismatch with FDP (current flags: 0x%x, FDP flags 0x%x)\r\n",
									csr.Pgno(), csr.Cpage().FFlags(), fFlagsFDP );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( csr.Cpage().Dbtime() > rgfmp[ifmp].Pdbfilehdr()->le_dbtimeDirtied )
		{
		(*popts->pcprintfWarning)( "page %d: dbtime is larger than database dbtime (0x%I64x, 0x%I64x)\n",
								csr.Cpage().Pgno(), csr.Cpage().Dbtime(), rgfmp[ifmp].Pdbfilehdr()->le_dbtimeDirtied );
		}
	
	//  check that this page is owned by this tree and not available to anyone
	
	OBJID objid;
	if( pttarrayOwnedSpace )
		{
		Call( pttarrayOwnedSpace->ErrGetValue( ppib, csr.Pgno(), &objid, NULL ) );
		if( csr.Cpage().ObjidFDP() != objid )
			{
			(*popts->pcprintfError)( "page %d: space allocation error. page is owned by objid %d, expecting %d\r\n",
										csr.Pgno(), objid, csr.Cpage().ObjidFDP() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	if( pttarrayAvailSpace )
		{
		Call( pttarrayAvailSpace->ErrGetValue( ppib, csr.Pgno(), &objid, NULL ) );
		if( objidNil != objid )
			{
			(*popts->pcprintfError)( "page %d: space allocation error. page is available to objid %d\r\n",
										csr.Pgno(), objid );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( csr.Cpage().FLeafPage() )
		{
		//  if we were the last leaf page page preread we preread our neighbour
		if( fPrereadSibling )
			{
			const PGNO pgnoNext = csr.Cpage().PgnoNext();
			if( pgnoNull != pgnoNext )
				{
				BFPrereadPageRange( ifmp, pgnoNext, g_cpgMinRepairSequentialPreread );
				}
			}

		Call( ErrREPAIRCheckLeaf(
				ppib,
				ifmp,
				csr,
				preccheck,
				fNonUnique,
				pbtstats,
				pbookmarkCurrParent,
				pbookmarkPrevParent,
				popts ) );
		}
	else
		{
		if( !csr.Cpage().FInvisibleSons() )
			{
			(*popts->pcprintfError)( "page %d: not an internal page\r\n" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		//  check the internal page before prereading its children
		
		Call( ErrREPAIRCheckInternal( 
				ppib, 
				ifmp, 
				csr, 
				pbtstats, 
				pbookmarkCurrParent, 
				pbookmarkPrevParent, 
				popts ) );

		PGNO		rgpgno[g_cbPageMax/(sizeof(PGNO) + cbNDInsertionOverhead + cbNDNullKeyData )];
		const INT 	cpgno = csr.Cpage().Clines();

		INT iline;
		for( iline = 0; iline < cpgno; iline++ )
			{
			csr.SetILine( iline );

			KEYDATAFLAGS kdf;			
			NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );
				
			rgpgno[iline] = *((UnalignedLittleEndian< PGNO > *)kdf.data.Pv() );
			}
		rgpgno[cpgno] = pgnoNull;

		BFPrereadPageList( ifmp, rgpgno );

		BOOKMARK	rgbookmark[2];
		BOOKMARK	*rgpbookmark[2];
		rgpbookmark[0] = NULL;
		rgpbookmark[1] = NULL;

		BOOL	fChildrenAreLeaf;
		BOOL	fChildrenAreParentOfLeaf;
		BOOL	fChildrenAreInternal;
		
		INT ipgno;
		for( ipgno = 0; ipgno < cpgno; ipgno++ )
			{
			csr.SetILine( ipgno );

			KEYDATAFLAGS kdf;
			NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );

			const INT ibookmarkCurr = ipgno % 2;
			const INT ibookmarkPrev = ( ipgno + 1 ) % 2;
			Assert( ibookmarkCurr != ibookmarkPrev );
			Assert( rgpbookmark[ibookmarkPrev] != NULL || 0 == ipgno );
			
			rgbookmark[ibookmarkCurr].key 	= kdf.key;
			rgbookmark[ibookmarkCurr].data 	= kdf.data;
			rgpbookmark[ibookmarkCurr] 		= &rgbookmark[ibookmarkCurr];

			if( rgbookmark[ibookmarkCurr].key.FNull() )
				{
				if( cpgno-1 != ipgno )
					{
					(*popts->pcprintfError)( "node [%d:%d]: NULL key is not last key in page\r\n", csr.Pgno(), csr.ILine() );
//					BFFree( rgpgno );
					Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
					}
				rgpbookmark[ibookmarkCurr] = NULL;
				}
				
			CSR	csrChild;
			err = csrChild.ErrGetReadPage(
								ppib,
								ifmp,
								rgpgno[ipgno],
								bflfNoTouch );
			if( err < 0 )
				{
				(*popts->pcprintfError)( "page %d: error %d on read\r\n", rgpgno[ipgno], err );
//				BFFree( rgpgno );
				goto HandleError;
				}
						
			err = ErrREPAIRICheck(
					ppib,
					ifmp,
					objidFDP,
					fFlagsFDP,
					csrChild,
					( cpgno - 1 == ipgno ),
					preccheck,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					fNonUnique,
					pbtstats,
					rgpbookmark[ibookmarkCurr],
					rgpbookmark[ibookmarkPrev],
					popts );
					
			if( err < 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: subtree check (page %d) failed with err %d\r\n", csr.Pgno(), csr.ILine(), rgpgno[ipgno], err );
				}
			else if( 0 == ipgno )
				{
				fChildrenAreLeaf 			= csrChild.Cpage().FLeafPage();
				fChildrenAreParentOfLeaf 	= csrChild.Cpage().FParentOfLeaf();
				fChildrenAreInternal 		= !fChildrenAreLeaf && !fChildrenAreParentOfLeaf;

				if( csr.Cpage().FParentOfLeaf() && !fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "page %d: child (%d) is not a leaf page but parent is parent-of-leaf\r\n", csr.Pgno(), rgpgno[ipgno] );
					}
				else if( !csr.Cpage().FParentOfLeaf() && fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "page %d: child (%d) is a leaf page but parent is not parent-of-leaf\r\n", csr.Pgno(), rgpgno[ipgno] );
					}
				}
			else
				{
				if( csrChild.Cpage().FLeafPage() != fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be leaf\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				else if( csrChild.Cpage().FParentOfLeaf() != fChildrenAreParentOfLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be parent-of-leaf\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				else if( fChildrenAreInternal && ( csrChild.Cpage().FLeafPage() || csrChild.Cpage().FParentOfLeaf() ) )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be internal\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				}
				
			csrChild.ReleasePage( fTrue );
			csrChild.Reset();
//			if ( err < 0 )
//				{
//				BFFree( rgpgno );
//				}
			Call( err );
			}
//		BFFree( rgpgno );
		}
		
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckNode(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts )
//  ================================================================	
	{
	ERR			err = JET_errSuccess;

	if( kdf.key.Cb() > JET_cbKeyMost * 2 )
		{
		(*popts->pcprintfError)( "node [%d:%d]: key is too long (%d bytes)\r\n", pgno, iline, kdf.key.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( kdf.data.Cb() > g_cbPage )
		{
		(*popts->pcprintfError)( "node [%d:%d]: data is too long (%d bytes)\r\n", pgno, iline, kdf.data.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( ( kdf.key.Cb() + kdf.data.Cb() ) > g_cbPage )	
		{
		(*popts->pcprintfError)( "node [%d:%d]: node is too big (%d bytes)\r\n", pgno, iline, kdf.key.Cb() + kdf.data.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	const BYTE * const pbKeyPrefix = (BYTE *)kdf.key.prefix.Pv();
	const BYTE * const pbKeySuffix = (BYTE *)kdf.key.suffix.Pv();
	const BYTE * const pbData = (BYTE *)kdf.data.Pv();

	if( FNDCompressed( kdf ) )
		{
		if( pbKeyPrefix < pbPage 
			|| pbKeyPrefix + kdf.key.prefix.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: prefix not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( kdf.key.suffix.Cb() > 0 )
		{
		if( pbKeySuffix < pbPage 
			|| pbKeySuffix + kdf.key.suffix.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: suffix not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( kdf.data.Cb() > 0 )
		{
		if( pbData < pbPage 
			|| pbData + kdf.data.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: data not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckRecord(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts )
//  ================================================================	
	{
	ERR		err = JET_errSuccess;
	const FIDLASTINTDB fidLastInTDB = { fidFixedMost, fidVarMost, fidTaggedMost }; 

	RECCHECKTABLE 	reccheck( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts ); 

	Call( reccheck.ErrCheckRecord( kdf ) );
			
HandleError:
	return err;

	}


//  ================================================================
LOCAL int LREPAIRHandleException(
	const PIB * const ppib,
	const IFMP ifmp,
	const CSR& csr,
	const EXCEPTION exception,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const DWORD dwExceptionId 			= ExceptionId( exception );
	const EExceptionFilterAction efa	= efaExecuteHandler;

	(*popts->pcprintfError)( "node [%d:%d]: caught exception 0x%x\r\n",
				csr.Pgno(),
				csr.ILine(),
				dwExceptionId
				);

	return efa;
	}


#pragma warning( disable : 4509 )
//  ================================================================
LOCAL ERR ErrREPAIRICheckInternalLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TRY
		{		
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfCurr );
		CallJ( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				kdfCurr,
				popts ), HandleTryError );
		
		if( FNDDeleted( kdfCurr ) )
			{
			(*popts->pcprintfError)( "node [%d:%d]: deleted node on internal page\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}
			
		if( FNDVersion( kdfCurr ) )
			{
			(*popts->pcprintfError)( "node [%d:%d]: versioned node on internal page\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( 1 == kdfCurr.key.prefix.Cb() )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}
				
		if( sizeof( PGNO ) != kdfCurr.data.Cb()  )
			{
			(*popts->pcprintfError)( "node [%d:%d]: bad internal data size\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( csr.ILine() > 0 && !kdfCurr.key.FNull() )
			{
			const INT cmp = CmpKey( kdfPrev.key, kdfCurr.key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on internal page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on internal page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			}
		
		if( FNDCompressed( kdfCurr ) )
			{
			++(pbtstats->cnodeCompressed);
			}			
		pbtstats->cbDataInternal += kdfCurr.data.Cb();
		++(pbtstats->rgckeyInternal[kdfCurr.key.Cb()]);
		++(pbtstats->rgckeySuffixInternal[kdfCurr.key.suffix.Cb()]);

HandleTryError:
		;
		}
	EXCEPT( LREPAIRHandleException( ppib, ifmp, csr, ExceptionInfo(), popts ) )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	ENDEXCEPT

	return err;
	}
#pragma warning( default : 4509 )


//  ================================================================
LOCAL ERR ErrREPAIRCheckInternal(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	KEYDATAFLAGS 	rgkdf[2];			

	if( csr.Cpage().Clines() <= 0 )
		{
		(*popts->pcprintfError)( "page %d: empty internal page\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoNull != csr.Cpage().PgnoNext() )
		{
		(*popts->pcprintfError)( "page %d: pgno next is non-NULL (%d)\r\n", csr.Pgno(), csr.Cpage().PgnoNext() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoNull != csr.Cpage().PgnoPrev() )
		{
		(*popts->pcprintfError)( "page %d: pgno next is non-NULL (%d)\r\n", csr.Pgno(), csr.Cpage().PgnoPrev() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}		

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );
	++(pbtstats->cpageInternal);
	if( csr.Cpage().FRootPage() && !csr.Cpage().FSpaceTree() )
		{
		if( sizeof( SPACE_HEADER ) != line.cb )
			{
			(*popts->pcprintfError)( "page %d: space header is wrong size (expected %d bytes, got %d bytes)\r\n",
				csr.Pgno(),
				sizeof( SPACE_HEADER ),
				line.cb
				);
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else
		{
		++(pbtstats->rgckeyPrefixInternal[line.cb]);
		}
	if( pbtstats->cpageDepth <= 0 )
		{
		--(pbtstats->cpageDepth);
		}

	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		KEYDATAFLAGS& kdfCurr = rgkdf[ iline % 2 ];
		const KEYDATAFLAGS& kdfPrev = rgkdf[ ( iline + 1 ) % 2 ];

		Call( ErrREPAIRICheckInternalLine(
					ppib,
					ifmp,
					csr,
					pbtstats,
					popts,
					kdfCurr,
					kdfPrev ) );
		}

	if( pbookmarkCurrParent )
		{
		csr.SetILine( csr.Cpage().Clines() - 1 );

		KEYDATAFLAGS kdfLast;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfLast );
		
		if( !FKeysEqual( kdfLast.key, pbookmarkCurrParent->key ) )
			{
			(*popts->pcprintfError)( "page %d: bad page pointer to internal page\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( pbookmarkPrevParent )
		{
		csr.SetILine( 0 );

		KEYDATAFLAGS kdfFirst;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfFirst );
		if( kdfFirst.key.Cb() != 0 )	//  NULL is greater than anything
			{
			const INT cmp = CmpKey( kdfFirst.key, pbookmarkPrevParent->key );
			if( cmp < 0 )
				{
				(*popts->pcprintfError)( "page %d: prev parent pointer > first node on internal page\r\n", csr.Pgno() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:
	return err;
	}

	
#pragma warning( disable : 4509 )
//  ================================================================
LOCAL ERR ErrREPAIRICheckLeafLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev,
	BOOL * const pfEmpty )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TRY
		{		
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfCurr );
		CallJ( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				kdfCurr,
				popts ), HandleTryError );
		
		if( kdfCurr.key.prefix.Cb() == 1 )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( !FNDDeleted( kdfCurr ) )
			{
			*pfEmpty = fFalse;
			err = (*preccheck)( kdfCurr );
			if( err > 0 )
				{
				(*popts->pcprintfWarning)( "node [%d:%d]: leaf node check failed\r\n", csr.Pgno(), csr.ILine() );
				}
			else if( err < 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: leaf node check failed\r\n", csr.Pgno(), csr.ILine() );
				CallJ( err, HandleTryError );
				}
			}
		else
			{
			++(pbtstats->cnodeDeleted);
			}

		if( csr.ILine() > 0 )
			{
			Assert( !fNonUnique || csr.Cpage().FIndexPage() );
			
			const INT cmp = fNonUnique ?
								CmpKeyData( kdfPrev, kdfCurr, NULL ) :
								CmpKey( kdfPrev.key, kdfCurr.key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on leaf page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on leaf page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			}

		if( FNDVersion( kdfCurr ) )
			{
			++(pbtstats->cnodeVersioned);
			}
			
		if( FNDCompressed( kdfCurr ) )
			{
			++(pbtstats->cnodeCompressed);
			}
			
		pbtstats->cbDataLeaf += kdfCurr.data.Cb();
		++(pbtstats->rgckeyLeaf[kdfCurr.key.Cb()]);
		++(pbtstats->rgckeySuffixLeaf[kdfCurr.key.suffix.Cb()]);

HandleTryError:
		;
		}
	EXCEPT( LREPAIRHandleException( ppib, ifmp, csr, ExceptionInfo(), popts ) )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	ENDEXCEPT

	return err;
	}
#pragma warning( default : 4509 )


//  ================================================================
LOCAL ERR ErrREPAIRCheckLeaf(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	BOOL			fEmpty	= fTrue;
	KEYDATAFLAGS 	rgkdf[2];			

	if( csr.Cpage().Clines() == 0 && !csr.Cpage().FRootPage() )
		{
		(*popts->pcprintfError)( "page %d: empty leaf page (non-root)\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( csr.Cpage().PgnoPrev() != pbtstats->pgnoLastSeen )
		{
		(*popts->pcprintfError)( "page %d: bad leaf page links\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( pgnoNull != pbtstats->pgnoNextExpected && csr.Cpage().Pgno() != pbtstats->pgnoNextExpected )
		{
		(*popts->pcprintfError)( "page %d: bad leaf page links\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	pbtstats->pgnoLastSeen 		= csr.Cpage().Pgno();
	pbtstats->pgnoNextExpected	= csr.Cpage().PgnoNext();	

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );
	++(pbtstats->cpageLeaf);
	if( csr.Cpage().FRootPage() && !csr.Cpage().FSpaceTree() )
		{
		if( sizeof( SPACE_HEADER ) != line.cb )
			{
			(*popts->pcprintfError)( "page %d: space header is wrong size (expected %d bytes, got %d bytes)\r\n",
				csr.Pgno(),
				sizeof( SPACE_HEADER ),
				line.cb
				);
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else
		{
		++(pbtstats->rgckeyPrefixLeaf[line.cb]);
		}
		
	if( pbtstats->cpageDepth <= 0 )
		{
		pbtstats->cpageDepth *= -1;
		++(pbtstats->cpageDepth);
		}

	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		KEYDATAFLAGS& kdfCurr = rgkdf[ iline % 2 ];
		const KEYDATAFLAGS& kdfPrev = rgkdf[ ( iline + 1 ) % 2 ];

		Call( ErrREPAIRICheckLeafLine(
					ppib,
					ifmp,
					csr,
					preccheck,
					fNonUnique,
					pbtstats,
					popts,
					kdfCurr,
					kdfPrev,
					&fEmpty ) );
		}

	if( pbookmarkCurrParent )
		{
		if( pgnoNull == csr.Cpage().PgnoNext() )
			{
			(*popts->pcprintfError)( "page %d: non-NULL page pointer to leaf page with no pgnoNext\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
			
		csr.SetILine( csr.Cpage().Clines() - 1 );

		KEYDATAFLAGS kdfLast;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfLast );

		const INT cmp = CmpKey( kdfLast.key, pbookmarkCurrParent->key );
		if( cmp >= 0 )
			{
			(*popts->pcprintfError)( "page %d: bad page pointer to leaf page\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else if( pgnoNull != csr.Cpage().PgnoNext() )	// NULL parent means we are at the end of the tree
		{
		(*popts->pcprintfError)( "page %d: NULL page pointer to leaf page with pgnoNext\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}

	if( pbookmarkPrevParent )
		{
		csr.SetILine( 0 );

		KEYDATAFLAGS kdfFirst;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfFirst );

		if( kdfFirst.key.Cb() != 0 )
			{
			//  for secondary indexes compare with the primary key that is in the data
			const INT cmp = CmpKeyWithKeyData( pbookmarkPrevParent->key, kdfFirst );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "page %d: prev parent pointer > first node on leaf page\r\n", csr.Pgno() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

	if( fEmpty )
		{
		++(pbtstats->cpageEmpty);
		}

HandleError:
	return err;
	}


//  ================================================================
OBJIDLIST::OBJIDLIST() :
//  ================================================================
	m_cobjid( 0 ),
	m_cobjidMax( 0 ),
	m_rgobjid( NULL ),
	m_fSorted( fFalse )
	{
	}


//  ================================================================
OBJIDLIST::~OBJIDLIST()
//  ================================================================
	{
	if( NULL != m_rgobjid )
		{
		Assert( 0 < m_cobjidMax );
		OSMemoryHeapFree( m_rgobjid );
		m_rgobjid = reinterpret_cast<OBJID *>( lBitsAllFlipped );
		}
	else
		{
		Assert( 0 == m_cobjidMax );
		Assert( 0 == m_cobjid );
		}
	}

	
//  ================================================================
ERR OBJIDLIST::ErrAddObjid( const OBJID objid )
//  ================================================================
	{
	if( m_cobjid == m_cobjidMax )
		{		
		//  resize/create the array

		OBJID * const rgobjidOld = m_rgobjid;
		const INT cobjidMaxNew 	 = m_cobjidMax + 16;
		OBJID * const rgobjidNew = reinterpret_cast<OBJID *>( PvOSMemoryHeapAlloc( cobjidMaxNew * sizeof( OBJID ) ) );
		if( NULL == rgobjidNew )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
			
		UtilMemCpy( rgobjidNew, m_rgobjid, sizeof( OBJID ) * m_cobjid );
		m_cobjidMax = cobjidMaxNew;
		m_rgobjid 	= rgobjidNew;
		OSMemoryHeapFree( rgobjidOld );
		}
	m_rgobjid[m_cobjid++] = objid;
	m_fSorted = fFalse;
	return JET_errSuccess;
	}
	

//  ================================================================
BOOL OBJIDLIST::FObjidPresent( const OBJID objid ) const
//  ================================================================
	{
	Assert( m_fSorted );
	return binary_search( (OBJID *)m_rgobjid, (OBJID *)m_rgobjid + m_cobjid, objid );
	}


//  ================================================================
VOID OBJIDLIST::Sort()
//  ================================================================
	{
	sort( m_rgobjid, m_rgobjid + m_cobjid );
	m_fSorted = fTrue;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAttachForIntegrity(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	IFMP * const pifmp,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Attach R/O, without attaching the SLV
//
	{
	ERR err = JET_errSuccess;

	//	

	const BOOL fAttachReadonly = popts->grbit & JET_bitDBUtilOptionDontRepair;

	Call( ErrIsamAttachDatabase(
		sesid,
		szDatabase,
		NULL,
		NULL,
		0,
		( fAttachReadonly ? JET_bitDbReadOnly : 0 ) | JET_bitDbRecoveryOff) );
	Assert( JET_wrnDatabaseAttached != err );

	Call( ErrIsamOpenDatabase(
		sesid,
		szDatabase,
		NULL,
		reinterpret_cast<JET_DBID *>( pifmp ),
		( fAttachReadonly ? JET_bitDbReadOnly : 0 ) | JET_bitDbRecoveryOff
		) );

	rgfmp[*pifmp].SetVersioningOff();

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAttachForRepair(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	IFMP * const pifmp,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Close/Detach a previously attached database, attach R/W and change the signature
//
	{
	ERR err = JET_errSuccess;
	
	CallR( ErrIsamCloseDatabase( sesid, (JET_DBID)*pifmp, 0 ) );
	CallR( ErrIsamDetachDatabase( sesid, NULL, szDatabase ) );
	CallR( ErrREPAIRChangeSignature( PinstFromPpib( (PIB *)sesid ), szDatabase, szSLV, dbtimeLast, objidLast, popts ) );
	CallR( ErrIsamAttachDatabase( sesid, szDatabase, NULL, NULL, 0, JET_bitDbRecoveryOff) );
	Assert( JET_wrnDatabaseAttached != err );
	CallR( ErrIsamOpenDatabase(
			sesid,
			szDatabase,
			NULL,
			reinterpret_cast<JET_DBID *>( pifmp ),
			JET_bitDbRecoveryOff ) );
	rgfmp[*pifmp].SetVersioningOff();
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeSignature(
	INST *pinst,
	const char * const szDatabase,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;

	//  the SLV file and the database must have the same signatures
	
	SIGNATURE signDb;
	SIGNATURE signSLV;

	Call( ErrREPAIRChangeDBSignature(
			pinst,
			szDatabase,
			dbtimeLast,
			objidLast,
			&signDb,
			&signSLV,
			popts ) );
			
	if( NULL != szSLV )
		{
		Call( ErrREPAIRChangeSLVSignature(
				pinst,
				szSLV,
				dbtimeLast,
				objidLast,
				&signDb,
				&signSLV,
				popts ) );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeDBSignature(
	INST *pinst,
	const char * const szDatabase,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	SIGNATURE * const psignDb,
	SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	DBFILEHDR * const pdfh = reinterpret_cast<DBFILEHDR * >( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pdfh )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szDatabase ),
				reinterpret_cast<BYTE*>( pdfh ),
				g_cbPage,
				OffsetOf( DBFILEHDR, le_cbPageSize ) );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read database header for %s\r\n", szDatabase );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		Assert( 0 != dwGlobalMajorVersion );
		
		(*popts->pcprintfVerbose)( "changing signature of %s\r\n", szDatabase );		

		pdfh->le_ulMagic 			= ulDAEMagic;
		pdfh->le_ulVersion 			= ulDAEVersion;
		pdfh->le_ulUpdate 			= ulDAEUpdate;
		if( 0 != dbtimeLast )
			{
			
			//	sometimes we may not have re-calculated the dbtime
			
			pdfh->le_dbtimeDirtied 		= dbtimeLast + 1;
			}
		pdfh->le_objidLast 			= objidLast + 1;
		pdfh->le_attrib 			= attribDb;
		pdfh->le_dwMajorVersion 	= dwGlobalMajorVersion;
		pdfh->le_dwMinorVersion 	= dwGlobalMinorVersion;
		pdfh->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pdfh->le_lSPNumber 			= lGlobalSPNumber;
		pdfh->le_lGenMinRequired 	= 0;
		pdfh->le_lGenMaxRequired 	= 0;
		if( objidNil != objidLast )
			{
			++(pdfh->le_ulRepairCount);
			}
		LGIGetDateTime( &pdfh->logtimeRepair );
		
		pdfh->ResetUpgradeDb();

		memset( &pdfh->signLog, 0, sizeof( SIGNATURE ) );
		
		memset( &pdfh->bkinfoFullPrev, 0, sizeof( BKINFO ) );
		memset( &pdfh->bkinfoIncPrev, 0, sizeof( BKINFO ) );
		memset( &pdfh->bkinfoFullCur, 0, sizeof( BKINFO ) );

		SIGGetSignature( &(pdfh->signDb) );
		SIGGetSignature( &(pdfh->signSLV) );

		*psignDb 	= pdfh->signDb;
		*psignSLV 	= pdfh->signSLV;

		(*popts->pcprintfVerbose)( "new DB signature is:\r\n" );		
		REPAIRPrintSig( &pdfh->signDb, popts->pcprintfVerbose );
		(*popts->pcprintfVerbose)( "new SLV signature is:\r\n" );		
		REPAIRPrintSig( &pdfh->signSLV, popts->pcprintfVerbose );

		Call( ErrUtilWriteShadowedHeader(	pinst->m_pfsapi, 
											szDatabase, 
											fTrue,
											reinterpret_cast<BYTE*>( pdfh ), 
											g_cbPage ) );
		}

HandleError:
	OSMemoryPageFree( pdfh );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeSLVSignature(
	INST *pinst,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const SIGNATURE * const psignDb,
	const SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	SLVFILEHDR * const pslvfilehdr = reinterpret_cast<SLVFILEHDR *>( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pslvfilehdr )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szSLV ),
				reinterpret_cast<BYTE*>( pslvfilehdr ),
				g_cbPage,
				OffsetOf( SLVFILEHDR, le_cbPageSize ) );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read SLV header for %s\r\n", szSLV );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		Assert( 0 != dwGlobalMajorVersion );
		
		(*popts->pcprintfVerbose)( "changing signature of %s\r\n", szSLV );		

		pslvfilehdr->SetDbstate( JET_dbstateCleanShutdown );

		pslvfilehdr->le_ulMagic 			= ulDAEMagic;
		pslvfilehdr->le_ulVersion 			= ulDAEVersion;
		pslvfilehdr->le_ulUpdate 			= ulDAEUpdate;
		pslvfilehdr->le_dbtimeDirtied 		= dbtimeLast + 1;
		pslvfilehdr->le_objidLast 			= objidLast + 1;
		pslvfilehdr->le_attrib 				= attribSLV;
		pslvfilehdr->le_dwMajorVersion 		= dwGlobalMajorVersion;
		pslvfilehdr->le_dwMinorVersion 		= dwGlobalMinorVersion;
		pslvfilehdr->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pslvfilehdr->le_lSPNumber 			= lGlobalSPNumber;
		pslvfilehdr->le_lGenMinRequired 	= 0;
		pslvfilehdr->le_lGenMaxRequired 	= 0;
		if( objidNil != objidLast )
			{
			++(pslvfilehdr->le_ulRepairCount);
			}
		LGIGetDateTime( &pslvfilehdr->logtimeRepair );
		
		memset( &pslvfilehdr->signLog, 0, sizeof( SIGNATURE ) );

		pslvfilehdr->signDb 	= *psignDb;
		pslvfilehdr->signSLV 	= *psignSLV;

		(*popts->pcprintfVerbose)( "new DB signature is:\r\n" );		
		REPAIRPrintSig( &pslvfilehdr->signDb, popts->pcprintfVerbose );
		(*popts->pcprintfVerbose)( "new SLV signature is:\r\n" );		
		REPAIRPrintSig( &pslvfilehdr->signSLV, popts->pcprintfVerbose );

		Call( ErrUtilWriteShadowedHeader(	pinst->m_pfsapi, 
											szSLV, 
											fFalse,
											reinterpret_cast<BYTE*>( pslvfilehdr ), 
											g_cbPage ) );
		}

HandleError:
	OSMemoryPageFree( pslvfilehdr );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairGlobalSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{	
	ERR err = JET_errSuccess;

	const PGNO pgnoLast = PgnoLast( ifmp );
	const CPG  cpgOwned = PgnoLast( ifmp ) - 3;	// we will insert three pages in the ErrSPCreate below

	FUCB 	*pfucb		= pfucbNil;
	FUCB	*pfucbOE	= pfucbNil;

	(*popts->pcprintfVerbose)( "repairing database root\r\n" );				

	OBJID			objidFDP;
	Call( ErrSPCreate(
				ppib,
				ifmp,
				pgnoNull,
				pgnoSystemRoot,
				3,
				fSPMultipleExtent,
				(ULONG)CPAGE::fPagePrimary,
				&objidFDP ) );
	Assert( objidSystemRoot == objidFDP );

	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucb ) );

	//  The tree has only one node so we can insert ths node without splitting
	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );

	(*popts->pcprintfDebug)( "Global OwnExt:  %d pages ending at %d\r\n", cpgOwned, pgnoLast );
	Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbOE,
					pgnoLast,
					cpgOwned,
					popts ) );

HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	if( pfucbNil != pfucbOE )
		{
		DIRClose( pfucbOE );
		}
	return err;
	}


//  ================================================================ 
LOCAL ERR ErrREPAIRBuildCatalogEntryToDeleteList( 
	INFOLIST **ppDeleteList, 
	const ENTRYINFO entryinfo )
//  ================================================================
	{
	//Insert entryinfo into the list based on its objidTable+objType+objidFDP
	
	ERR				err			= JET_errSuccess;
	INFOLIST 	*	pTemp 		= *ppDeleteList;
	INFOLIST 	* 	pInfo;

	pInfo = new INFOLIST;

	if( NULL == pInfo )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	memset( pInfo, 0, sizeof(INFOLIST) );
	pInfo->info = entryinfo;
	pInfo->pInfoListNext = NULL;

	if( NULL == pTemp // empty list
		|| pTemp->info.objidTable > entryinfo.objidTable 
		|| ( pTemp->info.objidTable == entryinfo.objidTable
			 && pTemp->info.objType > entryinfo.objType )  
		|| ( pTemp->info.objidTable == entryinfo.objidTable
			 && pTemp->info.objType == entryinfo.objType
			 && pTemp->info.objidFDP > entryinfo.objidFDP ) ) 
		{
		// insert into the first record of the list
		pInfo->pInfoListNext = pTemp;
		*ppDeleteList = pInfo;
		}
	else 
		{	
		while( NULL != pTemp->pInfoListNext 
			   && ( pTemp->pInfoListNext->info.objidTable < entryinfo.objidTable 
			   		|| ( pTemp->pInfoListNext->info.objidTable == entryinfo.objidTable
						 && pTemp->pInfoListNext->info.objType < entryinfo.objType ) 
			   		|| ( pTemp->pInfoListNext->info.objidTable == entryinfo.objidTable
			   			 && pTemp->pInfoListNext->info.objType == entryinfo.objType
			   			 && pTemp->pInfoListNext->info.objidFDP < entryinfo.objidFDP ) ) )
			{
			pTemp = pTemp->pInfoListNext;
			}
		pInfo->pInfoListNext = pTemp->pInfoListNext;
		pTemp->pInfoListNext = pInfo;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRDeleteCorruptedEntriesFromCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const INFOLIST *pTablesToDelete,
	const INFOLIST *pEntriesToDelete,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err				= JET_errSuccess;
	FUCB 		*	pfucbCatalog 	= pfucbNil;
	ENTRYINFO 		entryinfo;

	BOOL			fEntryToDelete  = fFalse;

	BOOL			fSeenSLVAvail	= fFalse;
	BOOL			fSeenSLVOwnerMap= fFalse;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	Call( ErrDIROpen( ppib, pgnoFDPMSO, ifmp, &pfucbCatalog ) );
	Assert( pfucbNil != pfucbCatalog );
	
	FUCBSetIndex( pfucbCatalog );
	FUCBSetSequential( pfucbCatalog );
	
	err = ErrDIRDown( pfucbCatalog, &dib );

	// if no more records in catalog or no more records to delete, exit
	while ( JET_errNoCurrentRecord != err  
			&& ( pTablesToDelete || pEntriesToDelete ) )
		{
		Call( err );
			
		Call( ErrDIRRelease( pfucbCatalog ) );
		
		memset( &entryinfo, 0, sizeof( entryinfo ) );
		Call( ErrREPAIRRetrieveCatalogColumns( ppib, ifmp, pfucbCatalog, &entryinfo, popts ) );

		while( pTablesToDelete && pTablesToDelete->info.objidTable < entryinfo.objidTable )
			{
			pTablesToDelete = pTablesToDelete->pInfoListNext;
			}
		while( pEntriesToDelete && pEntriesToDelete->info.objidTable < entryinfo.objidTable )
			{
			pEntriesToDelete = pEntriesToDelete->pInfoListNext;
			}

		if( pTablesToDelete && pTablesToDelete->info.objidTable == entryinfo.objidTable )
			{
			// find the corrupted table entries in catalog
			if( objidSystemRoot == entryinfo.objidTable &&
				sysobjSLVAvail == entryinfo.objType ) //special case
				{
				if ( fSeenSLVAvail )
					{
					// find the multiple SLVAvail tree entry in catalog
					fEntryToDelete = fTrue;
					}
				else
					{
					fSeenSLVAvail = fTrue;
					}
				}
			else if( objidSystemRoot == entryinfo.objidTable &&
					 sysobjSLVOwnerMap == entryinfo.objType ) //special case
				{
				if ( fSeenSLVOwnerMap )
					{
					// find the multiple SLVOwnerMap tree entry in catalog
					fEntryToDelete = fTrue;
					}
				else
					{
					fSeenSLVOwnerMap = fTrue;
					}
				}
			else
				{
				fEntryToDelete = fTrue;
				}
			}	
		else if( pEntriesToDelete
				 && pEntriesToDelete->info.objidTable == entryinfo.objidTable 
				 && pEntriesToDelete->info.objidFDP == entryinfo.objidFDP 
				 && ( sysobjIndex == entryinfo.objType 
				 	  || sysobjLongValue == entryinfo.objType ) )
				{
				// find the corrupted entry in catalog 
				fEntryToDelete = fTrue;
				
				pEntriesToDelete = pEntriesToDelete->pInfoListNext;	
				}
		else
			{
			// good entry in catalog
			}

		if( fEntryToDelete )
			{
			// delete the entry in the catalog
			(*popts->pcprintfVerbose)( "Deleting a catalog entry (%d %s)\t\n", 
										entryinfo.objidTable, entryinfo.szName );
			
			Call( ErrDIRDelete( pfucbCatalog, fDIRNoVersion ) );

			fEntryToDelete = fFalse;
			}

		err = ErrDIRNext( pfucbCatalog, fDIRNull );	
		}

	if( JET_errNoCurrentRecord == err
		|| JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		}
		
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}

	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertMSOEntriesToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err		= JET_errSuccess;
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrREPAIRCATCreate( 
					ppib, 
					ifmp, 
					objidFDPMSO_NameIndex, 
					objidFDPMSO_RootObjectIndex,
					fTrue ) );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
HandleError:
	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	
	return err;	
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL fCatalogCorrupt,
	const BOOL fShadowCatalogCorrupt, 
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;

	FUCB	* pfucbParent 			= pfucbNil;
	FUCB	* pfucbCatalog 			= pfucbNil;
	FUCB	* pfucbShadowCatalog 	= pfucbNil;
	FUCB	* pfucbSpace			= pfucbNil;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	if( fCatalogCorrupt || fShadowCatalogCorrupt )
		{
		//  we'll need this for the space
		Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbParent ) );
		}
		
	if( fCatalogCorrupt && fShadowCatalogCorrupt )
		{
		Call( ErrREPAIRScanDBAndRepairCatalogs( ppib, ifmp, popts ) );

		// Check New Catalog and Delete all records pertaining to a corrupted table
		err = ErrREPAIRCheckFixCatalogLogical( 
					ppib, 
					ifmp, 
					pobjidLast, 
					fFalse, 
					fTrue, 
					popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfVerbose)( "Repaired the logical inconsistency of the catalog\t\n" );
			}
		}
		
	if( fShadowCatalogCorrupt )
		{
		//  if the catalog was corrupted as well it was repaired above
		const PGNO pgnoLast = 32;
		const PGNO cpgOwned = 8 + 1;
		Assert( cpgOwned == cpgMSOShadowInitial );

		(*popts->pcprintf)( "\r\nRebuilding %s from %s.\r\n", szMSOShadow, szMSO );
		popts->psnprog->cunitTotal 	= PgnoLast( ifmp );
		popts->psnprog->cunitDone	= 0;
		(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );

		(*popts->pcprintfVerbose)( "rebuilding %s from %s\r\n", szMSOShadow, szMSO );
		
		//  copy from the catalog to the shadow
		Assert( pfucbNil != pfucbParent );
		Call( ErrSPCreateMultiple(
			pfucbParent,
			pgnoSystemRoot,
			pgnoFDPMSOShadow,
			objidFDPMSOShadow,
			pgnoFDPMSOShadow+1,
			pgnoFDPMSOShadow+2,
			pgnoLast,
			cpgOwned,
			fTrue,
			CPAGE::fPagePrimary ) );

		DIRClose( pfucbParent );
		pfucbParent = pfucbNil;

		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbShadowCatalog, szMSOShadow, NO_GRBIT ) );
		Call( ErrBTCopyTree( pfucbCatalog, pfucbShadowCatalog, fDIRNoLog | fDIRNoVersion ) );

		DIRClose( pfucbCatalog );
		pfucbCatalog = pfucbNil;
		DIRClose( pfucbShadowCatalog );
		pfucbShadowCatalog = pfucbNil;		
		}
	else if( fCatalogCorrupt )
		{
		const PGNO pgnoLast = 23;
		const PGNO cpgOwned = 23 - 3 - 3;	//  3 for system root, 3 for FDP
		Assert( cpgMSOInitial >= cpgOwned );

		(*popts->pcprintf)( "\r\nRebuilding %s from %s.\r\n", szMSO, szMSOShadow );
		popts->psnprog->cunitTotal 	= PgnoLast( ifmp );
		popts->psnprog->cunitDone	= 0;
		(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );

		(*popts->pcprintfVerbose)( "rebuilding %s from %s\r\n", szMSO, szMSOShadow );
		
		Assert( pfucbNil != pfucbParent );
		//  when we create this we cannot make all the pages available, some will be needed later
		//  for the index FDP's. The easiest thing to do is not add any pages to the AvailExt
		Call( ErrSPCreateMultiple(
			pfucbParent,
			pgnoSystemRoot,
			pgnoFDPMSO,
			objidFDPMSO,
			pgnoFDPMSO+1,
			pgnoFDPMSO+2,
			pgnoFDPMSO+2,
			3,
			fTrue,
			CPAGE::fPagePrimary ) );

		DIRClose( pfucbParent );
		pfucbParent = pfucbNil;
		
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );

		if ( !pfucbCatalog->u.pfcb->FSpaceInitialized() )
			{
			pfucbCatalog->u.pfcb->SetPgnoOE( pgnoFDPMSO+1 );
			pfucbCatalog->u.pfcb->SetPgnoAE( pgnoFDPMSO+2 );
			pfucbCatalog->u.pfcb->SetSpaceInitialized();
			}
		Call( ErrSPIOpenOwnExt( ppib, pfucbCatalog->u.pfcb, &pfucbSpace ) );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "%s OwnExt: %d pages ending at %d\r\n", szMSO, cpgOwned, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbSpace,
					pgnoLast,
					cpgOwned,
					popts ) );

		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbShadowCatalog, szMSOShadow, NO_GRBIT ) );
		Call( ErrBTCopyTree( pfucbShadowCatalog, pfucbCatalog, fDIRNoLog | fDIRNoVersion ) );

		DIRClose( pfucbSpace );
		pfucbSpace = pfucbNil;
		DIRClose( pfucbCatalog );
		pfucbCatalog = pfucbNil;
		DIRClose( pfucbShadowCatalog );
		pfucbShadowCatalog = pfucbNil;		
		}

	if( fCatalogCorrupt || !fShadowCatalogCorrupt )
		{
		//  we don't need to repair the indexes if just the shadow catalog was corrupt
		//  otherwise (i.e. the catalog was corrupt or neither catalog was corrupt) we
		//  need to rebuild the indexes
		(*popts->pcprintfVerbose)( "rebuilding indexes for %s\r\n", szMSO );

		REPAIRTABLE repairtable;
		memset( &repairtable, 0, sizeof( REPAIRTABLE ) );
		repairtable.objidFDP = objidFDPMSO;
		repairtable.objidLV	 = objidNil;
		repairtable.pgnoFDP  = pgnoFDPMSO;
		repairtable.pgnoLV   = pgnoNull;
		repairtable.fHasPrimaryIndex = fTrue;
		strcpy( repairtable.szTableName, szMSO );
				
		//  we should be able to open the catalog without referring to the catalog
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
		FUCBSetSystemTable( pfucbCatalog );
		Call( ErrREPAIRBuildAllIndexes( ppib, ifmp, &pfucbCatalog, &repairtable, popts ) );
		}
	
HandleError:
	if( pfucbNil != pfucbParent )
		{
		DIRClose( pfucbParent );
		}
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}
	if( pfucbNil != pfucbShadowCatalog )
		{
		DIRClose( pfucbShadowCatalog );
		}
	if( pfucbNil != pfucbSpace )
		{
		DIRClose( pfucbSpace );
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRScanDBAndRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  This is called if both copies of the system catalog are corrupt
//  we extract all the pages belonging to _either_ the catalog or
//  the shadow catalog and then take the union of their records (removing
//  duplicates).
//
//-
	{
	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	const CPG cpgPreread 	= 256;
	const PGNO	pgnoFirst 	= 1;
	const PGNO	pgnoLast	= PgnoLast( ifmp );
	
	ERR	err = JET_errSuccess;

	CPG cpgRemaining;

	CPG	cpgUninitialized 	= 0;
	CPG	cpgBad 				= 0;
	
	PGNO	pgno			= pgnoFirst;
	
	INT		cRecords			= 0;
	INT		cRecordsDuplicate	= 0;

	JET_COLUMNDEF	rgcolumndef[2] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLongBinary, 0, 0, 0, 0, JET_cbPrimaryKeyMost, JET_bitColumnTTKey },	//	KEY
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLongBinary, 0, 0, 0, 0, 0, 0 },										//	DATA
		};	

	JET_TABLEID tableid;
	JET_COLUMNID rgcolumnid[2];
	const JET_COLUMNID& columnidKey	 = rgcolumnid[0];
	const JET_COLUMNID& columnidData = rgcolumnid[1];
	
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		sizeof( rgcolumndef ) / sizeof( rgcolumndef[0] ),
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&tableid,
		rgcolumnid ) );

	(*popts->pcprintf)( "\r\nScanning the database catalog.\r\n" );
	(*popts->pcprintfVerbose)( "scanning the database for catalog records from page %d to page %d\r\n", pgnoFirst, pgnoLast );		

	popts->psnprog->cunitTotal = pgnoLast;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	
	
	BFPrereadPageRange( ifmp, pgnoFirst, min(cpgPreread * 2,pgnoLast-1) );
	cpgRemaining = cpgPreread;

	while( pgnoLast	!= pgno )
		{
		if( 0 == --cpgRemaining )
			{
			popts->psnprog->cunitDone = pgno;
			(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			if( ( pgno + ( cpgPreread * 2 ) ) < pgnoLast )
				{
				BFPrereadPageRange( ifmp, pgno + cpgPreread, cpgPreread );
				}
			cpgRemaining = cpgPreread;
			}
			
		CSR	csr;
		err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgno,
					bflfNoTouch );

		if( JET_errPageNotInitialized == err )
			{
			err = JET_errSuccess;
			}
		else if( JET_errReadVerifyFailure == err || JET_errDiskIO == err )
			{
			err = JET_errSuccess;
			}
		else if( err >= 0 )
			{
			if( ( 	csr.Cpage().ObjidFDP() == objidFDPMSO
					|| csr.Cpage().ObjidFDP() == objidFDPMSOShadow )
				&& csr.Cpage().FLeafPage()
				&& !csr.Cpage().FSpaceTree()
				&& !csr.Cpage().FEmptyPage()
				&& !csr.Cpage().FRepairedPage()
				&& csr.Cpage().Clines() > 0
				&& csr.Cpage().FPrimaryPage() 
				&& !csr.Cpage().FSLVOwnerMapPage() 	
				&& !csr.Cpage().FSLVAvailPage()  	
				&& !csr.Cpage().FLongValuePage() )
				{
				err = ErrREPAIRIFixLeafPage( 
							ppib, 
							ifmp,
							csr, 
					#ifdef SYNC_DEADLOCK_DETECTION
							NULL,
					#endif  //  SYNC_DEADLOCK_DETECTION
							popts );
				if( err < 0 )
					{
					(*popts->pcprintfError)( "page %d: err %d. discarding page\r\n", pgno, err );

					UtilReportEvent(
							eventWarning,
							REPAIR_CATEGORY,
							REPAIR_BAD_PAGE_ID,
							0, NULL );

					//  this page is not usable. skip it
					
					err = JET_errSuccess;
					}
				else if( 0 == csr.Cpage().Clines() )
					{
					(*popts->pcprintfError)( "page %d: all records were bad. discarding page\r\n", pgno );

					UtilReportEvent(
							eventWarning,
							REPAIR_CATEGORY,
							REPAIR_BAD_PAGE_ID,
							0, NULL );

					//  this page is now empty. skip it
					
					err = JET_errSuccess;
					goto HandleError;
					}					
				else
					{
					//  a non-empty leaf page of one of the catalogs. copy the records into the temp table
					INT iline;
					for( iline = 0;
						iline < csr.Cpage().Clines() && err >= 0;
						++iline )
						{
						KEYDATAFLAGS kdf;
						NDIGetKeydataflags( csr.Cpage(), iline, &kdf );
						if( !FNDDeleted( kdf ) )
							{
							++cRecords;

							//	X5:102291
							//
							//	a ranking violation assert occurs if we attempt
							//	to do the insert with a page latched (thanks
							//	to andygo)
							//
							//	copy the information to a separate page before inserting it
							//
							//	UNDONE:	consider skipping this step in retail as it is
							//	only working around an assert
							
							BYTE rgb[g_cbPageMax];
							BYTE * pb = rgb;
							
							memcpy( pb, kdf.key.prefix.Pv(), kdf.key.prefix.Cb() );
							kdf.key.prefix.SetPv( pb );
							pb += kdf.key.prefix.Cb();

							memcpy( pb, kdf.key.suffix.Pv(), kdf.key.suffix.Cb() );
							kdf.key.suffix.SetPv( pb );
							pb += kdf.key.suffix.Cb();
							
							memcpy( pb, kdf.data.Pv(), kdf.data.Cb() );
							kdf.data.SetPv( pb );
							pb += kdf.data.Cb();

							csr.ReleasePage( fFalse );							

							err = ErrREPAIRInsertCatalogRecordIntoTempTable(
									ppib,
									ifmp,
									kdf,
									tableid,
									columnidKey,
									columnidData,
									popts );
									
							if( JET_errKeyDuplicate == err )
								{
								++cRecordsDuplicate;
								err = JET_errSuccess;
								}

							Call( csr.ErrGetReadPage( 
										ppib, 
										ifmp,
										pgno,
										bflfNoTouch ) );
								
							}
						}
					}
				}			
			}
			
		csr.ReleasePage( fTrue );
		csr.Reset();
		Call( err );
		++pgno;
		}

	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );
	(*popts->pcprintfVerbose)( "%d catalog records found. %d unique\r\n", cRecords, cRecords - cRecordsDuplicate );		

	//  Now we have to insert the records back into the catalog
	(*popts->pcprintf)( "\r\nRebuilding %s.\r\n", szMSO );

	popts->psnprog->cunitTotal = cRecords - cRecordsDuplicate;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	

	Call( ErrREPAIRCopyTempTableToCatalog(
				ppib,
				ifmp,
				tableid,
				columnidKey,
				columnidData,
				popts ) );
		
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertCatalogRecordIntoTempTable(
	PIB * const ppib,
	const IFMP ifmp,
	const KEYDATAFLAGS& kdf,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const JET_SESID sesid = (JET_SESID)ppib;
	
	JET_ERR err = JET_errSuccess;
					
	Call( ErrDispPrepareUpdate(
				sesid,
				tableid,
				JET_prepInsert ) );

	BYTE rgbKey[JET_cbPrimaryKeyMost];
	kdf.key.CopyIntoBuffer( rgbKey, sizeof( rgbKey ) );

	Call( ErrDispSetColumn(
				sesid, 
				tableid, 
				columnidKey,
				rgbKey, 
				kdf.key.Cb(),
				0, 
				NULL ) );

	Call( ErrDispSetColumn(
				sesid, 
				tableid, 
				columnidData,
				kdf.data.Pv(), 
				kdf.data.Cb(),
				0, 
				NULL ) );

	err = ErrDispUpdate( sesid, tableid, NULL, 0, NULL,	0 );
	if( err < 0 )
		{
		CallS( ErrDispPrepareUpdate( sesid, tableid, JET_prepCancel ) );
		}
	
HandleError:
	return err;
	}
	

//  ================================================================
LOCAL ERR ErrREPAIRCopyTempTableToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Copy from the temp table to the catalog. The progress bar should have been initialized 
//  before and should be terminated afterwards.
//
//-
	{
	const PGNO pgnoLast = 23;
	const PGNO cpgOwned = 23 - 3 - 3;	//  3 for system root, 3 for FDP
	Assert( cpgMSOInitial >= cpgOwned );
	const JET_SESID sesid = reinterpret_cast<JET_SESID>( ppib );
	
	JET_ERR	err				= JET_errSuccess;

	FUCB	* pfucbParent	= pfucbNil;
	FUCB	* pfucbCatalog	= pfucbNil;
	FUCB	* pfucbSpace	= pfucbNil;

	//  UNDONE: this could be done in just one buffer, but this makes it easier
	
	VOID * pvKey = NULL;
	BFAlloc( &pvKey );
	VOID * pvData = NULL;
	BFAlloc( &pvData );
	
	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbParent ) );
	Assert( pfucbNil != pfucbParent );

	//  when we create this we cannot make all the pages available, some will be needed later
	//  for the index FDP's. The easiest thing to do is not add any pages to the AvailExt
	Call( ErrSPCreateMultiple(
		pfucbParent,
		pgnoSystemRoot,
		pgnoFDPMSO,
		objidFDPMSO,
		pgnoFDPMSO+1,
		pgnoFDPMSO+2,
		pgnoFDPMSO+2,
		3,
		fTrue,
		CPAGE::fPagePrimary ) );

	DIRClose( pfucbParent );
	pfucbParent = pfucbNil;

	Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
	
	if ( !pfucbCatalog->u.pfcb->FSpaceInitialized() )
		{
		pfucbCatalog->u.pfcb->SetPgnoOE( pgnoFDPMSO+1 );
		pfucbCatalog->u.pfcb->SetPgnoAE( pgnoFDPMSO+2 );
		pfucbCatalog->u.pfcb->SetSpaceInitialized();
		}
	Call( ErrSPIOpenOwnExt( ppib, pfucbCatalog->u.pfcb, &pfucbSpace ) );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
	(*popts->pcprintfDebug)( "%s OwnExt: %d pages ending at %d\r\n", szMSO, cpgOwned, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

	Call( ErrREPAIRInsertRunIntoSpaceTree(
				ppib,
				ifmp,
				pfucbSpace,
				pgnoLast,
				cpgOwned,
				popts ) );

	DIRClose( pfucbSpace );
	pfucbSpace = pfucbNil;

	LONG cRow;
	for( cRow = JET_MoveFirst;
		 ( err = ErrDispMove( sesid, tableid, cRow, NO_GRBIT ) ) == JET_errSuccess;
		 cRow = JET_MoveNext )
		{
		KEY key;
		DATA data;

		ULONG cbKey;
		ULONG cbData;

		Call( ErrDispRetrieveColumn( sesid, tableid, columnidKey, pvKey, g_cbPage, &cbKey, NO_GRBIT, NULL ) );
		Call( ErrDispRetrieveColumn( sesid, tableid, columnidData, pvData, g_cbPage, &cbData, NO_GRBIT, NULL ) );

		key.prefix.Nullify();
		key.suffix.SetPv( pvKey );
		key.suffix.SetCb( cbKey );
		data.SetPv( pvData );
		data.SetCb( cbData );
		
		Call( ErrBTInsert( pfucbCatalog, key, data, fDIRNoVersion|fDIRAppend, NULL ) );
		BTUp( pfucbCatalog );

		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		}
	if( JET_errNoCurrentRecord == err )
		{
		//  we moved off the end of the table
		err = JET_errSuccess;
		}
		
	DIRClose( pfucbCatalog );
	pfucbCatalog = pfucbNil;
			
HandleError:
	BFFree( pvKey );
	BFFree( pvData );
	if( pfucbNil != pfucbParent )
		{
		DIRClose( pfucbParent );
		}
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}
	if( pfucbNil != pfucbSpace )
		{
		DIRClose( pfucbSpace );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairDatabase(
			PIB * const ppib,
			const CHAR * const szDatabase,
			const CHAR * const szSLV,
			const CPG cpgSLV,
			IFMP * const pifmp,
			const OBJID objidLast,
			const PGNO pgnoLastOE,
			REPAIRTABLE * const prepairtable,
			const BOOL fRepairedCatalog,
			BOOL fRepairGlobalSpace,
			const BOOL fRepairSLVSpace,
			TTARRAY * const pttarrayOwnedSpace,
			TTARRAY * const pttarrayAvailSpace,
			TTARRAY * const pttarraySLVAvail,
			TTARRAY * const pttarraySLVChecksumLengths,
			TTARRAY	* const pttarraySLVOwnerMapColumnid,
			TTARRAY	* const pttarraySLVOwnerMapKey,		
			TTARRAY * const pttarraySLVChecksumsFromFile,
			TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	Assert( !(popts->grbit & JET_bitDBUtilOptionDontRepair ) );

	ERR err = JET_errSuccess;

	const JET_SESID sesid = (JET_SESID)ppib;
	
	REPAIRTT repairtt;	
	memset( &repairtt, 0, sizeof( REPAIRTT ) );
	repairtt.tableidBadPages	= JET_tableidNil;
	repairtt.tableidAvailable	= JET_tableidNil;
	repairtt.tableidOwned		= JET_tableidNil;
	repairtt.tableidUsed		= JET_tableidNil;

	REPAIRTABLE * 	prepairtableT			= NULL;
	DBTIME 			dbtimeLast 				= 0;
	const OBJID 	objidFDPMin				= 5; // normal table should have > 5 objidFDP
	OBJID			objidFDPLast			= objidLast; // objidLast from catalog
	PGNO			pgnoLastOESeen			= pgnoLastOE;

	ERR				errRebuildSLVSpaceTrees	= JET_errSuccess;
	INT				cTablesToRepair			= 0;
	
	OBJIDLIST 	objidlist;

	//  get a list of objids we'll be repairing
	
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
		++cTablesToRepair;
		Call( objidlist.ErrAddObjid( prepairtableT->objidFDP ) );
		prepairtableT = prepairtableT->prepairtableNext;
		}
	objidlist.Sort();

	//  scan the database
		
	(*popts->pcprintf)( "\r\nScanning the database.\r\n"  );
	(*popts->pcprintfVerbose).Indent();
	Call( ErrREPAIRAttachForRepair( 
			sesid, 
			szDatabase, 
			szSLV, 
			pifmp, 
			dbtimeLast, 
			objidFDPLast, 
			popts ) );
	Call( ErrREPAIRCreateTempTables( ppib, fRepairGlobalSpace, &repairtt, popts ) );
	Call( ErrREPAIRScanDB(
			ppib,
			*pifmp,
			&repairtt,
			&dbtimeLast,
			&objidFDPLast, 
			&pgnoLastOESeen,
			prepairtable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );
	(*popts->pcprintfVerbose).Unindent();

	if( 0 == dbtimeLast )
		{
		(*popts->pcprintfError)( "dbtimeLast is 0!\r\n" );
		Call( ErrERRCheck( JET_errInternalError ) );
		}	

	if( objidFDPMin > objidFDPLast )
		{
		(*popts->pcprintfError)( "objidLast is %d!\r\n", objidFDPLast );
		Call( ErrERRCheck( JET_errInternalError ) );
		}

	if( pgnoLastOESeen > pgnoLastOE )
		{
		(*popts->pcprintfError)( "Global space tree is too small (has %d pages, seen %d pages). "
								 "The space tree will be rebuilt\r\n",
								 pgnoLastOE, pgnoLastOESeen );
		fRepairGlobalSpace = fTrue;
		}

	//  set sequential access for the temp tables that we will be using
	
	FUCBSetSequential( (FUCB *)(repairtt.tableidAvailable) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidAvailable), cpgPrereadSequential );
	FUCBSetSequential( (FUCB *)(repairtt.tableidOwned) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidOwned), cpgPrereadSequential );
	FUCBSetSequential( (FUCB *)(repairtt.tableidUsed) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidUsed), cpgPrereadSequential );
			
	//  attach and set dbtimeLast and objidLast
	
	Call( ErrREPAIRAttachForRepair( 
			sesid,
			szDatabase, 
			szSLV, 
			pifmp, 
			dbtimeLast, 
			objidFDPLast, 
			popts ) );

	// Check new catalog and add system table entries if they did not exist 
	if( fRepairedCatalog )
		{		
		Call( ErrREPAIRInsertMSOEntriesToCatalog( ppib, *pifmp, popts ) );
		}


	(*popts->pcprintf)( "\r\nRepairing damaged tables.\r\n"  );
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	

	//  for the progress baruse the number of things we are going to repair
	//  (a really bad approximation, but better than nothing)
	
	popts->psnprog->cunitTotal 	= 0;
	popts->psnprog->cunitDone 	= 0;
	popts->psnprog->cunitTotal	= cTablesToRepair;

	if( fRepairGlobalSpace )
		{
		++(popts->psnprog->cunitTotal);
		}

	if( fRepairSLVSpace )
		{
		++(popts->psnprog->cunitTotal);
		}

	if( fRepairGlobalSpace )
		{
		Assert( 0 == popts->psnprog->cunitDone );
		Call( ErrREPAIRRepairGlobalSpace( ppib, *pifmp, popts ) );
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		(VOID)ErrBFFlush( *pifmp );
		}

	//  if necessary, rebuild the SLVSpaceTrees using the tables that are not being repaired
	
	if( fRepairSLVSpace )
		{
		Call( ErrREPAIRRebuildSLVSpaceTrees(
				ppib,
				*pifmp,
				szSLV,
				cpgSLV,
				pttarraySLVAvail,
				pttarraySLVChecksumLengths,
				pttarraySLVOwnerMapColumnid,
				pttarraySLVOwnerMapKey,
				pttarraySLVChecksumsFromFile,
				pttarraySLVChecksumLengthsFromSpaceMap,
				&objidlist,
				popts ) );
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		(VOID)ErrBFFlush( *pifmp );
		}

	(*popts->pcprintfVerbose).Indent();
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
		//  CONSIDER:  multi-thread the repair code. the main issue to deal with is 
		//  the call to DetachDatabase() which purges the FCBs. A more selective
		//  purge call should suffice. Also, any template tables should probably
		//  be repaired first, to avoid changing the FCB of a template table while
		//  a derived table is being repaired
		
		//  we are going to be changing pgnoFDPs so we need to purge all FCBs
		
		FCB::DetachDatabase( *pifmp, fFalse );
		Call( ErrREPAIRRepairTable( ppib, *pifmp, &repairtt, pttarraySLVAvail, prepairtableT, popts ) );

		//  Flush the entire database so that if we crash here we don't have to repair this
		//  table again
		
		(VOID)ErrBFFlush( *pifmp );

		prepairtableT = prepairtableT->prepairtableNext;
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		}
	(*popts->pcprintfVerbose).Unindent();

	//  CONSIDER:  add the pages in the BadPages TT to the OwnExt of a special table
	
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );	
	
HandleError:
	
	if( JET_tableidNil != repairtt.tableidBadPages )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidBadPages ) );
		repairtt.tableidBadPages = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidAvailable )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidAvailable ) );
		repairtt.tableidAvailable = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidOwned )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidOwned ) );
		repairtt.tableidOwned = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidUsed )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidUsed ) );
		repairtt.tableidUsed = JET_tableidNil;
		}

	if ( *pifmp != ifmpMax )
		{
		SLVClose( *pifmp );	
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRDeleteSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const REPAIROPTS * const popts )
//  ================================================================
//
//  The space used by the old trees is not reclaimed, the catalog
//	entries are simply removed.
//
//-
	{
	ERR err = JET_errSuccess;

	err = ErrCATDeleteDbObject( ppib, ifmp, szSLVAvail, sysobjSLVAvail );
	if( err < 0 )
		{
		(*popts->pcprintfVerbose)( "error %d trying to delete the SLVAvail tree\r\n", err );
		}
		
	err = ErrCATDeleteDbObject( ppib, ifmp, szSLVOwnerMap, sysobjSLVOwnerMap );
	if( err < 0 )
		{
		(*popts->pcprintfVerbose)( "error %d trying to delete the SLVOwnerMap tree\r\n", err );
		}
	
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbDb = pfucbNil;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	(*popts->pcprintfVerbose)( "Creating SLV space trees\r\n" );
	
	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbDb ) );
	
	Call( ErrSLVCreateAvailMap( ppib, pfucbDb ) );
	Call( ErrSLVCreateOwnerMap( ppib, pfucbDb ) );

	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVAvail() );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVOwnerMap() );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	if( NULL != pfucbDb )
		{
		DIRClose( pfucbDb );
		}
	if( err < 0 )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const CHAR * const szSLV,
			const CPG cpgSLV,
			TTARRAY * const pttarraySLVAvail,
			TTARRAY * const pttarraySLVChecksumLengths,
			TTARRAY * const pttarraySLVOwnerMapColumnid,
			TTARRAY * const pttarraySLVOwnerMapKey,
			TTARRAY * const pttarraySLVChecksumsFromFile,
			TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
			const OBJIDLIST * const pobjidlist,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  no need to worry about deadlock between these TTARRAYs because they are
	//  only being accessed by this thread for the duration of this function call
	
	TTARRAY::RUN availRun;
	TTARRAY::RUN columnidRun;
	TTARRAY::RUN keyRun;
	TTARRAY::RUN lengthRun;
	TTARRAY::RUN checksumsFromFileRun;
	TTARRAY::RUN checksumsLengthsFromSpaceMapRun;
	
	INT pgno 	= 1;	
	INT ipage 	= 0;

	SLVOWNERMAPNODE			slvownermapNode;

	FCB * pfcbSLVAvail 		= pfcbNil;	
	FUCB * pfucbSLVAvail 	= pfucbNil;
	
	pttarraySLVAvail->BeginRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->BeginRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->BeginRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->BeginRun( ppib, &lengthRun );
	pttarraySLVChecksumsFromFile->BeginRun( ppib, &checksumsFromFileRun );
	pttarraySLVChecksumLengthsFromSpaceMap->BeginRun( ppib, &checksumsLengthsFromSpaceMapRun );

	//	Delete the old SLV trees

	Call( ErrREPAIRDeleteSLVSpaceTrees( ppib, ifmp, popts ) );

	//	re-create the SLV FCBs in the FMP

	Call( ErrREPAIRCreateSLVSpaceTrees( ppib, ifmp, popts ) );

	pfcbSLVAvail = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcbSLVAvail );

	//  Open the SVLAvail Tree
	
	Call( ErrBTOpen( ppib, pfcbSLVAvail, &pfucbSLVAvail, fFalse ) );
	Assert( pfucbNil != pfucbSLVAvail );

	//	These entries were created when we created the SLV trees

	DIB dib;
	dib.pos = posFirst;
	dib.pbm = NULL;
	dib.dirflag = fDIRNull;

	Call( ErrBTDown( pfucbSLVAvail, &dib, latchRIW ) );
	Call( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
	
	while( pgno <= cpgSLVFileMin )
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil == objidOwning
			|| objidInvalid == objidOwning
			|| pobjidlist->FObjidPresent( objidOwning ) )
			{
			//	leave this entry empty
			}
		else
			{
			
			//  used page

			INT 			iul;
			ULONG 			ulChecksum 			= 0;
			ULONG			cbChecksumLength 	= 0;
			ULONG			cbChecksumLengthFromSpaceMap = 0;
			COLUMNID 		columnid;
			ULONG 			rgulKey[culSLVKeyToStore];
			BYTE * const 	pbKey = (BYTE *)rgulKey;

			BOOKMARK bm;			
			bm.key.Nullify();
			bm.key.suffix.SetPv( pbKey + 1 );
			bm.data.Nullify();
			
			Call( pttarraySLVChecksumsFromFile->ErrGetValue( ppib, pgno, &ulChecksum, &checksumsFromFileRun ) );
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			Call( pttarraySLVChecksumLengths->ErrGetValue( ppib, pgno, &cbChecksumLength, &lengthRun ) );
			Call( pttarraySLVChecksumLengthsFromSpaceMap->ErrGetValue(
					ppib,
					pgno,
					&cbChecksumLengthFromSpaceMap,
					&checksumsLengthsFromSpaceMapRun ) );
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			bm.key.suffix.SetCb( *pbKey );

#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
			Assert( bm.key.prefix.FNull() );
			char szBM[256];
						
			REPAIRDumpHex(
				szBM,
				sizeof( szBM ),
				(BYTE *)bm.key.suffix.Pv(),
				bm.key.suffix.Cb() );
			(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d, checksum %x, checksum length %d\r\n",
											pgno,
											objidOwning,
											szBM,
											columnid,
											ulChecksum,
											cbChecksumLength );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING
			
			//  insert the information into the SLVOwnerMap tree
			//	mark the page as used in the SLVAvail tree

			Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objidOwning, columnid, &bm ) );

			if( cbChecksumLength != cbChecksumLengthFromSpaceMap )
				{

				//	the checksum was invalid

				slvownermapNode.ResetFValidChecksum();				
				
				}
			else
				{
				if( 0 != cbChecksumLength )
					{
					slvownermapNode.SetUlChecksum( ulChecksum );
					slvownermapNode.SetCbDataChecksummed( static_cast<USHORT>( cbChecksumLength ) );
					slvownermapNode.SetFValidChecksum();
					}
				else
					{
					slvownermapNode.ResetFValidChecksum();
					}
				}

			//	UNDONE:  bundle these into contigous runs to reduce the number of calls
			//	to ErrBTMungeSLVSpace by increasing the cpages count
			
			Call( ErrBTMungeSLVSpace(
				pfucbSLVAvail,
				slvspaceoperFreeToCommitted,
				ipage,
				1,
				fDIRNull ) );			
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );

			Call( slvownermapNode.ErrSetData( ppib, fTrue ) );		
			}
		++pgno;
		++ipage;
		}

	BTUp( pfucbSLVAvail );
	slvownermapNode.ResetCursor();

	//	these have no entries yet
	
	Assert( cpgSLVFileMin + 1 == pgno );
	while( pgno <= cpgSLV )
		{
		if( ( pgno % SLVSPACENODE::cpageMap ) == 1 )
			{
			
			//  insert a new SLVSPACENODE entry

			const PGNO pgnoLast = pgno + SLVSPACENODE::cpageMap - 1;

			Call( ErrSLVInsertSpaceNode( pfucbSLVAvail, pgnoLast ) );
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			
			ipage = 0;
			}

		Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
		Call( slvownermapNode.ErrNew( ppib ) );
		slvownermapNode.ResetCursor();

		if( objidNil == objidOwning )
			{
			
			//  unused page. create an empty space map entry
			
			}
		else if( objidInvalid == objidOwning )
			{

			//	this page has a bad checksum. create an empty space map entry
			
			}
		else if( pobjidlist->FObjidPresent( objidOwning ) )
			{

			//  this page is used by a corrupted table. we'll fix up these entries while repairing the table
			//  create an empty space map entry

			}
		else
			{
			
			//  used page

			INT 			iul;
			ULONG 			ulChecksum = 0;
			COLUMNID 		columnid;
			ULONG 			rgulKey[culSLVKeyToStore];
			BYTE * const 	pbKey = (BYTE *)rgulKey;

			BOOKMARK bm;			
			bm.key.Nullify();
			bm.key.suffix.SetPv( pbKey + 1 );
			bm.data.Nullify();
			
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			bm.key.suffix.SetCb( *pbKey );

#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
			Assert( bm.key.prefix.FNull() );
			char szBM[256];
			REPAIRDumpHex(
				szBM,
				sizeof( szBM ),
				(BYTE *)bm.key.suffix.Pv(),
				bm.key.suffix.Cb() );
			(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d\r\n",
											pgno,
											objidOwning,
											szBM,
											columnid );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING
			
			//  insert the information into the SLVOwnerMap tree			
			//	mark the page as used in the SLVAvail tree

			Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objidOwning, columnid, &bm ) );
			Call( slvownermapNode.ErrSetData( ppib, fTrue ) );
			slvownermapNode.ResetCursor();

			//	UNDONE:  bundle these into contigous runs to reduce the number of calls
			//	to ErrBTMungeSLVSpace by increasing the cpages count
			
			Call( ErrBTMungeSLVSpace(
				pfucbSLVAvail,
				slvspaceoperFreeToCommitted,
				ipage,
				1,
				fDIRNull ) );			
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			}

		++pgno;
		++ipage;
		}
		
HandleError:

	pttarraySLVAvail->EndRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->EndRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->EndRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->EndRun( ppib, &lengthRun );
	pttarraySLVChecksumsFromFile->EndRun( ppib, &checksumsFromFileRun );
	pttarraySLVChecksumLengthsFromSpaceMap->EndRun( ppib, &checksumsLengthsFromSpaceMapRun );

	if( pfucbNil != pfucbSLVAvail )
		{
		BTClose( pfucbSLVAvail );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairTable(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 	err 				= JET_errSuccess;
	FUCB	* pfucb				= pfucbNil;
	FUCB	* pfucbSLVAvail		= pfucbNil;
	FUCB	* pfucbSLVOwnerMap	= pfucbNil;
	
	FDPINFO	fdpinfo;

	(*popts->pcprintfVerbose)( "repairing table \"%s\"\r\n", prepairtable->szTableName );
	(*popts->pcprintfVerbose).Indent();
	
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	if( prepairtable->fRepairTable )
		{
		(*popts->pcprintfVerbose)( "rebuilding data\r\n" );
		Call( ErrREPAIRRebuildBT(
					ppib,
					ifmp,
					prepairtable,
					pfucbNil,
					&prepairtable->pgnoFDP,
					CPAGE::fPagePrimary | CPAGE::fPageRepair,
					prepairtt,
					popts ) );
		}

	fdpinfo.pgnoFDP 	= prepairtable->pgnoFDP;
	fdpinfo.objidFDP 	= prepairtable->objidFDP;
	Call( ErrFILEOpenTable( ppib, ifmp, &pfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
	
	if( prepairtable->fRepairLV 
		&& objidNil != prepairtable->objidLV )
		{
		(*popts->pcprintfVerbose)( "rebuilding long value tree\r\n" );
		Call( ErrREPAIRRebuildBT(
				ppib,
				ifmp,
				prepairtable,
				pfucb,
				&prepairtable->pgnoLV,
				CPAGE::fPageLongValue | CPAGE::fPageRepair,
				prepairtt,
				popts ) );
		}

	if( prepairtable->fTableHasSLV )
		{
		Call( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVAvail(), &pfucbSLVAvail ) );
		Call( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVOwnerMap(), &pfucbSLVOwnerMap ) );
		}
		
	Call( ErrREPAIRFixupTable( ppib, ifmp, pttarraySLVAvail, pfucbSLVAvail, pfucbSLVOwnerMap, prepairtable, popts ) );

	if( prepairtable->fRepairIndexes )
		{
		(*popts->pcprintfVerbose)( "rebuilding indexes\r\n" );
		Call( ErrREPAIRBuildAllIndexes( ppib, ifmp, &pfucb, prepairtable, popts ) );
		}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

HandleError:

	(*popts->pcprintfVerbose).Unindent();

	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		pfucb = pfucbNil;
		}

	if( pfucbNil != pfucbSLVAvail )
		{
		DIRClose( pfucbSLVAvail );
		pfucbSLVAvail = pfucbNil;
		}

	if( pfucbNil != pfucbSLVOwnerMap )
		{
		DIRClose( pfucbSLVOwnerMap );
		pfucbSLVOwnerMap = pfucbNil;
		}
		
	if( err < 0 )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildBT(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	FUCB * const pfucbTable,
	PGNO * const ppgnoFDP,
	const ULONG fPageFlags,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucb = pfucbNil;
	PGNO	pgnoFDPOld = *ppgnoFDP;
	PGNO	pgnoFDPNew = pgnoNull;

	const OBJID	objidTable	= prepairtable->objidFDP;
	const BOOL fRepairLV	= ( pfucbNil != pfucbTable );
	const OBJID objidFDP	= ( fRepairLV ? prepairtable->objidLV : prepairtable->objidFDP );
	const PGNO pgnoParent	= fRepairLV ? prepairtable->pgnoFDP : pgnoSystemRoot;

	//  we change the pgnoFDP so this cannot be called on system tables
	Assert( !FCATSystemTable( prepairtable->szTableName ) );
	
	Call( ErrREPAIRCreateEmptyFDP(
		ppib,
		ifmp,
		objidFDP,
		pgnoParent,
		&pgnoFDPNew,
		fPageFlags,
		fTrue,	//	data and LV trees are always unique
		popts ) );

if( fRepairLV )
		{
		(*popts->pcprintfDebug)( "LV (%d). new pgnoFDP is %d\r\n", objidFDP, pgnoFDPNew );
		}
	else
		{
		(*popts->pcprintfDebug)( "table %s (%d). new pgnoFDP is %d\r\n", prepairtable->szTableName, objidFDP, pgnoFDPNew );
		}

	Assert( FCB::PfcbFCBGet( ifmp, pgnoFDPOld, NULL, fFalse ) == pfcbNil );
	Assert( FCB::PfcbFCBGet( ifmp, pgnoFDPNew, NULL, fFalse ) == pfcbNil );

	Call( ErrCATChangePgnoFDP(
			ppib,
			ifmp,
			objidTable,
			objidFDP,
			( fRepairLV ? sysobjLongValue : sysobjTable ),
			pgnoFDPNew ) );

	if( !fRepairLV && prepairtable->fHasPrimaryIndex )
		{
		Call( ErrCATChangePgnoFDP(
			ppib,
			ifmp,
			objidTable,
			objidFDP,
			sysobjIndex,
			pgnoFDPNew ) );
		}

	//  at this point:
	//     1.  the system tables have been repaired
	//     2.  we have a global space tree
	//     3.  we have a new (empty) pgnoFDP and space trees
	//     4.  the catalogs have been updated with our new pgnoFDP

	if( !fRepairLV )
		{
		FDPINFO	fdpinfo	= { pgnoFDPNew, objidFDP };
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
		}
	else
		{
		Call( ErrFILEOpenLVRoot( pfucbTable, &pfucb, fFalse ) );
		}
	FUCBSetRepair( pfucb );

	Call( ErrREPAIRRebuildSpace( ppib, ifmp, pfucb, pgnoParent, prepairtt, popts ) );
	Call( ErrREPAIRRebuildInternalBT( ppib, ifmp, pfucb, prepairtt, popts ) );

	*ppgnoFDP = pgnoFDPNew;
	
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateEmptyFDP(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoParent,
	PGNO * const ppgnoFDPNew,
	const ULONG fPageFlags,
	const BOOL fUnique,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgnoOE = pgnoNull;
	PGNO pgnoAE = pgnoNull;
	
	const CPG cpgMin	= cpgMultipleExtentMin;
	CPG cpgRequest		= cpgMin;
	
	FUCB * pfucb = pfucbNil;

	//  the fucb is just used to get the pib so we open it on the parent
	Call( ErrDIROpen( ppib, pgnoParent, ifmp, &pfucb ) );
	if( pgnoNull == *ppgnoFDPNew )
		{
		Call( ErrSPGetExt(
			pfucb,
			pgnoParent,
			&cpgRequest,
			cpgMin,
			ppgnoFDPNew,
			fSPUnversionedExtent ) );
		}
	pgnoOE = *ppgnoFDPNew + 1;
	pgnoAE = pgnoOE + 1;

	(*popts->pcprintfDebug)( "creating new FDP with %d pages starting at %d\r\n", cpgRequest, *ppgnoFDPNew );

	Call( ErrSPCreateMultiple(
		pfucb,
		pgnoParent,
		*ppgnoFDPNew,
		objid,
		pgnoOE,
		pgnoAE,
		*ppgnoFDPNew + cpgRequest - 1,
		cpgRequest,
		fUnique,			//  always unique
		fPageFlags ) );
	
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
class REPAIRRUN
//  ================================================================
	{
	public:
		REPAIRRUN(
			const JET_SESID sesid,
			const JET_TABLEID tableid,
			const JET_COLUMNID columnidFDP,
			const JET_COLUMNID columnidPgno,
			const OBJID objidFDP );
		~REPAIRRUN() {}

		ERR ErrREPAIRRUNInit();
		ERR ErrGetRun( PGNO * const ppgnoLast, CPG * const pcpgRun );

	private:
		const JET_SESID 	m_sesid;
		const JET_TABLEID	m_tableid;
		const JET_COLUMNID	m_columnidFDP;
		const JET_COLUMNID	m_columnidPgno;
		const OBJID			m_objidFDP;
	};

	
//  ================================================================
REPAIRRUN::REPAIRRUN(
			const JET_SESID sesid,
			const JET_TABLEID tableid,
			const JET_COLUMNID columnidFDP,
			const JET_COLUMNID columnidPgno,
			const OBJID objidFDP ) :
//  ================================================================
	m_sesid( sesid ),
	m_tableid( tableid ),
	m_columnidFDP( columnidFDP ),
	m_columnidPgno( columnidPgno ),
	m_objidFDP( objidFDP )
	{
	}
	

//  ================================================================
ERR REPAIRRUN::ErrREPAIRRUNInit()
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Call( ErrIsamMakeKey( m_sesid, m_tableid, &m_objidFDP, sizeof( m_objidFDP ), JET_bitNewKey ) );
	Call( ErrDispSeek( m_sesid, m_tableid, JET_bitSeekGE ) );

HandleError:
	if( JET_errRecordNotFound == err )
		{
		err = JET_errSuccess; 
		}
	return err;
	}


//  ================================================================
ERR REPAIRRUN::ErrGetRun( PGNO * const ppgnoLast, CPG * const pcpgRun )
//  ================================================================
	{
	ERR		err			= JET_errSuccess;
	PGNO 	pgnoStart 	= pgnoNull;
	CPG		cpgRun		= 0;

	while ( err >= JET_errSuccess )
		{
		ULONG	cbActual;
		PGNO	objidCurr;
		PGNO	pgnoCurr;

		err = ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidFDP,
				&objidCurr,
				sizeof( objidCurr ),
				&cbActual,
				NO_GRBIT,
				NULL );
		if ( JET_errNoCurrentRecord == err )
			{
			break;
			}
		Call( err );
		Assert( sizeof( objidCurr ) == cbActual );

		if ( objidCurr != m_objidFDP )
			{
			break;
			}

		Call( ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidPgno,
				&pgnoCurr,
				sizeof( pgnoCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( pgnoCurr ) == cbActual );

		if ( pgnoNull == pgnoStart )
			{
			pgnoStart = pgnoCurr;
			cpgRun = 1;
			}
		else if ( pgnoStart + cpgRun == pgnoCurr )
			{
			//  this is part of a contigous chunk
			++cpgRun;
			}
		else
			{
			Assert( pgnoStart + cpgRun < pgnoCurr );
			break;
			}

		err = ErrDispMove( m_sesid, m_tableid, JET_MoveNext, NO_GRBIT );
		}

	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	*ppgnoLast 	= pgnoStart + cpgRun - 1;
	*pcpgRun	= cpgRun;

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildSpace(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoParent,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;

	const JET_SESID	sesid	= reinterpret_cast<JET_SESID>( ppib );

#ifdef DEBUG
	PGNO	pgnoPrev		= pgnoNull;
	CPG		cpgPrev			= 0;
#endif	//	DEBUG
	
	PGNO 	pgnoLast 		= pgnoNull;
	CPG		cpgRun			= 0;

	FUCB	*pfucbOE 		= pfucbNil;
	FUCB	*pfucbAE 		= pfucbNil;
	FUCB	*pfucbParent	= pfucbNil;

	const OBJID	objidFDP	= pfucb->u.pfcb->ObjidFDP();

	Assert( JET_tableidNil != prepairtt->tableidOwned );
	REPAIRRUN repairrunOwnedExt(
		sesid,
		prepairtt->tableidOwned,
		prepairtt->rgcolumnidOwned[0],
		prepairtt->rgcolumnidOwned[1],
		objidFDP );

	Assert( JET_tableidNil != prepairtt->tableidAvailable );
	REPAIRRUN repairrunAvailExt(
		sesid,
		prepairtt->tableidAvailable,
		prepairtt->rgcolumnidAvailable[0],
		prepairtt->rgcolumnidAvailable[1],
		objidFDP );

	//  The FCB is new so the space should be uninit
	if ( !pfucb->u.pfcb->FSpaceInitialized() )
		{
		pfucb->u.pfcb->SetPgnoOE( pfucb->u.pfcb->PgnoFDP()+1 );
		pfucb->u.pfcb->SetPgnoAE( pfucb->u.pfcb->PgnoFDP()+2 );
		pfucb->u.pfcb->SetSpaceInitialized();
		}
	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );
	Call( ErrSPIOpenAvailExt( ppib, pfucb->u.pfcb, &pfucbAE ) )

	Assert( pgnoNull != pgnoParent );
	if ( pgnoNull != pgnoParent )
		{
		Call( ErrBTOpen( ppib, pgnoParent, ifmp, &pfucbParent ) );
		Assert( pfucbNil != pfucbParent );
		Assert( pfcbNil != pfucbParent->u.pfcb );
		Assert( pfucbParent->u.pfcb->FInitialized() );
		Assert( pcsrNil == pfucbParent->pcsrRoot );
		pfucbParent->pcsrRoot = Pcsr( pfucbParent );
		}

	Call( repairrunOwnedExt.ErrREPAIRRUNInit() );

#ifdef DEBUG
	pgnoPrev = pgnoNull;
	cpgPrev = 0;
#endif	//	DEBUG

	forever 
		{
		Call( repairrunOwnedExt.ErrGetRun( &pgnoLast, &cpgRun ) );
		if( pgnoNull == pgnoLast || 0 == cpgRun )
			{
			break;
			}
		Assert( pgnoLast - cpgRun > pgnoPrev );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "OwnExt:  %d pages ending at %d\r\n", cpgRun, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Call( ErrSPReservePagesForSplit( pfucbOE, pfucbParent ) );
		Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbOE,
					pgnoLast,
					cpgRun,
					popts ) );
		}

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		Assert( pcsrNil != pfucbParent->pcsrRoot );
		pfucbParent->pcsrRoot->ReleasePage();
		pfucbParent->pcsrRoot = pcsrNil;
		BTClose( pfucbParent );
		}
	if( pfucbNil != pfucbOE )
		{
		BTClose( pfucbOE );
		}
	if( pfucbNil != pfucbAE )
		{
		BTClose( pfucbAE );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertRunIntoSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	KEY			key;
	DATA 		data;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( FFUCBSpace( pfucb ) );

	KeyFromLong( rgbKey, pgnoLast );
	key.prefix.Nullify();
	key.suffix.SetPv( rgbKey );
	key.suffix.SetCb( sizeof(pgnoLast) );

	LittleEndian<LONG> le_cpgRun = cpgRun;
	data.SetPv( &le_cpgRun );
	data.SetCb( sizeof(cpgRun) );

	Call( ErrBTInsert( pfucb, key, data, fDIRNoVersion | fDIRNoLog ) );
	BTUp( pfucb );

HandleError:
	Assert( JET_errKeyDuplicate != err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRResetRepairFlags(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const PGNO pgnoFDP = pfucb->u.pfcb->PgnoFDP();
	
	CSR csr;
	CallR( csr.ErrGetRIWPage( ppib, ifmp, pgnoFDP ) );

	//  move down to the leaf level
	while( !csr.Cpage().FLeafPage() )
		{
		NDMoveFirstSon( pfucb, &csr );
		const PGNO pgnoChild	= *(UnalignedLittleEndian< PGNO > *) pfucb->kdfCurr.data.Pv();
		Call( csr.ErrSwitchPage( ppib, ifmp, pgnoChild ) );
		}
	Assert( pgnoNull == csr.Cpage().PgnoPrev() );

	for( ; ; )
		{

		ULONG ulFlags = csr.Cpage().FFlags();
		Assert( ulFlags & CPAGE::fPageLeaf );
		Assert( ulFlags & CPAGE::fPageRepair );
		if( 0 != csr.Cpage().Clines() )
			{
			ulFlags &= ~CPAGE::fPageLeaf;
			ulFlags = ulFlags | CPAGE::fPageParentOfLeaf;
			}
		else
			{
			//  This is an empty tree so this should be the root page
			Assert( csr.Cpage().FRootPage() );
			}
		ulFlags &= ~CPAGE::fPageRepair;
		Assert( !( ulFlags & CPAGE::fPageEmpty ) );
		Assert( !( ulFlags & CPAGE::fPageSpaceTree ) );
		Assert( !( ulFlags & CPAGE::fPageIndex ) );

		const PGNO pgnoNext = csr.Cpage().PgnoNext();

		Call( csr.ErrUpgrade( ) );
		csr.Dirty();
		csr.Cpage().SetPgnoPrev( pgnoNull );
		csr.Cpage().SetPgnoNext( pgnoNull );
		csr.Cpage().SetFlags( ulFlags );
		csr.Downgrade( latchReadNoTouch );
		csr.ReleasePage();
		csr.Reset();

		if( pgnoNull != pgnoNext )
			{
			Call( csr.ErrGetRIWPage( ppib, ifmp, pgnoNext ) );
			}
		else
			{
			break;
			}
		}

HandleError:
	csr.ReleasePage();
	csr.Reset();
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildInternalBT(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  for every used page
	//		compute the separator key
	//		insert the separator key into the new BT
	//		set pgnoPrev and pgnoNext of the page
	//	traverse the leaf level, unlink all the pages and change them to parent of leaf

	const CPG			cpgDatabase	= PgnoLast( ifmp );
	
	const OBJID			objidFDP	= pfucb->u.pfcb->ObjidFDP();
	const JET_SESID		sesid	= reinterpret_cast<JET_SESID>( ppib );
	const JET_TABLEID	tableid	= prepairtt->tableidUsed;
	const JET_COLUMNID	columnidFDP		= prepairtt->rgcolumnidUsed[0];
	const JET_COLUMNID	columnidKey		= prepairtt->rgcolumnidUsed[1];
	const JET_COLUMNID	columnidPgno	= prepairtt->rgcolumnidUsed[2];

	PGNO pgnoPrev = pgnoNull;
	PGNO pgnoNext = pgnoNull;
	PGNO pgnoCurr = pgnoNull;

	CPG	cpgInserted = 0;
	
	ERR	errMove		= JET_errSuccess;

	DIRUp( pfucb );
	
	Call( ErrIsamMakeKey( sesid, tableid, &objidFDP, sizeof( objidFDP ), JET_bitNewKey ) );
	err = ErrDispSeek( sesid, tableid, JET_bitSeekGE );
	if( JET_errRecordNotFound == err )
		{
		err 	= JET_errSuccess;
		errMove = JET_errNoCurrentRecord;
		}
	Call( err );

	while( JET_errSuccess == errMove )
		{
		ULONG	cbKey1	= 0;
		BYTE 	rgbKey1[JET_cbKeyMost];
		ULONG	cbKey2	= 0;
		BYTE 	rgbKey2[JET_cbKeyMost];
		INT 	ibKeyCommon;

		ULONG	cbActual;

		KEY 	key;
		DATA	data;

		OBJID objidFDPCurr;
		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidFDP,
				&objidFDPCurr,
				sizeof( objidFDPCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( objidFDP ) == cbActual );
		if( objidFDP != objidFDPCurr )
			{
			break;
			}

		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidPgno,
				&pgnoCurr,
				sizeof( pgnoCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( pgnoCurr ) == cbActual );
		Assert( pgnoNull == pgnoNext && pgnoNull == pgnoPrev
				|| pgnoCurr == pgnoNext );

		//  assume that the pages for the tree are found close together. preread them
		if( pgnoCurr > pgnoPrev + g_cpgMinRepairSequentialPreread
			&& pgnoCurr + g_cpgMinRepairSequentialPreread < cpgDatabase )
			{
			BFPrereadPageRange( ifmp, pgnoCurr, g_cpgMinRepairSequentialPreread );
			}
			
		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidKey,
				&rgbKey1,
				sizeof( rgbKey1 ),
				&cbKey1,
				NO_GRBIT,
				NULL ) );

NextPage:
		pgnoNext = pgnoNull;
		ibKeyCommon = -1;
		errMove = ErrDispMove( sesid, tableid, JET_MoveNext, NO_GRBIT );
		if( JET_errSuccess != errMove 
			&& JET_errNoCurrentRecord != errMove )
			{
			Call( errMove );
			}
		if( JET_errSuccess == errMove )
			{
			Call( ErrDispRetrieveColumn(
					sesid,
					tableid,
					columnidFDP,
					&objidFDPCurr,
					sizeof( objidFDPCurr ),
					&cbActual,
					NO_GRBIT,
					NULL ) );
			Assert( sizeof( objidFDPCurr ) == cbActual );
			if( objidFDP == objidFDPCurr )
				{
				Call( ErrDispRetrieveColumn(
						sesid,
						tableid,
						columnidPgno,
						&pgnoNext,
						sizeof( pgnoNext ),
						&cbActual,
						NO_GRBIT,
						NULL ) );
				Assert( sizeof( pgnoNext ) == cbActual );

				CSR csr;
				Call( csr.ErrGetReadPage( ppib, ifmp, pgnoNext, bflfNoTouch ) );
				csr.SetILine( 0 );
				KEYDATAFLAGS kdf;
				NDIGetKeydataflags( csr.Cpage(), 0, &kdf );
				cbKey2 = kdf.key.Cb();
				kdf.key.CopyIntoBuffer( rgbKey2, sizeof( rgbKey2 ) );
				csr.ReleasePage( fTrue );
				csr.Reset();

				for( ibKeyCommon = 0;
					 ibKeyCommon < cbKey1 && ibKeyCommon < cbKey2 && rgbKey1[ibKeyCommon] == rgbKey2[ibKeyCommon];
					 ++ibKeyCommon )
					 ;
				if( ibKeyCommon >= cbKey2
					|| ibKeyCommon < cbKey1 && rgbKey1[ibKeyCommon] > rgbKey2[ibKeyCommon] )
					{
					//  this removed inter-page duplicates. it won't remove intra-page duplicates
					(*popts->pcprintfVerbose)
						( "page %d and page %d have duplicate/overlapping keys. discarding page %d\r\n",
							pgnoCurr, pgnoNext, pgnoNext );		
					goto NextPage;
					}
				}
			}

		key.prefix.Nullify();
		key.suffix.SetPv( rgbKey2 );
		key.suffix.SetCb( ibKeyCommon + 1 );	//  turn from ib to cb

		LittleEndian<PGNO> le_pgnoCurr = pgnoCurr;
		data.SetPv( &le_pgnoCurr );
		data.SetCb( sizeof( pgnoCurr ) );

#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "Page %d: pgnoPrev %d, pgnoNext %d, key-length %d\r\n",
			pgnoCurr, pgnoPrev, pgnoNext, key.Cb() );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		++cpgInserted;
		
		if( pgnoNull != pgnoPrev )
			{
			//  we have been through here before

			Call( ErrDIRGet( pfucb ) );
			err = Pcsr( pfucb )->ErrUpgrade();
			
			while( errBFLatchConflict == err )
				{
				//	do a BTUp() so that the append code will do a BTInsert

				CallS( ErrDIRRelease( pfucb ) );
				Call( ErrDIRGet( pfucb ) );
				err = Pcsr( pfucb )->ErrUpgrade();
				}
			Call( err );
			}
		Call( ErrDIRAppend( pfucb, key, data, fDIRNoVersion | fDIRNoLog ) );
		if( pgnoNull != pgnoNext )
			{
			Call( ErrDIRRelease( pfucb ) );
			}
		else
			{
			//  this is our last time through
			DIRUp( pfucb );
			}

		CSR csr;
		Call( csr.ErrGetRIWPage( ppib, ifmp, pgnoCurr ) );
		Call( csr.ErrUpgrade( ) );
		csr.Dirty();
		Assert( csr.Cpage().Pgno() == pgnoCurr );
		Assert( csr.Cpage().ObjidFDP() == objidFDP );
		csr.Cpage().SetPgnoPrev( pgnoPrev );
		csr.Cpage().SetPgnoNext( pgnoNext );
		csr.ReleasePage( fTrue );
		csr.Reset();
		
		pgnoPrev = pgnoCurr;			
		}
		
	CallSx( errMove, JET_errNoCurrentRecord );

	Call( ErrREPAIRResetRepairFlags( ppib, ifmp, pfucb, popts ) );

	(*popts->pcprintfVerbose)( "b-tree rebuilt with %d data pages\r\n", cpgInserted );
	
HandleError:
	Assert( JET_errKeyDuplicate != err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixLVs(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapLVTree,
	const BOOL fFixMissingLVROOT,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Delete LVs that are not complete
//  Re-insert missing LVROOTs
//  Store the LV refcounts in the ttmap
//
//-
	{
	ERR	err = JET_errSuccess;
	
	FUCB * 	pfucb 		= pfucbNil;
	BOOL	fDone		= fFalse;
	LID		lidCurr		= 0;
	INT 	clvDeleted	= 0;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	Assert( pgnoNull != pgnoLV );

	(*popts->pcprintfVerbose)( "fixing long value tree\r\n" );
	
	Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	
	FUCBSetIndex( pfucb );
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	
	err = ErrDIRDown( pfucb, &dib );
	if( JET_errRecordNotFound == err )
		{
		
		// no long values
		
		err = JET_errSuccess;
		fDone = fTrue;
		}
	Call( err );
	
	while( !fDone )
		{
		const LID 	lidOld 		= lidCurr;
		BOOL 		fLVComplete = fFalse;
		BOOL		fLVHasRoot	= fFalse;
		ULONG		ulRefcount	= 0;
		ULONG		ulSize 		= 0;
		
		lidCurr = 0;
		
		Call( ErrREPAIRCheckLV( pfucb, &lidCurr, &ulRefcount, &ulSize, &fLVHasRoot, &fLVComplete, &fDone ) );
		if( !fLVComplete || ( !fLVHasRoot && !fFixMissingLVROOT ) )
			{
			Assert( 0 != lidCurr );
			(*popts->pcprintfVerbose)( "long value %d is not complete. deleting\r\n", lidCurr );
			Call( ErrREPAIRDeleteLV( pfucb, lidCurr ) );
			
			++clvDeleted;
			Call( ErrREPAIRNextLV( pfucb, lidCurr, &fDone ) );
			}
		else
			{
			if( !fLVHasRoot )
				{
				Assert( fFixMissingLVROOT );
				Assert( 0 != ulRefcount );
				Assert( 0 != ulSize );
				
				FUCB * pfucbLVRoot = pfucbNil;
				
				if( !fDone )
					{
					Call( ErrDIRRelease( pfucb ) );
					}
				else
					{
					DIRUp( pfucb );
					}
				
				Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucbLVRoot ) );
				
				(*popts->pcprintfVerbose)( "long value %d has no root. creating a root with refcount %d and size %d\r\n", lidCurr, ulRefcount, ulSize );
				
				err = ErrREPAIRCreateLVRoot( pfucbLVRoot, lidCurr, ulRefcount, ulSize );

				//	close the cursor before checking the error
				
				DIRClose( pfucbLVRoot );
				pfucbLVRoot = pfucbNil;
				
				Call( err );

				if( !fDone )
					{
					Call( ErrDIRGet( pfucb ) );
					}
				}
				
			//  we are already on the next LV
			
			Assert( lidCurr > lidOld );
			if( !fDone )
				{
				Call( ErrDIRRelease( pfucb ) );
				Call( pttmapLVTree->ErrInsertKeyValue( lidCurr, ulRefcount ) );
				Call( ErrDIRGet( pfucb ) );
				}
			else
				{
				DIRUp( pfucb );
				Call( pttmapLVTree->ErrInsertKeyValue( lidCurr, ulRefcount ) );
				}
			}
		}

HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVInfoForUpdateTrees( 
	PIB * const ppib,
	CSLVInfo * const pslvinfo,
	const OBJID objidFDP,
	TTARRAY * const pttarraySLVAvail,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	TTARRAY::RUN availRun;
	pttarraySLVAvail->BeginRun( ppib, &availRun );
	
	Call( pslvinfo->ErrMoveBeforeFirst() );
	while( ( err = pslvinfo->ErrMoveNext() ) == JET_errSuccess )
		{
		CSLVInfo::RUN slvRun;

		Call( pslvinfo->ErrGetCurrentRun( &slvRun ) );

		CPG cpg;
		PGNO pgnoSLVFirst;
		PGNO pgnoSLV;

		cpg 			= slvRun.Cpg();
		pgnoSLVFirst 	= slvRun.PgnoFirst();

		for( pgnoSLV = pgnoSLVFirst; pgnoSLV < pgnoSLVFirst + cpg; ++pgnoSLV )
			{
			OBJID objidOwning;
			Call( pttarraySLVAvail->ErrGetValue( ppib, pgnoSLV, &objidOwning, &availRun ) );

			if( objidInvalid == objidOwning )
				{
				(*popts->pcprintfError)(
					"SLV space allocation error page %d has a bad checksum\r\n",
					pgnoSLV );			
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
				
			if( objidFDP != objidOwning
				&& objidNil != objidOwning )
				{
				(*popts->pcprintfError)(
					"SLV space allocation error page %d is already owned by objid %d\r\n",
					pgnoSLV, objidOwning & 0x7fffffff );			
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			err = pttarraySLVAvail->ErrSetValue( ppib, pgnoSLV, objidFDP | 0x80000000, &availRun );
			if( JET_errRecordNotFound == err )
				{
				(*popts->pcprintfError)( "SLV space allocation error page %d is not in the streaming file\r\n", pgnoSLV );
				err = ErrERRCheck( JET_errDatabaseCorrupted );				
				}
			Call( err ); 								
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:
	pttarraySLVAvail->EndRun( ppib, &availRun );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVAvailFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const CSLVInfo::RUN& slvRun,
	FUCB * const pfucbSLVAvail,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	CPG cpgCurr 		= slvRun.Cpg();
	PGNO pgnoSLVCurr	= slvRun.PgnoFirst();
	
	while( cpgCurr > 0 )
		{
		const INT 	iChunk 			= ( pgnoSLVCurr + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;
		const PGNO 	pgnoSLVChunk 	= iChunk * SLVSPACENODE::cpageMap;
		const CPG 	cpg 			= min( cpgCurr, pgnoSLVChunk - pgnoSLVCurr + 1 );
		const INT	ipage			= SLVSPACENODE::cpageMap - ( pgnoSLVChunk - pgnoSLVCurr ) - 1;
		
		//	seek to the location

		BYTE		rgbKey[sizeof(PGNO)];
		BOOKMARK	bm;

		KeyFromLong( rgbKey, pgnoSLVChunk );
		bm.key.prefix.Nullify();
		bm.key.suffix.SetPv( rgbKey );
		bm.key.suffix.SetCb( sizeof( rgbKey ) );
		bm.data.Nullify();

		DIB dib;
		dib.pos = posDown;
		dib.pbm = &bm;
		dib.dirflag = fDIRNull;

		Call( ErrBTDown( pfucbSLVAvail, &dib, latchRIW ) );
		Call( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
		
		//	write latch the page

		Call( ErrBTMungeSLVSpace(
			pfucbSLVAvail,
			slvspaceoperFreeToCommitted,
			ipage,
			cpg,
			fDIRNull ) );			

		//	not terribly efficent if the run spans multiple chunks, but that should be very rare

		BTUp( pfucbSLVAvail );		

		cpgCurr 	-= cpg;
		pgnoSLVCurr += cpg;

		Assert( 0 == cpgCurr || 0 == ( ( pgnoSLVCurr - 1 ) % SLVSPACENODE::cpageMap ) );
		}			

HandleError:
	BTUp( pfucbSLVAvail );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVOwnerMapFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const COLUMNID columnid,
	const BOOKMARK& bm,
	const CSLVInfo::RUN& slvRun,
	SLVOWNERMAPNODE * const pslvownermapNode,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const PGNO pgnoSLVMin 	= slvRun.PgnoFirst();
	const PGNO pgnoSLVMax	= slvRun.PgnoFirst() + slvRun.Cpg();

	ERR err = JET_errSuccess;
	
	PGNO pgnoSLV;
	for( pgnoSLV = pgnoSLVMin; pgnoSLV < pgnoSLVMax; ++pgnoSLV )
		{
#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
		Assert( bm.key.prefix.FNull() );
		char szBM[256];
		REPAIRDumpHex(
			szBM,
			sizeof( szBM ),
			(BYTE *)bm.key.suffix.Pv(),
			bm.key.suffix.Cb() );
		(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d\r\n",
										pgnoSLV,
										objidFDP,
										szBM,
										columnid );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING

		Call( pslvownermapNode->ErrCreateForSet( ifmp, pgnoSLV, objidFDP, columnid, const_cast<BOOKMARK *>( &bm ) ) );			
		Call( pslvownermapNode->ErrSetData( ppib, fTrue ) );
		pslvownermapNode->NextPage();			
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVTablesFromSLVInfo( 
	PIB * const ppib,
	const IFMP ifmp,
	CSLVInfo * const pslvinfo,
	const BOOKMARK& bm,
	const OBJID objidFDP,
	const COLUMNID columnid,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,	
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR					err			= JET_errSuccess;
	SLVOWNERMAPNODE		slvownermapNode;

	Call( pslvinfo->ErrMoveBeforeFirst() );
	while( ( err = pslvinfo->ErrMoveNext() ) == JET_errSuccess )
		{		
		CSLVInfo::RUN slvRun;
		Call( pslvinfo->ErrGetCurrentRun( &slvRun ) );

		Call( ErrREPAIRUpdateSLVOwnerMapFromSLVRun(
				ppib,
				ifmp,
				objidFDP,
				columnid,
				bm,
				slvRun,
				&slvownermapNode,
				popts ) );

		Call( ErrREPAIRUpdateSLVAvailFromSLVRun(
				ppib,
				ifmp,
				slvRun,
				pfucbSLVAvail,
				popts ) );
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( tagfieldsIterator.FSLV() );

	CSLVInfo slvinfo;
	
	DATA data;
	data.SetPv( const_cast<BYTE *>( tagfieldsIterator.TagfldIterator().PbData() ) );
	data.SetCb( tagfieldsIterator.TagfldIterator().CbData() );

	const BOOKMARK bm = pfucb->bmCurr;

	if( pfucbNil == pfucbSLVAvail
		|| pfucbNil == pfucbSLVOwnerMap )
		{
		(*popts->pcprintfError)( "record (%d:%d) has an unexpected SLV\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	Call( slvinfo.ErrLoadFromData( pfucb, data, tagfieldsIterator.TagfldIterator().FSeparated() ) );			
		
	//	see if any of the pages in the SLV are already used or out of range

	Call( ErrREPAIRCheckSLVInfoForUpdateTrees( pfucb->ppib, &slvinfo, ObjidFDP( pfucb ), pttarraySLVAvail, popts ) );

HandleError:	
	slvinfo.Unload();

	if( JET_errSuccess != err )
		{
		err = ErrERRCheck( JET_errSLVCorrupted );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( tagfieldsIterator.FSLV() );

	CSLVInfo slvinfo;
	
	DATA data;
	data.SetPv( const_cast<BYTE *>( tagfieldsIterator.TagfldIterator().PbData() ) );
	data.SetCb( tagfieldsIterator.TagfldIterator().CbData() );

	const BOOKMARK bm = pfucb->bmCurr;

	Call( slvinfo.ErrLoadFromData( pfucb, data, tagfieldsIterator.TagfldIterator().FSeparated() ) );			

	//	set the ownership info
	
	Call( ErrREPAIRUpdateSLVTablesFromSLVInfo(
			pfucb->ppib,
			pfucb->ifmp,
			&slvinfo,
			bm,
			ObjidFDP( pfucb ),
			columnid,
			pfucbSLVAvail,
			pfucbSLVOwnerMap,
			popts ) );

HandleError:	
	slvinfo.Unload();
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	const TTMAP * const pttmapLVTree,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	while( JET_errSuccess == ( err = tagfldIterator.ErrMoveNext() ) )
		{					
		if( tagfldIterator.FSeparated() )
			{
			const LID lid = LidOfSeparatedLV( tagfldIterator.PbData() );
			
			ULONG ulUnused;
			err = pttmapLVTree->ErrGetValue( lid, &ulUnused );
			if( JET_errRecordNotFound == err )
				{						
				(*popts->pcprintfDebug)( "record (%d:%d) references non-existant LID (%d). deleting\r\n",
											Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine(), lid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			else
				{
				Call( err );
				}
			}
		}
		
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}

				
//  ================================================================
LOCAL ERR ErrREPAIRUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	TTMAP * const pttmapRecords,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	while( JET_errSuccess == ( err = tagfldIterator.ErrMoveNext() ) )
		{					
		if( tagfldIterator.FSeparated() )
			{
			const LID lid = LidOfSeparatedLV( tagfldIterator.PbData() );

			Call( pttmapRecords->ErrIncrementValue( lid ) );
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAddOneCatalogRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const ULONG	objidTable,
	const COLUMNID	columnid,
	const USHORT ibRecordOffset, 
	const ULONG	cbMaxLen,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 			err = JET_errSuccess;
	CHAR			szColumnName[32];
	FIELD			field;

	Assert( objidTable > objidSystemRoot );

	(*popts->pcprintfStats).Indent();


	// Set column name to be a bogus name of the form "JetStub_<objidFDP>_<fid>".
	strcpy( szColumnName, "JetStub_" );
	_ultoa( objidTable, szColumnName + strlen( szColumnName ), 10 );
	Assert( strlen( szColumnName ) < 32 );
	strcat( szColumnName, "_" );
	_ultoa( columnid, szColumnName + strlen( szColumnName ), 10 );
	Assert( strlen( szColumnName ) < 32 );

	//	must zero out to ensure unused fields are ignored
	memset( &field, 0, sizeof(field) );
	
	field.coltyp = JET_coltypNil;
	field.cbMaxLen = cbMaxLen;
	//field.ffield = ffieldDeleted;
	field.ffield = 0;
	field.ibRecordOffset = ibRecordOffset;
	field.cp = 0;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	err = ErrCATAddTableColumn( 
				ppib,
				ifmp,
				objidTable,
				szColumnName,
				columnid,
				&field,
				NULL,
				0,
				NULL,
				NULL,
				0 );

	if( JET_errSuccess == err )
	 	{
	 	(*popts->pcprintfVerbose)( "Inserted a column record (objidTable %d:columnid %d) into catalogs\t\n", objidTable, columnid );
	 	}
	 else
	 	{
	 	if( JET_errColumnDuplicate == err )
	 		{
	 		(*popts->pcprintfVerbose)( "The column record (%d) for table (%d) has already existed in catalogs\t\n", columnid, objidTable );
	 		err = JET_errSuccess;
	 		}
	 	(*popts->pcprintfVerbose)( "Inserting a column record (objidTable %d:columnid %d) into catalogs fails (%d)\t\n", objidTable, columnid, err );
	 	}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	(*popts->pcprintfVerbose).Unindent();

	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertDummyRecordsToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err		= JET_errSuccess;

	FUCB 	*	pfucbTable 				= pfucbNil;
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucbTable, prepairtable->szTableName, NO_GRBIT );

	if( JET_errSuccess != err )
		{
		err = JET_errSuccess;
		}
	else
		{

		if( prepairtable->fidFixedLast > pfucbTable->u.pfcb->Ptdb()->FidFixedLast() )
			{
			COLUMNID	columnidFixedLast = ColumnidOfFid( pfucbTable->u.pfcb->Ptdb()->FidFixedLast(), fFalse );
			
			ULONG colTypeLastOld = 0; 	
			USHORT ibRecordOffsetLastOld = 0; 

			// Get values of colType and ibRecordOffset !
			FUCB		* 	pfucbCatalog 	= pfucbNil;
			USHORT		sysobj 	= sysobjColumn;
	
			Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fFalse ) );
			Assert( pfucbNil != pfucbCatalog );
			Assert( pfucbNil == pfucbCatalog->pfucbCurIndex );
			Assert( !Pcsr( pfucbCatalog )->FLatched() );

			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&(prepairtable->objidFDP),
						sizeof(prepairtable->objidFDP),
						JET_bitNewKey ) );
			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&sysobj,
						sizeof(sysobj),
						NO_GRBIT ) );
			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&(columnidFixedLast),
						sizeof(COLUMNID),
						NO_GRBIT ) );
			err = ErrIsamSeek( ppib, pfucbCatalog, JET_bitSeekEQ );
			Assert( err <= JET_errSuccess );	//	SeekEQ shouldn't return warnings

			if ( JET_errSuccess == err )
				{
				DATA		dataField;
				
				Assert( !Pcsr( pfucbCatalog )->FLatched() );
				Call( ErrDIRGet( pfucbCatalog ) );
				
				Assert( FFixedFid( fidMSO_Coltyp ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_Coltyp,
							pfucbCatalog->kdfCurr.data,
							&dataField ) );
				Assert( dataField.Cb() == sizeof(JET_COLTYP) );
				colTypeLastOld = *( UnalignedLittleEndian< JET_COLTYP > *)dataField.Pv();

				Assert( FFixedFid( fidMSO_RecordOffset ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_RecordOffset,
							pfucbCatalog->kdfCurr.data,
							&dataField ) );
				if ( JET_wrnColumnNull == err )
					{
					Assert( dataField.Cb() == 0 );
					ibRecordOffsetLastOld = 0;		// Set to a dummy value.
					}
				else
					{
					Assert( dataField.Cb() == sizeof(REC::RECOFFSET) );
					ibRecordOffsetLastOld = *(UnalignedLittleEndian< REC::RECOFFSET > *)dataField.Pv();
					}				
				}
			
			if( pfucbNil != pfucbCatalog )
				{
				err =  ErrCATClose( ppib, pfucbCatalog );	
				}
			err = JET_errSuccess;
		

			USHORT cbFixedLastOld = 0;
			switch ( colTypeLastOld )
				{
				case JET_coltypBit:
				case JET_coltypUnsignedByte: 	cbFixedLastOld = 1; break;
				case JET_coltypShort: 			cbFixedLastOld = 2; break;
				case JET_coltypLong:
				case JET_coltypIEEESingle:		cbFixedLastOld = 4; break;
				case JET_coltypCurrency:			
				case JET_coltypIEEEDouble:		
				case JET_coltypDateTime:		cbFixedLastOld = 8; break; 
				default:						cbFixedLastOld = 0; break;
				}
			/* Calculate record offset and max column length
						
			 max column length	= ibEndOfFixedData 
									- offset of fidFixedLastInCAT
									- Length of fidFixedLastInCAT
									- sizeof fixed field bit array
									- (one byte for each fid in between these two fids)

			offset = offset of fidFixedLastInCAT + Length of fidFixedLastInCAT 
						+ (one byte for each fid in between these two fids)
			*/

			ULONG cbMaxLen = prepairtable->ibEndOfFixedData 
							- ibRecordOffsetLastOld 
							- cbFixedLastOld 
							- ( prepairtable->fidFixedLast % 8 ? prepairtable->fidFixedLast/8 + 1 : prepairtable->fidFixedLast/8 )
							- (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 );

			Assert( prepairtable->ibEndOfFixedData >= 
					( ibRecordOffsetLastOld + cbFixedLastOld 
					  + ( prepairtable->fidFixedLast % 8 ? prepairtable->fidFixedLast/8 + 1 : prepairtable->fidFixedLast/8 )
					  + (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 ) ) );
							
			USHORT ibRecordOffset = (USHORT)(ibRecordOffsetLastOld 
							+ cbFixedLastOld
							+ (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 ) );

			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP, 
						prepairtable->fidFixedLast, 
						ibRecordOffset, 
						cbMaxLen, 
						popts );
			}
			
		if( prepairtable->fidVarLast > pfucbTable->u.pfcb->Ptdb()->FidVarLast() )
			{
			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP, 
						prepairtable->fidVarLast, 
						0,
						0, 
						popts );
			}
	
		if( prepairtable->fidTaggedLast > pfucbTable->u.pfcb->Ptdb()->FidTaggedLast() )
			{
			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP,  
						prepairtable->fidTaggedLast, 
						0,
						0,
						popts );
			}

		// ignore errors
		err = JET_errSuccess;
		}
		
HandleError:
	if( pfucbNil != pfucbTable )
		{
		Call( ErrFILECloseTable( ppib, pfucbTable ) );
		pfucbTable = pfucbNil;
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	tagfieldsIterator.MoveBeforeFirst();
	
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		const COLUMNID columnid 	= tagfieldsIterator.Columnid( pfucb->u.pfcb->Ptdb() );		

		if( !tagfieldsIterator.FDerived() )
			{
			prepairtable->fidTaggedLast = max( tagfieldsIterator.Fid(), prepairtable->fidTaggedLast );
			}

		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
			
			if( tagfieldsIterator.FSLV() )
				{
				CallS( tagfieldsIterator.TagfldIterator().ErrMoveNext() );
						
				Call( ErrREPAIRCheckUpdateSLVsFromColumn(
						pfucb,
						columnid,
						tagfieldsIterator,
						pttarraySLVAvail,
						pfucbSLVAvail,
						pfucbSLVOwnerMap,
						popts ) );
				}
			
			if ( tagfieldsIterator.FLV() )
				{
				Call( ErrREPAIRCheckUpdateLVsFromColumn(
						pfucb,
						tagfieldsIterator.TagfldIterator(),
						pttmapLVTree,
						popts ) );						
				}				
			}
		}
			
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	Call( err );

HandleError:
	if( JET_errSLVCorrupted == err )
		{
		(*popts->pcprintfError)( "record (%d:%d) has SLV corruption\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	
	tagfieldsIterator.MoveBeforeFirst();
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		const COLUMNID columnid 	= tagfieldsIterator.Columnid( pfucb->u.pfcb->Ptdb() );
		
		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
		
			if( tagfieldsIterator.FSLV() )
				{
				Assert( pfucbNil != pfucbSLVAvail );
				Assert( pfucbNil != pfucbSLVOwnerMap );

				CallS( tagfieldsIterator.TagfldIterator().ErrMoveNext() );					
				Call( ErrREPAIRUpdateSLVsFromColumn(
						pfucb,
						columnid,
						tagfieldsIterator,
						pttarraySLVAvail,
						pfucbSLVAvail,
						pfucbSLVOwnerMap,
						popts ) );
				}
		
			if ( tagfieldsIterator.FLV() )
				{
				Call( ErrREPAIRUpdateLVsFromColumn(
						pfucb,
						tagfieldsIterator.TagfldIterator(),
						pttmapRecords,
						popts ) );
				}	
			}			
		}
		
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixRecords(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
// 	Delete records that reference non-existant LVs
//	Delete records whose SLV pages are used elsewhere
//	Update the SLV trees
//	Store the correct refcounts in the TTMAP
//
//-
	{
	ERR	err = JET_errSuccess;

	FUCB * 	pfucb 	= pfucbNil;

	INT	crecordDeleted 	= 0;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	(*popts->pcprintfVerbose)( "fixing records\r\n" );

	Call( ErrDIROpen( ppib, pgnoFDP, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	
	FUCBSetIndex( pfucb );
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );

	//	allocate working buffer
	
	Assert( NULL == pfucb->pvWorkBuf );
	RECIAllocCopyBuffer( pfucb );
	
	err = ErrDIRDown( pfucb, &dib );
	
	while( err >= 0 )
		{

		//  CONSIDER:  do we really need to copy the record? Try keeping the page latched
		
		UtilMemCpy( pfucb->dataWorkBuf.Pv(), pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
		
		const REC * const prec 		= (REC *)( pfucb->dataWorkBuf.Pv() );
		const BYTE * const pbRecMax = (BYTE *)( prec ) + pfucb->kdfCurr.data.Cb();

		DATA dataRec;
		dataRec.SetPv( pfucb->dataWorkBuf.Pv() );
		dataRec.SetCb( pfucb->kdfCurr.data.Cb() );
		
		Call( ErrDIRRelease( pfucb ) );
		
		if( prepairtable->fidFixedLast < prec->FidFixedLastInRec() )
			{
			prepairtable->fidFixedLast	= prec->FidFixedLastInRec();
			prepairtable->ibEndOfFixedData 	= prec->IbEndOfFixedData();
			}

		prepairtable->fidVarLast	= max( prec->FidVarLastInRec(), prepairtable->fidVarLast );

		err = ErrREPAIRCheckFixOneRecord(
				ppib,
				ifmp,
				pgnoFDP,
				dataRec,
				pfucb,
				pfucbSLVAvail,
				pfucbSLVOwnerMap,
				pttmapLVTree,
				pttarraySLVAvail,
				prepairtable,
				popts );		
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "record (%d:%d) is corrupted. Deleting\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
			UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_COLUMN_ID, 0, NULL );
			Call( ErrDIRDelete( pfucb, fDIRNoVersion ) );
			++crecordDeleted;
			}
		else if( JET_errSuccess == err )
			{
			Call( ErrREPAIRFixOneRecord(
				ppib,
				ifmp,
				pgnoFDP,
				dataRec,
				pfucb,
				pfucbSLVAvail,
				pfucbSLVOwnerMap,
				pttmapRecords,
				pttarraySLVAvail,
				prepairtable,
				popts ) );					
			}
		Call( err );		
		err = ErrDIRNext( pfucb, fDIRNull );
		}
		
	if( JET_errNoCurrentRecord == err
		|| JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	if( pfucbNil != pfucb )
		{
		Assert( NULL != pfucb->pvWorkBuf );

		//Insert dummy table column records to catalog 
		//if the last column in record is not the last one in TDB
		if( prepairtable->fidFixedLast > pfucb->u.pfcb->Ptdb()->FidFixedLast() 
			|| prepairtable->fidVarLast > pfucb->u.pfcb->Ptdb()->FidVarLast() 
			|| prepairtable->fidTaggedLast > pfucb->u.pfcb->Ptdb()->FidTaggedLast() )
			{
			err = ErrREPAIRInsertDummyRecordsToCatalog( ppib, ifmp, prepairtable, popts );
			}

		RECIFreeCopyBuffer( pfucb ); 
		DIRClose( pfucb );
		}

	return err;
	}		


//  ================================================================
LOCAL ERR ErrREPAIRFixLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapRecords,
	TTMAP * const pttmapLVTree,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Using the two TTMAPs find LVs with the wrong refcount and fix the refcounts
//
//-
	{
	ERR err 		= JET_errSuccess;
	FUCB * pfucb	= pfucbNil;

	(*popts->pcprintfVerbose)( "fixing long value refcounts\r\n" );

	err = pttmapRecords->ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		goto HandleError;
		}
	Call( pttmapLVTree->ErrMoveFirst() );

	Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucb ) );

	for( ;; )
		{
		LID 	lid;
		LID 	lidRecord;
		ULONG	ulRefcount;
		ULONG	ulRefcountRecord;

		Call( pttmapLVTree->ErrGetCurrentKeyValue( (ULONG *)&lid, &ulRefcount ) );
		Call( pttmapRecords->ErrGetCurrentKeyValue( (ULONG *)&lidRecord, &ulRefcountRecord ) );
		if( lid == lidRecord )
			{
			if( ulRefcount != ulRefcountRecord )
				{
				(*popts->pcprintfVerbose)(
					"lid %d has incorrect refcount (refcount is %d, should be %d). correcting\r\n",
					lid,
					ulRefcount,
					ulRefcountRecord
					);			
				Call( ErrREPAIRUpdateLVRefcount( pfucb, lid, ulRefcount, ulRefcountRecord ) );
				}
			err = pttmapRecords->ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				err = JET_errSuccess;
				break;
				}
			}
		Call( pttmapLVTree->ErrMoveNext() );
		}
		
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixupTable(
	PIB * const ppib,
	const IFMP ifmp,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err 		= JET_errSuccess;
	FUCB * pfucb	= pfucbNil;
	
	const PGNO	pgnoFDP		= prepairtable->pgnoFDP;
	const PGNO	pgnoLV		= prepairtable->pgnoLV;

	TTMAP ttmapLVTree( ppib );
	TTMAP ttmapRecords( ppib );
	
	Call( ttmapLVTree.ErrInit( PinstFromPpib( ppib ) ) );
	Call( ttmapRecords.ErrInit( PinstFromPpib( ppib ) ) );

	if( pgnoNull != pgnoLV )
		{
		Call( ErrREPAIRFixLVs( ppib, ifmp, pgnoLV, &ttmapLVTree, fTrue, prepairtable, popts ) );
		}

	Call( ErrREPAIRFixRecords(
			ppib,
			ifmp,
			pgnoFDP,
			pfucbSLVAvail,
			pfucbSLVOwnerMap,
			&ttmapRecords,
			&ttmapLVTree,
			pttarraySLVAvail,
			prepairtable,
			popts ) );
	Call( ErrREPAIRFixLVRefcounts( ppib, ifmp, pgnoLV, &ttmapRecords, &ttmapLVTree, prepairtable, popts ) );
		
HandleError:		
	return err;
	}


//  ================================================================
LOCAL BOOL FREPAIRIdxsegIsUserDefinedColumn(
	const IDXSEG idxseg,
	const FCB * const pfcbIndex )
//  ================================================================
	{
	const COLUMNID		columnid	= idxseg.Columnid();
	const TDB * const	ptdb		= pfcbIndex->PfcbTable()->Ptdb();
	const FIELD * const	pfield 		= ptdb->Pfield( columnid );

	return FFIELDUserDefinedDefault( pfield->ffield );
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateEmptyIndexes(
		PIB * const ppib,
		const IFMP ifmp,
		const PGNO pgnoFDPTable,
		FCB * const pfcbTable,
		const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 		err 		= JET_errSuccess;
	FCB		* 	pfcbIndex	= NULL;
	
	for( pfcbIndex = pfcbTable->PfcbNextIndex();
		pfcbNil != pfcbIndex;
		pfcbIndex = pfcbIndex->PfcbNextIndex() )
		{
		const BOOL fNonUnique 	= pfcbIndex->FNonUnique();
		const OBJID objidIndex 	= pfcbIndex->ObjidFDP();
		const PGNO pgnoFDPOld	= pfcbIndex->PgnoFDP();
		PGNO pgnoFDPNew			= pgnoNull;

		if( pgnoFDPMSO_NameIndex == pgnoFDPOld
			|| pgnoFDPMSO_RootObjectIndex == pgnoFDPOld )
			{
			pgnoFDPNew = pgnoFDPOld;
			}
		
		Call( ErrREPAIRCreateEmptyFDP(
			ppib,
			ifmp,
			pfcbIndex->ObjidFDP(),
			pgnoFDPTable,
			&pgnoFDPNew,
			CPAGE::fPageIndex,
			!fNonUnique,
			popts ) );

		if( pgnoFDPMSO_NameIndex != pgnoFDPOld
			&& pgnoFDPMSO_RootObjectIndex != pgnoFDPOld )
			{
			Call( ErrCATChangePgnoFDP(
					ppib,
					ifmp,
					pfcbTable->ObjidFDP(),
					pfcbIndex->ObjidFDP(),
					sysobjIndex,
					pgnoFDPNew ) );
			}
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRBuildAllIndexes(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB ** const ppfucb,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err			= JET_errSuccess;

	FCB		* pfcbTable	= (*ppfucb)->u.pfcb;
	FDPINFO	fdpinfo;

	Call( ErrREPAIRCreateEmptyIndexes( ppib, ifmp, prepairtable->pgnoFDP, pfcbTable, popts ) );
		
	//  close the table, purge FCBs and re-open to get the new PgnoFDPs
	
	DIRClose( *ppfucb );
	*ppfucb = pfucbNil;
	
	pfcbTable->PrepareForPurge();
	pfcbTable->Purge();
	

	fdpinfo.pgnoFDP = prepairtable->pgnoFDP;
	fdpinfo.objidFDP = prepairtable->objidFDP;
	Call( ErrFILEOpenTable( ppib, ifmp, ppfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
	FUCBSetIndex( *ppfucb );

	if( pfcbNil != (*ppfucb)->u.pfcb->PfcbNextIndex() )
		{
		Call( ErrFILEBuildAllIndexes( ppib, *ppfucb, (*ppfucb)->u.pfcb->PfcbNextIndex(), NULL ) );
		}

HandleError:
	if( pfucbNil != *ppfucb )
		{
		DIRClose( *ppfucb );
		*ppfucb = pfucbNil;
		}
		
	return err;
	}


//  ================================================================
RECCHECK::RECCHECK()
//  ================================================================
	{
	}

	
//  ================================================================
RECCHECK::~RECCHECK()
//  ================================================================
	{
	}


//  ================================================================
RECCHECKTABLE::RECCHECKTABLE(
	const OBJID objid,
	FUCB * const pfucb,
	const FIDLASTINTDB fidLastInTDB,
	TTMAP * const pttmapLVRefcounts,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,		
	TTARRAY * const pttarraySLVChecksumLengths,
	const REPAIROPTS * const popts ) :
//  ================================================================
	m_objid( objid ),
	m_pfucb( pfucb ),
	m_fidLastInTDB( fidLastInTDB ),
	m_pttmapLVRefcounts( pttmapLVRefcounts ),
	m_pttarraySLVAvail( pttarraySLVAvail ),
	m_pttarraySLVOwnerMapColumnid( pttarraySLVOwnerMapColumnid ),
	m_pttarraySLVOwnerMapKey( pttarraySLVOwnerMapKey ),
	m_pttarraySLVChecksumLengths( pttarraySLVChecksumLengths ),
	m_popts( popts )
	{
	}


//  ================================================================
RECCHECKTABLE::~RECCHECKTABLE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckRecord_( 
	const KEYDATAFLAGS& kdf, 
	const BOOL fCheckLVSLV )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  Check the basic REC structure
	
	Call( ErrCheckREC_( kdf ) );

	//  Check the fixed columns
	
	Call( ErrCheckFixedFields_( kdf ) );

	//  Check variable columns
	
	Call( ErrCheckVariableFields_( kdf ) );

	//	Check tagged columns
	
	Call( ErrCheckTaggedFields_( kdf ) );
	
	if( fCheckLVSLV)
		{
		//Check LV and SLV columns
		
		Call( ErrCheckLVAndSLVFields_( kdf ) );
		}
		
HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckREC_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<BYTE *>( kdf.data.Pv() ) + kdf.data.Cb();
	
	//  sanity check the pointers in the records
	
	if( prec->PbFixedNullBitMap() > pbRecMax )	//  can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbFixedNullBitMap is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	if( prec->PbVarData() > pbRecMax )	//	can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbVarData is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if(	prec->PbTaggedData() > pbRecMax )	//	can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbTaggedData is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

HandleError:
	return err;
	}

//  ================================================================
ERR RECCHECKTABLE::ErrCheckFixedFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const FID fidFixedLast  = prec->FidFixedLastInRec();

	// Check if the last fixed column info in catalog is correct
	if( fidFixedLast > m_fidLastInTDB.fidFixedLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last fixed column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidFixedLast, m_fidLastInTDB.fidFixedLastInTDB );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) ); 
		}
HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckVariableFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<BYTE *>( kdf.data.Pv() ) + kdf.data.Cb();
	
	const FID fidVariableFirst = fidVarLeast ;
	const FID fidVariableLast  = prec->FidVarLastInRec();
	const INT cColumnsVariable = max( 0, fidVariableLast - fidVariableFirst + 1 );
	const UnalignedLittleEndian<REC::VAROFFSET> * const pibVarOffs		= prec->PibVarOffsets();

	FID fid;

	REC::VAROFFSET ibStartOfColumnPrev = 0;
	REC::VAROFFSET ibEndOfColumnPrev = 0;

	//  check the variable columns
	
	for( fid = fidVariableFirst; fid <= fidVariableLast; ++fid )
		{
		const UINT				ifid			= fid - fidVarLeast;
		const REC::VAROFFSET	ibStartOfColumn	= prec->IbVarOffsetStart( fid );
		const REC::VAROFFSET	ibEndOfColumn	= IbVarOffset( pibVarOffs[ifid] );

		if( ibEndOfColumn < ibStartOfColumn )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, start: %d, end: %d)\r\n",
					fid, ibStartOfColumn, ibEndOfColumn );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
			
		if( ibStartOfColumn < ibStartOfColumnPrev )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, start: %d, startPrev: %d)\r\n",
					fid, ibStartOfColumn, ibStartOfColumnPrev );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		ibStartOfColumnPrev = ibStartOfColumn;

		if( ibEndOfColumn < ibEndOfColumnPrev )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, end: %d, endPrev: %d)\r\n",
					fid, ibEndOfColumn, ibEndOfColumnPrev );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		ibEndOfColumnPrev = ibEndOfColumn;
			
		if ( FVarNullBit( pibVarOffs[ifid] ) )
			{
			if( ibStartOfColumn != ibEndOfColumn )
				{
				const INT cbColumn				= ibEndOfColumn - ibStartOfColumn;
				(*m_popts->pcprintfError)(
					"Record corrupted: NULL variable field not zero length (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		else
			{
			const BYTE * const pbColumn 	= prec->PbVarData() + ibStartOfColumn;
			const INT cbColumn				= ibEndOfColumn - ibStartOfColumn;
			const BYTE * const pbColumnEnd 	= pbColumn + cbColumn;

			if( pbColumn >= pbRecMax )
				{
				(*m_popts->pcprintfError)(
					"Record corrupted: variable field is not in record (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}

			if( pbColumnEnd > pbRecMax )	//	can point to the end of the record
				{
				(*m_popts->pcprintfError)(
					"Record corrupted: variable field is too long (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		}		

	// Check if the last variable column info in catalog is correct
	if( fidVariableLast > m_fidLastInTDB.fidVarLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last variable column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidVariableLast, m_fidLastInTDB.fidVarLastInTDB );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) ); 
		}


HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckSeparatedLV_(
					const KEYDATAFLAGS& kdf,
					const COLUMNID columnid,
					const ULONG itagSequence,
					const LID lid )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( m_pttmapLVRefcounts )
		{
#ifdef SYNC_DEADLOCK_DETECTION
		COwner* const pownerSaved = Pcls()->pownerLockHead;
		Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION

		Call( m_pttmapLVRefcounts->ErrIncrementValue( lid ) );
		
#ifdef SYNC_DEADLOCK_DETECTION
		Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION		
		}
	else
		{
		(*m_popts->pcprintfError)( "separated LV was not expected (columnid: %d, itag: %d, lid: %d)\r\n",
			columnid, itagSequence, lid );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}

HandleError:
	return err;
	}

//  ================================================================
ERR RECCHECKTABLE::ErrCheckIntrinsicLV_(
		const KEYDATAFLAGS& kdf,
		const COLUMNID columnid,
		const ULONG itagSequence,
		const DATA& dataLV )
//  ================================================================
	{
	return JET_errSuccess;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckSLV_(
	const KEYDATAFLAGS& kdf, 
	const COLUMNID columnid,
	const ULONG itagSequence,
	const DATA& dataSLV,
	const BOOL	fSeparatedSLV
	)
//  ================================================================
	{
	ERR err = JET_errSuccess;
			
	CSLVInfo 			slvinfo;
	CSLVInfo::HEADER 	header;

	//  we need to be vigilant against deadlocks between the TTARRAYs because
	//  this function is called by multiple concurrent threads.  to this end,
	//  the TTARRAY runs must be accessed according to the hierarchy given
	//  below.  any lower level TTARRAY must have any outstanding runs ended
	//  before accessing a higher level TTARRAY
	//
	//      m_pttarraySLVAvail
	//      m_pttarraySLVOwnerMapColumnid
	//      m_pttarraySLVOwnerMapKey
	//      m_pttarraySLVChecksumLengths

	TTARRAY::RUN availRun;
	TTARRAY::RUN keyRun;

	QWORD	cbSLV;

	if( NULL == m_pttarraySLVAvail )
		{
		(*m_popts->pcprintfError)( "SLV was not expected (columnid: %d, itag: %d)\r\n",
									columnid, itagSequence );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}
	Assert( NULL != m_pttarraySLVOwnerMapColumnid );
	Assert( NULL != m_pttarraySLVOwnerMapKey );

	if ( fSeparatedSLV )
		{
		//  Separated SLV

///		(*m_popts->pcprintfVerbose)( "separated SLV: %d bytes (columnid: %d, itag: %d)\r\n",
///										dataSLV.Cb(), columnid, itagSequence );

		}
	Call( slvinfo.ErrLoadFromData( m_pfucb, dataSLV, fSeparatedSLV ) );

	Call( slvinfo.ErrGetHeader( &header ) );

	cbSLV = header.cbSize;

	Call( slvinfo.ErrMoveBeforeFirst() );

	m_pttarraySLVAvail->BeginRun( m_pfucb->ppib, &availRun );

	while( ( err = slvinfo.ErrMoveNext() ) == JET_errSuccess )
		{
		CSLVInfo::RUN slvRun;

		Call( slvinfo.ErrGetCurrentRun( &slvRun ) );

		CPG cpg;
		PGNO pgnoSLVFirst;
		PGNO pgnoSLV;

		cpg 			= slvRun.Cpg();
		pgnoSLVFirst 	= slvRun.PgnoFirst();

		for( pgnoSLV = pgnoSLVFirst; pgnoSLV < pgnoSLVFirst + cpg; ++pgnoSLV )
			{
			OBJID objidOwning;
			
			Call( m_pttarraySLVAvail->ErrGetValue( m_pfucb->ppib, pgnoSLV, &objidOwning, &availRun ) );

			if( objidNil != objidOwning )
				{
				(*m_popts->pcprintfError)(
					"SLV space allocation error page %d is already owned by %d (columnid: %d, itag: %d, objid: %d)\r\n",
					pgnoSLV, columnid, itagSequence, objidOwning, m_objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );				
				}
				
			err = m_pttarraySLVAvail->ErrSetValue( m_pfucb->ppib, pgnoSLV, m_objid, &availRun );
			if( JET_errRecordNotFound == err )
				{
				(*m_popts->pcprintfError)(
					"SLV space allocation error page %d is not in the streaming file (columnid: %d, itag: %d, objid: %d)\r\n",
					pgnoSLV, columnid, itagSequence, m_objid );
				err = ErrERRCheck( JET_errDatabaseCorrupted );				
				}
			Call( err ); 	

			//  store the COLUMNID
			
			Call( m_pttarraySLVOwnerMapColumnid->ErrSetValue( m_pfucb->ppib, pgnoSLV, columnid ) );

			//	store the checksum length

			ULONG cbChecksumLength;

			cbChecksumLength 	= static_cast<ULONG>( min( g_cbPage, cbSLV ) );

			cbSLV = ( cbSLV > g_cbPage ? cbSLV - g_cbPage : 0 );

			Call( m_pttarraySLVChecksumLengths->ErrSetValue( m_pfucb->ppib, pgnoSLV, cbChecksumLength ) );
			
			//  store the first 'n' bytes of the key (pad with 0)
			
			ULONG rgulKey[culSLVKeyToStore];
			BYTE * const pbKey = (BYTE *)rgulKey;
			memset( rgulKey, 0, sizeof( rgulKey ) );

			*pbKey = BYTE( kdf.key.Cb() );

			if( kdf.key.Cb() > sizeof( rgulKey ) - 1 )
				{
				(*m_popts->pcprintfError)(
					"INTERNAL ERROR: key of SLV-owning record is too large (%d bytes, buffer is %d bytes)\r\n",
					*pbKey, sizeof( rgulKey ) - 1 );
				Call( ErrERRCheck( JET_errInternalError ) );				
				}
				
			kdf.key.CopyIntoBuffer( pbKey+1, sizeof( rgulKey )-1 );

			TTARRAY::RUN keyRun;
			m_pttarraySLVOwnerMapKey->BeginRun( m_pfucb->ppib, &keyRun );
			
			INT iul;
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( m_pttarraySLVOwnerMapKey->ErrSetValue(
											m_pfucb->ppib,
											pgnoSLV * culSLVKeyToStore + iul,
											rgulKey[iul],
											&keyRun ) );
				}			
			m_pttarraySLVOwnerMapKey->EndRun( m_pfucb->ppib, &keyRun );
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:

	m_pttarraySLVAvail->EndRun( m_pfucb->ppib, &availRun );
	m_pttarraySLVOwnerMapKey->EndRun( m_pfucb->ppib, &keyRun );
	
	slvinfo.Unload();

	if( JET_errSLVCorrupted == err )
		{
		(*m_popts->pcprintfError)( "SLV corruption\r\n" );
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	return err;
	}
	

//  ================================================================
ERR RECCHECKTABLE::ErrCheckTaggedFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR 		err 			= JET_errSuccess;

	if ( !TAGFIELDS::FIsValidTagfields( kdf.data, m_popts->pcprintfError ) )
		{
		return ErrERRCheck( JET_errDatabaseCorrupted );
		}

	
	FID		fidTaggedLast 		= fidTaggedLeast - 1;
	VOID * 	pvWorkBuf;

	BFAlloc( &pvWorkBuf );

	BYTE * 	pb = (BYTE *)pvWorkBuf;
	UtilMemCpy( pb, kdf.data.Pv(), kdf.data.Cb() );
		
	DATA dataRec;
	dataRec.SetPv( pb );		
	dataRec.SetCb( kdf.data.Cb() );
		
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	tagfieldsIterator.MoveBeforeFirst();
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		if( !tagfieldsIterator.FDerived() )
			{
			fidTaggedLast = max( tagfieldsIterator.Fid(), fidTaggedLast );
			}
			
		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
		
			if( tagfieldsIterator.FSLV() )
				{
				Call( tagfieldsIterator.TagfldIterator().ErrMoveNext() );		
				}
			}
		}
			
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:
	BFFree( pvWorkBuf );

	// Check if the last tagged column info in catalog is correct
	if( fidTaggedLast > m_fidLastInTDB.fidTaggedLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last tagged column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidTaggedLast, m_fidLastInTDB.fidTaggedLastInTDB );
		return ErrERRCheck( JET_errDatabaseCorrupted );
		}

	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckLVAndSLVFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	// Form ErrCheckTaggedFields_(), we have already 
	// known that FIsValidTagfields is TRUE
	Assert( TAGFIELDS::FIsValidTagfields( kdf.data, m_popts->pcprintfError ) ); 

	TAGFIELDS	tagfields( kdf.data );
	return tagfields.ErrCheckLongValuesAndSLVs( kdf, this );
	}


//  ================================================================
ERR RECCHECKTABLE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	return ErrCheckRecord_( kdf );
	}


//  ================================================================
RECCHECKSLVSPACE::RECCHECKSLVSPACE( const IFMP ifmp, const REPAIROPTS * const popts ) :
//  ================================================================
	m_popts( popts ),
	m_ifmp( ifmp ),
	m_cpagesSeen( 0 )
	{
	}


//  ================================================================
RECCHECKSLVSPACE::~RECCHECKSLVSPACE()
//  ================================================================
	{
	}



//  ================================================================
ERR RECCHECKSLVSPACE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const SLVSPACENODE * const pslvspacenode = (SLVSPACENODE *)kdf.data.Pv();

	if( kdf.key.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "SLV space tree key is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	if( kdf.data.Cb() != sizeof( SLVSPACENODE ) )
		{
		(*m_popts->pcprintfError)( "SLV space tree data is incorrect size (%d bytes, expected %d)\r\n", kdf.data.Cb(), sizeof( SLVSPACENODE ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	m_cpagesSeen = m_cpagesSeen + SLVSPACENODE::cpageMap;

	ULONG pgnoCurr;
	LongFromKey( &pgnoCurr, kdf.key );
	if( m_cpagesSeen != pgnoCurr )
		{
		(*m_popts->pcprintfError)( "SLV space tree nodes out of order (pgnoCurr %d, expected %d)\r\n", pgnoCurr, m_cpagesSeen );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	Call( pslvspacenode->ErrCheckNode( m_popts->pcprintfError ) );
		
HandleError:
	return err;
	}


//  ================================================================
RECCHECKSLVOWNERMAP::RECCHECKSLVOWNERMAP(
	const REPAIROPTS * const popts ) :
//  ================================================================
	m_popts( popts )
	{
	}


//  ================================================================
RECCHECKSLVOWNERMAP::~RECCHECKSLVOWNERMAP()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSLVOWNERMAP::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if ( !SLVOWNERMAP::FValidData( kdf.data ) )
		{
		(*m_popts->pcprintfError)( "SLV space map node corrupted.\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	else
		{
		SLVOWNERMAP	slvownermap;
		slvownermap.Retrieve( kdf.data );

		const OBJID	objid				= slvownermap.Objid();
		if( objidNil != objid )
			{
			const COLUMNID columnid		= slvownermap.Columnid();
			const USHORT cbKey			= slvownermap.CbKey();
			const BOOL fValidChecksum 	= slvownermap.FValidChecksum();
			if( 0 == columnid )
				{
				(*m_popts->pcprintfError)( "SLV space map node invalid columnid (%d).\r\n", columnid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			if( cbKey > JET_cbKeyMost )
				{
				(*m_popts->pcprintfError)( "SLV space map node key is too long (%d bytes).\r\n", cbKey );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			if( fValidChecksum )
				{
				const ULONG ulChecksum 			= slvownermap.UlChecksum();
				const ULONG cbChecksumLength	= slvownermap.CbDataChecksummed();
				if( cbChecksumLength > g_cbPage )
					{
					(*m_popts->pcprintfError)( "SLV space map node checksum length is too large (%d bytes).\r\n", cbChecksumLength );
					Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
					}
				}
			}
		}
		
HandleError:
	return err;
	}


//  ================================================================
RECCHECKMACRO::RECCHECKMACRO() :
//  ================================================================
	m_creccheck( 0 )
	{
	memset( m_rgpreccheck, 0, sizeof( m_rgpreccheck ) );
	}

	
//  ================================================================
RECCHECKMACRO::~RECCHECKMACRO()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKMACRO::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	INT ipreccheck;
	for( ipreccheck = 0; ipreccheck < m_creccheck; ipreccheck++ )
		{
		Call( (*m_rgpreccheck[ipreccheck])( kdf ) );
		}

HandleError:
	return err;
	}


//  ================================================================
VOID RECCHECKMACRO::Add( RECCHECK * const preccheck )
//  ================================================================
	{
	m_rgpreccheck[m_creccheck++] = preccheck;
	Assert( m_creccheck < ( sizeof( m_rgpreccheck ) / sizeof( preccheck ) ) );
	}


//  ================================================================
RECCHECKSPACE::RECCHECKSPACE( PIB * const ppib, const REPAIROPTS * const popts ) :
//  ================================================================
	m_ppib( ppib ),
	m_popts( popts ),
	m_pgnoLast( pgnoNull ),
	m_cpgLast( 0 ),
	m_cpgSeen( 0 )
	{
	}


//  ================================================================
RECCHECKSPACE::~RECCHECKSPACE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgno = pgnoNull;
	PGNO pgnoT = pgnoNull;
	CPG cpg = 0;
	
	if( kdf.key.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "space tree key is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( kdf.data.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "space tree data is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( FNDVersion( kdf ) )
		{
		(*m_popts->pcprintfError)( "versioned node in space tree\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	LongFromKey( &pgno, kdf.key );
	cpg = *(UnalignedLittleEndian< CPG > *)kdf.data.Pv();

	if( pgno <= m_pgnoLast )
		{
		(*m_popts->pcprintfError)( "space tree corruption (previous pgno %d, current pgno %d)\r\n", m_pgnoLast, pgno );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( ( pgno - cpg ) < m_pgnoLast )
		{
		(*m_popts->pcprintfError)( "space tree overlap (previous extent was %d:%d, current extent is %d:%d)\r\n",
									m_pgnoLast, m_cpgLast, pgno, cpg );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	m_pgnoLast = pgno;
	m_cpgLast = cpg;
	m_cpgSeen += cpg;
		
HandleError:
	return err;
	}


//  ================================================================
RECCHECKSPACEOE::RECCHECKSPACEOE(
	PIB * const ppib,
	TTARRAY * const pttarrayOE,
	const OBJID objid,
	const OBJID objidParent,
	const REPAIROPTS * const popts ) :
//  ================================================================
	RECCHECKSPACE( ppib, popts ),
	m_pttarrayOE( pttarrayOE ),
	m_objid( objid ),
	m_objidParent( objidParent )
	{
	}


//  ================================================================
RECCHECKSPACEOE::~RECCHECKSPACEOE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACEOE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CallR( RECCHECKSPACE::operator()( kdf ) );

	Assert( sizeof( PGNO ) == kdf.key.Cb() );
	PGNO pgnoLast;
	LongFromKey( &pgnoLast, kdf.key );
	
	Assert( sizeof( CPG ) == kdf.data.Cb() );
	const CPG cpgRun = *(UnalignedLittleEndian< CPG > *)(kdf.data.Pv());

	CallR( ErrREPAIRInsertOERunIntoTT(
		m_ppib,
		pgnoLast,
		cpgRun,
		m_objid,
		m_objidParent,
		m_pttarrayOE,
		m_popts ) );
		
	return err;
	}


//  ================================================================
RECCHECKSPACEAE::RECCHECKSPACEAE(
	PIB * const ppib,
	TTARRAY * const pttarrayOE,
	TTARRAY * const pttarrayAE,
	const OBJID objid,
	const OBJID objidParent,
	const REPAIROPTS * const popts ) :
//  ================================================================
	RECCHECKSPACE( ppib, popts ),
	m_pttarrayOE( pttarrayOE ),
	m_pttarrayAE( pttarrayAE ),
	m_objid( objid ),
	m_objidParent( objidParent )
	{
	}


//  ================================================================
RECCHECKSPACEAE::~RECCHECKSPACEAE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACEAE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CallR( RECCHECKSPACE::operator()( kdf ) );

	Assert( sizeof( PGNO ) == kdf.key.Cb() );
	PGNO pgnoLast;
	LongFromKey( &pgnoLast, kdf.key );
	
	Assert( sizeof( CPG ) == kdf.data.Cb() );
	const CPG cpgRun = *(UnalignedLittleEndian< CPG > *)(kdf.data.Pv());	

	CallR( ErrREPAIRInsertAERunIntoTT(
		m_ppib,
		pgnoLast,
		cpgRun,
		m_objid,
		m_objidParent,
		m_pttarrayOE,
		m_pttarrayAE,
		m_popts ) );

	return err;
	}


//  ================================================================
REPAIROPTS::REPAIROPTS() :
//  ================================================================
	crit( CLockBasicInfo( CSyncBasicInfo( szRepairOpts ), rankRepairOpts, 0 ) )
	{
	}

	
//  ================================================================
REPAIROPTS::~REPAIROPTS()
//  ================================================================
	{
	}


//  ================================================================
CSLVAvailIterator::CSLVAvailIterator() :
//  ================================================================
	m_pfucb( pfucbNil ),
	m_pgnoCurr( 0 ),
	m_ipage( 0 )
	{
	}


//  ================================================================
CSLVAvailIterator::~CSLVAvailIterator()
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	}
	

//  ================================================================
ERR CSLVAvailIterator::ErrInit( PIB * const ppib, const IFMP ifmp )
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	
	ERR		err;
	
	PGNO 	pgnoSLVAvail;
	OBJID	objidSLVAvail;
	Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );

	Call( ErrDIROpen( ppib, pgnoSLVAvail, ifmp, &m_pfucb ) );	

	FUCBSetPrereadForward( m_pfucb, cpgPrereadSequential );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVAvailIterator::ErrTerm()
//  ================================================================
	{
	if( pfucbNil != m_pfucb )
		{
		DIRClose( m_pfucb );
		m_pfucb = pfucbNil;
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR CSLVAvailIterator::ErrMoveFirst()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;
	Call( ErrDIRDown( m_pfucb, &dib ) );

	m_ipage 		= 0;
	m_pgnoCurr 		= 0;
	m_pspacenode	= (SLVSPACENODE *)m_pfucb->kdfCurr.data.Pv();

HandleError:
	return err;
	}
	

//  ================================================================
ERR CSLVAvailIterator::ErrMoveNext()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	++m_ipage;
	if( SLVSPACENODE::cpageMap == m_ipage )
		{
		
		//  move to the next SLVSPACENODE
		
		Call( ErrDIRNext( m_pfucb, fDIRNull ) );
		m_ipage 		= 0;
		m_pgnoCurr		+= SLVSPACENODE::cpageMap;
		m_pspacenode	= (SLVSPACENODE *)m_pfucb->kdfCurr.data.Pv();	
		}

HandleError:
	return err;
	}

	
//  ================================================================
BOOL CSLVAvailIterator::FCommitted() const
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	Assert( NULL != m_pspacenode );
	Assert( m_ipage < SLVSPACENODE::cpageMap );
	
	const SLVSPACENODE::STATE state = m_pspacenode->GetState( m_ipage );
	return ( SLVSPACENODE::sCommitted == state );
	}

	
//  ================================================================
CSLVOwnerMapIterator::CSLVOwnerMapIterator() :
//  ================================================================
	m_pfucb( pfucbNil ),
	m_slvownermapnode()
	{
	}


//  ================================================================
CSLVOwnerMapIterator::~CSLVOwnerMapIterator()
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrInit( PIB * const ppib, const IFMP ifmp )
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	
	ERR		err;
	
	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;
	Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );

	Call( ErrDIROpen( ppib, pgnoSLVOwnerMap, ifmp, &m_pfucb ) );

	FUCBSetPrereadForward( m_pfucb, cpgPrereadSequential );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrTerm()
//  ================================================================
	{
	if( pfucbNil != m_pfucb )
		{
		DIRClose( m_pfucb );
		m_pfucb = pfucbNil;
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrMoveFirst()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;
	Call( ErrDIRDown( m_pfucb, &dib ) );

	m_slvownermapnode.Retrieve( m_pfucb->kdfCurr.data );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrMoveNext()
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	Call( ErrDIRNext( m_pfucb, fDIRNull ) );

	m_slvownermapnode.Retrieve( m_pfucb->kdfCurr.data );

HandleError:
	return err;
	}


//  ================================================================
BOOL CSLVOwnerMapIterator::FNull() const
//  ================================================================
	{
	return ( 0 == CbKey() );
	}

	
//  ================================================================
OBJID CSLVOwnerMapIterator::Objid() const
//  ================================================================
	{
	return m_slvownermapnode.Objid();
	}


//  ================================================================
COLUMNID CSLVOwnerMapIterator::Columnid() const
//  ================================================================
	{
	return m_slvownermapnode.Columnid();
	}


//  ================================================================
const VOID * CSLVOwnerMapIterator::PvKey() const
//  ================================================================
	{
	return m_slvownermapnode.PvKey();
	}
	

//  ================================================================
INT CSLVOwnerMapIterator::CbKey() const
//  ================================================================
	{
	return m_slvownermapnode.CbKey();
	}


//  ================================================================
ULONG CSLVOwnerMapIterator::UlChecksum() const
//  ================================================================
	{
	return m_slvownermapnode.UlChecksum();
	}


//  ================================================================
USHORT CSLVOwnerMapIterator::CbDataChecksummed() const
//  ================================================================
	{
	return m_slvownermapnode.CbDataChecksummed();
	}


//  ================================================================
BOOL CSLVOwnerMapIterator::FChecksumIsValid() const
//  ================================================================
	{
	return m_slvownermapnode.FValidChecksum();
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\stats.cxx ===
#include "std.hxx"

ERR ErrSTATSComputeIndexStats( PIB *ppib, FCB *pfcbIdx, FUCB *pfucbTable )
	{
	ERR				err = JET_errSuccess;
	FUCB			*pfucbIdx;
	SR				sr;
	JET_DATESERIAL	dt;
	OBJID			objidTable;
	CHAR 			szIndexName[JET_cbNameMost+1];

	CallR( ErrDIROpen( ppib, pfcbIdx, &pfucbIdx ) );
	Assert( pfucbIdx != pfucbNil );
	FUCBSetIndex( pfucbIdx );

	/*	initialize stats record
	/**/
	sr.cPages = sr.cItems = sr.cKeys = 0L;
	UtilGetDateTime( &dt );
	UtilMemCpy( &sr.dtWhenRun, &dt, sizeof sr.dtWhenRun );

	if ( pfcbIdx->FPrimaryIndex() )
		{
		objidTable = pfcbIdx->ObjidFDP();
		}
	else
		{
		FCB	*pfcbTable = pfucbTable->u.pfcb;
		objidTable = pfcbTable->ObjidFDP();
		Assert( pfcbTable->Ptdb() != ptdbNil );

		Assert( pfcbIdx->FTypeSecondaryIndex() );
		if ( pfcbIdx->FDerivedIndex() )
			{
			Assert( pfcbIdx->Pidb()->FTemplateIndex() );
			pfcbTable = pfcbTable->Ptdb()->PfcbTemplateTable();
			Assert( pfcbNil != pfcbTable );
			Assert( pfcbTable->FTemplateTable() );
			Assert( pfcbTable->Ptdb() != ptdbNil );
			}
		
		pfcbTable->EnterDML();
		Assert( pfcbIdx->Pidb()->ItagIndexName() != 0 );
		strcpy( szIndexName,
			pfcbTable->Ptdb()->SzIndexName( pfcbIdx->Pidb()->ItagIndexName() ) );
		pfcbTable->LeaveDML();
		FUCBSetSecondary( pfucbIdx );
		}
		
	Call( ErrDIRComputeStats( pfucbIdx, reinterpret_cast<INT *>( &sr.cItems ), reinterpret_cast<INT *>( &sr.cKeys ), 
								reinterpret_cast<INT *>( &sr.cPages ) ) );
	FUCBResetSecondary( pfucbIdx );

	/*	write stats
	/**/
	Call( ErrCATStats(
			ppib,
			pfucbIdx->ifmp,
			objidTable,
			pfcbIdx->FPrimaryIndex() ? NULL : szIndexName,
			&sr,
			fTrue ) );

HandleError:
	//	set secondary for cursor reuse support
	if ( !pfcbIdx->FPrimaryIndex() )
		FUCBSetSecondary( pfucbIdx );
	DIRClose( pfucbIdx );
	return err;
	}


ERR VTAPI ErrIsamComputeStats( JET_SESID sesid, JET_VTID vtid )
	{
 	PIB		*ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucb = reinterpret_cast<FUCB *>( vtid );

	ERR		err = JET_errSuccess;
	FCB		*pfcbTable;
	FCB		*pfcbIdx;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );

	/*	start a transaction, in case anything fails
	/**/
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	/*	compute stats for each index
	/**/
	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );

	Assert( !pfcbTable->FDeletePending() );
	Assert( !pfcbTable->FDeleteCommitted() );
	Call( ErrSTATSComputeIndexStats( ppib, pfcbTable, pfucb ) );
	
	pfcbTable->EnterDML();
	for ( pfcbIdx = pfcbTable->PfcbNextIndex(); pfcbIdx != pfcbNil; pfcbIdx = pfcbIdx->PfcbNextIndex() )
		{
		Assert( pfcbIdx->FTypeSecondaryIndex() );
		Assert( pfcbIdx->Pidb() != pidbNil );

		err = ErrFILEIAccessIndex( ppib, pfcbTable, pfcbIdx );

		if ( err < 0 )
			{
			if ( JET_errIndexNotFound != err )
				{
				pfcbTable->LeaveDML();
				goto HandleError;
				}
			}
		else
			{
			pfcbTable->LeaveDML();
			// Because we're in a transaction, this guarantees that the FCB won't
			// be cleaned up while we're trying to retrieve stats.
			Call( ErrSTATSComputeIndexStats( ppib, pfcbIdx, pfucb ) );
			pfcbTable->EnterDML();
			}
		}
	pfcbTable->LeaveDML();

	/*	commit transaction if everything went OK
	/**/
	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );

	return err;

HandleError:
	Assert( err < 0 );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
	return err;
	}


/*=================================================================
ErrSTATSRetrieveStats

Description: Returns the number of records and pages used for a table

Parameters:		ppib				pointer to PIB for current session or ppibNil
				ifmp				database id or 0
				pfucb				cursor or pfucbNil
				szTableName			the name of the table or NULL
				pcRecord			pointer to count of records
				pcPage				pointer to count of pages

Errors/Warnings:
				JET_errSuccess or error from called routine.

=================================================================*/
ERR ErrSTATSRetrieveTableStats(
	PIB			* ppib,
	const IFMP 	ifmp,
	char		* szTable,
	long		* pcRecord,
	long		* pcKey,
	long		* pcPage )
	{
	ERR			err;
	FUCB		* pfucb;
	SR			sr;

	CallR( ErrFILEOpenTable( ppib, ifmp, &pfucb, szTable, NO_GRBIT ) );

	Call( ErrCATStats(
				pfucb->ppib,
				pfucb->ifmp,
				pfucb->u.pfcb->ObjidFDP(),
				NULL,
				&sr,
				fFalse));

	/*	set output variables
	/**/
	if ( pcRecord )
		*pcRecord = sr.cItems;
	if ( pcPage )
		*pcPage = sr.cPages;
	if ( pcKey )
		*pcKey = sr.cKeys;

	CallS( err );

HandleError:
	CallS( ErrFILECloseTable( ppib, pfucb ) );
	return err;
	}


ERR ErrSTATSRetrieveIndexStats(
	FUCB   	*pfucbTable,
	char   	*szIndex,
	BOOL	fPrimary,
	long   	*pcItem,
	long   	*pcKey,
	long   	*pcPage )
	{
	ERR		err;
	SR		sr;

	// The name is assumed to be valid.

	CallR( ErrCATStats(
				pfucbTable->ppib,
				pfucbTable->ifmp,
				pfucbTable->u.pfcb->ObjidFDP(),
				( fPrimary ? NULL : szIndex),
				&sr,
				fFalse ) );

	/*	set output variables
	/**/
	if ( pcItem )
		*pcItem = sr.cItems;
	if ( pcPage )
		*pcPage = sr.cPages;
	if ( pcKey )
		*pcKey = sr.cKeys;

	CallS( err );

	return JET_errSuccess;
	}


	ERR VTAPI
ErrIsamGetRecordPosition( JET_SESID vsesid, JET_VTID vtid, JET_RECPOS *precpos, unsigned long cbRecpos )
	{
	ERR		err;
	ULONG  	ulLT;
	ULONG	ulTotal;
	PIB *ppib = (PIB *)vsesid;
	FUCB *pfucb = (FUCB *)vtid;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );
	Assert( FFUCBIndex( pfucb ) );

	if ( cbRecpos < sizeof(JET_RECPOS) )
		return ErrERRCheck( JET_errInvalidParameter );
	precpos->cbStruct = sizeof(JET_RECPOS);

	//	get position of secondary or primary cursor
	if ( pfucb->pfucbCurIndex != pfucbNil )
		{
		Call( ErrDIRGetPosition( pfucb->pfucbCurIndex, &ulLT, &ulTotal ) );
		}
	else
		{
		Call( ErrDIRGetPosition( pfucb, &ulLT, &ulTotal ) );
		}

	precpos->centriesLT = ulLT;
	//	CONSIDER:	remove this bogus field
	precpos->centriesInRange = 1;
	precpos->centriesTotal = ulTotal;

HandleError:
	return err;
	}


ERR ISAMAPI ErrIsamIndexRecordCount( JET_SESID sesid, JET_TABLEID tableid, unsigned long *pulCount, unsigned long ulCountMost )
	{
	ERR	 	err;
	PIB	 	*ppib = (PIB *)sesid;
	FUCB 	*pfucb;
	FUCB 	*pfucbIdx;

	CallR( ErrPIBCheck( ppib ) );

	/*	get pfucb from tableid
	/**/
	CallR( ErrFUCBFromTableid( ppib, tableid, &pfucb ) );

	CheckTable( ppib, pfucb );

	/*	get cursor for current index
	/**/
	if ( pfucb->pfucbCurIndex != pfucbNil )
		pfucbIdx = pfucb->pfucbCurIndex;
	else
		pfucbIdx = pfucb;

	err = ErrDIRIndexRecordCount( pfucbIdx, pulCount, ulCountMost, fTrue );
	AssertDIRNoLatch( ppib );
	return err;
	};
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\space.cxx ===
#include "std.hxx"
#include "_space.hxx"

PERFInstanceGlobal<> cSPCreate;
PM_CEF_PROC LSPCreateCEFLPv;
LONG LSPCreateCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	cSPCreate.PassTo( iInstance, pvBuf );
	return 0;
	}

PERFInstanceGlobal<> cSPDelete;
PM_CEF_PROC LSPDeleteCEFLPv;
LONG LSPDeleteCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	cSPDelete.PassTo( iInstance, pvBuf );
	return 0;
	}


#include "_bt.hxx"


const CHAR * SzNameOfTable( const FUCB * const pfucb )
	{
	if( pfucb->u.pfcb->FTypeTable() )
		{
		return pfucb->u.pfcb->Ptdb()->SzTableName();
		}
	else if( pfucb->u.pfcb->FTypeLV() )
		{
		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzTableName();
		}
	else if( pfucb->u.pfcb->FTypeSecondaryIndex() )
		{
		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzTableName();
///		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzIndexName(
///					pfucb->u.pfcb->Pidb()->ItagIndexName(),
///					pfucb->u.pfcb->FDerivedIndex() );
		}
	return "";
	}


///#define COALESCE_OE


#ifdef DEBUG
//#define SPACECHECK
//#define TRACE
///#define DEBUG_DUMP_SPACE_INFO
#endif

///#define CONVERT_VERBOSE
#ifdef CONVERT_VERBOSE
extern void VARARG CONVPrintF2(const CHAR * fmt, ...);
#endif


//	prototypes of internal functions
//
LOCAL ERR ErrSPIAddFreedExtent( FUCB *pfucbAE, const PGNO pgnoParentFDP, const PGNO pgnoLast, const CPG cpgSize );
LOCAL ERR ErrSPIGetSE(
	FUCB		* pfucb,
	FUCB		* pfucbAE,
	const CPG	cpgReq,
	const CPG	cpgMin,
	const BOOL	fSplitting,
	SPCACHE		*** pppspcache = NULL );
LOCAL ERR ErrSPIWasAlloc( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize );
LOCAL ERR ErrSPIValidFDP( PIB *ppib, IFMP ifmp, PGNO pgnoFDP );


//	types of extents in SPBUF
//
enum SPEXT	{ spextFreed, spextSecondary };

LOCAL ErrSPIReservePages( FUCB *pfucb, FUCB *pfucbParent, const SPEXT spext );
LOCAL ERR ErrSPIAddToAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoAELast,
	const CPG	cpgAESize,
	SPCACHE		***pppspcache = NULL );



INLINE VOID AssertSPIPfucbOnRoot( FUCB *pfucb )
	{
#ifdef	DEBUG
	//	check to make sure that FUCB
	//	passed in is on root page
	//	and has page RIW latched
	//
	Assert( pfucb->pcsrRoot != pcsrNil );
	Assert( pfucb->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
	Assert( pfucb->pcsrRoot->Latch() == latchRIW 
		|| pfucb->pcsrRoot->Latch() == latchWrite );
	Assert( !FFUCBSpace( pfucb ) );
#endif
	}

INLINE VOID AssertSPIPfucbOnSpaceTreeRoot( FUCB *pfucb, CSR *pcsr )
	{
#ifdef	DEBUG
	Assert( FFUCBSpace( pfucb ) );
	Assert( pcsr->FLatched() );
	Assert( pcsr->Pgno() == PgnoRoot( pfucb ) );
	Assert( pcsr->Cpage().FRootPage() );
	Assert( pcsr->Cpage().FSpaceTree() );
#endif
	}

//  write latch page before update
//
LOCAL VOID SPIUpgradeToWriteLatch( FUCB *pfucb )
	{
	CSR		*pcsrT;

	if ( Pcsr( pfucb )->Latch() == latchNone )
		{
		//	latch upgrades only work on root
		//
		Assert( pfucb->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );

		//	must want to upgrade latch in pcsrRoot
		//
		pcsrT = pfucb->pcsrRoot;
		Assert( latchRIW == pcsrT->Latch() );
		}
	else
		{
		//	latch upgrades only work on root
		//
		Assert( Pcsr( pfucb ) == pfucb->pcsrRoot );
		Assert( Pcsr(pfucb)->Pgno() == PgnoFDP( pfucb ) );
		pcsrT = &pfucb->csr;
		}

	if ( pcsrT->Latch() != latchWrite )
		{
		Assert( pcsrT->Latch() == latchRIW );
		pcsrT->UpgradeFromRIWLatch();
		}
	else
		{
		Assert( fFalse );
		}
	}


//	opens a cursor on Avail/owned extent of given tree
//	subsequent BT operations on cursor will place 
//	cursor on root page of available extent
//	this logic is embedded in ErrBTIGotoRootPage
//
ERR ErrSPIOpenAvailExt( PIB *ppib, FCB *pfcb, FUCB **ppfucbAE )
	{
	ERR		err;

	//	open cursor on FCB
	//	label it as a space cursor
	//
	CallR( ErrBTOpen( ppib, pfcb, ppfucbAE, fFalse ) );
	FUCBSetAvailExt( *ppfucbAE );
	FUCBSetIndex( *ppfucbAE );
	Assert( pfcb->FSpaceInitialized() );
	Assert( pfcb->PgnoAE() != pgnoNull );

	return err;
	}


ERR ErrSPIOpenOwnExt( PIB *ppib, FCB *pfcb, FUCB **ppfucbOE )
	{
	ERR	err;

	//	open cursor on FCB
	//	label it as a space cursor
	//
	CallR( ErrBTOpen( ppib, pfcb, ppfucbOE, fFalse ) );
	FUCBSetOwnExt( *ppfucbOE );
	FUCBSetIndex( *ppfucbOE );
	Assert( pfcb->FSpaceInitialized() );
	Assert( pfcb->PgnoOE() != pgnoNull );

	return err;
	}


//	gets pgno of last page owned by database
//
ERR	ErrSPGetLastPgno( PIB *ppib, IFMP ifmp, PGNO *ppgno )
	{
	ERR		err;
	FUCB	*pfucb = pfucbNil;
	FUCB	*pfucbOE = pfucbNil;
	DIB		dib;
	
	CallR( ErrBTOpen( ppib, pgnoSystemRoot, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );

	if ( PinstFromPpib( ppib )->m_plog->m_fRecovering && !pfucb->u.pfcb->FSpaceInitialized() )
		{
		//	pgnoOE and pgnoAE need to be obtained
		//
		Call( ErrSPInitFCB( pfucb ) );
		}
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );
	
	dib.dirflag = fDIRNull;
	dib.pos 	= posLast;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( ppgno, pfucbOE->kdfCurr.key );

HandleError:
	if ( pfucbOE != pfucbNil )
		{
		BTClose( pfucbOE );
		}
		
	Assert ( pfucb != pfucbNil );
	BTClose( pfucb );
		
	return err;
	}


LOCAL VOID SPIInitFCB( FUCB *pfucb, const BOOL fDeferredInit )
	{
	SPACE_HEADER	* psph;
	CSR				* pcsr	= ( fDeferredInit ? pfucb->pcsrRoot : Pcsr( pfucb ) );
	FCB				* pfcb	= pfucb->u.pfcb;

	Assert( pcsr->FLatched() );

	//	need to acquire FCB lock because that's what protects the Flags
	pfcb->Lock();

	if ( !pfcb->FSpaceInitialized() )
		{
		//	get external header
		//
		NDGetExternalHeader ( pfucb, pcsr );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

		if ( psph->FSingleExtent() )
			{
			pfcb->SetPgnoOE( pgnoNull );
			pfcb->SetPgnoAE( pgnoNull );
			}
		else
			{
			pfcb->SetPgnoOE( psph->PgnoOE() );
			pfcb->SetPgnoAE( psph->PgnoAE() );
			}

		if ( !fDeferredInit )
			{
			Assert( pfcb->FUnique() );		// FCB always initialised as unique
			if ( psph->FNonUnique() )
				pfcb->SetNonUnique();
			}

		pfcb->SetSpaceInitialized();
		}

	pfcb->Unlock();

	return;
	}

	
//	initializes FCB with pgnoAE and pgnoOE
//
ERR	ErrSPInitFCB( FUCB * const pfucb )
	{
	ERR				err;
	FCB				*pfcb	= pfucb->u.pfcb;

	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( !FFUCBSpace( pfucb ) );
	
	//	goto root page of tree
	//
	err = ErrBTIGotoRoot( pfucb, latchReadTouch );
	if ( err < 0 )
		{
		if ( fGlobalRepair )
			{
			//  ignore the error
			pfcb->SetPgnoOE( pgnoNull );
			pfcb->SetPgnoAE( pgnoNull );

			Assert( objidNil == pfcb->ObjidFDP() );
		
			err = JET_errSuccess;
			}
		}
	else
		{
		//	get objidFDP from root page, FCB can only be set once

		Assert( objidNil == pfcb->ObjidFDP()
			|| ( PinstFromIfmp( pfucb->ifmp )->FRecovering() && pfcb->ObjidFDP() == Pcsr( pfucb )->Cpage().ObjidFDP() ) );
		pfcb->SetObjidFDP( Pcsr( pfucb )->Cpage().ObjidFDP() );

		SPIInitFCB( pfucb, fFalse );

		BTUp( pfucb );
		}

	return err;
	}


ERR ErrSPDeferredInitFCB( FUCB * const pfucb )
	{
	ERR				err;
	FCB				* pfcb	= pfucb->u.pfcb;
	FUCB			* pfucbT	= pfucbNil;

	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( !FFUCBSpace( pfucb ) );
	
	//	goto root page of tree
	//
	CallR( ErrBTIOpenAndGotoRoot(
				pfucb->ppib,
				pfcb->PgnoFDP(),
				pfucb->ifmp,
				&pfucbT ) );
	Assert( pfucbNil != pfucbT );
	Assert( pfucbT->u.pfcb == pfcb );
	Assert( pcsrNil != pfucbT->pcsrRoot );

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucbT, fTrue );
		}

	pfucbT->pcsrRoot->ReleasePage();
	pfucbT->pcsrRoot = pcsrNil;

	Assert( pfucbNil != pfucbT );
	BTClose( pfucbT );

	return JET_errSuccess;
	}


INLINE SPACE_HEADER *PsphSPIRootPage( FUCB *pfucb )
	{
	SPACE_HEADER	*psph;
	
	AssertSPIPfucbOnRoot( pfucb );
	
	NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
	Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
	psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

	return psph;
	}

//	get objid of parentFDP of this tree
//
INLINE PgnoSPIParentFDP( FUCB *pfucb )
	{
	return PsphSPIRootPage( pfucb )->PgnoParent();
	}
	
//	get cpgPrimary of this tree
//
INLINE CPG CpgSPIPrimary( FUCB *pfucb )
	{
	return PsphSPIRootPage( pfucb )->CpgPrimary();
	}


INLINE SPLIT_BUFFER *PspbufSPISpaceTreeRootPage( FUCB *pfucb, CSR *pcsr )
	{
	SPLIT_BUFFER	*pspbuf;

	AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsr );
	
	NDGetExternalHeader( pfucb, pcsr );
	Assert( sizeof( SPLIT_BUFFER ) == pfucb->kdfCurr.data.Cb() );
	pspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );

	return pspbuf;
	}


#define REPORT_SPBUF

INLINE VOID SPIReportAllocatedSplitBuffer( const FUCB * const pfucb )
	{
#ifdef REPORT_SPBUF
	CHAR		szMsg[512];
	const CHAR	* rgszT[1]	= { szMsg };

	sprintf(
		szMsg,
		"Database '%s': Allocated SplitBuffer for a Btree (objidFDP %d, pgnoFDP %d).",
		rgfmp[pfucb->ifmp].SzDatabaseName(),
		pfucb->u.pfcb->ObjidFDP(),
		pfucb->u.pfcb->PgnoFDP() );
	UtilReportEvent( eventInformation, GENERAL_CATEGORY, PLAIN_TEXT_ID, 1, rgszT );
#endif
	}

INLINE VOID SPIReportPersistedSplitBuffer( const FUCB * const pfucb )
	{
#ifdef REPORT_SPBUF
	CHAR		szMsg[512];
	const CHAR	* rgszT[1]	= { szMsg };

	sprintf(
		szMsg,
		"Database '%s': Persisted SplitBuffer for a Btree (objidFDP %d, pgnoFDP %d).",
		rgfmp[pfucb->ifmp].SzDatabaseName(),
		pfucb->u.pfcb->ObjidFDP(),
		pfucb->u.pfcb->PgnoFDP() );
	UtilReportEvent( eventInformation, GENERAL_CATEGORY, PLAIN_TEXT_ID, 1, rgszT );
#endif
	}

INLINE VOID SPIReportGetPageFromSplitBuffer( const FUCB * const pfucb )
	{
#ifdef REPORT_SPBUF
	CHAR		szMsg[512];
	const CHAR	* rgszT[1]	= { szMsg };

	sprintf(
		szMsg,
		"Database '%s': Retrieved page from SplitBuffer for a Btree (objidFDP %d, pgnoFDP %d).",
		rgfmp[pfucb->ifmp].SzDatabaseName(),
		pfucb->u.pfcb->ObjidFDP(),
		pfucb->u.pfcb->PgnoFDP() );
	UtilReportEvent( eventInformation, GENERAL_CATEGORY, PLAIN_TEXT_ID, 1, rgszT );
#endif
	}

INLINE VOID SPIReportAddedPagesToSplitBuffer( const FUCB * const pfucb )
	{
#ifdef REPORT_SPBUF
	CHAR		szMsg[512];
	const CHAR	* rgszT[1]	= { szMsg };

	sprintf(
		szMsg,
		"Database '%s': Added pages to SplitBuffer for a Btree (objidFDP %d, pgnoFDP %d).",
		rgfmp[pfucb->ifmp].SzDatabaseName(),
		pfucb->u.pfcb->ObjidFDP(),
		pfucb->u.pfcb->PgnoFDP() );
	UtilReportEvent( eventInformation, GENERAL_CATEGORY, PLAIN_TEXT_ID, 1, rgszT );
#endif
	}


LOCAL ERR ErrSPIFixSpaceTreeRootPage( FUCB *pfucb, SPLIT_BUFFER **ppspbuf )
	{
	ERR			err			= JET_errSuccess;
	const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );

	AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
	Assert( latchRIW == Pcsr( pfucb )->Latch() );

#ifdef DEBUG
	const BOOL	fNotEnoughPageSpace	= ( Pcsr( pfucb )->Cpage().CbFree() < ( g_cbPage * 3 / 4 ) );
#else
	const BOOL	fNotEnoughPageSpace	= ( Pcsr( pfucb )->Cpage().CbFree() < sizeof(SPLIT_BUFFER) );
#endif

	if ( fNotEnoughPageSpace )
		{
		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CallR( pfucb->u.pfcb->ErrEnableSplitbuf( fAvailExt ) );
			SPIReportAllocatedSplitBuffer( pfucb );
			}
		*ppspbuf = pfucb->u.pfcb->Psplitbuf( fAvailExt );
		Assert( NULL != *ppspbuf );
		}
	else
		{
		const BOOL		fSplitbufDangling	= ( NULL != pfucb->u.pfcb->Psplitbuf( fAvailExt ) );
		SPLIT_BUFFER	spbuf;
		DATA			data;
		Assert( 0 == pfucb->kdfCurr.data.Cb() );

		//	if in-memory copy of split buffer exists, move it to the page
		if ( fSplitbufDangling )
			{
			memcpy( &spbuf, pfucb->u.pfcb->Psplitbuf( fAvailExt ), sizeof(SPLIT_BUFFER) );
			}
		else
			{
			memset( &spbuf, 0, sizeof(SPLIT_BUFFER) );
			}

		data.SetPv( &spbuf );
		data.SetCb( sizeof(spbuf) );

		Pcsr( pfucb )->UpgradeFromRIWLatch();
		err = ErrNDSetExternalHeader( pfucb, &data, fDIRNull );
		Pcsr( pfucb )->Downgrade( latchRIW );
		CallR( err );

		if ( fSplitbufDangling )
			{
			//	split buffer successfully moved to page, destroy in-memory copy
			pfucb->u.pfcb->DisableSplitbuf( fAvailExt );
			SPIReportPersistedSplitBuffer( pfucb );
			}

		//	re-cache external header
		NDGetExternalHeader( pfucb, Pcsr( pfucb ) );

		*ppspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );
		}

	return err;
	}

INLINE ERR ErrSPIGetSPBuf( FUCB *pfucb, SPLIT_BUFFER **ppspbuf )
	{
	ERR	err;
	
	AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
	
	NDGetExternalHeader( pfucb, Pcsr( pfucb ) );
	
	if ( sizeof( SPLIT_BUFFER ) != pfucb->kdfCurr.data.Cb() )
		{
		err = ErrSPIFixSpaceTreeRootPage( pfucb, ppspbuf );
		}
	else
		{
		Assert( NULL == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) );
		*ppspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );
		err = JET_errSuccess;
		}

	return err;
	}

INLINE VOID SPIDirtyAndSetMaxDbtime( CSR *pcsr1, CSR *pcsr2, CSR *pcsr3 )
	{ 
	Assert( pcsr1->Latch() == latchWrite );
	Assert( pcsr2->Latch() == latchWrite );
	Assert( pcsr3->Latch() == latchWrite );

	pcsr1->Dirty();
	pcsr2->Dirty();
	pcsr3->Dirty();
	DBTIME	dbtimeMax = pcsr1->Dbtime();

	if ( dbtimeMax < pcsr2->Dbtime() )
		{
		dbtimeMax = pcsr2->Dbtime();
		}

	if ( dbtimeMax < pcsr3->Dbtime() )
		{
		dbtimeMax = pcsr3->Dbtime();
		}

	if ( dbtimeMax < 3 )
		{
		Assert( fRecoveringRedo == PinstFromIfmp( pcsr1->Cpage().Ifmp() )->m_plog->m_fRecoveringMode );

		//	CPAGE::ErrGetNewPage() initialises the dbtime to 1.
		Assert( 1 == pcsr1->Dbtime() );
		Assert( 1 == pcsr2->Dbtime() );
		Assert( 1 == pcsr3->Dbtime() );
		}

	//	set dbtime in the three pages
	//
	pcsr1->SetDbtime( dbtimeMax );
	pcsr2->SetDbtime( dbtimeMax );
	pcsr3->SetDbtime( dbtimeMax );
	}


INLINE VOID SPIInitSplitBuffer( FUCB *pfucb, CSR *pcsr )
	{
	//	copy dummy split buffer into external header
	//
	DATA 			data;
	SPLIT_BUFFER	spbuf;

	memset( &spbuf, 0, sizeof(SPLIT_BUFFER) );
	
	Assert( FBTIUpdatablePage( *pcsr ) );		//	check is already performed by caller
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	Assert( pgnoNull != PgnoAE( pfucb ) || fGlobalRepair );
	Assert( pgnoNull != PgnoOE( pfucb ) || fGlobalRepair );
	Assert( ( FFUCBAvailExt( pfucb ) && pcsr->Pgno() == PgnoAE( pfucb ) )
		|| ( FFUCBOwnExt( pfucb ) && pcsr->Pgno() == PgnoOE( pfucb ) )
		|| fGlobalRepair );

	data.SetPv( (VOID *)&spbuf );
	data.SetCb( sizeof(spbuf) );
	NDSetExternalHeader( pfucb, pcsr, &data );
	}

	
//	creates extent tree for either owned or available extents.
//
VOID SPICreateExtentTree( FUCB *pfucb, CSR *pcsr, PGNO pgnoLast, CPG cpgExtent, BOOL fAvail )
	{
	if ( !FBTIUpdatablePage( *pcsr ) )
		{
		//	page does not redo
		//
		return;
		}
	
	//	cannot reuse a deferred closed cursor
	//
	Assert( !FFUCBVersioned( pfucb ) );
	Assert( !FFUCBSpace( pfucb ) );
	Assert( pgnoNull != PgnoAE( pfucb ) || fGlobalRepair );
	Assert( pgnoNull != PgnoOE( pfucb ) || fGlobalRepair );
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	
	if ( fAvail )
		{
		FUCBSetAvailExt( pfucb );
		}
	else
		{
		FUCBSetOwnExt( pfucb );
		}
	Assert( pcsr->FDirty() );

	SPIInitSplitBuffer( pfucb, pcsr );

	Assert( 0 == pcsr->Cpage().Clines() );
	pcsr->SetILine( 0 );
	
	//	goto Root before insert would place
	//	cursor on appropriate space tree
	//
	if ( cpgExtent != 0 )
		{
		BYTE			rgbKey[sizeof(PGNO)];
		KEYDATAFLAGS	kdf;
		
		KeyFromLong( rgbKey, pgnoLast );
		kdf.key.prefix.Nullify();
		kdf.key.suffix.SetCb( sizeof( PGNO ) );
		kdf.key.suffix.SetPv( rgbKey );

		LittleEndian<CPG> le_cpgExtent = cpgExtent;
		kdf.data.SetCb( sizeof( CPG ) );
		kdf.data.SetPv( &le_cpgExtent );

		kdf.fFlags = 0;

		NDInsert( pfucb, pcsr, &kdf );
		}
	else
		{
		//	avail has already been set up as an empty tree
		//
		Assert( FFUCBAvailExt( pfucb ) );
		}

	if ( fAvail )
		{
		FUCBResetAvailExt( pfucb );
		}
	else
		{
		FUCBResetOwnExt( pfucb );
		}
		
	return;
	}


VOID SPIInitPgnoFDP( FUCB *pfucb, CSR *pcsr, const SPACE_HEADER& sph )
	{
	if ( !FBTIUpdatablePage( *pcsr ) )
		{
		//	page does not redo
		//
		return;
		}
		
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	Assert( pcsr->Pgno() == PgnoFDP( pfucb ) || fGlobalRepair );

	PERFIncCounterTable( cSPCreate, PinstFromPfucb( pfucb ), pfucb->u.pfcb->Tableclass() );

	//	copy space information into external header
	//
	DATA 	data;
	
	data.SetPv( (VOID *)&sph );
	data.SetCb( sizeof(sph) );
	NDSetExternalHeader( pfucb, pcsr, &data );
	}


//	performs creation of multiple extent FDP 
//	
VOID SPIPerformCreateMultiple( FUCB *pfucb, CSR *pcsrFDP, CSR *pcsrOE, CSR *pcsrAE, PGNO pgnoPrimary, const SPACE_HEADER& sph )
	{
	const CPG	cpgPrimary = sph.CpgPrimary();
	const PGNO	pgnoLast = pgnoPrimary;
	Assert( fGlobalRepair || pgnoLast == PgnoFDP( pfucb ) + cpgPrimary - 1 );
	
	//	insert space header into FDP
	//
	SPIInitPgnoFDP( pfucb, pcsrFDP, sph );

	//	add allocated extents to OwnExt
	//
	SPICreateExtentTree( pfucb, pcsrOE, pgnoLast, cpgPrimary, fFalse );

	//	add extent minus pages allocated to AvailExt
	//
	SPICreateExtentTree( pfucb, pcsrAE, pgnoLast, cpgPrimary - 1 - 1 - 1, fTrue );
	
	return;
	}

//  ================================================================
ERR ErrSPCreateMultiple(
	FUCB		*pfucb,
	const PGNO	pgnoParent,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const PGNO	pgnoOE,
	const PGNO	pgnoAE,
	const PGNO	pgnoPrimary,
	const CPG	cpgPrimary,
	const BOOL	fUnique,
	const ULONG	fPageFlags )
//  ================================================================
	{
	ERR				err;
	CSR				csrOE;
	CSR				csrAE;
	CSR				csrFDP;
	SPACE_HEADER	sph;
	LGPOS			lgpos;

	Assert( objidFDP == ObjidFDP( pfucb ) || fGlobalRepair );
	Assert( objidFDP != objidNil );

	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree) );

	//	init space trees and root page
	//
	Call( csrOE.ErrGetNewPage( pfucb->ppib, 
							   pfucb->ifmp,
							   pgnoOE, 
							   objidFDP,
							   ( fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) & ~CPAGE::fPageRepair,
							   pfucb->u.pfcb->Tableclass() ) );
	Assert( csrOE.Latch() == latchWrite );

	Call( csrAE.ErrGetNewPage( pfucb->ppib, 
							   pfucb->ifmp,
							   pgnoAE, 
							   objidFDP,
							   ( fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) & ~CPAGE::fPageRepair,
							   pfucb->u.pfcb->Tableclass() ) );
	Assert( csrAE.Latch() == latchWrite );
	
	Call( csrFDP.ErrGetNewPage( pfucb->ppib,
								pfucb->ifmp,
								pgnoFDP,
								objidFDP,
								fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf,
								pfucb->u.pfcb->Tableclass() ) );
										
	//	set max dbtime for the three pages in all the pages
	//
	SPIDirtyAndSetMaxDbtime( &csrFDP, &csrOE, &csrAE );
		
	sph.SetCpgPrimary( cpgPrimary );
	sph.SetPgnoParent( pgnoParent );
	
	Assert( sph.FSingleExtent() );		// initialised with these defaults
	Assert( sph.FUnique() );

	sph.SetMultipleExtent();
	
	if ( !fUnique )
		sph.SetNonUnique();
		
	sph.SetPgnoOE( pgnoOE );

	//	log operation
	//
	Call( ErrLGCreateMultipleExtentFDP( pfucb, &csrFDP, &sph, fPageFlags, &lgpos ) );
	csrFDP.Cpage().SetLgposModify( lgpos );
	csrOE.Cpage().SetLgposModify( lgpos );
	csrAE.Cpage().SetLgposModify( lgpos );

	//	perform operation on all three pages
	//
	SPIPerformCreateMultiple( pfucb, &csrFDP, &csrOE, &csrAE, pgnoPrimary, sph );
	
HandleError:
	csrAE.ReleasePage();
	csrOE.ReleasePage();
	csrFDP.ReleasePage();	
	return err;
	}

	
//	initializes a FDP page with external headers for space.
//	Also initializes external space trees appropriately.
//	This operation is logged as an aggregate.
//
INLINE ERR ErrSPICreateMultiple(
	FUCB		*pfucb,
	const PGNO	pgnoParent,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const CPG	cpgPrimary,
	const ULONG	fPageFlags,
	const BOOL	fUnique )
	{
	return ErrSPCreateMultiple(
			pfucb,
			pgnoParent,
			pgnoFDP,
			objidFDP,
			pgnoFDP + 1,
			pgnoFDP + 2,
			pgnoFDP + cpgPrimary - 1,
			cpgPrimary,
			fUnique,
			fPageFlags );
	}


ERR ErrSPICreateSingle(
	FUCB			*pfucb,
	CSR				*pcsr,
	const PGNO		pgnoParent,
	const PGNO		pgnoFDP,
	const OBJID		objidFDP,
	CPG				cpgPrimary,
	const BOOL		fUnique,
	const ULONG		fPageFlags )
	{
	ERR				err;
	SPACE_HEADER	sph;

	//	copy space information into external header
	//
	sph.SetPgnoParent( pgnoParent );
	sph.SetCpgPrimary( cpgPrimary );

	Assert( sph.FSingleExtent() );		// always initialised to single-extent, unique
	Assert( sph.FUnique() );

	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree) );

	if ( !fUnique )
		sph.SetNonUnique();
	
	Assert( cpgPrimary > 0 );
	if ( cpgPrimary > cpgSmallSpaceAvailMost )
		{
		sph.SetRgbitAvail( 0xffffffff );
		}
	else
		{
		sph.SetRgbitAvail( 0 );
		while ( --cpgPrimary > 0 )
			{
			sph.SetRgbitAvail( ( sph.RgbitAvail() << 1 ) + 1 );
			}
		}

	Assert( objidFDP == ObjidFDP( pfucb ) );
	Assert( objidFDP != 0 );
	
	//	get pgnoFDP to initialize in current CSR pgno
	//
	Call( pcsr->ErrGetNewPage(
		pfucb->ppib,
		pfucb->ifmp,
		pgnoFDP,
		objidFDP,
		fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf,
		pfucb->u.pfcb->Tableclass() ) );
	pcsr->Dirty();

	if ( !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering )
		{
		LGPOS	lgpos;
		
		Call( ErrLGCreateSingleExtentFDP( pfucb, pcsr, &sph, fPageFlags, &lgpos ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}

	SPIInitPgnoFDP( pfucb, pcsr, sph );

	BTUp( pfucb );

HandleError:
	return err;
	}


//	opens a cursor on tree to be created, and uses cursor to 
//	initialize space data structures.
//
ERR ErrSPCreate(
	PIB 			*ppib, 
	const IFMP		ifmp,
	const PGNO		pgnoParent,
	const PGNO		pgnoFDP,
	const CPG		cpgPrimary,
	const BOOL		fSPFlags,
	const ULONG		fPageFlags,
	OBJID			*pobjidFDP )
	{
	ERR				err;
	FUCB			*pfucb = pfucbNil;
	const BOOL		fUnique = !( fSPFlags & fSPNonUnique );

	Assert( NULL != pobjidFDP );
	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree ) );

	//	UNDONE: an FCB will be allocated here, but it will be uninitialized,
	//	unless this is the special case where we are allocating the DB root,
	//	in which case it is initialized. Hence, the subsequent BTClose
	//	will free it.  It will have to be reallocated on a subsequent
	//	DIR/BTOpen, at which point it will get put into the FCB hash
	//	table.  Implement a fix to allow leaving the FCB in an uninitialized
	//	state, then have it initialized by the subsequent DIR/BTOpen.
	//
	CallR( ErrBTOpen( ppib, pgnoFDP, ifmp, &pfucb, openNew ) );

	FCB	*pfcb	= pfucb->u.pfcb;
	Assert( pfcbNil != pfcb );

	if ( pgnoSystemRoot == pgnoFDP )
		{
		Assert( pfcb->FInitialized() );
		Assert( pfcb->FTypeDatabase() );
		}
	else
		{
		Assert( !pfcb->FInitialized() );
		Assert( pfcb->FTypeNull() );
		}

	Assert( pfcb->FUnique() );		// FCB always initialised as unique
	if ( !fUnique )
		pfcb->SetNonUnique();

	//	get objid for this new FDP. This objid is then stored in catalog table.

	if ( fGlobalRepair && pgnoSystemRoot == pgnoFDP )
		{
		*pobjidFDP = objidSystemRoot;
		}
	else
		{
		Call( rgfmp[ pfucb->ifmp ].ErrObjidLastIncrementAndGet( pobjidFDP ) );
		}
	Assert( pgnoSystemRoot != pgnoFDP || objidSystemRoot == *pobjidFDP );

	//	objidFDP should be initialised to NULL

	Assert( objidNil == pfcb->ObjidFDP() );
	pfcb->SetObjidFDP( *pobjidFDP );

	Assert( !pfcb->FSpaceInitialized() );
	pfcb->SetSpaceInitialized();

	if ( fSPFlags & fSPMultipleExtent )
		{
		Assert( PgnoFDP( pfucb ) == pgnoFDP );
		pfcb->SetPgnoOE( pgnoFDP + 1 );
		pfcb->SetPgnoAE( pgnoFDP + 2 );
		err = ErrSPICreateMultiple(
					pfucb,
					pgnoParent,
					pgnoFDP,
					*pobjidFDP,
					cpgPrimary,
					fPageFlags,
					fUnique );
		}
	else
		{
		Assert( PgnoFDP( pfucb ) == pgnoFDP );
		pfcb->SetPgnoOE( pgnoNull );
		pfcb->SetPgnoAE( pgnoNull );
		err = ErrSPICreateSingle(
					pfucb, 
					Pcsr( pfucb ), 
					pgnoParent,
					pgnoFDP,
					*pobjidFDP,
					cpgPrimary,
					fUnique,
					fPageFlags );
		}

	Assert( !FFUCBSpace( pfucb ) );
	Assert( !FFUCBVersioned( pfucb ) );

HandleError:
	Assert( pfucb != pfucbNil );
	BTClose( pfucb );

	return err;
	}


//	calculates extent info in rgext 
//		number of extents in *pcextMac
LOCAL VOID SPIConvertCalcExtents(
	const SPACE_HEADER&	sph, 
	const PGNO			pgnoFDP,
	EXTENTINFO 			*rgext, 
	INT 				*pcext )
	{
	PGNO				pgno;
	UINT				rgbit		= 0x80000000;
	INT					iextMac		= 0;

	//	if available extent for space after rgbitAvail, then
	//	set rgextT[0] to extent, otherwise set rgextT[0] to
	//	last available extent.
	//
	if ( sph.CpgPrimary() - 1 > cpgSmallSpaceAvailMost )
		{
		pgno = pgnoFDP + sph.CpgPrimary() - 1; 
		rgext[iextMac].pgnoLastInExtent = pgno;
		rgext[iextMac].cpgExtent = sph.CpgPrimary() - cpgSmallSpaceAvailMost - 1;
		pgno -= rgext[iextMac].cpgExtent;
		iextMac++;

		Assert( pgnoFDP + cpgSmallSpaceAvailMost == pgno );
		Assert( 0x80000000 == rgbit );
		}
	else
		{
		pgno = pgnoFDP + cpgSmallSpaceAvailMost;
		while ( 0 != rgbit && 0 == iextMac )
			{
			Assert( pgno > pgnoFDP );
			if ( rgbit & sph.RgbitAvail() ) 
				{
				Assert( pgno <= pgnoFDP + sph.CpgPrimary() - 1 );
				rgext[iextMac].pgnoLastInExtent = pgno;
				rgext[iextMac].cpgExtent = 1;
				iextMac++;
				}

			//	even if we found an available extent, we still need
			//	to update the loop variables in preparation for the
			//	next loop below
			pgno--;
			rgbit >>= 1;
			}
		}

	//	continue through rgbitAvail finding all available extents.
	//	if iextMac == 0 then there was not even one available extent.
	//
	Assert( ( 0 == iextMac && 0 == rgbit )
		|| 1 == iextMac );
		
	//	find additional available extents
	//
	for ( ; rgbit != 0; pgno--, rgbit >>= 1 )
		{
		Assert( pgno > pgnoFDP );
		if ( rgbit & sph.RgbitAvail() )
			{
			const INT	iextPrev	= iextMac - 1;
			Assert( rgext[iextPrev].cpgExtent > 0 );
			Assert( rgext[iextPrev].cpgExtent <= rgext[iextPrev].pgnoLastInExtent );
			Assert( rgext[iextPrev].pgnoLastInExtent <= pgnoFDP + sph.CpgPrimary() - 1 );
			
			const PGNO	pgnoFirst = rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent + 1;
			Assert( pgnoFirst > pgnoFDP );
			
			if ( pgnoFirst - 1 == pgno )
				{
				Assert( pgnoFirst - 1 > pgnoFDP );
				rgext[iextPrev].cpgExtent++;
				}
			else
				{
				rgext[iextMac].pgnoLastInExtent = pgno;
				rgext[iextMac].cpgExtent = 1;
				iextMac++;
				}
			}
		}

	*pcext = iextMac;
	
	return;
	}


//	update OwnExt root page for a convert
//
LOCAL VOID SPIConvertUpdateOE( FUCB *pfucb, CSR *pcsrOE, const SPACE_HEADER& sph, PGNO pgnoSecondaryFirst, CPG cpgSecondary )
	{
	if ( !FBTIUpdatablePage( *pcsrOE ) )
		{
		Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		return;
		}

	Assert( pcsrOE->Latch() == latchWrite );
	Assert( !FFUCBOwnExt( pfucb ) );
	FUCBSetOwnExt( pfucb );

	SPIInitSplitBuffer( pfucb, pcsrOE );

	Assert( 0 == pcsrOE->Cpage().Clines() );
	Assert( cpgSecondary != 0 );
	pcsrOE->SetILine( 0 );

	//	insert secondary and primary extents
	//
	KEYDATAFLAGS kdf;
	LittleEndian<CPG> le_cpgSecondary = cpgSecondary;
	BYTE		rgbKey[sizeof(PGNO)];
	
	KeyFromLong( rgbKey, pgnoSecondaryFirst + cpgSecondary - 1 );
	kdf.Nullify();
	kdf.key.suffix.SetCb( sizeof( PGNO ) );
	kdf.key.suffix.SetPv( rgbKey );
	kdf.data.SetCb( sizeof( CPG ) );
	kdf.data.SetPv( &le_cpgSecondary );
	NDInsert( pfucb, pcsrOE, &kdf );

	CPG						cpgPrimary = sph.CpgPrimary();
	LittleEndian<CPG>		le_cpgPrimary = cpgPrimary;

	KeyFromLong( rgbKey, PgnoFDP( pfucb ) + cpgPrimary - 1 );
	kdf.key.suffix.SetCb( sizeof(PGNO) );
	kdf.key.suffix.SetPv( rgbKey );
	
	kdf.data.SetCb( sizeof(le_cpgPrimary) );
	kdf.data.SetPv( &le_cpgPrimary );

	ERR			err;
	BOOKMARK	bm;
	NDGetBookmarkFromKDF( pfucb, kdf, &bm );
	err = ErrNDSeek( pfucb, pcsrOE, bm );
	Assert( wrnNDFoundLess == err || wrnNDFoundGreater == err );
	if ( wrnNDFoundLess == err )
		{
		Assert( pcsrOE->Cpage().Clines() - 1 == pcsrOE->ILine() );
		pcsrOE->IncrementILine();
		}
	NDInsert( pfucb, pcsrOE, &kdf );
	FUCBResetOwnExt( pfucb );
	
	return;
	}


LOCAL VOID SPIConvertUpdateAE(
	FUCB			*pfucb, 
	CSR 			*pcsrAE, 
	EXTENTINFO		*rgext, 
	INT				iextMac, 
	PGNO			pgnoSecondaryFirst, 
	CPG				cpgSecondary )
	{
	if ( !FBTIUpdatablePage( *pcsrAE ) )
		{
		Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		return;
		}

	KEYDATAFLAGS	kdf;
	BYTE			rgbKey[sizeof(PGNO)];
	
	//	create available extent tree and insert avail extents
	//
	Assert( pcsrAE->Latch() == latchWrite );
	Assert( !FFUCBAvailExt( pfucb ) );
	FUCBSetAvailExt( pfucb );

	SPIInitSplitBuffer( pfucb, pcsrAE );

	Assert( cpgSecondary >= 2 );
	Assert( 0 == pcsrAE->Cpage().Clines() );
	pcsrAE->SetILine( 0 );
	
	//	insert secondary extent in AvailExt
	//
	CPG		cpgExtent = cpgSecondary - 1 - 1;
	PGNO	pgnoLast = pgnoSecondaryFirst + cpgSecondary - 1;
	if ( cpgExtent != 0 )
		{
		KeyFromLong( rgbKey, pgnoLast );
		kdf.Nullify();
		kdf.key.suffix.SetCb( sizeof( PGNO ) );
		kdf.key.suffix.SetPv( rgbKey );

		LittleEndian<CPG> le_cpgExtent = cpgExtent;
		kdf.data.SetCb( sizeof( CPG ) );
		kdf.data.SetPv( &le_cpgExtent );

		NDInsert( pfucb, pcsrAE, &kdf );
		}

	Assert( latchWrite == pcsrAE->Latch() );
	
	//	rgext contains list of available pages, with highest pages
	//	first, so traverse the list in reverse order to force append
	for ( INT iext = iextMac - 1; iext >= 0; iext-- )
		{
		
		//	extent may have been fully allocated for OE and AE trees
		//
//		if ( 0 == rgext[iext].cpgExtent )
//			continue;

		//	NO!!  Above code is wrong!  OE and AE trees are allocated
		//	from a secondary extent when the FDP is converted from
		//	single to multiple. -- JLIEM
		Assert( rgext[iext].cpgExtent > 0 );
		
		KeyFromLong( rgbKey, rgext[iext].pgnoLastInExtent );
		kdf.Nullify();
		kdf.key.suffix.SetCb( sizeof(PGNO) );
		kdf.key.suffix.SetPv( rgbKey );
		kdf.data.SetCb( sizeof(PGNO) );
		LittleEndian<CPG> le_cpgExtent = rgext[iext].cpgExtent;
		kdf.data.SetPv( &le_cpgExtent );
		kdf.data.SetCb( sizeof(rgext[iext].cpgExtent) );

		//	seek to point of insert
		//
		if ( 0 < pcsrAE->Cpage().Clines() )
			{
			ERR			err;
			BOOKMARK	bm;
			NDGetBookmarkFromKDF( pfucb, kdf, &bm );
			err = ErrNDSeek( pfucb, pcsrAE, bm );
			Assert( wrnNDFoundLess == err || wrnNDFoundGreater == err );
			if ( wrnNDFoundLess == err )
				pcsrAE->IncrementILine();
			}
			
		NDInsert( pfucb, pcsrAE, &kdf );
		}

	FUCBResetAvailExt( pfucb );
	Assert( pcsrAE->Latch() == latchWrite );
	
	return;
	}


//	update FDP page for convert
//
INLINE VOID SPIConvertUpdateFDP( FUCB *pfucb, CSR *pcsrRoot, SPACE_HEADER *psph )
	{
	Assert( latchWrite == pcsrRoot->Latch() );
	Assert( pcsrRoot->FDirty() );
	DATA	data;
	
	//	update external header to multiple extent space
	//
	data.SetPv( psph );
	data.SetCb( sizeof( *psph ) );
	NDSetExternalHeader( pfucb, pcsrRoot, &data );
	
	return;
	}


//	perform convert operation
//
VOID SPIPerformConvert( FUCB			*pfucb, 
						CSR				*pcsrRoot, 
						CSR				*pcsrAE, 
						CSR				*pcsrOE, 
						SPACE_HEADER	*psph, 
						PGNO			pgnoSecondaryFirst, 
						CPG				cpgSecondary,
						EXTENTINFO		*rgext,
						INT				iextMac )
	{
	SPIConvertUpdateOE( pfucb, pcsrOE, *psph, pgnoSecondaryFirst, cpgSecondary );

	SPIConvertUpdateAE( pfucb, pcsrAE, rgext, iextMac, pgnoSecondaryFirst, cpgSecondary );

	SPIConvertUpdateFDP( pfucb, pcsrRoot, psph );
	}


//	gets space header from root page
//	calcualtes extent info and places in rgext
//
VOID SPIConvertGetExtentinfo( FUCB			*pfucb, 
							  CSR			*pcsrRoot, 
							  SPACE_HEADER	*psph,
							  EXTENTINFO	*rgext, 
							  INT			*piextMac )
	{
	//	determine all available extents.  Maximum number of
	//	extents is (cpgSmallSpaceAvailMost+1)/2 + 1.
	//
	NDGetExternalHeader( pfucb, pcsrRoot );
	Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );

	//	get single extent allocation information
	//
	UtilMemCpy( psph, pfucb->kdfCurr.data.Pv(), sizeof(SPACE_HEADER) );
	
	//	if available extent for space after rgbitAvail, then
	//	set rgextT[0] to extent, otherwise set rgextT[0] to
	//	last available extent.
	//
	SPIConvertCalcExtents( *psph, PgnoFDP( pfucb ), rgext, piextMac );

	return;
	}

	
//	convert single extent root page external header to 
//	multiple extent space tree.
//
LOCAL ERR ErrSPIConvertToMultipleExtent( FUCB *pfucb, CPG cpgReq, CPG cpgMin )
	{
	ERR				err;
	SPACE_HEADER	sph;
	PGNO			pgnoSecondaryFirst	= pgnoNull;
	INT				iextMac				= 0;
	FUCB			*pfucbT;
	CSR				csrOE;
	CSR				csrAE;
	CSR				*pcsrRoot			= pfucb->pcsrRoot;
	DBTIME 			dbtimeBefore 		= dbtimeNil;
	LGPOS			lgpos;
	EXTENTINFO		rgextT[(cpgSmallSpaceAvailMost+1)/2 + 1];

	Assert( NULL != pcsrRoot );
	Assert( latchRIW == pcsrRoot->Latch() );
	AssertSPIPfucbOnRoot( pfucb );
	Assert( !FFUCBSpace( pfucb ) );
	Assert( ObjidFDP( pfucb ) != 0 );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	const ULONG fPageFlags = pcsrRoot->Cpage().FFlags()
								& ~CPAGE::fPageRepair
								& ~CPAGE::fPageRoot
								& ~CPAGE::fPageLeaf
								& ~CPAGE::fPageParentOfLeaf;

	//	get extent info from space header of root page
	//
	SPIConvertGetExtentinfo( pfucb, pcsrRoot, &sph, rgextT, &iextMac );
	Assert( sph.FSingleExtent() );
	
	//	allocate secondary extent for space trees
	//
	CallR( ErrBTOpen( pfucb->ppib, pfucb->u.pfcb, &pfucbT, fFalse ) );

	//	account for OE/AE root pages
	cpgMin += 2;
	cpgMin += 2;

	//  allocate enough pages for OE/AE root paages, plus enough
	//	to satisfy the space request that caused us to
	//	burst to multiple extent in the first place
	cpgMin = max( cpgMultipleExtentConvert, cpgMin );
	cpgReq = max( cpgMin, cpgReq );

	//	database always uses multiple extent
	//
	Assert( sph.PgnoParent() != pgnoNull );
	Call( ErrSPGetExt( pfucbT, sph.PgnoParent(), &cpgReq, cpgMin, &pgnoSecondaryFirst ) );
	Assert( cpgReq >= cpgMin );
	Assert( pgnoSecondaryFirst != pgnoNull );

	//	set pgnoOE and pgnoAE in B-tree
	//
	Assert( pgnoSecondaryFirst != pgnoNull );
	sph.SetMultipleExtent();
	sph.SetPgnoOE( pgnoSecondaryFirst );
	pfucb->u.pfcb->SetPgnoOE( sph.PgnoOE() );
	pfucb->u.pfcb->SetPgnoAE( sph.PgnoAE() );

	//	represent all space in space trees.  Note that maximum
	//	space allowed to accumulate before conversion to 
	//	large space format, should be representable in single
	//	page space trees.
	//
	//	create OwnExt and insert primary and secondary owned extents
	//
	Assert( !FFUCBVersioned( pfucbT ) );
	Assert( !FFUCBSpace( pfucbT ) );
		
	Assert( pgnoNull != PgnoAE( pfucb ) );
	Assert( pgnoNull != PgnoOE( pfucb ) );
	
	//	get pgnoOE and depend on pgnoRoot
	//
	FUCBSetOwnExt( pfucbT );
	Call( csrOE.ErrGetNewPage( pfucbT->ppib, 
							   pfucbT->ifmp,
							   PgnoRoot( pfucbT ), 
							   ObjidFDP( pfucbT ), 
							   fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree,
							   pfucbT->u.pfcb->Tableclass() ) );
	Assert( latchWrite == csrOE.Latch() );
	FUCBResetOwnExt( pfucbT );

	Call( ErrBFDepend( csrOE.Cpage().PBFLatch(), pfucb->pcsrRoot->Cpage().PBFLatch() ) );

	//	get pgnoAE and depend on pgnoRoot
	//
	FUCBSetAvailExt( pfucbT );
	Call( csrAE.ErrGetNewPage( pfucbT->ppib, 
							   pfucbT->ifmp,
							   PgnoRoot( pfucbT ), 
							   ObjidFDP( pfucbT ),
							   fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree,
							   pfucbT->u.pfcb->Tableclass() ) );
	Assert( latchWrite == csrAE.Latch() );
	FUCBResetAvailExt( pfucbT );

	Call( ErrBFDepend( csrAE.Cpage().PBFLatch(), pfucb->pcsrRoot->Cpage().PBFLatch() ) );

	SPIUpgradeToWriteLatch( pfucb );

	dbtimeBefore = pfucb->pcsrRoot->Dbtime();
	Assert ( dbtimeNil != dbtimeBefore );
	//	set max dbtime for the three pages in all the pages
	//
	SPIDirtyAndSetMaxDbtime( pfucb->pcsrRoot, &csrOE, &csrAE );

	if ( pfucb->pcsrRoot->Dbtime() > dbtimeBefore )
		{
		//	log convert 
		//	convert can not fail after this operation
		err = ErrLGConvertFDP( pfucb, pcsrRoot, &sph, pgnoSecondaryFirst, cpgReq, dbtimeBefore, &lgpos );
		}
	else
		{
		FireWall();
		err = ErrERRCheck( JET_errDbTimeCorrupted );
		}

	// see if we have to revert the time on the page
	if ( JET_errSuccess > err )
		{
		pfucb->pcsrRoot->RevertDbtime( dbtimeBefore );
		}
	Call ( err );
	
	pcsrRoot->Cpage().SetLgposModify( lgpos );
	csrOE.Cpage().SetLgposModify( lgpos );
	csrAE.Cpage().SetLgposModify( lgpos );

	//	perform convert operation
	//
	SPIPerformConvert(
			pfucbT,
			pcsrRoot,
			&csrAE,
			&csrOE,
			&sph,
			pgnoSecondaryFirst,
			cpgReq,
			rgextT,
			iextMac );

	AssertSPIPfucbOnRoot( pfucb );

HandleError:
	Assert( err != JET_errKeyDuplicate );
	csrAE.ReleasePage();
	csrOE.ReleasePage();
	Assert( pfucbT != pfucbNil );
	BTClose( pfucbT );
	return err;
	}

 
INLINE BOOL FSPIAllocateAllAvail(
	const CPG	cpgAvailExt,
	const CPG	cpgReq,
	const LONG	lPageFragment,
	const PGNO	pgnoFDP )
	{
	BOOL fAllocateAllAvail	= fTrue;

	// Use up all available pages if less than or equal to
	// requested amount.

	if ( cpgAvailExt > cpgReq )
		{
		if ( pgnoSystemRoot == pgnoFDP )
			{
			// If allocating extent from database, ensure we don't
			// leave behind an extent smaller than cpgSmallGrow,
			// because that's the smallest size a given table
			// will grow
			if ( cpgAvailExt >= cpgReq + cpgSmallGrow )
				fAllocateAllAvail = fFalse;
			}
		else if ( cpgReq < lPageFragment || cpgAvailExt >= cpgReq + lPageFragment )
			{
			// Don't use all if only requested a small amount
			// (ie. cpgReq < lPageFragment) or there is way more available
			// than we requested (ie. cpgAvailExt >= cpgReq+lPageFragment)
			fAllocateAllAvail = fFalse;
			}
		} 

	return fAllocateAllAvail;
	}


//	gets an extent from tree
//	if space not available in given tree, get from its parent
//
//	pfucbParent cursor placed on root of tree to get the extent from
//		root page should be RIW latched
//	
//	*pcpgReq input is number of pages requested; 
//			 output is number of pages granted
//	cpgMin is minimum neeeded
//	*ppgnoFirst input gives locality of extent needed
//				output is first page of allocated extent
//
LOCAL ERR ErrSPIGetExt(
	FUCB		*pfucbParent,
	CPG			*pcpgReq,
	CPG			cpgMin,
	PGNO		*ppgnoFirst,
	BOOL		fSPFlags = 0,
	UINT		fPageFlags = 0,
	OBJID		*pobjidFDP = NULL )
	{
	ERR			err;
	PIB			* const ppib			= pfucbParent->ppib;
	FCB			* const pfcb			= pfucbParent->u.pfcb;
	CPG 		cpgReq					= *pcpgReq;
	FUCB		*pfucbAE				= pfucbNil;
	SPCACHE		**ppspcache				= NULL;
	BOOL		fFoundNextAvailSE		= fFalse;
	DIB		 	dib;
	CPG			cpgAvailExt;
	PGNO		pgnoAELast;
	BYTE		rgbKey[sizeof(PGNO)];
	BOOKMARK	bm;

	//	check parameters.  If setting up new FDP, increment requested number of
	//	pages to account for consumption of first page to make FDP.
	//
	Assert( *pcpgReq > 0 || ( (fSPFlags & fSPNewFDP) && *pcpgReq == 0 ) );
	Assert( *pcpgReq >= cpgMin );
	AssertSPIPfucbOnRoot( pfucbParent );
	
#ifdef SPACECHECK
	Assert( !( ErrSPIValidFDP( ppib, pfucbParent->ifmp, PgnoFDP( pfucbParent ) ) < 0 ) );
#endif

	//	if a new FDP is requested, increment request count by FDP overhead,
	//	unless the count was smaller than the FDP overhead, in which case
	//	just allocate enough to satisfy the overhead (because the FDP will
	//	likely be small).
	//
	if ( fSPFlags & fSPNewFDP )
		{
		const CPG	cpgFDPMin = ( fSPFlags & fSPMultipleExtent ?
										cpgMultipleExtentMin :
										cpgSingleExtentMin );
		cpgMin = max( cpgMin, cpgFDPMin );
		*pcpgReq = max( *pcpgReq, cpgMin );

		Assert( NULL != pobjidFDP );
		}

	Assert( cpgMin > 0 );

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucbParent, fTrue );
		}

	//	if single extent optimization, then try to allocate from
	//	root page space map.  If cannot satisfy allcation, convert
	//	to multiple extent representation.
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		AssertSPIPfucbOnRoot( pfucbParent );

		//	if any chance in satisfying request from extent
		//	then try to allcate requested extent, or minimum
		//	extent from first available extent.  Only try to 
		//	allocate the minimum request, to facillitate
		//	efficient space usage.
		//
		if ( cpgMin <= cpgSmallSpaceAvailMost )
			{
			SPACE_HEADER	sph;
			UINT			rgbitT;
			PGNO			pgnoAvail;
			DATA			data;
			INT				iT;

			//	get external header
			//
			NDGetExternalHeader( pfucbParent, pfucbParent->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbParent->kdfCurr.data.Cb() );

			//	get single extent allocation information
			//
			UtilMemCpy( &sph, pfucbParent->kdfCurr.data.Pv(), sizeof(sph) );

			const PGNO	pgnoFDP			= PgnoFDP( pfucbParent );
			const CPG	cpgSingleExtent	= min( sph.CpgPrimary(), cpgSmallSpaceAvailMost+1 );	//	+1 because pgnoFDP is not represented in the single extent space map
			Assert( cpgSingleExtent > 0 );
			Assert( cpgSingleExtent <= cpgSmallSpaceAvailMost+1 );

			//	find first fit
			//
			//	make mask for minimum request
			//
			Assert( cpgMin > 0 );
			Assert( cpgMin <= 32 );
			for ( rgbitT = 1, iT = 1; iT < cpgMin; iT++ )
				{
				rgbitT = (rgbitT<<1) + 1;
				}

			for( pgnoAvail = pgnoFDP + 1;
				pgnoAvail + cpgMin <= pgnoFDP + cpgSingleExtent;
				pgnoAvail++, rgbitT <<= 1 )
				{
				Assert( rgbitT != 0 );
				if ( ( rgbitT & sph.RgbitAvail() ) == rgbitT ) 
					{
					SPIUpgradeToWriteLatch( pfucbParent );

					sph.SetRgbitAvail( sph.RgbitAvail() ^ rgbitT );
					data.SetPv( &sph );
					data.SetCb( sizeof(sph) );
					CallR( ErrNDSetExternalHeader( pfucbParent, pfucbParent->pcsrRoot, &data, fDIRNull ) );

					//	set up allocated extent as FDP if requested
					//
					Assert( pgnoAvail != pgnoNull );
					*ppgnoFirst = pgnoAvail;
					*pcpgReq = cpgMin;
					goto NewFDP;
					}
				}
			}

		CallR( ErrSPIConvertToMultipleExtent( pfucbParent, *pcpgReq, cpgMin ) ); 
		}

	//	for secondary extent allocation, only the normal Btree operations
	//	are logged. For allocating a new FDP, a special create FDP
	//	record is logged instead, since the new FDP and space pages need
	//	to be initialized as part of recovery.
	//
	//	move to available extents
	//
	CallR( ErrSPIOpenAvailExt( ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

#ifdef CONVERT_VERBOSE
		CONVPrintF2( "\n(a)Dest DB (pgnoFDP %d): Looking for %d pages...", 
			PgnoFDP( pfucbAE ), cpgMin );
#endif
		

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;

	//	begin search for first extent with size greater than request.
	//	Allocate secondary extent recursively until satisfactory extent found
	//
	//	most of the time, we simply start searching from the beginning of the AvailExt tree,
	//	but if this is the db root, optimise for the common case (request for an SE) by skipping
	//	small AvailExt nodes
	PGNO	pgnoSeek;
	pgnoSeek = ( cpgMin >= cpageSEDefault ? pfcb->PgnoNextAvailSE() : pgnoNull );
	KeyFromLong( rgbKey, pgnoSeek );

	dib.dirflag = fDIRFavourNext;

	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadTouch ) ) < 0 )
		{
		Assert( err != JET_errNoCurrentRecord );
		if ( err == JET_errRecordNotFound )
			{
			//	no record in availExt tree
			//
			goto GetFromSecondaryExtent;
			}
		#ifdef DEBUG
			DBGprintf( "ErrSPGetExt could not down into available extent tree.\n" );
		#endif
		goto HandleError;
		}


	//	loop through extents looking for one large enough for allocation
	//
	do
		{
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAvailExt >= 0 );

		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

		if ( 0 == cpgAvailExt )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents.
			Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
			Pcsr( pfucbAE )->Downgrade( latchReadTouch );
			}
		else
			{
			if ( !fFoundNextAvailSE
				&& cpgAvailExt >= cpageSEDefault )
				{
				pfcb->SetPgnoNextAvailSE( pgnoAELast - cpgAvailExt + 1 );
				fFoundNextAvailSE = fTrue;
				}

			if ( cpgAvailExt >= cpgMin )
				{
				//	if no extent with cpg >= cpageSEDefault, then ensure NextAvailSE
				//	pointer is at least as far as we've scanned
				if ( !fFoundNextAvailSE
					&& pfcb->PgnoNextAvailSE() <= pgnoAELast )
					{
					pfcb->SetPgnoNextAvailSE( pgnoAELast + 1 );
					}
				goto AllocateCurrent;
				}
			}

		err = ErrBTNext( pfucbAE, fDIRNull );
		}
	while ( err >= 0 );

	if ( err != JET_errNoCurrentRecord )
		{
		#ifdef DEBUG
			DBGprintf( "ErrSPGetExt could not scan available extent tree.\n" );
		#endif
		Assert( err < 0 );
		goto HandleError;
		}

	if ( !fFoundNextAvailSE )
		{
		//	didn't find any extents with at least cpageSEDefault pages, so
		//	set pointer to beyond last extent
		pfcb->SetPgnoNextAvailSE( pgnoAELast + 1 );
		}

GetFromSecondaryExtent:
	BTUp( pfucbAE );
	Call( ErrSPIGetSE(
			pfucbParent,
			pfucbAE,
			*pcpgReq, 
			cpgMin,
			fSPFlags & fSPSplitting ) );
	Assert( Pcsr( pfucbAE )->FLatched() );
	Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(CPG) );
	cpgAvailExt = *(UnalignedLittleEndian< CPG > *) pfucbAE->kdfCurr.data.Pv();
	Assert( cpgAvailExt > 0 );
	Assert( cpgAvailExt >= cpgMin );

	Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

AllocateCurrent:
	*ppgnoFirst = pgnoAELast - cpgAvailExt + 1;

	if ( FSPIAllocateAllAvail( cpgAvailExt, *pcpgReq, PinstFromPpib( ppib )->m_lPageFragment, pfcb->PgnoFDP() ) )
		{
		*pcpgReq = cpgAvailExt;

		// UNDONE: *pcpgReq may actually be less than cpgMin, because
		// some pages may have been used up if split occurred while
		// updating AvailExt.  However, as long as there's at least
		// one page, this is okay, because the only ones to call this
		// function are GetPage() (for split) and CreateDirectory()
		// (for CreateTable/Index).  The former only ever asks for
		// one page at a time, and the latter can deal with getting
		// only one page even though it asked for more.
		// Assert( *pcpgReq >= cpgMin );
		Assert( cpgAvailExt > 0 );
		
		Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node	
		}
	else
		{
		DATA	data;
		
		//	*pcpgReq is already set to the return value
		//
		Assert( cpgAvailExt > *pcpgReq );
		
		LittleEndian<CPG>	le_cpg;
		le_cpg = cpgAvailExt - *pcpgReq;
		data.SetCb( sizeof(PGNO) );
		data.SetPv( &le_cpg );
		Call( ErrBTReplace( pfucbAE, data, fDIRNoVersion ) );
		CallS( err );		// do we need the following stmt? 
		err = JET_errSuccess;
		}

	BTUp( pfucbAE );


NewFDP:
	//	initialize extent as new tree, including support for
	//	localized space allocation.
	//
	if ( fSPFlags & fSPNewFDP )
		{
		Assert( PgnoFDP( pfucbParent ) != *ppgnoFirst );
		if ( fSPFlags & fSPMultipleExtent )
			{
			Assert( *pcpgReq >= cpgMultipleExtentMin );
			}
		else
			{
			Assert( *pcpgReq >= cpgSingleExtentMin );
			}

		//	database root is allocated by DBInitDatabase
		//
		Assert( pgnoSystemRoot != *ppgnoFirst );

		VEREXT	verext;
		verext.pgnoFDP = PgnoFDP( pfucbParent );
		verext.pgnoChildFDP = *ppgnoFirst;
		verext.pgnoFirst = *ppgnoFirst;
		verext.cpgSize = *pcpgReq;

		if ( !( fSPFlags & fSPUnversionedExtent ) )
			{
			VER *pver = PverFromIfmp( pfucbParent->ifmp );
			Call( pver->ErrVERFlag( pfucbParent, operAllocExt, &verext, sizeof(verext) ) );
			}
		
		Call( ErrSPCreate(
					ppib,
					pfucbParent->ifmp,
					PgnoFDP( pfucbParent ),
					*ppgnoFirst,
					*pcpgReq,
					fSPFlags,
					fPageFlags,
					pobjidFDP ) );
		Assert( *pobjidFDP > objidSystemRoot );
						
		//	reduce *pcpgReq by pages allocated for tree root
		//
		if ( fSPFlags & fSPMultipleExtent )
			{
			(*pcpgReq) -= cpgMultipleExtentMin;
			}
		else
			{
			(*pcpgReq) -= cpgSingleExtentMin;
			}

#ifdef DEBUG
		if ( 0 == ppib->level )
			{
			FMP		*pfmp	= &rgfmp[pfucbParent->ifmp];
			Assert( fSPFlags & fSPUnversionedExtent );
			Assert( dbidTemp == pfmp->Dbid()
				|| pfmp->FCreatingDB() );
			}
#endif			
		}

	//	assign error
	//
	err = JET_errSuccess;

#ifdef TRACE
	if ( (fSPFlags & fSPNewFDP) )
		{
		DBGprintf( "get space %lu at %lu for FDP from %d.%lu\n", 
		  *pcpgReq + ( fSPFlags & fSPMultipleExtent ? cpgMultipleExtentMin : cpgSingleExtentMin ),
		  *ppgnoFirst, 
		  pfucbParent->ifmp, 
		  PgnoFDP( pfucbParent ) );
		}
	else
		{
		DBGprintf( "get space %lu at %lu from %d.%lu\n", 
			*pcpgReq, 
			*ppgnoFirst, 
			pfucbParent->ifmp, 
			PgnoFDP( pfucbParent ) );
		}
#endif

#ifdef DEBUG
	if ( PinstFromPpib( ppib )->m_plog->m_fDBGTraceBR )
		{
		INT cpg = 0;
		for ( ; 
				cpg < ( (fSPFlags & fSPNewFDP) ? 
						*pcpgReq + ( (fSPFlags & fSPMultipleExtent) ? 3 : 1 ) : *pcpgReq ); 
				cpg++ )
			{
			char sz[256];
			sprintf( sz, "ALLOC ONE PAGE (%d:%ld) %d:%ld",
				pfucbParent->ifmp, PgnoFDP( pfucbParent ),
				pfucbParent->ifmp, *ppgnoFirst + cpg );
			CallS( PinstFromPpib( ppib )->m_plog->ErrLGTrace( ppib, sz ) );
			}
		}
#endif

HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	return err;
	}


ERR ErrSPGetExt(
	FUCB	*pfucb,
	PGNO	pgnoFDP,
	CPG		*pcpgReq,
	CPG		cpgMin,
	PGNO	*ppgnoFirst,
	BOOL	fSPFlags,
	UINT	fPageFlags,
	OBJID	*pobjidFDP )
	{
	ERR 	err;
	FUCB	*pfucbParent = pfucbNil;

	Assert( !Pcsr( pfucb )->FLatched() );

	//	open cursor on Parent and RIW latch root page
	//
	CallR( ErrBTIOpenAndGotoRoot( pfucb->ppib, pgnoFDP, pfucb->ifmp, &pfucbParent ) );
					 
	//  allocate an extent
	//
	err = ErrSPIGetExt( pfucbParent, pcpgReq, cpgMin, ppgnoFirst, fSPFlags, fPageFlags, pobjidFDP );
	Assert( Pcsr( pfucbParent ) == pfucbParent->pcsrRoot );
	
	//	latch may have been upgraded to write latch
	//	by single extent space allocation operation.
	//
	Assert( pfucbParent->pcsrRoot->Latch() == latchRIW 
		|| pfucbParent->pcsrRoot->Latch() == latchWrite );
	pfucbParent->pcsrRoot->ReleasePage();
	pfucbParent->pcsrRoot = pcsrNil;

	Assert( pfucbParent != pfucbNil );
	BTClose( pfucbParent );
	return err;
	}


#ifdef DEBUG	
INLINE ERR ErrSPIFindOE( PIB *ppib, FCB *pfcb, const PGNO pgnoFirst, const PGNO pgnoLast )
	{
	ERR			err;
	FUCB		*pfucbOE;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	BOOKMARK	bm;
	DIB			dib;
	BYTE		rgbKey[sizeof(PGNO)];
	
	CallR( ErrSPIOpenOwnExt( ppib, pfcb, &pfucbOE ) );
	
	//	find bounds of owned extent which contains extent to be freed
	//
	KeyFromLong( rgbKey, pgnoFirst );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	if ( wrnNDFoundLess == err )
		{
		//	landed on node previous to one we want
		//	move next
		//
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 ==
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		}

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(CPG) );
	cpgOESize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

	// Verify that the extent to be freed is contained entirely within
	// this OwnExt node (since we don't allow coalescing of AvailExt
	// nodes across OwnExt boundaries).
	Assert( pgnoOELast - cpgOESize + 1 <= pgnoFirst );
	Assert( pgnoOELast >= pgnoLast );

	err = JET_errSuccess;
	
HandleError:
	BTClose( pfucbOE );
	
	return err;
	}
#endif	// DEBUG	
	

//	ErrSPGetPage
//	========================================================================
//	ERR ErrSPGetPage( FUCB *pfucb, PGNO *ppgnoLast )
//
//	Allocates page from FUCB cache. If cache is nil,
//	allocate from available extents. If availalbe extent tree is empty, 
//	a secondary extent is allocated from the parent FDP to 
//	satisfy the page request. A page closest to 
//	*ppgnoLast is allocated. If *ppgnoLast is pgnoNull, 
//	first free page is allocated
//
//	PARAMETERS	
//		pfucb  		FUCB providing FDP page number and process identifier block
//					cursor should be on root page RIW latched
//		ppgnoLast   may contain page number of last allocated page on
//		   			input, on output contains the page number of the allocated page
//
//-
ERR ErrSPGetPage( FUCB *pfucb, PGNO *ppgnoLast )
	{
	ERR			err;
	PIB			* const ppib			= pfucb->ppib;
	FCB			* const pfcb			= pfucb->u.pfcb;
	FUCB 		*pfucbAE				= pfucbNil;
	CPG			cpgAvailExt;
	PGNO		pgnoAELast;
	SPCACHE		**ppspcache				= NULL;
	BYTE		rgbKey[sizeof(PGNO)];
	BOOKMARK	bm;
	DIB			dib;
#ifdef DEBUG
	PGNO		pgnoSave				= *ppgnoLast;
#endif
	
	//	check for valid input
	//
	Assert( ppgnoLast != NULL );
	Assert( *ppgnoLast != pgnoNull );
	
	//	check FUCB work area for active extent and allocate first available
	//	page of active extent
	//
	if ( FFUCBSpace( pfucb ) )
		{
		const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );
		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CSR				*pcsrRoot	= pfucb->pcsrRoot;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsrRoot );

			UtilMemCpy( &spbuf, PspbufSPISpaceTreeRootPage( pfucb, pcsrRoot ), sizeof(SPLIT_BUFFER) );

			CallR( spbuf.ErrGetPage( ppgnoLast, fAvailExt ) );
			
			Assert( latchRIW == pcsrRoot->Latch() );
			pcsrRoot->UpgradeFromRIWLatch();
			
			data.SetPv( &spbuf );
			data.SetCb( sizeof(spbuf) );
			err = ErrNDSetExternalHeader( pfucb, pcsrRoot, &data, fDIRNull );

			//	reset to RIW latch
			pcsrRoot->Downgrade( latchRIW );

			return err;
			}
		else
			{
			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pfucb->pcsrRoot );
			SPIReportGetPageFromSplitBuffer( pfucb );
			return pfucb->u.pfcb->Psplitbuf( fAvailExt )->ErrGetPage( ppgnoLast, fAvailExt );
			}
		}

	//	check for valid input when alocating page from FDP
	//
#ifdef SPACECHECK
	Assert( !( ErrSPIValidFDP( pfucb->ppib, pfucb->ifmp, PgnoFDP( pfucb ) ) < 0 ) );
	if ( 0 != *ppgnoLast )
		{
		CallS( ErrSPIWasAlloc( pfucb, *ppgnoLast, (CPG)1 ) );
		}
#endif

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

	//	if single extent optimization, then try to allocate from
	//	root page space map.  If cannot satisfy allcation, convert
	//	to multiple extent representation.
	//
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER	sph;
		UINT			rgbitT;
		PGNO			pgnoAvail;
		DATA			data;

		AssertSPIPfucbOnRoot( pfucb );

		//	if any chance in satisfying request from extent
		//	then try to allcate requested extent, or minimum
		//	extent from first available extent.  Only try to 
		//	allocate the minimum request, to facillitate
		//	efficient space usage.
		//
		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );

		//	get single extent allocation information
		//
		UtilMemCpy( &sph, pfucb->kdfCurr.data.Pv(), sizeof(sph) );

		const PGNO	pgnoFDP			= PgnoFDP( pfucb );
		const CPG	cpgSingleExtent	= min( sph.CpgPrimary(), cpgSmallSpaceAvailMost+1 );	//	+1 because pgnoFDP is not represented in the single extent space map
		Assert( cpgSingleExtent > 0 );
		Assert( cpgSingleExtent <= cpgSmallSpaceAvailMost+1 );

		//	allocate first page
		//
		for( pgnoAvail = pgnoFDP + 1, rgbitT = 1;
			pgnoAvail <= pgnoFDP + cpgSingleExtent - 1;
			pgnoAvail++, rgbitT <<= 1 )
			{
			Assert( rgbitT != 0 );
			if ( rgbitT & sph.RgbitAvail() ) 
				{
				Assert( ( rgbitT & sph.RgbitAvail() ) == rgbitT );

				//  write latch page before update
				//
				SPIUpgradeToWriteLatch( pfucb );

				sph.SetRgbitAvail( sph.RgbitAvail() ^ rgbitT );
				data.SetPv( &sph );
				data.SetCb( sizeof(sph) );
				CallR( ErrNDSetExternalHeader( pfucb, pfucb->pcsrRoot, &data, fDIRNull ) );

				//	set output parameter and done
				//
				*ppgnoLast = pgnoAvail;
				goto Done;
				}
			}

		CallR( ErrSPIConvertToMultipleExtent( pfucb, 1, 1 ) );
		}

	//	open cursor on available extent tree
	//
	AssertSPIPfucbOnRoot( pfucb );
	CallR( ErrSPIOpenAvailExt( pfucb->ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;

	//	get node of next contiguous page
	//
FindPage:
	KeyFromLong( rgbKey, *ppgnoLast );

	dib.dirflag = fDIRNull;

	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadTouch ) ) < 0 )
		{
		Assert( err != JET_errNoCurrentRecord );
		if ( JET_errRecordNotFound == err )
			{
			//	no node in available extent tree,
			//	get a secondary extent
			//
			Assert( pgnoSave == *ppgnoLast );
			BTUp( pfucbAE );
			Call( ErrSPIGetSE( pfucb, pfucbAE, (CPG)1, (CPG)1, fTrue ) );
			Assert( Pcsr( pfucbAE )->FLatched() );
			}
		else
			{
			#ifdef DEBUG
				DBGprintf( "ErrSPGetPage could not go down into available extent tree.\n" );
			#endif
			Call( err );
			}
		}

	else if ( JET_errSuccess == err )
		{
		//	page in use is also in AvailExt
		AssertSz( fFalse, "AvailExt corrupted." );
		Call( ErrERRCheck( JET_errSPAvailExtCorrupted ) );
		}
	else
		{
		//	keep locality of reference
		//	get page closest to *ppgnoLast
		//
		
		//	we always favour FoundGreater, except in the pathological case of no more nodes
		//	greater, in which case we return FoundLess
		Assert( wrnNDFoundGreater == err || wrnNDFoundLess == err );

		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert ( cpgAvailExt >= 0 );
		if ( 0 == cpgAvailExt )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents and retry.
			Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
			BTUp( pfucbAE );
			goto FindPage;
			}
		}


	//	allocate first page in node and return code
	//
	Assert( err >= 0 );
	Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
	Assert( cpgAvailExt > 0 );

	Assert( pfucbAE->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

	if ( *ppgnoLast >= pgnoAELast - cpgAvailExt + 1
		&& *ppgnoLast <= pgnoAELast )
		{
		//	page in use is also in AvailExt
		AssertSz( fFalse, "AvailExt corrupted." );
		err = ErrERRCheck( JET_errSPAvailExtCorrupted );
		}

	*ppgnoLast = pgnoAELast - cpgAvailExt + 1;

	//	do not return the same page
	//
	Assert( *ppgnoLast != pgnoSave );

	if ( --cpgAvailExt == 0 )
		{
		Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
		}
	else
		{
		DATA	data;
		LittleEndian<CPG> le_cpgAvailExt;

		le_cpgAvailExt = cpgAvailExt;
		data.SetPv( &le_cpgAvailExt );
		data.SetCb( sizeof(PGNO) );
		Call( ErrBTReplace( pfucbAE, data, fDIRNoVersion ) );
		}

	BTUp( pfucbAE );
	err = JET_errSuccess;


Done:

#ifdef TRACE
	DBGprintf( "get space 1 at %lu from %d.%lu\n", *ppgnoLast, pfucb->ifmp, PgnoFDP( pfucb ) );
#endif

#ifdef DEBUG
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fDBGTraceBR )
		{
		char sz[256];
		sprintf( sz, "ALLOC ONE PAGE (%d:%ld) %d:%ld",
				pfucb->ifmp, pfcb->PgnoFDP(),
				pfucb->ifmp, *ppgnoLast );
		CallS( PinstFromIfmp( pfucb->ifmp )->m_plog->ErrLGTrace( pfucb->ppib, sz ) );
		}
#endif

HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	return err;
	}


LOCAL ERR ErrSPIFreeSEToParent(
	FUCB		*pfucb,
	FUCB		*pfucbOE,
	FUCB		*pfucbAE,
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	ERR			err;
	FCB			*pfcb = pfucb->u.pfcb;
	FUCB		*pfucbParent = pfucbNil;
	FCB			*pfcbParent;
	BOOL		fState;
	BOOKMARK	bm;
	DIB 		dib;
	BYTE		rgbKey[sizeof(PGNO)];
			
	Assert( pfcbNil != pfcb );

	//	get parentFDP's root pgno
	//	cursor passed in should be at root of tree 
	//	so we can access pgnoParentFDP from the external header
	const PGNO	pgnoParentFDP = PgnoSPIParentFDP( pfucb );
	if ( pgnoParentFDP == pgnoNull )
		{
		//	UNDONE:	free secondary extents to device
		return JET_errSuccess;
		}
	
	//	parent must always be in memory
	//
	pfcbParent = FCB::PfcbFCBGet( pfucb->ifmp, pgnoParentFDP, &fState );
	Assert( pfcbParent != pfcbNil );
	Assert( fFCBStateInitialized == fState );
	Assert( !pfcb->FTypeNull() );
			
	if ( pfcb->FTypeSecondaryIndex() || pfcb->FTypeLV() )
		{
		Assert( pfcbParent->FTypeTable() );
		}
	else
		{
		Assert( pfcbParent->FTypeDatabase() );
		Assert( !pfcbParent->FDeletePending() );
		Assert( !pfcbParent->FDeleteCommitted() );
		}

	//	delete available extent node
	//
	Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
	BTUp( pfucbAE );

	//	seek to owned extent node and delete it
	//
	KeyFromLong( rgbKey, pgnoLast );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRNull;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
				
	Call( ErrBTFlagDelete( pfucbOE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
	BTUp( pfucbOE );

	//	free extent to parent FDP
	//
	//	open cursor on parent
	//	access root page with RIW latch
	//
	Call( ErrBTOpen( pfucb->ppib, pfcbParent, &pfucbParent ) );
	Call( ErrBTIGotoRoot( pfucbParent, latchRIW ) );
	
	pfucbParent->pcsrRoot = Pcsr( pfucbParent );
	Assert( !FFUCBSpace( pfucbParent ) );
	err = ErrSPFreeExt( pfucbParent, pgnoLast - cpgSize + 1, cpgSize );
	pfucbParent->pcsrRoot = pcsrNil;

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		BTClose( pfucbParent );
		}

	pfcbParent->Release();

	return err;
	}


LOCAL VOID SPIReportLostPages(
	const DBID	ifmp,
	const OBJID	objidFDP,
	const PGNO	pgnoLast,
	const CPG	cpgLost )
	{
	CHAR		szStartPagesLost[16];
	CHAR		szEndPagesLost[16];
	CHAR		szObjidFDP[16];
	const CHAR	*rgszT[4];

	sprintf( szStartPagesLost, "%d", pgnoLast - cpgLost + 1 );
	sprintf( szEndPagesLost, "%d", pgnoLast );
	sprintf( szObjidFDP, "%d", objidFDP );
		
	rgszT[0] = rgfmp[ifmp].SzDatabaseName();
	rgszT[1] = szStartPagesLost;
	rgszT[2] = szEndPagesLost;
	rgszT[3] = szObjidFDP;
	
	UtilReportEvent(
			eventWarning,
			SPACE_MANAGER_CATEGORY,
			SPACE_LOST_ON_FREE_ID,
			4,
			rgszT );
	}


//	ErrSPFreeExt
//	========================================================================
//	ERR ErrSPFreeExt( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize )
//
//	Frees an extent to an FDP.	The extent, starting at page pgnoFirst
//	and cpgSize pages long, is added to available extent of the FDP.  If the
//	extent freed is a complete secondary extent of the FDP, or can be
//	coalesced with other available extents to form a complete secondary
//	extent, the complete secondary extent is freed to the parent FDP.
//
//	Besides, if the freed extent is contiguous with the FUCB space cache 
//	in pspbuf, the freed extent is added to the FUCB cache. Also, when 
//	an extent is freed recursively to the parentFDP, the FUCB on the parent 
//	shares the same FUCB cache.
//
//	PARAMETERS	pfucb			tree to which the extent is freed,
//								cursor should have currency on root page [RIW latched]
// 				pgnoFirst  		page number of first page in extent to be freed
// 				cpgSize			number of pages in extent to be freed
//
//
//	SIDE EFFECTS
//	COMMENTS
//-
ERR ErrSPFreeExt( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize )
	{
	ERR			err;
	PIB			* const ppib	= pfucb->ppib;
	FCB			* const pfcb	= pfucb->u.pfcb;
	PGNO  		pgnoLast		= pgnoFirst + cpgSize - 1;
	BOOL		fRootLatched	= fFalse;
	BOOL		fCoalesced		= fFalse;

	// FDP available extent and owned extent operation variables
	//
	BOOKMARK	bm;
	DIB 		dib;
	BYTE		rgbKey[sizeof(PGNO)];

	// owned extent and avail extent variables
	//
	FUCB 		*pfucbAE		= pfucbNil;
	FUCB		*pfucbOE		= pfucbNil;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	PGNO		pgnoAELast;
	CPG			cpgAESize;

	// check for valid input
	//
	Assert( cpgSize > 0 );

#ifdef SPACECHECK
	CallS( ErrSPIValidFDP( ppib, pfucb->ifmp, PgnoFDP( pfucb ) ) );
#endif

	//	if in space tree, return page back to split buffer
	if ( FFUCBSpace( pfucb ) ) 
		{
		const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );

		//	must be returning space due to split failure
		Assert( 1 == cpgSize );

		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CSR				*pcsrRoot	= pfucb->pcsrRoot;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsrRoot );

			UtilMemCpy( &spbuf, PspbufSPISpaceTreeRootPage( pfucb, pcsrRoot ), sizeof(SPLIT_BUFFER) );

			spbuf.ReturnPage( pgnoFirst );
		
			Assert( latchRIW == pcsrRoot->Latch() );
			pcsrRoot->UpgradeFromRIWLatch();

			data.SetPv( &spbuf );
			data.SetCb( sizeof(spbuf) );
			err = ErrNDSetExternalHeader( pfucb, pcsrRoot, &data, fDIRNull );

			//	reset to RIW latch
			pcsrRoot->Downgrade( latchRIW );
			}
		else
			{
			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pfucb->pcsrRoot );
			pfucb->u.pfcb->Psplitbuf( fAvailExt )->ReturnPage( pgnoFirst );
			err = JET_errSuccess;
			}

		return err;
		}

	//	if caller did not have root page latched, get root page
	//
	if ( pfucb->pcsrRoot == pcsrNil )
		{
		Assert( !Pcsr( pfucb )->FLatched() );
		CallR( ErrBTIGotoRoot( pfucb, latchRIW ) );
		pfucb->pcsrRoot = Pcsr( pfucb );
		fRootLatched = fTrue;
		}
	else
		{
		Assert( pfucb->pcsrRoot->Pgno() == PgnoRoot( pfucb ) );
		Assert( pfucb->pcsrRoot->Latch() == latchRIW
			|| ( pfucb->pcsrRoot->Latch() == latchWrite && pgnoNull == pfcb->PgnoOE() ) ); // SINGLE EXTEND
		}

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

#ifdef SPACECHECK
	CallS( ErrSPIWasAlloc( pfucb, pgnoFirst, cpgSize ) == JET_errSuccess );
#endif

	//	if single extent format, then free extent in external header
	//
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER	sph;
		UINT			rgbitT;
		INT				iT;
		DATA			data;
		
		AssertSPIPfucbOnRoot( pfucb );
		Assert( cpgSize <= cpgSmallSpaceAvailMost );
		Assert( pgnoFirst > PgnoFDP( pfucb ) );								//	can't be equal, because then you'd be freeing root page to itself
		Assert( pgnoFirst - PgnoFDP( pfucb ) <= cpgSmallSpaceAvailMost );	//	extent must start and end within single-extent range
		Assert( pgnoFirst + cpgSize - 1 - PgnoFDP( pfucb ) <= cpgSmallSpaceAvailMost );

		//  write latch page before update
		//
		if ( latchWrite != pfucb->pcsrRoot->Latch() )
			{
			SPIUpgradeToWriteLatch( pfucb );
			}

		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		
		//	get single extent allocation information
		//
		UtilMemCpy( &sph, pfucb->kdfCurr.data.Pv(), sizeof(sph) );

		//	make mask for extent to free
		//
		for ( rgbitT = 1, iT = 1; iT < cpgSize; iT++ )
			{
			rgbitT = ( rgbitT << 1 ) + 1;
			}
		rgbitT <<= ( pgnoFirst - PgnoFDP( pfucb ) - 1 );
		sph.SetRgbitAvail( sph.RgbitAvail() | rgbitT );
		data.SetPv( &sph );
		data.SetCb( sizeof(sph) );
		Call( ErrNDSetExternalHeader( pfucb, pfucb->pcsrRoot, &data, fDIRNull ) );

		goto HandleError;  //  done
		}

	AssertSPIPfucbOnRoot( pfucb );

	//	open owned extent tree
	//
	Call( ErrSPIOpenOwnExt( ppib, pfcb, &pfucbOE ) );
	Assert( pfcb == pfucbOE->u.pfcb );

	//	find bounds of owned extent which contains extent to be freed
	//
	KeyFromLong( rgbKey, pgnoFirst );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	if ( err == wrnNDFoundLess )
		{
		//	landed on node previous to one we want
		//	move next
		//
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 ==
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		}

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgOESize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

	// Verify that the extent to be freed is contained entirely within
	// this OwnExt node (since we don't allow coalescing of AvailExt
	// nodes across OwnExt boundaries).
	if ( pgnoFirst > pgnoOELast
		|| pgnoLast < pgnoOELast - cpgOESize + 1 )
		{
		//	SPACE CORRUPTION!! One possibility is a crash in ErrSPIReservePagesForSplit()
		//	after inserting into the SPLIT_BUFFER but before being able to insert into the OwnExt.
		//	This space is now likely gone forever.  Log an event, but allow to continue.
		err = JET_errSuccess;
		goto HandleError;
		}
	else if ( pgnoOELast - cpgOESize + 1 > pgnoFirst
		|| pgnoOELast < pgnoLast )
		{
		FireWall();
		Call( ErrERRCheck( JET_errSPOwnExtCorrupted ) );
		}
	
	BTUp( pfucbOE );

	//	if available extent empty, add extent to be freed.	
	//	Otherwise, coalesce with left extents by deleting left extents 
	//	and augmenting size. 
	//	Coalesce right extent replacing size of right extent. 
	//
	Call( ErrSPIOpenAvailExt( ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

	if ( pgnoLast == pgnoOELast
		&& cpgSize == cpgOESize )
		{
		//	we're freeing an entire extent, so no point
		//	trying to coalesce
		goto InsertExtent;
		}

FindPage:
	KeyFromLong( rgbKey, pgnoFirst - 1 );
	Assert( bm.key.Cb() == sizeof(PGNO) );
	Assert( bm.key.suffix.Pv() == rgbKey );
	Assert( bm.data.Pv() == NULL );
	Assert( bm.data.Cb() == 0 );
	Assert( dib.pos == posDown );
	Assert( dib.pbm == &bm );
	dib.dirflag = fDIRFavourNext;

	err = ErrBTDown( pfucbAE, &dib, latchReadTouch );
	if ( JET_errRecordNotFound != err )
		{
		BOOL	fOnNextExtent	= fFalse;

		Assert( err != JET_errNoCurrentRecord );
		
#ifdef DEBUG
		if ( err < 0 )
			{
			DBGprintf( "ErrSPFreeExt could not go down into nonempty available extent tree.\n" );
			}
#endif
		Call( err );

		//	found an available extent node 
		//
		cpgAESize = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAESize >= 0 );
		if ( 0 == cpgAESize )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents and retry.
			Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node
			BTUp( pfucbAE );
			goto FindPage;
			}
		
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

		//	assert no page is common between available extent node
		//	and freed extent
		//
		Assert( pgnoFirst > pgnoAELast ||
				pgnoLast < pgnoAELast - cpgAESize + 1 );
				
		if ( wrnNDFoundGreater == err )
			{
			Assert( pgnoAELast > pgnoFirst - 1 );

			//	already on the next node, no need to move there
			fOnNextExtent = fTrue;
			}
		else if ( wrnNDFoundLess == err )
			{
			//	available extent nodes last page < pgnoFirst - 1
			//	no possible coalescing on the left
			//	(this is the last node in available extent tree)
			//
			Assert( pgnoAELast < pgnoFirst - 1 );
			}
		else
			{
			Assert( pgnoAELast == pgnoFirst - 1 );
			CallS( err );
			}

		if ( JET_errSuccess == err
			&& pgnoFirst > pgnoOELast - cpgOESize + 1 )
			{
			//	found available extent node whose last page == pgnoFirst - 1
			//	can coalesce freed extent with this node after re-keying
			//
			//	the second condition is to ensure that we do not coalesce
			//		two available extent nodes that belong to different owned extent nodes
			//
			Assert( pgnoAELast - cpgAESize + 1 >= pgnoOELast - cpgOESize + 1 );
			Assert( cpgAESize == 
						*(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv() );
			Assert( pgnoAELast == pgnoFirst - 1 );
						
			cpgSize += cpgAESize;
			pgnoFirst -= cpgAESize;
			Assert( pgnoLast == pgnoFirst + cpgSize - 1 );
			Call( ErrBTFlagDelete( pfucbAE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node

			//	successfully coalesced on the left, now
			//	attempt coalescing on the right
			//	if we haven't formed a full extent
			Assert( !fOnNextExtent );
			if ( pgnoLast == pgnoOELast
				&& cpgSize == cpgOESize )
				{
				goto InsertExtent;
				}

			Pcsr( pfucbAE )->Downgrade( latchReadTouch );

			//	verify we're still within the boundaries of OwnExt
			Assert( pgnoOELast - cpgOESize + 1 <= pgnoFirst );
			Assert( pgnoOELast >= pgnoLast );
			}

		//	now see if we can coalesce with next node, first moving there if necessary
		//
		if ( !fOnNextExtent )
			{
			err = ErrBTNext( pfucbAE, fDIRNull );
			if ( err < 0 )
				{
				if ( JET_errNoCurrentRecord == err )
					{
					err = JET_errSuccess;
					}
				else
					{
					Call( err );
					}
				}
			else
				{
				//	successfully moved to next extent,
				//	so we can try to coalesce
				fOnNextExtent = fTrue;
				}
			}

		if ( fOnNextExtent )
			{
			cpgAESize = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
			Assert( cpgAESize != 0 );

			LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
			
			//	verify no page is common between available extent node
			//	and freed extent
			//
			if ( pgnoLast >= pgnoAELast - cpgAESize + 1 )
				{
				AssertSz( fFalse, "AvailExt corrupted." );
				Call( ErrERRCheck( JET_errSPAvailExtCorrupted ) );
				}
				
			if ( pgnoLast == pgnoAELast - cpgAESize && 
				 pgnoAELast <= pgnoOELast )
				{
				//	freed extent falls exactly in front of available extent node 
				//			-- coalesce freed extent with current node
				//
				//	the second condition ensures that we do not coalesce
				//		two available extent nodes that are from different
				//		owned extent nodes
				//
				DATA	data;

				cpgSize += cpgAESize;

				LittleEndian<CPG> le_cpgSize;
				le_cpgSize = cpgSize;
				data.SetPv( &le_cpgSize );
				data.SetCb( sizeof(CPG) );
				Call( ErrBTReplace( pfucbAE, data, fDIRNoVersion ) );
				Assert( Pcsr( pfucbAE )->FLatched() );

				pgnoLast = pgnoAELast;
				fCoalesced = fTrue;

				if ( cpgSize >= cpageSEDefault
					&& pgnoNull != pfcb->PgnoNextAvailSE()
					&& pgnoFirst < pfcb->PgnoNextAvailSE() )
					{
					pfcb->SetPgnoNextAvailSE( pgnoFirst );
					}
				}
			}
		}
		
	//	add new node to available extent tree
	//
	if ( !fCoalesced )
		{
InsertExtent:
		BTUp( pfucbAE );
		AssertSPIPfucbOnRoot( pfucb );
		Call( ErrSPIAddFreedExtent(
					pfucbAE,
					PsphSPIRootPage( pfucb )->PgnoParent(),
					pgnoLast,
					cpgSize ) );
		}
	Assert( Pcsr( pfucbAE )->FLatched() );
	

	//	if extent freed coalesced with available extents 
	//	form a complete secondary extent, remove the secondary extent
	//	from the FDP and free it to the parent FDP.	
	//	Since FDP is first page of primary extent, 
	//	we do not have to guard against freeing
	//	primary extents.  
	//	UNDONE: If parent FDP is NULL, FDP is device level and
	//			complete, free secondary extents to device.
	//
	Assert( pgnoLast != pgnoOELast || cpgSize <= cpgOESize );
	if ( pgnoLast == pgnoOELast && cpgSize == cpgOESize )
		{
		Assert( cpgSize > 0 );
		
#ifdef DEBUG		
		Assert( Pcsr( pfucbAE )->FLatched() );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
		cpgAESize = *(UnalignedLittleEndian< CPG > *) pfucbAE->kdfCurr.data.Pv();
		Assert( pgnoAELast == pgnoLast );
		Assert( cpgAESize == cpgSize );
#endif		
		//	owned extent node is same as available extent node
		Call( ErrSPIFreeSEToParent( pfucb, pfucbOE, pfucbAE, pgnoOELast, cpgOESize ) );
		}


HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	Assert( pfucb->pcsrRoot != pcsrNil );
	if ( fRootLatched )
		{
		Assert( Pcsr( pfucb ) == pfucb->pcsrRoot );
		Assert( pfucb->pcsrRoot->FLatched() );
		pfucb->pcsrRoot->ReleasePage();
		pfucb->pcsrRoot = pcsrNil;
		}
		
#ifdef TRACE
	DBGprintf( "free space %lu at %lu to FDP %d.%lu\n", cpgSize, pgnoFirst, pfucb->ifmp, PgnoFDP( pfucb ) );
#endif

#ifdef DEBUG
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fDBGTraceBR )
		{
		INT cpg = 0;

		Assert( err >= 0 );
		for ( ; cpg < cpgSize; cpg++ )
			{
			char sz[256];
			sprintf( sz, "FREE (%d:%ld) %d:%ld",
					pfucb->ifmp, PgnoFDP( pfucb ),
					pfucb->ifmp, pgnoFirst + cpg );
			CallS( PinstFromIfmp( pfucb->ifmp )->m_plog->ErrLGTrace( ppib, sz ) );
			}
		}
#endif

	Assert( err != JET_errKeyDuplicate );

	return err;
	}


const ULONG cOEListEntriesInit	= 32;
const ULONG cOEListEntriesMax	= 127;

class OWNEXT_LIST
	{
	public:
		OWNEXT_LIST( OWNEXT_LIST **ppOEListHead );
		~OWNEXT_LIST();

	public:
		EXTENTINFO		*RgExtentInfo()			{ return m_extentinfo; }
		ULONG			CEntries() const;
		OWNEXT_LIST		*POEListNext() const	{ return m_pOEListNext; }
		VOID			AddExtentInfoEntry( const PGNO pgnoLast, const CPG cpgSize );

	private:
		EXTENTINFO		m_extentinfo[cOEListEntriesMax];
		ULONG			m_centries;
		OWNEXT_LIST		*m_pOEListNext;
	};

INLINE OWNEXT_LIST::OWNEXT_LIST( OWNEXT_LIST **ppOEListHead )
	{
	m_centries = 0;
	m_pOEListNext = *ppOEListHead;
	*ppOEListHead = this;
	}

INLINE ULONG OWNEXT_LIST::CEntries() const
	{
	Assert( m_centries <= cOEListEntriesMax );
	return m_centries;
	}

INLINE VOID OWNEXT_LIST::AddExtentInfoEntry(
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	Assert( m_centries < cOEListEntriesMax );
	m_extentinfo[m_centries].pgnoLastInExtent = pgnoLast;
	m_extentinfo[m_centries].cpgExtent = cpgSize;
	m_centries++;
	}

INLINE ERR ErrSPIFreeOwnedExtentsInList(
	FUCB		*pfucbParent,
	EXTENTINFO	*rgextinfo,
	const ULONG	cExtents )
	{
	ERR			err;
	INT			i;

	for ( i = 0; i < cExtents; i++ )
		{
		const CPG	cpgSize = rgextinfo[i].cpgExtent;
		const PGNO	pgnoFirst = rgextinfo[i].pgnoLastInExtent - cpgSize + 1;

		Assert( !FFUCBSpace( pfucbParent ) );
		CallR( ErrSPFreeExt( pfucbParent, pgnoFirst, cpgSize ) );
		}

	return JET_errSuccess;
	}


LOCAL ERR ErrSPIFreeAllOwnedExtents( FUCB *pfucbParent, FCB *pfcb )
	{
	ERR			err;
	FUCB  		*pfucbOE;
	DIB			dib;
	PGNO		pgnoLast;
	CPG			cpgSize;

	Assert( pfcb != pfcbNil );

	//	open owned extent tree of freed FDP
	//	free each extent in owned extent to parent FDP.
	//
	CallR( ErrSPIOpenOwnExt( pfucbParent->ppib, pfcb, &pfucbOE ) );
	
	dib.pos = posFirst;
	dib.dirflag = fDIRNull;
	if ( ( err = ErrBTDown( pfucbOE, &dib, latchReadTouch ) ) < 0 )
		{
		BTClose( pfucbOE );
		return err;
		}
	Assert( wrnNDFoundLess != err );
	Assert( wrnNDFoundGreater != err );

	EXTENTINFO	extinfo[cOEListEntriesInit];
	OWNEXT_LIST	*pOEList		= NULL;
	OWNEXT_LIST	*pOEListCurr	= NULL;
	ULONG		cOEListEntries	= 0;

	//	Collect all Own extent and free them all at once.
	//	Note that the pages kept tracked by own extent tree contains own
	//	extend tree itself. We can not free it while scanning own extent
	//	tree since we could free the pages used by own extend let other
	//	thread to use the pages.

	//	UNDONE: Because we free it all at once, if we crash, we may loose space.
	//	UNDONE: we need logical logging to log the remove all extent is going on
	//	UNDONE: and remember its state so that during recovery it will be able
	//	UNDONE: to redo the clean up.

	do
		{
		cpgSize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();
		Assert( cpgSize != 0 );
		LongFromKey( &pgnoLast, pfucbOE->kdfCurr.key );

		// Can't coalesce this OwnExt with previous OwnExt because
		// we may cross OwnExt boundaries in the parent.

		if ( cOEListEntries < cOEListEntriesInit )
			{
			// This entry can fit in the initial EXTENTINFO structure
			// (the one that was allocated on the stack).
			extinfo[cOEListEntries].pgnoLastInExtent = pgnoLast;
			extinfo[cOEListEntries].cpgExtent = cpgSize;
			}
		else 
			{
			Assert( ( NULL == pOEListCurr && NULL == pOEList )
				|| ( NULL != pOEListCurr && NULL != pOEList ) );
			if ( NULL == pOEListCurr || pOEListCurr->CEntries() == cOEListEntriesMax )
				{
				pOEListCurr = (OWNEXT_LIST *)PvOSMemoryHeapAlloc( sizeof( OWNEXT_LIST ) );
				if ( NULL == pOEListCurr )
					{
					Assert( pfucbNil != pfucbOE );
					BTClose( pfucbOE );
					err = ErrERRCheck( JET_errOutOfMemory );
					goto HandleError;
					}
				new( pOEListCurr ) OWNEXT_LIST( &pOEList );

				Assert( pOEList == pOEListCurr );
				}

			pOEListCurr->AddExtentInfoEntry( pgnoLast, cpgSize );
			}

		cOEListEntries++;

		err = ErrBTNext( pfucbOE, fDIRNull );
		}
	while ( err >= 0 );

	//	Close the pfucbOE right away to release any latch on the pages that
	//	are going to be freed and used by others.

	Assert( pfucbNil != pfucbOE );
	BTClose( pfucbOE );

	//	Check the error code.

	if ( err != JET_errNoCurrentRecord )
		{
		Assert( err < 0 );
		goto HandleError;
		}

	Call( ErrSPIFreeOwnedExtentsInList(
			pfucbParent,
			extinfo,
			min( cOEListEntries, cOEListEntriesInit ) ) );

	for ( pOEListCurr = pOEList;
		pOEListCurr != NULL;
		pOEListCurr = pOEListCurr->POEListNext() )
		{
		Assert( cOEListEntries > cOEListEntriesInit );
		Call( ErrSPIFreeOwnedExtentsInList(
				pfucbParent,
				pOEListCurr->RgExtentInfo(),
				pOEListCurr->CEntries() ) );
		}
		
HandleError:
	pOEListCurr = pOEList;
	while ( pOEListCurr != NULL )
		{
		OWNEXT_LIST	*pOEListKill = pOEListCurr;

#ifdef DEBUG
		//	this variable is no longer used, so we can
		//	re-use it for DEBUG-only purposes
		Assert( cOEListEntries > cOEListEntriesInit );
		Assert( cOEListEntries > pOEListCurr->CEntries() );
		cOEListEntries -= pOEListCurr->CEntries();
#endif		
		
		pOEListCurr = pOEListCurr->POEListNext();

		OSMemoryHeapFree( pOEListKill );
		}
	Assert( cOEListEntries <= cOEListEntriesInit );
	
	return err;
	}
	
//	ErrSPFreeFDP
//	========================================================================
//	ERR ErrSPFreeFDP( FUCB *pfucbParent, PGNO pgnoFDPFreed )
//
//	Frees all owned extents of an FDP to its parent FDP.  The FDP page is freed
//	with the owned extents to the parent FDP.
//
//	PARAMETERS	pfucbParent		cursor on tree space is freed to
//				pgnoFDPFreed	pgnoFDP of FDP to be freed
//
ERR ErrSPFreeFDP(
	PIB			*ppib,
	FCB			*pfcbFDPToFree,
	const PGNO	pgnoFDPParent )
	{
	ERR			err;
	const IFMP	ifmp			= pfcbFDPToFree->Ifmp();
	const PGNO	pgnoFDPFree		= pfcbFDPToFree->PgnoFDP();
	FUCB		*pfucbParent	= pfucbNil;
	FUCB		*pfucb			= pfucbNil;

	PERFIncCounterTable( cSPDelete, PinstFromIfmp( pfcbFDPToFree->Ifmp() ), pfcbFDPToFree->Tableclass() );

	//	begin transaction if one is already not begun
	//
	BOOL	fBeginTrx	= fFalse;
	if ( ppib->level == 0 )
		{
		CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fBeginTrx	= fTrue;
		}

	Assert( pgnoNull != pgnoFDPParent );
	Assert( pgnoNull != pgnoFDPFree );
	
	Assert( dbidTemp != rgfmp[ ifmp ].Dbid() || pgnoSystemRoot == pgnoFDPParent );
	
	Call( ErrBTOpen( ppib, pgnoFDPParent, ifmp, &pfucbParent ) );
	Assert( pfucbNil != pfucbParent );
	Assert( pfucbParent->u.pfcb->FInitialized() );
	
	//	check for valid parameters.
	//
#ifdef SPACECHECK
	CallS( ErrSPIValidFDP( ppib, pfucbParent->ifmp, pgnoFDPFree ) );
#endif

#ifdef TRACE
	DBGprintf( "free space FDP at %d.%lu\n", pfucbParent->ifmp, pgnoFDPFree );
#endif
	
#ifdef DEBUG
	if ( PinstFromPpib( ppib )->m_plog->m_fDBGTraceBR )
		{
		char sz[256];

		sprintf( sz, "FREE FDP (%d:%ld)", ifmp, pgnoFDPFree );
		CallS( PinstFromPpib( ppib )->m_plog->ErrLGTrace( ppib, sz ) );
		}
#endif

	//	get temporary FUCB
	//
	Call( ErrBTOpen( ppib, pfcbFDPToFree, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( pfucb->u.pfcb->FInitialized() );
	Assert( pfucb->u.pfcb->FDeleteCommitted() );
	FUCBSetIndex( pfucb );

	Call( ErrBTIGotoRoot( pfucb, latchRIW ) );
	pfucb->pcsrRoot = Pcsr( pfucb );

#ifdef SPACECHECK
	CallS( ErrSPIWasAlloc( pfucb, pgnoFDPFree, 1 ) );
#endif

	//	get parent FDP pgno
	//
	Assert( pgnoFDPParent == PgnoSPIParentFDP( pfucb ) );
	Assert( pgnoFDPParent == PgnoFDP( pfucbParent ) );

	if ( !pfucb->u.pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

	//	if single extent format, then free extent in external header
	//
	if ( pfucb->u.pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER 	*psph;
		
		AssertSPIPfucbOnRoot( pfucb );

		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

		ULONG cpgPrimary = psph->CpgPrimary();
		Assert( psph->CpgPrimary() != 0 );

		//	Close the cursor to make sure it latches no buffer whose page
		//	is going to be freed.

		pfucb->pcsrRoot = pcsrNil;
		BTClose( pfucb );
		pfucb = pfucbNil;

		Assert( !FFUCBSpace( pfucbParent ) );
		err = ErrSPFreeExt( pfucbParent, pgnoFDPFree, cpgPrimary );
		}
	else
		{
		//	Close the cursor to make sure it latches no buffer whose page
		//	is going to be freed.

		FCB *pfcb = pfucb->u.pfcb;
		pfucb->pcsrRoot = pcsrNil;
		BTClose( pfucb );
		pfucb = pfucbNil;

		Call( ErrSPIFreeAllOwnedExtents( pfucbParent, pfcb ) );
		Assert( !Pcsr( pfucbParent )->FLatched() );
		}

HandleError:
	if ( pfucbNil != pfucb )
		{
		pfucb->pcsrRoot = pcsrNil;
		BTClose( pfucb );
		}
		
	if ( pfucbNil != pfucbParent )
		{
		BTClose( pfucbParent );
		}

	if ( dbidTemp == rgfmp[ ifmp ].Dbid() )
#ifndef RFS2
		{
		AssertSz( JET_errSuccess == err, "Space potentially lost in temporary database." );
		}
	else
		{
		AssertSz( JET_errSuccess == err, "Space potentially lost permanently in user database." );
		}
#else // RFS2
		{
		AssertSz( JET_errSuccess == err || JET_errOutOfCursors == err || JET_errOutOfMemory == err || JET_errDiskIO == err, "Space potentially lost in temporary database." );
		}
	else
		{
		AssertSz( JET_errSuccess == err || JET_errOutOfCursors == err || JET_errOutOfMemory == err || JET_errDiskIO == err, "Space potentially lost permanently in user database." );
		}
#endif // RFS2

	if ( fBeginTrx )
		{
		if ( err >= 0 )
			{
			err = ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush );
			}
		if ( err < 0 )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		}

	return err;
	}

INLINE ERR ErrSPIAddExtent(
	FUCB		*pfucb,
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	ERR			err;
	KEY			key;
	DATA 		data;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( FFUCBSpace( pfucb ) );
	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( cpgSize > 0 );

	// begin log macro
	//
	KeyFromLong( rgbKey, pgnoLast );
	key.prefix.Nullify();
	key.suffix.SetCb( sizeof(PGNO) );
	key.suffix.SetPv( rgbKey );

	LittleEndian<CPG> le_cpgSize = cpgSize;
	data.SetPv( (VOID *)&le_cpgSize );
	data.SetCb( sizeof(CPG) );

	BTUp( pfucb );
	Call( ErrBTInsert( pfucb, key, data, fDIRNoVersion ) );
	Assert( Pcsr( pfucb )->FLatched() );

HandleError:	
	Assert( errSPOutOfOwnExtCacheSpace != err );
	Assert( errSPOutOfAvailExtCacheSpace != err );
	return err;
	}

LOCAL ERR ErrSPIAddToAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoAELast,
	const CPG	cpgAESize,
	SPCACHE		***pppspcache )
	{
	ERR			err;
	FCB			* const pfcb	= pfucbAE->u.pfcb;

	Assert( FFUCBAvailExt( pfucbAE ) );
	err = ErrSPIAddExtent( pfucbAE, pgnoAELast, cpgAESize );
	if ( err < 0 )
		{
		if ( JET_errKeyDuplicate == err )
			{
			err = ErrERRCheck( JET_errSPAvailExtCorrupted );
			}
		}

	else if ( cpgAESize >= cpageSEDefault
		&& pgnoNull != pfcb->PgnoNextAvailSE() )
		{
		const PGNO	pgnoFirst	= pgnoAELast - cpgAESize + 1;
		if ( pgnoFirst < pfcb->PgnoNextAvailSE() )
			{
			pfcb->SetPgnoNextAvailSE( pgnoFirst );
			}
		}

	return err;
	}
	
LOCAL ERR ErrSPIAddToOwnExt(
	FUCB		*pfucb,
	const PGNO	pgnoOELast,
	CPG			cpgOESize,
	CPG			*pcpgCoalesced = NULL )
	{
	ERR			err;
	FUCB		*pfucbOE;
	

	//	open cursor on owned extent
	//
	CallR( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );
	Assert( FFUCBOwnExt( pfucbOE ) );
	
#ifdef COALESCE_OE
	const BOOL	fPermitCoalescing	= ( NULL != pcpgCoalesced );

	//	coalescing OWNEXT is done only for databases without recovery
	//	
	if ( !rgfmp[pfucb->ifmp].FLogOn() && fPermitCoalescing )
		{
		CPG			cpgOECoalesced	= 0;
		BOOKMARK	bmSeek;
		DIB			dib;
		BYTE		rgbKey[sizeof(PGNO)];
		
		//	set up variables for coalescing
		//
		KeyFromLong( rgbKey, pgnoOELast - cpgOESize );
		bmSeek.key.prefix.Nullify();
		bmSeek.key.suffix.SetCb( sizeof(PGNO) );
		bmSeek.key.suffix.SetPv( rgbKey );
		bmSeek.data.Nullify();

		//	look for extent that ends at pgnoOELast - cpgOESize, 
		//	the only extent we can coalesce with
		//
		dib.pos		= posDown;
		dib.pbm		= &bmSeek;
		dib.dirflag	= fDIRExact;
		err = ErrBTDown( pfucbOE, &dib, latchReadTouch );
		if ( JET_errRecordNotFound == err )
			{
			err = JET_errSuccess;
			}
		else if ( JET_errSuccess == err )
			{
			//  we found a match, so get the old extent's size, delete the old extent,
			//  and add it's size to the new extent to insert
			//
			Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
			
#ifdef DEBUG
			PGNO	pgnoOELastPrev;
			LongFromKey( &pgnoOELastPrev, pfucbOE->kdfCurr.key );
			Assert( pgnoOELastPrev == pgnoOELast - cpgOESize );
			Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(CPG) );
#endif
			
			cpgOECoalesced = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();
			Assert( cpgOECoalesced > 0 );
			Call( ErrBTFlagDelete( pfucbOE, fDIRNoVersion ) );		// UNDONE: Synchronously remove the node

			Assert( NULL != pcpgCoalesced );
			*pcpgCoalesced = cpgOECoalesced;

			cpgOESize += cpgOECoalesced;
			}
		else
			{
			Call( err );
			}

		BTUp( pfucbOE );
		}
#endif	//	COALESCE_OE

	Call( ErrSPIAddExtent( pfucbOE, pgnoOELast, cpgOESize ) );

HandleError:
	Assert( errSPOutOfOwnExtCacheSpace != err );
	Assert( errSPOutOfAvailExtCacheSpace != err );
	pfucbOE->pcsrRoot = pcsrNil;
	BTClose( pfucbOE );
	return err;
	}

#ifdef COALESCE_OE
LOCAL ERR ErrSPICoalesceAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoLast,
	const CPG	cpgSize,
	CPG			*pcpgCoalesce )
	{
	ERR			err;
	BOOKMARK	bmSeek;
	DIB			dib;
	BYTE		rgbKey[sizeof(PGNO)];

	//	coalescing AVAILEXT is done only for databases without recovery
	//	
	Assert( !rgfmp[pfucbAE->ifmp].FLogOn() );

	*pcpgCoalesce = 0;
		
	//	Set up seek key to Avail Size
	//
	KeyFromLong( rgbKey, pgnoLast - cpgSize );
		
	//	set up variables for coalescing
	//
	bmSeek.key.prefix.Nullify();
	bmSeek.key.suffix.SetCb( sizeof(PGNO) );
	bmSeek.key.suffix.SetPv( rgbKey );
	bmSeek.data.Nullify();

	//	look for extent that ends at pgnoLast - cpgSize, 
	//	the only extent we can coalesce with
	//
	dib.pos		= posDown;
	dib.pbm		= &bmSeek;
	dib.dirflag	= fDIRNull;
	err = ErrBTDown( pfucbAE, &dib, latchReadTouch );
	if ( JET_errRecordNotFound == err )
		{
		//	no record in available extent
		//	mask error
		err = JET_errSuccess;
		}
	else if ( JET_errSuccess == err )
		{
		//  we found a match, so get the old extent's size, delete the old extent,
		//  and add it's size to the new extent to insert
		//
#ifdef DEBUG
		PGNO	pgnoAELast;
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
		Assert( pgnoAELast == pgnoLast - cpgSize );
#endif
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		*pcpgCoalesce = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		err = ErrBTFlagDelete( pfucbAE, fDIRNoVersion );		// UNDONE: Synchronously remove the node
		}

	BTUp( pfucbAE );
		
	return err;
	}
#endif	//	COALESCE_OE


//	if Secondary extent, add given extent owned extent and available extent
//	if Freed extent, add extent to available extent
//	splits caused during insertion into owned extent and available extent will
//		use space from FUCB space cache, which is initialized here
//	pufcbAE is cursor on available extent tree, should be positioned on 
//		added available extent node
//
LOCAL ERR ErrSPIAddSecondaryExtent(
	FUCB		*pfucb,
	FUCB		*pfucbAE,
	const PGNO	pgnoLast, 
	CPG			cpgSize,
	SPCACHE		***pppspcache )
	{
	ERR			err;
	CPG			cpgOECoalesced	= 0;

	//	if this is a secondary extent, insert new extent into OWNEXT and
	//	AVAILEXT, coalescing with an existing extent to the left, if possible.
	//
	Call( ErrSPIAddToOwnExt(
				pfucb,
				pgnoLast,
				cpgSize,
				&cpgOECoalesced ) );


#ifdef COALESCE_OE
	//	We shouldn't even try coalescing AvailExt if no coalescing of OwnExt was done
	//	(since we cannot coalesce AvailExt across OwnExt boundaries)
	if ( cpgOECoalesced > 0 )
		{
		CPG	cpgAECoalesced;
		
		Call( ErrSPICoalesceAvailExt( pfucbAE, pgnoLast, cpgSize, &cpgAECoalesced ) );
		
		// Ensure AvailExt wasn't coalesced across OwnExt boundaries.
		Assert( cpgAECoalesced <= cpgOECoalesced );
		cpgSize += cpgAECoalesced;
		}
#else
	Assert( 0 == cpgOECoalesced );
#endif	//	COALESCE_OE		


	Call( ErrSPIAddToAvailExt( pfucbAE, pgnoLast, cpgSize, pppspcache ) );
	Assert( Pcsr( pfucbAE )->FLatched() );
	
HandleError:
	return err;
	}


INLINE ERR ErrSPICheckSmallFDP( FUCB *pfucb, BOOL *pfSmallFDP )
	{
	ERR		err;
	FUCB	*pfucbOE	= pfucbNil;
	CPG		cpgOwned;
	DIB		dib;

	CallR( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );
	Assert( pfucbNil != pfucbOE );
	
	//  determine if this FDP owns a lot of space [> cpgSmallFDP]
	//	
	dib.pos = posFirst;
	dib.dirflag = fDIRNull;
	err = ErrBTDown( pfucbOE, &dib, latchReadTouch );
	Assert( err != JET_errNoCurrentRecord );
	Assert( err != JET_errRecordNotFound );
	Call( err );

	Assert( dib.dirflag == fDIRNull );
	cpgOwned = 0;

	// Count pages until we reach the end or hit short-circuit value (cpgSmallFDP).
	do
		{
		const CPG	cpgOwnedCurr = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

		Assert( pfucbOE->kdfCurr.data.Cb() == sizeof( PGNO ) );
		Assert( cpgOwnedCurr != 0 );
			
		cpgOwned += cpgOwnedCurr;
		err = ErrBTNext( pfucbOE, fDIRNull );
		}
	while ( err >= 0 && cpgOwned <= cpgSmallFDP );

	if ( JET_errNoCurrentRecord == err )
		err = JET_errSuccess;

	Call( err );

	*pfSmallFDP = ( cpgOwned <= cpgSmallFDP );

HandleError:
	Assert( pfucbNil != pfucbOE );
	BTClose( pfucbOE );
	return err;
	}


VOID SPReportMaxDbSizeExceeded( const CHAR *szDbName, const CPG cpg )
	{
	//	Log event to tell user that it reaches the database size limit.

	CHAR		szCurrentSizeMb[16];
	const CHAR	* rgszT[2]			= { szDbName, szCurrentSizeMb };

	sprintf( szCurrentSizeMb, "%d", (ULONG)(( (QWORD)cpg * (QWORD)( g_cbPage >> 10 /* Kb */ ) ) >> 10 /* Mb */) );
				
	UtilReportEvent(
			eventWarning,
			SPACE_MANAGER_CATEGORY,
			SPACE_MAX_DB_SIZE_REACHED_ID,
			2,
			rgszT );
	}


LOCAL ERR ErrSPIExtendDB(
	FUCB		*pfucb,
	const CPG	cpgSEMin,
	CPG			*pcpgSEReq,
	PGNO		*ppgnoSELast )
	{
	ERR			err;
	CPG			cpgSEReq	= *pcpgSEReq;
	FUCB		*pfucbOE	= pfucbNil;
	PGNO		pgnoSELast	= pgnoNull;
	BOOL		fAllocAE	= fFalse;
	DIB			dib;

	Assert( pgnoSystemRoot == pfucb->u.pfcb->PgnoFDP() );

	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );

	dib.pos = posLast;
	dib.dirflag = fDIRNull;

	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoSELast, pfucbOE->kdfCurr.key );
	BTUp( pfucbOE );

	//	allocate more space from device
	//
	Assert( pgnoSysMax >= pgnoSELast );
	Assert( cpgSEMin > 0 );
	if ( pgnoSysMax - pgnoSELast < (PGNO)cpgSEMin )
		{
		err = ErrERRCheck( JET_errOutOfDatabaseSpace );
		goto HandleError;
		}

	//	NOTE: casting below means that requests of >= 8TB chunks will cause problems
	//
	Assert( cpgSEReq > 0 );
	Assert( pgnoSysMax > pgnoSELast );
	cpgSEReq = (CPG)min( (PGNO)cpgSEReq, (PGNO)( pgnoSysMax - pgnoSELast ) );
	Assert( cpgSEReq > 0 );
	Assert( cpgSEMin <= cpgSEReq );

	err = ErrIONewSize( pfucb->ifmp, pgnoSELast + cpgSEReq );
	if ( err < 0 )
		{
		Call( ErrIONewSize( pfucb->ifmp, pgnoSELast + cpgSEMin ) );
		cpgSEReq = cpgSEMin;
		}

	//	calculate last page of device level secondary extent
	//
	pgnoSELast += cpgSEReq;

	Assert( cpgSEReq >= cpgSEMin );
	*pcpgSEReq = cpgSEReq;
	*ppgnoSELast = pgnoSELast;

HandleError:
	if ( pfucbNil != pfucbOE )
		BTClose( pfucbOE );

	return err;
	}

LOCAL ERR ErrSPIReservePagesForSplit( FUCB *pfucb, FUCB *pfucbParent )
	{
	ERR				err;
	ERR				wrn		= JET_errSuccess;
	SPLIT_BUFFER	*pspbuf;
	CPG				cpgMin;
	CPG				cpgReq;
#ifdef DEBUG
	ULONG			crepeat	= 0;
#endif	

	Assert( FFUCBSpace( pfucb ) );
	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	forever
		{
#ifdef DEBUG
		Assert( crepeat < 3 );		//	shouldn't have to make more than 2 iterations in common case
									//	and 3 when we crash in the middle of operation. Consequential
									//	recovery will not reclaim the space.
		crepeat++;
#endif
		Call( ErrBTIGotoRoot( pfucb, latchRIW ) );

		AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );

		Call( ErrSPIGetSPBuf( pfucb, &pspbuf ) );

		if ( pfucb->csr.Cpage().FLeafPage() )
			{
			//	root page is also leaf page,
			//	see if we're close to splitting
			if ( pfucb->csr.Cpage().CbFree() < 100 )
				{
				cpgMin = cpgMaxRootPageSplit;
				cpgReq = cpgReqRootPageSplit;
				}
			else
				{
				cpgMin = 0;
				cpgReq = 0;
				}
			}
		else if ( pfucb->csr.Cpage().FParentOfLeaf() )
			{
			cpgMin = cpgMaxParentOfLeafRootSplit;
			cpgReq = cpgReqParentOfLeafRootSplit;
			}
		else
			{
			cpgMin = cpgMaxSpaceTreeSplit;
			cpgReq = cpgReqSpaceTreeSplit;
			}

		if ( pspbuf->CpgBuffer1() + pspbuf->CpgBuffer2() >= cpgMin )
			{
			break;
			}
		else
			{
			PGNO			pgnoLast;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			UtilMemCpy( &spbuf, pspbuf, sizeof(SPLIT_BUFFER) );

			//	one of the buffers must have dropped to zero -- that's the
			//	one we'll be replacing
			Assert( 0 == spbuf.CpgBuffer1() || 0 == spbuf.CpgBuffer2() );
			
			if ( pfucbNil != pfucbParent )
				{
				PGNO	pgnoFirst	= pgnoNull;
				err = ErrSPIGetExt(
							pfucbParent,
							&cpgReq,
							cpgMin,
							&pgnoFirst );
				AssertSPIPfucbOnRoot( pfucbParent );
				AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
				Call( err );

				pgnoLast = pgnoFirst + cpgReq - 1;
				}
			else
				{
				Assert( pgnoSystemRoot == pfucb->u.pfcb->PgnoFDP() );

				//	don't want to grow database by only 1 or 2 pages at a time, so
				//	choose largest value to satisfy max. theoretical split requirements
				cpgMin = cpgMaxSpaceTreeSplit;
				cpgReq = cpgReqSpaceTreeSplit;

				BTUp( pfucb );
				Call( ErrSPIExtendDB( pfucb, cpgMin, &cpgReq, &pgnoLast ) );
				
				Call( ErrBTIGotoRoot( pfucb, latchRIW ) );
				AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
				}
			Assert( cpgReq >= cpgMin );

			if ( NULL == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) )
				{
				pfucb->csr.UpgradeFromRIWLatch();

				//	verify no one snuck in underneath us
				Assert( 0 == memcmp( &spbuf,
										PspbufSPISpaceTreeRootPage( pfucb, Pcsr( pfucb ) ),
										sizeof(SPLIT_BUFFER) ) );
				Assert( sizeof( SPLIT_BUFFER ) == pfucb->kdfCurr.data.Cb() );	//	WARNING: relies on NDGetExternalHeader() in the previous assert

				spbuf.AddPages( pgnoLast, cpgReq );
				data.SetPv( &spbuf );
				data.SetCb( sizeof(spbuf) );
				Call( ErrNDSetExternalHeader( pfucb, &data, fDIRNull ) );
				}
			else
				{
				//	verify no one snuck in underneath us
				Assert( pspbuf == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) );
				Assert( 0 == memcmp( &spbuf, pspbuf, sizeof(SPLIT_BUFFER) ) );
				pspbuf->AddPages( pgnoLast, cpgReq );
				SPIReportAddedPagesToSplitBuffer( pfucb );
				}

			BTUp( pfucb );
			
			Call( ErrSPIAddToOwnExt( pfucb, pgnoLast, cpgReq ) );
			wrn = ErrERRCheck( wrnSPReservedPages );				//	indicate that we reserved pages

			if ( FFUCBAvailExt( pfucb ) )
				{
				//	if we reserved pages for AE, then the reserved pages didn't
				//	get used to satisfy any splits in the OE, so we can just exit
				break;
				}
			else
				{
				//	if we reserved pages for OE, we have to check again in case
				//	the insertion into OE for the reserved pages caused a split
				//	and consumed some of those pages.
				Assert( FFUCBOwnExt( pfucb ) );
				}
			}

		}	//	forever

	CallS( err );
	err = wrn;

HandleError:
	BTUp( pfucb );
	return err;
	}

ERR ErrSPReservePagesForSplit( FUCB *pfucb, FUCB *pfucbParent )
	{
	ERR		err;

	Assert( !Pcsr( pfucbParent )->FLatched() );
	Assert( pcsrNil != pfucbParent->pcsrRoot );
	CallR( ErrBTIGotoRoot( pfucbParent, latchRIW ) );

	err = ErrSPIReservePagesForSplit( pfucb, pfucbParent );

	BTUp( pfucbParent );

	return err;
	}



LOCAL ErrSPIReservePages( FUCB *pfucb, FUCB *pfucbParent, const SPEXT spext )
	{
	ERR		err;
	FCB		* const pfcb	= pfucb->u.pfcb;
	FUCB	* pfucbOE		= pfucbNil;
	FUCB	* pfucbAE		= pfucbNil;

	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfcb, &pfucbOE ) );
	Call( ErrSPIReservePagesForSplit( pfucbOE, pfucbParent ) );

	Call( ErrSPIOpenAvailExt( pfucb->ppib, pfcb, &pfucbAE ) );
	Call( ErrSPIReservePagesForSplit( pfucbAE, pfucbParent ) );

	if ( spextSecondary == spext && wrnSPReservedPages == err )
		{
		//	if reserving for a secondary extent, must check OE again
		//	in case allocation of reserved pages in AE used up some of
		//	the OE reserved pages when inserting the extent into OE;
		//	if reserving for a freed extent, this is unnecessary because
		//	we won't be updating OE
		Call( ErrSPIReservePagesForSplit( pfucbOE, pfucbParent ) );
		}
	
HandleError:
	if ( pfucbNil != pfucbOE )
		BTClose( pfucbOE );
	if ( pfucbNil != pfucbAE )
		BTClose( pfucbAE );

	return err;
	}



//	gets secondary extent from parentFDP, and adds to available extent
//	tree if pfucbAE is non-Nil.
//
//	INPUT:	pfucb		cursor on FDP root page with RIW latch
//			pfucbAE		cursor on available extent tree
//			cpgReq		requested number of pages
//			cpgMin		minimum number of pages required
//
//	
LOCAL ERR ErrSPIGetSE(
	FUCB 			*pfucb, 
	FUCB 			*pfucbAE,
	const CPG		cpgReq, 
	const CPG		cpgMin,
	const BOOL		fSplitting,
	SPCACHE			***pppspcache )
	{
	ERR				err;
	PIB 			*ppib					= pfucb->ppib;
	FUCB 			*pfucbParent			= pfucbNil;
	SPACE_HEADER	*psph;
	PGNO			pgnoParentFDP;
	CPG				cpgPrimary;
	CPG				cpgSEReq;
	CPG				cpgSEMin;
	PGNO			pgnoSELast;
	BOOL			fHoldExtendingDBLatch	= fFalse;
	FMP				*pfmp					= &rgfmp[ pfucb->ifmp ];

	//	check validity of input parameters
	//
	AssertSPIPfucbOnRoot( pfucb );
	Assert( pfucbNil != pfucbAE );
	Assert( !Pcsr( pfucbAE )->FLatched() );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );
	
	//	get parent FDP page number
	//	and primary extent size
	//
	psph = PsphSPIRootPage( pfucb );
	pgnoParentFDP = psph->PgnoParent();
	cpgPrimary = psph->CpgPrimary();

///	pfucb->u.pfcb->CritSpaceTrees().Enter();

	//	pages of allocated extent may be used to split Owned extents and
	//	AVAILEXT trees.  If this happens, then subsequent added
	//	extent will not have to split and will be able to satisfy
	//	requested allocation.
	//
	if ( pgnoNull != pgnoParentFDP )
		{
		PGNO	pgnoSEFirst	= pgnoNull;
		BOOL	fSmallFDP	= fFalse;

		cpgSEMin = cpgMin;
		
		Call( ErrSPICheckSmallFDP( pfucb, &fSmallFDP ) );

		//  if this FDP owns a lot of space, allocate a fraction of the primary
		//  extent (or more if requested), but at least a given minimum amount
		//
		if ( fSmallFDP )
			{
			//  if this FDP owns just a little space, 
			//	add a very small constant amount of space, 
			//	unless more is requested, 
			//	in order to optimize space allocation
			//  for small tables, indexes, etc.
			//
			cpgSEReq = max( cpgReq, cpgSmallGrow );
			}
		else
			{
			cpgSEReq = max( cpgReq, max( cpgPrimary/cSecFrac, cpageSEDefault ) );
			}

		//	open cursor on parent FDP to get space from
		//
		Call( ErrBTIOpenAndGotoRoot( pfucb->ppib, pgnoParentFDP, pfucb->ifmp, &pfucbParent ) );
		
		Call( ErrSPIReservePages( pfucb, pfucbParent, spextSecondary ) );

		//  allocate extent
		//
		err = ErrSPIGetExt(
					pfucbParent,
					&cpgSEReq,
					cpgSEMin,
					&pgnoSEFirst,
					fSplitting ? fSPSplitting : 0 );
		AssertSPIPfucbOnRoot( pfucbParent );
		AssertSPIPfucbOnRoot( pfucb );
		Call( err );

		Assert( cpgSEReq >= cpgSEMin );
		pgnoSELast = pgnoSEFirst + cpgSEReq - 1;
			
#ifdef CONVERT_VERBOSE
		CONVPrintF2( "\n  (p)PgnoFDP %d: Attempt to add SE of %d pages ending at page %d.",
				PgnoFDP( pfucb ), cpgSEReq, pgnoSELast );
#endif

		BTUp( pfucbAE );
		err = ErrSPIAddSecondaryExtent(
						pfucb,
						pfucbAE,
						pgnoSELast,
						cpgSEReq,
						pppspcache );
		Assert( errSPOutOfOwnExtCacheSpace != err );
		Assert( errSPOutOfAvailExtCacheSpace != err );
		Call( err );
		}
		
	else
		{
		//	allocate a secondary extent from the operating system
		//	by getting page number of last owned page, extending the
		//	file as possible and adding the sized secondary extent
		//	NOTE: only one user can do this at one time.
		//
		if ( dbidTemp == rgfmp[ pfucb->ifmp ].Dbid() )
			cpgSEMin = max( cpgMin, cpageSEDefault );
		else			
			cpgSEMin = max( cpgMin, PinstFromPpib( ppib )->m_cpgSESysMin );

		cpgSEReq = max( cpgReq, max( cpgPrimary/cSecFrac, cpgSEMin ) );
		
		if ( dbidTemp != rgfmp[ pfucb->ifmp ].Dbid() )
			{
			//	Round up to multiple of system parameter.

			cpgSEReq += PinstFromPpib( ppib )->m_cpgSESysMin - 1;
			cpgSEReq -= cpgSEReq % PinstFromPpib( ppib )->m_cpgSESysMin;
		
			//	Round up to multiple of reasonable constant.

			cpgSEReq += cpageDbExtensionDefault - 1;
			cpgSEReq -= cpgSEReq % cpageDbExtensionDefault;
			}

		//	get extendingDB latch in order to check/update fExtendingDB
		//
		pfmp->GetExtendingDBLatch();
		fHoldExtendingDBLatch = fTrue;

		if ( pfmp->CpgDatabaseSizeMax() )
			{
			//	Check if it reaches the max size
			const ULONG		cpgNow	= pfmp->PgnoLast();

			Assert( 0 == pfmp->PgnoSLVLast() || pfmp->FSLVAttached() );

			if ( cpgNow + cpgSEReq + (CPG)pfmp->PgnoSLVLast() > pfmp->CpgDatabaseSizeMax() )
				{
				SPReportMaxDbSizeExceeded( pfmp->SzDatabaseName(), cpgNow + (CPG)pfmp->PgnoSLVLast() );

				err = ErrERRCheck( JET_errOutOfDatabaseSpace );
				goto HandleError;
				}
			}

		Call( ErrSPIReservePages( pfucb, pfucbNil, spextSecondary ) );

		Call( ErrSPIExtendDB( pfucb, cpgSEMin, &cpgSEReq, &pgnoSELast ) );
		
		BTUp( pfucbAE );
		err = ErrSPIAddSecondaryExtent( pfucb, pfucbAE, pgnoSELast, cpgSEReq, pppspcache );
		Assert( errSPOutOfOwnExtCacheSpace != err );
		Assert( errSPOutOfAvailExtCacheSpace != err );
		Call( err );
		}
		
	AssertSPIPfucbOnRoot( pfucb );
	Assert( Pcsr( pfucbAE )->FLatched() );
	Assert( cpgSEReq >= cpgSEMin );

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		Assert( pgnoNull != pgnoParentFDP );
		if ( pcsrNil != pfucbParent->pcsrRoot )
			{
			pfucbParent->pcsrRoot->ReleasePage();
			pfucbParent->pcsrRoot = pcsrNil;
			}
			
		Assert( !Pcsr( pfucbParent )->FLatched() );
		BTClose( pfucbParent );
		}
		
	if ( fHoldExtendingDBLatch )
		{
		Assert( pgnoNull == pgnoParentFDP );
		pfmp->ReleaseExtendingDBLatch();
		}

///	pfucb->u.pfcb->CritSpaceTrees().Leave();

	return err;
	}


LOCAL ERR ErrSPIAddFreedExtent(
	FUCB		*pfucbAE,
	const PGNO	pgnoParentFDP,
	const PGNO	pgnoLast, 
	const CPG	cpgSize )
	{
	ERR			err;
	FUCB		*pfucbParent	= pfucbNil;

	Assert( !Pcsr( pfucbAE )->FLatched() );

///	pfucbAE->u.pfcb->CritSpaceTrees().Enter();

	//	if pgnoNull == pgnoParentFDP, then we're in the database's space tree
	if ( pgnoNull != pgnoParentFDP )
		{
		Call( ErrBTIOpenAndGotoRoot( pfucbAE->ppib, pgnoParentFDP, pfucbAE->ifmp, &pfucbParent ) );
		}
		
	Call( ErrSPIReservePages( pfucbAE, pfucbParent, spextFreed ) );

	Call( ErrSPIAddToAvailExt( pfucbAE, pgnoLast, cpgSize ) );
	Assert( Pcsr( pfucbAE )->FLatched() );

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		if ( pcsrNil != pfucbParent->pcsrRoot )
			{
			pfucbParent->pcsrRoot->ReleasePage();
			pfucbParent->pcsrRoot = pcsrNil;
			}
		BTClose( pfucbParent );
		}

///	pfucbAE->u.pfcb->CritSpaceTrees().Leave();

	return err;
	}


//	Check that the buffer passed to ErrSPGetInfo() is big enough to accommodate
//	the information requested.
//
INLINE ERR ErrSPCheckInfoBuf( const ULONG cbBufSize, const ULONG fSPExtents )
	{
	ULONG	cbUnchecked		= cbBufSize;

	if ( FSPOwnedExtent( fSPExtents ) )
		{
		if ( cbUnchecked < sizeof(CPG) )
			{
			return ErrERRCheck( JET_errBufferTooSmall );
			}
		cbUnchecked -= sizeof(CPG);

		//	if list needed, ensure enough space for list sentinel
		//
		if ( FSPExtentList( fSPExtents ) )
			{
			if ( cbUnchecked < sizeof(EXTENTINFO) )
				{
				return ErrERRCheck( JET_errBufferTooSmall );
				}
			cbUnchecked -= sizeof(EXTENTINFO);
			}
		}

	if ( FSPAvailExtent( fSPExtents ) )
		{
		if ( cbUnchecked < sizeof(CPG) )
			{
			return ErrERRCheck( JET_errBufferTooSmall );
			}
		cbUnchecked -= sizeof(CPG);

		//	if list needed, ensure enough space for list sentinel
		//
		if ( FSPExtentList( fSPExtents ) )
			{
			if ( cbUnchecked < sizeof(EXTENTINFO) )
				{
				return ErrERRCheck( JET_errBufferTooSmall );
				}
			cbUnchecked -= sizeof(EXTENTINFO);
			}
		}

	return JET_errSuccess;
	}
						  

LOCAL ERR ErrSPIGetInfo(
	FUCB		*pfucb,
	INT			*pcpgTotal,
	INT			*piext,
	INT			cext,
	EXTENTINFO	*rgext,
	INT			*pcextSentinelsRemaining,
	CPRINTF		* const pcprintf )
	{
	ERR			err;
	DIB			dib;
	INT			iext			= *piext;
	const BOOL	fExtentList		= ( cext > 0 );

	PGNO		pgnoLastSeen	= pgnoNull;
	CPG			cpgSeen 		= 0;
	ULONG		cRecords 		= 0;
	ULONG		cRecordsDeleted = 0;
	
	Assert( !fExtentList || NULL != pcextSentinelsRemaining );

	*pcpgTotal = 0;

	//  we will be traversing the entire tree in order, preread all the pages
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	
	dib.dirflag = fDIRNull;
	dib.pos = posFirst;
	Assert( FFUCBSpace( pfucb ) );
	err = ErrBTDown( pfucb, &dib, latchReadNoTouch );

	if ( err != JET_errRecordNotFound )
		{
		Call( err );

		forever
			{
#ifdef DEBUG_DUMP_SPACE_INFO
			PGNO	pgnoLast;
			CPG		cpg;
			
			LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
			cpg = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();
			printf( "\n    %d-%d %d", pgnoLast-cpg+1, pgnoLast, Pcsr( pfucb )->Pgno() );
			if ( FNDDeleted( pfucb->kdfCurr ) )
				printf( " (Del)" );
#endif

			if( pcprintf )
				{
				PGNO	pgnoLast;
				LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
				
				const CPG cpg = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();	

				if( pgnoLastSeen != Pcsr( pfucb )->Pgno() )
					{
					pgnoLastSeen = Pcsr( pfucb )->Pgno();
					++cpgSeen;
					}
					
				(*pcprintf)( "%s:\t%d-%d %d [%d]%s\n",
								SzNameOfTable( pfucb ),
								pgnoLast-cpg+1,
								pgnoLast,
								cpg,
								Pcsr( pfucb )->Pgno(),
								FNDDeleted( pfucb->kdfCurr ) ? " (DEL)" : "" );

				++cRecords;
				if( FNDDeleted( pfucb->kdfCurr ) )
					{
					++cRecordsDeleted;
					}
				}				
				
			Assert( pfucb->kdfCurr.data.Cb() == sizeof(PGNO) );
			*pcpgTotal += *(UnalignedLittleEndian< PGNO > *)pfucb->kdfCurr.data.Pv();

			if ( fExtentList )
				{
				Assert( iext < cext );
				Assert( pfucb->kdfCurr.key.Cb() == sizeof(PGNO) );

				//	be sure to leave space for the sentinels
				//	(if no more room, we still want to keep
				//	calculating page count - we just can't
				//	keep track of individual extents anymore
				//
				Assert( iext + *pcextSentinelsRemaining <= cext );
				if ( iext + *pcextSentinelsRemaining < cext )
					{
					PGNO pgno;
					LongFromKey( &pgno, pfucb->kdfCurr.key );
					rgext[iext].pgnoLastInExtent = pgno;
					rgext[iext].cpgExtent = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();
					iext++;
					}
				}

			err = ErrBTNext( pfucb, fDIRNull );
			if ( err < 0 )
				{
				if ( err != JET_errNoCurrentRecord )
					goto HandleError;
				break;
				}
			}
		}

	if ( fExtentList )
		{
		Assert( iext < cext );

		rgext[iext].pgnoLastInExtent = pgnoNull;
		rgext[iext].cpgExtent = 0;
		iext++;

		Assert( NULL != pcextSentinelsRemaining );
		Assert( *pcextSentinelsRemaining > 0 );
		(*pcextSentinelsRemaining)--;
		}
		
	*piext = iext;
	Assert( *piext + *pcextSentinelsRemaining <= cext );

	if( pcprintf )
		{
		(*pcprintf)( "%s:\t%d pages, %d nodes, %d deleted nodes\n", 
						SzNameOfTable( pfucb ),
						cpgSeen,
						cRecords,
						cRecordsDeleted );
		}

	err = JET_errSuccess;

HandleError:
	return err;
	}


ERR ErrSPGetInfo( 
	PIB			*ppib, 
	const IFMP	ifmp, 
	FUCB		*pfucb,
	BYTE		*pbResult, 
	const ULONG	cbMax, 
	const ULONG	fSPExtents,
	CPRINTF * const pcprintf )
	{
	ERR			err;
	CPG			*pcpgOwnExtTotal;
	CPG			*pcpgAvailExtTotal;
	EXTENTINFO	*rgext;
	FUCB 		*pfucbT				= pfucbNil;
	INT			iext;

	//	must specify either owned extent or available extent (or both) to retrieve
	//
	if ( !( FSPOwnedExtent( fSPExtents ) || FSPAvailExtent( fSPExtents ) ) )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	CallR( ErrSPCheckInfoBuf( cbMax, fSPExtents ) );

	memset( pbResult, '\0', cbMax );

	//	setup up return information.  owned extent is followed by available extent.  
	//	This is followed by extent list for both trees
	//
	if ( FSPOwnedExtent( fSPExtents ) )
		{
		pcpgOwnExtTotal = (CPG *)pbResult;
		if ( FSPAvailExtent( fSPExtents ) )
			{
			pcpgAvailExtTotal = pcpgOwnExtTotal + 1;
			rgext = (EXTENTINFO *)( pcpgAvailExtTotal + 1 );
			}
		else
			{
			pcpgAvailExtTotal = NULL;
			rgext = (EXTENTINFO *)( pcpgOwnExtTotal + 1 );
			}
		}
	else
		{
		Assert( FSPAvailExtent( fSPExtents ) );
		pcpgOwnExtTotal = NULL;
		pcpgAvailExtTotal = (CPG *)pbResult;
		}

	const BOOL	fExtentList		= FSPExtentList( fSPExtents );
	const INT	cext			= fExtentList ? ( ( pbResult + cbMax ) - ( (BYTE *)rgext ) ) / sizeof(EXTENTINFO) : 0;
	INT			cextSentinelsRemaining;
	if ( fExtentList )
		{
		if ( FSPOwnedExtent( fSPExtents ) && FSPAvailExtent( fSPExtents ) )
			{
			Assert( cext >= 2 );
			cextSentinelsRemaining = 2;
			}
		else
			{
			Assert( cext >= 1 );
			cextSentinelsRemaining = 1;
			}
		}
	else
		{
		cextSentinelsRemaining = 0;
		}

	if ( pfucbNil == pfucb )
		{
		err = ErrBTOpen( ppib, pgnoSystemRoot, ifmp, &pfucbT );
		}
	else
		{
		err = ErrBTOpen( ppib, pfucb->u.pfcb, &pfucbT );
		}
	CallR( err );
	Assert( pfucbNil != pfucbT );

	Call( ErrBTIGotoRoot( pfucbT, latchReadTouch ) );
	Assert( pcsrNil == pfucbT->pcsrRoot );
	pfucbT->pcsrRoot = Pcsr( pfucbT );

	if ( !pfucbT->u.pfcb->FSpaceInitialized() )
		{
		//	UNDONE: Are there cuncurrency issues with updating the FCB
		//	while we only have a read latch?
		SPIInitFCB( pfucbT, fTrue );
		if( pgnoNull != pfucbT->u.pfcb->PgnoOE() )
			{
			BFPrereadPageRange( pfucbT->ifmp, pfucbT->u.pfcb->PgnoOE(), 2 );
			}
		}

	//	initialize number of extent list entries
	//
	iext = 0;

	if ( FSPOwnedExtent( fSPExtents ) )
		{
		Assert( NULL != pcpgOwnExtTotal );

		//	if single extent format, then free extent in external header
		//
		if ( pfucbT->u.pfcb->PgnoOE() == pgnoNull )
			{
			SPACE_HEADER 	*psph;

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: OWNEXT: single extent\n", SzNameOfTable( pfucbT ) );
				}
				
			Assert( pfucbT->pcsrRoot != pcsrNil );
			Assert( pfucbT->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
			Assert( pfucbT->pcsrRoot->FLatched() );
			Assert( !FFUCBSpace( pfucbT ) );

			//	get external header
			//
			NDGetExternalHeader( pfucbT, pfucbT->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbT->kdfCurr.data.Cb() );
			psph = reinterpret_cast <SPACE_HEADER *> ( pfucbT->kdfCurr.data.Pv() );

			*pcpgOwnExtTotal = psph->CpgPrimary();
			if ( fExtentList )
				{
				Assert( iext + cextSentinelsRemaining <= cext );
				if ( iext + cextSentinelsRemaining < cext )
					{
					rgext[iext].pgnoLastInExtent = PgnoFDP( pfucbT ) + psph->CpgPrimary() - 1;
					rgext[iext].cpgExtent = psph->CpgPrimary();
					iext++;
					}
					
				Assert( iext + cextSentinelsRemaining <= cext );
				rgext[iext].pgnoLastInExtent = pgnoNull;
				rgext[iext].cpgExtent = 0;
				iext++;
				
				Assert( cextSentinelsRemaining > 0 );
				cextSentinelsRemaining--;

				if ( iext == cext )
					{
					Assert( !FSPAvailExtent( fSPExtents ) );
					Assert( 0 == cextSentinelsRemaining );
					}
				else
					{
					Assert( iext < cext );
					Assert( iext + cextSentinelsRemaining <= cext );
					}
				}
			}
			
		else
			{
			//	open cursor on owned extent tree
			//
			FUCB	*pfucbOE = pfucbNil;
		
			Call( ErrSPIOpenOwnExt( ppib, pfucbT->u.pfcb, &pfucbOE ) );

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: OWNEXT\n", SzNameOfTable( pfucbT ) );
				}

			err = ErrSPIGetInfo(
				pfucbOE,
				reinterpret_cast<INT *>( pcpgOwnExtTotal ),
				&iext,
				cext,
				rgext,
				&cextSentinelsRemaining,
				pcprintf );

			Assert( pfucbOE != pfucbNil );
			BTClose( pfucbOE );
			Call( err );
			}
		}

	if ( FSPAvailExtent( fSPExtents ) )
		{
		Assert( NULL != pcpgAvailExtTotal );
		Assert( !fExtentList || 1 == cextSentinelsRemaining );
		
		//	if single extent format, then free extent in external header
		//
		if ( pfucbT->u.pfcb->PgnoOE() == pgnoNull )
			{
			SPACE_HEADER 	*psph;

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: AVAILEXT: single extent\n", SzNameOfTable( pfucbT ) );
				}

			Assert( pfucbT->pcsrRoot != pcsrNil );
			Assert( pfucbT->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
			Assert( pfucbT->pcsrRoot->FLatched() );
			Assert( !FFUCBSpace( pfucbT ) );

			//	get external header
			//
			NDGetExternalHeader( pfucbT, pfucbT->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbT->kdfCurr.data.Cb() );
			psph = reinterpret_cast <SPACE_HEADER *> ( pfucbT->kdfCurr.data.Pv() );

			*pcpgAvailExtTotal = 0;
			
			//	continue through rgbitAvail finding all available extents
			//
			PGNO	pgnoT			= PgnoFDP( pfucbT ) + 1;
			CPG		cpgPrimarySeen	= 1;		//	account for pgnoFDP
			PGNO	pgnoPrevAvail	= pgnoNull;
			UINT	rgbitT;

			for ( rgbitT = 0x00000001;
				rgbitT != 0 && cpgPrimarySeen < psph->CpgPrimary();
				cpgPrimarySeen++, pgnoT++, rgbitT <<= 1 )
				{
				Assert( pgnoT <= PgnoFDP( pfucbT ) + cpgSmallSpaceAvailMost );
				
				if ( rgbitT & psph->RgbitAvail() ) 
					{
					(*pcpgAvailExtTotal)++;

					if ( fExtentList )
						{
						Assert( iext + cextSentinelsRemaining <= cext );
						if ( pgnoT == pgnoPrevAvail + 1 )
							{
							Assert( iext > 0 );
							const INT	iextPrev	= iext - 1;
							Assert( pgnoNull != pgnoPrevAvail );
							Assert( pgnoPrevAvail == rgext[iextPrev].pgnoLastInExtent );
							rgext[iextPrev].pgnoLastInExtent = pgnoT;
							rgext[iextPrev].cpgExtent++;
							
							Assert( rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent
									>= PgnoFDP( pfucbT ) );
									
							pgnoPrevAvail = pgnoT;
							}
						else if ( iext + cextSentinelsRemaining < cext )
							{
							rgext[iext].pgnoLastInExtent = pgnoT;
							rgext[iext].cpgExtent = 1;
							iext++;

							Assert( iext + cextSentinelsRemaining <= cext );
							
							pgnoPrevAvail = pgnoT;
							}
						}
					}
				}


			//	must also account for any pages that were not present in
			//	the space bitmap
			if ( psph->CpgPrimary() > cpgSmallSpaceAvailMost + 1 )
				{
				Assert( cpgSmallSpaceAvailMost + 1 == cpgPrimarySeen );
				const CPG	cpgRemaining		= psph->CpgPrimary() - ( cpgSmallSpaceAvailMost + 1 );

				(*pcpgAvailExtTotal) += cpgRemaining;

				if ( fExtentList )
					{
					Assert( iext + cextSentinelsRemaining <= cext );
					
					const PGNO	pgnoLastOfRemaining	= PgnoFDP( pfucbT ) + psph->CpgPrimary() - 1;
					if ( pgnoLastOfRemaining - cpgRemaining == pgnoPrevAvail )
						{
						Assert( iext > 0 );
						const INT	iextPrev	= iext - 1;
						Assert( pgnoNull != pgnoPrevAvail );
						Assert( pgnoPrevAvail == rgext[iextPrev].pgnoLastInExtent );
						rgext[iextPrev].pgnoLastInExtent = pgnoLastOfRemaining;
						rgext[iextPrev].cpgExtent += cpgRemaining;
						
						Assert( rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent
								>= PgnoFDP( pfucbT ) );
						}
					else if ( iext + cextSentinelsRemaining < cext )
						{
						rgext[iext].pgnoLastInExtent = pgnoLastOfRemaining;
						rgext[iext].cpgExtent = cpgRemaining;
						iext++;

						Assert( iext + cextSentinelsRemaining <= cext );
						}
					}
				
				}
				
			if ( fExtentList )
				{
				Assert( iext < cext );	//  otherwise ErrSPCheckInfoBuf would fail
				rgext[iext].pgnoLastInExtent = pgnoNull;
				rgext[iext].cpgExtent = 0;
				iext++;

				Assert( cextSentinelsRemaining > 0 );
				cextSentinelsRemaining--;
				
				Assert( iext + cextSentinelsRemaining <= cext );
				}
			}
			
		else
			{
			//	open cursor on available extent tree
			//
			FUCB	*pfucbAE = pfucbNil;

			Call( ErrSPIOpenAvailExt( ppib, pfucbT->u.pfcb, &pfucbAE ) );

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: AVAILEXT\n", SzNameOfTable( pfucbT ) );
				}

			err = ErrSPIGetInfo(
				pfucbAE,
				reinterpret_cast<INT *>( pcpgAvailExtTotal ),
				&iext,
				cext,
				rgext,
				&cextSentinelsRemaining,
				pcprintf );
				
#ifdef DEBUG_DUMP_SPACE_INFO
printf( "\n\n" );
#endif

			Assert( pfucbAE != pfucbNil );
			BTClose( pfucbAE );
			Call( err );
			}

		//	if possible, verify AvailExt total against OwnExt total
		Assert( !FSPOwnedExtent( fSPExtents )
			|| ( *pcpgAvailExtTotal <= *pcpgOwnExtTotal ) );
		}
		
	Assert( 0 == cextSentinelsRemaining );
		

HandleError:
	Assert( pfucbNil != pfucbT );
	pfucbT->pcsrRoot = pcsrNil;
	BTClose( pfucbT );

	return err;
	}


#ifdef SPACECHECK

LOCAL ERR ErrSPIValidFDP( PIB *ppib, IFMP ifmp, PGNO pgnoFDP )
	{
	ERR			err;
	FUCB		*pfucb = pfucbNil;
	FUCB		*pfucbOE = pfucbNil;
	DIB			dib;
	BOOKMARK	bm;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( pgnoFDP != pgnoNull );

	//	get temporary FUCB
	//
	Call( ErrBTOpen( ppib, pgnoFDP, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( pfucb->u.pfcb->FInitialized() );

	if ( pfucb->u.pfcb->PgnoOE() != pgnoNull )
		{
		//	open cursor on owned extent
		//
		Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );

		//	search for pgnoFDP in owned extent tree
		//
		KeyFromLong( rgbKey, pgnoFDP );
		bm.Nullify();
		bm.key.suffix.SetPv( rgbKey );
		bm.key.suffix.SetCb( sizeof(PGNO) );
		dib.pos = posDown;
		dib.pbm = &bm;
		dib.dirflag = fDIRNull;
		Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
		if ( err == wrnNDFoundGreater )
			{
			Call( ErrBTGet( pfucbOE ) );
			}

		Assert( pfucbOE->kdfCurr.key.Cb() == sizeof( PGNO ) );
		LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );

		Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgOESize = *(UnalignedLittleEndian< PGNO > *)pfucbOE->kdfCurr.data.Pv();

		//	FDP page should be first page of primary extent
		//
		Assert( pgnoFDP == pgnoOELast - cpgOESize + 1 );
		}

HandleError:
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	if ( pfucb != pfucbNil )
		BTClose( pfucb );
	return err;
	}


//	checks if an extent described by pgnoFirst, cpgSize was allocated
//
LOCAL ERR ErrSPIWasAlloc(
	FUCB	*pfucb,
	PGNO	pgnoFirst, 
	CPG		cpgSize )
	{
	ERR		err;
	FUCB	*pfucbOE = pfucbNil;
	FUCB	*pfucbAE = pfucbNil;
	DIB		dib;
	BOOKMARK bm;
	PGNO	pgnoOwnLast;
	CPG		cpgOwnExt;
	PGNO	pgnoAvailLast;
	PGNO	pgnoLast = pgnoFirst + cpgSize - 1;
	CPG  	cpgAvailExt;
	BYTE	rgbKey[sizeof(PGNO)];

	if ( pfucb->u.pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER 	*psph;
		UINT			rgbitT;
		INT				iT;

		AssertSPIPfucbOnRoot( pfucb );

		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );
		//	make mask for extent to check
		//
		for ( rgbitT = 1, iT = 1; 
			iT < cpgSize && iT < cpgSmallSpaceAvailMost; 
			iT++ )
			rgbitT = (rgbitT<<1) + 1;
		Assert( pgnoFirst - PgnoFDP( pfucb ) < cpgSmallSpaceAvailMost );
		if ( pgnoFirst != PgnoFDP( pfucb ) )
			{
			rgbitT<<(pgnoFirst - PgnoFDP( pfucb ) - 1);
			Assert( ( psph->RgbitAvail() & rgbitT ) == 0 );
			}

		goto HandleError;
		}

	//	open cursor on owned extent
	//
	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );

	//	check that the given extent is owned by the given FDP but not
	//	available in the FDP available extent.
	//
	KeyFromLong( rgbKey, pgnoLast );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRNull;
	Call( ErrBTDown( pfucbOE, &dib, latchReadNoTouch ) );
	if ( err == wrnNDFoundLess )
		{
		Assert( fFalse );
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 == 
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		err = ErrERRCheck( wrnNDFoundGreater );

		#ifdef DEBUG
		PGNO	pgnoT;
		Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoT, pfucbOE->kdfCurr.key );
		Assert( pgnoT > pgnoLast );
		#endif
		}
		
	if ( err == wrnNDFoundGreater )
		{
		Call( ErrBTGet( pfucbOE ) );
		}
	
	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOwnLast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgOwnExt = *(UnalignedLittleEndian< PGNO > *) pfucbOE->kdfCurr.data.Pv();
	Assert( pgnoFirst >= pgnoOwnLast - cpgOwnExt + 1 );
	BTUp( pfucbOE );

	//	check that the extent is not in available extent.  Since the BT search
	//	is keyed with the last page of the extent to be freed, it is sufficient
	//	to check that the last page of the extent to be freed is in the found
	//	extent to determine the full extent has not been allocated.  
	//	If available extent is empty then the extent cannot be in available extent
	//	and has been allocated.
	//
	Call( ErrSPIOpenAvailExt( pfucb->ppib, pfucb->u.pfcb, &pfucbAE ) );
	
	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadNoTouch ) ) < 0 )
		{
		if ( err == JET_errNoCurrentRecord )
			{
			err = JET_errSuccess;
			goto HandleError;
			}
		goto HandleError;
		}

	//	extent should not be found in available extent tree
	//
	Assert( err != JET_errSuccess );
		
	if ( err == wrnNDFoundGreater )
		{
		Call( ErrBTNext( pfucbAE, fDIRNull ) );
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAvailLast, pfucbAE->kdfCurr.key );
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< PGNO > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAvailExt != 0 );
		//	last page of extent should be < first page in available extent node
		//	 
		Assert( pgnoLast < pgnoAvailLast - cpgAvailExt + 1 );
		}
	else
		{
		Assert( err == wrnNDFoundLess );
		Call( ErrBTPrev( pfucbAE, fDIRNull ) );
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAvailLast, pfucbAE->kdfCurr.key );
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< PGNO > *)pfucbAE->kdfCurr.data.Pv();

		//	first page of extent > last page in available extent node
		//
		Assert( pgnoFirst > pgnoAvailLast );
		}
		
HandleError:
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	
	return JET_errSuccess;
	}

#endif	//	SPACE_CHECK
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\sortapi.cxx ===
#include "std.hxx"
#include "_comp.hxx"

///#define NETDOG_DEFRAG_HACK
///#define DEFRAG_SCAN_ONLY


LOCAL LIDMAP	lidmapDefrag;

const INT fCOLSDELETEDNone		= 0;		//	Flags to determine if any columns have been deleted.
const INT fCOLSDELETEDFixedVar	= (1<<0);
const INT fCOLSDELETEDTagged	= (1<<1);

INLINE BOOL FCOLSDELETEDNone	( INT fColumnsDeleted )		{ return ( fColumnsDeleted == fCOLSDELETEDNone ); }
INLINE BOOL FCOLSDELETEDFixedVar( INT fColumnsDeleted )		{ return ( fColumnsDeleted & fCOLSDELETEDFixedVar ); }
INLINE BOOL FCOLSDELETEDTagged	( INT fColumnsDeleted )		{ return ( fColumnsDeleted & fCOLSDELETEDTagged ); }

INLINE VOID FCOLSDELETEDSetNone		( INT& fColumnsDeleted )
	{ 
	fColumnsDeleted = fCOLSDELETEDNone;
	Assert( FCOLSDELETEDNone( fColumnsDeleted ) );
	}
INLINE VOID FCOLSDELETEDSetFixedVar	( INT& fColumnsDeleted )
	{
	fColumnsDeleted |= fCOLSDELETEDFixedVar;
	Assert( FCOLSDELETEDFixedVar( fColumnsDeleted ) );
	}
INLINE VOID FCOLSDELETEDSetTagged	( INT& fColumnsDeleted )
	{ 
	fColumnsDeleted |= fCOLSDELETEDTagged;
	Assert( FCOLSDELETEDTagged( fColumnsDeleted ) );
	}


LOCAL ERR ErrSORTTableOpen(
	PIB				*ppib,
	JET_COLUMNDEF	*rgcolumndef,
	ULONG			ccolumndef,
	IDXUNICODE		*pidxunicode,
	JET_GRBIT		grbit,
	FUCB			**ppfucb,
	JET_COLUMNID	*rgcolumnid )
	{
	ERR				err						= JET_errSuccess;
	INT				icolumndefMax			= (INT)ccolumndef;
	FUCB  			*pfucb					= pfucbNil;
	TDB				*ptdb					= ptdbNil;
	JET_COLUMNDEF	*pcolumndef				= NULL;
	JET_COLUMNID	*pcolumnid				= NULL;
	JET_COLUMNDEF	*pcolumndefMax			= rgcolumndef+icolumndefMax;
	TCIB			tcib					= { fidFixedLeast-1, fidVarLeast-1, fidTaggedLeast-1 };
	WORD			ibRec;
	BOOL			fTruncate;
	BOOL			fIndexOnLocalizedText	= fFalse;
	IDXSEG			rgidxseg[JET_ccolKeyMost];
	ULONG			iidxsegMac;
	IDB				idb;
	idb.SetCidxsegConditional( 0 );

	CheckPIB( ppib );

	INST			*pinst = PinstFromPpib( ppib );

	CallJ( ErrSORTOpen( ppib, &pfucb, fTrue, fFalse ), SimpleError );
	*ppfucb = pfucb;

	//	save open flags
	//
	pfucb->u.pscb->grbit = grbit;

	//	determine max field ids and fix up lengths
	//

	//====================================================
	// Determine field "mode" as follows:
	// if ( JET_bitColumnTagged given ) or "long" ==> TAGGED
	// else if ( numeric type || JET_bitColumnFixed given ) ==> FIXED
	// else ==> VARIABLE
	//====================================================
	// Determine maximum field length as follows:
	// switch ( field type )
	//	   case numeric:
	//		   max = <exact length of specified type>;
	//	   case "short" textual:
	//		   if ( specified max == 0 ) max = JET_cbColumnMost
	//		   else max = MIN( JET_cbColumnMost, specified max )
	//====================================================
	for ( pcolumndef = rgcolumndef, pcolumnid = rgcolumnid; pcolumndef < pcolumndefMax; pcolumndef++, pcolumnid++ )
		{
		if ( FRECSLV( pcolumndef->coltyp ) )
			{
#ifdef TEMP_SLV
			if ( ( *pcolumnid = ++tcib.fidTaggedLast ) > fidTaggedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
#else
			Assert( fFalse );	//	streaming file on temp db currently not supported
			Error( ErrERRCheck( JET_errSLVStreamingFileNotCreated ), HandleError );
#endif			
			}
		else if ( ( pcolumndef->grbit & JET_bitColumnTagged )
			|| FRECLongValue( pcolumndef->coltyp ) )
			{
			if ( ( *pcolumnid = ++tcib.fidTaggedLast ) > fidTaggedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
			}
		else if ( pcolumndef->coltyp == JET_coltypBit ||
			pcolumndef->coltyp == JET_coltypUnsignedByte ||
			pcolumndef->coltyp == JET_coltypShort ||
			pcolumndef->coltyp == JET_coltypLong ||
			pcolumndef->coltyp == JET_coltypCurrency ||
			pcolumndef->coltyp == JET_coltypIEEESingle ||
			pcolumndef->coltyp == JET_coltypIEEEDouble ||
			pcolumndef->coltyp == JET_coltypDateTime ||
			( pcolumndef->grbit & JET_bitColumnFixed ) )
			{
			if ( ( *pcolumnid = ++tcib.fidFixedLast ) > fidFixedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
			}
		else
			{
			if ( ( *pcolumnid = ++tcib.fidVarLast ) > fidVarMost )
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
			}
		}

	Assert( pfucb->u.pscb->fcb.FTypeSort() );
	Assert( pfucb->u.pscb->fcb.FPrimaryIndex() );
	Assert( pfucb->u.pscb->fcb.FSequentialIndex() );
	
	Call( ErrTDBCreate( pinst, &ptdb, &tcib ) );

	Assert( ptdb->ItagTableName() == 0 );		// No name associated with temp. tables

	pfucb->u.pscb->fcb.SetPtdb( ptdb );
	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );
	Assert( pidbNil == pfucb->u.pscb->fcb.Pidb() );

	ibRec = ibRECStartFixedColumns;

	iidxsegMac = 0;
	for ( pcolumndef = rgcolumndef, pcolumnid = rgcolumnid; pcolumndef < pcolumndefMax; pcolumndef++, pcolumnid++ )
		{
		FIELD	field;
		BOOL	fLocalizedText	= fFalse;

		memset( &field, 0, sizeof(FIELD) );
		field.coltyp = FIELD_COLTYP( pcolumndef->coltyp );
		if ( FRECTextColumn( field.coltyp ) )
			{
			//	check code page
			//
			switch( pcolumndef->cp )
				{
				case usUniCodePage:
					fLocalizedText = fTrue;
					field.cp = usUniCodePage;
					break;
					
				case 0:
				case usEnglishCodePage:
					field.cp = usEnglishCodePage;
					break;

				default:
					Error( ErrERRCheck( JET_errInvalidCodePage ), HandleError );
					break;
				}
			}

		Assert( field.coltyp != JET_coltypNil );
		field.cbMaxLen = UlCATColumnSize( field.coltyp, pcolumndef->cbMax, &fTruncate );

		//	ibRecordOffset is only relevant for fixed fields.  It will be ignored by
		//	RECAddFieldDef(), so do not set it.
		//
		if ( FCOLUMNIDFixed( *pcolumnid ) )
			{
			field.ibRecordOffset = ibRec;
			ibRec = WORD( ibRec + field.cbMaxLen );
			}

		Assert( !FCOLUMNIDTemplateColumn( *pcolumnid ) );
		Call( ErrRECAddFieldDef( ptdb, *pcolumnid, &field ) );

		if ( ( pcolumndef->grbit & JET_bitColumnTTKey ) && iidxsegMac < JET_ccolKeyMost )
			{
			rgidxseg[iidxsegMac].ResetFlags();
			rgidxseg[iidxsegMac].SetFid( FidOfColumnid( *pcolumnid ) );

			if ( pcolumndef->grbit & JET_bitColumnTTDescending )
				rgidxseg[iidxsegMac].SetFDescending();

			if ( fLocalizedText )
				fIndexOnLocalizedText = fTrue;

			iidxsegMac++;
			}
		}
	Assert( ibRec <= cbRECRecordMost );
	ptdb->SetIbEndFixedColumns( ibRec, ptdb->FidFixedLast() );

	//	set up the IDB and index definition if necessary
	//
	Assert( iidxsegMac <= JET_ccolKeyMost );
	idb.SetCidxseg( (BYTE)iidxsegMac );
	if ( iidxsegMac > 0 )
		{
		idb.SetCbVarSegMac( JET_cbPrimaryKeyMost );

		idb.ResetFlags();

		if ( NULL != pidxunicode )
			{
			*( idb.Pidxunicode() ) = *pidxunicode;
			Call( ErrFILEICheckUserDefinedUnicode( idb.Pidxunicode() ) );

			idb.SetFLocaleId();
			}
		else
			{
			Assert( !idb.FLocaleId() );
			*( idb.Pidxunicode() ) = idxunicodeDefault;
			}

		if ( fIndexOnLocalizedText )
			idb.SetFLocalizedText();
		if ( grbit & JET_bitTTSortNullsHigh )
			idb.SetFSortNullsHigh();

		idb.SetFPrimary();
		idb.SetFAllowAllNulls();
		idb.SetFAllowFirstNull();
		idb.SetFAllowSomeNulls();
		idb.SetFUnique();
		idb.SetItagIndexName( 0 );	// Sorts and temp tables don't store index name

		Call( ErrIDBSetIdxSeg( &idb, ptdb, rgidxseg ) );

		Call( ErrFILEIGenerateIDB( &( pfucb->u.pscb->fcb ), ptdb, &idb ) );

		Assert( pidbNil != pfucb->u.pscb->fcb.Pidb() );

		pfucb->u.pscb->fcb.ResetSequentialIndex();
		}

	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );

	//	reset copy buffer
	//
	pfucb->pvWorkBuf = NULL;
	pfucb->dataWorkBuf.SetPv( NULL );
	FUCBResetUpdateFlags( pfucb );

	//	reset key buffer
	//
	pfucb->dataSearchKey.Nullify();
	pfucb->cColumnsInSearchKey = 0;
	KSReset( pfucb );

	return JET_errSuccess;

HandleError:
	SORTClose( pfucb );
SimpleError:
	*ppfucb = pfucbNil;
	return err;
	}


ERR VTAPI ErrIsamSortOpen(
	PIB					*ppib,
	JET_COLUMNDEF		*rgcolumndef,
	ULONG				ccolumndef,
	JET_UNICODEINDEX	*pidxunicode,
	JET_GRBIT			grbit,
	FUCB				**ppfucb,
	JET_COLUMNID		*rgcolumnid )
	{
	ERR				err;
	FUCB 			*pfucb;

	CallR( ErrSORTTableOpen( ppib, rgcolumndef, ccolumndef, (IDXUNICODE *)pidxunicode, grbit, &pfucb, rgcolumnid ) );
	Assert( pfucb->u.pscb->fcb.WRefCount() == 1 );
	SORTBeforeFirst( pfucb );

	pfucb->pvtfndef = &vtfndefTTSortIns;

	//	sort is done on the temp database which is always updatable
	//
	FUCBSetUpdatable( pfucb );
	*ppfucb = pfucb;

	return err;
	}



ERR VTAPI ErrIsamSortSetIndexRange( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pfucb->u.pscb->grbit & JET_bitTTScrollable|JET_bitTTIndexed );

	if ( !FKSPrepared( pfucb ) )
		{
		return ErrERRCheck( JET_errKeyNotMade );
		}

	FUCBSetIndexRange( pfucb, grbit );
	err =  ErrSORTCheckIndexRange( pfucb );

	//	reset key status
	//
	KSReset( pfucb );

	//	if instant duration index range, then reset index range.
	//
	if ( grbit & JET_bitRangeInstantDuration )
		{
		DIRResetIndexRange( pfucb );
		}

	return err;
	}


ERR VTAPI ErrIsamSortMove( JET_SESID sesid, JET_VTID vtid, LONG csrid, JET_GRBIT grbit )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	Assert( !FSCBInsert( pfucb->u.pscb ) );

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );

	//	move forward csrid records
	//
	if ( csrid > 0 )
		{
		if ( csrid == JET_MoveLast )
			{
			err = ErrSORTLast( pfucb );
			}
		else
			{
			while ( csrid-- > 0 )
				{
				if ( ( err = ErrSORTNext( pfucb ) ) < 0 )
					break;
				}
			}
		}
	else if ( csrid < 0 )
		{
		Assert( ( pfucb->u.pscb->grbit & ( JET_bitTTScrollable | JET_bitTTIndexed ) ) );
		if ( csrid == JET_MoveFirst )
			{
			err = ErrSORTFirst( pfucb );
			}
		else
			{
			while ( csrid++ < 0 )
				{
				if ( ( err = ErrSORTPrev( pfucb ) ) < 0 )
					break;
				}
			}
		}
	else
		{
		//	return currency status for move 0
		//
		Assert( csrid == 0 );
		if ( pfucb->u.pscb->ispairMac > 0
			&& pfucb->ispairCurr < pfucb->u.pscb->ispairMac
			&& pfucb->ispairCurr >= 0 )
			{
			err = JET_errSuccess;
			}
		else
			{
			err = ErrERRCheck( JET_errNoCurrentRecord );
			}
		}
		
	return err;
	}


ERR VTAPI ErrIsamSortSeek( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
	const BOOL 	fGT = ( grbit & ( JET_bitSeekGT | JET_bitSeekGE ) );
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;
	KEY		key;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );
	Assert( pfucb->u.pscb->grbit & JET_bitTTIndexed );

	if ( !( FKSPrepared( pfucb ) ) )
		{
		return ErrERRCheck( JET_errKeyNotMade );
		}

	FUCBAssertValidSearchKey( pfucb );

	//	ignore segment counter
	key.prefix.Nullify();
	key.suffix.SetPv( pfucb->dataSearchKey.Pv() );
	key.suffix.SetCb( pfucb->dataSearchKey.Cb() );

	//	perform seek for equal to or greater than
	//
	err = ErrSORTSeek( pfucb, &key, fGT );
	if ( err >= 0 )
		{
		KSReset( pfucb );
		}

	Assert( err == JET_errSuccess
		|| err == JET_errRecordNotFound
		|| err == JET_wrnSeekNotEqual );

	const INT bitSeekAll = ( JET_bitSeekEQ
							| JET_bitSeekGE
							| JET_bitSeekGT
							| JET_bitSeekLE
							| JET_bitSeekLT );

	//	take additional action if necessary or polymorph error return
	//	based on grbit
	//
	switch ( grbit & bitSeekAll )
		{
		case JET_bitSeekEQ:
			if ( JET_wrnSeekNotEqual == err )
				err = ErrERRCheck( JET_errRecordNotFound );
			case JET_bitSeekGE:
			case JET_bitSeekLE:
				break;
		case JET_bitSeekLT:
			if ( JET_wrnSeekNotEqual == err )
				err = JET_errSuccess;
			else if ( JET_errSuccess == err )
				{
				err = ErrIsamSortMove( ppib, pfucb, JET_MovePrevious, NO_GRBIT );
				if ( JET_errNoCurrentRecord == err )
					err = ErrERRCheck( JET_errRecordNotFound );
				}
			break;
		default:
			Assert( grbit == JET_bitSeekGT );
			if ( JET_wrnSeekNotEqual == err )
				err = JET_errSuccess;
			else if ( JET_errSuccess == err )
				{
				err = ErrIsamSortMove( ppib, pfucb, JET_MoveNext, NO_GRBIT );
				if ( JET_errNoCurrentRecord == err )
					err = ErrERRCheck( JET_errRecordNotFound );
				}
			break;
		}

	return err;
	}


ERR VTAPI ErrIsamSortGetBookmark(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID * const	pvBookmark,
	const ULONG		cbMax,
	ULONG * const	pcbActual )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err = JET_errSuccess;
	LONG	ipb;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pvBookmark != NULL );
	Assert( pscb->crun == 0 );

	if ( cbMax < sizeof( ipb ) )
		{
		return ErrERRCheck( JET_errBufferTooSmall );
		}

	//	bookmark on sort is index to pointer to byte
	//
	ipb = pfucb->ispairCurr;
	if ( ipb < 0 || ipb >= pfucb->u.pscb->ispairMac )
		return ErrERRCheck( JET_errNoCurrentRecord );
	
	*(long *)pvBookmark = ipb;

	if ( pcbActual )
		{
		*pcbActual = sizeof(ipb);
		}

	Assert( err == JET_errSuccess );
	return err;
	}


ERR VTAPI ErrIsamSortGotoBookmark(
	JET_SESID			sesid,
	JET_VTID			vtid,
	const VOID * const	pvBookmark,
	const ULONG			cbBookmark )
	{
	ERR					err;
	PIB * const			ppib	= ( PIB * )( sesid );
	FUCB * const 		pfucb	= ( FUCB * )( vtid );

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pfucb->u.pscb->crun == 0 );
	
	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );

	if ( cbBookmark != sizeof( long )
		|| NULL == pvBookmark )
		{
		return ErrERRCheck( JET_errInvalidBookmark );
		}

	Assert( *( long *)pvBookmark < pfucb->u.pscb->ispairMac );
	Assert( *( long *)pvBookmark >= 0 );
	
	pfucb->ispairCurr = *(LONG *)pvBookmark;

	Assert( err == JET_errSuccess );
	return err;
	}


#ifdef DEBUG

ERR VTAPI ErrIsamSortRetrieveKey(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID*			pv,
	const ULONG		cbMax,
	ULONG*			pcbActual,
	JET_GRBIT		grbit )
	{
 	PIB* const		ppib	= ( PIB * )( sesid );
	FUCB* const		pfucb	= ( FUCB * )( vtid );

	return ErrIsamRetrieveKey( ppib, pfucb, (BYTE *)pv, cbMax, pcbActual, NO_GRBIT );
	}

#endif	// DEBUG


/*	update only supports insert
/**/
ERR VTAPI ErrIsamSortUpdate( 
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID			* pv,
	ULONG			cbMax,
	ULONG			* pcbActual,
	JET_GRBIT		grbit )
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );

	BYTE  	rgbKeyBuf[ JET_cbPrimaryKeyMost ];
	KEY		key;
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	Assert( FFUCBSort( pfucb ) );
	Assert( pfucb->u.pscb->fcb.FTypeSort() );
	if ( !( FFUCBInsertPrepared( pfucb ) ) )
		{
		return ErrERRCheck( JET_errUpdateNotPrepared );
		}
	Assert( pfucb->u.pscb != pscbNil );
	Assert( pfucb->u.pscb->fcb.Ptdb() != ptdbNil );
	/*	cannot get bookmark before sorting.
	/**/

	/*	record to use for put
	/**/
	if ( pfucb->dataWorkBuf.FNull() )
		{
		return ErrERRCheck( JET_errRecordNoCopy );
		}
	else
		{
		BOOL fIllegalNulls;
		CallR( ErrRECIIllegalNulls( pfucb, pfucb->dataWorkBuf, &fIllegalNulls ) )
		if ( fIllegalNulls )
			return ErrERRCheck( JET_errNullInvalid );
		}

	key.prefix.Nullify();
	key.suffix.SetPv( rgbKeyBuf );
	key.suffix.SetCb( sizeof( rgbKeyBuf ) );

	//	UNDONE:	sort to support tagged columns
	Assert( pfucb->u.pscb->fcb.Pidb() != pidbNil );
	CallR( ErrRECRetrieveKeyFromCopyBuffer(
		pfucb, 
		pfucb->u.pscb->fcb.Pidb(),
		&key, 
		1,
		0,
		prceNil ) );

	CallS( ErrRECValidIndexKeyWarning( err ) );
	Assert( wrnFLDOutOfKeys != err );
	Assert( wrnFLDOutOfTuples != err );
	Assert( wrnFLDNotPresentInIndex != err );

	/*	return err if sort requires no NULL segment and segment NULL
	/**/
	if ( pfucb->u.pscb->fcb.Pidb()->FNoNullSeg()
		&& ( wrnFLDNullSeg == err || wrnFLDNullFirstSeg == err || wrnFLDNullKey == err ) )
		{
		return ErrERRCheck( JET_errNullKeyDisallowed );
		}

	/*	add if sort allows
	/**/
	if ( JET_errSuccess == err
		|| ( wrnFLDNullKey == err && pfucb->u.pscb->fcb.Pidb()->FAllowAllNulls() )
		|| ( wrnFLDNullFirstSeg == err && pfucb->u.pscb->fcb.Pidb()->FAllowFirstNull() )
		|| ( wrnFLDNullSeg == err && pfucb->u.pscb->fcb.Pidb()->FAllowFirstNull() ) )
		{
		Assert( key.Cb() <= JET_cbPrimaryKeyMost );
		CallR( ErrSORTInsert( pfucb, key, pfucb->dataWorkBuf ) );
		}

	RECIFreeCopyBuffer( pfucb );
	FUCBResetUpdateFlags( pfucb );

	return err;
	}


ERR VTAPI ErrIsamSortDupCursor(
	JET_SESID		sesid,
	JET_VTID		vtid,
	JET_TABLEID		*ptableid,
	JET_GRBIT		grbit )
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );

	FUCB   			**ppfucbDup	= (FUCB **)ptableid;
	FUCB   			*pfucbDup = pfucbNil;
	ERR				err = JET_errSuccess;

	if ( FFUCBIndex( pfucb ) )
		{
		err = ErrIsamDupCursor( ppib, pfucb, ppfucbDup, grbit );
		return err;
		}

	INST *pinst = PinstFromPpib( ppib );
	Call( ErrFUCBOpen( ppib, pinst->m_mpdbidifmp[ dbidTemp ], &pfucbDup ) );
  	pfucb->u.pscb->fcb.Link( pfucbDup );
	pfucbDup->pvtfndef = &vtfndefTTSortIns;

	pfucbDup->ulFlags = pfucb->ulFlags;

	pfucbDup->dataSearchKey.Nullify();
	pfucbDup->cColumnsInSearchKey = 0;
	KSReset( pfucbDup );

	/*	initialize working buffer to unallocated
	/**/
	pfucbDup->pvWorkBuf = NULL;
	pfucbDup->dataWorkBuf.SetPv( NULL );
	FUCBResetUpdateFlags( pfucbDup );

	/*	move currency to the first record and ignore error if no records
	/**/
	err = ErrIsamSortMove( ppib, pfucbDup, (ULONG)JET_MoveFirst, 0 );
	if ( err < 0  )
		{
		if ( err != JET_errNoCurrentRecord )
			goto HandleError;
		err = JET_errSuccess;
		}

	*ppfucbDup = pfucbDup;

	return JET_errSuccess;

HandleError:
	if ( pfucbDup != pfucbNil )
		{
		FUCBClose( pfucbDup );
		}
	return err;
	}


ERR VTAPI ErrIsamSortClose( 
	JET_SESID		sesid,
	JET_VTID		vtid
	)
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	Assert( pfucb->pvtfndef != &vtfndefInvalidTableid );
	pfucb->pvtfndef = &vtfndefInvalidTableid;

	if ( FFUCBIndex( pfucb ) )
		{
		CheckTable( ppib, pfucb );
		CallS( ErrFILECloseTable( ppib, pfucb ) );
		}
	else
		{
		CheckSort( ppib, pfucb );
		Assert( FFUCBSort( pfucb ) );
		
		/*	release key buffer
		/**/
		RECReleaseKeySearchBuffer( pfucb );

		/*	release working buffer
		/**/
		RECIFreeCopyBuffer( pfucb );

		SORTClose( pfucb );
		}

	return err;
	}


ERR VTAPI ErrIsamSortGetTableInfo(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID			* pv,
	ULONG			cbOutMax,
	ULONG			lInfoLevel )
	{
	FUCB	* const pfucb	= ( FUCB * )( vtid );  

	if ( lInfoLevel != JET_TblInfo )
		{
		return ErrERRCheck( JET_errInvalidOperation );
		}

	/*	check buffer size
	/**/
	if ( cbOutMax < sizeof(JET_OBJECTINFO) )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	memset( (BYTE *)pv, 0x00, (SHORT)cbOutMax );
	( (JET_OBJECTINFO *)pv )->cbStruct = sizeof(JET_OBJECTINFO);
	( (JET_OBJECTINFO *)pv )->objtyp   = JET_objtypTable;

	// Get maximum number of retrievable records.  If sort is totally
	// in-memory, then this number is exact, because we can predetermine
	// how many dupes there are (see CspairSORTIUnique()).  However,
	// if the sort had to be moved to disk, we cannot predetermine
	// how many dupes will be filtered out.  In this case, this number
	// may be larger than the number of retrievable records (it is
	// equivalent to the number of records pumped into the sort
	// in the first place).
	( (JET_OBJECTINFO *)pv )->cRecord  = pfucb->u.pscb->cRecords;

	return JET_errSuccess;
	}


// Advances the copy progress meter.
INLINE ERR ErrSORTCopyProgress(
	STATUSINFO	* pstatus,
	const ULONG	cPagesTraversed )
	{
	JET_SNPROG	snprog;

	Assert( pstatus->pfnStatus );
	Assert( pstatus->snt == JET_sntProgress );

	pstatus->cunitDone += ( cPagesTraversed * pstatus->cunitPerProgression );
	
	Assert( pstatus->cunitProjected <= pstatus->cunitTotal );
	if ( pstatus->cunitDone > pstatus->cunitProjected )
		{
		Assert( fGlobalRepair );
		pstatus->cunitPerProgression = 0;
		pstatus->cunitDone = pstatus->cunitProjected;
		}
		
	Assert( pstatus->cunitDone <= pstatus->cunitTotal );

	snprog.cbStruct = sizeof( JET_SNPROG );
	snprog.cunitDone = pstatus->cunitDone;
	snprog.cunitTotal = pstatus->cunitTotal;

	return ( ERR )( *pstatus->pfnStatus )(
		pstatus->sesid,
		pstatus->snp,
		pstatus->snt,
		&snprog );
	}


ERR ErrSORTIncrementLVRefcountDest(
	PIB				* ppib,
	const LID		lidSrc,
	LID				* const plidDest )
	{
	Assert( JET_tableidNil != lidmapDefrag.Tableid() );
	return lidmapDefrag.ErrIncrementLVRefcountDest(
				ppib,
				lidSrc,
				plidDest );
	}

// This function assumes that the source record has already been completely copied
// over to the destination record.  The only thing left to do is rescan the tagged
// portion of the record looking for separated long values.  If we find any,
// copy them over and update the record's LID accordingly.
INLINE ERR ErrSORTUpdateSeparatedLVs(
	FUCB				* pfucbSrc,
	FUCB				* pfucbDest,
	JET_COLUMNID		* mpcolumnidcolumnidTagged,
	STATUSINFO			* pstatus )
	{
	TAGFIELDS			tagfields( pfucbDest->dataWorkBuf );
	return tagfields.ErrUpdateSeparatedLongValuesAfterCopy(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged,
						pstatus );
	}

INLINE ERR ErrSORTCopyTaggedColumns(
	FUCB				* pfucbSrc,
	FUCB				* pfucbDest,	
	BYTE				* pbRecBuf,
	JET_COLUMNID		* mpcolumnidcolumnidTagged,
	STATUSINFO			* pstatus )
	{
	Assert( Pcsr( pfucbSrc )->FLatched() );

	TAGFIELDS			tagfields( pfucbSrc->kdfCurr.data );

	//	Copy the tagged columns into the record buffer,
	//	to avoid complex latching/unlatching/refreshing
	//	while copying separated long values
	tagfields.Migrate( pbRecBuf );
	CallS( ErrDIRRelease( pfucbSrc ) );

	tagfields.CopyTaggedColumns(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged );

 	return ErrSORTUpdateSeparatedLVs(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged,
						pstatus );
	}


// Returns a count of the bytes copied.
INLINE SIZE_T CbSORTCopyFixedVarColumns(
	TDB				*ptdbSrc,
	TDB				*ptdbDest,
   	CPCOL			*rgcpcol,			// Only used for DEBUG
	ULONG			ccpcolMax,			// Only used for DEBUG
	BYTE			* const pbRecSrc,
	BYTE			* const pbRecDest )
	{
	REC										* const precSrc = (REC *)pbRecSrc;
	REC										* const precDest = (REC *)pbRecDest;
	UnalignedLittleEndian< REC::VAROFFSET > *pibVarOffsSrc;
	UnalignedLittleEndian< REC::VAROFFSET > *pibVarOffsDest;
	BYTE									*prgbitNullSrc;
	BYTE									*prgbitNullDest;
	BYTE									*prgbitNullT;
	FID										fidFixedLastDest;
	FID										fidVarLastDest;
	FID										fidT;
	BYTE									*pbChunkSrc = 0;
	BYTE									*pbChunkDest = 0;
	INT										cbChunk;

	const FID		fidFixedLastSrc	= precSrc->FidFixedLastInRec();
	const FID		fidVarLastSrc	= precSrc->FidVarLastInRec();

	prgbitNullSrc = precSrc->PbFixedNullBitMap();
	pibVarOffsSrc = precSrc->PibVarOffsets();

	Assert( (BYTE *)pibVarOffsSrc > pbRecSrc );
	Assert( (BYTE *)pibVarOffsSrc < pbRecSrc + REC::CbRecordMax() );

	// Need some space for the null-bit array.  Use the space after the
	// theoretical maximum space for fixed columns (ie. if all fixed columns
	// were set).  Assert that the null-bit array will fit in the pathological case.
	const INT	cFixedColumnsDest = ( ptdbDest->FidFixedLast() - fidFixedLeast + 1 );
	Assert( ptdbSrc->IbEndFixedColumns() < REC::CbRecordMax() );
	Assert( ptdbDest->IbEndFixedColumns() <= ptdbSrc->IbEndFixedColumns() );
	Assert( ptdbDest->IbEndFixedColumns() + ( ( cFixedColumnsDest + 7 ) / 8 )
			<= REC::CbRecordMax() );
	prgbitNullT = pbRecDest + ptdbDest->IbEndFixedColumns();
	memset( prgbitNullT, 0, ( cFixedColumnsDest + 7 ) / 8 );

	pbChunkSrc = pbRecSrc + ibRECStartFixedColumns;
	pbChunkDest = pbRecDest + ibRECStartFixedColumns;
	cbChunk = 0;

#ifdef DEBUG
	BOOL			fSawPlaceholderColumn	= fFalse;
#endif	

	Assert( !ptdbSrc->FInitialisingDefaultRecord() );

	fidFixedLastDest = fidFixedLeast-1;
	for ( fidT = fidFixedLeast; fidT <= fidFixedLastSrc; fidT++ )
		{
		const BOOL	fTemplateColumn	= ptdbSrc->FFixedTemplateColumn( fidT );
		const WORD	ibNextOffset	= ptdbSrc->IbOffsetOfNextColumn( fidT );
		const FIELD	*pfieldFixedSrc	= ptdbSrc->PfieldFixed( ColumnidOfFid( fidT, fTemplateColumn ) );
		
		// Copy only undeleted columns
		if ( JET_coltypNil == pfieldFixedSrc->coltyp
			|| FFIELDPrimaryIndexPlaceholder( pfieldFixedSrc->ffield ) )
			{
#ifdef DEBUG			
			if ( FFIELDPrimaryIndexPlaceholder( pfieldFixedSrc->ffield ) )
				fSawPlaceholderColumn = fTrue;
			else
				Assert( !fTemplateColumn );
#endif				
			if ( cbChunk > 0 )
				{
				UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
				pbChunkDest += cbChunk;
				}

			pbChunkSrc = pbRecSrc + ibNextOffset;
			cbChunk = 0;
			}
		else
			{
#ifdef DEBUG		// Assert that the fids match what the columnid map says
			BOOL	fFound = fFalse;
			INT		i;

			for ( i = 0; i < ccpcolMax; i++ )
				{
				if ( rgcpcol[i].columnidSrc == ColumnidOfFid( fidT, fTemplateColumn )  )
					{
					const COLUMNID	columnidT	= ColumnidOfFid(
														FID( fidFixedLastDest+1 ),
														ptdbDest->FFixedTemplateColumn( FID( fidFixedLastDest+1 ) ) );
					Assert( rgcpcol[i].columnidDest == columnidT );
					fFound = fTrue;
					break;
					}
				}
			if ( fidT >= ptdbSrc->FidFixedFirst() )
				{
				// Only columns in the derived table are in the columnid map.
				Assert( fFound );
				}
			else
				{
				// Base table columns are not in the columnid map.  Since base
				// tables have fixed DDL, the columnid in the source and destination
				// table should not have changed.
				Assert( !fFound );
				Assert( fidT == fidFixedLastDest+1
					|| ( fSawPlaceholderColumn && fidT == fidFixedLastDest+1+1 ) );		//	should only be one placeholder column
				}
#endif

			// If the source field is null, assert that the destination column
			// has also been flagged as such.
			const UINT	ifid	= fidT - fidFixedLeast;
	
			Assert( FFixedNullBit( prgbitNullSrc + ( ifid/8 ), ifid )
				|| !FFixedNullBit( prgbitNullT + ( fidFixedLastDest / 8 ), fidFixedLastDest ) );
			if ( FFixedNullBit( prgbitNullSrc + ( ifid/8 ), ifid ) )
				{
				SetFixedNullBit(
					prgbitNullT + ( fidFixedLastDest / 8 ),
					fidFixedLastDest );
 				}

			// Don't increment till here, because the code above requires
			// the fid as an index.
			
			fidFixedLastDest++;

#ifdef DEBUG
			const COLUMNID	columnidT		= ColumnidOfFid(
													fidFixedLastDest,
													ptdbDest->FFixedTemplateColumn( fidFixedLastDest ) );
			const FIELD		* const pfieldT	= ptdbDest->PfieldFixed( columnidT );
			Assert( ibNextOffset > pfieldFixedSrc->ibRecordOffset );
			Assert( pfieldT->ibRecordOffset == pbChunkDest + cbChunk - pbRecDest );
			Assert( pfieldT->ibRecordOffset >= ibRECStartFixedColumns );
			Assert( pfieldT->ibRecordOffset < REC::CbRecordMax() );
			Assert( pfieldT->ibRecordOffset < ptdbDest->IbEndFixedColumns() );
#endif					

			Assert( pfieldFixedSrc->cbMaxLen ==
						(ULONG)( ibNextOffset - pfieldFixedSrc->ibRecordOffset ) );
			cbChunk += pfieldFixedSrc->cbMaxLen;
			}
		}

	Assert( fidFixedLastDest <= ptdbDest->FidFixedLast() );
	
	if ( cbChunk > 0 )
		{
		UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
		}
		
	Assert( prgbitNullT == pbRecDest + ptdbDest->IbEndFixedColumns() );
	if ( fidFixedLastDest < ptdbDest->FidFixedLast() )
		{
		const COLUMNID	columnidT		= ColumnidOfFid(
												FID( fidFixedLastDest+1 ),
												ptdbDest->FFixedTemplateColumn( FID( fidFixedLastDest+1 ) ) );
		const FIELD		* const pfieldT	= ptdbDest->PfieldFixed( columnidT );
		prgbitNullDest = pbRecDest + pfieldT->ibRecordOffset;
					
		// Shift the null-bit array into place.
		memmove( prgbitNullDest, prgbitNullT, ( fidFixedLastDest + 7 ) / 8 );
		}
	else
		{
		prgbitNullDest = prgbitNullT;
		}

	// Should end up at the start of the null-bit array.
	Assert( pbChunkDest + cbChunk == prgbitNullDest );


	// The variable columns must be done in two passes.  The first pass
	// just determines the highest variable columnid in the record.
	// The second pass does the work.
	pibVarOffsDest = (UnalignedLittleEndian<REC::VAROFFSET> *)(
						prgbitNullDest + ( ( fidFixedLastDest + 7 ) / 8 ) );

	fidVarLastDest = fidVarLeast-1;
	for ( fidT = fidVarLeast; fidT <= fidVarLastSrc; fidT++ )
		{
		const COLUMNID	columnidVarSrc			= ColumnidOfFid( fidT, ptdbSrc->FVarTemplateColumn( fidT ) );
		const FIELD		* const pfieldVarSrc	= ptdbSrc->PfieldVar( columnidVarSrc );

		// Only care about undeleted columns
		Assert( pfieldVarSrc->coltyp != JET_coltypNil
			|| !FCOLUMNIDTemplateColumn( columnidVarSrc ) );
		if ( pfieldVarSrc->coltyp != JET_coltypNil )
			{
#ifdef DEBUG		// Assert that the fids match what the columnid map says
			BOOL		fFound = fFalse;
			INT			i;

			for ( i = 0; i < ccpcolMax; i++ )
				{
				if ( rgcpcol[i].columnidSrc == columnidVarSrc )
					{
					const COLUMNID	columnidT		= ColumnidOfFid(
															FID( fidVarLastDest+1 ),
															ptdbDest->FVarTemplateColumn( FID( fidVarLastDest+1 ) ) );
					Assert( rgcpcol[i].columnidDest == columnidT );
					fFound = fTrue;
					break;
					}
				}
			if ( fidT >= ptdbSrc->FidVarFirst() )
				{
				// Only columns in the derived table are in the columnid map.
				Assert( fFound );
				}
			else
				{
				// Base table columns are not in the columnid map.  Since base
				// tables have fixed DDL, the columnid in the source and destination
				// table should not have changed.
				Assert( !fFound );
				Assert( fidT == fidVarLastDest+1 );
				}
#endif

			fidVarLastDest++;
			}
		}
	Assert( fidVarLastDest <= ptdbDest->FidVarLast() );

	// The second iteration through the variable columns, we copy the column data
	// and update the offsets and nullity.
	pbChunkSrc = (BYTE *)( pibVarOffsSrc + ( fidVarLastSrc - fidVarLeast + 1 ) );
	Assert( pbChunkSrc == precSrc->PbVarData() );
	pbChunkDest = (BYTE *)( pibVarOffsDest + ( fidVarLastDest - fidVarLeast + 1 ) );
	cbChunk = 0;

#ifdef DEBUG
	const FID	fidVarLastSave = fidVarLastDest;
#endif

	fidVarLastDest = fidVarLeast-1;
	for ( fidT = fidVarLeast; fidT <= fidVarLastSrc; fidT++ )
		{
		const COLUMNID	columnidVarSrc			= ColumnidOfFid( fidT, ptdbSrc->FVarTemplateColumn( fidT ) );
		const FIELD		* const pfieldVarSrc	= ptdbSrc->PfieldVar( columnidVarSrc );
		const UINT		ifid					= fidT - fidVarLeast;

		// Only care about undeleted columns
		Assert( pfieldVarSrc->coltyp != JET_coltypNil
			|| !FCOLUMNIDTemplateColumn( columnidVarSrc ) );
		if ( pfieldVarSrc->coltyp == JET_coltypNil )
			{
			if ( cbChunk > 0 )
				{
				UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
				pbChunkDest += cbChunk;
				}

			pbChunkSrc = precSrc->PbVarData() + IbVarOffset( pibVarOffsSrc[ifid] );
			cbChunk = 0;
			}
		else
			{
			const REC::VAROFFSET	ibStart = ( fidVarLeast-1 == fidVarLastDest ?
													REC::VAROFFSET( 0 ) :
													IbVarOffset( pibVarOffsDest[fidVarLastDest-fidVarLeast] ) );
			INT	cb;
			
			fidVarLastDest++;
			if ( FVarNullBit( pibVarOffsSrc[ifid] ) )
				{
				pibVarOffsDest[fidVarLastDest-fidVarLeast] = ibStart;
				SetVarNullBit( *( UnalignedLittleEndian< WORD >*)(&pibVarOffsDest[fidVarLastDest-fidVarLeast]) );
				cb = 0;
				}
			else
				{
				if ( ifid > 0 )
					{
					Assert( IbVarOffset( pibVarOffsSrc[ifid] ) >=
								IbVarOffset( pibVarOffsSrc[ifid-1] ) );
					cb = IbVarOffset( pibVarOffsSrc[ifid] )
							- IbVarOffset( pibVarOffsSrc[ifid-1] );
					}
				else
					{
					Assert( IbVarOffset( pibVarOffsSrc[ifid] ) >= 0 );
					cb = IbVarOffset( pibVarOffsSrc[ifid] );
					}
					
				pibVarOffsDest[fidVarLastDest-fidVarLeast] = REC::VAROFFSET( ibStart + cb );
				Assert( !FVarNullBit( pibVarOffsDest[fidVarLastDest-fidVarLeast] ) );
				}

			cbChunk += cb;
			}
		}

	Assert( fidVarLastDest == fidVarLastSave );

	if ( cbChunk > 0 )
		{
		UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
		}

	precDest->SetFidFixedLastInRec( fidFixedLastDest );
	precDest->SetFidVarLastInRec( fidVarLastDest );
	precDest->SetIbEndOfFixedData( REC::RECOFFSET( (BYTE *)pibVarOffsDest - pbRecDest ) );

	// Should end up at the start of the tagflds.
	Assert( precDest->PbTaggedData() == pbChunkDest + cbChunk );

	Assert( precDest->IbEndOfVarData() <= precSrc->IbEndOfVarData() );
		
	return precDest->PbVarData() + precDest->IbEndOfVarData() - pbRecDest;
	}


INLINE VOID SORTCheckVarTaggedCols( const REC *prec, const ULONG cbRec, const TDB *ptdb )
	{
#if 0	//	enable only to fix corruption
	const BYTE			*pbRecMax			= (BYTE *)prec + cbRec;
	TAGFLD				*ptagfld;
	TAGFLD				*ptagfldPrev;
	FID					fid;
	const WORD			wCorruptBit			= 0x0400;

	Assert( prec->FidFixedLastInRec() <= ptdb->FidFixedLast() );
	Assert( prec->FidVarLastInRec() <= ptdb->FidVarLast() );


	UnalignedLittleEndian<REC::VAROFFSET>	*pibVarOffs		= prec->PibVarOffsets();

	Assert( (BYTE *)pibVarOffs <= pbRecMax );

	Assert( !FVarNullBit( pibVarOffs[fidVarLastInRec+1-fidVarLeast] ) );
	Assert( ibVarOffset( pibVarOffs[fidVarLastInRec+1-fidVarLeast] ) ==
		pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
	Assert( pibVarOffs[fidVarLastInRec+1-fidVarLeast] > sizeof(RECHDR) );
	Assert( pibVarOffs[fidVarLastInRec+1-fidVarLeast] <= cbRec );

	for ( fid = fidVarLeast; fid <= fidVarLastInRec; fid++ )
		{
		WORD	db;
		Assert( ibVarOffset( pibVarOffs[fid-fidVarLeast] ) > sizeof(RECHDR) );
		Assert( (ULONG)ibVarOffset( pibVarOffs[fid-fidVarLeast] ) <= cbRec );
		Assert( ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) > sizeof(RECHDR) );
		Assert( (ULONG)ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) <= cbRec );
		Assert( ibVarOffset( pibVarOffs[fid-fidVarLeast] ) <= ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) );

		db = ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) - ibVarOffset( pibVarOffs[ fid-fidVarLeast] );
		Assert( db >= 0 );
		if ( db > JET_cbColumnMost && ( pibVarOffs[fid+1-fidVarLeast] & wCorruptBit ) )
			{
			pibVarOffs[fid+1-fidVarLeast] &= ~wCorruptBit;
			db = ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) - ibVarOffset( pibVarOffs [fid-fidVarLeast] );
			printf( "\nReset corrupt bit in VarOffset entry." );
			}
		Assert( db <= JET_cbColumnMost );
		}

CheckTagged:
	ptagfldPrev = (TAGFLD*)( pbRec + pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
	for ( ptagfld = (TAGFLD*)( pbRec + pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
		(BYTE *)ptagfld < pbRecMax;
		ptagfld = PtagfldNext( ptagfld ) )
		{
		if ( ptagfld->fid > pfdb->fidTaggedLast && ( ptagfld->fid & wCorruptBit ) )
			{
			ptagfld->fid &= ~wCorruptBit;
			printf( "\nReset corrupt bit in Fid of TAGFLD." );
			}
		Assert( ptagfld->fid <= pfdb->fidTaggedLast );
		Assert( FTaggedFid( ptagfld->fid ) );
		if ( (BYTE *)PtagfldNext( ptagfld ) > pbRecMax
			&& ( ptagfld->cbData & wCorruptBit ) )
			{
			ptagfld->cbData &= ~wCorruptBit;
			printf( "\nReset corrupt bit in Cb of TAGFLD." );
			}

		if ( ptagfld->fid < ptagfldPrev->fid )
			{
#ifdef INTRINSIC_LV
			BYTE	rgb[g_cbPageMax];
#else // INTRINSIC_LV
			BYTE	rgb[sizeof(TAGFLD)+cbLVIntrinsicMost];
#endif // INTRINSIC_LV	
			ULONG	cbCopy			= sizeof(TAGFLD)+ptagfldPrev->cb;
			BYTE	*pbNext			= (BYTE *)PtagfldNext( ptagfld );
			
			Assert( cbCopy-sizeof(TAGFLD) <= cbLVIntrinsicMost );
			UtilMemCpy( rgb, ptagfldPrev, cbCopy );
			memmove( ptagfldPrev, ptagfld, sizeof(TAGFLD)+ptagfld->cb );

			ptagfld = PtagfldNext( ptagfldPrev );
			UtilMemCpy( ptagfld, rgb, cbCopy );
			Assert( PtagfldNext( ptagfld ) == m_pbNext );
			printf( "\nFixed TAGFLD out of order." );

			//	restart from beginning to verify order
			goto CheckTagged;
			}
		ptagfldPrev = ptagfld;
		
		Assert( (BYTE *)PtagfldNext( ptagfld ) <= pbRecMax );
		}
	Assert( (BYTE *)ptagfld == pbRecMax );
#endif	//	0	
	}


#ifdef NETDOG_DEFRAG_HACK


/*
 -	FIsMsgFolderTable
 *
 *	Purpose: Tell jet if the given table is a msg folder table.
 *			 Will catch both regular msg folder tables and search msg
 *			 folder tables.
 *			 The format of msg folder table names is "replid-globcnt" all
 *			 textized ofcourse. Search folders have "S-" prefix also.
*/

LOCAL BOOL FIsMsgFolderTable(const char * const szName)
{
	static const char mpnibblech[] = "0123456789ABCDEFabcdef";

	const char*		pchName;
	ULONG			ulLen;
	BOOL			fRet;
	char			ch;
	BOOL			fFoundDash;
	
	// need atleast 3 characters to qualify
	ulLen = strlen(szName);
	if (ulLen < 3)
	{
		fRet = fFalse;
		goto Cleanup;
	}
	
	// First handle the search extension
	if (szName[0] == 'S' && szName[1] == '-')
	{
		pchName = &szName[2];
		ulLen = strlen(pchName);
		if (ulLen < 3)
		{
			fRet = fFalse;
			goto Cleanup;
		}
	}		
	else
		pchName = szName;


	// Now make sure that each character is in the alphabet
	// {'0' - '9', 'A' - 'F'} (mpnibblech) and with exactly one '-'.
	
	fRet = fTrue;		// innocent until proven otherwise
	fFoundDash = fFalse;
	while (ch = *pchName)
	{
		if (ch == '-')
		{
			if (fFoundDash)
			{
				fRet = fFalse;
				break;
			}
			else
				fFoundDash = fTrue;
		}
		else if (!strchr(mpnibblech, ch))
		{
			fRet = fFalse;
			break;
		}
		++pchName;
	}		

	// We need to have one dash in msg folder table name.
	if (!fFoundDash)
		fRet = fFalse;
		
Cleanup:
	return fRet;
}


//  ================================================================
LOCAL BOOL FColumnIsCorrupted( const FUCB * const pfucb, const INT ib )
//  ================================================================
	{
	BOOL fColumnIsCorrupted = fFalse;
	
	const QWORD	qwMaxTime = 0x01BF500000000000;
	const QWORD qwMinTime = 0x01BB000000000000;

	const BYTE * const pbRec	= (BYTE *)pfucb->dataWorkBuf.Pv();
	const BYTE * const pbColumn	= pbRec + ib;
	const QWORD * const pqwTime	= (QWORD *)pbColumn;
	const QWORD qwTime			= *pqwTime;

	if( qwTime > qwMaxTime ) 
		{
		printf( "\nqwTime (%I64d) > qwMaxTime (%I64d)", qwTime, qwMaxTime );
		fColumnIsCorrupted = fTrue;
		}
	else if( qwTime < qwMinTime )
		{
		printf( "\nqwTime (%I64d) < qwMinTime (%I64d)", qwTime, qwMinTime );
		fColumnIsCorrupted = fTrue;
		}	
	else
		{
		fColumnIsCorrupted = fFalse;
		}
		
	return fColumnIsCorrupted;
	}
	

//  ================================================================
LOCAL BOOL FRecordIsCorrupted( const FUCB * const pfucb )
//  ================================================================
	{
	BOOL fRecordIsCorrupted = fFalse;
	
	const INT ibColumnT3007 = 70;
	const INT ibColumnT3008 = 78;

	if( FColumnIsCorrupted( pfucb, ibColumnT3007 ) )
		{
		printf( "\ncolumn T3007 is corrupted" );
		fRecordIsCorrupted = fTrue;
		}
	else if( FColumnIsCorrupted( pfucb, ibColumnT3008 ) )
		{
		printf( "\ncolumn T3008 is corrupted" );
		fRecordIsCorrupted = fTrue;
		}
	else
		{
		fRecordIsCorrupted = fFalse;
		}
		
	return fRecordIsCorrupted;
	}


//  ================================================================
LOCAL VOID FixCorruptedRecord( FUCB * const pfucb )
//  ================================================================
	{

	//	remove the offending byte
	
	const INT ibToRemove = 23;
	const INT cbToRemove = 1;

	BYTE * const pbRec 			= (BYTE *)pfucb->dataWorkBuf.Pv();	
	const BYTE * const pbRecMax	= pbRec + pfucb->dataWorkBuf.Cb();
	Assert( pbRec );
	Assert( pbRecMax > pbRec );
	
	const BYTE * const pbSrc 	= pbRec + ibToRemove + cbToRemove;
	BYTE * const pbDest			= pbRec + ibToRemove;
	Assert( pbSrc > pbDest );

	const INT cbMove			= pbRecMax - pbSrc;
	Assert( cbMove > 0 );
	
	memmove( pbDest, pbSrc, cbMove );
	pfucb->dataWorkBuf.DeltaCb( -cbToRemove );

	//	reduce the ibEndofFixedData

	REC * const 			prec 				= (REC *)pfucb->dataWorkBuf.Pv();
	const REC::RECOFFSET 	ibEndOfFixedDataOld = prec->IbEndOfFixedData();
	const REC::RECOFFSET	ibEndOfFixedDataNew	= REC::RECOFFSET( ibEndOfFixedDataOld - cbToRemove );
	
	prec->SetIbEndOfFixedData( ibEndOfFixedDataNew );
	
	//	reduce the fidFixedLast by 1
	
	const FID fidFixedLast		= prec->FidFixedLastInRec();
	const FID fidFixedLastNew	= FID( fidFixedLast - 1 );

	prec->SetFidFixedLastInRec( fidFixedLastNew );
	}
	

//  ================================================================
LOCAL ERR ErrPossiblyFixCorruptedRecord( FUCB * const pfucb, const PGNO pgno, const INT iline )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const CHAR * const szTableName = pfucb->u.pfcb->Ptdb()->SzTableName();
	if( FIsMsgFolderTable( szTableName ) )
		{
		if( FRecordIsCorrupted( pfucb ) )
			{
			printf( "\nrecord [%d:%d] is corrupted. attempting a fix", pgno, iline );

			FixCorruptedRecord( pfucb );

			if( FRecordIsCorrupted( pfucb ) )
				{
				AssertSz( fFalse, "ErrPossiblyFixCorrupted record could fix a record" );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}			
		}

HandleError:
	return err;
	}


#endif	//	NETDOG_DEFRAG_HACK

	
INLINE ERR ErrSORTCopyOneRecord(
	FUCB			* pfucbSrc,
	FUCB			* pfucbDest,
	BOOKMARK		* const pbmPrimary,
	INT				fColumnsDeleted,
	BYTE			* pbRecBuf,
	CPCOL			* rgcpcol,
	ULONG			ccpcolMax,
	JET_COLUMNID	* mpcolumnidcolumnidTagged,
	STATUSINFO		* pstatus )
	{
	ERR				err				= JET_errSuccess;
	BYTE			* pbRecSrc		= 0;
	BYTE			* pbRecDest		= 0;
	ULONG			cbRecSrc;
	SIZE_T			cbRecSrcFixedVar;
	SIZE_T			cbRecDestFixedVar;

	Assert( Pcsr( pfucbSrc )->FLatched() );
	CallS( ErrDIRRelease( pfucbSrc ) );		// Must release latch in order to call prepare update.

	// Work buffer pre-allocated IsamPrepareUpdate won't continually
	// allocate one.
	Assert( NULL != pfucbDest->pvWorkBuf );
	Assert( pfucbDest->dataWorkBuf.Pv() == pfucbDest->pvWorkBuf );
	
	//	setup pfucbDest for insert
	//
	Call( ErrIsamPrepareUpdate( pfucbDest->ppib, pfucbDest, JET_prepInsert ) );

	//	re-access source record
	//
	Assert( !Pcsr( pfucbSrc )->FLatched() );
	Call( ErrDIRGet( pfucbSrc ) );

	pbRecSrc = (BYTE *)pfucbSrc->kdfCurr.data.Pv();
	cbRecSrc = pfucbSrc->kdfCurr.data.Cb();
	pbRecDest = (BYTE *)pfucbDest->dataWorkBuf.Pv();

	Assert( cbRecSrc >= REC::cbRecordMin );
	Assert( cbRecSrc <= REC::CbRecordMax() );
	
	Assert( ( (REC *)pbRecSrc )->FidFixedLastInRec() <= pfucbSrc->u.pfcb->Ptdb()->FidFixedLast() );
	Assert( ( (REC *)pbRecSrc )->FidVarLastInRec() <= pfucbSrc->u.pfcb->Ptdb()->FidVarLast() );

	cbRecSrcFixedVar = ( (REC *)pbRecSrc )->PbTaggedData() - pbRecSrc;
	Assert( cbRecSrcFixedVar >= REC::cbRecordMin );
	Assert( cbRecSrcFixedVar <= cbRecSrc );

	SORTCheckVarTaggedCols( (REC *)pbRecSrc, cbRecSrc, pfucbSrc->u.pfcb->Ptdb() );

	if ( FCOLSDELETEDNone( fColumnsDeleted ) )
		{
		// Do the copy as one big chunk.
		UtilMemCpy( pbRecDest, pbRecSrc, cbRecSrc );
		pfucbDest->dataWorkBuf.SetCb( cbRecSrc );
		cbRecDestFixedVar = cbRecSrcFixedVar;

#ifdef NETDOG_DEFRAG_HACK
		err = ErrPossiblyFixCorruptedRecord(
				pfucbDest,
				Pcsr( pfucbSrc )->Pgno(),
				Pcsr( pfucbSrc )->ILine() );
		if( JET_errDatabaseCorrupted == err )
			{
			printf( "\nBUH-BYE! (%d:%d)",
				Pcsr( pfucbSrc )->Pgno(),
				Pcsr( pfucbSrc )->ILine() );
			CallS( ErrDIRRelease( pfucbSrc ) );
			CallS( ErrIsamPrepareUpdate( pfucbDest->ppib, pfucbDest, JET_prepCancel ) );
			goto HandleError;
			}
		Call( err );
#endif	//	NETDOG_DEFRAG_HACK
		}
	else
		{
		if ( FCOLSDELETEDFixedVar( fColumnsDeleted ) )
			{
			cbRecDestFixedVar = CbSORTCopyFixedVarColumns(
										pfucbSrc->u.pfcb->Ptdb(),
										pfucbDest->u.pfcb->Ptdb(),
										rgcpcol,
										ccpcolMax,
										pbRecSrc,
										pbRecDest );
			}

		else
			{
			UtilMemCpy( pbRecDest, pbRecSrc, cbRecSrcFixedVar );
			cbRecDestFixedVar = cbRecSrcFixedVar;
			}

		Assert( cbRecDestFixedVar >= REC::cbRecordMin );
		Assert( cbRecDestFixedVar <= cbRecSrcFixedVar );

		if ( FCOLSDELETEDTagged( fColumnsDeleted ) )
			{
			pfucbDest->dataWorkBuf.SetCb( cbRecDestFixedVar );
			Call( ErrSORTCopyTaggedColumns(
				pfucbSrc,
				pfucbDest,
				pbRecBuf,
				mpcolumnidcolumnidTagged,
				pstatus ) );
			AssertDIRNoLatch( pfucbSrc->ppib );

			Assert( pfucbDest->dataWorkBuf.Cb() >= cbRecDestFixedVar );
			Assert( pfucbDest->dataWorkBuf.Cb() <= cbRecSrc );

			// When we copied the tagged columns, we also took care of
			// copying the separated LV's.  We're done now, so go ahead and
			// insert the record.
			goto InsertRecord;
			}
		else
			{
			UtilMemCpy(
				pbRecDest+cbRecDestFixedVar,
				pbRecSrc+cbRecSrcFixedVar,
				cbRecSrc - cbRecSrcFixedVar );
			pfucbDest->dataWorkBuf.SetCb( cbRecDestFixedVar + ( cbRecSrc - cbRecSrcFixedVar ) );
				 
			Assert( pfucbDest->dataWorkBuf.Cb() >= cbRecDestFixedVar );
			Assert( pfucbDest->dataWorkBuf.Cb() <= cbRecSrc );
			}
		}

	Assert( Pcsr( pfucbSrc )->FLatched() );
	CallS( ErrDIRRelease( pfucbSrc ) );

	// Now fix up the LIDs for separated long values, if any.
	Call( ErrSORTUpdateSeparatedLVs(
		pfucbSrc,
		pfucbDest,
		mpcolumnidcolumnidTagged,
		pstatus ) );

InsertRecord:
	if ( pstatus != NULL )
		{
		const FID	fidFixedLast	= ( (REC *)pbRecDest )->FidFixedLastInRec();
		const FID	fidVarLast		= ( (REC *)pbRecDest )->FidVarLastInRec();

		Assert( fidFixedLast >= fidFixedLeast-1 );
		Assert(	fidFixedLast <= pfucbDest->u.pfcb->Ptdb()->FidFixedLast() );
		Assert( fidVarLast >= fidVarLeast-1 );
		Assert( fidVarLast <= pfucbDest->u.pfcb->Ptdb()->FidVarLast() );

		// Do not count record header.
		const INT	cbOverhead =
						ibRECStartFixedColumns								// Record header + offset to tagged fields
						+ ( ( fidFixedLast + 1 - fidFixedLeast ) + 7 ) / 8	// Null array for fixed columns
						+ ( fidVarLast + 1 - fidVarLeast ) * sizeof(WORD);	// Variable offsets array
		Assert( cbRecDestFixedVar >= cbOverhead );

		// Do not count offsets tables or null arrays.
		pstatus->cbRawData += ( cbRecDestFixedVar - cbOverhead );
		}

	// for the moment we try to preserve de sequencial index in order to don't change the bookmark of the record
	// its' a temporary hack for SLV SPACEMAP
	if ( pidbNil == pfucbSrc->u.pfcb->Pidb() )
		{
		//	file is sequential
		//
		DBK	dbk;
		
		Assert ( pfucbSrc->bmCurr.key.Cb() == sizeof(DBK) );
		LongFromKey( &dbk, pfucbSrc->bmCurr.key );
		Assert( dbk > 0 );
				
		pfucbDest->u.pfcb->Ptdb()->SetDbkMost( dbk );
		}

#ifdef DEFRAG_SCAN_ONLY
	FUCBResetUpdateFlags( pfucbDest );
#else	
#ifdef DEBUG
	FID	fidAutoInc;
	BOOL f8BytesAutoInc;
	fidAutoInc = pfucbDest->u.pfcb->Ptdb()->FidAutoincrement();
	f8BytesAutoInc = pfucbDest->u.pfcb->Ptdb()->F8BytesAutoInc();
	if ( fidAutoInc != 0 )
		{
		Assert( FFixedFid( fidAutoInc ) );
		Assert(	fidAutoInc <= pfucbDest->u.pfcb->Ptdb()->FidFixedLast() );
		Assert( FFUCBColumnSet( pfucbDest, fidAutoInc ) );

		// Need to set fid to zero to short-circuit AutoInc check
		// in ErrRECIInsert().
		pfucbDest->u.pfcb->Ptdb()->ResetFidAutoincrement();
		}
		
	err = ErrRECInsert( pfucbDest, pbmPrimary );

	if ( fidAutoInc != 0 )
		{
		// Reset AutoInc after update.
		pfucbDest->u.pfcb->Ptdb()->SetFidAutoincrement( fidAutoInc, f8BytesAutoInc );
		}

	Call( err );
	

#else
	Call( ErrRECInsert( pfucbDest, pbmPrimary ) );
#endif	//	DEBUG
#endif	//	DEFRAG_SCAN_ONLY

HandleError:
	// Work buffer preserved for next record.
	Assert( NULL != pfucbDest->pvWorkBuf || err < 0 );
	
	return err;
	}


// Verify integrity of columnid maps.
INLINE VOID SORTAssertColumnidMaps(
	TDB				*ptdb,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	INT				fColumnsDeleted,
	const BOOL		fTemplateTable )
	{
#ifdef DEBUG

	INT	i;
	if ( FCOLSDELETEDFixedVar( fColumnsDeleted ) )
		{
		// Ensure columnids are monotonically increasing.
		for ( i = 0; i < (INT)ccpcolMax; i++ )
			{
			if ( FCOLUMNIDTemplateColumn( rgcpcol[i].columnidSrc ) )
				{
				Assert( fTemplateTable );
				Assert( FCOLUMNIDTemplateColumn( rgcpcol[i].columnidDest ) );
				}
			else
				{
				Assert( !fTemplateTable );
				Assert( !FCOLUMNIDTemplateColumn( rgcpcol[i].columnidDest ) );
				}

			Assert( FidOfColumnid( rgcpcol[i].columnidDest ) <= FidOfColumnid( rgcpcol[i].columnidSrc ) );
			if ( FCOLUMNIDFixed( rgcpcol[i].columnidSrc ) )
				{
				Assert( FCOLUMNIDFixed( rgcpcol[i].columnidDest ) );
				if ( i > 0 )
					{
					Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
					}
				}
			else
				{
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidSrc ) );
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidDest ) );
				if ( i > 0 )
					{
					if ( FCOLUMNIDVar( rgcpcol[i-1].columnidDest ) )
						{
						Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
						}
					else
						{
						Assert( FCOLUMNIDFixed( rgcpcol[i-1].columnidDest ) );
						}
					}
				}
			}
		}
	else
		{
		// No deleted columns, so ensure columnids didn't change.  Additionally,
		// columnids should be monotonically increasing.
		for ( i = 0; i < (INT)ccpcolMax; i++ )
			{
			Assert( rgcpcol[i].columnidDest == rgcpcol[i].columnidSrc );

			if ( FCOLUMNIDFixed( rgcpcol[i].columnidSrc ) )
				{
				Assert( i == 0 ?
					FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidFixedFirst() :
					rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
				}
			else
				{
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidSrc ) );
				if ( i == 0 )
					{
					// If we get here, there's no fixed columns.
					Assert( FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidVarFirst() );
					Assert( ptdb->FidFixedLast() == ptdb->FidFixedFirst() - 1 );
					}
				else if ( FCOLUMNIDVar( rgcpcol[i-1].columnidDest ) )
					{					
					Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
					}
				else
					{
					// Must be the beginning of the variable columns.
					Assert( FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidVarFirst() );
					Assert( FidOfColumnid( rgcpcol[i-1].columnidDest ) == ptdb->FidFixedLast() );
					}
				}
			}
		}


	// Check tagged columns.  Note that base table columns do not appear in
	// the columnid map.
	FID			fidT		= ptdb->FidTaggedFirst();
	const FID	fidLast		= ptdb->FidTaggedLast();
	if ( FCOLSDELETEDTagged( fColumnsDeleted ) )
		{
		for ( ; fidT <= fidLast; fidT++ )
			{
			const FIELD	*pfieldTagged = ptdb->PfieldTagged( ColumnidOfFid( fidT, fTemplateTable ) );
			if ( pfieldTagged->coltyp != JET_coltypNil )
				{
				Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) );
				Assert(	FidOfColumnid( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) <= fidT );
				}
			}
		}
	else
		{
		// No deleted columns, so ensure columnids didn't change.
		for ( ; fidT <= fidLast; fidT++ )
			{
			Assert( ptdb->PfieldTagged( ColumnidOfFid( fidT, fTemplateTable ) )->coltyp != JET_coltypNil );
			if ( fidT > ptdb->FidTaggedFirst() )
				{
				Assert( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast]
						== mpcolumnidcolumnidTagged[fidT-fidTaggedLeast-1] + 1 );
				}
			Assert( FidOfColumnid( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) == fidT );
			}
		}

#endif	// DEBUG
	}


ERR ErrSORTCopyRecords(
	PIB				*ppib,
	FUCB			*pfucbSrc,
	FUCB			*pfucbDest,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	LONG			crecMax,
	ULONG			*pcsridCopied,
	ULONG			*precidLast,
	BYTE			*pbLVBuf,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	STATUSINFO		*pstatus )
	{
	ERR				err						= JET_errSuccess;
	TDB				*ptdb					= ptdbNil;
	INT				fColumnsDeleted			= 0;
	LONG			dsrid					= 0;
	BYTE			*pbRecBuf				= NULL;		// buffer for source record
	VOID			*pvWorkBuf				= NULL;		// buffer for destination record
	BOOL			fDoAll					= ( 0 == crecMax );
	PGNO			pgnoCurrPage;
	FCB				*pfcbSecondaryIndexes	= pfcbNil;
	FCB				*pfcbNextIndexBatch		= pfcbNil;
	FUCB			*rgpfucbSort[cFILEIndexBatchMax];
	ULONG			rgcRecInput[cFILEIndexBatchMax];
	BOOKMARK		*pbmPrimary				= NULL;
	BOOKMARK		bmPrimary;
	KEY				keyBuffer;
	BYTE			rgbPrimaryBM[JET_cbBookmarkMost];
	BYTE  	 		rgbSecondaryKey[JET_cbSecondaryKeyMost];
	INT				cIndexesToBuild;
	INT				iindex;
	const BOOL		fOldSeq					= FFUCBSequential( pfucbSrc );
	BOOL			fInTrx					= fFalse;

	//	Copy LV tree before copying data records.
	Assert( JET_tableidNil == lidmapDefrag.Tableid() );
	err = ErrCMPCopyLVTree(
				ppib,
				pfucbSrc,
				pfucbDest,
				pbLVBuf, 
				g_cbLVBuf,
				pstatus );
	if ( err < 0 )
		{
		if ( JET_tableidNil != lidmapDefrag.Tableid() )
			{
			CallS( lidmapDefrag.ErrTerm( ppib ) );
			Assert( JET_tableidNil == lidmapDefrag.Tableid() );
			}
		return err;
		}

	//  set FUCB to sequential mode for a more efficient scan
	FUCBSetSequential( pfucbSrc );

//	tableidGlobal may be set in copy long value tree function.
//	Assert( JET_tableidNil == tableidGlobalLIDMap );

	BFAlloc( (VOID **)&pbRecBuf );
	Assert ( NULL != pbRecBuf );
	Assert( NULL != pbLVBuf );

	// Preallocate work buffer so ErrIsamPrepareUpdate() doesn't continually
	// allocate one.
	Assert( NULL == pfucbDest->pvWorkBuf );
	RECIAllocCopyBuffer( pfucbDest );
	pvWorkBuf = pfucbDest->pvWorkBuf;
	Assert ( NULL != pvWorkBuf );

	Assert( ppib == pfucbSrc->ppib );
	Assert( ppib == pfucbDest->ppib );


	ptdb = pfucbSrc->u.pfcb->Ptdb();

	// Need to determine if there were any columns deleted.
	FCOLSDELETEDSetNone( fColumnsDeleted );

	// The fixed/variable columnid map already filters out deleted columns.
	// If the size of the map is not equal to the number of fixed and variable
	// columns in the source table, then we know some have been deleted.
	// Note that for derived tables, we don't have to bother checking deleted
	// columns in the base table, since the DDL of base tables is fixed.
	Assert( ccpcolMax <=
		(ULONG)( ( ptdb->FidFixedLast() + 1 - ptdb->FidFixedFirst() )
					+ ( ptdb->FidVarLast() + 1 - ptdb->FidVarFirst() ) ) );
	if ( ccpcolMax < (ULONG)( ( ptdb->FidFixedLast() + 1 - ptdb->FidFixedFirst() )
								+ ( ptdb->FidVarLast() + 1 - ptdb->FidVarFirst() ) ) )
		{
		FCOLSDELETEDSetFixedVar( fColumnsDeleted );	
		}

	//	LAURIONB_HACK
	extern BOOL g_fCompactTemplateTableColumnDropped;
	if( g_fCompactTemplateTableColumnDropped && pfcbNil != ptdb->PfcbTemplateTable() )
		{
		FCOLSDELETEDSetFixedVar( fColumnsDeleted );	
		}
		
	/*	tagged columnid map works differently than the fixed/variable columnid
	/*	map; deleted columns are not filtered out (they have an entry of 0).  So we
	/*	have to consult the source table's TDB.
	/**/
	FID			fidT		= ptdb->FidTaggedFirst();
	const FID	fidLast		= ptdb->FidTaggedLast();
	Assert( fidLast == fidT-1 || FTaggedFid( fidLast ) );

	if ( ptdb->FESE97DerivedTable() )
		{
		//	columnids will be renumbered - set Deleted flag to 
		//	force FIDs in TAGFLDs to be recalculated
		FCOLSDELETEDSetTagged( fColumnsDeleted );
		}
	else
		{
		for ( ; fidT <= fidLast; fidT++ )
			{
			const FIELD	*pfieldTagged = ptdb->PfieldTagged( ColumnidOfFid( fidT, ptdb->FTemplateTable() ) );
			if ( pfieldTagged->coltyp == JET_coltypNil )
				{
				FCOLSDELETEDSetTagged( fColumnsDeleted );
				break;
				}
			}
		}

	SORTAssertColumnidMaps(
		ptdb,
		rgcpcol,
		ccpcolMax,
		mpcolumnidcolumnidTagged,
		fColumnsDeleted,
		ptdb->FTemplateTable() );

	Assert( crecMax >= 0 );	

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	DIRBeforeFirst( pfucbSrc );
	err = ErrDIRNext( pfucbSrc, fDIRNull );
	if ( err < 0 )
		{
		Assert( JET_errRecordNotFound != err );
		if ( JET_errNoCurrentRecord == err )
			err = JET_errSuccess;		// empty table
		goto HandleError;
		}

	// Disconnect secondary indexes to prevent Update from attempting
	// to update secondary indexes.
	pfcbSecondaryIndexes = pfucbDest->u.pfcb->PfcbNextIndex();
	pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbNil );

	//	NOTE: do not update FAboveThreshold() because we will be putting
	//		  the secondary indexes back before the FCBs refcnt can go
	//		  to 0 (forcing the FCB into the wrong avail list)

#ifdef DEFRAG_SCAN_ONLY
#else
	if ( pfcbNil != pfcbSecondaryIndexes )
		{
		for ( iindex = 0; iindex < cFILEIndexBatchMax; iindex++ )
			rgpfucbSort[iindex] = pfucbNil;

		Call( ErrFILEIndexBatchInit(
					ppib,
					rgpfucbSort,	
					pfcbSecondaryIndexes,
					&cIndexesToBuild,
					rgcRecInput,
					&pfcbNextIndexBatch,
					cFILEIndexBatchMax ) );
					
		bmPrimary.key.prefix.Nullify();
		bmPrimary.key.suffix.SetCb( sizeof( rgbPrimaryBM ) );
		bmPrimary.key.suffix.SetPv( rgbPrimaryBM );
		bmPrimary.data.Nullify();
		pbmPrimary = &bmPrimary;
		
		keyBuffer.prefix.Nullify();
		keyBuffer.suffix.SetCb( sizeof( rgbSecondaryKey ) );
		keyBuffer.suffix.SetPv( rgbSecondaryKey );
		}
#endif		

	pgnoCurrPage = Pcsr( pfucbSrc )->Pgno();
	forever
		{
		Assert( Pcsr( pfucbSrc )->FLatched() );
		err = ErrSORTCopyOneRecord(
			pfucbSrc,
			pfucbDest,
			pbmPrimary,
			fColumnsDeleted,
			pbRecBuf,
			rgcpcol,						// Only used for DEBUG
			ccpcolMax,						// Only used for DEBUG
			mpcolumnidcolumnidTagged,
			pstatus );
		if ( err < 0 )
			{
#ifndef NETDOG_DEFRAG_HACK			
			if ( fGlobalRepair )
				{
				UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_RECORD_ID, 0, NULL );
				}
			else
				goto HandleError;
#endif	//	NETDOG_DEFRAG_HACK				
			}
		else
			{
			// Latch released in order to copy record.
			Assert( !Pcsr( pfucbSrc )->FLatched() );
			Assert( !Pcsr( pfucbDest )->FLatched() );
			
			// Work buffer preserved.
			Assert( pfucbDest->dataWorkBuf.Pv() == pvWorkBuf );
			Assert( pfucbDest->dataWorkBuf.Cb() >= REC::cbRecordMin );
			Assert( pfucbDest->dataWorkBuf.Cb() <= REC::CbRecordMax() );
			}
			

		dsrid++;

#ifdef DEFRAG_SCAN_ONLY
#else
		if ( err >= 0 && pfcbNil != pfcbSecondaryIndexes )
			{
			Assert( cIndexesToBuild > 0 );
			Assert( cIndexesToBuild <= cFILEIndexBatchMax );
			Assert( pbmPrimary == &bmPrimary );
			Call( ErrFILEIndexBatchAddEntry(
						rgpfucbSort,
						pfucbDest,
						pbmPrimary,
						pfucbDest->dataWorkBuf,
						pfcbSecondaryIndexes,
						cIndexesToBuild,
						rgcRecInput,
						keyBuffer ) );	//lint !e644
			}
#endif			

		/*	break if copied required records or if no next/prev record
		/**/

		if ( !fDoAll  &&  --crecMax == 0 )
			break;

		err = ErrDIRNext( pfucbSrc, fDIRNull );
		if ( err < 0 )
			{
			Assert( JET_errRecordNotFound != err );
			if ( err != JET_errNoCurrentRecord  )
				goto HandleError;
			
			if ( pstatus != NULL )
				{
				pstatus->cLeafPagesTraversed++;
				Call( ErrSORTCopyProgress( pstatus, 1 ) );
				}
			err = JET_errSuccess;
			break;
			}

		else if ( pstatus != NULL && pgnoCurrPage != Pcsr( pfucbSrc )->Pgno() )
			{
			pgnoCurrPage = Pcsr( pfucbSrc )->Pgno();
			pstatus->cLeafPagesTraversed++;
			Call( ErrSORTCopyProgress( pstatus, 1 ) );
			}
		}

	Assert( fInTrx );
	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	fInTrx = fFalse;

	if ( JET_tableidNil != lidmapDefrag.Tableid() )
		{
		// Validate LV refcounts
		Call( lidmapDefrag.ErrUpdateLVRefcounts( ppib, pfucbDest ) );
		}
		

	// Reattach secondary indexes.
	Assert( pfucbDest->u.pfcb->PfcbNextIndex() == pfcbNil );
	pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbSecondaryIndexes );

#ifdef DEFRAG_SCAN_ONLY
#else
	if ( pfcbNil != pfcbSecondaryIndexes )
		{
		if ( pstatus )
			{
			Assert( pstatus->cSecondaryIndexes > 0 );
			
			// if cunitPerProgression is 0, then corruption was detected
			// during GlobalRepair
			Assert( pstatus->cunitPerProgression > 0 || fGlobalRepair );
			if ( pstatus->cunitPerProgression > 0 )
				{
				Assert( pstatus->cunitDone <= pstatus->cunitProjected );
				Assert( pstatus->cunitProjected <= pstatus->cunitTotal );
				const ULONG	cpgRemaining = pstatus->cunitProjected - pstatus->cunitDone;

				// Each secondary index has at least an FDP.
				Assert( cpgRemaining >= pstatus->cSecondaryIndexes );

				pstatus->cunitPerProgression = cpgRemaining / pstatus->cSecondaryIndexes;
				Assert( pstatus->cunitPerProgression >= 1 );
				Assert( pstatus->cunitPerProgression * pstatus->cSecondaryIndexes <= cpgRemaining );
				}
			}

		// Finish first batch of indexes, then make the rest.
		Call( ErrFILEIndexBatchTerm(
					ppib,
					rgpfucbSort,	
					pfcbSecondaryIndexes,
					cIndexesToBuild,
					rgcRecInput,
					pstatus ) );

		if ( pfcbNil != pfcbNextIndexBatch )
			{
			Assert( cFILEIndexBatchMax == cIndexesToBuild );
			Call( ErrFILEBuildAllIndexes(
							ppib,
							pfucbDest,
							pfcbNextIndexBatch,
							pstatus ) );
			}
		}
#endif		

HandleError:
	if ( pcsridCopied )
		*pcsridCopied = dsrid;
	if ( precidLast )
		*precidLast = 0xffffffff;

	if ( err < 0 && pfcbNil != pfcbSecondaryIndexes )
		{
#ifdef DEFRAG_SCAN_ONLY
#else
		for ( iindex = 0; iindex < cFILEIndexBatchMax; iindex++ )
			{
			if ( pfucbNil != rgpfucbSort[iindex] )	//lint !e644
				{
				SORTClose( rgpfucbSort[iindex] );
				rgpfucbSort[iindex] = pfucbNil;
				}
			}
#endif			
		
		// Ensure secondary indexes reattached.
		pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbSecondaryIndexes );
		}

	if ( fInTrx )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	// This gets allocated by CopyLVTree()
	if ( JET_tableidNil != lidmapDefrag.Tableid() )
		{
		CallS( lidmapDefrag.ErrTerm( ppib ) );
		Assert( JET_tableidNil == lidmapDefrag.Tableid() );
		}

	Assert( NULL != pvWorkBuf );
	Assert( pvWorkBuf == pfucbDest->pvWorkBuf
		|| ( err < 0 && NULL == pfucbDest->pvWorkBuf ) );	//	work buffer may have been deallocated on error
	Assert( pfucbDest->dataWorkBuf.Pv() == pfucbDest->pvWorkBuf );
	RECIFreeCopyBuffer( pfucbDest );

	Assert ( NULL != pbRecBuf );
	BFFree( pbRecBuf );
	
	if ( !fOldSeq )
		FUCBResetSequential( pfucbSrc );
		
	return err;
	}

ERR ISAMAPI ErrIsamCopyRecords(
	JET_SESID		sesid,
	JET_TABLEID		tableidSrc,
	JET_TABLEID		tableidDest,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	LONG			crecMax,
	ULONG			*pcsridCopied,
	ULONG			*precidLast,
	BYTE			*pbLVBuf,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	STATUSINFO		*pstatus )
	{
	ERR				err;
	PIB				*ppib;
	FUCB			*pfucbSrc;
	FUCB			*pfucbDest;
	
	ppib = reinterpret_cast<PIB *>( sesid );
	pfucbSrc = reinterpret_cast<FUCB *>( tableidSrc );
	pfucbDest = reinterpret_cast<FUCB *>( tableidDest );
		
	/*	ensure tableidSrc and tableidDest are system ISAM
	/**/
	Assert( pfucbSrc->pvtfndef == (VTFNDEF *)&vtfndefIsam
			|| pfucbSrc->pvtfndef == (VTFNDEF *)&vtfndefTTBase );
	Assert( pfucbDest->pvtfndef == (VTFNDEF *)&vtfndefIsam
			|| pfucbDest->pvtfndef == (VTFNDEF *)&vtfndefTTBase );

	err = ErrSORTCopyRecords(
				ppib,
				pfucbSrc,
				pfucbDest,
				rgcpcol,
				ccpcolMax,
				crecMax,
				pcsridCopied,
				precidLast,
				pbLVBuf,
				mpcolumnidcolumnidTagged,
				pstatus );

	return err;
	}


//	UNDONE:  use GetTempFileName()
INLINE ULONG ulSORTTempNameGen( VOID )
	{
	static ULONG ulTempNum = 0;
	return ulTempNum++;
	}

LOCAL ERR ErrIsamSortMaterialize( PIB * const ppib, FUCB * const pfucbSort, const BOOL fIndex )
	{
	ERR		err;
	FUCB   	*pfucbTable			= pfucbNil;
	FCB		*pfcbTable			= pfcbNil;
	TDB		*ptdbTable			= ptdbNil;
	FCB		*pfcbSort			= pfcbNil;
	TDB		*ptdbSort			= ptdbNil;
	MEMPOOL	mempoolSave;
	CHAR   	szName[JET_cbNameMost + 1];
	BOOL	fBeginTransaction	= fFalse;
	JET_TABLECREATE2 tablecreate	= {
		sizeof(JET_TABLECREATE2),
		szName,
		NULL,
	   	16, 100, 			// Pages and density
	   	NULL, 0, NULL, 0, NULL, 0,	// Columns and indexes and callbacks
	   	NO_GRBIT,			// grbit
	   	0,					// returned tableid
	   	0 };				// returned count of objects created

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucbSort );

	Assert( ppib->level < levelMax );
	Assert( pfucbSort->ppib == ppib );
	Assert( !( FFUCBIndex( pfucbSort ) ) );

	/*	causes remaining runs to be flushed to disk
	/**/
	if ( FSCBInsert( pfucbSort->u.pscb ) )
		{
		// If fSCBInsert is set, must have been called from ErrIsamOpenTempTable(),
		// in which case there should be no records.
		Assert( 0 == pfucbSort->u.pscb->cRecords );
		CallR( ErrSORTEndInsert( pfucbSort ) );
		}


	/*	generate temporary file name
	/**/
	sprintf( szName, "TEMP%lu", ulSORTTempNameGen() );

	/*	create table
	/**/
	INST *pinst = PinstFromPpib( ppib );
	CallR( ErrFILECreateTable( ppib, pinst->m_mpdbidifmp[ dbidTemp ], &tablecreate ) );
	pfucbTable = (FUCB *)( tablecreate.tableid );
	Assert( pfucbNil != pfucbTable );
	/*	only one table created
	/**/
	Assert( tablecreate.cCreated == 1 );

	/*	move to DATA root
	/**/
	const INT	fDIRFlags = ( fDIRNoVersion|fDIRAppend );	// No versioning -- on error, entire FDP is freed
	DIRGotoRoot( pfucbTable );
	Call( ErrDIRInitAppend( pfucbTable ) );

	pfcbSort = &pfucbSort->u.pscb->fcb;
	Assert( pfcbSort->FTypeSort() );
	
	ptdbSort = pfcbSort->Ptdb();
	Assert( ptdbNil != ptdbSort );

	pfcbTable = pfucbTable->u.pfcb;
	Assert( pfcbTable->FTypeTemporaryTable() );

	ptdbTable = pfcbTable->Ptdb();
	Assert( ptdbNil != ptdbTable );

	err = ErrSORTFirst( pfucbSort );

	if ( fIndex )
		{
		while ( err >= 0 )
			{
			Call( ErrDIRAppend(
						pfucbTable, 
						pfucbSort->kdfCurr.key, 
						pfucbSort->kdfCurr.data,
						fDIRFlags ) );

			Assert( Pcsr( pfucbTable )->FLatched() );
				
			err = ErrSORTNext( pfucbSort );
			}
		}
	else
		{
		KEY		key;
		DBK		dbk = 0;
		BYTE  	rgb[4];

		key.prefix.Nullify();
		key.suffix.SetPv( rgb );
		key.suffix.SetCb( sizeof(DBK) );

		while ( err >= 0 )
			{
			rgb[0] = (BYTE)(dbk >> 24);
			rgb[1] = (BYTE)((dbk >> 16) & 0xff);
			rgb[2] = (BYTE)((dbk >> 8) & 0xff);
			rgb[3] = (BYTE)(dbk & 0xff);
			dbk++;

			Call( ErrDIRAppend(
				pfucbTable,
				key,
				pfucbSort->kdfCurr.data,
				fDIRFlags ) );
			err = ErrSORTNext( pfucbSort );
			}

		dbk++;			//	add one to set to next available dbk
		
		Assert( ptdbTable->DbkMost() == 0 );
		ptdbTable->InitDbkMost( dbk );			//	should not conflict with anyone, since we have exclusive use
		Assert( ptdbTable->DbkMost() == dbk );
		}

	Assert( err < 0 );
	if ( err != JET_errNoCurrentRecord )
		goto HandleError;

	Call( ErrDIRTermAppend( pfucbTable ) );
	
	//	convert sort cursor into table cursor by changing flags.
	//
	Assert( pfcbTable->PfcbNextIndex() == pfcbNil );
	Assert( rgfmp[ pfcbTable->Ifmp() ].Dbid() == dbidTemp );
	pfcbTable->SetCbDensityFree( 0 );

	// UNDONE: This strategy of swapping the sort and table FCB's is a real
	// hack with the potential to cause future problems.  I've already been
	// bitten several times because ulFCBFlags is forcefully cleared.
	// Is there a better way to do this?

	// UNDONE:	clean up flag reset
	//
	Assert( pfcbTable->FDomainDenyReadByUs( ppib ) );
	Assert( pfcbTable->FTypeTemporaryTable() );
	Assert( pfcbTable->FPrimaryIndex() );
	Assert( pfcbTable->FInitialized() );
	Assert( pfcbTable->ErrErrInit() == JET_errSuccess );
	Assert( pfcbTable->FInList() );

	pfcbTable->ResetFlags();

	pfcbTable->SetDomainDenyRead( ppib );
	pfcbTable->SetTypeTemporaryTable();
	pfcbTable->SetFixedDDL();
	pfcbTable->SetPrimaryIndex();
	pfcbTable->CreateComplete();	//	FInitialized() = fTrue, m_errInit = JET_errSuccess
	pfcbTable->SetInList();		// Already placed in list by FILEOpenTable()

	// Swap field info in the sort and table TDB's.  The TDB for the sort
	// will then be released when the sort is closed.
	Assert( ptdbSort != ptdbNil );
	Assert( ptdbSort->PfcbTemplateTable() == pfcbNil );
	Assert( ptdbSort->IbEndFixedColumns() >= ibRECStartFixedColumns );
	Assert( ptdbTable != ptdbNil );
	Assert( ptdbTable->FidFixedLast() == fidFixedLeast-1 );
	Assert( ptdbTable->FidVarLast() == fidVarLeast-1 );
	Assert( ptdbTable->FidTaggedLast() == fidTaggedLeast-1 );
	Assert( ptdbTable->IbEndFixedColumns() == ibRECStartFixedColumns );
	Assert( ptdbTable->FidVersion() == 0 );
	Assert( ptdbTable->FidAutoincrement() == 0 );
	Assert( pfieldNil == ptdbTable->PfieldsInitial() );
	Assert( ptdbTable->PfcbTemplateTable() == pfcbNil );

	//	copy FIELD structures into byte pool (note that
	//	although it would be dumb, it is theoretically
	//	possible to create a sort without any columns)
	//	UNDONE: it would be nicer to copy the FIELD
	//	structure to the temp. table's PfieldsInitial(),
	//	but can't modify the fidLastInitial constants
	//	anymore
	Assert( 0 == ptdbSort->CDynamicColumns() );
	Assert( 0 == ptdbTable->CInitialColumns() );
	Assert( 0 == ptdbTable->CDynamicColumns() );
	const ULONG		cCols	= ptdbSort->CInitialColumns();
	if ( cCols > 0 )
		{
		//	Add the FIELD structures to the sort's byte pool
		//	so that it will be a simple matter to swap byte
		//	pools between the sort and the table
		Call( ptdbSort->MemPool().ErrReplaceEntry(
					itagTDBFields,
					(BYTE *)ptdbSort->PfieldsInitial(),
					cCols * sizeof(FIELD) ) );
		}
	

	// Add the table name to the sort's byte pool so that it will be a simple
	// matter to swap byte pools between the sort and the table.
	Assert( ptdbSort->ItagTableName() == 0 );
	MEMPOOL::ITAG	itagNew;
	Call( ptdbSort->MemPool().ErrAddEntry(
				(BYTE *)szName,
				(ULONG)strlen( szName ) + 1,
				&itagNew ) );
	if ( fIndex
		|| pfcbSort->Pidb() != pidbNil )	// UNDONE: Temporarily add second clause to silence asserts. -- JL
		{
		Assert( pfcbSort->Pidb() != pidbNil );
		if ( pfcbSort->Pidb()->FIsRgidxsegInMempool() )
			{
			Assert( pfcbSort->Pidb()->ItagRgidxseg() == itagTDBTempTableIdxSeg );
			Assert( itagNew == itagTDBTempTableNameWithIdxSeg );
			}
		else
			{
			Assert( pfcbSort->Pidb()->Cidxseg() > 0 );
			Assert( itagNew == itagTDBTableName );
			}
		}
	else
		{
		Assert( pfcbSort->Pidb() == pidbNil );
		Assert( itagNew == itagTDBTableName );
		}
	ptdbSort->SetItagTableName( itagNew );

	// Try to compact the byte pool.  If it fails, don't worry about it.
	// It just means the byte pool may contain unused space.
	ptdbSort->MemPool().FCompact();

	// Since sort is about to be closed, everything can be cannibalised,
	// except the byte pool, which must be explicitly freed.
	mempoolSave = ptdbSort->MemPool();
	ptdbSort->SetMemPool( ptdbTable->MemPool() );

	// WARNING: From this point on pfcbTable will be irrevocably
	// cannibalised before being transferred to the sort cursor.
	// Thus, we must ensure success all the way up to the
	// DIRClose( pfucbTable ), because we will no longer be
	// able to close the table properly via FILECloseTable()
	// in HandleError.
	
	ptdbTable->SetMemPool( mempoolSave );
	ptdbTable->MaterializeFids( ptdbSort );
	ptdbTable->SetItagTableName( ptdbSort->ItagTableName() );

	//	shouldn't have a default record, but propagate just to be safe
	Assert( NULL == ptdbSort->PdataDefaultRecord() );
	Assert( NULL == ptdbTable->PdataDefaultRecord() );
	ptdbTable->SetPdataDefaultRecord( ptdbSort->PdataDefaultRecord() );
	ptdbSort->SetPdataDefaultRecord( NULL );

	//	switch sort and table IDB
	Assert( pfcbTable->Pidb() == pidbNil );
	if ( fIndex )
		{
		Assert( pfcbSort->Pidb() != pidbNil );
		Assert( pfcbSort->Pidb()->ItagIndexName() == 0 );	// Sort and temp table indexes have no name.
		pfcbTable->SetPidb( pfcbSort->Pidb() );
		pfcbSort->SetPidb( pidbNil );
		}

	//	convert sort cursor flags to table flags
	//
	Assert( rgfmp[ pfucbSort->ifmp ].Dbid() == dbidTemp );
	Assert( pfucbSort->pfucbCurIndex == pfucbNil );
	FUCBSetIndex( pfucbSort );
	FUCBResetSort( pfucbSort );

	// release SCB, upgrade sort cursor to table cursor,
	// then close original table cursor
	//
	Assert( pfucbSort->u.pscb->fcb.WRefCount() == 1 );
	Assert( pfucbSort->u.pscb->fcb.Pfucb() == pfucbSort );
	if ( pfucbSort->u.pscb->fcb.PgnoFDP() != pgnoNull )
		SORTICloseRun( ppib, pfucbSort->u.pscb );
	SORTClosePscb( pfucbSort->u.pscb );
	pfcbTable->Link( pfucbSort );
	DIRBeforeFirst( pfucbSort );

	CheckTable( ppib, pfucbTable );
	Assert( pfucbTable->pvtfndef == &vtfndefInvalidTableid );
	Assert( !FFUCBUpdatePrepared( pfucbTable ) );
	Assert( NULL == pfucbTable->pvWorkBuf );
	FUCBAssertNoSearchKey( pfucbTable );
	Assert( pfucbNil == pfucbTable->pfucbCurIndex );
	Assert( pfucbTable->u.pfcb->FTypeTemporaryTable() );
	DIRClose( pfucbTable );
	pfucbTable = pfucbNil;

	// WARNING: Materialisation complete.  Sort has been transformed
	// into a temp table.  Must return success so dispatch table will
	// be updated accordingly.
	pfucbSort->pvtfndef = &vtfndefTTBase;
	return JET_errSuccess;


HandleError:
	if ( pfucbNil != pfucbTable )
		{
		// On error, release temporary table.
		Assert( err < JET_errSuccess );
		Assert( pfucbTable->u.pfcb->FTypeTemporaryTable() );
		CallS( ErrFILECloseTable( ppib, pfucbTable ) );
		}
		
	return err;
	}


#pragma warning(disable:4028 4030)	//  parameter mismatch errors

#ifndef DEBUG
#define ErrIsamSortRetrieveKey		ErrIsamRetrieveKey
#endif

#ifdef DB_DISPATCHING
extern VDBFNCapability				ErrIsamCapability;
extern VDBFNCloseDatabase			ErrIsamCloseDatabase;
extern VDBFNCreateObject			ErrIsamCreateObject;
extern VDBFNCreateTable 			ErrIsamCreateTable;
extern VDBFNDeleteObject			ErrIsamDeleteObject;
extern VDBFNDeleteTable 			ErrIsamDeleteTable;
extern VDBFNGetColumnInfo			ErrIsamGetColumnInfo;
extern VDBFNGetDatabaseInfo 		ErrIsamGetDatabaseInfo;
extern VDBFNGetIndexInfo			ErrIsamGetIndexInfo;
extern VDBFNGetObjectInfo			ErrIsamGetObjectInfo;
extern VDBFNOpenTable				ErrIsamOpenTable;
extern VDBFNRenameTable 			ErrIsamRenameTable;
extern VDBFNGetObjidFromName		ErrIsamGetObjidFromName;
extern VDBFNRenameObject			ErrIsamRenameObject;


CODECONST(VDBFNDEF) vdbfndefIsam =
	{
	sizeof(VDBFNDEF),
	0,
	NULL,
	ErrIsamCapability,
	ErrIsamCloseDatabase,
	ErrIsamCreateObject,
	ErrIsamCreateTable,
	ErrIsamDeleteObject,
	ErrIsamDeleteTable,
	ErrIllegalExecuteSql,
	ErrIsamGetColumnInfo,
	ErrIsamGetDatabaseInfo,
	ErrIsamGetIndexInfo,
	ErrIsamGetObjectInfo,
	ErrIsamOpenTable,
	ErrIsamRenameObject,
	ErrIsamRenameTable,
	ErrIsamGetObjidFromName,
	};
#endif


extern VTFNAddColumn				ErrIsamAddColumn;
extern VTFNCloseTable				ErrIsamCloseTable;
extern VTFNComputeStats 			ErrIsamComputeStats;
extern VTFNCreateIndex				ErrIsamCreateIndex;
extern VTFNDelete					ErrIsamDelete;
extern VTFNDeleteColumn 			ErrIsamDeleteColumn;
extern VTFNDeleteIndex				ErrIsamDeleteIndex;
extern VTFNDupCursor				ErrIsamDupCursor;
extern VTFNEscrowUpdate		  		ErrIsamEscrowUpdate;
extern VTFNGetBookmark				ErrIsamGetBookmark;
extern VTFNGetIndexBookmark			ErrIsamGetIndexBookmark;
extern VTFNGetChecksum				ErrIsamGetChecksum;
extern VTFNGetCurrentIndex			ErrIsamGetCurrentIndex;
extern VTFNGetCursorInfo			ErrIsamGetCursorInfo;
extern VTFNGetRecordPosition		ErrIsamGetRecordPosition;
extern VTFNGetTableColumnInfo		ErrIsamGetTableColumnInfo;
extern VTFNGetTableIndexInfo		ErrIsamGetTableIndexInfo;
extern VTFNGetTableInfo 			ErrIsamGetTableInfo;
extern VTFNGotoBookmark 			ErrIsamGotoBookmark;
extern VTFNGotoIndexBookmark		ErrIsamGotoIndexBookmark;
extern VTFNGotoPosition 			ErrIsamGotoPosition;
extern VTFNMakeKey					ErrIsamMakeKey;
extern VTFNMove 					ErrIsamMove;
extern VTFNPrepareUpdate			ErrIsamPrepareUpdate;
extern VTFNRenameColumn 			ErrIsamRenameColumn;
extern VTFNRenameIndex				ErrIsamRenameIndex;
extern VTFNRetrieveColumn			ErrIsamRetrieveColumn;
extern VTFNRetrieveColumns			ErrIsamRetrieveColumns;
extern VTFNRetrieveKey				ErrIsamRetrieveKey;
extern VTFNSeek 					ErrIsamSeek;
extern VTFNSeek 					ErrIsamSortSeek;
extern VTFNSetCurrentIndex			ErrIsamSetCurrentIndex;
extern VTFNSetColumn				ErrIsamSetColumn;
extern VTFNSetColumns				ErrIsamSetColumns;
extern VTFNSetIndexRange			ErrIsamSetIndexRange;
extern VTFNSetIndexRange			ErrIsamSortSetIndexRange;
extern VTFNUpdate					ErrIsamUpdate;
extern VTFNGetLock 					ErrIsamGetLock;
extern VTFNEnumerateColumns			ErrIsamEnumerateColumns;

extern VTFNDupCursor				ErrIsamSortDupCursor;
extern VTFNGetTableInfo		 		ErrIsamSortGetTableInfo;
extern VTFNCloseTable				ErrIsamSortClose;
extern VTFNMove 					ErrIsamSortMove;
extern VTFNGetBookmark				ErrIsamSortGetBookmark;
extern VTFNGotoBookmark 			ErrIsamSortGotoBookmark;
extern VTFNRetrieveKey				ErrIsamSortRetrieveKey;
extern VTFNUpdate					ErrIsamSortUpdate;

extern VTFNDupCursor				ErrTTSortRetDupCursor;

extern VTFNDupCursor				ErrTTBaseDupCursor;
extern VTFNMove 					ErrTTSortInsMove;
extern VTFNSeek 					ErrTTSortInsSeek;


CODECONST(VTFNDEF) vtfndefIsam =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIsamAddColumn,
	ErrIsamCloseTable,
	ErrIsamComputeStats,
	ErrIsamCreateIndex,
	ErrIsamDelete,
	ErrIsamDeleteColumn,
	ErrIsamDeleteIndex,
	ErrIsamDupCursor,
	ErrIsamEscrowUpdate,
	ErrIsamGetBookmark,
	ErrIsamGetIndexBookmark,
	ErrIsamGetChecksum,
	ErrIsamGetCurrentIndex,
	ErrIsamGetCursorInfo,
	ErrIsamGetRecordPosition,
	ErrIsamGetTableColumnInfo,
	ErrIsamGetTableIndexInfo,
	ErrIsamGetTableInfo,
	ErrIsamGotoBookmark,
	ErrIsamGotoIndexBookmark,
	ErrIsamGotoPosition,
	ErrIsamMakeKey,
	ErrIsamMove,
	ErrIsamPrepareUpdate,
	ErrIsamRenameColumn,
	ErrIsamRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamRetrieveKey,
	ErrIsamSeek,
	ErrIsamSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIsamSetIndexRange,
	ErrIsamUpdate,
	ErrIsamGetLock,
	ErrIsamRegisterCallback,
	ErrIsamUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIsamIndexRecordCount,
	ErrIsamRetrieveTaggedColumnList,
	ErrIsamSetSequential,
	ErrIsamResetSequential,
	ErrIsamEnumerateColumns
	};


CODECONST(VTFNDEF) vtfndefTTSortIns =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,			// WARNING: Must be same as vtfndefTTSortClose
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrIllegalDupCursor,
	ErrIsamEscrowUpdate,
	ErrIllegalGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIllegalGetTableInfo,
	ErrIllegalGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrTTSortInsMove,
	ErrIsamPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIllegalRetrieveColumn,
	ErrIllegalRetrieveColumns,
	ErrIsamSortRetrieveKey,
	ErrTTSortInsSeek,
	ErrIllegalSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIllegalSetIndexRange,
	ErrIsamSortUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIllegalSetLS,
	ErrIllegalGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIllegalEnumerateColumns
	};


CODECONST(VTFNDEF) vtfndefTTSortRet =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,			// WARNING: Must be same as vtfndefTTSortClose
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrTTSortRetDupCursor,
	ErrIllegalEscrowUpdate,
	ErrIsamSortGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIsamSortGetTableInfo,
	ErrIsamSortGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrIsamSortMove,
	ErrIllegalPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamSortRetrieveKey,
	ErrIsamSortSeek,
	ErrIllegalSetCurrentIndex,
	ErrIllegalSetColumn,
	ErrIllegalSetColumns,
	ErrIsamSortSetIndexRange,
	ErrIllegalUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIsamEnumerateColumns
	};

// for temp table (ie. a sort that's been materialized)
CODECONST(VTFNDEF) vtfndefTTBase =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIsamDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrTTBaseDupCursor,
	ErrIsamEscrowUpdate,
	ErrIsamGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIsamGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIsamGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIsamSortGetTableInfo,
	ErrIsamGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrIsamMove,
	ErrIsamPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamRetrieveKey,
	ErrIsamSeek,
	ErrIllegalSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIsamSetIndexRange,
	ErrIsamUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIsamEnumerateColumns
	};

// Inconsistent sort -- must be closed.
LOCAL CODECONST(VTFNDEF) vtfndefTTSortClose =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrIllegalDupCursor,
	ErrIllegalEscrowUpdate,
	ErrIllegalGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIllegalGetTableInfo,
	ErrIllegalGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIllegalMakeKey,
	ErrIllegalMove,
	ErrIllegalPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIllegalRetrieveColumn,
	ErrIllegalRetrieveColumns,
	ErrIllegalRetrieveKey,
	ErrIllegalSeek,
	ErrIllegalSetCurrentIndex,
	ErrIllegalSetColumn,
	ErrIllegalSetColumns,
	ErrIllegalSetIndexRange,
	ErrIllegalUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIllegalSetLS,
	ErrIllegalGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIllegalEnumerateColumns
	};


/*=================================================================
// ErrIsamOpenTempTable
//
// Description:
//
//	Returns a tableid for a temporary (lightweight) table.	The data
//	definitions for the table are specified at open time.
//
// Parameters:
//	JET_SESID			sesid				user session id
//	JET_TABLEID			*ptableid			new JET (dispatchable) tableid
//	ULONG				csinfo				count of JET_COLUMNDEF structures
//											(==number of columns in table)
//	JET_COLUMNDEF		*rgcolumndef		An array of column and key defintions
//											Note that TT's do require that a key be
//											defined. (see jet.h for JET_COLUMNDEF)
//	JET_GRBIT			grbit				valid values
//											JET_bitTTUpdatable (for insert and update)
//											JET_bitTTScrollable (for movement other then movenext)
//
// Return Value:
//	err			jet error code or JET_errSuccess.
//	*ptableid	a dispatchable tableid
//
// Errors/Warnings:
//
// Side Effects:
//
=================================================================*/
ERR VDBAPI ErrIsamOpenTempTable(
	JET_SESID				sesid,
	const JET_COLUMNDEF		*rgcolumndef,
	ULONG					ccolumndef,
	JET_UNICODEINDEX		*pidxunicode,
	JET_GRBIT				grbit,
	JET_TABLEID				*ptableid,
	JET_COLUMNID			*rgcolumnid )
	{
	ERR						err;
	FUCB					*pfucb;

	Assert( ptableid );
	*ptableid = JET_tableidNil;

	CallR( ErrIsamSortOpen(
				(PIB *)sesid,
				(JET_COLUMNDEF *)rgcolumndef,
				ccolumndef,
				pidxunicode,
				grbit,
				&pfucb,
				rgcolumnid ) );
	Assert( pfucbNil != pfucb );
	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );
	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );

	BOOL	fIndexed = fFalse;
	BOOL	fLongValues = fFalse;
	for ( UINT iColumndef = 0; iColumndef < (INT)ccolumndef; iColumndef++ )
		{
		fIndexed |= ( ( rgcolumndef[iColumndef].grbit & JET_bitColumnTTKey ) != 0);
		fLongValues |= FRECLongValue( rgcolumndef[iColumndef].coltyp ) | FRECSLV( rgcolumndef[iColumndef].coltyp );
		}

	//	if no index, force materialisation to avoid unnecessarily sorting
	//	if long values exist, must materialise because sorts don't support LV's
	//	if user wants error to be returned on insertion of duplicates, then must
	//		must materialise because sorts remove dupes silently
	if ( !fIndexed || fLongValues || ( grbit & JET_bitTTForceMaterialization ) )
		{
		err = ErrIsamSortMaterialize( (PIB *)sesid, pfucb, fIndexed );
		Assert( err <= 0 );
		if ( err < 0 )
			{
			CallS( ErrIsamSortClose( sesid, (JET_VTID)pfucb ) );
			return err;
			}
		}

	*ptableid = (JET_TABLEID)pfucb;
	return err;
	}


ERR ErrTTEndInsert( JET_SESID sesid, JET_TABLEID tableid, BOOL *pfMovedToFirst )
	{
	ERR				err;
	FUCB			*pfucb			= (FUCB *)tableid;
	BOOL			fOverflow;
	BOOL			fMaterialize;
	const JET_GRBIT	grbitOpen		= pfucb->u.pscb->grbit;

	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );

	Assert( pfMovedToFirst );
	*pfMovedToFirst = fFalse;

	Call( ErrSORTEndInsert( pfucb ) );

	fOverflow = ( JET_wrnSortOverflow == err );
	Assert( JET_errSuccess == err || fOverflow );
	
	fMaterialize = ( grbitOpen & JET_bitTTUpdatable )
					|| ( ( grbitOpen & ( JET_bitTTScrollable|JET_bitTTIndexed ) )
						&& fOverflow );
	if ( fMaterialize )
		{
		Call( ErrIsamSortMaterialize( (PIB *)sesid, pfucb, ( grbitOpen & JET_bitTTIndexed ) != 0 ) );
		Assert( JET_errSuccess == err );
		pfucb->pvtfndef = &vtfndefTTBase;
		}
	else
		{
		// In case we have runs, we must call SORTFirst() to
		// start last merge and get first record
		err = ErrSORTFirst( pfucb );
		Assert( err <= JET_errSuccess );
		if ( err < 0 )
			{
			if ( JET_errNoCurrentRecord != err )
				goto HandleError;
			}
		
		*pfMovedToFirst = fTrue;
		pfucb->pvtfndef = &vtfndefTTSortRet;
		}

	Assert( JET_errSuccess == err
		|| ( JET_errNoCurrentRecord == err && *pfMovedToFirst ) );
	return err;

HandleError:
	Assert( err < 0 );
	Assert( JET_errNoCurrentRecord != err );
	
	// On failure, sort is no longer consistent.  It must be
	// invalidated.  The only legal operation left is to close it.
	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );
	pfucb->pvtfndef = &vtfndefTTSortClose;
		
	return err;
	}


/*=================================================================
// ErrTTSortInsMove
//
//	Functionally the same as JetMove().  This routine traps the first
//	move call on a TT, to perform any necessary transformations.
//	Routine should only be used by ttapi.c via disp.asm.
//
//	May cause a sort to be materialized
=================================================================*/
ERR VTAPI ErrTTSortInsMove( JET_SESID sesid, JET_TABLEID tableid, long crow, JET_GRBIT grbit )
	{
	ERR		err;
	BOOL	fMovedToFirst;

	if ( FFUCBUpdatePrepared( (FUCB *)tableid ) )
		{
		CallR( ErrIsamPrepareUpdate( (PIB *)sesid, (FUCB *)tableid, JET_prepCancel ) );
		}

	err = ErrTTEndInsert( sesid, tableid, &fMovedToFirst );
	Assert( JET_errNoCurrentRecord != err || fMovedToFirst );
	CallR( err );
	Assert( JET_errSuccess == err );

	if ( fMovedToFirst )
		{
		// May have already moved to first record if we had an on-disk sort
		// that wasn't materialised (because it's not updatable or
		// backwards-scrollable).
		if ( crow > 0 && crow < JET_MoveLast )
			crow--;
			
		if ( JET_MoveFirst == crow || 0 == crow )
			return JET_errSuccess;
		}

	err = ErrDispMove( sesid, tableid, crow, grbit );
	return err;
	}


/*=================================================================
// ErrTTSortInsSeek
//
//	Functionally the same as JetSeek().  This routine traps the first
//	seek call on a TT, to perform any necessary transformations.
//	Routine should only be used by ttapi.c via disp.asm.
//
//	May cause a sort to be materialized
=================================================================*/
ERR VTAPI ErrTTSortInsSeek( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	ERR		err;
	BOOL	fMovedToFirst;
	
	if ( FFUCBUpdatePrepared( (FUCB *)tableid ) )
		{
		CallR( ErrIsamPrepareUpdate( (PIB *)sesid, (FUCB *)tableid, JET_prepCancel ) );
		}

	err = ErrTTEndInsert( sesid, tableid, &fMovedToFirst );
	Assert( err <= JET_errSuccess );
	if ( err < 0 )
		{
		if ( JET_errNoCurrentRecord == err )
			{
			Assert( fMovedToFirst );
			err = ErrERRCheck( JET_errRecordNotFound );
			}
		}
	else
		{
		err = ErrDispSeek( sesid, tableid, grbit );
		}

	Assert( JET_errNoCurrentRecord != err );
	return err;
	}


ERR VTAPI ErrTTSortRetDupCursor( JET_SESID sesid, JET_TABLEID tableid, JET_TABLEID *ptableidDup, JET_GRBIT grbit )
	{
	ERR err = ErrIsamSortDupCursor( sesid, tableid, ptableidDup, grbit );
	if ( err >= 0 )
		{
		*(const VTFNDEF **)(*ptableidDup) = &vtfndefTTSortRet;
		}

	return err;
	}


ERR VTAPI ErrTTBaseDupCursor( JET_SESID sesid, JET_TABLEID tableid, JET_TABLEID *ptableidDup, JET_GRBIT grbit )
	{
	ERR err = ErrIsamSortDupCursor( sesid, tableid, ptableidDup, grbit );
	if ( err >= 0 )
		{
		*(const VTFNDEF **)(*ptableidDup) = &vtfndefTTBase;
		}

	return err;
	}




//++++++++++++++++++++++++++++++++++++++++++++++++++++++
//	Special hack - copy LV a tree of a table.
//++++++++++++++++++++++++++++++++++++++++++++++++++++++

ERR	ErrCMPCopyLVTree(
	PIB			*ppib,
	FUCB		*pfucbSrc,
	FUCB		*pfucbDest,
	BYTE		*pbBuf,
	ULONG		cbBufSize,
	STATUSINFO	*pstatus )
	{
	ERR			err;
	JET_TABLEID	tableidLIDMap	= JET_tableidNil;
	FUCB		*pfucbGetLV		= pfucbNil;
	FUCB		*pfucbCopyLV	= pfucbNil;
	LID			lidSrc			= 0;
	LVROOT		lvroot;
	DATA		dataNull;
	PGNO		pgnoLastLV		= pgnoNull;

	dataNull.Nullify();

	//	for efficiency, ensure buffer is a multiple of chunk size
	Assert( cbBufSize % g_cbColumnLVChunkMost == 0 );

	Assert( JET_tableidNil == lidmapDefrag.Tableid() );

	err =  ErrCMPGetSLongFieldFirst(
					pfucbSrc,
					&pfucbGetLV,
					&lidSrc,
					&lvroot );
	if ( err < 0 )
		{
		Assert( pfucbNil == pfucbGetLV );
		if ( JET_errRecordNotFound == err )
			err = JET_errSuccess;
		return err;
		}

	Assert( pfucbNil != pfucbGetLV );

	Call( lidmapDefrag.ErrLIDMAPInit( ppib ) );
	Assert( JET_tableidNil != lidmapDefrag.Tableid() );

	do
		{
		LID		lidDest;
		ULONG	ibLongValue = 0;
			
		Assert( pgnoNull !=  Pcsr( pfucbGetLV )->Pgno() );

		// Create destination LV with same refcount as source LV.  Update
		// later if correction is needed.
		Assert( lvroot.ulReference > 0 );
		Assert( pfucbNil == pfucbCopyLV );
		Call( ErrRECSeparateLV(
					pfucbDest,
					&dataNull,
					&lidDest,
					&pfucbCopyLV,
					&lvroot ) );
		Assert( pfucbNil != pfucbCopyLV );
		
		do {
			ULONG cbReturned;

			// On each iteration, retrieve as many LV chunks as can fit
			// into our LV buffer.  For this to work optimally,
			// ibLongValue must always point to the beginning of a chunk.
			Assert( ibLongValue % g_cbColumnLVChunkMost == 0 );
			Call( ErrCMPRetrieveSLongFieldValueByChunks(
						pfucbGetLV,		// pfucb must start on a LVROOT node
						lidSrc,
						lvroot.ulSize,
						ibLongValue,
						pbBuf,
						cbBufSize,
						&cbReturned ) );
			
			Assert( cbReturned > 0 );
			Assert( cbReturned <= cbBufSize );

#ifdef DEFRAG_SCAN_ONLY
#else
			Call( ErrRECAppendLVChunks(
						pfucbCopyLV,
						lidDest,
						ibLongValue,
						pbBuf,
						cbReturned ) );
#endif					

			Assert( err != JET_wrnCopyLongValue );

			ibLongValue += cbReturned;					// Prepare for next chunk.
			}
		while ( lvroot.ulSize > ibLongValue );

		Assert( pfucbNil != pfucbCopyLV );
		DIRClose( pfucbCopyLV );
		pfucbCopyLV = pfucbNil;

		Assert( lvroot.ulSize == ibLongValue );
		
		// insert src LID and dest LID into the global LID map table
		Call( lidmapDefrag.ErrInsert( ppib, lidSrc, lidDest, lvroot.ulReference ) );

		Assert( pgnoNull !=  Pcsr( pfucbGetLV )->Pgno() );
		
		if ( pstatus != NULL )
			{
			ULONG	cLVPagesTraversed;
			
			if ( lvroot.ulSize > g_cbColumnLVChunkMost )
				{
				Assert( Pcsr( pfucbGetLV )->Pgno() != pgnoLastLV );
				cLVPagesTraversed = lvroot.ulSize / g_cbColumnLVChunkMost;
				}
			else if ( Pcsr( pfucbGetLV )->Pgno() != pgnoLastLV )
				{
				cLVPagesTraversed = 1;
				}
			else
				{
				cLVPagesTraversed = 0;
				}

			pgnoLastLV = Pcsr( pfucbGetLV )->Pgno();

			pstatus->cbRawDataLV += lvroot.ulSize;
			pstatus->cLVPagesTraversed += cLVPagesTraversed;
			Call( ErrSORTCopyProgress( pstatus, cLVPagesTraversed ) );
			}
			
		err = ErrCMPGetSLongFieldNext( pfucbGetLV, &lidSrc, &lvroot );
		}
	while ( err >= JET_errSuccess );

	if ( JET_errNoCurrentRecord == err )
		err = JET_errSuccess;

HandleError:
	if ( pfucbNil != pfucbCopyLV )
		DIRClose( pfucbCopyLV );
		
	Assert( pfucbNil != pfucbGetLV );
	CMPGetSLongFieldClose( pfucbGetLV );

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\std.cxx ===
#include "std.hxx"
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\taskmgr.cxx ===
#include "std.hxx"


//	ctor

TASKMGR::TASKMGR()
	{
	m_cContext		= 0;
	m_rgdwContext	= NULL;

	m_fInit			= fFalse;
	}


//	dtor

TASKMGR::~TASKMGR()
	{
	}


//	initialize the TASKMGR

ERR TASKMGR::ErrInit( INST *const pinst, const INT cThread )
	{
	ERR err;

	//	verify input

	Assert( NULL != pinst );
	Assert( cThread > 0 );

	//	the task manager should not be initialized yet

	Assert( !m_fInit );

	//	verify the state of the TASKMGR

	Assert( 0 == m_cContext );
	Assert( NULL == m_rgdwContext );

	//	allocate an array of session handles (the per-thread contexts)

	Assert( sizeof( PIB * ) <= sizeof( DWORD_PTR ) );
	m_cContext = 0;
	m_rgdwContext = new DWORD_PTR[cThread];
	if ( NULL == m_rgdwContext )
		{
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
		}
	memset( m_rgdwContext, 0, sizeof( DWORD_PTR ) * cThread );

	//	open all sessions

	while ( m_cContext < cThread )
		{
		PIB *ppib;

		//	open a session

		Call( ErrPIBBeginSession( pinst, &ppib, procidNil, fFalse ) );
		ppib->grbitsCommitDefault = JET_bitCommitLazyFlush;
		ppib->SetFSystemCallback();

		//	bind it to the per-thread context array (AFTER the session has been successfully opened)

		m_rgdwContext[m_cContext++] = DWORD_PTR( ppib );
		}

	//	initialize the real task manager

	Call( m_tm.ErrTMInit( m_cContext, m_rgdwContext ) );

	//	turn the task manager "on"

	{
#ifdef DEBUG
	const BOOL fInitBefore =
#endif	//	DEBUG
	AtomicExchange( (long *)&m_fInit, fTrue );
	Assert( !fInitBefore );
	}

	return JET_errSuccess;
	
HandleError:

	//	cleanup

	CallS( ErrTerm() );

	return err;
	}


//	terminate the TASKMGR
//	NOTE: does not need to return an error-code (left-over from previous version)

ERR TASKMGR::ErrTerm()
	{
	ULONG iThread;

	if ( m_fInit )
		{

		//	turn the task manager "off"

#ifdef DEBUG
		const BOOL fInitBefore =
#endif	//	DEBUG
		AtomicExchange( (long *)&m_fInit, fFalse );
		Assert( fInitBefore );

		//	wait for everyone currently in the middle of posting a task to finish

		m_cmsPost.Partition();
		}

	//	term the real task manager

	m_tm.TMTerm();

	//	cleanup the per-thread contexts

	for ( iThread = 0; iThread < m_cContext; iThread++ )
		{
		Assert( NULL != m_rgdwContext );
		Assert( 0 != m_rgdwContext[iThread] );
		PIBEndSession( (PIB *)m_rgdwContext[iThread] );
		}
	if ( NULL != m_rgdwContext )
		{
		delete m_rgdwContext;
		m_rgdwContext = NULL;
		}
	m_cContext = 0;

	return JET_errSuccess;
	}


//	post a task

ERR TASKMGR::ErrPostTask( const TASK pfnTask, const ULONG_PTR ul )
	{
	ERR				err;
	TASKMGRCONTEXT	*ptmc;
	int				iGroup;

	//	allocate a private context

	ptmc = new TASKMGRCONTEXT;
	if ( NULL == ptmc )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	//	populate the context

	ptmc->pfnTask = pfnTask;
	ptmc->ul = ul;

	//	enter the task-post metered section

	iGroup = m_cmsPost.Enter();

	if ( m_fInit )
		{

		//	pass the task down to the real task manager (will be dispatched via TASKMGR::Dispatch)

		err = m_tm.ErrTMPost( CTaskManager::PfnCompletion( Dispatch ), 0, DWORD_PTR( ptmc ) );
		CallSx( err, JET_errOutOfMemory );	//	we should only see JET_errSuccess or JET_errOutOfMemory
		}
	else
		{

		//	the task manager is not initialized so the task must be dropped

		err = ErrERRCheck( JET_errTaskDropped );
		AssertTracking();	//	this shouldn't happen; trap it because caller may not handle this case well
		}

	//	leave the task-post metered section

	m_cmsPost.Leave( iGroup );

	if ( err < JET_errSuccess )
		{

		//	cleanup the task context

		delete ptmc;
		}

	return err;
	}


//	dispatch a task

VOID TASKMGR::Dispatch(	const DWORD_PTR dwThreadContext,
						const DWORD		dwCompletionKey1,
						const DWORD_PTR	dwCompletionKey2 )
	{
	PIB				*ppib;
	TASKMGRCONTEXT	*ptmc;
	TASK			pfnTask;
	ULONG_PTR		ul;

	//	verify input 

	Assert( 0 != dwThreadContext );
	Assert( 0 == dwCompletionKey1 );
	Assert( 0 != dwCompletionKey2 );

	//	extract the session

	ppib = (PIB *)dwThreadContext;

	//	extract the task's context and clean it up

	ptmc = (TASKMGRCONTEXT *)dwCompletionKey2;
	pfnTask = ptmc->pfnTask;
	ul = ptmc->ul;
	delete ptmc;

	//	run the task

	Assert( NULL != pfnTask );
	pfnTask( ppib, ul );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\sysinit.cxx ===
#include "std.hxx"

#include <ctype.h>
#include <io.h>

extern	INT 	itibGlobal;


#if defined( DEBUG ) || defined( PERFDUMP )
BOOL	fDBGPerfOutput = fFalse;
long	lAPICallLogLevel = 4;
#endif	/* DEBUG || PERFDUMP */

//	system parameter constants
//
long	g_lSessionsMax = cpibDefault;
long	g_lOpenTablesMax = cfcbDefault;
long	g_lOpenTablesPreferredMax = 0;
long	g_lTemporaryTablesMax = cscbDefault;
long	g_lCursorsMax = cfucbDefault;
long	g_lVerPagesMax = cbucketDefault;
long	g_lVerPagesMin = cbucketDefault;
long	g_lVerPagesPreferredMax = long( cbucketDefault * 0.9 );
long	g_lLogBuffers = csecLogBufferDefault;
long	g_lLogFileSize = csecLogFileSizeDefault;
BOOL	g_fSetLogFileSize = fFalse;
long	g_grbitsCommitDefault = NO_GRBIT;
long	g_lPageFragment = lPageFragmentDefault;
CHAR	g_szLogFilePath[cbFilenameMost] = ".\\";
CHAR	g_szLogFileFailoverPath[cbFilenameMost] = "";
BOOL	g_fLogFileCreateAsynch = fTrue;
BOOL	fDoNotOverWriteLogFilePath = fFalse;
CHAR	g_szRecovery[cbFilenameMost] = "on";
CHAR	g_szAlternateDbDirDuringRecovery[IFileSystemAPI::cchPathMax] = "";
BOOL	g_fAlternateDbDirDuringRecovery = fFalse;
#ifdef ESENT
BOOL	g_fDeleteOldLogs = fTrue;
#else
BOOL	g_fDeleteOldLogs = fFalse;
#endif
BOOL   g_fDeleteOutOfRangeLogs =fFalse;
BOOL	g_fImprovedSeekShortcut = fFalse;
BOOL	g_fSortedRetrieveColumns = fFalse;
LONG	g_cbEventHeapMax	= cbEventHeapMaxDefault;
LONG	g_cpgBackupChunk = cpgBackupChunkDefault;
LONG	g_cBackupRead = cBackupReadDefault;
BOOL	g_fLGCircularLogging = fFalse;

LONG	g_cpageTempDBMin				= cpageTempDBMinDefault;
BOOL	g_fTempTableVersioning			= fTrue;
BOOL	g_fScrubDB						= fFalse;

LONG	g_fGlobalOLDLevel				= JET_OnlineDefragAll;
LONG	g_lEventLoggingLevel			= JET_EventLoggingLevelMax;

ULONG	g_ulVERTasksPostMax				= cpibSystemFudge;

IDXUNICODE	idxunicodeDefault			= { lcidDefault, dwLCMapFlagsDefault };
ULONG	g_chIndexTuplesLengthMin		= chIDXTuplesLengthMinDefault;
ULONG	g_chIndexTuplesLengthMax		= chIDXTuplesLengthMaxDefault;
ULONG	g_chIndexTuplesToIndexMax		= chIDXTuplesToIndexMaxDefault;

JET_CALLBACK	g_pfnRuntimeCallback	= NULL;

CHAR	g_szSystemPath[IFileSystemAPI::cchPathMax]	= ".\\";

BOOL	g_fSLVProviderEnabled = fSLVProviderEnabledDefault;
wchar_t	g_wszSLVProviderRootPath[IFileSystemAPI::cchPathMax] = wszSLVProviderRootPathDefault;

LONG g_lSLVDefragFreeThreshold = lSLVDefragFreeThresholdDefault;  // chunks whose free % is >= this will be allocated from
LONG g_lSLVDefragMoveThreshold = lSLVDefragMoveThresholdDefault;  // chunks whose free % is <= this will be relocated

CHAR	g_szTempDatabase[IFileSystemAPI::cchPathMax]	= szDefaultTempDbFileName szDefaultTempDbExt;	// pathed filename
CHAR	szBaseName[16]				= "edb";
CHAR	szJet[16] 					= "edb";
CHAR	szJetLog[16] 				= "edb.log";
CHAR	szJetLogNameTemplate[16] 	= "edb00000";
CHAR	szJetTmp[16] 				= "edbtmp";
CHAR	szJetTmpLog[16]  			= "edbtmp.log";
CHAR	szJetTxt[16] 				= "edb.txt";

LONG g_cpgSESysMin = cpageDbExtensionDefault;

#ifdef DEBUG
BOOL	g_fCbPageSet = fFalse;
#endif // DEBUG

LONG	g_cbPage = g_cbPageDefault;
LONG	g_cbColumnLVChunkMost = g_cbPage - JET_cbColumnLVPageOverhead;
LONG 	g_cbLVBuf = 8 * g_cbColumnLVChunkMost;
INT		g_shfCbPage = 12;
INT		cbRECRecordMost = REC::CbRecordMax();
#ifdef INTRINSIC_LV
ULONG 	cbLVIntrinsicMost = cbRECRecordMost - sizeof(REC::cbRecordHeader) - sizeof(TAGFLD) - sizeof(TAGFLD_HEADER);
#endif // INTRINSIC_LV

BOOL 	g_fCallbacksDisabled = fFalse;

BOOL	g_fCreatePathIfNotExist = fFalse;

BOOL	g_fCleanupMismatchedLogFiles = fFalse;

LONG	g_cbPageHintCache = cbPageHintCacheDefault;

BOOL	g_fOneDatabasePerSession	= fFalse;

CHAR	g_szUnicodeIndexLibrary[ IFileSystemAPI::cchPathMax ] = "";

char const *szCheckpoint			= "Checkpoint";
char const *szCritCallbacks			= "Callbacks";
#if defined(DEBUG) && defined(MEM_CHECK)
char const *szCALGlobal				= "rgCAL";
#endif	//	DEBUG && MEM_CHECK
char const *szCritOLDSFS			= "INST::m_critOLDSFS";
char const *szLGBuf					= "LGBuf";
char const *szLGTrace				= "LGTrace";
char const *szLGResFiles			= "LGResFiles";
char const *szLGWaitQ				= "LGWaitQ";
char const *szLGFlush				= "LGFlush";
char const *szRES					= "RES";
char const *szPIBGlobal				= "PIBGlobal";
char const *szOLDTaskq				= "OLDTaskQueue";
char const *szFCBCreate				= "FCBCreate";
char const *szFCBList				= "FCBList";
char const *szFCBRCEList			= "FCBRCEList";
char const *szFMPSLVSpace			= "FMPSLVSpace";
char const *szFCBSXWL				= "FCB::m_sxwl";
char const *szFMPGlobal				= "FMPGlobal";
char const *szFMP					= "FMP";
char const *szFMPDetaching			= "FMPDetaching";
char const *szDBGPrint				= "DBGPrint";
char const *szRCEClean				= "RCEClean";
char const *szBucketGlobal			= "BucketGlobal";
char const *szRCEChain				= "RCEChain";
char const *szPIBTrx				= "PIBTrx";
char const *szPIBLogDeferBeginTrx	= "PIBLogDeferBeginTrx";
char const *szPIBCursors			= "PIBCursors";
char const *szVERPerf				= "VERPerf";
char const *szLVCreate				= "LVCreate";
char const *szTrxOldest				= "TrxOldest";
char const *szFILEUnverCol			= "FILEUnverCol";
char const *szFILEUnverIndex		= "FILEUnverIndex";
char const *szInstance				= "Instance";
char const *szAPI					= "API";
char const *szBegin0Commit0			= "Begin0/Commit0";
char const *szIndexingUpdating		= "Indexing/Updating";
char const *szDDLDML				= "DDL/DML";
char const *szTTMAP					= "TTMAP";
char const *szIntegGlobals			= "IntegGlobals";
char const *szRepairOpts			= "RepairOpts";
char const *szUpgradeOpts			= "UpgradeOpts";
char const *szCritSLVSPACENODECACHE = "SLVSPACENODECACHE";
char const *szBFDUI					= "BFDUI";
char const *szBFHash				= "BFHash";
char const *szBFLRUK				= "BFLRUK";
char const *szBFOB0					= "BFOB0";
char const *szBFFMPContext			= "BFFMPContext Updating/Accessing";
char const *szBFLatch				= "BFLatch Read/RDW/Write";
char const *szBFDepend				= "BFDepend";
char const *szBFParm				= "BFParm";
char const *szRestoreInstance		= "RestoreInstance";

//	these are actually also used in non-RTM for the Error Trap
const _TCHAR szDEBUGRoot[]			= _T( "DEBUG" );
#define DEBUG_ENV_VALUE_LEN			256


#ifdef DEBUG

BOOL fDBGPrintToStdOut;

_TCHAR* GetDebugEnvValue( _TCHAR* szEnvVar )
	{
	_TCHAR	szBufTemp[ DEBUG_ENV_VALUE_LEN ];

	if ( FOSConfigGet( szDEBUGRoot, szEnvVar, szBufTemp, DEBUG_ENV_VALUE_LEN ) )
		{
		if ( szBufTemp[0] )
			{
			// UNDONE  we don't really want to deal with an OutOfMemory
			// error at this point. Anyway it is debug only.
			
			RFSDisable();
			_TCHAR* szBuf = new _TCHAR[ _tcslen( szBufTemp ) + 1 ];
			RFSEnable();

			//	if we really are OutOfMemory we return NULL as the EnvVariable is not set
			//	we do the same also if the one set in Env too long so ...
			//	and probably we exit soon anyway as we are OutOfMemory
			if ( szBuf )
				{
				_tcscpy( szBuf, szBufTemp );
				}
			return szBuf;
			}
		else
			{
//			FOSConfigGet() will create the key if it doesn't exist
			}
		}
	else
		{
		//  UNDONE:  gripe in the event log that the value was too big
		}

	return NULL;
	}

VOID ITDBGSetConstants( INST * pinst )
	{
	CHAR		*sz;					

	if ( ( sz = GetDebugEnvValue( "Error Trap" ) ) != NULL )
		{
		extern ERR g_errTrap;
		g_errTrap = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue( "Redo Trap" ) ) != NULL )
		{
		ULONG lGeneration;
		ULONG isec;
		ULONG ib;
		sscanf(	sz,
				"%06X,%04X,%04X",
				&lGeneration,
				&isec,
				&ib );
				
		extern LGPOS g_lgposRedoTrap;
		g_lgposRedoTrap.lGeneration	= lGeneration;
		g_lgposRedoTrap.isec		= USHORT( isec );
		g_lgposRedoTrap.ib			= USHORT( ib );
		
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "PrintToStdOut" ) ) ) != NULL )
		{
		fDBGPrintToStdOut = fTrue;
		OSMemoryHeapFree( sz );
		}
	else
		{
		fDBGPrintToStdOut = fFalse;
		}

#ifdef PROFILE_JET_API
	if ( ( sz = GetDebugEnvValue ( _T( "JETProfileName" ) ) ) != NULL )
		{
		extern CHAR profile_szFileName[];
		strncpy( profile_szFileName, sz, IFileSystemAPI::cchPathMax );
		profile_szFileName[IFileSystemAPI::cchPathMax-1] = 0;
		OSMemoryHeapFree( sz );
		}
	if ( ( sz = GetDebugEnvValue ( _T( "JETProfileOptions" ) ) ) != NULL )
		{
		extern INT profile_detailLevel;
		profile_detailLevel = atoi( sz );
		OSMemoryHeapFree( sz );
		}
#endif // PROFILE_JET_API

	//	use system environment variables to overwrite the default.
	//	if the JETUSEENV is set.
	//
	if ( ( sz = GetDebugEnvValue ( _T( "JETUseEnv" ) ) ) == NULL )
		return;
	
	OSMemoryHeapFree( sz );

	// UNDONE: check what param can be set per instance and move the code before

	// only the previous params can be set with a runnign instance
	if ( NULL != pinst )
		{
		return;
		}
		
	if ( ( sz = GetDebugEnvValue ( _T( "JETEnableOnlineDefrag" ) ) ) != NULL )
		{
		g_fGlobalOLDLevel = atol( sz );
		OSMemoryHeapFree( sz );
		}
		 
	if ( ( sz = GetDebugEnvValue ( _T( "JETRecovery" ) ) ) != NULL )
		{
		if ( strlen( sz ) > sizeof(g_szRecovery) )
			{
			OSMemoryHeapFree( sz );
			return;
			}
		strcpy( g_szRecovery, sz );
		OSMemoryHeapFree( sz );
		}
		 
	if ( ( sz = GetDebugEnvValue ( _T( "JETLogFilePath" ) ) ) != NULL )
		{
		if ( strlen( sz ) > sizeof( g_szLogFilePath ) )
			{
			OSMemoryHeapFree( sz );
			return;
			}
		strcpy( g_szLogFilePath, sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETDbExtensionSize" ) ) ) != NULL )
		{
		g_cpgSESysMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETPageTempDBMin" ) ) ) != NULL )
		{
		g_cpageTempDBMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxSessions" ) ) ) != NULL )
		{
		g_lSessionsMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxOpenTables" ) ) ) != NULL )
		{
		g_lOpenTablesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxTemporaryTables" ) ) ) != NULL )
		{
		g_lTemporaryTablesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxCursors" ) ) ) != NULL )
		{
		g_lCursorsMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxVerPages" ) ) ) != NULL )
		{
		g_lVerPagesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETGlobalMinVerPages" ) ) ) != NULL )
		{
		g_lVerPagesMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogBuffers" ) ) ) != NULL )
		{
		g_lLogBuffers = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogFileSize" ) ) ) != NULL )
		{
		g_lLogFileSize = atol(sz);
		OSMemoryHeapFree(sz);
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogCircularLogging" ) ) ) != NULL )
		{
		g_fLGCircularLogging = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "PERFOUTPUT" ) ) ) != NULL )
		{
		fDBGPerfOutput = fTrue;
		OSMemoryHeapFree( sz );
		}
	else
		{
		fDBGPerfOutput = fFalse;
		}

	if ( ( sz = GetDebugEnvValue ( _T( "APICallLogLevel" ) ) ) != NULL )
		{
		lAPICallLogLevel = atol(sz);
		OSMemoryHeapFree(sz);
		}
	else
		lAPICallLogLevel = 4;
	}
#endif	//	DEBUG


ERR ErrITSetConstants( INST * pinst )
	{
#if defined( DEBUG )
	ITDBGSetConstants( pinst );
#elif defined( RTM )
	//	Error/Redo Trap is disabled in RTM builds
	return JET_errSuccess;
#else
	_TCHAR		szBuf[ DEBUG_ENV_VALUE_LEN ];
	
	if ( FOSConfigGet( szDEBUGRoot, _T( "Error Trap" ), szBuf, DEBUG_ENV_VALUE_LEN )
		&& 0 != szBuf[0] )
		{
		extern ERR g_errTrap;
		g_errTrap = atol( szBuf );
		}
#endif

	_TCHAR		szBuf2[ DEBUG_ENV_VALUE_LEN ];
	
	if ( FOSConfigGet( szDEBUGRoot, _T( "Redo Trap" ), szBuf2, DEBUG_ENV_VALUE_LEN )
		&& 0 != szBuf2[0] )
		{
		ULONG lGeneration;
		ULONG isec;
		ULONG ib;
		sscanf(	szBuf2,
				"%06X,%04X,%04X",
				&lGeneration,
				&isec,
				&ib );
				
		extern LGPOS g_lgposRedoTrap;
		g_lgposRedoTrap.lGeneration	= lGeneration;
		g_lgposRedoTrap.isec		= USHORT( isec );
		g_lgposRedoTrap.ib			= USHORT( ib );
		}
		
	//	initialize rgres.  system path in rgtib[itib].g_szSystemPath,
	//	is initialized by JET layer.
	//
//	rgres[iresFCB].cblockAlloc = g_lOpenTablesMax;
//	rgres[iresFUCB].cblockAlloc = g_lCursorsMax;
//	rgres[iresTDB].cblockAlloc = g_lOpenTablesMax + g_lTemporaryTablesMax;
//	rgres[iresIDB].cblockAlloc = g_lOpenTablesMax + g_lTemporaryTablesMax;

	//	each user session can begin another one for BMCleanBeforeSplit
	//
//	rgres[iresPIB].cblockAlloc = g_lSessionsMax + cpibSystem; 
//	rgres[iresSCB].cblockAlloc = g_lTemporaryTablesMax;
//	if ( g_plog->m_fRecovering )
//		{
//		rgres[iresVER].cblockAlloc =
//			(LONG) max( (ULONG) g_lVerPagesMax * 1.1, (ULONG) g_lVerPagesMax + 2 ) + cbucketSystem;
//		}
//	else
//		{
//		rgres[iresVER].cblockAlloc = g_lVerPagesMax + cbucketSystem;
//		}

	return JET_errSuccess;
	}


//+API
//	ErrINSTInit
//	========================================================
//	ERR ErrINSTInit( )
//
//	Initialize the storage system: page buffers, log buffers, and the
//	database files.
//
//	RETURNS		JET_errSuccess
//-
ERR INST::ErrINSTInit( )
	{
	ERR		err;
	PIB		*ppib			= ppibNil;
	INT		cSessions;
	IFMP	ifmp			= ifmpMax;
	BOOL	fFCBInitialized	= fFalse;

	LOG		*plog			= m_plog;
	VER		*pver			= m_pver;

	//	sleep while initialization is in progress
	//
	while ( m_fSTInit == fSTInitInProgress )
		{
		UtilSleep( 1000 );
		}

	//	serialize system initialization
	//
	if ( m_fSTInit == fSTInitDone )
		{
		return JET_errSuccess;
		}

	//	initialization in progress
	//
	m_fSTInit = fSTInitInProgress;

	//	initialize Global variables
	//
	Assert( m_ppibGlobal == ppibNil );

	// Set to FALSE (may have gotten set to TRUE by recovery).
	m_fTermAbruptly = fFalse;

	//	initialize subcomponents
	//
	Call( ErrIOInit( this ) );

	cSessions = m_lSessionsMax + cpibSystem;
	CallJ( ErrPIBInit( this, cSessions ), TermIO )

	// initialize FCB, TDB, and IDB.
	// each table can potentially use 2 FCBs (one for the table and one for the LV tree)
	// so we double the number of FCBs

	CallJ( FCB::ErrFCBInit( this, cSessions, m_lOpenTablesMax * 2, m_lTemporaryTablesMax, m_lOpenTablesPreferredMax * 2 ), TermPIB );
	fFCBInitialized = fTrue;

	// initialize SCB

	CallJ( ErrSCBInit( this, m_lTemporaryTablesMax ), TermPIB );

	CallJ( ErrFUCBInit( this, m_lCursorsMax ), TermSCB );

	//	begin storage level session to support all future system
	//	initialization activites that require a user for
	//	transaction control
	//
	if ( m_lTemporaryTablesMax > 0 && !FRecovering() )
		{
		//	temp db not needed during recovery
		
		CallJ( ErrPIBBeginSession( this, &ppib, procidNil, fTrue ), TermFUCB );

		//  open and set size of temp database
		//
		Assert( m_mpdbidifmp[ dbidTemp ] >= ifmpMax );


#ifdef TEMP_SLV
//	UNDONE: If a client needs a streaming file on the temp database,
//	add a system param to indicate that we should pass in JET_bitDbCreateStreamingFile
//	or possibly even the streaming file naae and root
		const ULONG		grbit	= JET_bitDbCreateStreamingFile
									| JET_bitDbRecoveryOff
									| ( m_fTempTableVersioning ? 0 : JET_bitDbVersioningOff );
#else
		const ULONG		grbit	= JET_bitDbRecoveryOff
									| ( m_fTempTableVersioning ? 0 : JET_bitDbVersioningOff );
#endif									

		CallJ( ErrDBCreateDatabase(
					ppib,
					NULL,
					m_szTempDatabase,
					NULL,
					NULL,
					0,
					&ifmp,
					dbidTemp,
					m_cpageTempDBMin,
					grbit,
					NULL ), ClosePIB );
			
		Assert( rgfmp[ ifmp ].Dbid() == dbidTemp );

		PIBEndSession( ppib );
		}
		
	//	begin backup session 
	//
	Assert( m_plog->m_ppibBackup == ppibNil );
	if ( !m_plog->m_fRecovering )
		{
		CallJ( ErrPIBBeginSession( this, &m_plog->m_ppibBackup, procidNil, fFalse ), ClosePIB );
		}

	err = m_taskmgr.ErrTMInit();
	//	Should be success all the time because it is just another reference to the global task manager
	Assert( JET_errSuccess == err );

	//	intialize version store
	
	if ( plog->m_fRecovering )
		{
		CallJ( pver->ErrVERInit( (INT) max( m_lVerPagesMax * 1.1, m_lVerPagesMax + 2 ) + cbucketSystem,
							m_lVerPagesMax + cbucketSystem,
							cSessions ), CloseBackupPIB );
		}
	else
		{
		CallJ( pver->ErrVERInit( m_lVerPagesMax + cbucketSystem,
							m_lVerPagesPreferredMax,
							cSessions ), CloseBackupPIB );
		}

	// initialize LV critical section

	CallJ( ErrLVInit( this ), TermVER );

	this->m_fSTInit = fSTInitDone;

/*	All services threads simply wait on a signal
	(there is no init involved), so shouldn't
	need this sleep any longer

	//	give theads a chance to initialize
	UtilSleep( 1000 );
*/
	return JET_errSuccess;

TermVER:

	m_pver->m_fSyncronousTasks = fTrue;	// BUGFIX(X5:124414): avoid an assert in VERTerm()
	m_pver->VERTerm( fFalse );	//	not normal

	m_taskmgr.TMTerm();

CloseBackupPIB:
	//	terminate backup session
	//
	if ( m_plog->m_ppibBackup != ppibNil )
		{
		PIBEndSession( m_plog->m_ppibBackup );
		m_plog->m_ppibBackup = ppibNil;
		}
		
ClosePIB:
	if ( ifmp != ifmpMax )
		{
		//	clean up buffers for this temp database
		
		Assert( rgfmp[ ifmp ].Dbid() == dbidTemp );
		BFPurge( ifmp );
		BFPurge( ifmp | ifmpSLV );
		}

	if ( ppib != ppibNil )
		PIBEndSession( ppib );

TermFUCB:
	FUCBTerm( this );

TermSCB:
	SCBTerm( this );

TermPIB:
	PIBTerm( this );

TermIO:
	(VOID)ErrIOTerm( this, m_pfsapi, fFalse );	//	not normal

	//	must defer FCB temination to the end because IOTerm() will clean up FCB's
	//	allocated for the temp. database
	if ( fFCBInitialized )
		FCB::Term( this );


HandleError:

	this->m_fSTInit = fSTInitNotDone;
	return err;
	}


//+api------------------------------------------------------
//
//	ErrINSTTerm
//	========================================================
//
//	ERR ErrITTerm( VOID )
//
//	Flush the page buffers to disk so that database file be in 
//	consistent state.  If error in RCCleanUp or in BFFlush, then DO NOT
//	terminate log, thereby forcing redo on next initialization.
//
//----------------------------------------------------------

ERR INST::ErrINSTTerm( TERMTYPE termtype )
	{
	ERR			err;
	ERR			errRet = JET_errSuccess;
	ULONG		icall = 0;

	Assert( m_plog->m_fRecovering || m_fTermInProgress || termtypeError == termtype );

	//	sleep while initialization is in progress
	//
	while ( m_fSTInit == fSTInitInProgress )
		{
		UtilSleep( 1000 );
		}

	//	make sure no other transactions in progress
	//
	//	if write error on page, RCCleanup will return -err
	//	if write error on buffer, BFFlush will return -err
	//	-err passed to LGTerm will cause correctness flag to
	//	be omitted from log thereby forcing recovery on next
	//	startup, and causing LGTerm to return +err, which
	//	may be used by JetQuit to show error
	//
	if ( m_fSTInit == fSTInitNotDone )
		return ErrERRCheck( JET_errNotInitialized );

// If no error (termtype != termError), then need to check if we can continue.
	
	if ( ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp ) && m_plog->m_fBackupInProgress )
		{
		return ErrERRCheck( JET_errBackupInProgress );
		}

	OLDTermInst( this );

	//	force the version store into synchronous-cleanup mode 
	//	(e.g. circumvent TASKMGR because it is about to go away)
	{
	ENTERCRITICALSECTION	enterCritRCEClean( &(m_pver->m_critRCEClean) );
	m_pver->m_fSyncronousTasks = fTrue;	
	}

	//  Cleanup all the tasks. Tasks will not be accepted by the TASKMGR
	//  until it is re-inited
	//  OLD has been stopped and the version store has been cleaned (as best as possible)
	//  so we shouldn't see any more tasks, unless we are about to fail below
	m_taskmgr.TMTerm();

	//	clean up all entries
	//
	err = m_pver->ErrVERRCEClean( );
	
	if ( termtype == termtypeCleanUp )
		{
		if ( err == JET_wrnRemainingVersions )
			{
			UtilSleep( 3000 );
			err = m_pver->ErrVERRCEClean();
			}
		if ( err < 0 )
			{
			termtype = termtypeError;
			if ( errRet >= 0 )
				{
				errRet = err;
				}
			}
		else
			{
			FCBAssertAllClean( this );
			}
		}
	else if ( JET_wrnRemainingVersions != err ) 
		{
		CallS( err );
		}

	//  fail if there are still active transactions and we are not doing a
	//  clean shutdown

	if ( ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp ) && TrxOldest( this ) != trxMax )
		{
		err = m_taskmgr.ErrTMInit();
		Assert( JET_errSuccess == err );
		return ErrERRCheck( JET_errTooManyActiveUsers );
		}
	
	//	Enter no-returning point. Once we kill one thread, we kill them all !!!!
	//
	m_fSTInit = fSTInitNotDone;

	m_fTermAbruptly = ( termtype == termtypeNoCleanUp || termtype == termtypeError );

	//	terminate backup session
	//
	if ( m_plog->m_ppibBackup != ppibNil )
		{
		PIBEndSession( m_plog->m_ppibBackup );
		}
	m_plog->m_ppibBackup = ppibNil;
	
	//  close LV critical section and remove session
	//  do not try to insert long values after calling this
	//
	LVTerm( this );

	//	flush and purge all buffers
	//
	if ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp )
		{
		for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( m_mpdbidifmp[ dbid ] < ifmpMax )
				{
				err = ErrBFFlush( m_mpdbidifmp[ dbid ] );
				if ( err < 0 )
					{
					termtype = termtypeError;
					m_fTermAbruptly = fTrue;
					if ( errRet >= 0 )
						errRet = err;
					}
				err = ErrBFFlush( m_mpdbidifmp[ dbid ] | ifmpSLV );
				if ( err < 0 )
					{
					termtype = termtypeError;
					m_fTermAbruptly = fTrue;
					if ( errRet >= 0 )
						errRet = err;
					}
				}
			}
		}
	for ( DBID dbid = dbidMin; dbid < dbidMax; dbid++ )
		{
		if ( m_mpdbidifmp[ dbid ] < ifmpMax )
			{
			BFPurge( m_mpdbidifmp[ dbid ] );
			BFPurge( m_mpdbidifmp[ dbid ] | ifmpSLV );
			}
		}

	// terminate the version store only after buffer manager as there are links to undo info RCE's
	// that will point to freed memory if we end the version store before the BF
	m_pver->VERTerm( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp );

	//	Before we term BF, we disable the checkpoint if error occurred

	if ( errRet < 0 || termtype == termtypeError )
		m_plog->m_fDisableCheckpoint = fTrue;

	//	reset initialization flag

	FCB::PurgeAllDatabases( this );

	PIBTerm( this );
	FUCBTerm( this );
	SCBTerm( this );

	//	clean up the fmp entries

	err = ErrIOTerm( this, m_pfsapi, termtype == termtypeCleanUp || termtype == termtypeNoCleanUp );
	if ( err < 0 )
		{
		termtype = termtypeError;
		m_fTermAbruptly = fTrue;
		if ( errRet >= 0 )
			errRet = err;
		}

	//  terminate FCB

	FCB::Term( this );

	//	delete temp file. Temp file should be cleaned up in IOTerm.

	//	the temp database is always on the OS file-system

	DBDeleteDbFiles( this, m_pfsapi, m_szTempDatabase );
	
	return errRet;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\util.cxx ===
#include "std.hxx"

#ifdef DEBUG
#include <stdarg.h>
#endif

VOID UtilLoadDbinfomiscFromPdbfilehdr(
	JET_DBINFOMISC *pdbinfomisc,
	DBFILEHDR_FIX *pdbfilehdr )
	{
	pdbinfomisc->ulVersion			= pdbfilehdr->le_ulVersion;
	pdbinfomisc->dbstate			= pdbfilehdr->le_dbstate;				
	pdbinfomisc->signDb				= *(JET_SIGNATURE *) &(pdbfilehdr->signDb);	
	pdbinfomisc->lgposConsistent	= *(JET_LGPOS *) &(pdbfilehdr->le_lgposConsistent);
	pdbinfomisc->logtimeConsistent	= *(JET_LOGTIME *) &(pdbfilehdr->logtimeConsistent);	
	pdbinfomisc->logtimeAttach		= *(JET_LOGTIME *) &(pdbfilehdr->logtimeAttach);
	pdbinfomisc->lgposAttach		= *(JET_LGPOS *) &(pdbfilehdr->le_lgposAttach);
	pdbinfomisc->logtimeDetach		= *(JET_LOGTIME *) &(pdbfilehdr->logtimeDetach);
	pdbinfomisc->lgposDetach		= *(JET_LGPOS *) &(pdbfilehdr->le_lgposDetach);
	pdbinfomisc->signLog			= *(JET_SIGNATURE *) &(pdbfilehdr->signLog);
	pdbinfomisc->bkinfoFullPrev		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoFullPrev);
	pdbinfomisc->bkinfoIncPrev		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoIncPrev);
	pdbinfomisc->bkinfoFullCur		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoFullCur);
	pdbinfomisc->fShadowingDisabled	= pdbfilehdr->FShadowingDisabled();
	pdbinfomisc->fUpgradeDb			= pdbfilehdr->FUpgradeDb();
	pdbinfomisc->dwMajorVersion		= pdbfilehdr->le_dwMajorVersion;
	pdbinfomisc->dwMinorVersion		= pdbfilehdr->le_dwMinorVersion;
	pdbinfomisc->dwBuildNumber		= pdbfilehdr->le_dwBuildNumber;
	pdbinfomisc->lSPNumber			= pdbfilehdr->le_lSPNumber;
	pdbinfomisc->ulUpdate			= pdbfilehdr->le_ulUpdate;
	pdbinfomisc->cbPageSize			= pdbfilehdr->le_cbPageSize == 0 ? (ULONG) g_cbPageDefault : (ULONG) pdbfilehdr->le_cbPageSize;
	}
	

CODECONST(unsigned char) mpcoltypcb[] =
	{
	0,					/* JET_coltypNil (coltypNil is used for vltUninit parms) */
	sizeof(char),		/* JET_coltypBit */
	sizeof(char),		/* JET_coltypUnsignedByte */
	sizeof(short),		/* JET_coltypShort */
	sizeof(long),		/* JET_coltypLong */
	sizeof(long)*2,		/* JET_coltypCurrency */
	sizeof(float),		/* JET_coltypIEEESingle */
	sizeof(double),		/* JET_coltypIEEEDouble */
	sizeof(double),		/* JET_coltypDateTime */
	0,					/* JET_coltypBinary */
	0,					/* JET_coltypText */
	sizeof(long),		/* JET_coltypLongBinary */
	sizeof(long),		/* JET_coltypLongText */
	0,					/* JET_coltypDatabase */
	sizeof(JET_TABLEID)	/* JET_coltypTableid */
	};


LOCAL CODECONST(unsigned char) rgbValidName[16] = {
	0xff,			       /* 00-07 No control characters */
	0xff,			       /* 08-0F No control characters */
	0xff,			       /* 10-17 No control characters */
	0xff,			       /* 18-1F No control characters */
	0x02,			       /* 20-27 No ! */
	0x40,			       /* 28-2F No . */
	0x00,			       /* 30-37 */
	0x00,			       /* 38-3F */
	0x00,			       /* 40-47 */
	0x00,			       /* 48-4F */
	0x00,			       /* 50-57 */
	0x28,			       /* 58-5F No [ or ] */
	0x00,			       /* 60-67 */
	0x00,			       /* 68-6F */
	0x00,			       /* 70-77 */
	0x00,			       /* 78-7F */
	};



//	WARNING: Assumes an output buffer of JET_cbNameMost+1
ERR ErrUTILICheckName(
	CHAR * const		szNewName,
	const CHAR * const	szName, 
	const BOOL			fTruncate )
	{
	CHAR *				pchLast		= szNewName;
	SIZE_T				cch;
	BYTE				ch;

	//	a name may not begin with a space
	if ( ' ' == *szName )
		return ErrERRCheck( JET_errInvalidName );

	for ( cch = 0;
		cch < JET_cbNameMost && ( ( ch = (BYTE)szName[cch] ) != '\0' );
		cch++ )
		{
		//	extended characters always valid
		if ( ch < 0x80 )
			{
			if ( ( rgbValidName[ch >> 3] >> (ch & 0x7) ) & 1 )
				return ErrERRCheck( JET_errInvalidName );
			}

		szNewName[cch] = (CHAR)ch;

		//	last significant character
		if ( ' ' != ch )
			pchLast = szNewName + cch + 1;
		}

	//	check name too long
	//	UNDONE: insignificant trailing spaces that cause
	//	the length of the name to exceed cbNameMost will
	//	currently trigger an error
	if ( JET_cbNameMost == cch )
		{
		if ( !fTruncate && '\0' != szName[JET_cbNameMost] )
			return ErrERRCheck( JET_errInvalidName );
		}

	//	length of significant portion
	Assert( pchLast >= szNewName );
	Assert( pchLast <= szNewName + JET_cbNameMost );
	cch = pchLast - szNewName;

	if ( 0 == cch )
		return ErrERRCheck( JET_errInvalidName );

	//	we assume an output buffer of JET_cbNameMost+1
	Assert( cch <= JET_cbNameMost );
	szNewName[cch] = '\0';

	return JET_errSuccess;
	}


#ifdef DEBUG

typedef void ( *PFNvprintf)(const char  *, va_list);

struct {
	PFNvprintf pfnvprintf;
	}  pfn = { NULL };


void VARARG DebugPrintf(const char  *szFmt, ...)
	{
	va_list arg_ptr;

	if (pfn.pfnvprintf == NULL)	       /* No op if no callback registered */
		return;

	va_start(arg_ptr, szFmt);
	(*pfn.pfnvprintf)(szFmt, arg_ptr);
	va_end(arg_ptr);
	}


	/*	The following pragma affects the code generated by the C
	/*	compiler for all FAR functions.  Do NOT place any non-API
	/*	functions beyond this point in this file.
	/**/

void JET_API JetDBGSetPrintFn(JET_SESID sesid, PFNvprintf pfnParm)
	{
	Unused( sesid );
	
	pfn.pfnvprintf = pfnParm;
	}

/*
 *	level 0 - all log s.
 *	level 1 - log read and update operations.
 *	level 2 - log update operations only.
 *	level 99- never log
 */

LOCAL CODECONST(unsigned char) mpopLogLevel[opMax] = {
/*							0	*/		0,
/*  opIdle					1	*/		2,
/*	opGetTableIndexInfo		2	*/		1,
/*	opGetIndexInfo			3	*/		1,
/*	opGetObjectInfo			4	*/		1,
/*	opGetTableInfo			5	*/		1,
/*	opCreateObject			6	*/		2,
/*	opDeleteObject			7	*/		2,
/*	opRenameObject			8	*/		2,
/*	opBeginTransaction		9	*/		2,
/*	opCommitTransaction		10	*/		2,
/*	opRollback				11	*/		2,
/*	opOpenTable				12	*/		1,
/*	opDupCursor				13	*/		1,
/*	opCloseTable			14	*/		1,
/*	opGetTableColumnInfo	15	*/		1,
/*	opGetColumnInfo			16	*/		1,
/*	opRetrieveColumn		17	*/		1,
/*	opRetrieveColumns		18	*/		1,
/*	opSetColumn				19	*/		2,
/*	opSetColumns			20	*/		2,
/*	opPrepareUpdate			21	*/		2,
/*	opUpdate				22	*/		2,
/*	opDelete				23	*/		2,
/*	opGetCursorInfo			24	*/		1,
/*	opGetCurrentIndex		25	*/		1,
/*	opSetCurrentIndex		26	*/		1,
/*	opMove					27	*/		1,
/*	opMakeKey				28	*/		1,
/*	opSeek					29	*/		1,
/*	opGetBookmark			30	*/		1,
/*	opGotoBookmark			31	*/		1,
/*	opGetRecordPosition		32	*/		1,
/*	opGotoPosition			33	*/		1,
/*	opRetrieveKey			34	*/		1,
/*	opCreateDatabase		35	*/		2,
/*	opOpenDatabase			36	*/		1,
/*	opGetDatabaseInfo		37	*/		1,
/*	opCloseDatabase			38	*/		1,
/*	opCapability			39	*/		1,
/*	opCreateTable			40	*/		2,
/*	opRenameTable			41	*/		2,
/*	opDeleteTable			42	*/		2,
/*	opAddColumn				43	*/		2,
/*	opRenameColumn			44	*/		2,
/*	opDeleteColumn			45	*/		2,
/*	opCreateIndex			46	*/		2,
/*	opRenameIndex			47	*/		2,
/*	opDeleteIndex			48	*/		2,
/*	opComputeStats			49	*/		2,
/*	opAttachDatabase		50	*/		2,
/*	opDetachDatabase		51	*/		2,
/*	opOpenTempTable			52	*/		2,
/*	opSetIndexRange			53	*/		1,
/*	opIndexRecordCount		54	*/		1,
/*	opGetChecksum			55	*/		1,
/*	opGetObjidFromName		56	*/		1,
/*	opEscrowUpdate			57	*/		1,
/*	opGetLock				58	*/		1,
/*	opRetrieveTaggedColumnList	59	*/	1,
/*	opCreateTableColumnIndex	60	*/	2,
/*	opSetColumnDefaultValue	61	*/		2,
/*	opPrepareToCommitTransaction 62 */	2,
/*	opSetTableSequential	63	*/		99,
/*	opResetTableSequential	64	*/		99,
/*	opRegisterCallback		65	*/		99,
/*	opUnregisterCallback	66	*/		99,
/*	opSetLS					67	*/		99,
/*	opGetLS					68	*/		99,
/*	opGetVersion			69	*/		99,
/*	opBeginSession			70	*/		99,
/*	opDupSession			71	*/		99,
/*	opEndSession			72	*/		99,
/*	opBackupInstance		73	*/		99,
/*	opBeginExternalBackupInstance 74 */	99,
/*	opGetAttachInfoInstance	75	*/		99,
/*	opOpenFileInstance		76	*/		99,
/*	opReadFileInstance		77	*/		99,
/*	opCloseFileInstance		78	*/		99,
/*	opGetLogInfoInstance	79	*/		99,
/*	opGetTruncateLogInfoInstance 80 */	99,
/*	opTruncateLogInstance	81	*/		99,
/*	opEndExternalBackupInstance	82	*/	99,
/*	opSnapshotStart			83	*/		99,
/*	opSnapshotStop			84	*/		99,
/*	opResetCounter			85	*/		99,
/*	opGetCounter			86	*/		99,
/*	opCompact				87	*/		99,
/*	opConvertDDL			88	*/		99,
/*	opUpgradeDatabase		89	*/		99,
/*	opDefragment			90	*/		99,
/*	opSetDatabaseSize		91	*/		99,
/*	opGrowDatabase			92	*/		99,
/*	opSetSessionContext		93	*/		99,
/*	opResetSessionContext	94	*/		99,
/*	opSetSystemParameter	95	*/		99,
/*	opGetSystemParameter	96	*/		99,
/*	opTerm					97	*/		99,
/*	opInit					98	*/		99,
/*	opIntersectIndexes		99	*/		99,
/*	opDBUtilities			100	*/		99,
/*	opEnumerateColumns		101	*/		1,
};

/* function in logapi to store jetapi calls */
extern void LGJetOp( JET_SESID sesid, int op );

void DebugLogJetOp( JET_SESID sesid, int op )
	{
	Unused( sesid );
	
	// UNDONE: should be controlled by a system parameter to decide
	// UNDONE: which log level it should be.

	/* log level 2 operations */
	if ( op < opMax && mpopLogLevel[ op ] >= lAPICallLogLevel )
		{
//		LGJetOp( sesid, op );
		}
	}

#endif	/* DEBUG */
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\upgrade.cxx ===
/*******************************************************************

Converting a database from ESE97 to ESE98 format

*******************************************************************/

#include "std.hxx"


//  ****************************************************************
//  STRUCTURES
//  ****************************************************************


//  ================================================================
struct UPGRADEPAGESTATS
//  ================================================================
	{
	LONG 		err;				//  error condition from the first thread to encounter an error
	LONG		cpgSeen;			//  total pages seen
	};


/*	long value column in old record format
/**/

PERSISTED
struct LV
	{
	union
		{
		BYTE	bFlags;
		struct
			{
			BYTE	fSeparated:1;
			BYTE	fSLVField:1;
			BYTE	fReserved:6;
			};
		}; 		// ATTENTION: the size of the union must remain 1 byte
	
	UnalignedLittleEndian< LID >		m_lid;
	};


//  ****************************************************************
//  CLASSES
//  ****************************************************************


//  ================================================================
class CONVERTPAGETASK : public DBTASK
//  ================================================================
	{
	public:
		CONVERTPAGETASK( const IFMP ifmp, const PGNO pgnoFirst, const CPG cpg, UPGRADEPAGESTATS * const pstats );
		virtual ~CONVERTPAGETASK();
		
		virtual ERR ErrExecute( PIB * const ppib );
		
	protected:
		const PGNO 		m_pgnoFirst;
		const CPG		m_cpg;

		UPGRADEPAGESTATS * const m_pstats;
		
	private:
		CONVERTPAGETASK( const CONVERTPAGETASK& );
		CONVERTPAGETASK& operator=( const CONVERTPAGETASK& );
	};
	

//  ================================================================
class CONVERTPAGETASKPOOL
//  ================================================================
	{
	public:
		CONVERTPAGETASKPOOL( const IFMP ifmp );
		~CONVERTPAGETASKPOOL();

		ERR ErrInit( PIB * const ppib, const INT cThreads );
		ERR ErrTerm();

		ERR ErrConvertPages( const PGNO pgnoFirst, const CPG cpg );

		const volatile UPGRADEPAGESTATS& Stats() const;
		
	private:
		CONVERTPAGETASKPOOL( const CONVERTPAGETASKPOOL& );
		CONVERTPAGETASKPOOL& operator=( const CONVERTPAGETASKPOOL& );
		
	private:
		const IFMP 	m_ifmp;
		TASKMGR		m_taskmgr;

		UPGRADEPAGESTATS	m_stats;
	};


//  ****************************************************************
//  PROTOTYPES
//  ****************************************************************


//	record conversion

VOID UPGRADECheckConvertNode(
	const VOID * const pvRecOld,
	const INT cbRecOld,
	const VOID * const pvRecNew,
	const INT cbRecNew );
ERR ErrUPGRADEConvertNode(
	CPAGE * const pcpage,
	const INT iline,
	VOID * const pvBuf );
ERR ErrUPGRADEConvertPage(
	CPAGE * const pcpage,
	VOID * const pvBuf );


//	perf counters

PM_CEF_PROC LUPGRADEPagesConvertedCEFLPv;
PM_CEF_PROC LUPGRADENodesConvertedCEFLPv;

//  ****************************************************************
//  GLOBAL VARIABLES
//  ****************************************************************

PERFInstanceGlobal<> cUPGRADEPagesConverted;
PERFInstanceGlobal<> cUPGRADENodesConverted;

//  ****************************************************************
//  FUNCTIONS
//  ****************************************************************


//  ================================================================
long LUPGRADEPagesConvertedCEFLPv( long iInstance, void* pvBuf )
//  ================================================================
	{
	cUPGRADEPagesConverted.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
long LUPGRADENodesConvertedCEFLPv( long iInstance, void* pvBuf )
//  ================================================================
	{
	cUPGRADENodesConverted.PassTo( iInstance, pvBuf );
	return 0;
	}


//  ================================================================
CONVERTPAGETASKPOOL::CONVERTPAGETASKPOOL( const IFMP ifmp ) :
	m_ifmp( ifmp )
//  ================================================================
	{
	m_stats.err					= JET_errSuccess;
	m_stats.cpgSeen 			= 0;	
	}


//  ================================================================
CONVERTPAGETASKPOOL::~CONVERTPAGETASKPOOL()
//  ================================================================
	{
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrInit( PIB * const ppib, const INT cThreads )
//  ================================================================
	{
	ERR err;
	
	INST * const pinst = PinstFromIfmp( m_ifmp );

	Call( m_taskmgr.ErrInit( pinst, cThreads ) );
	
	return err;
	
HandleError:
	CallS( ErrTerm() );
	return err;	
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrTerm()
//  ================================================================
	{
	ERR err;
	
	Call( m_taskmgr.ErrTerm() );

	err = m_stats.err;
	
HandleError:
	return err;
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrConvertPages( const PGNO pgnoFirst, const CPG cpg )
//  ================================================================
	{
	CONVERTPAGETASK * ptask = new CONVERTPAGETASK( m_ifmp, pgnoFirst, cpg, &m_stats );
	if( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	const ERR err = m_taskmgr.ErrPostTask( TASK::Dispatch, (ULONG_PTR)ptask );
	if( err < JET_errSuccess )
		{
		//  The task was not enqued sucessfully.
		delete ptask;
		}
	return err;	
	}


//  ================================================================
const volatile UPGRADEPAGESTATS& CONVERTPAGETASKPOOL::Stats() const
//  ================================================================
	{
	return m_stats;
	}
		

//  ================================================================
CONVERTPAGETASK::CONVERTPAGETASK(
	const IFMP ifmp,
	const PGNO pgnoFirst,
	const CPG cpg,
	UPGRADEPAGESTATS * const pstats ) :
//  ================================================================
	DBTASK( ifmp ),
	m_pgnoFirst( pgnoFirst ),
	m_cpg( cpg ),
	m_pstats( pstats )
	{
	}


//  ================================================================
CONVERTPAGETASK::~CONVERTPAGETASK()
//  ================================================================
	{
	}
		

//  ================================================================
ERR CONVERTPAGETASK::ErrExecute( PIB * const ppib )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CSR csr;
	
	PGNO pgno;
	for( pgno = m_pgnoFirst; pgno < m_pgnoFirst + m_cpg; ++pgno )
		{
		AtomicExchangeAdd( &m_pstats->cpgSeen, 1 );
		
		err = csr.ErrGetRIWPage( ppib, m_ifmp, pgno );
		if( JET_errPageNotInitialized == err )
			{
			//	error is expected
			continue;
			}
		Call( err );
		
///		Call( csr.ErrUpgrade() );
///		UNDONE:	reorganize the data on the page
///		Call( csr.Cpage().ReorganizeAndZero( 'C' ) );

		csr.ReleasePage( fTrue );
		csr.Reset();
		}

HandleError:
	return err;
	}
		

//  ================================================================
ERR ErrDBUTLConvertRecords( JET_SESID sesid, const JET_DBUTIL * const pdbutil )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	PIB * const ppib = reinterpret_cast<PIB *>( sesid );
	CONVERTPAGETASKPOOL * pconverttasks = NULL;
	
	Call( ErrIsamAttachDatabase( sesid, pdbutil->szDatabase, pdbutil->szSLV, pdbutil->szSLV, 0, NO_GRBIT ) );

	IFMP	ifmp;	
	Call( ErrIsamOpenDatabase(
		sesid,
		pdbutil->szDatabase,
		NULL,
		reinterpret_cast<JET_DBID *>( &ifmp ),
		NO_GRBIT
		) );

	PGNO pgnoLast;
	pgnoLast = rgfmp[ifmp].PgnoLast();

	pconverttasks = new CONVERTPAGETASKPOOL( ifmp );
	if( NULL == pconverttasks )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	Call( pconverttasks->ErrInit( ppib, CUtilProcessProcessor() ) );
	
	JET_SNPROG snprog;
	memset( &snprog, 0, sizeof( snprog ) );	
	snprog.cunitTotal 	= pgnoLast;
	snprog.cunitDone 	= 0;

	JET_PFNSTATUS pfnStatus;
	pfnStatus = reinterpret_cast<JET_PFNSTATUS>( pdbutil->pfnCallback );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntBegin, NULL );	
		}

	PGNO pgno;
	pgno = 1;

	while( pgnoLast	>= pgno && JET_errSuccess == pconverttasks->Stats().err )
		{
		const CPG cpgChunk = 256;
		const CPG cpgPreread = min( cpgChunk, pgnoLast - pgno + 1 );
		BFPrereadPageRange( ifmp, pgno, cpgPreread );

		Call( pconverttasks->ErrConvertPages( pgno, cpgPreread ) );
		pgno += cpgPreread;

		snprog.cunitDone = pconverttasks->Stats().cpgSeen;
		if ( NULL != pfnStatus )
			{
			(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
			}

		while( 	( pconverttasks->Stats().cpgSeen + ( cpgChunk * 4 ) ) < pgno
				&& JET_errSuccess == pconverttasks->Stats().err )
			{
			snprog.cunitDone = pconverttasks->Stats().cpgSeen;
			if ( NULL != pfnStatus )
				{
				(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
				}
			UtilSleep( cmsecWaitGeneric );
			}
		}

	snprog.cunitDone = pconverttasks->Stats().cpgSeen;
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
		}

	Call( pconverttasks->ErrTerm() );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntComplete, NULL );
		}
	
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages seen\n", pconverttasks->Stats().cpgSeen );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages converted\n", cUPGRADEPagesConverted.Get( PinstFromPpib( ppib ) ) );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d records converted\n", cUPGRADENodesConverted.Get( PinstFromPpib( ppib ) ) );

	err = pconverttasks->Stats().err;
	
	delete pconverttasks;
	pconverttasks = NULL;

	Call( err );
	
HandleError:
	if( NULL != pconverttasks )
		{
		const ERR errT = pconverttasks->ErrTerm();
		if( err >= 0 && errT < 0 )
			{
			err = errT;
			}
		delete pconverttasks;
		}
	return err;
	}


//  ================================================================
ERR ErrUPGRADEPossiblyConvertPage(
		CPAGE * const pcpage,
		VOID * const pvWorkBuf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( pcpage->FNewRecordFormat() )
		{
		return JET_errSuccess;
		}
	
	if( pcpage->FPrimaryPage()
		&& !pcpage->FRepairedPage()
		&& pcpage->FLeafPage()
		&& !pcpage->FSpaceTree()
		&& !pcpage->FLongValuePage()
		&& !pcpage->FSLVAvailPage()
		&& !pcpage->FSLVOwnerMapPage() )
		{
		Call( ErrUPGRADEConvertPage( pcpage, pvWorkBuf ) );
		//	CONSIDER: return a warning saying the page was converted
		}
	else
		{
		//	this page doesn't need converting but we will flag it to avoid
		//	trying to convert in the future
		pcpage->SetFNewRecordFormat();
		BFDirty( pcpage->PBFLatch(), CPAGE::bfdfRecordUpgradeFlags );	
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrUPGRADECheckConvertNode(
	const VOID* const	pvRecOld,
	const SIZE_T		cbRecOld,
	const VOID* const	pvRecNew,
	const SIZE_T		cbRecNew )
//  ================================================================
	{
	const REC* const	precOld		= reinterpret_cast<const REC *>( pvRecOld );
	const REC* const	precNew		= reinterpret_cast<const REC *>( pvRecNew );

	if( cbRecOld < cbRecNew )
		{
		AssertSz( fFalse, "Record format conversion: the new record is larger" );
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
	
	if( precOld->FidFixedLastInRec() != precNew->FidFixedLastInRec() )
		{
		AssertSz( fFalse, "Record format conversion: fid fixed last changed" );
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->FidVarLastInRec() != precNew->FidVarLastInRec() )
		{
		AssertSz( fFalse, "Record format conversion: fid var last changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->IbEndOfFixedData() != precNew->IbEndOfFixedData() )
		{
		AssertSz( fFalse, "Record format conversion: end of fixed data changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->CbFixedNullBitMap() != precNew->CbFixedNullBitMap() )
		{
		AssertSz( fFalse, "Record format conversion: size of fixed null bitmap changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->IbEndOfVarData() != precNew->IbEndOfVarData() )
		{
		AssertSz( fFalse, "Record format conversion: end if var data changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( !!precOld->FTaggedData( cbRecOld ) != !!precNew->FTaggedData( cbRecNew ) )
		{
		AssertSz( fFalse, "Record format conversion: tagged data missing/added!" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	const TAGFLD_OLD* const		ptagfldoldMin		= reinterpret_cast<TAGFLD_OLD *>( precOld->PbTaggedData() );
	const TAGFLD_OLD* const		ptagfldoldMax		= reinterpret_cast<TAGFLD_OLD *>( (BYTE*)pvRecOld + cbRecOld );	
	const TAGFLD_OLD*			ptagfldold 			= ptagfldoldMin;

	DATA dataRecNew;
	dataRecNew.SetPv( const_cast<VOID *>( pvRecNew ) );
	dataRecNew.SetCb( cbRecNew );

	if( !TAGFIELDS::FIsValidTagfields( dataRecNew, CPRINTFNULL::PcprintfInstance() ) )
		{
		AssertSz( fFalse, "Record format conversion: new record is not valid" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}


	//	sadly, the code below can't deal with derived/non-derived columns
	/*
	

	TAGFIELDS tagfields( dataRecNew );
	
	COLUMNID columnidPrev = 0;
	INT itagSequence = 1;
	for( ; ptagfldold < ptagfldoldMax; ptagfldold = ptagfldold->PtagfldNext() )
		{
		const FID 			fid			= ptagfldold->Fid();
		const COLUMNID		columnid	= ColumnidOfFid( fid, !ptagfldold->FDerived() );
		const VOID * const 	pvDataOld 	= ptagfldold->Rgb() + !!ptagfldold->FLongValue();
		const INT 			cbDataOld 	= ptagfldold->CbData() - !!ptagfldold->FLongValue();
		
		DATA 				dataOld;
		DATA				dataNew;
		
		dataOld.SetPv( const_cast<VOID *>( pvDataOld ) );
		dataOld.SetCb( cbDataOld );

		if( columnid == columnidPrev )
			{
			++itagSequence;
			}
		else
			{
			columnidPrev 	= columnid;
			itagSequence	= 1;
			}

		DATA dataNewRec;

		const JET_ERR err = tagfields.ErrRetrieveColumn(
				pfcbNil,
				columnid,
				itagSequence,
				dataRecNew,
				&dataNew,
				JET_bitRetrieveIgnoreDefault );
						
		AssertRtl( dataNew.Cb() == dataOld.Cb() );
		AssertRtl( 0 == memcmp( dataNew.Pv(), dataOld.Pv(), dataNew.Cb() ) );
		}

	*/
	
	return JET_errSuccess;
	}


//  ================================================================
ERR ErrUPGRADEConvertNode(
	CPAGE * const pcpage,
	const INT iline,
	VOID * const pvBuf )
//  ================================================================
//
//	+------+-----+------+-----+-----+------+-----+-------+-------+-----+-------+
//	| fid1 | ib1 | fid2 | ib2 | ... | fidn | ibn | data1 | data2 | ... | datan |
//	+------+-----+------+-----+-----+------+-----+-------+-------+-----+-------+
//     2B    2B     2B    2B           2B    2B
//
//	The high bits of the ib's are used to store derived, extended info byte and NULL
//	bits
//
//-
	{
	ERR	err = JET_errSuccess;

	Assert( !pcpage->FNewRecordFormat() );

	//	get the record from the page
	
	KEYDATAFLAGS kdf;
	NDIGetKeydataflags( *pcpage, iline, &kdf );

	if( FNDDeleted( kdf ) )
		{
		return JET_errSuccess;
		}
		
	const INT cbRec				= kdf.data.Cb();
	const BYTE * const pbRec	= reinterpret_cast<const BYTE *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<const BYTE *>( kdf.data.Pv() ) + cbRec;

	const REC * const prec = reinterpret_cast<const REC *>( pbRec );

	//	how much tagged and non-tagged data is there
	
	const SIZE_T cbNonTaggedData 	= prec->PbTaggedData() - pbRec;
	const SIZE_T cbTaggedData 		= cbRec - cbNonTaggedData;

	//	go through the old-format TAGFLDs. How many are there? Are there multivalues?

	const TAGFLD_OLD * const ptagfldoldMin		= reinterpret_cast<const TAGFLD_OLD *>( prec->PbTaggedData() );
	const TAGFLD_OLD * const ptagfldoldMax		= reinterpret_cast<const TAGFLD_OLD *>( pbRecMax );	
	const TAGFLD_OLD * 	ptagfldold 				= NULL;
	
	BOOL				fRecordHasMultivalues	= fFalse;
	INT 				cTAGFLD 				= 0;		//	number of unique multi-values
	
	FID					fidPrev 				= 0;
	for( ptagfldold = ptagfldoldMin; ptagfldold < ptagfldoldMax; ptagfldold = ptagfldold->PtagfldNext() )
		{
		if( ptagfldold->Fid() == fidPrev )
			{
			fRecordHasMultivalues = fTrue;
			}
		else
			{
			fidPrev = ptagfldold->Fid();
			++cTAGFLD;
			}
		}

	//	copy in the non-tagged data
	
	BYTE * const pb	= reinterpret_cast<BYTE *>( pvBuf );
	memcpy( pb, pbRec, cbNonTaggedData );

	//	create the TAGFLD array and copy in the data

	BYTE * const pbTagfldsStart = pb + cbNonTaggedData;
	BYTE * pbTagflds 			= pbTagfldsStart;
	BYTE * const pbDataStart 	= pbTagfldsStart + ( cTAGFLD * sizeof(TAGFLD) );
	BYTE * pbData				= pbDataStart;

	INT ibCurr = cTAGFLD * sizeof(TAGFLD);

	ptagfldold = ptagfldoldMin;
	while( ptagfldold < ptagfldoldMax )
		{		
		static const USHORT fDerived		= 0x8000;		//	if TRUE, then current column is derived from a template
		static const USHORT fExtendedInfo	= 0x4000;		//	if TRUE, must go to TAGFLD_HEADER byte to check more flags
		static const USHORT fNull			= 0x2000;		//	if TRUE, column set to NULL to override default value
		
		const FID fid 							= ptagfldold->Fid();
		const TAGFLD_OLD * const ptagfldoldNext = ptagfldold->PtagfldNext();
		
		if( ptagfldoldNext < ptagfldoldMax
			&& ptagfldoldNext->Fid() == fid
			&& ptagfldoldNext->FDerived() == ptagfldold->FDerived() )
			{			
			const TAGFLD_OLD * 	ptagfldoldNextFid 	= ptagfldold;
			INT 				cMULTIVALUES 		= 0;
			INT 				cbDataTotal			= 0;
			
			while( 	ptagfldoldNextFid < ptagfldoldMax
					&& ptagfldoldNextFid->Fid() == fid )
				{
				++cMULTIVALUES;
				cbDataTotal += ptagfldoldNextFid->CbData();
				ptagfldoldNextFid = ptagfldoldNextFid->PtagfldNext();
				}

			if( 2 == cMULTIVALUES
				&& !ptagfldold->FLongValue() )
				{
				//	convert to the TWOVALUES format
				//
				//	+---------------+---------------+-----------+-----------+
				//	| extended info | cbData1       | data1 ... | data2 ... |
				//	+---------------+---------------+-----------+-----------+
				//            1B             1B       
				//
				//	note that the length is one byte (non-lv columns
				//	are limited to 255 bytes)
				
				Assert( !ptagfldold->FLongValue() );
				Assert( !ptagfldoldNext->FLongValue() );
				Assert( !ptagfldold->FNull() );
				Assert( !ptagfldoldNext->FNull() );

				//	create the entry in the TAGFLD array

				USHORT ibFlags		= fExtendedInfo;
				if( ptagfldold->FDerived() )
					{
					ibFlags |= fDerived;
					}
			
				const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
				pbTagflds += sizeof( USHORT );
				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
				pbTagflds += sizeof( USHORT );
			
				ibCurr += sizeof( TAGFLD_HEADER ) + sizeof( BYTE ) + cbDataTotal;

				//	set extended info to say we are in the twovalues format

				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );
				ptagfldheader->SetFTwoValues();				
				ptagfldheader->SetFMultiValues();				
				pbData += sizeof( TAGFLD_HEADER );
				
				//	set cbData1

				*pbData = (BYTE)ptagfldold->CbData();
				pbData += sizeof( BYTE );
				
				//	copy in the data
			
				memcpy( pbData, ptagfldold->Rgb(), ptagfldold->CbData() );
				pbData 		+= ptagfldold->CbData();

				memcpy( pbData, ptagfldoldNext->Rgb(), ptagfldoldNext->CbData() );
				pbData 		+= ptagfldoldNext->CbData();
				}
			else
				{
				//	convert to the MULTIVALUES format
				//
				//	+-------------------+------------+-----+-----------+-----------+-----------+-----+-----------+
				//	| info |     ib1    |     ib2    | ... |    ibn    | data1 ... | data2 ... | ... | datan ... |
				//	+-------------------+------------+-----+-----------+-----------+-----------+-----+-----------+
				//	   1B        2B           2B                2B
				//
				//
				//	if the data is a separated LV the high bit of its ib will be set
					
				//	create the entry in the TAGFLD array
				
				USHORT ibFlags		= fExtendedInfo;
				if( ptagfldold->FDerived() )
					{
					ibFlags |= fDerived;
					}
			
				const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
				pbTagflds += sizeof( USHORT );
				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
				pbTagflds += sizeof( USHORT );

				if( !ptagfldold->FLongValue() )
					{
					ibCurr += sizeof( TAGFLD_HEADER ) + ( sizeof( USHORT ) * cMULTIVALUES ) + cbDataTotal;
					}
				else
					{
					//	we will be losing the header byte from the LV structure
					
					ibCurr += sizeof( TAGFLD_HEADER ) + ( sizeof( USHORT ) * cMULTIVALUES ) + cbDataTotal - ( sizeof( BYTE ) * cMULTIVALUES );
					}

				//	set extended info to say we are in the multivalues format

				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );
				ptagfldheader->SetFMultiValues();				
				if( ptagfldold->FLongValue() )
					{
					const LV * const plv = reinterpret_cast<const LV *>( ptagfldold->Rgb() );					
					if( plv->fSLVField )
						{
						ptagfldheader->SetFSLV();
						}
					else
						{
						ptagfldheader->SetFLongValue();
						}
					}
				pbData += sizeof( TAGFLD_HEADER );				

				//	first make space for the ibOffsets

				INT		ibOffsetCurr = sizeof( USHORT ) * cMULTIVALUES;
				BYTE * pbIbOffsets = pbData;
				pbData += sizeof( USHORT ) * cMULTIVALUES;

				const TAGFLD_OLD * 	ptagfldoldT 	= NULL;
				for( 	ptagfldoldT = ptagfldold;
						ptagfldoldT < ptagfldoldNextFid;
						ptagfldoldT = ptagfldoldT->PtagfldNext() )
					{
					Assert( ptagfldoldT->Fid() == fid );
					if( ptagfldoldT->FLongValue() )
						{												

						//	do we need to set the separated bit?
						
						const LV * const plv = reinterpret_cast<const LV *>( ptagfldoldT->Rgb() );
						if( plv->fSeparated )
							{
							static const USHORT fSeparatedInstance = 0x8000;					
							*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr | fSeparatedInstance );
							}
						else
							{
							*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr );
							}
						pbIbOffsets += sizeof( USHORT );					

						memcpy( pbData, ptagfldoldT->Rgb() + sizeof( BYTE ), ptagfldoldT->CbData() - sizeof( BYTE ) );
						ibOffsetCurr += ptagfldoldT->CbData() - sizeof( BYTE );
						pbData += ptagfldoldT->CbData() - sizeof( BYTE );
						}
					else
						{
						*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr );
						pbIbOffsets += sizeof( USHORT );					
						memcpy( pbData, ptagfldoldT->Rgb(), ptagfldoldT->CbData() );
						ibOffsetCurr += ptagfldoldT->CbData();
						pbData += ptagfldoldT->CbData();
						}
					}

				}
				
			ptagfldold 	= ptagfldoldNextFid;
			}
		else
			{
			USHORT ibFlags		= 0;
			if( ptagfldold->FNull() )
				{
				ibFlags |= fNull;
				}
			else
				{
				if( ptagfldold->FLongValue() )
					{
					ibFlags |= fExtendedInfo;	//	LVs have a header byte so the length isn't changed
					}
				}
				
			if( ptagfldold->FDerived() )
				{
				ibFlags |= fDerived;
				}
			
			const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

			*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
			pbTagflds += sizeof( USHORT );
			*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
			pbTagflds += sizeof( USHORT );
			
			ibCurr += ptagfldold->CbData();

			//	copy in the data
			
			memcpy( pbData, ptagfldold->Rgb(), ptagfldold->CbData() );
			
			//	set the extended info byte
			//	in this pass we only do it for LVs

			if( ibFlags & fExtendedInfo )
				{
				Assert( ptagfldold->FLongValue() );

				const LV * const plv = reinterpret_cast<const LV *>( ptagfldold->Rgb() );
				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );

				if( plv->fSLVField )
					{
					ptagfldheader->SetFSLV();
					}
				else
					{
					ptagfldheader->SetFLongValue();
					}
					
				if( plv->fSeparated )
					{
					ptagfldheader->SetFSeparated();
					}
				}

			pbData 		+= ptagfldold->CbData();
			ptagfldold 	= ptagfldoldNext;
			}
		}
		
	const BYTE* const	pbMax		= pbData;
	const SIZE_T		cbRecNew	= pbMax - pb;

	CallR( ErrUPGRADECheckConvertNode( kdf.data.Pv(), kdf.data.Cb(), pvBuf, cbRecNew ) );

	DATA data;
	data.SetPv( pvBuf );
	data.SetCb( cbRecNew );
	
	NDReplaceForUpgrade( pcpage, iline, &data, kdf );

	Assert( NULL != PinstFromIfmp( pcpage->Ifmp() ) );
	cUPGRADENodesConverted.Inc( PinstFromIfmp( pcpage->Ifmp() ) );

	CallS( err );
	return err;
	}


//  ================================================================
ERR ErrUPGRADEConvertPage(
	CPAGE * const pcpage,
	VOID * const pvBuf )
//  ================================================================
	{
	ERR	err = JET_errSuccess;

	Assert( !pcpage->FNewRecordFormat() );

#ifdef DEBUG
	memset( pvBuf, 0xff, g_cbPage );
#endif	//	DEBUG

	INT iline;
	for( iline = 0; iline < pcpage->Clines(); ++iline )
		{
		Call( ErrUPGRADEConvertNode( pcpage, iline, pvBuf ) );
		}

	pcpage->SetFNewRecordFormat();
	BFDirty( pcpage->PBFLatch(), CPAGE::bfdfRecordUpgradeFlags );

	Assert( NULL != PinstFromIfmp( pcpage->Ifmp() ) );
	cUPGRADEPagesConverted.Inc( PinstFromIfmp( pcpage->Ifmp() ) );

HandleError:
	if( err < 0 )
		{
		if( JET_errRecordFormatConversionFailed == err )
			{

			//	eventlog this failure

			const CHAR * rgsz[3];
			INT isz = 0;
			
			const OBJID objid = pcpage->ObjidFDP();			
			CHAR szObjid[16];
			sprintf( szObjid, "%d", objid );
			
			const PGNO pgno = pcpage->Pgno();
			CHAR szPgno[16];
			sprintf( szPgno, "%d", pgno );

			CHAR szIline[16];
			sprintf( szIline, "%d", iline );
			
			rgsz[isz++] = szObjid;
			rgsz[isz++] = szPgno;
			rgsz[isz++] = szIline;

			Assert( isz == sizeof( rgsz ) / sizeof( rgsz[0] ) );

			UtilReportEvent(
				eventError,
				CONVERSION_CATEGORY,
				RECORD_FORMAT_CONVERSION_FAILED_ID,
				isz,
				rgsz );				
			
			}
		else
			{

			//	we only expect the above error

			Assert( fFalse );

			}
		}
	else
		{
		CallS( err );
		Assert( pcpage->FNewRecordFormat() );
		}
	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\tm.cxx ===
#include "std.hxx"

BOOL g_fSystemInit = fFalse;
INT itibGlobal = 0;

//+api
//	ErrIsamBeginSession
//	========================================================
//	ERR ErrIsamBeginSession( PIB **pppib )
//
//	Begins a session with DAE.  Creates and initializes a PIB for the
//	user and returns a pointer to it.  Calls system initialization.
//
//	PARAMETERS	pppib			Address of a PIB pointer.  On return, *pppib
//		   						will point to the new PIB.
//
//	RETURNS		Error code, one of:
//					JET_errSuccess
//					JET_errTooManyActiveUsers
//
//	SEE ALSO		ErrIsamEndSession
//-
ERR ISAMAPI ErrIsamBeginSession( JET_INSTANCE inst, JET_SESID *psesid )
	{
	ERR			err;
//	JET_SESID	sesid = *psesid;
	PIB			**pppib;
	INST *pinst = (INST *)inst; 

	Assert( psesid != NULL );
	Assert( sizeof(JET_SESID) == sizeof(PIB *) );
	pppib = (PIB **)psesid;

//	critUserPIB.Enter();

	//	alllocate process information block
	//
	Call( ErrPIBBeginSession( pinst, pppib, procidNil, fFalse ) );
	(*pppib)->grbitsCommitDefault = pinst->m_grbitsCommitDefault;    /* set default commit flags */
	(*pppib)->SetFUserSession();

//	critUserPIB.Leave();

HandleError:
	return err;
	}


//+api
//	ErrIsamEndSession
//	=========================================================
//	ERR ErrIsamEndSession( PIB *ppib, JET_GRBIT grbit )
//
//	Ends the session associated with a PIB.
//
//	PARAMETERS	ppib		Pointer to PIB for ending session.
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS 
//		Rolls back all transaction levels active for this PIB.
//		Closes all FUCBs for files and sorts open for this PIB.
//
//	SEE ALSO 	ErrIsamBeginSession
//-
ERR ISAMAPI ErrIsamEndSession( JET_SESID sesid, JET_GRBIT grbit )
	{		
	ERR	 	err;
	PIB	 	*ppib	= (PIB *)sesid;
	
	CallR( ErrPIBCheck( ppib ) );

	NotUsed( grbit );

	//	lock session
	err = ErrPIBSetSessionContext( ppib, dwPIBSessionContextUnusable );
	if ( err < 0 )
		{
		if ( JET_errSessionContextAlreadySet == err )
			err = ErrERRCheck( JET_errSessionInUse );
		return err;
		}

	INST	*pinst	= PinstFromPpib( ppib );

	//	rollback all transactions
	//
	if ( ppib->level > 0 )
		{
		if ( ppib->FUseSessionContextForTrxContext() )
			{
			Assert( !pinst->FRecovering() );

			//	fake out TrxContext to allow us to rollback on behalf of another thread
			ppib->dwTrxContext = ppib->dwSessionContext;
			}
		else
			{
			//	not using SessionContext model, so if the transaction wasn't started by
			//	this thread, the rollback call below will err out with SessionSharingViolation
			}

		Assert( sizeof(JET_SESID) == sizeof(ppib) );
		Call( ErrIsamRollback( (JET_SESID)ppib, JET_bitRollbackAll ) );
		Assert( 0 == ppib->level );
		}

	//	close all databases for this PIB
	//
	Call( ErrDBCloseAllDBs( ppib ) );

	//	close all cursors still open
	//	should only be sort and temporary file cursors
	//
	while( ppib->pfucbOfSession != pfucbNil )
		{
		FUCB	*pfucb	= ppib->pfucbOfSession;

		//	close materialized or unmaterialized temporary tables
		//
		if ( FFUCBSort( pfucb ) )
			{
			Assert( !( FFUCBIndex( pfucb ) ) );
			Call( ErrIsamSortClose( ppib, pfucb ) );
			}
		else if ( pinst->FRecovering() || FFUCBSecondary( pfucb ) )
			{
			//  If the fucb is used for redo, then it is
			//  always being opened as a primary FUCB with default index.
			//  Use DIRClose to close such a fucb.
			//  Else, it is not for recovering, cursor is on index fucb,
			//  main fucb may still be ahead.  Close this index fucb.
			//
			DIRClose( pfucb );
			}
		else
			{
			while ( FFUCBSecondary( pfucb ) )
				{
				pfucb = pfucb->pfucbNextOfSession;
				}
			
			Assert( FFUCBIndex( pfucb ) );
			
			if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
				{
				CallS( ErrDispCloseTable( (JET_SESID)ppib, (JET_TABLEID) pfucb ) );
				}
			else
				{
				//	Open internally, not exported to user
				//
				CallS( ErrFILECloseTable( ppib, pfucb ) );
				}
			}
		}
	Assert( ppib->pfucbOfSession == pfucbNil );

	PIBEndSession( ppib );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	//	could not properly terminate session, must leave it in usable state
	Assert( dwPIBSessionContextUnusable == ppib->dwSessionContext );
	PIBResetSessionContext( ppib );

	return err;
	}


ERR ISAMAPI ErrIsamIdle( JET_SESID sesid, JET_GRBIT grbit )
	{
	ERR		err = JET_errSuccess;

	if ( grbit & JET_bitIdleFlushBuffers )
		{
		//	flush some dirty buffers
		Call( ErrERRCheck( JET_errInvalidGrbit ) );
		}
	else if ( grbit & JET_bitIdleStatus )
		{
		//	return error code for status
		VER *pver = PverFromPpib( (PIB *) sesid );
		CallR( pver->ErrVERStatus() );
		}
#ifndef RTM
	else if( grbit & JET_bitIdleVersionStoreTest )
		{
		//	return error code for status
		VER *pver = PverFromPpib( (PIB *) sesid );
		CallR( pver->ErrInternalCheck() );
		}
#endif	//	!RTM
	else if ( 0 == grbit || JET_bitIdleCompact & grbit  )
		{
		//	clean all version buckets
		(void) PverFromPpib( (PIB *)sesid )->ErrVERRCEClean();
		err = ErrERRCheck( JET_wrnNoIdleActivity );
		}

HandleError:
	return err;
	}


ERR VTAPI ErrIsamCapability( JET_SESID vsesid,
	JET_DBID vdbid,
	ULONG ulArea,
	ULONG ulFunction,
	ULONG *pgrbitFeature )
	{
	ERR		err = JET_errSuccess;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	NotUsed( vdbid );
	NotUsed( ulArea );
	NotUsed( ulFunction );
	NotUsed( pgrbitFeature );

	return ErrERRCheck( JET_errFeatureNotAvailable );
	}


BOOL fGlobalRepair				= fFalse;

BOOL fGlobalIndexChecking		= fTrue;
DWORD dwGlobalMajorVersion;
DWORD dwGlobalMinorVersion;
DWORD dwGlobalBuildNumber;
LONG lGlobalSPNumber;


//	Accumulated number of each instance

LONG	g_lSessionsMac = 0;
LONG	g_lVerPagesMac = 0;
LONG	g_lVerPagesPreferredMac = 0;

ERR ISAMAPI ErrIsamSystemInit()
	{
	ERR			err;
	CHAR		szT[4][8];
	const CHAR	*rgszT[4];

	//	US English MUST be installed (to ensure
	//	something like NT:125253 doesn't bite
	//	us again in the future if the NT locale
	//	team decides to start silently returning
	//	different LCMapString() results based on
	//	whether the specified lcid is installed
	CallR( ErrNORMCheckLcid( lcidDefault ) );
	AssertNORMConstants();
	
	//	Get OS info, LocalID etc

	dwGlobalMajorVersion = DwUtilSystemVersionMajor();
	dwGlobalMinorVersion = DwUtilSystemVersionMinor();
	dwGlobalBuildNumber = DwUtilSystemBuildNumber();
	lGlobalSPNumber = DwUtilSystemServicePackNumber();

	//	Set JET constant.

	CallR( ErrITSetConstants( ) );

	//	Initialize global counters
	g_lSessionsMac = 0;
	g_lVerPagesMac = 0;
	g_lVerPagesPreferredMac = 0;

	//	Initialize instances
	
	CallR( INST::ErrINSTSystemInit() );

	//	initialize the OSU layer

	CallJ( ErrOSUInit(), TermInstInit );

	//	initialize the SFS task manager (1 thread, no local contexts)
	//	NOTE: the SFS task manager can only have 1 thread -- it is heavily dependant on this assumption!

	CallJ( LOG::ErrLGSystemInit(), TermOSU );

	//	initialize file map

	CallJ( FMP::ErrFMPInit(), TermLOGTASK );

	CallJ( ErrCALLBACKInit(), TermFMP );

	CallJ( CPAGE::ErrInit(), TermCALLBACK );
	
	CallJ( ErrBFInit(), TermCPAGE );

	CallJ( ErrCATInit(), TermBF );

	CallJ( VER::ErrVERSystemInit(), TermCAT );

	/*	write jet start event
	/**/
	sprintf( szT[0], "%d", DwUtilImageVersionMajor() );
	rgszT[0] = szT[0];
	sprintf( szT[1], "%02d", DwUtilImageVersionMinor() );
	rgszT[1] = szT[1];
	sprintf( szT[2], "%04d", DwUtilImageBuildNumberMajor() );
	rgszT[2] = szT[2];
	sprintf( szT[3], "%04d", DwUtilImageBuildNumberMinor() );
	rgszT[3] = szT[3];
	UtilReportEvent(
			eventInformation,
			GENERAL_CATEGORY,
			START_ID,
			CArrayElements( rgszT ),
			rgszT );

	g_fSystemInit = fTrue;
	
	return JET_errSuccess;

// re-instate the following if anything gets added after the call to ErrVERSystemInit() above
//TermVER:
//	VER::VERSystemTerm();

TermCAT:
	CATTerm();
	
TermBF:
	BFTerm();

TermCPAGE:
	CPAGE::Term();

TermCALLBACK:
	CALLBACKTerm();

TermFMP:
	FMP::Term();

TermLOGTASK:
	LOG::LGSystemTerm();

TermOSU:
	OSUTerm();

TermInstInit:
	INST::INSTSystemTerm();

	return err;
	}
	

VOID ISAMAPI IsamSystemTerm()
	{
	if ( !g_fSystemInit )
		{
		return;
		}
		
	g_fSystemInit = fFalse;
	
	//	write jet stop event

	UtilReportEvent(
			eventInformation,
			GENERAL_CATEGORY,
			STOP_ID,
			0,
			NULL );

	VER::VERSystemTerm();

	CATTerm();

	BFTerm( );

	CPAGE::Term();

	CALLBACKTerm();
	
	FMP::Term();

	LOG::LGSystemTerm();

	OSUTerm();

	INST::INSTSystemTerm();
}


ERR ISAMAPI ErrIsamInit(	JET_INSTANCE	inst, 
							JET_GRBIT		grbit )
	{
	ERR			err;
	INST		*pinst							= (INST *)inst;
	LOG			*plog							= pinst->m_plog;
	BOOL		fLGInitIsDone					= fFalse;
	BOOL		fJetLogGeneratedDuringSoftStart	= fFalse;
	BOOL		fNewCheckpointFile				= fTrue;
	BOOL		fFSInit							= fFalse;
	CHAR		szT[8];
	const CHAR	*rgszT[1];

	Assert( pinst->m_plog );
	Assert( pinst->m_pver );
	Assert( pinst->m_pfsapi );
	Assert( !pinst->FRecovering() );

	//	create paths now if they do not exist

	if ( pinst->m_fCreatePathIfNotExist )
		{

		//	make the temp path does NOT have a trailing '\' and the log/sys paths do

		Assert( !FOSSTRTrailingPathDelimiter( pinst->m_szTempDatabase ) );
		Assert( FOSSTRTrailingPathDelimiter( pinst->m_szSystemPath ) );
		Assert( FOSSTRTrailingPathDelimiter( plog->m_szLogFilePath ) );

		//	create paths

		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, pinst->m_szTempDatabase, NULL ) );
		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, pinst->m_szSystemPath, NULL ) );
		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, plog->m_szLogFilePath, NULL ) );
		}

	//	Get basic global parameters for checking LG parameters
	
	CallR( plog->ErrLGInitSetInstanceWiseParameters( pinst->m_pfsapi ) );

	//	Check all the system parameter before we continue

	VERSanitizeParameters( &pinst->m_lVerPagesMax, &pinst->m_lVerPagesPreferredMax );

	if ( !plog->FLGCheckParams() ||
		 !FCB::FCheckParams( pinst->m_lOpenTablesMax, pinst->m_lOpenTablesPreferredMax ) ||
		 ( pinst->m_fCleanupMismatchedLogFiles && !plog->m_fLGCircularLogging ) )
		{
		return ErrERRCheck( JET_errInvalidSettings );
		}

	//	Add the new setting to the accumlating variables
	
	g_lSessionsMac += pinst->m_lSessionsMax;
	g_lVerPagesMac += pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac += pinst->m_lVerPagesPreferredMax;
	
	//	Set other variables global to this instance
	
	Assert( pinst->m_updateid == updateidMin );

	
	//	set log disable state
	//
	plog->m_fLogDisabled = pinst->FComputeLogDisabled( );
	
	// No need to display instance name and id in single instance mode
	if ( pinst->m_szInstanceName || pinst->m_szDisplayName ) // runInstModeMultiInst
		{
		/*	write jet instance start event */
		sprintf( szT, "%d", IpinstFromPinst( pinst ) );
		rgszT[0] = szT;
		UtilReportEvent( 
			eventInformation, 
			GENERAL_CATEGORY, 
			START_INSTANCE_ID, 
			CArrayElements( rgszT ), 
			rgszT, 
			0, 
			NULL, 
			pinst );
		}

	/*	initialize system according to logging disabled
	/**/
	if ( !plog->m_fLogDisabled )
		{
		DBMS_PARAM	dbms_param;
//		LGBF_PARAM	lgbf_param;

		/*	initialize log manager, and	check the last generation
		/*	of log files to determine if recovery needed.
		/*  (do not create the reserve logs -- this is done later during soft recovery)
		/**/
		CallJ( plog->ErrLGInit( pinst->m_pfsapi, &fNewCheckpointFile, fFalse ), Uninitialize );
		fLGInitIsDone = fTrue;

		/*	store the system parameters
		 */
		pinst->SaveDBMSParams( &dbms_param );
//		LGSaveBFParams( &lgbf_param );
		
		/*	recover attached databases to consistent state
		/*	if recovery is successful, then we should have
		/*	a proper edbchk.sys file
		/**/
#ifdef IGNORE_BAD_ATTACH
		plog->m_fReplayingIgnoreMissingDB = grbit & JET_bitReplayIgnoreMissingDB;
#endif // IGNORE_BAD_ATTACH

#ifdef LOGPATCH_UNIT_TEST
		extern void TestLogPatch( INST* const pinst );
		TestLogPatch( pinst );

		err = -1;
		goto TermLG;
#endif	//	LOGPATCH_UNIT_TEST

		err = plog->ErrLGSoftStart( pinst->m_pfsapi, fNewCheckpointFile, &fJetLogGeneratedDuringSoftStart );

		//	HACK: 
		//		LGSaveDBMSParams saves 'csecLGFile' which is the WRONG thing to do:
		//		a> csecLGFile is based on cbSec which can change from machine to machine
		//		b> csecLGFile is not a system parameter; rather, it is deteremined by the system parameter 
		//		   'lLogFileSize' which is REAL thing we should be saving here
		//		c> with the new automatic log-file size management, we don't need to save anything
		//
		//	the fix here is to prevent LGRestoreDBMSParams from restoring csecLGFile
		//	NOTE: we cannot remove csecLGFile from the save/restore structures because it would be format change

		/*	initialize constants again.
		/**/
		const INT csecLGFileSave = plog->m_csecLGFile;
		pinst->RestoreDBMSParams( &dbms_param );
		plog->m_csecLGFile = csecLGFileSave;
//		LGRestoreBFParams( &lgbf_param );

		CallJ( err, TermLG );

		/*  add the first log record
		/**/
		CallJ( ErrLGStart( pinst ), TermLG );

		// Ensure that all data is flushed.
		// Calling ErrLGFlushLog() may not flush all data in the log buffers.
		CallJ( plog->ErrLGWaitAllFlushed( pinst->m_pfsapi ), TermLG );

//		/*	initialize constants again.
//		/**/
//		CallJ( ErrITSetConstants( ), TermLG );
		}

	/*  initialize remaining system
	/**/
	CallJ( pinst->ErrINSTInit(), TermLG );

	Assert( !pinst->FRecovering() );
	Assert( !fJetLogGeneratedDuringSoftStart || !plog->m_fLogDisabled );

	/*	set up FMP from checkpoint.
	/**/
	if ( !plog->m_fLogDisabled )
		{
		CHAR	szPathJetChkLog[IFileSystemAPI::cchPathMax];

#ifdef DEBUG
#ifdef UNLIMITED_DB
		for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
			{
			Assert( ifmpMax == pinst->m_mpdbidifmp[ dbidT ] );
			}
#else
		BYTE	bAttachInfo;		//	shouldn't be any attachments, so one byte for sentinel is sufficient
		LGLoadAttachmentsFromFMP( pinst, &bAttachInfo );
		Assert( 0 == bAttachInfo );
#endif	//	UNLIMITED_DB
#endif	//	DEBUG
		
		plog->LGFullNameCheckpoint( pinst->m_pfsapi, szPathJetChkLog );
		err = plog->ErrLGReadCheckpoint( pinst->m_pfsapi, szPathJetChkLog, plog->m_pcheckpoint, fFalse );
		if ( JET_errCheckpointFileNotFound == err
			&& ( fNewCheckpointFile || fJetLogGeneratedDuringSoftStart ) )
			{
			//	could not locate checkpoint file, but we had previously
			//	deemed it necessary to create a new one (either because it
			//	was missing or because we're beginning from gen 1)
			plog->m_fLGFMPLoaded		= fTrue;
			(VOID) plog->ErrLGUpdateCheckpointFile( pinst->m_pfsapi, fTrue );
			if ( fJetLogGeneratedDuringSoftStart )
				{
				Assert( plog->m_plgfilehdr->lgfilehdr.le_lGeneration == 1 );
				plog->m_plgfilehdr->rgbAttach[0] = 0;
				CallJ( plog->ErrLGWriteFileHdr( plog->m_plgfilehdr ), TermIT );
				}
			}
		else
			{
			CallJ( err, TermIT );

			plog->m_logtimeFullBackup 	= plog->m_pcheckpoint->checkpoint.logtimeFullBackup;
			plog->m_lgposFullBackup 	= plog->m_pcheckpoint->checkpoint.le_lgposFullBackup;
			plog->m_logtimeIncBackup 	= plog->m_pcheckpoint->checkpoint.logtimeIncBackup;
			plog->m_lgposIncBackup 		= plog->m_pcheckpoint->checkpoint.le_lgposIncBackup;
			plog->m_fLGFMPLoaded		= fTrue;
			}

		extern PERFInstance<QWORD> cLGCheckpoint;
		cLGCheckpoint.Set( pinst, plog->CbOffsetLgpos( plog->m_pcheckpoint->checkpoint.le_lgposCheckpoint, lgposMin ) );
		err = JET_errSuccess;
		}
	return err;

TermIT:
	CallS( pinst->ErrINSTTerm( termtypeError ) );

TermLG:
	if ( fLGInitIsDone )
		{
		(VOID)plog->ErrLGTerm( pinst->m_pfsapi, fFalse /* do not flush log */ );
		}
	
	if ( fJetLogGeneratedDuringSoftStart )
		{
		CHAR	szLogName[ IFileSystemAPI::cchPathMax ];

		//	Instead of using m_szLogName (part of the shared-state monster),
		//	generate edb.log's filename now.
		plog->LGMakeLogName( szLogName, plog->m_szJet );
		(VOID)pinst->m_pfsapi->ErrFileDelete( szLogName );
		}

Uninitialize:

	g_lSessionsMac -= pinst->m_lSessionsMax;
	g_lVerPagesMac -= pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac -= pinst->m_lVerPagesPreferredMax;

	return err;
	}


ERR ISAMAPI ErrIsamTerm( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	ERR				err;
	INST 			* const pinst			= (INST *)instance;
	const BOOL		fInstanceUnavailable	= pinst->FInstanceUnavailable();
	const TERMTYPE	termtype				= ( fInstanceUnavailable ?
													termtypeError :
													( grbit & JET_bitTermAbrupt ? termtypeNoCleanUp : termtypeCleanUp ) );

	err = pinst->ErrINSTTerm( termtype );
	if ( pinst->m_fSTInit != fSTInitNotDone )
		{
		/*	before getting an error before reaching no-returning point in ITTerm().
		 */
		Assert( err < 0 );
		return err;
		}

	const ERR	errT	= pinst->m_plog->ErrLGTerm(
											pinst->m_pfsapi,
											( err >= JET_errSuccess && termtypeError != termtype ) );
	if ( err >= JET_errSuccess && errT < JET_errSuccess )
		{
		err = errT;
		}

	//	term the file-system

	delete pinst->m_pfsapi;
	pinst->m_pfsapi = NULL;

	// No need to display instance name and id in single instance mode
	if ( pinst->m_szInstanceName || pinst->m_szDisplayName ) // runInstModeMultiInst
		/*	write jet stop event */
		{
		if ( err >= JET_errSuccess && termtypeError != termtype )
			{
			CHAR		szT[8];
			const CHAR	*rgszT[1];

			Assert( !fInstanceUnavailable );
	
			sprintf( szT, "%d", IpinstFromPinst( pinst ) );
			rgszT[0] = szT;
			UtilReportEvent(
				eventInformation,
				GENERAL_CATEGORY,
				STOP_INSTANCE_ID,
				CArrayElements( rgszT ),
				rgszT,
				0,
				NULL,
				pinst );
			}
		else		
			{
			CHAR		sz1[8];
			CHAR		sz2[8];
			const CHAR	*rgszT[2];
			
			sprintf( sz1, "%d", IpinstFromPinst( pinst ) );
			sprintf( sz2, "%d", fInstanceUnavailable ? JET_errInstanceUnavailable : err );
			rgszT[0] = sz1;
			rgszT[1] = sz2;
			UtilReportEvent(
				eventInformation,
				GENERAL_CATEGORY,
				STOP_INSTANCE_ID_WITH_ERROR,
				CArrayElements( rgszT ),
				rgszT, 
				0,
				NULL,
				pinst );
			}
		}

	//	uninitialize the global variables

	g_lSessionsMac -= pinst->m_lSessionsMax;
	g_lVerPagesMac -= pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac -= pinst->m_lVerPagesPreferredMax;
	
	return err;
	}


#ifdef DEBUG
ERR ISAMAPI ErrIsamGetTransaction( JET_SESID vsesid, ULONG_PTR *plevel )
	{
	ERR		err = JET_errSuccess;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	*plevel = (ULONG_PTR)ppib->level;
	return err;
	}
#endif


//+api
//	ErrIsamBeginTransaction
//	=========================================================
//	ERR ErrIsamBeginTransaction( PIB *ppib )
//
//	Starts a transaction for the current user.  The user's transaction
//	level increases by one.
//
//	PARAMETERS	ppib 			pointer to PIB for user
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS	
//		The CSR stack for each active FUCB of this user is copied
//		to the new transaction level.
//
// SEE ALSO		ErrIsamCommitTransaction, ErrIsamRollback
//-
ERR ISAMAPI ErrIsamBeginTransaction( JET_SESID vsesid, JET_GRBIT grbit )
	{
	PIB		* ppib	= (PIB *)vsesid;
	ERR		err;

	CallR( ErrPIBCheck( ppib ) );
	Assert( ppibNil != ppib );

	if ( ppib->level < levelUserMost )
		{
		ppib->ptls = Ptls();

#ifdef DTC
		const JET_GRBIT		grbitsSupported		= ( JET_bitTransactionReadOnly|JET_bitDistributedTransaction );
#else
		const JET_GRBIT		grbitsSupported		= JET_bitTransactionReadOnly;
#endif
		err = ErrDIRBeginTransaction(
					ppib,
					( grbit & grbitsSupported ) );	//	filter out unsupported grbits
		}
	else
		{
		Assert( levelUserMost == ppib->level );
		err = ErrERRCheck( JET_errTransTooDeep );
		}

	return err;
	}


#ifdef DTC
ERR ISAMAPI ErrIsamPrepareToCommitTransaction(
	JET_SESID		sesid,
	const VOID		* const pvData,
	const ULONG		cbData )
	{
	ERR				err;
	PIB				* const ppib	= (PIB *)sesid;

	CallR( ErrPIBCheck( ppib ) );

	if ( 0 == ppib->level )
		err = ErrERRCheck( JET_errNotInTransaction );
	else if ( ppib->level > 1 )
		err = ErrERRCheck( JET_errMustCommitDistributedTransactionToLevel0 );
	else if ( !ppib->FDistributedTrx() )
		err = ErrERRCheck( JET_errNotInDistributedTransaction );
	else
		err = ErrDIRPrepareToCommitTransaction( ppib, pvData, cbData );

	return err;
	}
#endif	//	DTC

//+api
//	ErrIsamCommitTransaction
//	========================================================
//	ERR ErrIsamCommitTransaction( JET_SESID vsesid, JET_GRBIT grbit )
//
//	Commits the current transaction for this user.  The transaction level
//	for this user is decreased by the number of levels committed.
//
//	PARAMETERS	
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS
//		The CSR stack for each active FUCB of this user is copied
//		from the old ( higher ) transaction level to the new ( lower )
//		transaction level.
//
//	SEE ALSO	ErrIsamBeginTransaction, ErrIsamRollback
//-
ERR ISAMAPI ErrIsamCommitTransaction( JET_SESID vsesid, JET_GRBIT grbit )
	{
	ERR		err;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	//	may not be in a transaction, but wait for flush of previous
	//	lazy committed transactions.
	//
	if ( grbit & JET_bitWaitLastLevel0Commit )
		{
		//	no other grbits may be specified in conjunction with WaitLastLevel0Commit
		if ( JET_bitWaitLastLevel0Commit != grbit )
			{
			return ErrERRCheck( JET_errInvalidGrbit );
			}

		//	wait for last level 0 commit and rely on good user behavior
		//
		if ( CmpLgpos( &ppib->lgposCommit0, &lgposMax ) == 0 )
			{
			return JET_errSuccess;
			}

		LOG *plog = PinstFromPpib( ppib )->m_plog;
		err = plog->ErrLGWaitCommit0Flush( ppib );
		Assert( err >= 0 || plog->m_fLGNoMoreLogWrite );
		
		return err;
		}

	if ( ppib->level == 0 )
		{
		return ErrERRCheck( JET_errNotInTransaction );
		}

#ifdef DEBUG
	/*	disable lazy flush for debug build
	/**/
	grbit &= ~JET_bitCommitLazyFlush;
#endif

	err = ErrDIRCommitTransaction( ppib, grbit );

	return err;
	}


//+api
//	ErrIsamRollback
//	========================================================
//	ERR ErrIsamRollback( PIB *ppib, JET_GRBIT grbit )
//
//	Rolls back transactions for the current user.  The transaction level of
//	the current user is decreased by the number of levels aborted.
//
//	PARAMETERS	ppib		pointer to PIB for user
//				grbit		unused
//
//	RETURNS		
//		JET_errSuccess
//-
ERR ISAMAPI ErrIsamRollback( JET_SESID vsesid, JET_GRBIT grbit )
	{
	ERR   	 	err;
	PIB    		* ppib			= (PIB *)vsesid;
	FUCB   		* pfucb;
	FUCB   		* pfucbNext;

	/*	check session id before using it
	/**/
	CallR( ErrPIBCheck( ppib ) );
	
	if ( ppib->level == 0 )
		{
		return ErrERRCheck( JET_errNotInTransaction );
		}

	const LEVEL	levelRollback	= LEVEL( ppib->level - 1 );

	do
		{
		/*	get first primary index cusor
		/**/
		for ( pfucb = ppib->pfucbOfSession;
			pfucb != pfucbNil && FFUCBSecondary( pfucb );
			pfucb = pfucb->pfucbNextOfSession )
			NULL;

		/*	LOOP 1 -- first go through all open cursors, and close them
		/*	or reset secondary index cursors, if opened in transaction
		/*	rolled back.  Reset copy buffer status and move before first.
		/*	Some cursors will be fully closed, if they have not performed any
		/*	updates.  This will include secondary index cursors
		/*	attached to primary index cursors, so pfucbNext must
		/*	always be a primary index cursor, to ensure that it will
		/*	be valid for the next loop iteration.  Note that no information
		/*	necessary for subsequent rollback processing is lost, since
		/*	the cursors will only be released if they have performed no
		/*	updates including DDL.
		/**/
		for ( ; pfucb != pfucbNil; pfucb = pfucbNext )
			{
			/*	get next primary index cusor
			/**/
			for ( pfucbNext = pfucb->pfucbNextOfSession;
			  	pfucbNext != pfucbNil && FFUCBSecondary( pfucbNext );
			  	pfucbNext = pfucbNext->pfucbNextOfSession )
				NULL;

			/*	if defer closed then continue
			/**/
			if ( FFUCBDeferClosed( pfucb ) )
				continue;

			//	reset copy buffer status for each cursor on rollback
			if ( FFUCBUpdatePreparedLevel( pfucb, pfucb->ppib->level ) )
				{
				RECIFreeCopyBuffer( pfucb );
				FUCBResetUpdateFlags( pfucb );
				}
		
			/*	if current cursor is a table, and was opened in rolled back
			/*	transaction, then close cursor.
			/**/
			if ( FFUCBIndex( pfucb ) && pfucb->u.pfcb->FPrimaryIndex() )
				{
				if ( pfucb->levelOpen > levelRollback )
					{
					if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
						{
						CallS( ErrDispCloseTable( (JET_SESID)ppib, (JET_TABLEID) pfucb ) );
						}
					else
						{
						//	Open internally, not exported to user.
						CallS( ErrFILECloseTable( ppib, pfucb ) );
						}
					continue;
					}

				/*	if primary index cursor, and secondary index set
				/*	in rolled back transaction, then change index to primary
				/*	index.  This must be done, since secondary index
				/*	definition may be rolled back, if the index was created
				/*	in the rolled back transaction.
				/**/
				if ( pfucb->pfucbCurIndex != pfucbNil )
					{
					if ( pfucb->pfucbCurIndex->levelOpen > levelRollback )
						{
						CallS( ErrRECSetCurrentIndex( pfucb, NULL, NULL ) );
						}
					}
				}

			/*	if current cursor is a sort, and was opened in rolled back
			/*	transaction, then close cursor.
			/**/
			if ( FFUCBSort( pfucb ) )
				{
				if ( pfucb->levelOpen > levelRollback )
					{
					SORTClose( pfucb );
					continue;
					}
				}

			/*	if not sort and not index, and was opened in rolled back
			/*	transaction, then close DIR cursor directly.
			/**/
			if ( pfucb->levelOpen > levelRollback )
				{
				DIRClose( pfucb );
				continue;
				}
			}

		/*	call lower level abort routine
		/**/
		err = ErrDIRRollback( ppib );
		if ( JET_errRollbackError == err )
			{
#ifdef INDEPENDENT_DB_FAILURE			
			// this can happen only if run using g_fOneDatabasePerSession  
			Assert( g_fOneDatabasePerSession );

			const IFMP	ifmpError	= IfmpFirstDatabaseOpen( ppib );
			FMP::AssertVALIDIFMP( ifmpError );
			Assert( rgfmp[ifmpError].FAllowForceDetach() );
			err = ppib->m_errFatal;
#else
			err = ppib->ErrRollbackFailure();
#endif			
			
			// recover the error from rollback here
			// and return that one
			Assert( err < JET_errSuccess );
			}
		CallR( err );
		}
	while ( ( grbit & JET_bitRollbackAll ) != 0 && ppib->level > 0 );

	return JET_errSuccess;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\tagfld.cxx ===
#include "std.hxx"

INLINE VOID DeleteEntryAndData(
	BYTE		* const pbEntry,
	const ULONG	cbEntry,
	BYTE		* const pbData,
	const ULONG	cbData,
	BYTE		* const pbMax )
	{
	const BYTE		* const pbNextEntry		= pbEntry + cbEntry;

	UtilMemMove(
		pbEntry,
		pbNextEntry,
		pbMax - pbNextEntry );

	if( 0 != cbData )
		{
		//	we have already shifted the data down by cbEntry so pbNextEntry has changes

		const BYTE * const		pbMaxNew		= pbMax - cbEntry;
		BYTE * const 			pbDataNew		= pbData - cbEntry;
		const BYTE		* const pbNextData		= pbData + cbData - cbEntry;

		UtilMemMove( 
			pbDataNew,
			pbNextData,
			pbMaxNew - pbNextData );		
		}
	}


LOCAL VOID MULTIVALUES::AddInstance(
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	const ULONG			imvAdd				= CMultiValues();
	const ULONG			cMultiValuesCurr	= CMultiValues();
	const ULONG			cbMultiValuesCurr	= CbMultiValues();
	BYTE				* const pbMaxCurr	= PbMax();
	
	Assert( cMultiValuesCurr >= 2 );

	//	shift to make room for new MVOFFSET
	UtilMemMove(
		PbStartOfMVData() + sizeof(MVOFFSET),
		PbStartOfMVData(),
		PbMax() - PbStartOfMVData() );

	//	fix up offsets
	for ( ULONG imv = 0; imv < cMultiValuesCurr; imv++ )
		{
		DeltaIb( imv, sizeof(MVOFFSET) );
		}

	MVOFFSET	* const pmvoffAdd	= Rgmvoffs() + imvAdd;
	*pmvoffAdd = USHORT( sizeof(MVOFFSET) + cbMultiValuesCurr );		//	implicitly clears high bit
	if ( fSeparatedLV )
		{
		Assert( Pheader()->FColumnCanBeSeparated() );
		Assert( !fSeparatedLV || sizeof(LID) == pdataToSet->Cb() );
		SetFSeparatedInstance( imvAdd );
		UtilMemCpy(
			pbMaxCurr + sizeof(MVOFFSET),
			pdataToSet->Pv(),
			sizeof(LID) );
		}
	else
		{
		RECCopyData(
			pbMaxCurr + sizeof(MVOFFSET),
			pdataToSet,
			coltyp );
		}
		
	m_cMultiValues++;
	m_cbMultiValues += sizeof(MVOFFSET) + pdataToSet->Cb();
	}

LOCAL VOID MULTIVALUES::RemoveInstance( const ULONG itagSequence )
	{
	Assert( itagSequence > 0 );
	Assert( itagSequence <= CMultiValues() );
	Assert( CMultiValues() > 2 );

	const ULONG			imvDelete			= itagSequence - 1;
	const ULONG			cbDataDelete		= CbData( imvDelete );

	Assert( imvDelete < CMultiValues() );

	DeleteEntryAndData( 
		(BYTE *)( Rgmvoffs() + imvDelete ),
		sizeof(MVOFFSET),
		cbDataDelete > 0 ? PbData( imvDelete ) : NULL,
		cbDataDelete,
		PbMax() );


	//	update MULTIVALUES members to reflect deleted instance
	m_cbMultiValues -= sizeof(MVOFFSET) + cbDataDelete;
	m_cMultiValues--;

	//	update offsets
	ULONG imv;
	for ( imv = 0;
		imv < imvDelete;
		imv++ )
		{
		const INT	cbMVOffset	= sizeof(MVOFFSET);
		DeltaIb( imv, -cbMVOffset );
		}
	Assert( imvDelete == imv );
	for ( ;
		imv < CMultiValues();
		imv++ )
		{
		const SHORT	cbMVOffsetAndData		= SHORT( sizeof(MVOFFSET) + cbDataDelete );
		DeltaIb( imv, SHORT( -cbMVOffsetAndData ) );
		}
	}


LOCAL VOID MULTIVALUES::UpdateInstance(
	const ULONG			itagSequence,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	const ULONG			imvReplace				= itagSequence - 1;
	BYTE 				* const pbDataReplace	= PbData( imvReplace );
	const ULONG			cbDataReplace			= CbData( imvReplace );
	const INT			delta					= pdataToSet->Cb() - cbDataReplace;
	BYTE				* const pbDataNext		= pbDataReplace + cbDataReplace;

	Assert( itagSequence > 0 );
	Assert( itagSequence <= CMultiValues() );

	if ( 0 != delta )
		{
		//	shift data to accommodate updated instance
		UtilMemMove(
			pbDataReplace + pdataToSet->Cb(),
			pbDataNext,
			PbMax() - pbDataNext );

		//	update offsets to reflect shifted data
		for ( ULONG imv = imvReplace + 1; imv < CMultiValues(); imv++ )
			{
			DeltaIb( imv, (SHORT)delta );
			}
		}

	m_cbMultiValues += delta;
	
	if ( fSeparatedLV )
		{
		Assert( Pheader()->FColumnCanBeSeparated() );
		Assert( sizeof(LID) == pdataToSet->Cb() );
		SetFSeparatedInstance( imvReplace );
		UtilMemCpy(
			pbDataReplace,
			pdataToSet->Pv(),
			sizeof(LID) );
		}
	else
		{
		ResetFSeparatedInstance( imvReplace );
		RECCopyData(
			pbDataReplace,
			pdataToSet,
			coltyp );
		}
	}


LOCAL VOID TWOVALUES::UpdateInstance(
	const ULONG			itagSequence,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	Assert( !Pheader()->FColumnCanBeSeparated() );
	Assert( !Pheader()->FSeparated() );
	Assert( 1 == itagSequence || 2 == itagSequence );
	if ( 1 == itagSequence )
		{
		//	shift second value accordingly
		UtilMemMove(
			PbData() + pdataToSet->Cb(),
			PbData() + CbFirstValue(),
			CbSecondValue() );

		//	copy in updated value
		RECCopyData(
			PbData(),
			pdataToSet,
			coltyp );

		//	update length
		*m_pcbFirstValue = TVLENGTH( pdataToSet->Cb() );
		}
	else
		{
		RECCopyData(
			PbData() + CbFirstValue(),
			pdataToSet,
			coltyp );
		m_cbSecondValue = TVLENGTH( pdataToSet->Cb() );
		}
	}


LOCAL VOID TAGFIELDS::ConvertTwoValuesToMultiValues(
	TWOVALUES			* const ptv,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	TAGFLD_HEADER		* const pheader		= ptv->Pheader();
	const ULONG			cbFirstValue		= ptv->CbFirstValue();
	const ULONG			cbSecondValue		= ptv->CbSecondValue();
	BYTE				* const pbData		= ptv->PbData();

	Assert( !FRECLongValue( coltyp ) );
	Assert( !FRECSLV( coltyp ) );
	Assert( !pheader->FSeparated() );
	Assert( !pheader->FColumnCanBeSeparated() );

	//	must be adding a 3rd instance, so make room for the
	//	appropriate offsets array
	UtilMemMove(
		pbData + ( 3 * sizeof(MULTIVALUES::MVOFFSET) ) - sizeof(TWOVALUES::TVLENGTH),
		pbData,
		cbFirstValue + cbSecondValue );

	MULTIVALUES::MVOFFSET	* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)ptv->PcbFirstValue();

	rgmvoffs[0] = 3 * sizeof(MULTIVALUES::MVOFFSET);
	rgmvoffs[1] = USHORT( rgmvoffs[0] + cbFirstValue );
	rgmvoffs[2] = USHORT( rgmvoffs[1] + cbSecondValue );

	//	copy in new instance
	RECCopyData(
		(BYTE *)rgmvoffs + rgmvoffs[2],
		pdataToSet,
		coltyp );

	Assert( pheader->FMultiValues() );
	pheader->ResetFTwoValues();
	}


LOCAL VOID TAGFIELDS::InsertTagfld(
	const ULONG			itagfldInsert,
	TAGFLD				* const ptagfldInsert,
	const DATA			* const pdataToInsert,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	//	if pdataToInsert is NULL, we must be setting a column explicitly
	//	to NULL to override a default value
	Assert( ( ptagfldInsert->FNull() && NULL == pdataToInsert )
		|| ( !ptagfldInsert->FNull() && NULL != pdataToInsert ) );
	ULONG				cbDataToInsert;
	BYTE				* pbMoveFrom;

	if ( NULL == pdataToInsert )
		{
		cbDataToInsert = 0;
		}
	else
		{
		cbDataToInsert = pdataToInsert->Cb();
		if ( FRECLongValue( coltyp ) || FRECSLV( coltyp ) )
			{
			//	add header byte
			cbDataToInsert++;
			}
		}

	Assert( itagfldInsert <= CTaggedColumns() );

	if ( itagfldInsert < CTaggedColumns() )
		{
		const ULONG		ibMoveDataFrom	= Ptagfld( itagfldInsert )->Ib();

		//	make room for the new tagged data
		pbMoveFrom = PbData( itagfldInsert );
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD) + cbDataToInsert,
			pbMoveFrom,
			PbMax() - pbMoveFrom );

		//	make room for the new TAGFLD entry
		pbMoveFrom = (BYTE *)Ptagfld( itagfldInsert );
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD),
			pbMoveFrom,
			ibMoveDataFrom - ( itagfldInsert * sizeof(TAGFLD) ) );

		//	insert at the point we vacated, after making room for the new TAGFLD entry
		ptagfldInsert->SetIb( USHORT( ibMoveDataFrom + sizeof(TAGFLD) ) );
		}
	else if ( CTaggedColumns() > 0 )
		{
		//	append to the end, after making room the new TAGFLD entry
		pbMoveFrom = PbStartOfTaggedData();
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD),
			pbMoveFrom,
			CbTaggedData() );
		ptagfldInsert->SetIb( USHORT( CbTaggedColumns() + sizeof(TAGFLD) ) );
		}
	else
		{
		ptagfldInsert->SetIb( sizeof(TAGFLD) );
		}

	//	update TAGFIELD members to reflect new TAGFLD
	m_cbTaggedColumns += sizeof(TAGFLD) + cbDataToInsert;
	m_cTaggedColumns++;

	//	insert new data
	if ( cbDataToInsert > 0 )
		{
		BYTE	* pbInsert		= PbTaggedColumns() + ptagfldInsert->Ib();
			
		Assert( NULL != pdataToInsert );
		Assert( !ptagfldInsert->FNull() );
		if ( ptagfldInsert->FExtendedInfo() )
			{
			//	reserve byte for extended info
			new( (TAGFLD_HEADER *)pbInsert ) TAGFLD_HEADER( coltyp, fSeparatedLV );
			pbInsert++;
			}

		RECCopyData(
			pbInsert,
			pdataToInsert,
			coltyp );
		}

	//	insert new TAGFLD
	UtilMemCpy(
		Ptagfld( itagfldInsert ),
		ptagfldInsert,
		sizeof(TAGFLD) );
	Assert( Ptagfld( itagfldInsert )->Ib() >= ( sizeof(TAGFLD) * CTaggedColumns() ) );
	Assert( Ptagfld( itagfldInsert )->Ib() <= CbTaggedColumns() );

	//	just need to update tag array
	ULONG	itagfld;
	for ( itagfld = 0;
		itagfld < itagfldInsert;
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( sizeof(TAGFLD) );
		}
	for ( itagfld++;		//	skip inserted TAGFLD
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( SHORT( sizeof(TAGFLD) + cbDataToInsert ) );
		}
	}

LOCAL VOID TAGFIELDS::ResizeTagfld(
	const ULONG			itagfldResize,
	const INT			delta )
	{
	Assert( itagfldResize < CTaggedColumns() );
	Assert( 0 != delta );

	//	shift data after this column as needed
	//	to collapse space or make room
	if ( itagfldResize < CTaggedColumns() - 1 )
		{
		BYTE	* const pbMoveFrom	= PbData( itagfldResize+1 );
		UtilMemMove(
			pbMoveFrom + delta,
			pbMoveFrom,
			CbTaggedColumns() - Ptagfld( itagfldResize+1 )->Ib() );
		}

	//	update TAGFIELDS members to reflect new data
	m_cbTaggedColumns += delta;

	//	update offsets
	for ( ULONG itagfld = itagfldResize+1;
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( (SHORT)delta );
		}
	}

LOCAL VOID TAGFIELDS::ReplaceTagfldData(
	const ULONG			itagfldReplace,
	const DATA			* const pdataNew,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	TAGFLD				* const ptagfld		= Ptagfld( itagfldReplace );
	TAGFLD_HEADER		* pheader			= Pheader( itagfldReplace );
	BYTE				* pbData			= PbData( itagfldReplace );

	ptagfld->ResetFNull();

	if ( NULL != pheader )
		{
		Assert( CbData( itagfldReplace ) == pdataNew->Cb() + sizeof(TAGFLD_HEADER) );
		pbData += sizeof(TAGFLD_HEADER);
		if ( fSeparatedLV )
			{
			//	force fSeparated flag to be set
			Assert( pheader->FColumnCanBeSeparated() );
			pheader->SetFSeparated();
			}
		else
			{
			//	reset fSeparated flag
			pheader->ResetFSeparated();
			}
		}
	else
		{
		const BOOL	fNeedHeader		= ( FRECLongValue( coltyp ) || FRECSLV( coltyp ) );

		if ( fNeedHeader )
			{
			Assert( CbData( itagfldReplace ) == pdataNew->Cb() + sizeof(TAGFLD_HEADER) );
			pheader = (TAGFLD_HEADER *)pbData;
			new( pheader ) TAGFLD_HEADER( coltyp, fSeparatedLV );

			ptagfld->SetFExtendedInfo();
			pbData += sizeof(TAGFLD_HEADER);
			}
		else
			{
			Assert( CbData( itagfldReplace ) == pdataNew->Cb() );
#ifdef UNLIMITED_MULTIVALUES
			UNDONE: convert from intrinsic to separated
#else
			Assert( !FRECLongValue( coltyp ) );
			Assert( !FRECSLV( coltyp ) );
			Assert( !fSeparatedLV );
#endif
			}
		}

	RECCopyData( pbData, pdataNew, coltyp );
	}

LOCAL VOID TAGFIELDS::DeleteTagfld(
	const ULONG			itagfldDelete )
	{
	const ULONG			cbDataDelete		= CbData( itagfldDelete );

	Assert( itagfldDelete < CTaggedColumns() );

	DeleteEntryAndData(
		(BYTE *)Ptagfld( itagfldDelete ),
		sizeof(TAGFLD),
		cbDataDelete > 0 ? PbData( itagfldDelete ) : NULL,
		cbDataDelete,
		PbMax() );

	//	update TAGFLD members to reflect deleted TAGFLD
	m_cbTaggedColumns -= sizeof(TAGFLD) + cbDataDelete;
	m_cTaggedColumns--;

	//	update offsets
	ULONG itagfld;
	for ( itagfld = 0;
		itagfld < itagfldDelete;
		itagfld++ )
		{
		const INT	cbTagfld	= sizeof(TAGFLD);
		Ptagfld( itagfld )->DeltaIb( -cbTagfld );
		}
	Assert( itagfldDelete == itagfld );
	for ( ;
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		const SHORT	cbTagfldAndData		= SHORT( sizeof(TAGFLD) + cbDataDelete );
		Ptagfld( itagfld )->DeltaIb( SHORT( -cbTagfldAndData ) );
		}
	}


LOCAL VOID TAGFIELDS::ConvertToTwoValues(
	const ULONG			itagfld,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	Assert( itagfld < CTaggedColumns() );
	BYTE			* const pbData		= PbData( itagfld );
	const ULONG		cbDataCurr			= CbData( itagfld );
	Assert( cbDataCurr <= JET_cbColumnMost );		//	otherwise, it would have been LongText/Binary

	Assert( NULL != pdataToSet );
	ResizeTagfld(
		itagfld,
		sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) + pdataToSet->Cb() );

	//	make room for the header and cbFirstValue
	UtilMemMove(
		pbData + sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH),
		pbData,
		cbDataCurr );

	TAGFLD_HEADER		* const pheader			= (TAGFLD_HEADER *)pbData;
	TWOVALUES::TVLENGTH	* const pcbFirstValue	= (TWOVALUES::TVLENGTH *)( pbData + sizeof(TAGFLD_HEADER) );

	Assert( !FRECLongValue( coltyp ) );
	Assert( !FRECSLV( coltyp ) );
	new( pheader ) TAGFLD_HEADER( coltyp, fFalse );
	pheader->SetFMultiValues();
	pheader->SetFTwoValues();

	*pcbFirstValue = (TWOVALUES::TVLENGTH)cbDataCurr;

	RECCopyData( 
		(BYTE *)pcbFirstValue + sizeof(TWOVALUES::TVLENGTH) + cbDataCurr,
		pdataToSet,
		coltyp );

	Assert( CbData( itagfld ) ==
				sizeof(TAGFLD_HEADER)
				+ sizeof(TWOVALUES::TVLENGTH)
				+ cbDataCurr
				+ pdataToSet->Cb() );
	}

LOCAL VOID TAGFIELDS::ConvertToMultiValues(
	const ULONG			itagfld,
	const DATA			* const pdataToSet,
	const BOOL			fDataToSetIsSeparated )
	{
	Assert( itagfld < CTaggedColumns() );
	BYTE			* const pbData			= PbData( itagfld );
	const ULONG		cbDataCurr				= CbData( itagfld );
	TAGFLD_HEADER	* const pheader			= (TAGFLD_HEADER *)pbData;

	Assert( cbDataCurr >= sizeof(TAGFLD_HEADER) );
	const ULONG		cbDataCurrWithoutHeader	= cbDataCurr - sizeof(TAGFLD_HEADER);

	//	must already have a header (ie. either LongValue or SLV),
	//	just upgrade it to MultiValues
	Assert( NULL != Pheader( itagfld ) );
	Assert( pheader == Pheader( itagfld ) );
	Assert( pheader->FColumnCanBeSeparated() );
	const BOOL		fDataCurrIsSeparated	= pheader->FSeparated();
	pheader->ResetFSeparated();
	Assert( !pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );
	pheader->SetFMultiValues();

	Assert( NULL != pdataToSet );
	ResizeTagfld(
		itagfld,
		( 2 * sizeof(MULTIVALUES::MVOFFSET) ) + pdataToSet->Cb() );

	//	make room for the offset info
	UtilMemMove(
		pbData + sizeof(TAGFLD_HEADER) + ( 2 * sizeof(MULTIVALUES::MVOFFSET) ),
		pbData + sizeof(TAGFLD_HEADER),
		cbDataCurrWithoutHeader );

	MULTIVALUES::MVOFFSET	* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)( pbData + sizeof(TAGFLD_HEADER) );

	rgmvoffs[0] = ( 2 * sizeof(MULTIVALUES::MVOFFSET) );
	rgmvoffs[1] = USHORT( ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
						+ cbDataCurrWithoutHeader );

	//	must be long values, so no need to do endian conversion
//	RECCopyData( (BYTE *)rgmvoffs + rgmvoffs[1], pdataToSet, coltyp );
	Assert( !( rgmvoffs[0] & MULTIVALUES::maskFlags ) );
	Assert( !( rgmvoffs[1] & MULTIVALUES::maskFlags ) );
	UtilMemCpy(
		(BYTE *)rgmvoffs + rgmvoffs[1],
		pdataToSet->Pv(),
		pdataToSet->Cb() );

	if ( fDataCurrIsSeparated )
		{
		Assert( sizeof(LID) == cbDataCurrWithoutHeader );
		rgmvoffs[0] = USHORT( rgmvoffs[0] | MULTIVALUES::fSeparatedInstance );
		}
	if ( fDataToSetIsSeparated )
		{
		Assert( sizeof(LID) == pdataToSet->Cb() );
		rgmvoffs[1] = USHORT( rgmvoffs[1] | MULTIVALUES::fSeparatedInstance );
		}

	Assert( CbData( itagfld ) ==
				sizeof(TAGFLD_HEADER)
				+ ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
				+ cbDataCurrWithoutHeader
				+ pdataToSet->Cb() );
	}

LOCAL ULONG TAGFIELDS::CbConvertTwoValuesToSingleValue(
	const ULONG			itagfld,
	const ULONG			itagSequence )
	{
	ULONG				cbShrink		= 0;

#ifdef DEBUG
	Assert( itagfld < CTaggedColumns() );
	const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( pheader->FTwoValues() );
	Assert( !pheader->FLongValue() );
	Assert( !pheader->FSLV() );
#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !pheader->FSeparated() );
#endif	
#endif	

	if ( 1 == itagSequence || 2 == itagSequence )
		{
		BYTE			* const pbDataCurr		= PbData( itagfld );
		const ULONG		cbDataCurr				= CbData( itagfld );
		const BYTE		* pbDataRemaining;
		ULONG			cbDataRemaining;
		TWOVALUES		tv( pbDataCurr, cbDataCurr );

		if ( 1 == itagSequence )
			{
			//	remove first value, retain second value
	 		pbDataRemaining = tv.PbData() + tv.CbFirstValue();
			cbDataRemaining = tv.CbSecondValue();
			}
		else
			{
			//	remove second value, retain first value
			pbDataRemaining = tv.PbData();
			cbDataRemaining = tv.CbFirstValue();
			}

		Assert( cbDataCurr > cbDataRemaining );
		cbShrink	= cbDataCurr - cbDataRemaining;

		//	move remaining data to beginning of this TAGFLD<
		//	since we have no more need for the TAGFLD_HEADER
		UtilMemMove(
			pbDataCurr,
			pbDataRemaining,
			cbDataRemaining );

		//	shift rest of columns, if necessary
		if ( itagfld < CTaggedColumns() - 1 )
			{
			const BYTE	* const pbDataNextColumn	= PbData( itagfld+1 );
			UtilMemMove(
				pbDataCurr + cbDataRemaining,
				pbDataNextColumn,
				PbMax() - pbDataNextColumn );
			}

		//	clear flags
		Assert( Ptagfld( itagfld )->FExtendedInfo() );
		Ptagfld( itagfld )->ResetFExtendedInfo();

		//	update offsets
		for ( ULONG itagfldT = itagfld+1;
			itagfldT < CTaggedColumns();
			itagfldT++ )
			{
			const SHORT		cbT		= (SHORT)cbShrink;
			Ptagfld( itagfldT )->DeltaIb( SHORT( -cbT ) );
			}
		}
	else
		{
		//	appending NULL, which is a NOP
		}

	//	update size
	m_cbTaggedColumns -= cbShrink;

	return cbShrink;
	}

LOCAL ULONG TAGFIELDS::CbDeleteMultiValue(
	const ULONG		itagfld,
	const ULONG		itagSequence )
	{
	ULONG			cbShrink	= 0;
	MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );

#ifdef DEBUG
	Assert( itagfld < CTaggedColumns() );
	const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );
#endif

	if ( itagSequence > 0 && itagSequence <= mv.CMultiValues() )
		{
		TAGFLD_HEADER	* const pheader		= mv.Pheader();
		ULONG			cbDataRemaining;

		Assert( NULL != pheader );
		Assert( sizeof(TAGFLD_HEADER) + mv.CbMultiValues() == CbData( itagfld ) );

		if ( mv.CMultiValues() > 2 )
			{
			mv.RemoveInstance( itagSequence );
			cbDataRemaining = sizeof(TAGFLD_HEADER) + mv.CbMultiValues();
			}
		else
			{
			//	only one value will be left, so convert to non-multivalue
			Assert( 1 == itagSequence || 2 == itagSequence );
			const ULONG		imvRemaining		= itagSequence % 2;		//	calculate remaining itagSequence and convert to imv
			const BOOL		fSeparatedInstance	= mv.FSeparatedInstance( imvRemaining );
			BYTE			* const pbMoveFrom	= mv.PbData( imvRemaining );
			const ULONG		cbMove				= mv.CbData( imvRemaining );
			BYTE			* pbMoveTo;

			cbDataRemaining = cbMove;

#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !pheader->FSeparated() );
#endif				
			Assert( !pheader->FTwoValues() );
			pheader->ResetFMultiValues();
			if ( pheader->FNeedExtendedInfo() )
				{
#ifdef UNLIMITED_MULTIVALUES
				UNDONE: how to deal with one remaining
				multi-value (may or may not be separated)
				when the MULTIVALUES structure itself has
				been separated?
#else
				//	if remaining instance is separated,
				//	must now flag it as such in the
				//	extended info
				Assert( pheader->FColumnCanBeSeparated() );
				if ( fSeparatedInstance )
					pheader->SetFSeparated();
				else
					{
					Assert( !pheader->FSeparated() );		//	should already be unset, but better safe than sorry
					pheader->ResetFSeparated();
					}
#endif

				pbMoveTo = (BYTE *)pheader + sizeof(TAGFLD_HEADER);
				cbDataRemaining++;
				}
			else
				{
				//	if no other flags set, then can get rid of header byte
				Assert( !fSeparatedInstance );
				Ptagfld( itagfld )->ResetFExtendedInfo();
				pbMoveTo = (BYTE *)pheader;
				}
			Assert( pbMoveFrom > pbMoveTo );

			UtilMemMove(
				pbMoveTo,
				pbMoveFrom,
				cbMove );
			}

		Assert( cbDataRemaining < CbData( itagfld ) );
		cbShrink = CbData( itagfld ) - cbDataRemaining;

		//	shift rest of columns, if necessary
		if ( itagfld < CTaggedColumns() - 1 )
			{
			const BYTE	* const pbDataNextColumn	= PbData( itagfld+1 );
			UtilMemMove(
				(BYTE *)pheader + cbDataRemaining,
				pbDataNextColumn,
				PbMax() - pbDataNextColumn );
			}

		//	update size
		m_cbTaggedColumns -= cbShrink;

		//	update offsets
		for ( ULONG itagfldT = itagfld+1;
			itagfldT < CTaggedColumns();
			itagfldT++ )
			{
			const SHORT		cbT		= (SHORT)cbShrink;
			Ptagfld( itagfldT )->DeltaIb( SHORT( -cbT ) );
			}
		}

	else
		{
		//	appending NULL, which is a NOP
		}

	return cbShrink;
	}


LOCAL ERR TWOVALUES::ErrCheckUnique(
	const FIELD			* const pfield,
	const DATA&			dataToSet,
	const ULONG			itagSequence,
	const BOOL			fNormalizedDataToSetIsTruncated )
	{
	ERR					err;
	const BOOL			fNormalize		= ( pfieldNil != pfield );
	DATA				dataRec;

	Assert( !fNormalizedDataToSetIsTruncated || fNormalize );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !Pheader()->FSeparated() );
#endif	
	Assert( !Pheader()->FLongValue() );
	Assert( !Pheader()->FSLV() );

	if ( 1 != itagSequence )
		{
		dataRec.SetPv( PbData() );
		dataRec.SetCb( CbFirstValue() );
		CallR( ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					fNormalizedDataToSetIsTruncated ) );
		}

	if ( 2 != itagSequence )
		{
		dataRec.SetPv( PbData() + CbFirstValue() );
		dataRec.SetCb( CbSecondValue() );
		CallR( ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					fNormalizedDataToSetIsTruncated ) );
		}

	return JET_errSuccess;
	}

LOCAL ERR MULTIVALUES::ErrCheckUnique(
	const FIELD			* const pfield,
	const DATA&			dataToSet,
	const ULONG			itagSequence,
	const BOOL			fNormalizedDataToSetIsTruncated )
	{
	ERR					err;
	const BOOL			fNormalize		= ( pfieldNil != pfield );
	DATA				dataRec;
	ULONG				imv;

	Assert( !fNormalizedDataToSetIsTruncated || fNormalize );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !Pheader()->FSeparated() );
#endif	
	Assert( !Pheader()->FLongValue() );
	Assert( !Pheader()->FSLV() );

	for ( imv = 0; imv < CMultiValues(); imv++ )
		{
		Assert( !FSeparatedInstance( imv ) );
		if ( itagSequence != imv+1 )
			{
			dataRec.SetPv( PbData( imv ) );
			dataRec.SetCb( CbData( imv ) );
			CallR( ErrRECICheckUnique(
						pfield,
						dataToSet,
						dataRec,
						fNormalizedDataToSetIsTruncated ) );
			}
		}

	return JET_errSuccess;
	}

LOCAL ERR TAGFIELDS::ErrCheckUniqueMultiValues(
	const FIELD		* const pfield,
	const DATA&		dataToSet,
	const ULONG		itagfld,
	const ULONG		itagSequence,
	const BOOL		fNormalizedDataToSetIsTruncated )
	{
	ERR				err						= JET_errSuccess;

	Assert( !fNormalizedDataToSetIsTruncated || pfieldNil != pfield );
	Assert( !Ptagfld( itagfld )->FNull() );

	const TAGFLD_HEADER	* const pheader		= Pheader( itagfld );
	if ( NULL != pheader
		&& pheader->FMultiValues() )
		{
		Assert( !pheader->FSeparated() );
		Assert( !pheader->FLongValue() );			//	long values are handled in ErrFLDSetOneColumn()
		Assert( !pheader->FSLV() );
		if ( pheader->FTwoValues() )
			{
			TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
			err = tv.ErrCheckUnique(
							pfield,
							dataToSet,
							itagSequence,
							fNormalizedDataToSetIsTruncated );
			}
		else
			{
			MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
			err = mv.ErrCheckUnique(
							pfield,
							dataToSet,
							itagSequence,
							fNormalizedDataToSetIsTruncated );
			}
		}
	else if ( 1 != itagSequence )
		{
		DATA	dataRec;
		dataRec.SetPv( PbData( itagfld ) );
		dataRec.SetCb( CbData( itagfld ) );
		err = ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					fNormalizedDataToSetIsTruncated );
		}
	else
		{
		//	overwriting the only instance, so no need to check anything
		}

	return err;
	}


LOCAL ERR TAGFIELDS::ErrCheckUniqueNormalizedMultiValues(
	const FIELD		* const pfield,
	const DATA&		dataToSet,
	const ULONG		itagfld,
	const ULONG		itagSequence )
	{
	ERR				err;
	DATA			dataToSetNorm;
	BYTE			rgbNorm[KEY::cbKeyMax];
	BOOL			fNormalizedDataToSetIsTruncated;

	Assert( pfieldNil != pfield );

	dataToSetNorm.SetPv( rgbNorm );
	CallR( ErrFLDNormalizeTaggedData(
				pfield,
				dataToSet,
				dataToSetNorm,
				&fNormalizedDataToSetIsTruncated ) );

	CallR( ErrCheckUniqueMultiValues(
				pfield,
				dataToSetNorm,
				itagfld,
				itagSequence,
				fNormalizedDataToSetIsTruncated ) );

	return JET_errSuccess;
	}


ERR TAGFIELDS::ErrSetColumn(
	FUCB			* const pfucb,
	const FIELD		* const pfield,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const DATA		* const pdataToSet,
	const JET_GRBIT	grbit )
	{
	const FID		fid							= FidOfColumnid( columnid );
	const ULONG		cbRec						= pfucb->dataWorkBuf.Cb();
	const BOOL		fUseDerivedBit				= ( grbit & grbitSetColumnUseDerivedBit );
	const BOOL		fEnforceUniqueMultiValues	= ( ( grbit & ( JET_bitSetUniqueMultiValues|JET_bitSetUniqueNormalizedMultiValues ) )
													&& NULL != pdataToSet
													&& !FRECLongValue( pfield->coltyp )	//	long value uniqueness is checked in ErrFLDSetOneColumn()
													&& !FRECSLV( pfield->coltyp ) );

	Assert( ptdbNil != pfucb->u.pfcb->Ptdb() );
	AssertValid( pfucb->u.pfcb->Ptdb() );

	TAGFLD			tagfldNew( fid, fUseDerivedBit );
	const ULONG		itagfld			= ItagfldFind( tagfldNew );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	const BOOL		fExists			= ( itagfld < CTaggedColumns()
										&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
	Assert( fExists
		|| itagfld == CTaggedColumns()
		|| Ptagfld( itagfld )->FIsGreaterThan( fid, fUseDerivedBit ) );

	if ( !fExists )
		{
		//	Specified column not found, so insert or append new
		ULONG 	cbField;

		//	Adding NULL: In most cases, we do nothing.  However, there
		//	is one specialised case where we have to insert a null entry.
		//	This is the case where there are currently no instances of this
		//	column in the record, and where there is also a default value
		//	for this column.
		//
		if ( pdataToSet == NULL )
			{
			if ( FFIELDDefault( pfield->ffield )
				&& 1 == itagSequence
				&& !( grbit & JET_bitSetRevertToDefaultValue ) )
				{
				tagfldNew.SetFNull();
				cbField = 0;
				}
			else
				return JET_errSuccess;
			}
		else
			{
			cbField = pdataToSet->Cb();

			Assert( !tagfldNew.FExtendedInfo() );
			if ( FRECLongValue( pfield->coltyp ) || FRECSLV( pfield->coltyp ) )
				{
				cbField++;
				tagfldNew.SetFExtendedInfo();
				}
			else
				{
				Assert( cbField <= JET_cbColumnMost );
				Assert( !( grbit & grbitSetColumnSeparated ) );
				}
			}

		//	will column fit?
		//
		if ( cbRec + sizeof(TAGFLD) + cbField > cbRECRecordMost )
			return ErrERRCheck( JET_errRecordTooBig );

		InsertTagfld(
			itagfld,
			&tagfldNew,
			pdataToSet,
			pfield->coltyp,
			grbit & grbitSetColumnSeparated );

		pfucb->dataWorkBuf.DeltaCb( sizeof(TAGFLD) + cbField );
		}

	else if ( pdataToSet != NULL )				// Overwrite with a non-null value.
		{
#ifdef DEBUG		
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( FFIELDDefault( pfield->ffield ) );
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( 0 == CbData( itagfld ) );
			}
#endif

		if ( fEnforceUniqueMultiValues && !Ptagfld( itagfld )->FNull() )
			{
			ERR		errT;
			Assert( !FRECLongValue( pfield->coltyp ) );		//	long values are handled in ErrFLDSetOneColumn()
			Assert( !FRECSLV( pfield->coltyp ) );
			if ( grbit & JET_bitSetUniqueNormalizedMultiValues )
				{
				errT = ErrCheckUniqueNormalizedMultiValues(
								pfield,
								*pdataToSet,
								itagfld,
								itagSequence );
				}
			else
				{
				errT = ErrCheckUniqueMultiValues(
								pfieldNil,			//	not normalising, so don't need pfield
								*pdataToSet,
								itagfld,
								itagSequence,
								fFalse );
				}
			if ( errT < 0 )
				return errT;
			}

		Assert( FRECLongValue( pfield->coltyp )
			|| FRECSLV( pfield->coltyp )
			|| pdataToSet->Cb() <= JET_cbColumnMost );

		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			INT			delta		= pdataToSet->Cb();
			Assert( !pheader->FSeparated() );
			if ( pheader->FTwoValues() )
				{
				TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );

				Assert( !pheader->FColumnCanBeSeparated() );
				Assert( !( grbit & grbitSetColumnSeparated ) );
				if ( 1 == itagSequence || 2 == itagSequence )
					{
					delta -= ( 1 == itagSequence ? tv.CbFirstValue() : tv.CbSecondValue() );
					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	if value is growing, must make room for it
					//	if shrinking, cannot resize until after update is done
					if ( delta > 0 )
						ResizeTagfld( itagfld, delta );

					tv.UpdateInstance(
							itagSequence,
							pdataToSet,
							pfield->coltyp );

					if ( delta < 0 )
						ResizeTagfld( itagfld, delta );

					Assert( sizeof(TAGFLD_HEADER)
								+ sizeof(TWOVALUES::TVLENGTH)
								+ tv.CbFirstValue()
								+ tv.CbSecondValue()
							== CbData( itagfld ) );
					}
				else
					{
					//	adding a new instance, so must convert to MULTIVALUES
					delta += ( ( 3 * sizeof(MULTIVALUES::MVOFFSET) ) - sizeof(TWOVALUES::TVLENGTH) );
					Assert( delta > 0 );

					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	first make room for the new data
					ResizeTagfld( itagfld, delta );
					
					ConvertTwoValuesToMultiValues(
							&tv,
							pdataToSet,
							pfield->coltyp );
					}
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );

				if ( 0 == itagSequence || itagSequence > mv.CMultiValues() )
					{
					//	adding a new instance, so must convert to MULTIVALUES
					delta += sizeof(MULTIVALUES::MVOFFSET);
					Assert( delta > 0 );

					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					ResizeTagfld( itagfld, delta );
					mv.AddInstance(
							pdataToSet,
							pfield->coltyp,
							grbit & grbitSetColumnSeparated );

					}
				else
					{
					delta -= mv.CbData( itagSequence-1 );
					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	if value is growing, must make room for it
					//	if shrinking, cannot resize until after update is done
					if ( delta > 0 )
						ResizeTagfld( itagfld, delta );

					mv.UpdateInstance(
							itagSequence,
							pdataToSet,
							pfield->coltyp,
							grbit & grbitSetColumnSeparated );

					if ( delta < 0 )
						ResizeTagfld( itagfld, delta );
					}

				Assert( sizeof(TAGFLD_HEADER) + mv.CbMultiValues() == CbData( itagfld ) );
				}

			pfucb->dataWorkBuf.DeltaCb( delta );
			}

		else if ( 1 == itagSequence || Ptagfld( itagfld )->FNull() )
			{
			//	overwrite with non-NULL value: have to shift record data
			//	Compute change in column size.
			//
			const ULONG		cbTagField		= CbData( itagfld );
			INT				dbFieldData		= pdataToSet->Cb() - cbTagField;

			//	this column will no longer be NULL
			
			Ptagfld( itagfld )->ResetFNull();

			if ( FRECLongValue( pfield->coltyp ) || FRECSLV( pfield->coltyp ) )
				{
				//	need header byte
				dbFieldData += sizeof(TAGFLD_HEADER);
				}
			else
				{
				Assert( !Ptagfld( itagfld )->FExtendedInfo() );
				Assert( NULL == pheader );
				}

			if ( cbRec + dbFieldData > cbRECRecordMost )
				return ErrERRCheck( JET_errRecordTooBig );

			if ( 0 != dbFieldData )
				ResizeTagfld( itagfld, dbFieldData );

			ReplaceTagfldData(
				itagfld,
				pdataToSet,
				pfield->coltyp,
				grbit & grbitSetColumnSeparated );

			pfucb->dataWorkBuf.DeltaCb( dbFieldData );
			}
		else
			{
			ULONG	cbGrow;

			//	adding a second instance, so must convert to TWOVALUES/MULTIVALUES
			if ( NULL != pheader )
				{
				Assert( FRECLongValue( pfield->coltyp )
					|| FRECSLV( pfield->coltyp ) );
				Assert( pheader->FColumnCanBeSeparated() );
				cbGrow = ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
							+ pdataToSet->Cb();
				if ( cbRec + cbGrow > cbRECRecordMost )
					return ErrERRCheck( JET_errRecordTooBig );

				//	first make room for new data
				ConvertToMultiValues(
						itagfld,
						pdataToSet,
						grbit & grbitSetColumnSeparated );
				}
			else
				{
				Assert( !FRECLongValue( pfield->coltyp ) );
				Assert( !FRECSLV( pfield->coltyp ) );
				cbGrow = sizeof(TAGFLD_HEADER)
							+ sizeof(TWOVALUES::TVLENGTH)
							+ pdataToSet->Cb();
				if ( cbRec + cbGrow > cbRECRecordMost )
					return ErrERRCheck( JET_errRecordTooBig );

				//	first make room for new data
				ConvertToTwoValues(
					itagfld,
					pdataToSet,
					pfield->coltyp );
				Assert( !Ptagfld( itagfld )->FExtendedInfo() );
				Ptagfld( itagfld )->SetFExtendedInfo();
				}

			pfucb->dataWorkBuf.DeltaCb( cbGrow );
			}
		}

	else if ( !Ptagfld( itagfld )->FNull() )	// Overwriting non-null with null
		{
		// Ensure that we've found a field
		//
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );

		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			const ULONG cbDataOld	= CbData( itagfld );
			const ULONG	cbShrink	= ( pheader->FTwoValues() ?
												CbConvertTwoValuesToSingleValue( itagfld, itagSequence ) :
												CbDeleteMultiValue( itagfld, itagSequence ) );
			Assert( cbShrink < cbDataOld );
			pfucb->dataWorkBuf.DeltaCb( -cbShrink );
			}

		else if ( 1 == itagSequence )
			{
			//	Overwrite with NULL: In most cases, just delete the occurrence from
			//	the record.  However, there is one rare case where we have to
			//	leave behind a null entry.  This is the case where there are no
			//	other instances of this column for this record, and where this
			//	column has a default value.
			const ULONG		cbTagField		= CbData( itagfld );
			if ( FFIELDDefault( pfield->ffield )
				&& !( grbit & JET_bitSetRevertToDefaultValue ) )
				{
				if ( cbTagField > 0 )
					ResizeTagfld( itagfld, -cbTagField );

				Ptagfld( itagfld )->ResetFExtendedInfo();
				Ptagfld( itagfld )->SetFNull();
				
				pfucb->dataWorkBuf.DeltaCb( -cbTagField );
				}
			else
				{
				DeleteTagfld( itagfld );
				pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) + cbTagField ) );
				}
			}
		else
			{
			//	appending NULL, which is a NOP
			}
		}

	else									// Overwriting null with null.  Either revert to default or do nothing.
		{
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
		Assert( FFIELDDefault( pfield->ffield ) );
		Assert( !Ptagfld( itagfld )->FExtendedInfo() );
		Assert( 0 == CbData( itagfld ) );

		if ( grbit & JET_bitSetRevertToDefaultValue )
			{
			DeleteTagfld( itagfld );
			pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) ) );
			}
		}

#ifdef DEBUG
	const REC	* prec						= (REC *)pfucb->dataWorkBuf.Pv();
	const BYTE	* pbRecMax					= (BYTE *)prec + pfucb->dataWorkBuf.Cb();
	const BYTE	* pbStartOfTaggedColumns	= prec->PbTaggedData();

	Assert( pbStartOfTaggedColumns <= pbRecMax );
	Assert( (BYTE *)Rgtagfld() == pbStartOfTaggedColumns );
	Assert( pbStartOfTaggedColumns + CbTaggedColumns() == pbRecMax );
	AssertValid( pfucb->u.pfcb->Ptdb() );
#endif	

	return JET_errSuccess;
	}


ERR TAGFIELDS::ErrRetrieveColumn(
	FCB				* const pfcb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const DATA&		dataRec,
	DATA			* const pdataRetrieveBuffer,
	const ULONG		grbit )
	{
	const TDB		* const ptdb		= ( pfcbNil == pfcb ? ptdbNil : pfcb->Ptdb() );
	const FID		fid					= FidOfColumnid( columnid );
	const BOOL		fUseDerivedBit		= ( grbit & grbitRetrieveColumnUseDerivedBit );
	ULONG			itagfld;

#ifdef DEBUG
	const BOOL		fUseDMLLatchDBG		= ( fid > ptdb->FidTaggedLastInitial()
											&& ( grbit & grbitRetrieveColumnDDLNotLocked ) );

	if ( pfcbNil == pfcb )
		{
		//	don't need any meta data info if we're not retrieving default values
		Assert( grbit & JET_bitRetrieveIgnoreDefault );
		}
	else
		{
		Assert( ptdbNil != ptdb );

		Assert( fid >= ptdb->FidTaggedFirst() );
		Assert( fid <= ptdb->FidTaggedLast() );

		AssertValid( ptdb );

		// RECIAccessColumn() should have already been called to verify FID.
		if ( fUseDMLLatchDBG )
			pfcb->EnterDML();
		Assert( fid <= ptdb->FidTaggedLast() );
		Assert( JET_coltypNil != ptdb->PfieldTagged( columnid )->coltyp
			|| ( grbit & grbitRetrieveColumnReadSLVInfo ) );
		if ( fUseDMLLatchDBG )
			pfcb->LeaveDML();
		}
#endif

	TAGFLD			tagfldRetrieve( fid, fUseDerivedBit );

	itagfld = ItagfldFind( tagfldRetrieve );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	if ( itagfld < CTaggedColumns()
		&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( NULL == pheader );
			Assert( 0 == CbData( itagfld ) );
#ifdef DEBUG
			if ( pfcbNil != pfcb )
				{
				if ( fUseDMLLatchDBG )
					pfcb->EnterDML();
				Assert( FFIELDDefault( ptdb->PfieldTagged( columnid )->ffield ) );
				if ( fUseDMLLatchDBG )
					pfcb->LeaveDML();
				}
#endif				
			}

		else if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			Assert( Ptagfld( itagfld )->FExtendedInfo() );
			Assert( itagSequence > 0 );

			if ( pheader->FTwoValues() )
				{
				if ( itagSequence <= 2 )
					{
					TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
					tv.RetrieveInstance( itagSequence, pdataRetrieveBuffer );
					return JET_errSuccess;
					}
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				if ( itagSequence <= mv.CMultiValues() )
					{
					return mv.ErrRetrieveInstance( itagSequence, pdataRetrieveBuffer );
					}
				}

			// If we reached here, our desired occurrence is not in the
			// record.  Fall through to NullField.
			}

		else if ( 1 == itagSequence )
			{
			pdataRetrieveBuffer->SetPv( PbData( itagfld ) );
			pdataRetrieveBuffer->SetCb( CbData( itagfld ) );

			if ( NULL != pheader )
				{
				Assert( Ptagfld( itagfld )->FExtendedInfo() );
				const INT	iDelta	= sizeof(TAGFLD_HEADER);
				pdataRetrieveBuffer->DeltaPv( iDelta );
				pdataRetrieveBuffer->DeltaCb( -iDelta );
				return pheader->ErrRetrievalResult();
				}
			else
				{
				return JET_errSuccess;
				}
			}

		else
			{
			//	non-existent itagSequence, so return NULL
			}
		}

	else if ( !( grbit & JET_bitRetrieveIgnoreDefault ) && 1 == itagSequence && ptdb->FTableHasNonEscrowDefault() )
		{
		Assert( pfcbNil != pfcb );
		Assert( ptdbNil != ptdb );

		Assert( !( grbit & grbitRetrieveColumnReadSLVInfo ) );

		const BOOL	fUseDMLLatch	= ( FidOfColumnid( columnid ) > ptdb->FidTaggedLastInitial()
										&& ( grbit & grbitRetrieveColumnDDLNotLocked ) );

		if ( fUseDMLLatch )
			pfcb->EnterDML();
		
		//	assert no infinite recursion
		Assert( dataRec.Pv() != ptdb->PdataDefaultRecord() );

		const FIELDFLAG	ffield	= ptdb->PfieldTagged( columnid )->ffield;
		if ( FFIELDUserDefinedDefault( ffield ) )
			{
			Assert( FFIELDDefault( ffield ) );

			//	no occurrrences found, but a user-defined default value
			//	exists and we are retrieving first occcurence.
			
			if ( fUseDMLLatch )
				pfcb->LeaveDML();

			pdataRetrieveBuffer->Nullify();
			return ErrERRCheck( wrnRECUserDefinedDefault );
			}

		else if ( FFIELDDefault( ffield ) )
			{
			//	no occurrrences found, but a default value exists and
			//	we are retrieving first occcurence.
			const ERR	errT	= ErrRECIRetrieveTaggedDefaultValue(
										pfcb,
										columnid,
										pdataRetrieveBuffer );

			if ( fUseDMLLatch )
				pfcb->LeaveDML();
			
			return errT;			
			}
			
		if ( fUseDMLLatch )
			pfcb->LeaveDML();
		}
		
	//	null column common exit point
	//
	pdataRetrieveBuffer->Nullify();
	return ErrERRCheck( JET_wrnColumnNull );
	}


ULONG TAGFIELDS::UlColumnInstances(
	FCB				* const pfcb,
	const COLUMNID	columnid,
	const BOOL		fUseDerivedBit )
	{
	const FID		fid					= FidOfColumnid( columnid );
	ULONG			itagfld;

#ifdef DEBUG
	Assert( pfcbNil != pfcb );
	
	const TDB		* const ptdb		= pfcb->Ptdb();
	Assert( ptdbNil != ptdb );

	const BOOL		fUseDMLLatchDBG		= ( fid > ptdb->FidTaggedLastInitial() );

	if ( fUseDMLLatchDBG )
		pfcb->EnterDML();

	// RECIAccessColumn() should have already been called to verify FID.
	Assert( fid >= ptdb->FidTaggedFirst() );
	Assert( fid <= ptdb->FidTaggedLast() );

	AssertValid( ptdb );

	const FIELD	*pfield = ptdb->PfieldTagged( columnid );
	Assert( pfieldNil != pfield );
	Assert( JET_coltypNil != pfield->coltyp );

	const BOOL	fDefaultValue = FFIELDDefault( pfield->ffield );

	if ( fUseDMLLatchDBG )
		pfcb->LeaveDML();
#endif

	TAGFLD			tagfldRetrieve( fid, fUseDerivedBit );
	ULONG			ulInstances			= 0;

	itagfld = ItagfldFind( tagfldRetrieve );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	if ( itagfld < CTaggedColumns()
		&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( NULL == pheader );
			Assert( 0 == CbData( itagfld ) );
			Assert( 0 == ulInstances );
			Assert( fDefaultValue );
			}

		else if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			Assert( Ptagfld( itagfld )->FExtendedInfo() );
			if ( pheader->FTwoValues() )
				{
				Assert( !pheader->FColumnCanBeSeparated() );
				ulInstances = 2;
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ulInstances = mv.CMultiValues();
				}
			}

		else
			{
			ulInstances = 1;
			}
		}

	else
		{
		const TDB * const	ptdb			= pfcb->Ptdb();
		const BOOL			fUseDMLLatch	= ( fid > ptdb->FidTaggedLastInitial() );

		if ( fUseDMLLatch )
			pfcb->EnterDML();

		if ( FFIELDDefault( pfcb->Ptdb()->PfieldTagged( columnid )->ffield ) )
			{
			//	no occurrrences found, but a default value exists
			//
			ulInstances = 1;
			Assert( fDefaultValue );
			}
		else
			{
			Assert( 0 == ulInstances );
			Assert( !fDefaultValue );
			}

		if ( fUseDMLLatch )
			pfcb->LeaveDML();
		}
		
	return ulInstances;
	}


ERR TAGFIELDS::ErrScan(
	FUCB			* const pfucb,
	const ULONG		itagSequence,
	const DATA&		dataRec,
	DATA			* const pdataField,
	COLUMNID		* const pcolumnidRetrieved,
	ULONG			* const pitagSequenceRetrieved,	
	const JET_GRBIT	grbit )
	{
	ERR				err;
	FCB				* const pfcb		= pfucb->u.pfcb;
	const BOOL		fRetrieveNulls		= ( grbit & JET_bitRetrieveNull );
	const BOOL		fRefreshNeeded		= ( dataRec.Pv() == pfucb->kdfCurr.data.Pv() );
	
	if ( fRefreshNeeded )
		{
		Assert( dataRec.Cb() == pfucb->kdfCurr.data.Cb() );
		
		// Only need to refresh ptagfld if we're accessing the database,
		// in which case we must have the page latched.  Note that we
		// may have a page latched, but we may not be accessing the
		// the database (ie. no refresh needed).
		Assert( Pcsr( pfucb )->FLatched() );
		}

	// Verify we're in a transaction in order to ensure read-consistency
	// in case we have a page latch and need to release it while scanning/
	Assert( pfucb->ppib->level > 0 );
	Assert( pfcb != pfcbNil );
	Assert( pdataField != NULL );
	Assert( pcolumnidRetrieved != NULL );

	// If itagSequence == 0, then we're counting the number of tagged columns
	// in the record and we will output the result in pitagSequenceRetrieved.
	Assert( itagSequence != 0 || pitagSequenceRetrieved != NULL );

	Assert( pfcb->Ptdb() != ptdbNil );
	const TDB		* const ptdb		= pfcb->Ptdb();
	AssertValid( ptdb );

	const BOOL		fRetrieveDefaults 	= ( !( grbit & JET_bitRetrieveIgnoreDefault )
											&& ptdb->FTableHasNonEscrowDefault() );
	ULONG			itagfld;
	ULONG			ulNumOccurrences	= 0;
	COLUMNID		columnidCurr		= ColumnidRECFirstTaggedForScan( ptdb );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		Assert( FCOLUMNIDTagged( columnidCurr ) );

#ifdef DEBUG
#else
		if ( fGlobalRepair )
#endif		
			{
			const TAGFLD	* ptagfldT			= Ptagfld( itagfld );
			BOOL			fBadColumn;
			if ( ptagfldT->FDerived() )
				{
				const FCB	* const pfcbTemplate	= ptdb->PfcbTemplateTable();
				fBadColumn = ( pfcbNil == pfcbTemplate
							|| ptagfldT->Fid() > pfcbTemplate->Ptdb()->FidTaggedLast()
							|| ptagfldT->Fid() < pfcbTemplate->Ptdb()->FidTaggedFirst() );
				}
			else
				{
				fBadColumn = ( ptagfldT->Fid() > ptdb->FidTaggedLast()
							|| ptagfldT->Fid() < ptdb->FidTaggedFirst() );
				}
			if ( fBadColumn )
				{
				//	log event
				//
				AssertTracking();
				UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_COLUMN_ID, 0, NULL );
				break;
				}
			}


		// Check for any "gaps" caused by default values (if we
		// want default values retrieved).
		if ( fRetrieveDefaults )
			{
			//	make copy of tagfld so we don't have to worry about losing the page
			//	latch on column access check
			const TAGFLD	* ptagfldT			= Ptagfld( itagfld );
			TAGFLD			tagfldT( ptagfldT->Fid(), ptagfldT->FDerived() );

			for( ;
				tagfldT.FIsGreaterThan( columnidCurr, ptdb );
				columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr ) )
				{
				FCB		*pfcbT		= pfcb;

				Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

				if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
					{
					if ( pfcbNil != ptdb->PfcbTemplateTable() )
						{
						ptdb->AssertValidDerivedTable();
						pfcbT = ptdb->PfcbTemplateTable();
						}
					else
						{
						ptdb->AssertValidTemplateTable();
						}
					}
				else
					{
					err = ErrRECIAccessColumn( pfucb, columnidCurr );
					if ( err < 0 )
						{
						if ( JET_errColumnNotFound == err )
							continue;
						return err;
						}
					}

				const TDB *	const	ptdbT			= pfcbT->Ptdb();
				const BOOL			fUseDMLLatch	= ( FidOfColumnid( columnidCurr ) > ptdbT->FidTaggedLastInitial() );

				if ( fUseDMLLatch )
					pfcbT->EnterDML();

				Assert( JET_coltypNil != ptdbT->PfieldTagged( columnidCurr )->coltyp );
				const FIELDFLAG		ffield			= ptdbT->PfieldTagged( columnidCurr )->ffield;

				if ( FFIELDUserDefinedDefault( ffield ) )
					{
					if ( ++ulNumOccurrences == itagSequence )
						{
						Assert( itagSequence != 0 );
						*pcolumnidRetrieved = columnidCurr;
						if ( pitagSequenceRetrieved != NULL )
							*pitagSequenceRetrieved = 1;

						//	assert no infinite recursion
						Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );

						if ( fUseDMLLatch )
							pfcbT->LeaveDML();
						return ErrERRCheck( wrnRECUserDefinedDefault );
						}
					}
				else if ( FFIELDDefault( ffield ) )
					{
					if ( ++ulNumOccurrences == itagSequence )
						{
						Assert( itagSequence != 0 );
						*pcolumnidRetrieved = columnidCurr;
						if ( pitagSequenceRetrieved != NULL )
							*pitagSequenceRetrieved = 1;

						//	assert no infinite recursion
						Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );
						err = ErrRECIRetrieveTaggedDefaultValue( pfcbT, columnidCurr, pdataField );

						if ( fUseDMLLatch )
							pfcbT->LeaveDML();
						return err;
						}
					}

				if ( fUseDMLLatch )
					pfcbT->LeaveDML();
				}

			Assert( tagfldT.FIsEqual( columnidCurr, ptdb ) );
			}
		else
			{
			columnidCurr = Ptagfld( itagfld )->Columnid( ptdb );
			}

		if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
			{
#ifdef DEBUG
			DATA	dataSav;

			if ( fRefreshNeeded )
				{
				dataSav.SetPv( pfucb->kdfCurr.data.Pv() );
				dataSav.SetCb( pfucb->kdfCurr.data.Cb() );
				}

			CallS( ErrRECIAccessColumn( pfucb, columnidCurr ) );

			if ( fRefreshNeeded )
				{
				//	verify pointers didn't change - we should not lose latch
				//	because we shouldn't have to consult catalog
				Assert(	pfucb->kdfCurr.data == dataSav );
				}
#endif
			//	template column obtained from either TDB or from record,
			//	so it must exist
			err = JET_errSuccess;
			}
		else
			{
			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 && JET_errColumnNotFound != err )
				return err;

			if ( fRefreshNeeded )
				{
				// We may have invalidated our pointer if we had to give
				// up the latch. Force refresh.
				Refresh( pfucb->kdfCurr.data );
				}
			}

		Assert( Ptagfld( itagfld )->FIsEqual( columnidCurr, ptdb ) );

		CallSx( err, JET_errColumnNotFound );
		if ( JET_errColumnNotFound == err )
			{
			// Column not visible to this session.  Skip to next one.
			}

		else if ( Ptagfld( itagfld )->FNull() )
			{
			// If there's an explicit null entry, it should be the only
			// occurrence of this fid.  Also the only reason for an explict
			// null entry is to override a default value.
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( 0 == CbData( itagfld ) );
			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );
#ifdef DEBUG
			pfcb->EnterDML();
			Assert( FFIELDDefault( ptdb->PfieldTagged( columnidCurr )->ffield ) );
			pfcb->LeaveDML();
#endif

			// Only count columns explicitly set to null if the RetrieveNulls
			// flag is passed.  Otherwise, just skip it.
			if ( fRetrieveNulls && ++ulNumOccurrences == itagSequence )
				{
				Assert( itagSequence != 0 );
				*pcolumnidRetrieved = columnidCurr;
				if ( pitagSequenceRetrieved != NULL )
					*pitagSequenceRetrieved = 1;

				pdataField->Nullify();
				return ErrERRCheck( JET_wrnColumnSetNull );
				}
			}

		else
			{
			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );
			const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
			if ( NULL != pheader
				&& pheader->FMultiValues() )
				{
				const ULONG		itagSequenceToRetrieve	= ( 0 == itagSequence ? 0 : itagSequence - ulNumOccurrences );
				if ( pheader->FTwoValues() )
					{
					ulNumOccurrences += 2;
					if ( 1 == itagSequenceToRetrieve
						|| 2 == itagSequenceToRetrieve )
						{
						*pcolumnidRetrieved = columnidCurr;
						if ( NULL != pitagSequenceRetrieved )
							*pitagSequenceRetrieved = itagSequenceToRetrieve;

						TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
						tv.RetrieveInstance( itagSequenceToRetrieve, pdataField );
						return JET_errSuccess;
						}
					}
				else
					{
					MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
					ulNumOccurrences += mv.CMultiValues();

					if ( itagSequenceToRetrieve > 0
						&& itagSequenceToRetrieve <= mv.CMultiValues() )
						{
						*pcolumnidRetrieved = columnidCurr;
						if ( NULL != pitagSequenceRetrieved )
							*pitagSequenceRetrieved = itagSequenceToRetrieve;

						return mv.ErrRetrieveInstance( itagSequenceToRetrieve, pdataField );
						}
					}
				}
			else if ( ++ulNumOccurrences == itagSequence )
				{
				Assert( 0 != itagSequence );
				
				pdataField->SetCb( CbData( itagfld ) );
				pdataField->SetPv( PbData( itagfld ) );
				if ( NULL != pheader )
					{
					Assert( Ptagfld( itagfld )->FExtendedInfo() );
					const INT	iDelta	= sizeof(TAGFLD_HEADER);
					pdataField->DeltaPv( iDelta );
					pdataField->DeltaCb( -iDelta );
					}

				*pcolumnidRetrieved = columnidCurr;
				if ( pitagSequenceRetrieved != NULL )
					*pitagSequenceRetrieved = 1;

				return ( NULL == pheader ? JET_errSuccess : pheader->ErrRetrievalResult() );
				}
			}

		//	if we got here, we haven't foudn the instance we're looking for
		Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

		columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr );
		}	// while ( itagfld < CTaggedColumns() )


	if ( fRetrieveDefaults )
		{
		// Take snapshot of FidTaggedLast.  Even if someone's adding
		// columns, we don't have access to it anyways.
		FID	fidTaggedLast = ptdb->FidTaggedLast();

		for( ;
			( FCOLUMNIDTemplateColumn( columnidCurr ) && !ptdb->FTemplateTable() ) || FidOfColumnid( columnidCurr ) <= fidTaggedLast;
			columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr ) )
			{
			FCB		*pfcbT		= pfcb;

			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

			if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
				{
				if ( pfcbNil != ptdb->PfcbTemplateTable() )
					{
					ptdb->AssertValidDerivedTable();
					pfcbT = ptdb->PfcbTemplateTable();
					}
				else
					{
					ptdb->AssertValidTemplateTable();
					}
				}
			else
				{
				err = ErrRECIAccessColumn( pfucb, columnidCurr );
				if ( err < 0 )
					{
					if ( JET_errColumnNotFound == err )
						continue;
					return err;
					}
				}

			const TDB *	const	ptdbT			= pfcbT->Ptdb();
			const BOOL			fUseDMLLatch	= ( FidOfColumnid( columnidCurr ) > ptdbT->FidTaggedLastInitial() );

			if ( fUseDMLLatch )
				pfcbT->EnterDML();

			Assert( JET_coltypNil != ptdbT->PfieldTagged( columnidCurr )->coltyp );
			const FIELDFLAG		ffield			= ptdbT->PfieldTagged( columnidCurr )->ffield;

			if ( FFIELDUserDefinedDefault( ffield ) )
				{
				if ( ++ulNumOccurrences == itagSequence )
					{
					Assert( itagSequence != 0 );
					*pcolumnidRetrieved = columnidCurr;
					if ( pitagSequenceRetrieved != NULL )
						*pitagSequenceRetrieved = 1;

					//	assert no infinite recursion
					Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );

					if ( fUseDMLLatch )
						pfcbT->LeaveDML();
					return ErrERRCheck( wrnRECUserDefinedDefault );
					}
				}
			else if ( FFIELDDefault( ffield ) )
				{
				if ( ++ulNumOccurrences == itagSequence )
					{
					Assert( itagSequence != 0 );
					*pcolumnidRetrieved = columnidCurr;
					if ( pitagSequenceRetrieved != NULL )
						*pitagSequenceRetrieved = 1;

					//	assert no infinite recursion
					Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );
					err = ErrRECIRetrieveTaggedDefaultValue( pfcbT, columnidCurr, pdataField );

					if ( fUseDMLLatch )
						pfcbT->LeaveDML();
					return err;
					}
				}

			if ( fUseDMLLatch )
				pfcbT->LeaveDML();
			}
		}

	// If we reached here, no more tagged columns.
	*pcolumnidRetrieved = 0;
	if ( pitagSequenceRetrieved != NULL )
		*pitagSequenceRetrieved = ( itagSequence == 0 ? ulNumOccurrences : 0 );

	//	null column common exit point
	//
	pdataField->Nullify();
	return ErrERRCheck( JET_wrnColumnNull );
	}


ERR TAGFIELDS::ErrAffectLongValuesInWorkBuf(
	FUCB			* const pfucb,
	const LVAFFECT	lvaffect )
	{
	ERR				err				= JET_errSuccess;
	TDB				* const ptdb	= pfucb->u.pfcb->Ptdb();
	ULONG			itagfld			= 0;

#ifdef DEBUG
	const ULONG		cTaggedColumns	= CTaggedColumns();		//	snapshot original count for debugging
	const REC		* prec			= (REC *)( pfucb->dataWorkBuf.Pv() );
	Assert( prec->PbTaggedData() == (BYTE *)m_rgtagfld );
#endif	

	Assert( ptdbNil != ptdb );
	AssertValid( ptdb );
	Assert( !Pcsr( pfucb )->FLatched() );

	//	WARNING: This function performs LV updates and also modifies
	//	the copy buffer, so if this function returns an error and
	//	the LV updates are rolled back, it is up to the caller to
	//	ensure that either the copy buffer is discarded or the
	//	original copy buffer is re-instated
	Assert( pfucb->ppib->level > 0 );
	Assert( lvaffectSeparateAll == lvaffect
		|| ( lvaffectReferenceAll == lvaffect && FFUCBInsertCopyPrepared( pfucb ) ) );

	while ( itagfld < CTaggedColumns() )
		{
		const COLUMNID	columnidCurr	= Ptagfld( itagfld )->Columnid( ptdb );
		TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
		const BOOL		fRemoveSLV		= ( lvaffectReferenceAll == lvaffect	//	can't single-instance SLVs, so must remove them
												&& NULL != pheader
												&& pheader->FSLV() );
		BOOL			fRemoveColumn	= fRemoveSLV;
		
		Assert( !Pcsr( pfucb )->FLatched() );

		if ( !fRemoveColumn && !FCOLUMNIDTemplateColumn( columnidCurr ) )
			{
			//	only check column visibility if we don't already
			//	know we're going to remove it
			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 )
				{
				if ( JET_errColumnNotFound != err )
					goto HandleError;

				err = JET_errSuccess;
				fRemoveColumn = fTrue;
				}
			else
				{
				Assert( !fRemoveSLV );
				CallS( err );
				}
			}

		if ( fRemoveColumn )
			{
			const ULONG		cbColumnToRemove	= CbData( itagfld );
			Assert( !FCOLUMNIDTemplateColumn( columnidCurr ) );

			//	Two cases where we must remove the column:
			//	1) SLV: don't support single-instancing SLVs, so
			//	must remove from copy buffer
			//	2) Column not visible to this session.  Since the
			//	column exists in this session's record, the column
			//	could not have been version-added by someone else.
			//	Therefore, it must have been deleted by this session,
			//	or deleted and committed before this transaction began.
#ifdef DEBUG
			if ( !fRemoveSLV )
				{
				pfucb->u.pfcb->EnterDML();
				Assert( FFIELDDeleted( ptdb->PfieldTagged( columnidCurr )->ffield ) );
				pfucb->u.pfcb->LeaveDML();
				}
#endif

			//	if SeparateAll, must first deref all LVs before removing
			//	them from the record
			//	if ReferenceAll, don't deref because we're coming
			//	from InsertCopy and the only ref belongs to the
			//	original record
			if ( lvaffectSeparateAll == lvaffect
				&& NULL != pheader
				&& pheader->FColumnCanBeSeparated() )
				{
				Assert( !pheader->FTwoValues() );
				if ( pheader->FMultiValues() )
					{
					MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
					ULONG			imv;

#ifdef UNLIMITED_MULTIVALUES
#else
					Assert( !pheader->FSeparated() );
#endif					

					for ( imv = 0; imv < mv.CMultiValues(); imv++ )
						{
						if ( mv.FSeparatedInstance( imv ) )
							{
							//	set flag so that on prepCancel, delta RCE
							//	will be properly rolled back
							FUCBSetUpdateSeparateLV( pfucb );
				 			Assert( sizeof(LID) == mv.CbData( imv ) );
							LID		lidT	= LidOfSeparatedLV( mv.PbData( imv ) );
							Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVDereference ) );
							Assert( JET_wrnCopyLongValue != err );
							}
						}
					}
				else if ( pheader->FSeparated() )
					{
					FUCBSetUpdateSeparateLV( pfucb );
		 			Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					LID		lidT	= LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
					Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVDereference ) );
					Assert( JET_wrnCopyLongValue != err );
					}
				}

			DeleteTagfld( itagfld );
			pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) + cbColumnToRemove ) );

			//	don't increment itagfld, so we will retrieve whatever
			//	tagged column now occupies the space vacated by
			//	the deleted column
			continue;
			}

		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			Assert( !Ptagfld( itagfld )->FNull() );
			Assert( !pheader->FTwoValues() );
			Assert( CbData( itagfld ) >= sizeof(TAGFLD_HEADER) );

			switch ( lvaffect )
				{
				case lvaffectSeparateAll:
					//	note that we do not separate those long values that are
					//	so short that they take even less space in a record
					//	than a LID for separated long value would.
					if ( pheader->FMultiValues() )
						{
#ifdef UNLIMITED_MULTIVALUES
#else
						Assert( !pheader->FSeparated() );
#endif						
						MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
						ULONG			imv;
						ULONG			cbColumnShrink		= 0;

						for ( imv = 0; imv < mv.CMultiValues(); imv++ )
							{
							if ( !mv.FSeparatedInstance( imv )
								&& mv.CbData( imv ) > sizeof(LID) )
								{
								DATA			dataField;
								LID				lid;
								const ULONG		cbData		= mv.CbData( imv );
								const ULONG		cbShrink	= cbData - sizeof(LID);
								BYTE			rgbT[sizeof(LID)];

								//	set flag so that on prepCancel, insert RCE for
								//	this new separated LV will be properly rolled back
								FUCBSetUpdateSeparateLV( pfucb );
						
			 					dataField.SetPv( mv.PbData( imv ) );
			  					dataField.SetCb( cbData );
		 						Call( ErrRECSeparateLV( pfucb, &dataField, &lid, NULL ) );
								Assert( JET_wrnCopyLongValue == err );

								LVSetLidInRecord(
									rgbT,
									lid );
								dataField.SetPv( rgbT );
								dataField.SetCb( sizeof(LID) );

								mv.UpdateInstance(
									imv + 1,
									&dataField,
									JET_coltypNil,
									fTrue );

								cbColumnShrink += cbShrink;
								}
							}
						if ( cbColumnShrink > 0 )
							{
							ResizeTagfld( itagfld, -cbColumnShrink );
							//	update record size
							pfucb->dataWorkBuf.DeltaCb( -cbColumnShrink );
							}
						}
					else if ( !pheader->FSeparated()
							&& CbData( itagfld ) > sizeof(TAGFLD_HEADER) + sizeof(LID) )
						{
						DATA			dataField;
						LID				lid;
						const ULONG		cbData		= CbData( itagfld ) - sizeof(TAGFLD_HEADER);
						const ULONG		cbShrink	= cbData - sizeof(LID);

						//	set flag so that on prepCancel, insert RCE for
						//	this new separated LV will be properly rolled back
						FUCBSetUpdateSeparateLV( pfucb );
						
	 					dataField.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
	  					dataField.SetCb( cbData );
 						Call( ErrRECSeparateLV( pfucb, &dataField, &lid, NULL ) );
						Assert( JET_wrnCopyLongValue == err );

						ResizeTagfld( itagfld, -cbShrink );

						pheader->SetFSeparated();
						LVSetLidInRecord(
							PbData( itagfld ) + sizeof(TAGFLD_HEADER),
							lid );

						//	update record size
						pfucb->dataWorkBuf.DeltaCb( -cbShrink );
						}
					break;
				
				case lvaffectReferenceAll:
					//	all SLVs are removed above
					Assert( !pheader->FSLV() );
					if ( pheader->FMultiValues() )
						{
						MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
						ULONG			imv;

						for ( imv = 0; imv < mv.CMultiValues(); imv++ )
							{
							if ( mv.FSeparatedInstance( imv ) )
								{
								//	set flag so that on prepCancel, delta RCE's will
								//	be properly rolled back
								FUCBSetUpdateSeparateLV( pfucb );
					 			Assert( sizeof(LID) == mv.CbData( imv ) );
								LID		lidT	= LidOfSeparatedLV( mv.PbData( imv ) );
								Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVReference ) );
								if ( JET_wrnCopyLongValue == err )
									{
									// long value got burst, update LID
									Assert( lidT > LidOfSeparatedLV( mv.PbData( imv ) ) );
									LVSetLidInRecord( mv.PbData( imv ), lidT );
									}
								}
							}
						}
					else if ( pheader->FSeparated() )
						{
						//	set flag so that on prepCancel, delta RCE's will
						//	be properly rolled back
						FUCBSetUpdateSeparateLV( pfucb );
			 			Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
						LID		lidT	= LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
						Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVReference ) );
						if ( JET_wrnCopyLongValue == err )
							{
							// long value got burst, update LID
							Assert( lidT > LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) ) );
							LVSetLidInRecord(
								PbData( itagfld ) + sizeof(TAGFLD_HEADER),
								lidT );
							}
						}
					break;

				default:
					Assert( fFalse );
					break;
				}
			}

		itagfld++;

		}	//	while ( itagfld < CTaggedColumns() )

HandleError:
	//	this function should never increase the size of the record
	//	(in fact, we typically call this function to free up record space)
	Assert( JET_errRecordTooBig != err );

	Assert( !Pcsr( pfucb )->FLatched() );
	return err;
	}


ERR TAGFIELDS::ErrDereferenceLongValuesInRecord(
	FUCB		* const pfucb )
	{
	ERR			err;
	TDB			* const ptdb	= pfucb->u.pfcb->Ptdb();
	ULONG		itagfld;

	Assert( ptdbNil != ptdb );
	AssertValid( ptdb );
	Assert( Pcsr( pfucb )->FLatched() );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		Assert( Pcsr( pfucb )->FLatched() );
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( ptdb );
			const BOOL			fSLV				= pheader->FSLV();

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif						

				Assert( Pcsr( pfucb )->FLatched() );
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				if ( fSLV )
					{
					CallR( ErrDIRRelease( pfucb ) );

					//	eaiser to iterate twice than iterate once
					//	and deal with latching/unlatching
					for ( imv = 0; imv < mv.CMultiValues(); imv++ )
						{
						CallR( ErrSLVDelete( pfucb, columnidCurr, imv+1, fFalse ) );
						}

					CallR( ErrDIRGet( pfucb ) );
					Refresh( pfucb->kdfCurr.data );
					mv.Refresh( PbData( itagfld ), CbData( itagfld ) );
					}

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					Assert( Pcsr( pfucb )->FLatched() );
					const BOOL	fSeparatedLV	= mv.FSeparatedInstance( imv );
					if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == mv.CbData( imv ) );
						LID		lidToDeref		= LidOfSeparatedLV( mv.PbData( imv ) );

						CallR( ErrDIRRelease( pfucb ) );

						CallR( ErrRECAffectSeparateLV( pfucb, &lidToDeref, fLVDereference ) );
						Assert( JET_wrnCopyLongValue != err );

						//	re-latch for next iteration
						CallR( ErrDIRGet( pfucb ) );
						Refresh( pfucb->kdfCurr.data );
						mv.Refresh( PbData( itagfld ), CbData( itagfld ) );
						}
					}

				}
			else
				{
				Assert( Pcsr( pfucb )->FLatched() );
				const BOOL	fSeparatedLV	= pheader->FSeparated();

				if ( fSLV || fSeparatedLV )
					{
					Assert( !pheader->FSeparated() || sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					LID		lidToDeref		= ( fSeparatedLV ?
													LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) ) :
													0 );
					CallR( ErrDIRRelease( pfucb ) );

					if ( fSLV )
						{
						CallR( ErrSLVDelete( pfucb, columnidCurr, 1, fFalse ) );
						}
					if ( fSeparatedLV )
						{
						CallR( ErrRECAffectSeparateLV( pfucb, &lidToDeref, fLVDereference ) );
						Assert( JET_wrnCopyLongValue != err );
						}

					//	re-latch for next iteration
					CallR( ErrDIRGet( pfucb ) );
					Refresh( pfucb->kdfCurr.data );
					}
				}
			}
		}

	Assert( Pcsr( pfucb )->FLatched() );
	return JET_errSuccess;
	}


VOID TAGFIELDS::CopyTaggedColumns(
	FUCB			* const pfucbSrc,
	FUCB			* const pfucbDest,
	JET_COLUMNID	* const mpcolumnidcolumnidTagged )
	{
	const TDB		* const ptdbSrc								= pfucbSrc->u.pfcb->Ptdb();
	BOOL			fESE97DerivedColumnsExist					= fFalse;
	BOOL			fESE98DerivedColumnsExist					= fFalse;
	ULONG			cColumnsToCopy								= 0;
	ULONG			itagfldToCopy								= 0;
	ULONG			itagfld;

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
		const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
		const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

		Assert( JET_coltypNil != pfieldTagged->coltyp
			|| !FCOLUMNIDTemplateColumn( columnid ) );
		if ( JET_coltypNil != pfieldTagged->coltyp )
			{
			cColumnsToCopy++;

			if ( FCOLUMNIDTemplateColumn( columnid )
				&& !ptdbSrc->FTemplateTable() )
				{
				if ( ptagfld->FDerived() )
					{
					//	shouldn't have seen yet any derived columns with the derived bit not set
					Assert( !fESE97DerivedColumnsExist );
					fESE98DerivedColumnsExist = fTrue;
					}
				else
					{
					Assert( ptdbSrc->FESE97DerivedTable() );
					fESE97DerivedColumnsExist = fTrue;
					}
				}
			}
		}

	if ( 0 == cColumnsToCopy )
		return;

	USHORT	ibDataDest				= USHORT( cColumnsToCopy * sizeof(TAGFLD) );
	TAGFLD	* const rgtagfldDest	= (TAGFLD *)(
											(BYTE *)pfucbDest->dataWorkBuf.Pv()
											+ pfucbDest->dataWorkBuf.Cb() );

	//	verify currently no tagged data
	Assert( (BYTE *)rgtagfldDest
		== ( (REC *)pfucbDest->dataWorkBuf.Pv() )->PbTaggedData() );


	//	if both ESE97 and ESE98 derived columns exist, must copy ESE97 derived columns first
	const BOOL	fNeedSeparatePassForESE97DerivedColumns		= ( fESE97DerivedColumnsExist
																&& fESE98DerivedColumnsExist );
	if ( fNeedSeparatePassForESE97DerivedColumns )
		{
		Assert( !ptdbSrc->FTemplateTable() );
		Assert( ptdbSrc->FESE97DerivedTable() );
		ptdbSrc->AssertValidDerivedTable();

		for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
			{
			const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
			const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
			const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

			Assert( JET_coltypNil != pfieldTagged->coltyp
				|| !FCOLUMNIDTemplateColumn( columnid ) );
			if ( JET_coltypNil != pfieldTagged->coltyp )
				{
				const FID	fidSrc					= ptagfld->Fid();

				Assert( itagfldToCopy < cColumnsToCopy );

				if ( !FCOLUMNIDTemplateColumn( columnid ) )
					{
					Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
					Assert( !FCOLUMNIDTemplateColumn( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
					Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= pfucbDest->u.pfcb->Ptdb()->FidTaggedLast() );
					Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= fidSrc );
					Assert( !ptagfld->FDerived() );

					//	hit the non-derived columns, so should be no more derived columns left
					break;
					}

				Assert( pfucbSrc->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast()
					== pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );
				Assert( fidSrc <= pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );

				//	ignore ESE98 derived columns
				if ( !ptagfld->FDerived() )
					{
					// If column belongs to base table, then FID will not have changed,
					// since base table's DDL is fixed.  Thus, we don't have to bother
					// updating the FID in the destination record.
					new( rgtagfldDest + itagfldToCopy ) TAGFLD( fidSrc, fTrue );
					Assert( rgtagfldDest[itagfldToCopy].FDerived() );
					rgtagfldDest[itagfldToCopy].SetIb( ibDataDest ); 

					if ( ptagfld->FNull() )
						{
						rgtagfldDest[itagfldToCopy].SetFNull();
						}
					else
						{
						if ( ptagfld->FExtendedInfo() )
							{
							rgtagfldDest[itagfldToCopy].SetFExtendedInfo();
							}

						const ULONG		cbData		= CbData( itagfld );
						UtilMemCpy(
							(BYTE *)rgtagfldDest + 	ibDataDest,
							PbData( itagfld ),
							cbData );

						ibDataDest = USHORT( ibDataDest + cbData );
						}

					itagfldToCopy++;
					}
				}
			}

		Assert( itagfldToCopy <= cColumnsToCopy );
		}

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
		const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
		const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

		Assert( JET_coltypNil != pfieldTagged->coltyp
			|| !FCOLUMNIDTemplateColumn( columnid ) );
		if ( JET_coltypNil != pfieldTagged->coltyp )
			{
			const FID	fidSrc					= ptagfld->Fid();
			FID			fidDest;
			BOOL		fDerivedDest			= fFalse;

			Assert( itagfldToCopy <= cColumnsToCopy );

			if ( !FCOLUMNIDTemplateColumn( columnid ) )
				{
				Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
				Assert( !FCOLUMNIDTemplateColumn( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
				Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= pfucbDest->u.pfcb->Ptdb()->FidTaggedLast() );
				Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= fidSrc );
				fidDest = FidOfColumnid( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] );

				Assert( !ptagfld->FDerived() );
				}
			else
				{
				if ( ptdbSrc->FTemplateTable() )
					{
					ptdbSrc->AssertValidTemplateTable();
					Assert( !ptagfld->FDerived() );
					Assert( !ptdbSrc->FESE97DerivedTable() );
					}
				else
					{
					ptdbSrc->AssertValidDerivedTable();
					Assert( pfucbSrc->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast()
						== pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );
					Assert( ptagfld->FDerived() || ptdbSrc->FESE97DerivedTable() );
					Assert( fidSrc <= pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );

					if ( !ptagfld->FDerived() )
						{
						Assert( ptdbSrc->FESE97DerivedTable() );
						if ( fNeedSeparatePassForESE97DerivedColumns )
							{
							//	ESE97 derived columns were copied in the previous pass
							continue;
							}
						}

					fDerivedDest = fTrue;
					}

				// If column belongs to base table, then FID will not have changed,
				// since base table's DDL is fixed.  Thus, we don't have to bother
				// updating the FID in the destination record.
				fidDest = fidSrc;
				}

			new( rgtagfldDest + itagfldToCopy ) TAGFLD( fidDest, fDerivedDest );
			rgtagfldDest[itagfldToCopy].SetIb( ibDataDest ); 

			if ( ptagfld->FNull() )
				{
				rgtagfldDest[itagfldToCopy].SetFNull();
				}
			else
				{
				if ( ptagfld->FExtendedInfo() )
					{
					rgtagfldDest[itagfldToCopy].SetFExtendedInfo();
					}

				const ULONG		cbData		= CbData( itagfld );
				UtilMemCpy(
					(BYTE *)rgtagfldDest + ibDataDest,
					PbData( itagfld ),
					cbData );

				ibDataDest = USHORT( ibDataDest + cbData );
				}

			Assert( itagfldToCopy < cColumnsToCopy );
			itagfldToCopy++;
			}
		}

	Assert( itagfldToCopy == cColumnsToCopy );
	pfucbDest->dataWorkBuf.DeltaCb( ibDataDest );

	Assert( pfucbDest->dataWorkBuf.Cb() >= ibRECStartFixedColumns );
	Assert( pfucbDest->dataWorkBuf.Cb() <= pfucbSrc->kdfCurr.data.Cb() );
	}


ERR TAGFIELDS::ErrUpdateSeparatedLongValuesAfterCopy(
	FUCB		* const pfucbSrc,
	FUCB		* const pfucbDest,
	JET_COLUMNID* const mpcolumnidcolumnidTagged,
	STATUSINFO	* const pstatus )
	{
	ERR			err;
	ULONG		itagfld;

	TDB			* const ptdbDest	= pfucbDest->u.pfcb->Ptdb();
	Assert( ptdbNil != ptdbDest );
	AssertValid( ptdbDest );
	
	Assert( !Pcsr( pfucbSrc )->FLatched() );
	Assert( !Pcsr( pfucbDest )->FLatched() );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		TAGFLD_HEADER		* const pheader			= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( ptdbDest );
			const BOOL			fSLV				= pheader->FSLV() && rgfmp[ pfucbDest->ifmp ].FDefragSLVCopy();
			Assert(	FRECLongValue( ptdbDest->PfieldTagged( columnidCurr )->coltyp ) || FRECSLV( ptdbDest->PfieldTagged( columnidCurr )->coltyp ) );
			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif						

				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					const BOOL	fSeparatedLV	= mv.FSeparatedInstance( imv );
					if ( fSLV )
						{
						if ( fSeparatedLV )
							{
							Assert( sizeof(LID) == mv.CbData( imv ) );
							mv.ResetFSeparatedInstance( imv );
							}
						JET_COLUMNID columnid = columnidCurr - fidTaggedLeast;
						if ( mpcolumnidcolumnidTagged[columnid] != columnidCurr )
							{	
							FID fidTaggedHighest = pfucbSrc->u.pfcb->Ptdb()->FidTaggedLast();
							for ( columnid++; columnid <= fidTaggedHighest + 1 - fidTaggedLeast; columnid++ )
								{
								if ( mpcolumnidcolumnidTagged[columnid] == columnidCurr )
									{
									break;
									}
								}
							Assert( columnid <= fidTaggedHighest + 1 - fidTaggedLeast );
							}
						Assert( columnidCurr >= fidTaggedLeast );
						DATA dataNil;
						dataNil.Nullify();
						CallR( ErrRECSetLongField( pfucbDest, columnidCurr, imv+1, &dataNil, JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV | JET_bitSetSizeLV ) );
						CallR( ErrSLVCopyUsingData( pfucbSrc, columnid + fidTaggedLeast, imv+1, pfucbDest, columnidCurr, imv+1 ) );
						m_cbTaggedColumns = ULONG( (BYTE *)pfucbDest->dataWorkBuf.Pv() + pfucbDest->dataWorkBuf.Cb() - ((REC *)pfucbDest->dataWorkBuf.Pv())->PbTaggedData() );
						}
					else if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == mv.CbData( imv ) );
						BYTE 		* const pbLid	= mv.PbData( imv );
						const LID	lidSrc			= LidOfSeparatedLV( pbLid );
						LID			lidDest;

						CallR( ErrSORTIncrementLVRefcountDest(
									pfucbSrc->ppib,
									lidSrc,
									&lidDest ) );
						LVSetLidInRecord( pbLid, lidDest );
						}
					else if ( NULL != pstatus )
						{
						pstatus->cbRawData += mv.CbData( imv );
						}
					}
				}
			else
				{
				const BOOL	fSeparatedLV	= pheader->FSeparated();
				if ( fSLV )
					{
					if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
						pheader->ResetFSeparated();
						}
					JET_COLUMNID columnid = columnidCurr - fidTaggedLeast;
					if ( mpcolumnidcolumnidTagged[columnid] != columnidCurr )
						{	
						FID fidTaggedHighest = pfucbSrc->u.pfcb->Ptdb()->FidTaggedLast();
						for ( columnid++; columnid <= fidTaggedHighest + 1 - fidTaggedLeast; columnid++ )
							{
							if ( mpcolumnidcolumnidTagged[columnid] == columnidCurr )
								{
								break;
								}
							}
						Assert( columnid <= fidTaggedHighest + 1 - fidTaggedLeast );
						}
					Assert( columnidCurr >= fidTaggedLeast );
					DATA dataNil;
					dataNil.Nullify();
					CallR( ErrRECSetLongField( pfucbDest, columnidCurr, 1, &dataNil, JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV | JET_bitSetSizeLV ) );
					CallR( ErrSLVCopyUsingData( pfucbSrc, columnid + fidTaggedLeast, 1, pfucbDest, columnidCurr, 1 ) );
					m_cbTaggedColumns = ULONG( (BYTE *)pfucbDest->dataWorkBuf.Pv() + pfucbDest->dataWorkBuf.Cb() - ((REC *)pfucbDest->dataWorkBuf.Pv())->PbTaggedData() );
					}
				else if ( fSeparatedLV )
					{
					Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					BYTE 		* const pbLid	= PbData( itagfld ) + sizeof(TAGFLD_HEADER);
					const LID	lidSrc			= LidOfSeparatedLV( pbLid );
					LID			lidDest;

					CallR( ErrSORTIncrementLVRefcountDest(
								pfucbSrc->ppib,
								lidSrc,
								&lidDest ) );
					LVSetLidInRecord( pbLid, lidDest );
					}
				else if ( NULL != pstatus )
					{
					pstatus->cbRawData += CbData( itagfld ) - sizeof(TAGFLD_HEADER);
					}
				}
			}
		
		else if ( NULL != pstatus )
			{
			pstatus->cbRawData +=
						CbData( itagfld )
						- ( NULL != pheader ? sizeof(TAGFLD_HEADER) : 0 );
			}
		}

#ifdef DEBUG
	AssertValid( ptdbDest );
#endif
	
	return JET_errSuccess;
	}


ERR TAGFIELDS::ErrCopySLVColumns(
	FUCB		* const pfucb )
	{
	ERR			err;
	ULONG		itagfld;

	Assert( ptdbNil != pfucb->u.pfcb->Ptdb() );
	AssertValid( pfucb->u.pfcb->Ptdb() );
	Assert( Pcsr( pfucb )->FLatched() );

	//	we start with the page latched (though we may not end up that way)

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		//  reacquire our latch if we lost it
		if ( !Pcsr( pfucb )->FLatched() )
			{
			CallR( ErrDIRGet( pfucb ) );
			Refresh( pfucb->kdfCurr.data );
			}

		const TAGFLD_HEADER	* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FSLV() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( pfucb->u.pfcb->Ptdb() );
			ULONG				cMultiValues		= 1;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				cMultiValues = mv.CMultiValues();
				Assert( cMultiValues > 1 );
				}
			else
				{
				cMultiValues = 1;
				}

			Assert( Pcsr( pfucb )->FLatched() );
			CallR( ErrDIRRelease( pfucb ) );

			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 )
				{
				// ignore column if not visible, exit if other error
				if ( JET_errColumnNotFound != err )
					return err;
				}
			else
				{
				Assert( !Pcsr( pfucb )->FLatched() );

				for ( ULONG itagSequenceCurr = 1; itagSequenceCurr <= cMultiValues; itagSequenceCurr++ )
					{
					CallR( ErrSLVCopy( pfucb, columnidCurr, itagSequenceCurr ) );
					}

				//  remember that we have operations to rollback on JET_prepCancel
				FUCBSetUpdateSeparateLV( pfucb );
				}
			}
		}

	return JET_errSuccess;
	}


//	UNDONE: Move this function to SLV.CXX
LOCAL ERR ErrSLVUpdateOwnerMapForColumnInstance(
	FUCB			* const pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	BOOKMARK		* const pbmInsert,
	DATA			* const pdata,
	const BOOL		fSeparatedSLV )
	{
	ERR				err;
	CSLVInfo 		slvinfo;

	CallR( slvinfo.ErrLoad(
				pfucb,
				columnid,
				itagSequence,
				fTrue,
				pdata,
				fSeparatedSLV 
				) );

	CallS( slvinfo.ErrMoveBeforeFirst() );
	
	err = slvinfo.ErrMoveNext();
	while ( JET_errSuccess <= err )
		{
		CSLVInfo::RUN	run;
		
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		Call( ErrSLVOwnerMapSetUsageRange(
					pfucb->ppib,
					pfucb->ifmp,
					run.PgnoFirst(),
					run.Cpg(),
					pfucb->u.pfcb->ObjidFDP(),
					columnid,
					pbmInsert,
					fSLVOWNERMAPSetRECInsert,
					FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) // force update the new owner
					) );
			
		err = slvinfo.ErrMoveNext();
		}
	Assert ( JET_errNoCurrentRecord == err );
	err = JET_errSuccess;
		
HandleError:
	slvinfo.Unload();

	return err;
	}

ERR TAGFIELDS::ErrUpdateSLVOwnerMapForRecordInsert(
	FUCB		* const pfucb,
	BOOKMARK&	bmInsert )
	{
	ERR			err					= JET_errSuccess;
	const TDB	* const ptdb		= pfucb->u.pfcb->Ptdb();
	ULONG		itagfld;

	Assert( pfucb->u.pfcb->FPrimaryIndex() );
	Assert( pfucb->u.pfcb->FTypeTable() );
	AssertValid( ptdb );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );

		if ( NULL != pheader
			&& pheader->FSLV() )
			{
			const COLUMNID	columnidCurr	= Ptagfld( itagfld )->Columnid( ptdb );
			DATA			data;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			itagSequenceCurr;

#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif

				for ( itagSequenceCurr = 1; itagSequenceCurr <= mv.CMultiValues(); itagSequenceCurr++ )
					{
					const ULONG	imv		= itagSequenceCurr - 1;
					data.SetPv( mv.PbData( imv ) );
					data.SetCb( mv.CbData( imv ) );
					Call( ErrSLVUpdateOwnerMapForColumnInstance(
								pfucb,
								columnidCurr,
								itagSequenceCurr,
								&bmInsert,
								&data,
								mv.FSeparatedInstance( imv ) ) );
					}
				}
			else
				{
				data.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				data.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
				Call( ErrSLVUpdateOwnerMapForColumnInstance(
						pfucb,
						columnidCurr,
						1,
						&bmInsert,
						&data,
						pheader->FSeparated() ) );
				}
			}
		}

HandleError:
	return err;
	}


VOID TAGFIELDS::Dump(
	const CPRINTF	* const pcprintf,
	CHAR			* const szBuf,
	const ULONG		cbWidth )
	{
	ULONG	itagfld;
	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld		= Ptagfld( itagfld );

		(*pcprintf)( "%d:\tFlags: ", ptagfld->Fid() );
		if ( ptagfld->FDerived() )
			{
			(*pcprintf)( "FDerived " );
			}
		if ( Ptagfld( itagfld )->FNull() )
			{
			(*pcprintf)( "FNull\n" );
			}
		else
			{
			const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
			const BYTE			* const pbData	= PbData( itagfld );
			const ULONG			cbData			= CbData( itagfld );

			if ( NULL != pheader )
				{
				if ( pheader->FLongValue() )
					(*pcprintf)( "FLongValue " );
				
				if ( pheader->FSLV() )
					(*pcprintf)( "FSLV " );

				if ( pheader->FSeparated() )
					(*pcprintf)( "FSeparated " );

				if ( pheader->FMultiValues() )
					(*pcprintf)( "FMultiValues " );

				if ( pheader->FTwoValues() )
					(*pcprintf)( "FTwoValues " );
				}

			(*pcprintf)( "\n\tData: %d bytes\n\t      ", cbData );

			//	UNDONE_SORTED_TAGGED_COLUMNS: properly output individual tagged data
			DBUTLSprintHex( szBuf, pbData, cbData, cbWidth );
			(*pcprintf)( "%s\n", szBuf );
			}
		}
	}


ERR TAGFIELDS::ErrCheckLongValuesAndSLVs(
	const KEYDATAFLAGS&	kdf,
	RECCHECKTABLE		* const precchecktable )
	{
	Assert( NULL != precchecktable );

	ERR		err;
	ULONG	itagfld;
	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const TAGFLD	* const ptagfld		= Ptagfld( itagfld );
			const COLUMNID	columnidCurr		= ColumnidOfFid(
														ptagfld->Fid(),
														ptagfld->FDerived() );
			DATA			dataT;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					dataT.SetPv( mv.PbData( imv ) );
					dataT.SetCb( mv.CbData( imv ) );
					if ( pheader->FLongValue() )
						{
						CallR( precchecktable->ErrCheckLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
						}
					else if ( pheader->FSLV() )
						{
						CallR( precchecktable->ErrCheckLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
						
						CallR( precchecktable->ErrCheckSLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
						}
					}
				}
			else if ( pheader->FLongValue() )
				{
				dataT.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				dataT.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
				CallR( precchecktable->ErrCheckLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
				}
			else if ( pheader->FSLV() )
				{
				dataT.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				dataT.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );

				CallR( precchecktable->ErrCheckLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
				
				CallR( precchecktable->ErrCheckSLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
				}
			else
				{
				//	should be either LV or SLV
				Assert( fFalse );
				}
			}
		else
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( NULL == pheader
				|| !pheader->FSeparated() );
#endif				
			}
		}

	return JET_errSuccess;
	}


BOOL TAGFIELDS::FIsValidTwoValues(
	const ULONG				itagfld,
	const CPRINTF			* const pcprintf ) const
	{
	const TAGFLD_HEADER		* const pheader			= (TAGFLD_HEADER *)PbData( itagfld );

	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( pheader->FTwoValues() );
	Assert( !pheader->FSeparated() );
	Assert( !pheader->FColumnCanBeSeparated() );

	if ( CbData( itagfld ) < sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) )
		{
		(*pcprintf)( "Column is too small to contain TwoValues.\r\n" );
		AssertSz( fFalse, "Column is too small to contain TwoValues." );
		return fFalse;
		}

	if ( CbData( itagfld ) > sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) + ( 2 * JET_cbColumnMost ) )
		{
		(*pcprintf)( "Column is larger than maximum possible size for TWOVALUES.\r\n" );
		AssertSz( fFalse, "Column is larger than maximum possible size for TWOVALUES." );
		return fFalse;
		}

	const ULONG						cbTwoValues			= CbData( itagfld ) - sizeof(TAGFLD_HEADER ) - sizeof(TWOVALUES::TVLENGTH );
	const TWOVALUES::TVLENGTH		cbFirstValue		= *(TWOVALUES::TVLENGTH *)( pheader + 1 );
	if ( cbFirstValue > cbTwoValues )
		{
		(*pcprintf)( "First TWOVALUE is too long.\r\n" );
		AssertSz( fFalse, "First TWOVALUE is too long." );
		return fFalse;
		}

	if ( cbTwoValues - cbFirstValue > JET_cbColumnMost )
		{
		(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
		AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
		return fFalse;
		}

	return fTrue;
	}

BOOL MULTIVALUES::FValidate(
	const CPRINTF	* const pcprintf ) const
	{
	const BOOL		fLongValue				= ( Pheader()->FLongValue() || Pheader()->FSLV() );
	BOOL			fPrevWasSeparated		= fFalse;
	ULONG			ibPrev					= 0;
	ULONG			imv;
	
	for ( imv = 0; imv < CMultiValues(); imv++ )
		{
		const ULONG		ibCurr		= Ib( imv );
		if ( ibCurr < ibPrev
			|| ibCurr > CbMultiValues() )
			{
			(*pcprintf)( "MULTIVALUE either overlaps previous MULTIVALUE or is out of TAGFLD range.\r\n" );
			AssertSz( fFalse, "MULTIVALUE either overlaps previous MULTIVALUE or is out of TAGFLD range." );
			return fFalse;
			}

		if ( !fLongValue )
			{
			if ( imv == CMultiValues() - 1 )
				{
				if ( CbMultiValues() - ibCurr > JET_cbColumnMost )
					{
					(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
					AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
					return fFalse;
					}
				}
			else if ( imv > 0 )
				{
				if ( ibCurr - ibPrev > JET_cbColumnMost )
					{
					(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
					AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
					return fFalse;
					}
				}
			}

		if ( fPrevWasSeparated )
			{
			if ( ibCurr - ibPrev != sizeof(LID) )
				{
				(*pcprintf)( "Separated column has invalid LID.\r\n" );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}
			fPrevWasSeparated = fFalse;
			}

		ibPrev = ibCurr;				//	save off ib for next iteration

		if ( FSeparatedInstance( imv ) )
			{
			if ( !Pheader()->FColumnCanBeSeparated() )
				{
				(*pcprintf)( "Separated column is not a LongValue or an SLV.\r\n" );
				AssertSz( fFalse, "Separated column is not a LongValue or an SLV." );
				return fFalse;
				}

			if ( imv == CMultiValues() - 1
				&& CbMultiValues() - ibCurr != sizeof(LID) )
				{
				(*pcprintf)( "Separated column has invalid LID.\r\n" );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}

			fPrevWasSeparated = fTrue;
			}
		}

	return fTrue;
	}

BOOL TAGFIELDS::FIsValidMultiValues(
	const ULONG			itagfld,
	const CPRINTF		* const pcprintf ) const
	{
	const TAGFLD_HEADER	* const pheader			= (TAGFLD_HEADER *)PbData( itagfld );

	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !pheader->FSeparated() );
#endif


	if ( CbData( itagfld ) < sizeof(TAGFLD_HEADER) + ( 2 * sizeof(MULTIVALUES::MVOFFSET) ) )
		{
		(*pcprintf)( "Column is too small to contain MultiValues.\r\n" );
		AssertSz( fFalse, "Column is too small to contain MultiValues." );
		return fFalse;
		}

	const MULTIVALUES::MVOFFSET		* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)( pheader + 1 );
	const ULONG						cbMultiValues		= CbData( itagfld ) - sizeof(TAGFLD_HEADER);
	const ULONG						ibFirstMV			= ( rgmvoffs[0] & MULTIVALUES::maskIb );
	if ( ibFirstMV < 2 * sizeof(MULTIVALUES::MVOFFSET)
		|| ibFirstMV > cbMultiValues
		|| ibFirstMV % sizeof(MULTIVALUES::MVOFFSET) != 0 )
		{
		(*pcprintf)( "First MULTIVALUE has invalid Ib.\r\n" );
		AssertSz( fFalse, "First MULTIVALUE has invalid Ib." );
		return fFalse;
		}

	MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
	return mv.FValidate( pcprintf );
	}

BOOL TAGFIELDS::FValidate(
	const CPRINTF	* const pcprintf ) const
	{
	BOOL			fSawNonDerived		= fFalse;
	FID				fidPrev				= 0;
	USHORT			ibPrev				= 0;
 	BOOL			fPrevWasNull		= fFalse;
 	BOOL			fPrevWasSeparated	= fFalse;
 	BOOL			fPrevWasTwoValues	= fFalse;
 	BOOL			fPrevWasMultiValues	= fFalse;
 	BOOL			fPrevWasLongValue	= fFalse;
	ULONG			itagfld;

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld		= Ptagfld( itagfld );

		if ( !FTaggedFid( ptagfld->Fid() ) )
			{
			(*pcprintf)( "FID %d is not a tagged column.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "FID is not a tagged column." );
			return fFalse;
			}

		if ( ptagfld->FDerived() )
			{
			if ( fSawNonDerived )
				{
				//	all derived columns must come first
				(*pcprintf)( "Derived/NonDerived columns out of order.\r\n" );
				AssertSz( fFalse, "Derived/NonDerived columns out of order." );
				return fFalse;
				}
			if ( ptagfld->Fid() <= fidPrev )
				{
				//	FIDs must be monotonically increasing
				(*pcprintf)( "Columns are not in monotonically-increasing FID order (FID %d <= FID %d).\r\n", ptagfld->Fid(), fidPrev );
				AssertSz( fFalse, "Columns are not in monotonically-increasing FID order." );
				return fFalse;
				}
			}
		else if ( fSawNonDerived )
			{
			if ( ptagfld->Fid() <= fidPrev )
				{
				//	FIDs must be monotonically increasing
				(*pcprintf)( "Columns are not in monotonically-increasing FID order (FID %d <= FID %d).\r\n", ptagfld->Fid(), fidPrev );
				AssertSz( fFalse, "Columns are not in monotonically-increasing FID order." );
				return fFalse;
				}
			}
		else
			{
			fSawNonDerived = fTrue;
			}
		fidPrev = ptagfld->Fid();			//	save off FID for next iteration

		if ( ptagfld->Ib() < ibPrev
			|| ptagfld->Ib() > CbTaggedColumns() )
			{
			(*pcprintf)( "TAGFLD %d either overlaps previous TAGFLD or is out of record range.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "TAGFLD either overlaps previous TAGFLD or is out of record range." );
			return fFalse;
			}

		if ( !fPrevWasLongValue )
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !fPrevWasSeparated );
#endif
			if ( !fPrevWasMultiValues
				&& !fPrevWasTwoValues
				&& 0 != ibPrev
				&& ptagfld->Ib() - ibPrev > JET_cbColumnMost )
				{
				(*pcprintf)( "Column %d is greater than 255 bytes, but is not a LongValue or SLV column.\r\n", ptagfld->Fid() );				
				AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
				return fFalse;
				}
			}


		//	if we needed to extract the length of the previous TAGFLD, we can
		//	now do it because we've validated the ib of this TAGFLD
		if ( fPrevWasNull )
			{
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasMultiValues );
			Assert( !fPrevWasSeparated );
			Assert( !fPrevWasLongValue );
			if ( ptagfld->Ib() != ibPrev )
				{
				(*pcprintf)( "Previous TAGFLD was NULL but not zero-length.\r\n" );
				AssertSz( fFalse, "Previous TAGFLD was NULL but not zero-length." );
				return fFalse;
				}
			}

		else if ( fPrevWasTwoValues )
			{
			Assert( !fPrevWasNull );
			Assert( !fPrevWasMultiValues );
			Assert( !fPrevWasSeparated );
			Assert( !fPrevWasLongValue );
			Assert( itagfld > 0 );
			if ( !FIsValidTwoValues( itagfld-1, pcprintf ) )
				return fFalse;
			fPrevWasTwoValues = fFalse;
			}
		else if ( fPrevWasMultiValues )
			{
			Assert( !fPrevWasNull );
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasSeparated );
			Assert( itagfld > 0 );
			if ( !FIsValidMultiValues( itagfld-1, pcprintf ) )
				return fFalse;
			fPrevWasMultiValues = fFalse;
			}
		else if ( fPrevWasSeparated )
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !fPrevWasNull );
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasMultiValues );
			if ( ptagfld->Ib() - ibPrev != sizeof(TAGFLD_HEADER) + sizeof(LID) )
				{
				(*pcprintf)( "Separated column %d has invalid LID.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}
#endif				
			fPrevWasSeparated = fFalse;		//	reset for next iteration
			}

		ibPrev = USHORT( ptagfld->Ib() );	//	save off ib for next iteration

		if ( ptagfld->FNull() )
			{
			if ( ptagfld->FExtendedInfo() )
				{
				//	these two bits are mutually exclusive
				(*pcprintf)( "TAGFLD %d has both NULL and ExtendedInfo flags set.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "TAGFLD has both NULL and ExtendedInfo flags set." );
				return fFalse;
				}

			if ( itagfld == CTaggedColumns() - 1
				&& ptagfld->Ib() != CbTaggedColumns() )
				{
				//	if last column is NULL, it must point to the end of the tagged data
				(*pcprintf)( "Last TAGFLD is NULL but does not point to the end of the tagged data.\r\n" );
				AssertSz( fFalse, "Last TAGFLD is NULL but does not point to the end of the tagged data." );
				return fFalse;
				}
			}
		fPrevWasNull = ptagfld->FNull();	//	save off for next iteration

		fPrevWasLongValue = fFalse;			//	reset for next iteration

		if ( ptagfld->FExtendedInfo() )
			{
			const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );

			//	these are already checked, so just assert
			Assert( NULL != pheader );
			Assert( (BYTE *)pheader >= PbStartOfTaggedData() );
			Assert( (BYTE *)pheader <= PbStartOfTaggedData() + CbTaggedData() );

			if ( *(BYTE *)pheader & BYTE( ~TAGFLD_HEADER::maskFlags ) )
				{
				//	these bits should be unused
				(*pcprintf)( "TAGFLD header (%x) has invalid bits set.\r\n", *(BYTE*)pheader );
				AssertSz( fFalse, "TAGFLD header has invalid bits set." );
				return fFalse;
				}

			if ( pheader->FLongValue() || pheader->FSLV() )
				{
				if ( pheader->FLongValue() && pheader->FSLV() )
					{
					(*pcprintf)( "TAGFLD %d is marked as both a LongValue and an SLV.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "TAGFLD is marked as both a LongValue and an SLV." );
					return fFalse;
					}

				fPrevWasLongValue = fTrue;		//	save off for next iteration
				}

			else if ( !pheader->FMultiValues() )
				{
				if( pheader->FTwoValues() )
					{
					(*pcprintf)( "Column %d is not MultiValues but is TwoValues.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Column is MultiValues and TwoValues." );
					}
					
				//	if MultiValues not set, no other reason for non-LV, non-SLV
				//	column to have a header byte
				(*pcprintf)( "Column %d has inappropriate header byte.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "Column has inappropriate header byte." );
				return fFalse;
				}

			if ( pheader->FTwoValues() )
				{
				if ( !pheader->FMultiValues() )
					{
					(*pcprintf)( "TAGFLD %d is marked as TwoValues but not MultiValues.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "TAGFLD is marked as TwoValues but not MultiValues." );
					return fFalse;
					}
				if ( pheader->FLongValue()
					|| pheader->FSLV()
					|| pheader->FSeparated() )	//	even with UNLIMITED_MULTIVALUES, we would make this a true MULTIVALUES before separating it
					{
					(*pcprintf)( "TAGFLD %d is marked as TwoValues but cannot be a LongValue, an SLV, or Separated.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "A TAGFLD marked as TwoValues cannot be a LongValue, an SLV, or Separated." );
					return fFalse;
					}

				if ( itagfld == CTaggedColumns() - 1
					&& !FIsValidTwoValues( itagfld, pcprintf ) )
					{
					return fFalse;
					}

				fPrevWasTwoValues = fTrue;
				}

			else if ( pheader->FMultiValues() )
				{
				if ( pheader->FSeparated() )
					{
#ifdef UNLIMITED_MULTIVALUES
					fPrevWasSeparated = fTrue;
#else
					(*pcprintf)( "Separated multi-value list not currently supported.\r\n" );
					AssertSz( fFalse, "Separated multi-value list not currently supported." );
					return fFalse;
#endif					
					}

				if ( itagfld == CTaggedColumns() - 1
					&& !FIsValidMultiValues( itagfld, pcprintf ) )
					{
					return fFalse;
					}

				fPrevWasMultiValues = fTrue;
				}

			else if ( pheader->FSeparated() )
				{
				if ( !pheader->FColumnCanBeSeparated() )
					{
					(*pcprintf)( "Separated column %d is not a LongValue or an SLV.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Separated column is not a LongValue or an SLV." );
					return fFalse;
					}
				
				if ( itagfld == CTaggedColumns() - 1
					&& ptagfld->Ib() + sizeof(TAGFLD_HEADER) + sizeof(LID) != CbTaggedColumns() )
					{
					//	if last column is NULL, it must point to the end of the tagged data
					(*pcprintf)( "Separated column %d has invalid LID.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Separated column has invalid LID." );
					return fFalse;
					}

				//	column length will be checked on next iteration
				fPrevWasSeparated = fTrue;
				}
			}

		else if ( itagfld == CTaggedColumns() - 1
			&& CbTaggedColumns() - ptagfld->Ib() > JET_cbColumnMost )
			{
			(*pcprintf)( "Column %d is greater than 255 bytes, but is not a LongValue or SLV column.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
			return fFalse;
			}
		}

	return fTrue;
	}


BOOL TAGFIELDS::FIsValidTagfields(
	const DATA&		dataRec,
	const CPRINTF	* const pcprintf )
	{
	if ( NULL == dataRec.Pv()
		|| dataRec.Cb() < REC::cbRecordMin
		|| dataRec.Cb() > REC::CbRecordMax() )
		{
		(*pcprintf)( "Record is an invalid size.\r\n" );
		AssertSz( fGlobalRepair, "Record is an invalid size." );
		return fFalse;
		}

	const REC	* prec						= (REC *)dataRec.Pv();
	const BYTE	* pbRecMax					= (BYTE *)prec + dataRec.Cb();

	//	WARNING: PbTaggedData() could GPF if the record is messed up
	const BYTE	* pbStartOfTaggedColumns	= prec->PbTaggedData();

	if ( pbStartOfTaggedColumns < (BYTE *)dataRec.Pv() + REC::cbRecordMin
		|| pbStartOfTaggedColumns > pbRecMax )
		{
		(*pcprintf)( "Start of tagged columns is out of record range.\r\n" );
		AssertSz( fGlobalRepair, "Start of tagged columns is out of record range." );
		return fFalse;
		}

	const SIZE_T	cbTaggedColumns				= pbRecMax - pbStartOfTaggedColumns;
	if ( cbTaggedColumns > 0 )
		{
		//	there's at least some tagged data
		const TAGFLD	* const ptagfldFirst	= (TAGFLD *)pbStartOfTaggedColumns;

		if ( ptagfldFirst->Ib() < sizeof(TAGFLD)		//	must be at least one TAGFLD
			|| ptagfldFirst->Ib() > cbTaggedColumns
			|| ptagfldFirst->Ib() % sizeof(TAGFLD) != 0 )
			{
			(*pcprintf)( "First TAGFLD has an invalid Ib.\r\n" );
			AssertSz( fGlobalRepair, "First TAGFLD has an invalid Ib." );
			return fFalse;
			}
		}

	//	at this point, it should be safe to call the constructor
	TAGFIELDS	tagfields( dataRec );
	return tagfields.FValidate( pcprintf );
	}


//  ****************************************************************
//	TAGFLD_ITERATOR
//  ****************************************************************


TAGFLD_ITERATOR::TAGFLD_ITERATOR()
	{
	}

	
TAGFLD_ITERATOR::~TAGFLD_ITERATOR()
	{
	}


INT TAGFLD_ITERATOR::Ctags() const
	{
	return 0;
	}

	
ERR TAGFLD_ITERATOR::ErrSetItag( const INT itag )
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR::MoveBeforeFirst()
	{
	}

	
VOID TAGFLD_ITERATOR::MoveAfterLast()
	{
	}


ERR TAGFLD_ITERATOR::ErrMovePrev()
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}

	
ERR TAGFLD_ITERATOR::ErrMoveNext()
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


INT TAGFLD_ITERATOR::Itag() const
	{
	Assert( fFalse );
	return 0;
	}
	

BOOL TAGFLD_ITERATOR::FSeparated() const
	{
	Assert( fFalse );
	return fFalse;
	}
	

INT TAGFLD_ITERATOR::CbData() const
	{
	Assert( fFalse );
	return 0;
	}
	

const BYTE * TAGFLD_ITERATOR::PbData() const
	{
	Assert( fFalse );
	return NULL;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_INVALID
//  ****************************************************************


class TAGFLD_ITERATOR_INVALID : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_INVALID() {}
		~TAGFLD_ITERATOR_INVALID() {}
	};


//  ****************************************************************
//	TAGFLD_ITERATOR_NULLVALUE
//  ****************************************************************


class TAGFLD_ITERATOR_NULLVALUE : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_NULLVALUE() {}
		~TAGFLD_ITERATOR_NULLVALUE() {}
	};


//  ****************************************************************
//	TAGFLD_ITERATOR_SINGLEVALUE
//  ****************************************************************


class TAGFLD_ITERATOR_SINGLEVALUE : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_SINGLEVALUE( const DATA& data, const BOOL fSeparated );
		~TAGFLD_ITERATOR_SINGLEVALUE();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:
		const BOOL 			m_fSeparated;
		const INT 			m_cbData;
		const BYTE * const 	m_pbData;

		INT m_itag;	//	our current location
	};

INT 	TAGFLD_ITERATOR_SINGLEVALUE::Ctags() 		const { return 1; }
INT 	TAGFLD_ITERATOR_SINGLEVALUE::Itag() 		const { return ( 1 == m_itag ) ? 1 : 0; }
BOOL 	TAGFLD_ITERATOR_SINGLEVALUE::FSeparated() 	const { return ( 1 == m_itag ) ? m_fSeparated : fFalse; }
INT 	TAGFLD_ITERATOR_SINGLEVALUE::CbData() 		const { return ( 1 == m_itag ) ? m_cbData : 0; }

const BYTE * TAGFLD_ITERATOR_SINGLEVALUE::PbData() const { return m_pbData; }

TAGFLD_ITERATOR_SINGLEVALUE::TAGFLD_ITERATOR_SINGLEVALUE( const DATA& data, const BOOL fSeparated ) :
		m_fSeparated( fSeparated ),
		m_cbData( data.Cb() ),
		m_pbData( reinterpret_cast<BYTE *>( data.Pv() ) ),
		m_itag( 0 )
	{
	}			


TAGFLD_ITERATOR_SINGLEVALUE::~TAGFLD_ITERATOR_SINGLEVALUE()
	{
	}


ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrSetItag( const INT itag )
	{
	if( 1 == itag )
		{
		m_itag = 1;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_SINGLEVALUE::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_SINGLEVALUE::MoveAfterLast()
	{
	m_itag = 2;
	}


ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrMovePrev()
	{
	ERR	err;
	switch( m_itag )
		{
		case 2:
			m_itag = 1;
			err = JET_errSuccess;
			break;
		case 1:
		case 0:
			MoveBeforeFirst();
			err = ErrERRCheck( JET_errNoCurrentRecord );
			break;
		default:
			Assert( fFalse );
			err = ErrERRCheck( JET_errInternalError );
			break;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrMoveNext()
	{
	ERR	err;
	switch( m_itag )
		{
		case 0:
			m_itag = 1;
			err = JET_errSuccess;
			break;
		case 1:
		case 2:
			MoveAfterLast();
			err = ErrERRCheck( JET_errNoCurrentRecord );
			break;
		default:
			Assert( fFalse );
			err = ErrERRCheck( JET_errInternalError );
			break;
		}
	return err;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_TWOVALUES
//  ****************************************************************


class TAGFLD_ITERATOR_TWOVALUES : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_TWOVALUES( const DATA& data );
		~TAGFLD_ITERATOR_TWOVALUES();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:

		const TWOVALUES m_twovalues;
		INT m_itag;	//	our current location
	};


TAGFLD_ITERATOR_TWOVALUES::TAGFLD_ITERATOR_TWOVALUES( const DATA& data ) :
	m_twovalues( reinterpret_cast<BYTE *>( data.Pv() ), data.Cb() ),
	m_itag( 0 )
	{
	}


TAGFLD_ITERATOR_TWOVALUES::~TAGFLD_ITERATOR_TWOVALUES()
	{
	}


INT TAGFLD_ITERATOR_TWOVALUES::Ctags() const
	{
	return 2;
	}

	
ERR TAGFLD_ITERATOR_TWOVALUES::ErrSetItag( const INT itag )
	{
	if( 1 == itag
		|| 2 == itag )
		{
		m_itag = 1;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_TWOVALUES::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_TWOVALUES::MoveAfterLast()
	{
	m_itag = 3;
	}


ERR TAGFLD_ITERATOR_TWOVALUES::ErrMovePrev()
	{
	ERR err;
	if( --m_itag < 1 )
		{
		MoveBeforeFirst();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_TWOVALUES::ErrMoveNext()
	{
	ERR err;
	if( ++m_itag > 2 )
		{
		MoveAfterLast();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}


INT TAGFLD_ITERATOR_TWOVALUES::Itag() const
	{
	if( 1 == m_itag || 2 == m_itag )
		{
		return m_itag;
		}
	return 0;
	}

	
BOOL TAGFLD_ITERATOR_TWOVALUES::FSeparated() const
	{
	//	TWOVALUES are never used for LV columns
	return fFalse;
	}

	
INT TAGFLD_ITERATOR_TWOVALUES::CbData() const
	{
	int cbData;
	switch( m_itag )
		{
		case 2:
			cbData = m_twovalues.CbSecondValue();
			break;
		case 1:
			cbData = m_twovalues.CbFirstValue();
			break;
		case 0:
			Assert( fFalse );
			cbData = 0;
			break;
		default:
			Assert( fFalse );
			cbData = 0xffffffff;
			break;
		}
	return cbData;
	}

	
const BYTE * TAGFLD_ITERATOR_TWOVALUES::PbData() const
	{
	const BYTE * pbData;
	switch( m_itag )
		{
		case 2:
			pbData = m_twovalues.PbData() + m_twovalues.CbFirstValue();
			break;
		case 1:
			pbData = m_twovalues.PbData();
			break;
		case 0:
			Assert( fFalse );
			pbData = 0;
			break;
		default:
			Assert( fFalse );
			pbData = (BYTE *)(~0);
			break;
		}
	return pbData;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_MULTIVALUES
//  ****************************************************************


class TAGFLD_ITERATOR_MULTIVALUES : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_MULTIVALUES( const DATA& data );
		~TAGFLD_ITERATOR_MULTIVALUES();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:

		const MULTIVALUES m_multivalues;
		INT m_itag;	//	our current location
	};


TAGFLD_ITERATOR_MULTIVALUES::TAGFLD_ITERATOR_MULTIVALUES( const DATA& data ) :
	m_multivalues( reinterpret_cast<BYTE *>( data.Pv() ), data.Cb() ),
	m_itag( 0 )
	{
	}


TAGFLD_ITERATOR_MULTIVALUES::~TAGFLD_ITERATOR_MULTIVALUES()
	{
	}


INT TAGFLD_ITERATOR_MULTIVALUES::Ctags() const
	{
	return m_multivalues.CMultiValues();
	}

	
ERR TAGFLD_ITERATOR_MULTIVALUES::ErrSetItag( const INT itag )
	{
	if( itag > 0
		&& itag <= m_multivalues.CMultiValues() )
		{
		m_itag = itag;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_MULTIVALUES::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_MULTIVALUES::MoveAfterLast()
	{
	m_itag = m_multivalues.CMultiValues() + 1;
	}


ERR TAGFLD_ITERATOR_MULTIVALUES::ErrMovePrev()
	{
	ERR err;
	if( --m_itag < 1 )
		{
		MoveBeforeFirst();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_MULTIVALUES::ErrMoveNext()
	{
	ERR err;
	if( ++m_itag > m_multivalues.CMultiValues() )
		{
		MoveAfterLast();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}


INT TAGFLD_ITERATOR_MULTIVALUES::Itag() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_itag;
		}
	return 0;
	}


BOOL TAGFLD_ITERATOR_MULTIVALUES::FSeparated() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.FSeparatedInstance( m_itag - 1 );
		}
	return 0;
	}

	
INT TAGFLD_ITERATOR_MULTIVALUES::CbData() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.CbData( m_itag - 1 );
		}
	return 0;
	}


const BYTE * TAGFLD_ITERATOR_MULTIVALUES::PbData() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.PbData( m_itag - 1 );
		}
	return 0;
	}


//  ****************************************************************
//	TAGFIELDS_ITERATOR
//  ****************************************************************


TAGFIELDS_ITERATOR::TAGFIELDS_ITERATOR( const DATA& dataRec ) :
	m_tagfields( dataRec ),
	m_ptagfldMic( m_tagfields.Rgtagfld() - 1 ),
	m_ptagfldMax( m_tagfields.Rgtagfld() + m_tagfields.CTaggedColumns() ),
	m_ptagfldCurr( m_ptagfldMic ),
	m_ptagflditerator( new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID )
	{
	}

	
TAGFIELDS_ITERATOR::~TAGFIELDS_ITERATOR()
	{
	}


#ifdef DEBUG

VOID TAGFIELDS_ITERATOR::AssertValid() const
	{
	Assert( m_ptagflditerator == (TAGFLD_ITERATOR *)m_rgbTagfldIteratorBuf );
	Assert( m_ptagfldCurr >= m_ptagfldMic );
	Assert( m_ptagfldCurr <= m_ptagfldMax );
	}

#endif

VOID TAGFIELDS_ITERATOR::MoveBeforeFirst()
	{
	m_ptagfldCurr = m_ptagfldMic;
	new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID; 
	}

	
VOID TAGFIELDS_ITERATOR::MoveAfterLast()
	{
	m_ptagfldCurr = m_ptagfldMax;
	new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID; 
	}


ERR TAGFIELDS_ITERATOR::ErrMovePrev()
	{
	if( --m_ptagfldCurr <= m_ptagfldMic )
		{
		MoveBeforeFirst();
		return ErrERRCheck( JET_errNoCurrentRecord );
		}
	CreateTagfldIterator_();
	return JET_errSuccess;
	}

	
ERR TAGFIELDS_ITERATOR::ErrMoveNext()
	{
	if( ++m_ptagfldCurr >= m_ptagfldMax )
		{
		MoveAfterLast();
		return ErrERRCheck( JET_errNoCurrentRecord );
		}
	CreateTagfldIterator_();
	return JET_errSuccess;
	}


FID TAGFIELDS_ITERATOR::Fid() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->Fid();
	}


COLUMNID TAGFIELDS_ITERATOR::Columnid( const TDB * const ptdb ) const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->Columnid( ptdb );
	}


BOOL TAGFIELDS_ITERATOR::FTemplateColumn( const TDB * const ptdb ) const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->FTemplateColumn( ptdb );
	}


BOOL TAGFIELDS_ITERATOR::FNull() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	return m_ptagfldCurr->FNull();
	}


BOOL TAGFIELDS_ITERATOR::FDerived() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	return m_ptagfldCurr->FDerived();
	}

	
BOOL TAGFIELDS_ITERATOR::FLV() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	if( !m_ptagfldCurr->FExtendedInfo() )
		{
		return fFalse;
		}
	const BYTE * const pbData = m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
	const BYTE bExtendedInfo  = *pbData;
	return bExtendedInfo & TAGFLD_HEADER::fLongValue;
	}

	
BOOL TAGFIELDS_ITERATOR::FSLV() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	if( !m_ptagfldCurr->FExtendedInfo() )
		{
		return fFalse;
		}
	const BYTE * const pbData = m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
	const BYTE bExtendedInfo  = *pbData;
	return bExtendedInfo & TAGFLD_HEADER::fSLV;
	}


TAGFLD_ITERATOR& TAGFIELDS_ITERATOR::TagfldIterator()
	{
	return *m_ptagflditerator;
	}

	
const TAGFLD_ITERATOR& TAGFIELDS_ITERATOR::TagfldIterator() const
	{
	return *m_ptagflditerator;
	}


VOID TAGFIELDS_ITERATOR::CreateTagfldIterator_()
	{
	Assert( m_ptagfldCurr > m_ptagfldMic );
	Assert( m_ptagfldCurr < m_ptagfldMax );
	if( m_ptagfldCurr->FNull() )
		{
		Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_NULLVALUE ) );
		new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_NULLVALUE; 
		}
	else
		{
		DATA data;
		
		const BYTE * const pbData 	= m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
		const SIZE_T cbData 		= m_tagfields.CbData( ULONG( m_ptagfldCurr - m_ptagfldMic - 1 ) );

		data.SetPv( const_cast<BYTE *>( pbData ) );
		data.SetCb( cbData );

		if( !m_ptagfldCurr->FExtendedInfo() )
			{
			
			//	ordinary value

			Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_SINGLEVALUE ) );
			new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_SINGLEVALUE( data, fFalse ); 
			
			}
		else
			{
			const BYTE bExtendedInfo  = *pbData;
			if( bExtendedInfo & TAGFLD_HEADER::fTwoValues )
				{
				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_TWOVALUES ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_TWOVALUES( data ); 
				}
			else if( bExtendedInfo & TAGFLD_HEADER::fMultiValues )
				{
				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_MULTIVALUES ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_MULTIVALUES( data ); 
				}
			else
				{

				//	ordinary column with header byte. skip the header byte
				
				const BOOL fSeparated = bExtendedInfo & TAGFLD_HEADER::fSeparated;
				data.DeltaPv( sizeof( TAGFLD_HEADER ) );
				data.DeltaCb( - (ULONG)sizeof( TAGFLD_HEADER ) );

				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_SINGLEVALUE ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_SINGLEVALUE( data, fSeparated ); 
				
				}				
			}
		}
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\ver.cxx ===
/***********************************************************
  
  There are several layers

  RCE
	These need a bookmark, ifmp and pgnoFDP to identify a
	version
	A single hash table is used to access RCEs.  Typical hash
	overflow is supported

  VER
	These use FUCB's and CSR's

*************************************************************/


#include "std.hxx"
#include "_bt.hxx"
#include "_ver.hxx"


///#define BREAK_ON_PREFERRED_BUCKET_LIMIT
#define	MOVEABLE_RCE

#ifdef DEBUG

//	DEBUG_VER:  check the consistency of the version store hash table
///#define DEBUG_VER

//  DEBUG_VER_EXPENSIVE:  time-consuming version store consistency check
///#define DEBUG_VER_EXPENSIVE

#endif	//  DEBUG


const DIRFLAG	fDIRUndo = fDIRNoLog | fDIRNoVersion | fDIRNoDirty;

//  ****************************************************************
//  GLOBALS
//  ****************************************************************

//  exported
volatile LONG	crefVERCreateIndexLock	= 0;

//	the bucket resource pool, shared among all instance
#ifdef GLOBAL_VERSTORE_MEMPOOL
CRES		*g_pcresVERPool 						= NULL;
#endif

//  the last global RCE id assigned
ULONG	RCE::rceidLast = rceidMin;


//  ****************************************************************
//  VER class
//  ****************************************************************

VER::VER( INST *pinst ) : 
		m_pinst( pinst ),
		m_fVERCleanUpWait( 2 ),
		m_asigRCECleanDone( CSyncBasicInfo( _T( "m_asigRCECleanDone" ) ) ),
		m_critRCEClean( CLockBasicInfo( CSyncBasicInfo( szRCEClean ), rankRCEClean, 0 ) ),
		m_critBucketGlobal( CLockBasicInfo( CSyncBasicInfo( szBucketGlobal ), rankBucketGlobal, 0 ) ),		
		m_cbucketGlobalAlloc( 0 ),
		m_cbucketGlobalAllocDelete( 0 ),
		m_cbucketDynamicAlloc( 0 ),
		m_pbucketGlobalHead( pbucketNil ),
		m_pbucketGlobalTail( pbucketNil ),
		m_pbucketGlobalLastDelete( pbucketNil ),
		m_cbucketGlobalAllocMost( 0 ),
		m_cbucketGlobalAllocPreferred( 0 ),
		m_ulVERTasksPostMax( g_ulVERTasksPostMax ),
		m_ppibRCEClean( ppibNil ),
		m_ppibRCECleanCallback( ppibNil ),
		m_fSyncronousTasks( fFalse ),
		m_trxBegin0LastLongRunningTransaction( trxMin ),
		m_ppibTrxOldestLastLongRunningTransaction( ppibNil ),
		m_dwTrxContextLastLongRunningTransaction( 0 )
#ifdef VERPERF
		,
		m_cbucketCleaned( 0 ),
		m_cbucketSeen( 0 ),
		m_crceSeen( 0 ),
		m_crceCleaned( 0 ),
		m_crceFlagDelete( 0 ),
		m_crceDeleteLV( 0 ),
		m_crceMoved( 0 ),
		m_crceDiscarded( 0 ),
		m_crceMovedDeleted( 0 ),
		m_critVERPerf( CLockBasicInfo( CSyncBasicInfo( szVERPerf ), rankVERPerf, 0 ) ),
#endif
#ifdef GLOBAL_VERSTORE_MEMPOOL
		m_pcresVERPool( g_pcresVERPool )
#else
		m_pcresVERPool( NULL )
#endif
		{
#ifdef VERPERF
		qwStartTicks = QwUtilHRTCount();
		m_rgcRCEHashChainLengths = (LONG *)PvOSMemoryHeapAlloc( crceheadGlobalHashTable * sizeof( m_rgcRCEHashChainLengths[0] ) );
#endif
		}
		
VER::~VER()
		{
		OSMemoryHeapFree( m_rgcRCEHashChainLengths );
		}


//  ****************************************************************
//  BUCKET LAYER
//  ****************************************************************
//
//  A bucket is a contiguous block of memory used to hold versions.
//
//-

//  ================================================================
INLINE INT CbBUFree( const BUCKET * pbucket )
//  ================================================================
//
//	the cast to (INT) below is necessary to catch the negative case
//
//-
	{
	const INT_PTR	cbBUFree =
			reinterpret_cast<INT_PTR>( pbucket + 1 ) - reinterpret_cast<INT_PTR> (pbucket->hdr.prceNextNew );
	Assert( cbBUFree >= 0 );
	Assert( cbBUFree < sizeof(BUCKET) );
	return (INT)cbBUFree;
	}


//  ================================================================
INLINE BUCKET *VER::PbucketVERIAlloc( const CHAR* szFile, ULONG ulLine )
//  ================================================================
	{
#ifdef GLOBAL_VERSTORE_MEMPOOL
	//	GlobalAlloc should always be less than or equal
	//	to GlobalAllocMost, but our check should err
	//	on the side of safety just in case
	Assert( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocMost );
	if ( m_cbucketGlobalAlloc >= m_cbucketGlobalAllocMost )
		{
		// don't exceed per instance max
		return NULL;
		}
	Assert ( g_pcresVERPool );
	BUCKET *pbucket = reinterpret_cast<BUCKET *>( g_pcresVERPool->PbAlloc( szFile, ulLine ) );
#else
	Assert ( m_pcresVERPool );
	BUCKET *pbucket = reinterpret_cast<BUCKET *>( m_pcresVERPool->PbAlloc( szFile, ulLine ) );
#endif
	Assert( m_cbucketGlobalAlloc >= 0 );
	if ( NULL != pbucket )
		{
		++m_cbucketGlobalAlloc;

		if ( !g_pcresVERPool->FContains( (BYTE *)pbucket ) )
			m_cbucketDynamicAlloc++;

#ifdef VERPERF
		extern PERFInstanceG<> cVERcbucketAllocated;
		cVERcbucketAllocated.Inc( m_pinst );
#endif // VERPERF
#ifdef BREAK_ON_PREFERRED_BUCKET_LIMIT
		AssertRTL( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocPreferred );
#endif
		}
	return pbucket;
	}
#define PbucketMEMAlloc()		PbucketVERIAlloc( __FILE__, __LINE__ )


//  ================================================================
INLINE VOID VER::VERIReleasePbucket( BUCKET * pbucket )
//  ================================================================
	{
	Assert( m_cbucketGlobalAlloc > 0 );
	m_cbucketGlobalAlloc--;

	if ( !g_pcresVERPool->FContains( (BYTE *)pbucket ) )
		m_cbucketDynamicAlloc--;

#ifdef VERPERF
		extern PERFInstanceG<> cVERcbucketAllocated;
		cVERcbucketAllocated.Dec( m_pinst );
#endif // VERPERF
#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
	g_pcresVERPool->Release( reinterpret_cast<BYTE *>( pbucket ) );
#else
	Assert ( m_pcresVERPool );
	m_pcresVERPool->Release( reinterpret_cast<BYTE *>( pbucket ) );
#endif
	}


//  ================================================================
INLINE BOOL VER::FVERICleanWithoutIO()
//  ================================================================
//
//  Should we move FlagDelete RCE's instead of cleaning them up
//  (only if not cleaning the last bucket)
//
	{
#ifdef MOVEABLE_RCE	
#ifdef DEBUG
	const double dblThresh = 0.3;
#else
	const double dblThresh = 0.8;
#endif	//	DEBUG
	const BOOL	fRCECleanWithoutIO	= ( m_pbucketGlobalHead != m_pbucketGlobalTail )
										&& ( ( (double)m_cbucketGlobalAlloc > ( (double)m_cbucketGlobalAllocPreferred * dblThresh ) )
											|| ( ( m_cbucketGlobalAllocMost - m_cbucketGlobalAlloc ) < 5 )
											|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )
										&& !m_pinst->m_plog->m_fRecovering
										&& !m_pinst->m_fTermInProgress;
#else
	const BOOL	fRCECleanWithoutIO	= fFalse;
#endif
	
	return fRCECleanWithoutIO;
	}


//  ================================================================
INLINE BOOL VER::FVERICleanDiscardDeletes()
//  ================================================================
//
//  If the version store is really full we will simply discard the 
//  RCEs (only if not cleaning the last bucket)
//  
	{
	BOOL fRCECleanDiscardDeletes;

	//  discard deletes if over 25% of the version store is used to store moved deletes
	//  or we are over our preferred threshold or there are fewer than two buckets left
	fRCECleanDiscardDeletes =
		( m_cbucketGlobalAlloc > m_cbucketGlobalAllocPreferred )
		|| (
				( m_pbucketGlobalHead != m_pbucketGlobalTail )
				&& (
					(double)m_cbucketGlobalAllocDelete > ( (double)m_cbucketGlobalAllocMost * 0.25 )
					|| ( ( m_cbucketGlobalAllocMost - m_cbucketGlobalAlloc ) < 2 )
					)
			);
							
	
	return fRCECleanDiscardDeletes;
	}

LOCAL VOID VER::VERIReportDiscardedDeletes( RCE * const prce )
	{
	Assert( m_critRCEClean.FOwner() );

	//	if OLD is already running on this database,
	//	don't bother reporting discarded deletes

	//	we keep track of the NEWEST trx at the time discards were last
	//	reported, and will only generate future discard reports for
	//	deletes that occurred after the current trxNewest (this is a
	//	means of ensuring we don't report too often)

	FMP * const	pfmp	= rgfmp + prce->Ifmp();

	if ( !pfmp->FRunningOLD()
		&& TrxCmp( pfmp->TrxNewestWhenDiscardsLastReported(), prce->TrxBegin0() ) >= 0 )
		{
		const CHAR	* rgszT[1]	= { pfmp->SzDatabaseName() };
		UtilReportEvent(
				eventWarning,
				PERFORMANCE_CATEGORY,
				MANY_LOST_COMPACTION_ID,
				1,
				rgszT );

		//	this ensures no further reports will be generated for
		//	deletes which were present in the version store at
		//	the time this report was generated
		pfmp->SetTrxNewestWhenDiscardsLastReported( m_pinst->m_trxNewest );
		}
	}

LOCAL VOID VER::VERIReportVersionStoreOOM()
	{
	//	only called by ErrVERIBUAllocBucket()
	Assert( m_critBucketGlobal.FOwner() );

	m_pinst->m_critPIBTrxOldest.Enter();

	PIB	* const		ppibTrxOldest		= (PIB *)m_pinst->m_ppibTrxOldest;

	//	only generate the eventlog entry once per long-running transaction
	if ( ppibNil != ppibTrxOldest )
		{
		const TRX	trxBegin0			= ppibTrxOldest->trxBegin0;
		
		if ( trxBegin0 != m_trxBegin0LastLongRunningTransaction )
			{
			CHAR	szSession[32];
			CHAR	szSesContext[32]; 
			CHAR	szSesContextThreadID[32];

			sprintf( szSession, "0x%p", ppibTrxOldest );
			if ( ppibTrxOldest->FUseSessionContextForTrxContext() )
				{
				sprintf( szSesContext, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwSessionContext ) );
				sprintf( szSesContextThreadID, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwSessionContextThreadId ) );
				}
			else
				{
				sprintf( szSesContext, "0x%08X", 0 );
				sprintf( szSesContextThreadID, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwTrxContext ) );
				}

			Assert( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocMost );
			if ( m_cbucketGlobalAlloc < m_cbucketGlobalAllocMost )
				{
				//	OOM because NT returned OOM
				const UINT	csz		= 7;
				const CHAR	* rgsz[csz];
				CHAR		rgszDw[4][16];
				
				sprintf( rgszDw[0], "%d", IpinstFromPinst( m_pinst ) );
				sprintf( rgszDw[1], "%d", ( m_cbucketGlobalAlloc * cbBucket ) / ( 1024 * 1024 ) );
				sprintf( rgszDw[2], "%d", ( m_cbucketGlobalAllocMost * cbBucket ) / ( 1024 * 1024 ) );
				sprintf( rgszDw[3], "%d", ( g_lVerPagesMin * cbBucket ) / ( 1024 * 1024 ) );

				rgsz[0] = rgszDw[0];
				rgsz[1] = rgszDw[1];
				rgsz[2] = rgszDw[2];
				rgsz[3] = rgszDw[3];
				rgsz[4] = szSession;
				rgsz[5] = szSesContext;
				rgsz[6] = szSesContextThreadID;

				UtilReportEvent(
						eventError,
						TRANSACTION_MANAGER_CATEGORY,
						VERSION_STORE_OUT_OF_MEMORY_ID,
						csz,
						rgsz,
						0,
						NULL, 
						m_pinst );
				}
			else
				{
				//	OOM because the user-specified max. has been reached
				const UINT	csz		= 5;
				const CHAR	* rgsz[csz];
				CHAR		rgszInst[16];
				CHAR		rgszMaxVerPages[16];

				sprintf( rgszInst, "%d", IpinstFromPinst( m_pinst ) );
				sprintf( rgszMaxVerPages, "%d", ( m_cbucketGlobalAllocMost * cbBucket ) / ( 1024 * 1024 ) );

				rgsz[0] = rgszInst;
				rgsz[1] = rgszMaxVerPages;
				rgsz[2] = szSession;
				rgsz[3] = szSesContext;
				rgsz[4] = szSesContextThreadID;

				UtilReportEvent(
						eventError,
						TRANSACTION_MANAGER_CATEGORY,
						VERSION_STORE_REACHED_MAXIMUM_ID,
						csz,
						rgsz,
						0,
						NULL,
						m_pinst );
				}
			
			m_trxBegin0LastLongRunningTransaction = trxBegin0;
			m_ppibTrxOldestLastLongRunningTransaction = ppibTrxOldest;
			m_dwTrxContextLastLongRunningTransaction = ( ppibTrxOldest->FUseSessionContextForTrxContext() ?
															ppibTrxOldest->dwSessionContextThreadId :
															ppibTrxOldest->dwTrxContext );
			}
		}

	m_pinst->m_critPIBTrxOldest.Leave();
	}


//  ================================================================
INLINE ERR VER::ErrVERIBUAllocBucket()
//  ================================================================
//
//	Inserts a bucket to the top of the bucket chain, so that new RCEs
//	can be inserted.  Note that the caller must set ibNewestRCE himself.
//
//-
	{
	//	use m_critBucketGlobal to make sure only one allocation
	//	occurs at one time.
	Assert( m_critBucketGlobal.FOwner() );

	//	signal RCE clean in case it can now do work
	VERSignalCleanup();

	//	Must allocate within m_critBucketGlobal to make sure that
	//  the allocated bucket will be in circularly increasing order.
	BUCKET * const pbucket = PbucketMEMAlloc();

	//	We are really out of bucket, return to high level function
	//	call to retry.
	if ( pbucketNil == pbucket )
		{
		//	return the higher level (DIR) to release the page latch and retry
		VERIReportVersionStoreOOM();
		return ErrERRCheck( JET_errVersionStoreOutOfMemory );
		}

	//	ensure RCE's will be properly aligned
	Assert( FAlignedForAllPlatforms( pbucket ) );
	Assert( FAlignedForAllPlatforms( pbucket->rgb ) );
	Assert( (BYTE *)PvAlignForAllPlatforms( pbucket->rgb ) == pbucket->rgb );

	//	Initialize the bucket.
	pbucket->hdr.prceOldest		= (RCE *)pbucket->rgb;
	pbucket->hdr.prceNextNew	= (RCE *)pbucket->rgb;
	pbucket->hdr.pbLastDelete	= pbucket->rgb;
	pbucket->hdr.pbucketNext	= pbucketNil;
	
	//	Link up the bucket to global list.
	pbucket->hdr.pbucketPrev = m_pbucketGlobalHead;
	if ( pbucket->hdr.pbucketPrev )
		{
		//  there is a bucket after us
		pbucket->hdr.pbucketPrev->hdr.pbucketNext = pbucket;
		}
	else
		{
		//  we are last in the chain
		m_pbucketGlobalTail = pbucket;
		}
	m_pbucketGlobalHead = pbucket;

	return JET_errSuccess;
	}


//  ================================================================
INLINE BUCKET *VER::PbucketVERIGetOldest( )
//  ================================================================
//
//	find the oldest bucket in the bucket chain
//
//-
	{
	Assert( m_critBucketGlobal.FOwner() );

	BUCKET  * const pbucket = m_pbucketGlobalTail;

	Assert( pbucketNil == pbucket || pbucketNil == pbucket->hdr.pbucketPrev );
	return pbucket;
	}


//  ================================================================
BUCKET *VER::PbucketVERIFreeAndGetNextOldestBucket( BUCKET * pbucket )
//  ================================================================
	{
	Assert( m_critBucketGlobal.FOwner() );

	BUCKET * const pbucketNext = (BUCKET *)pbucket->hdr.pbucketNext;
	BUCKET * const pbucketPrev = (BUCKET *)pbucket->hdr.pbucketPrev;

	if ( pbucket == m_pbucketGlobalLastDelete )
		{
		m_pbucketGlobalLastDelete = pbucketNil;
		}

	//	unlink bucket from bucket chain and free.
	if ( pbucketNil != pbucketNext )
		{
		Assert( m_pbucketGlobalHead != pbucket );
		pbucketNext->hdr.pbucketPrev = pbucketPrev;
		}
	else	//  ( pbucketNil == pbucketNext )
		{
		Assert( m_pbucketGlobalHead == pbucket );
		m_pbucketGlobalHead = pbucketPrev;
		}

	if ( pbucketNil != pbucketPrev )
		{
		Assert( m_pbucketGlobalTail != pbucket );
		pbucketPrev->hdr.pbucketNext = pbucketNext;
		}
	else	//  ( pbucketNil == pbucketPrev )
		{
		m_pbucketGlobalTail = pbucketNext;
		}

	Assert( ( m_pbucketGlobalHead && m_pbucketGlobalTail ) 
			|| ( !m_pbucketGlobalHead && !m_pbucketGlobalTail ) );

	VERIReleasePbucket( pbucket );
	return pbucketNext;
	}


//  ****************************************************************
//  PERFMON STATISTICS
//  ****************************************************************
#ifdef VERPERF

PERFInstanceG<> cVERcbucketAllocated;
PERFInstanceG<> cVERcbucketDeleteAllocated;
PERFInstance<QWORD>	cVERcrceHashUsage;
PERFInstanceG<> cVERcbBookmarkTotal;
PERFInstanceG<> cVERcrceHashEntries;
PERFInstanceG<> cVERUnnecessaryCalls;

PM_CEF_PROC LVERcbucketAllocatedCEFLPv;
PM_CEF_PROC	LVERcbucketDeleteAllocatedCEFLPv;
PM_CEF_PROC LVERcrceHashUsageCEFLPv;
PM_CEF_PROC LVERcbAverageBookmarkCEFLPv;
PM_CEF_PROC LVERUnnecessaryCallsCEFLPv;


//  ================================================================
LONG LVERcbucketAllocatedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERcbucketAllocated.PassTo( iInstance, pvBuf );
	return 0;
	}


//  ================================================================
LONG LVERcbucketDeleteAllocatedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	if ( pvBuf )
		{
		LONG counter = cVERcbucketDeleteAllocated.Get( iInstance );
		if ( counter < 0 )
			{
			cVERcbucketDeleteAllocated.Clear( iInstance );
			*((LONG *)pvBuf) = 0;
			}
		else
			{
			*((LONG *)pvBuf) = counter;
			}
		}
	return 0;
	}


//  ================================================================
LONG LVERcrceHashUsageCEFLPv( LONG iInstance, VOID * pvBuf)
//  ================================================================
	{
	if ( NULL != pvBuf )
		{
		QWORD cUsage = cVERcrceHashUsage.Get( iInstance );
		LONG cEntries = cVERcrceHashEntries.Get( iInstance );
		if ( cUsage <= 0 || cEntries <= 0 )
			{
			*(LONG *)pvBuf = 0;
			}
		else
			{
			QWORD cT = (QWORD)cEntries*cEntries/crceheadGlobalHashTable;
			if ( cUsage <= cT )
				{
				*(LONG *)pvBuf = 0;
				}
			else
				{
				*(LONG *)pvBuf = (LONG)((cUsage - cT)/crceheadGlobalHashTable);
				}
			}
		}

	return 0;
	}


//  ================================================================
LONG LVERcbAverageBookmarkCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	if ( NULL != pvBuf )
		{
		LONG cHash = cVERcrceHashEntries.Get( iInstance );
		LONG cBookmark = cVERcbBookmarkTotal.Get( iInstance );
		if ( 0 < cHash && 0 <= cBookmark )
			{
			*( LONG * )pvBuf = cBookmark/cHash;
			}
		else if ( 0 > cHash || 0 > cBookmark )
			{
			cVERcrceHashEntries.Clear( iInstance );
			cVERcbBookmarkTotal.Clear( iInstance );
			*( LONG * )pvBuf = 0;
			}
		else
			{
			*( LONG * )pvBuf = 0;
			}
		}
	return 0;
	}


//  ================================================================
LONG LVERUnnecessaryCallsCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERUnnecessaryCalls.PassTo( iInstance, pvBuf );
	return 0;
	}


#endif	//  VERPERF

//  ================================================================
INLINE CCriticalSection& VER::CritRCEChain( UINT ui )
//  ================================================================
	{
	Assert( ui < crceheadGlobalHashTable );
	return m_rgrceheadHashTable[ ui ].crit;
	}

INLINE RCE *VER::GetChain( UINT ui ) const
	{
	Assert( ui < crceheadGlobalHashTable );
	return m_rgrceheadHashTable[ ui ].prceChain;
	}

INLINE RCE **VER::PGetChain( UINT ui )
	{
	Assert( ui < crceheadGlobalHashTable );
	return &m_rgrceheadHashTable[ ui ].prceChain;
	}

INLINE VOID VER::SetChain( UINT ui, RCE *prce )
	{
	Assert( ui < crceheadGlobalHashTable );
	m_rgrceheadHashTable[ ui ].prceChain = prce;
	}

//  ================================================================
CCriticalSection& RCE::CritChain()
//  ================================================================
	{
	Assert( ::FOperInHashTable( m_oper ) );
	Assert( uiHashInvalid != m_uiHash );
	return PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash );
	}


//  ================================================================
LOCAL UINT UiRCHashFunc( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
	{
	ASSERT_VALID( &bookmark );
	Assert( pgnoNull != pgnoFDP );

	//  OPTIMIZATION:  turning on optimizations here always (#pragma)
	//  OPTIMIZATION:  unrolling the loop for speed

///#define FAST_HASH
///#define SEDGEWICK_HASH
#define SEDGEWICK_HASH2

#ifdef FAST_HASH
	UINT uiHash = ifmp + pgnoFDP +
					bookmark.key.prefix.Cb() +
					bookmark.key.suffix.Cb() +
					bookmark.data.cb;

	union {
		ULONG ul;
		BYTE rgb[4];
		} u;
	
	if ( bookmark.key.prefix.Cb() > 4 )
		uiHash += *(ULONG *)bookmark.key.prefix.pv;
	else
		{
		u.ul = 0;

		BYTE *pb = u.rgb;
		UtilMemCpy( pb, bookmark.key.prefix.pv, bookmark.key.prefix.Cb() );
		pb += bookmark.key.prefix.Cb();
		
		INT cbCopy = min( u.rgb + 4 - pb, bookmark.key.suffix.Cb() );
		UtilMemCpy( pb, bookmark.key.suffix.pv, cbCopy );
		pb += cbCopy;

		cbCopy = min( u.rgb + 4 - pb, bookmark.data.cb );
		UtilMemCpy( pb, bookmark.data.pv, cbCopy );
		
		uiHash += u.ul;
		}

	if ( bookmark.data.cb > 4 )
		{
		uiHash += *(ULONG *)( (BYTE*)bookmark.data.pv + bookmark.data.cb - 4 );
		}
	else
		{
		u.ul = 0;

		BYTE *pb = u.rgb + 4 - bookmark.data.cb;
		UtilMemCpy( pb, bookmark.data.pv, bookmark.data.cb );

		INT cbCopy = min( pb - u.rgb, bookmark.key.suffix.Cb() );
		pb -= cbCopy;
		UtilMemCpy( pb, (BYTE*)bookmark.key.suffix.pv + bookmark.key.suffix.Cb() - cbCopy, cbCopy );

		cbCopy = min( pb - u.rgb, bookmark.key.prefix.Cb() );
		pb -= cbCopy;
		UtilMemCpy( pb, (BYTE*)bookmark.key.prefix.pv + bookmark.key.prefix.Cb() - cbCopy, cbCopy );
		
		uiHash += u.ul;
		}

	uiHash %= crceheadGlobalHashTable;
#endif	//	FAST_HASH

#ifdef SEDGEWICK_HASH
	//  This is taken from "Algorithms in C++" by Sedgewick
	UINT uiHash = ( ifmp + pgnoFDP ) % crceheadGlobalHashTable;	//  bookmark is only unique in one table of a database

	INT ib;
	const BYTE * pb;

	for ( ib = 0, pb = (BYTE *)bookmark.key.prefix.pv; ib < (INT)bookmark.key.prefix.Cb(); ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
	for ( ib = 0, pb = (BYTE *)bookmark.key.suffix.pv; ib < (INT)bookmark.key.suffix.Cb(); ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
	for ( ib = 0, pb = (BYTE *)bookmark.data.pv; ib < (INT)bookmark.data.cb; ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
#endif	//	SEDGEWICK_HASH

#ifdef SEDGEWICK_HASH2
	//  An optimized version of the hash function from "Algorithms in C++" by Sedgewick

	//lint -e616 -e646 -e744
	
	UINT uiHash = 	(UINT)ifmp
					+ pgnoFDP
					+ bookmark.key.prefix.Cb()
					+ bookmark.key.suffix.Cb()
					+ bookmark.data.Cb();

	INT ib;
	INT cb;
	const BYTE * pb;

	ib = 0;
	cb = (INT)bookmark.key.prefix.Cb();
	pb = (BYTE *)bookmark.key.prefix.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:	
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}

	ib = 0;
	cb = (INT)bookmark.key.suffix.Cb();
	pb = (BYTE *)bookmark.key.suffix.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:	
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}

	ib = 0;
	cb = (INT)bookmark.data.Cb();
	pb = (BYTE *)bookmark.data.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:	
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}
		
	uiHash %= crceheadGlobalHashTable;

	//lint -restore
	
#endif	//	SEDGEWICK_HASH2

	return uiHash;
	}


#ifdef DEBUGGER_EXTENSION

//  ================================================================
UINT UiVERHash( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//  Used by the debugger extension
//
	{
	return UiRCHashFunc( ifmp, pgnoFDP, bookmark );
	}

#endif	//	DEBUGGER_EXTENSION

#ifdef DEBUG


//  ================================================================
BOOL RCE::FAssertCritHash_() const
//  ================================================================
	{
	Assert( ::FOperInHashTable( m_oper ) );
	return PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash ).FOwner();
	}


//  ================================================================
BOOL RCE::FAssertCritFCBRCEList_() const
//  ================================================================
	{
	Assert( !::FOperNull( m_oper ) );
	return Pfcb()->CritRCEList().FOwner();
	}


//  ================================================================
BOOL RCE::FAssertCritPIB_() const
//  ================================================================
	{
	Assert( trxMax == m_trxCommitted );
	Assert( !::FOperNull( m_oper ) );
	return ( m_pfucb->ppib->critTrx.FOwner() || Ptls()->fAddColumn );
	}


//  ================================================================
BOOL RCE::FAssertReadable_() const
//  ================================================================
//
//  We can read the const members of a RCE if we are holding an accessing
//  critical section or the RCE is committed but younger than the oldest active
//  transaction or the RCE is older than the oldest transaction and we are
//  RCECleanup
//
//-
	{
	AssertRTL( !::FOperNull( m_oper ) );
	return fTrue;
	}


#endif	//	DEBUG

//  ================================================================
VOID RCE::AssertValid() const
//  ================================================================
	{
	Assert( FAlignedForAllPlatforms( this  ) );
	Assert( FAssertReadable_() );
	AssertRTL( rceidMin != m_rceid );
	AssertRTL( pfcbNil != Pfcb() );
	AssertRTL( trxMax != m_trxBegin0 );
	AssertRTL( TrxCmp( m_trxBegin0, m_trxCommitted ) < 0 );
	if ( trxMax == m_trxCommitted )
		{
		ASSERT_VALID( m_pfucb );
		}
	if ( ::FOperInHashTable( m_oper ) )
		{
		BOOKMARK	bookmark;
		GetBookmark( &bookmark );
		AssertRTL( UiRCHashFunc( Ifmp(), PgnoFDP(), bookmark ) == m_uiHash );
		}
	else
		{
		//  No-one can see these members if we are not in the hash table
		AssertRTL( prceNil == m_prceNextOfNode );
		AssertRTL( prceNil == m_prcePrevOfNode );
		}
	AssertRTL( !::FOperNull( m_oper ) );

	switch( m_oper )
		{
		default:
			AssertSzRTL( fFalse, "Invalid RCE operand" );
		case operReplace:
		case operInsert:
		case operReadLock:
		case operWriteLock:
		case operPreInsert:
		case operWaitLock:
		case operFlagDelete:
		case operDelta:
		case operAllocExt:
		case operSLVSpace:
		case operCreateTable:
		case operDeleteTable:
		case operAddColumn:
		case operDeleteColumn:
		case operCreateLV:
		case operCreateIndex:
		case operDeleteIndex:
		case operSLVPageAppend:
			break;
		};
	
	if ( trxMax == m_trxCommitted )
		{
		}
	else
		{
		//  we are committed to level 0 
		AssertRTL( 0 == m_level );
		AssertRTL( prceNil == m_prceNextOfSession );
		AssertRTL( prceNil == m_prcePrevOfSession );
		}
		
	if ( ::FOperInHashTable( m_oper ) && PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash ).FOwner() )
		{
		if ( prceNil != m_prceNextOfNode )
			{
			AssertRTL( m_rceid < m_prceNextOfNode->m_rceid || operWriteLock == m_prceNextOfNode->Oper() || m_fNoWriteConflict );
			AssertRTL( this == m_prceNextOfNode->m_prcePrevOfNode );
			}
		if ( prceNil != m_prcePrevOfNode )
			{
			//	UNDONE: there's nothing preventing m_prcePrevOfNode from getting
			//	nullified, so we may crash here sometimes
			AssertRTL( m_rceid > m_prcePrevOfNode->m_rceid || operWriteLock == Oper() || m_prcePrevOfNode->m_fNoWriteConflict );
			AssertRTL( this == m_prcePrevOfNode->m_prceNextOfNode );
			if ( trxMax == m_prcePrevOfNode->m_trxCommitted )
				{
				AssertRTL( TrxCmp( m_trxCommitted, TrxVERISession( m_prcePrevOfNode->m_pfucb ) ) > 0 );
				}
			}
			
		//  these cases should have caused a write conflict in VERModify

		const BOOL fRedo = ( fRecoveringRedo == PinstFromIfmp( Ifmp() )->m_plog->m_fRecoveringMode );
		if( !fRedo )
			{
			const BOOL fUndo = ( fRecoveringUndo == PinstFromIfmp( Ifmp() )->m_plog->m_fRecoveringMode );
			if( !fUndo )
				{
				const BOOL fNoPrevRCE = ( prceNil == m_prcePrevOfNode );
				if( !fNoPrevRCE )
					{
					const BOOL fPrevRCEIsDelete = ( operFlagDelete == m_prcePrevOfNode->m_oper );
					if( fPrevRCEIsDelete )
						{
						const BOOL fRCEWasMoved = ( m_prcePrevOfNode->FMoved() );
						if( !fRCEWasMoved )
							{
							const BOOL fInserting = ( operInsert == m_oper );
							if( !fInserting )
								{
								//  operPreInsert is a prelude to an insert
								const BOOL fPreInsert = ( operPreInsert == m_oper ) || ( operWriteLock == m_oper );
								if( !fPreInsert )
									{
									//	due to OLDSLV we can do a versioned delete, 
									//	unversioned insert and then a versioned replace in the slv ownership map
									//	or slv avail trees
									const BOOL fIsSLVTree = ( m_pfcb->FTypeSLVAvail() || m_pfcb->FTypeSLVOwnerMap() );
									AssertSzRTL( fIsSLVTree, "RCEs should have write-conflicted" );
									}
								}
							}
						}
					}
				}
			}
		}
	}


#ifndef RTM


//  ================================================================
ERR VER::ErrCheckRCEChain( const RCE * const prce, const UINT uiHash ) const
//  ================================================================
	{
	const RCE	* prceChainOld	= prceNil;
	const RCE	* prceChain		= prce;
	while ( prceNil != prceChain )
		{
		AssertRTL( prceChain->PrceNextOfNode() == prceChainOld );
		prceChain->AssertValid();
		AssertRTL( prceChain->PgnoFDP() == prce->PgnoFDP() );
		AssertRTL( prceChain->Ifmp() == prce->Ifmp() );
		AssertRTL( prceChain->UiHash() == uiHash );
		AssertRTL( !prceChain->FOperDDL() );

		prceChainOld	= prceChain;
		prceChain 		= prceChain->PrcePrevOfNode();
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR VER::ErrCheckRCEHashList( const RCE * const prce, const UINT uiHash ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;
	const RCE * prceT = prce;
	for ( ; prceNil != prceT; prceT = prceT->PrceHashOverflow() )
		{
		CallR( ErrCheckRCEChain( prceT, uiHash ) );
		}
	return err;
	}


//  ================================================================
ERR VER::ErrInternalCheck()
//  ================================================================
//
//  check the consistency of the entire hash table, entering the
//  critical sections if needed
//
//-
	{
	ERR err = JET_errSuccess;
	UINT uiHash = 0;
	for ( ; uiHash < crceheadGlobalHashTable; ++uiHash )
		{
		if( NULL != GetChain( uiHash ) )
			{
			ENTERCRITICALSECTION crit( &(CritRCEChain( uiHash )) );
			const RCE * const prce = GetChain( uiHash );
			CallR( ErrCheckRCEHashList( prce, uiHash ) );
			}
		}
	return err;
	}


#endif  //  !RTM


//  ================================================================
INLINE RCE::RCE( 
			FCB		*pfcb,
			FUCB	*pfucb,
			UPDATEID	updateid,
			TRX		trxBegin0,
			LEVEL	level,
			USHORT	cbBookmarkKey,
			USHORT	cbBookmarkData,
			USHORT	cbData,
			OPER	oper,
			BOOL	fDoesNotWriteConflict,
			UINT	uiHash,
			BOOL	fProxy,
			RCEID	rceid
			) :
//  ================================================================
	m_pfcb( pfcb ),
	m_pfucb( pfucb ),
	m_ifmp( pfcb->Ifmp() ),
	m_pgnoFDP( pfcb->PgnoFDP() ),
	m_updateid( updateid ),
	m_trxBegin0( trxBegin0 ),
	m_cbBookmarkKey( cbBookmarkKey ),
	m_cbBookmarkData( cbBookmarkData ),
	m_cbData( cbData ),
	m_level( level ),
	m_prceNextOfNode( prceNil ),
	m_prcePrevOfNode( prceNil ),
	m_trxCommitted( trxMax ),
	m_oper( (USHORT)oper ),
	m_fNoWriteConflict( fDoesNotWriteConflict ),
	m_uiHash( uiHash ),
	m_pgnoUndoInfo( pgnoNull ),
	m_fRolledBack( fFalse ),			//  protected by m_critBucketGlobal
	m_fMoved( fFalse ),
	m_fProxy( fProxy ? fTrue : fFalse ),
	m_rceid( rceid )
		{
#ifdef DEBUG
		Assert( m_rceid > rceidMin );
		AssertSz( m_rceid < LONG_MAX, "rceid overflow" );
		m_prceUndoInfoNext				= prceInvalid;
		m_prceUndoInfoPrev				= prceInvalid;
		m_prceHashOverflow				= prceInvalid;
		m_prceNextOfSession				= prceInvalid;
		m_prcePrevOfSession				= prceInvalid;		
		m_prceNextOfFCB					= prceInvalid;
		m_prcePrevOfFCB					= prceInvalid;
#endif	//	DEBUG

		}
		

//  ================================================================
INLINE BYTE * RCE::PbBookmark()
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	return m_rgbData + m_cbData; 
	}


//  ================================================================
INLINE RCE *&RCE::PrceHashOverflow()
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	return m_prceHashOverflow;
	}

		
//  ================================================================
INLINE VOID RCE::SetPrceHashOverflow( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	m_prceHashOverflow = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrceNextOfNode( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	Assert( prceNil == prce
		|| m_rceid < prce->Rceid()
		|| m_fNoWriteConflict
		|| prce->m_fNoWriteConflict );	//	OLDSLV's wait-lock may mess up RCEID order
	m_prceNextOfNode = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrcePrevOfNode( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	Assert( prceNil == prce
		|| m_rceid > prce->Rceid()
		|| m_fNoWriteConflict 
		|| prce->m_fNoWriteConflict );	//	OLDSLV's wait-lock may mess up RCEID order
	m_prcePrevOfNode = prce;
	}
	

//  ================================================================
INLINE VOID RCE::FlagRolledBack()
//  ================================================================
	{
	Assert( FAssertCritPIB_() );
	m_fRolledBack = fTrue;
	}


//  ================================================================
INLINE VOID RCE::FlagMoved()
//  ================================================================
	{
	m_fMoved = fTrue;
	}


//  ================================================================
INLINE VOID RCE::SetPrcePrevOfSession( RCE * prce )
//  ================================================================
	{
	m_prcePrevOfSession = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrceNextOfSession( RCE * prce )
//  ================================================================
	{
	m_prceNextOfSession = prce;
	}


//  ================================================================
INLINE VOID RCE::SetLevel( LEVEL level )
//  ================================================================
	{
	Assert( FAssertCritPIB_() || PinstFromIfmp( m_ifmp )->m_plog->m_fRecovering );
	Assert( m_level > level );	// levels are always decreasing
	m_level = level;
	}


//  ================================================================
INLINE VOID RCE::SetTrxCommitted( TRX trx )
//  ================================================================
	{
	Assert( !FOperInHashTable() || FAssertCritHash_() );
	Assert( FAssertCritPIB_() || PinstFromIfmp( m_ifmp )->m_plog->m_fRecovering );
	Assert( FAssertCritFCBRCEList_()  );
	Assert( trxMax == m_trxCommitted );
	Assert( prceNil == m_prcePrevOfSession );	//  only uncommitted RCEs in session list
	Assert( prceNil == m_prceNextOfSession );
	Assert( prceInvalid == m_prceUndoInfoNext );	//  no before image when committing to 0
	Assert( prceInvalid == m_prceUndoInfoPrev );
	Assert( pgnoNull == m_pgnoUndoInfo );
	Assert( TrxCmp( trx, m_trxBegin0 ) > 0 );

	m_trxCommitted 	= trx;
	}


//  ================================================================
INLINE VOID RCE::NullifyOper()
//  ================================================================
//
//  This is the end of the RCEs lifetime. It must have been removed
//  from all lists
//
//-
	{
	Assert( prceNil == m_prceNextOfNode );
	Assert( prceNil == m_prcePrevOfNode );
	Assert( prceNil == m_prceNextOfSession || prceInvalid == m_prceNextOfSession );
	Assert( prceNil == m_prcePrevOfSession || prceInvalid == m_prcePrevOfSession );
	Assert( prceNil == m_prceNextOfFCB || prceInvalid == m_prceNextOfFCB );
	Assert( prceNil == m_prcePrevOfFCB || prceInvalid == m_prcePrevOfFCB );
	Assert( prceInvalid == m_prceUndoInfoNext );
	Assert( prceInvalid == m_prceUndoInfoPrev );
	Assert( pgnoNull == m_pgnoUndoInfo );
	
	m_oper |= operMaskNull;
	}


//  ================================================================
INLINE VOID RCE::NullifyOperForMove()
//  ================================================================
//
//	RCE has been copied elsewhere -- nullify this copy
//
//-
	{
	m_oper |= ( operMaskNull | operMaskNullForMove );
	}


//  ================================================================
LOCAL BOOL FRCECorrect( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark, const RCE * prce )
//  ================================================================
//
//	Checks whether a RCE describes the given BOOKMARK
//
//-
	{
	ASSERT_VALID( prce );

	BOOL fRCECorrect = fFalse;

	//  same database and table
	if ( prce->Ifmp() == ifmp && prce->PgnoFDP() == pgnoFDP )
		{
		if ( bookmark.key.Cb() == prce->CbBookmarkKey() && (INT)bookmark.data.Cb() == prce->CbBookmarkData() ) 
			{
			//  bookmarks are the same length
			BOOKMARK bookmarkRCE;
			prce->GetBookmark( &bookmarkRCE );

			fRCECorrect = ( CmpKeyData( bookmark, bookmarkRCE ) == 0 );
			}
		}
	return fRCECorrect;
	}


//  ================================================================
LOCAL RCE **PprceRCEChainGet( UINT uiHash, IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, get the correct RCEHEAD.
//
//-
	{
	Assert( PverFromIfmp( ifmp )->CritRCEChain( uiHash ).FOwner() );

	AssertRTL( UiRCHashFunc( ifmp, pgnoFDP, bookmark ) == uiHash );

	RCE **pprceChain = PverFromIfmp( ifmp )->PGetChain( uiHash );
	while ( prceNil != *pprceChain )
		{
		RCE * const prceT = *pprceChain;

		AssertRTL( prceT->UiHash() == uiHash );

		if ( FRCECorrect( ifmp, pgnoFDP, bookmark, prceT ) )
			{
			Assert( prceNil == prceT->PrcePrevOfNode()
				|| prceT->PrcePrevOfNode()->UiHash() == prceT->UiHash() );
			if ( prceNil == prceT->PrceNextOfNode() )
				{
				Assert( prceNil == prceT->PrceHashOverflow()
					|| prceT->PrceHashOverflow()->UiHash() == prceT->UiHash() );
				}
			else
				{
				//	if there is a NextOfNode, then we are not in the hash overflow
				//	chain, so can't check PrceHashOverflow().
				Assert( prceT->PrceNextOfNode()->UiHash() == prceT->UiHash() );
				}

#ifdef DEBUG
			if ( prceNil != prceT->PrcePrevOfNode()
				&& prceT->PrcePrevOfNode()->UiHash() != prceT->UiHash() )
				{
				Assert( fFalse );
				}
			if ( prceNil != prceT->PrceNextOfNode() )
				{
				if ( prceT->PrceNextOfNode()->UiHash() != prceT->UiHash() )
					{
					Assert( fFalse );
					}
				}
			//	can only check prceHashOverflow if there is no prceNextOfNode
			else if ( prceNil != prceT->PrceHashOverflow()
				&& prceT->PrceHashOverflow()->UiHash() != prceT->UiHash() )
				{
				Assert( fFalse );
				}
#endif

			return pprceChain;
			}

		pprceChain = &prceT->PrceHashOverflow();
		}

	Assert( prceNil == *pprceChain );
	return NULL;
	}


//  ================================================================
LOCAL RCE *PrceRCEGet( UINT uiHash, IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, get the correct hash chain of RCEs.
//
//-
	{
	ASSERT_VALID( &bookmark );

	AssertRTL( UiRCHashFunc( ifmp, pgnoFDP, bookmark ) == uiHash );

	RCE *			prceChain	= prceNil;
	RCE **	const	pprceChain	= PprceRCEChainGet( uiHash, ifmp, pgnoFDP, bookmark );
	if ( NULL != pprceChain )
		{
		prceChain = *pprceChain;
		}

	return prceChain;
	}

	
//  ================================================================
LOCAL RCE * PrceFirstVersion ( const UINT uiHash, const FUCB * pfucb, const BOOKMARK& bookmark )
//  ================================================================
//
//  gets the first version of a node
//
//-
	{
	RCE * const prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	Assert( prceNil == prce
			|| prce->Oper() == operReplace
			|| prce->Oper() == operInsert
			|| prce->Oper() == operFlagDelete
			|| prce->Oper() == operDelta
			|| prce->Oper() == operReadLock
			|| prce->Oper() == operWriteLock
			|| prce->Oper() == operPreInsert
			|| prce->Oper() == operWaitLock
			|| prce->Oper() == operSLVSpace
			);
			
	return prce;
	}


//  ================================================================
LOCAL const RCE *PrceVERIGetPrevReplace( const RCE * const prce )
//  ================================================================
//
//	Find previous replace of changed by the session within
//	current transaction. The found rce may be in different transaction level.
//
//-
	{
	//	Look for previous replace on this node of this transaction ( level > 0 ).
	const RCE * prcePrevReplace = prce->PrcePrevOfNode();
	for ( ; prceNil != prcePrevReplace
			&& !prcePrevReplace->FOperReplace()
			&& prcePrevReplace->TrxCommitted() == trxMax;
		 prcePrevReplace = prcePrevReplace->PrcePrevOfNode() )
		;

	//  did we find a previous replace operation at level greater than 0
	if ( prceNil != prcePrevReplace )
		{
		if ( !prcePrevReplace->FOperReplace()
			|| prcePrevReplace->TrxCommitted() != trxMax )
			{
			prcePrevReplace = prceNil;
			}
		else
			{
			Assert( prcePrevReplace->FOperReplace() );
			Assert( trxMax == prcePrevReplace->TrxCommitted() );
			Assert( prce->Pfucb()->ppib == prcePrevReplace->Pfucb()->ppib || prcePrevReplace->FDoesNotWriteConflict() );
			}
		}
		
	return prcePrevReplace;
	}

	
//  ================================================================
BOOL FVERActive( const IFMP ifmp, const PGNO pgnoFDP, const BOOKMARK& bm, const TRX trxSession )
//  ================================================================
//
//	is there a version on the node that is visible to an uncommitted trx?
//
//-
	{
	ASSERT_VALID( &bm );

	const UINT				uiHash		= UiRCHashFunc( ifmp, pgnoFDP, bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	BOOL					fVERActive	= fFalse;

	//	get RCE
	const RCE * prce = PrceRCEGet( uiHash, ifmp, pgnoFDP, bm );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		if ( TrxCmp( prce->TrxCommitted(), trxSession ) > 0 )
			{
			fVERActive = fTrue;
			break;
			}
		}
		
	return fVERActive;
	}

	
//  ================================================================
BOOL FVERActive( const FUCB * pfucb, const BOOKMARK& bm, const TRX trxSession )
//  ================================================================
//
//	is there a version on the node that is visible to an uncommitted trx?
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bm );

	Assert( trxMax != trxSession || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering ); 

	const UINT				uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	BOOL					fVERActive	= fFalse;

	//	get RCE
	const RCE * prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		if ( TrxCmp( prce->TrxCommitted(), trxSession ) > 0 )
			{
			fVERActive = fTrue;
			break;
			}
		}
		
	return fVERActive;
	}


//  ================================================================
INLINE BOOL FVERIAddUndoInfo( const RCE * const prce )
//  ================================================================
//
//  Do we need to create a deferred before image for this RCE?
//
//	UNDONE:	do not create dependency for 
//				replace at same level as before
	{
	BOOL	fAddUndoInfo = fFalse;

	Assert( prce->TrxCommitted() == trxMax );

	Assert( !rgfmp[prce->Pfucb()->ifmp].FLogOn() || !PinstFromIfmp( prce->Pfucb()->ifmp )->m_plog->m_fLogDisabled );
	if ( rgfmp[prce->Pfucb()->ifmp].FLogOn() )
		{
		if ( prce->FOperReplace() )
			{
//			ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );
//			const RCE				*prcePrevReplace = PrceVERIGetPrevReplace( prce );

			//	if previous replace is at the same level
			//	before image for rollback is in the other RCE
//			Assert( prce->Level() > 0 );
//			if ( prceNil == prcePrevReplace
//				|| prcePrevReplace->Level() != prce->Level() )
//				{
				fAddUndoInfo = fTrue;
//				}
			}
		else if ( operFlagDelete == prce->Oper() )
			{
			fAddUndoInfo = fTrue;
			}
		else
			{
			//	these operations log the logical bookmark or do not log
			Assert( operInsert == prce->Oper()
					|| operDelta == prce->Oper()
					|| operReadLock == prce->Oper()
			 		|| operWriteLock == prce->Oper()
			 		|| operWaitLock == prce->Oper()
			 		|| operPreInsert == prce->Oper()
			 		|| operAllocExt == prce->Oper()
			 		|| operSLVSpace == prce->Oper()
			 		|| operCreateLV == prce->Oper()
			 		|| operSLVPageAppend == prce->Oper()
			 		|| prce->FOperDDL() );
			}
		}
		
	return fAddUndoInfo;
	}


//  ================================================================
VOID VERMoveUndoInfo( FUCB *pfucb, CSR *pcsrSrc, CSR *pcsrDest, const KEY& keySep )
//  ================================================================
//	
//	moves undo info of nodes >= keySep from pgnoSrc to pgnoDest
//
//-
	{
	//  move all undo info of nodes whose keys are GTE the seperator key from the
	//  source page to the destination page.  if the destination page is not Write
	//  Latched, we can throw away the undo info because, by convention, we are
	//  recovering and that page does not need to be redone

	BFLatch* pbflDest = NULL;
	if (	pcsrDest->Latch() == latchWrite &&
			rgfmp[ pfucb->ifmp ].FLogOn() )
		{
		Assert( !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fLogDisabled );
		pbflDest = pcsrDest->Cpage().PBFLatch();
		}

	BFMoveUndoInfo( pcsrSrc->Cpage().PBFLatch(), pbflDest, keySep );
	}

	
//  ================================================================
ERR VER::ErrVERIAllocateRCE( INT cbRCE, RCE ** pprce )
//  ================================================================
//
//  Allocates enough space for a new RCE or return out of memory
//  error. New buckets may be allocated
//
//-
	{
	Assert( m_critBucketGlobal.FOwner() );

	ERR err = JET_errSuccess;
	
	//	if insufficient bucket space, then allocate new bucket.

	while ( m_pbucketGlobalHead == pbucketNil || cbRCE > CbBUFree( m_pbucketGlobalHead ) )
		{
		Call( ErrVERIBUAllocBucket() );
		}
	Assert( cbRCE <= CbBUFree( m_pbucketGlobalHead ) );

	//	pbucket always on double-word boundary
	Assert( FAlignedForAllPlatforms( m_pbucketGlobalHead ) );
			
	//	set prce to next avail RCE location, and assert aligned

	*pprce = m_pbucketGlobalHead->hdr.prceNextNew;
	m_pbucketGlobalHead->hdr.prceNextNew =
		reinterpret_cast<RCE *>( PvAlignForAllPlatforms( reinterpret_cast<BYTE *>( *pprce ) + cbRCE ) );

	Assert( FAlignedForAllPlatforms( *pprce ) );
	Assert( FAlignedForAllPlatforms( m_pbucketGlobalHead->hdr.prceNextNew ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL SIZE_T CbBUMoveFree( const volatile BUCKET * const pbucket )
//  ================================================================
	{
	const BYTE * const pbOldest = reinterpret_cast<BYTE *>( pbucket->hdr.prceOldest );
	const BYTE * const pbDelete = pbucket->hdr.pbLastDelete;
	Assert( pbOldest >= pbDelete );
	return pbOldest - pbDelete;
	}

	
//  ================================================================
ERR VER::ErrVERIMoveRCE( RCE * prce )
//  ================================================================
//
//  Allocates enough space for a new RCE or return out of memory
//  error. New buckets may NOT be allocated
//
//-
	{
	Assert( m_critRCEClean.FOwner() );
	
	m_critBucketGlobal.Enter();
	
	Assert( pbucketNil != m_pbucketGlobalHead );
	Assert( pbucketNil != m_pbucketGlobalTail );
	Assert( m_pbucketGlobalHead != m_pbucketGlobalTail );

	const LONG_PTR cbRce = IbAlignForAllPlatforms( prce->CbRce() );

	if ( pbucketNil == m_pbucketGlobalLastDelete )
		{
		m_pbucketGlobalLastDelete = m_pbucketGlobalTail;
		}
	Assert( ( reinterpret_cast<ULONG_PTR>( m_pbucketGlobalLastDelete->hdr.pbLastDelete ) % sizeof( TRX ) ) == 0 );

	//  advance until we find a bucket that has enough space or we discover that
	//  we are the first non-cleaned RCE in the bucket
	while (
		pbucketNil != m_pbucketGlobalLastDelete
		&& m_pbucketGlobalLastDelete->hdr.prceOldest == m_pbucketGlobalLastDelete->hdr.prceNextNew	//  completely cleaned
		&& cbRce > CbBUMoveFree( m_pbucketGlobalLastDelete )
		)
		{
		++m_cbucketGlobalAllocDelete;
		m_pbucketGlobalLastDelete = m_pbucketGlobalLastDelete->hdr.pbucketNext;
#ifdef VERPERF
		cVERcbucketDeleteAllocated.Inc( m_pinst );
#endif // VERPERF
		}

	const BOOL	fMoveable	= ( pbucketNil != m_pbucketGlobalLastDelete
								&& cbRce <= CbBUMoveFree( m_pbucketGlobalLastDelete ) );

	m_critBucketGlobal.Leave();
	
	if ( !fMoveable )
		{
		//  the version store is too full. throw away this RCE
///		return ErrERRCheck( JET_errVersionStoreOutOfMemory );
		return JET_errSuccess;
		}
		
	//	assert we don't overlap the RCE we're moving
	Assert( (BYTE *)prce < m_pbucketGlobalLastDelete->hdr.pbLastDelete
		|| (BYTE *)prce >= m_pbucketGlobalLastDelete->hdr.pbLastDelete + cbRce );
	Assert( m_pbucketGlobalLastDelete->hdr.pbLastDelete < (BYTE *)prce
		|| m_pbucketGlobalLastDelete->hdr.pbLastDelete >= (BYTE *)prce + cbRce );
		
	//  we found enough free space in this bucket. copy the RCE here
	RCE * const prceNewLocation = reinterpret_cast<RCE *>( m_pbucketGlobalLastDelete->hdr.pbLastDelete );
	Assert( prceNil != prceNewLocation );
		
	ENTERCRITICALSECTION	enterCritChain( &( prce->CritChain() ) );
	ENTERCRITICALSECTION	enterCritRCEList( &( prce->Pfcb()->CritRCEList() ) );

	FCB * const pfcb	= prce->Pfcb();

	//	skip the RCE if:
	//		- someone nullified the RCE from underneath us (eg. VERNullifyAllVersionsOnBM())
	//		- the FDP is scheduled for deletion
	if ( prce->FOperNull() || pfcb->FDeletePending() )
		return JET_errSuccess;
		
	RCE * const prceNextOfNode = prce->PrceNextOfNode();
	RCE * const prcePrevOfNode = prce->PrcePrevOfNode();

	RCE * const prceNextOfFCB = prce->PrceNextOfFCB();
	RCE * const prcePrevOfFCB = prce->PrcePrevOfFCB();
	
	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );

	RCE ** pprceChain	= NULL;
	if ( prceNil == prceNextOfNode )
		{
		pprceChain	= PprceRCEChainGet(
							prce->UiHash(),
							prce->Ifmp(),
							prce->PgnoFDP(),
							bookmark );
		Assert( NULL != pprceChain );
		
		if ( NULL == pprceChain )
			{
			FireWall();
			return JET_errSuccess;
			}
		}

	//	only increment pbLastDelete when we're sure the move will occur
	m_pbucketGlobalLastDelete->hdr.pbLastDelete += cbRce;
	
	//	assert that the two memory locations don't overlap
	Assert( (BYTE *)prce < (BYTE *)prceNewLocation
		|| (BYTE *)prce >= (BYTE *)prceNewLocation + prce->CbRce() );
	Assert( (BYTE *)prceNewLocation < (BYTE *)prce
		|| (BYTE *)prceNewLocation >= (BYTE *)prce + prce->CbRce() );
	
	memcpy( prceNewLocation, prce, prce->CbRce() );
	prceNewLocation->FlagMoved();

	//	nullify old RCE to ensure RCEClean doesn't try to clean it
	prce->NullifyOperForMove();
	
	//  Do not use the old RCE after this point!

	if ( prceNil == prceNextOfNode )
		{
		Assert( NULL != pprceChain );
		*pprceChain = prceNewLocation;
		}
	else
		{
		Assert( prceNil != prceNextOfNode );
		prceNextOfNode->SetPrcePrevOfNode( prceNewLocation );
		}

	if ( prceNil != prcePrevOfNode )
		{
		prcePrevOfNode->SetPrceNextOfNode( prceNewLocation );
		}
	
	if ( prceNil == prceNextOfFCB )
		{
		//  we were at the top of the FCB chain
		Assert( pfcb->PrceNewest() == prce );
		pfcb->SetPrceNewest( prceNewLocation );
		}
	else
		{
		Assert( pfcb->PrceNewest() != prce );
		prceNextOfFCB->SetPrcePrevOfFCB( prceNewLocation );
		}

	if ( prceNil == prcePrevOfFCB )
		{
		//  we were at the end of the FCB chain
		Assert( pfcb->PrceOldest() == prce );
		pfcb->SetPrceOldest( prceNewLocation );
		}
	else
		{
		Assert( pfcb->PrceOldest() != prce );
		prcePrevOfFCB->SetPrceNextOfFCB( prceNewLocation );
		}

	Assert( prceNewLocation->PrceNextOfFCB() == NULL
			|| prceNewLocation->PrceNextOfFCB()->Pfcb() == pfcb );
	Assert( prceNewLocation->PrcePrevOfFCB() == NULL
			|| prceNewLocation->PrcePrevOfFCB()->Pfcb() == pfcb );
	
#ifdef VERPERF
	++m_crceMoved;
#endif	//	VERPERF

	
	return wrnVERRCEMoved;
	}

//  ================================================================
ERR VER::ErrVERICreateRCE(
	INT			cbNewRCE,
	FCB			*pfcb,
	FUCB		*pfucb,
	UPDATEID	updateid,
	const TRX	trxBegin0,
	const LEVEL	level,
	INT			cbBookmarkKey,
	INT			cbBookmarkData,
	OPER		oper,
	BOOL		fDoesNotWriteConflict,
	UINT		uiHash,
	RCE			**pprce,
	const BOOL	fProxy,
	RCEID		rceid
	)
//  ================================================================
//
//  Allocate a new RCE in a bucket. m_critBucketGlobal is used to 
//  protect the RCE until its oper and trxCommitted are set
//
//-
	{
	// For concurrent operations, we must be in critHash
	// from the moment the rceid is allocated until the RCE
	// is placed in the hash table, otherwise we may get
	// rceid's out of order.
	if ( FOperConcurrent( oper ) )
		{
		CritRCEChain( uiHash ).Enter();
		}

	ERR						err		= JET_errSuccess;
	RCE						*prce	= prceNil;
	
	m_critBucketGlobal.Enter();

	Assert( pfucbNil == pfucb ? 0 == level : level > 0 );

	Call( ErrVERIAllocateRCE( cbNewRCE, &prce ) );
	
#ifdef DEBUG
	if ( !PinstFromIfmp( pfcb->Ifmp() )->m_plog->m_fRecovering )
		{
		Assert( rceidNull == rceid );
		}
	else
		{
		Assert( rceidNull != rceid && uiHashInvalid != uiHash ||
				rceidNull == rceid && uiHashInvalid == uiHash );
		}
#endif

	Assert( trxMax != trxBegin0 );

	//	UNDONE: break this new function into two parts. One that only need to
	//	UNDONE: be recognizable by Clean up (trxTime?) and release
	//	UNDONE: the m_critBucketGlobal as soon as possible.

	new( prce ) RCE(
			pfcb,
			pfucb,
			updateid,
			trxBegin0,
			level,
			USHORT( cbBookmarkKey ),
			USHORT( cbBookmarkData ),
			USHORT( cbNewRCE - ( sizeof(RCE) + cbBookmarkKey + cbBookmarkData ) ),
			oper,
			fDoesNotWriteConflict,
			uiHash,
			fProxy,
			rceidNull == rceid ? RCE::RceidLastIncrement() : rceid
			);	//lint !e522

HandleError:
	m_critBucketGlobal.Leave();

	if ( err >= 0 )
		{
		Assert( prce != prceNil );

		//	check RCE
		Assert( FAlignedForAllPlatforms( prce ) );
		Assert( (RCE *)PvAlignForAllPlatforms( prce ) == prce );
	
		*pprce = prce;
		}
	else if ( err < JET_errSuccess && FOperConcurrent( oper ) )
		{
		CritRCEChain( uiHash ).Leave();
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIInsertRCEIntoHash( RCE * const prce )
//  ================================================================
//
//	Inserts an RCE to hash table. We must already be in the critical
//  section of the hash chain.
//
//-
	{
	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	RCE ** const	pprceChain	= PprceRCEChainGet( prce->UiHash(),
													prce->Ifmp(),
													prce->PgnoFDP(),
													bookmark );

	if ( pprceChain )
		{
		//	hash chain for node already exists
		Assert( *pprceChain != prceNil );

		//	insert in order of rceid
		Assert( prce->Rceid() != rceidNull );

		RCE * prceNext		= prceNil;
		RCE * prcePrev 		= *pprceChain;
		const RCEID rceid 	= prce->Rceid();


		if( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering )
			{
			for ( ;
				  prcePrev != prceNil && prcePrev->Rceid() >= rceid; 
				  prceNext = prcePrev, 
				  prcePrev = prcePrev->PrcePrevOfNode() )
				{
				if ( prcePrev->Rceid() == rceid )
					{
					//	release last version created 
					//	remove RCE from bucket
					//
					Assert( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );
					Assert( FAlignedForAllPlatforms( prce ) );
					
				    VER *pver = PverFromIfmp( prce->Ifmp() );
					Assert( (RCE *)PvAlignForAllPlatforms( (BYTE *)prce + prce->CbRce() ) == 
								pver->m_pbucketGlobalHead->hdr.prceNextNew );
					pver->m_pbucketGlobalHead->hdr.prceNextNew = prce;

					ERR	err = ErrERRCheck( JET_errPreviousVersion );
					return err;
					}
				}
			}
		else
			{

			//	if we are not recovering don't insert in RCEID order -- if we have waited
			//	for a wait-latch we actually want to insert after it
			
			}

		//  adjust head links
		if ( prceNil == prceNext )
			{
			//	insert before first rce in chain
			//
			Assert( prcePrev == *pprceChain );
			prce->SetPrceHashOverflow( (*pprceChain)->PrceHashOverflow() );
			*pprceChain = prce;

			#ifdef DEBUG
			prcePrev->SetPrceHashOverflow( prceInvalid );
			#endif	//	DEBUG
			Assert( prceNil == prce->PrceNextOfNode() );
			}
		else
			{
			Assert( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );
			Assert( prcePrev != prceNext );
			Assert( prcePrev != *pprceChain );

			prceNext->SetPrcePrevOfNode( prce );
			prce->SetPrceNextOfNode( prceNext );

			Assert( prce->UiHash() == prceNext->UiHash() );
			}

		if ( prcePrev != prceNil )
			{
			prcePrev->SetPrceNextOfNode( prce );

			Assert( prce->UiHash() == prcePrev->UiHash() );
			}

		//  adjust RCE links
		prce->SetPrcePrevOfNode( prcePrev );
		}
	else
		{
		Assert( prceNil == prce->PrceNextOfNode() );
		Assert( prceNil == prce->PrcePrevOfNode() );

		//	create new rce chain
		prce->SetPrceHashOverflow( PverFromIfmp( prce->Ifmp() )->GetChain( prce->UiHash() ) );
		PverFromIfmp( prce->Ifmp() )->SetChain( prce->UiHash(), prce );
		}

	Assert( prceNil != prce->PrceNextOfNode()
		|| prceNil == prce->PrceHashOverflow()
		|| prce->PrceHashOverflow()->UiHash() == prce->UiHash() );

#ifdef DEBUG
	if ( prceNil == prce->PrceNextOfNode()
		&& prceNil != prce->PrceHashOverflow()
		&& prce->PrceHashOverflow()->UiHash() != prce->UiHash() )
		{
		Assert( fFalse );
		}
#endif

#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	Assert( prceNil == prce->PrceNextOfNode() ||
			PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering && prce->PrceNextOfNode()->Rceid() > prce->Rceid() );
	
	Assert( prce->Pfcb() != pfcbNil );

	//  monitor statistics
#ifdef VERPERF
	{
	//	Do not need critical section here. It is OK to miss a little.

	const INT cbBookmark = prce->CbBookmarkKey() + prce->CbBookmarkData();
    VER *pver = PverFromIfmp( prce->Ifmp() );
    INST *pinst = pver->m_pinst;
    cVERcrceHashEntries.Inc( pinst );
	const LONG counter = ++pver->m_rgcRCEHashChainLengths[prce->UiHash()];
	cVERcrceHashUsage.Add( pinst, counter*2-1 );
	cVERcbBookmarkTotal.Add( pinst, cbBookmark );
	}
#endif	//  VERPERF

	return JET_errSuccess;
	}


//  ================================================================
LOCAL VOID VERIDeleteRCEFromHash( RCE * const prce )
//  ================================================================
//
//	Deletes RCE from hashtable/RCE chain, and may set hash table entry to
//	prceNil. Must already be in the critical section for the hash chain.
//
//-
	{
	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	RCE ** const pprce = PprceRCEChainGet( prce->UiHash(), prce->Ifmp(), prce->PgnoFDP(), bookmark );
	Assert( pprce != NULL );

	if ( prce == *pprce )
		{
		Assert( prceNil == prce->PrceNextOfNode() );

		//  the RCE is at the head of the chain
		if ( prceNil != prce->PrcePrevOfNode() )
			{
			Assert( prce->PrcePrevOfNode()->UiHash() == prce->UiHash() );
			prce->PrcePrevOfNode()->SetPrceHashOverflow( prce->PrceHashOverflow() );
			*pprce = prce->PrcePrevOfNode();
			(*pprce)->SetPrceNextOfNode( prceNil );
			Assert( prceInvalid != (*pprce)->PrceHashOverflow() );
			ASSERT_VALID( *pprce );
			}
		else
			{
			*pprce = prce->PrceHashOverflow();
			Assert( prceInvalid != *pprce );
			}
		}
	else
		{
		Assert( prceNil != prce->PrceNextOfNode() );
		RCE * const prceNext = prce->PrceNextOfNode();

		Assert( prceNext->UiHash() == prce->UiHash() );

		prceNext->SetPrcePrevOfNode( prce->PrcePrevOfNode() );
		if ( prceNil != prceNext->PrcePrevOfNode() )
			{
			Assert( prceNext->PrcePrevOfNode()->UiHash() == prce->UiHash() );
			prceNext->PrcePrevOfNode()->SetPrceNextOfNode( prceNext );
			}
		}

	prce->SetPrceNextOfNode( prceNil );
	prce->SetPrcePrevOfNode( prceNil );

	//  monitor statistics
#ifdef VERPERF
	//	Do not need critical section here. It is OK to miss a little.

	VER *pver = PverFromIfmp( prce->Ifmp() );
	INST *pinst = pver->m_pinst;
	cVERcrceHashEntries.Dec( pinst );
	const LONG counter = --pver->m_rgcRCEHashChainLengths[prce->UiHash()];
	cVERcrceHashUsage.Add( pinst, -( 2*counter+1 ) );
	cVERcbBookmarkTotal.Add( pinst, -(prce->CbBookmarkKey() + prce->CbBookmarkData()) );
#endif	// VERPERF
	}


//  ================================================================
INLINE VOID VERIInsertRCEIntoSessionList( PIB * const ppib, RCE * const prce )
//  ================================================================
//
//  Inserts the RCE into the head of the session list of the pib provided.
//  No critical section needed to do this, because the only session that
//	inserts at the head of the list is the pib itself.
//
//-
	{
	RCE			*prceNext	= prceNil;
	RCE			*prcePrev 	= ppib->prceNewest;
	const RCEID	rceid 		= prce->Rceid();
	const LEVEL	level		= prce->Level();
	INST		*pinst		= PinstFromPpib( ppib );
	LOG			*plog		= pinst->m_plog;

	Assert( level > 0 );
	Assert( level == ppib->level
		|| ( level < ppib->level && plog->m_fRecovering ) );

	if ( !plog->m_fRecovering
		|| prceNil == prcePrev
		|| level > prcePrev->Level()
		|| ( level == prcePrev->Level() && rceid > prcePrev->Rceid() ) )
		{
		prce->SetPrceNextOfSession( prceNil );
		prce->SetPrcePrevOfSession( ppib->prceNewest );
		if ( prceNil != ppib->prceNewest )
			{
			Assert( prceNil == ppib->prceNewest->PrceNextOfSession() );
			Assert( ppib->prceNewest->Level() <= prce->Level() );
			ppib->prceNewest->SetPrceNextOfSession( prce );
			}
		PIBSetPrceNewest( ppib, prce );
		Assert( prce == ppib->prceNewest );
		}
		
	else
		{
		Assert( plog->m_fRecovering );

		//	insert RCE in rceid order at the same level
		Assert( level < prcePrev->Level() || rceid < prcePrev->Rceid() );
		for ( ;
			prcePrev != prceNil
				&& ( level < prcePrev->Level()
					|| ( level == prcePrev->Level() && rceid < prcePrev->Rceid() ) );
			prceNext = prcePrev, 
			prcePrev = prcePrev->PrcePrevOfSession() )
			{
			NULL;
			}

		Assert( prceNil != prceNext );
		Assert( prceNext->Level() > level
			|| prceNext->Level() == level && prceNext->Rceid() > rceid );
		prceNext->SetPrcePrevOfSession( prce );

		if ( prceNil != prcePrev )
			{
			Assert( prcePrev->Level() < level
				|| ( prcePrev->Level() == level && prcePrev->Rceid() < rceid ) );
			prcePrev->SetPrceNextOfSession( prce );
			}

		prce->SetPrcePrevOfSession( prcePrev );
		prce->SetPrceNextOfSession( prceNext );
		}

	Assert( prceNil == ppib->prceNewest->PrceNextOfSession() );
	}


//  ================================================================
INLINE VOID VERIInsertRCEIntoSessionList( PIB * const ppib, RCE * const prce, RCE * const prceParent )
//  ================================================================
//
//  Inserts the RCE after the given parent RCE in the session list.
//  NEVER inserts at the head (since the insert occurs after an existing
//	RCE), so no possibility of conflict with regular session inserts, and
//	therefore no critical section needed.  However, may conflict with
//	other concurrent create indexers, which is why we must obtain critTrx
//	(to ensure only one concurrent create indexer is processing the parent
//	at a time.
//
//-
	{
	Assert( ppib->critTrx.FOwner() );
	
	Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
	
	Assert( prceParent->TrxCommitted() == trxMax );
	Assert( prceParent->Level() == prce->Level() );
	Assert( prceNil != ppib->prceNewest );
	Assert( ppib == prce->Pfucb()->ppib );

	RCE	*prcePrev = prceParent->PrcePrevOfSession();
		
	prce->SetPrceNextOfSession( prceParent );
	prce->SetPrcePrevOfSession( prcePrev );
	prceParent->SetPrcePrevOfSession( prce );

	if ( prceNil != prcePrev )
		{
		prcePrev->SetPrceNextOfSession( prce );
		}
		
	Assert( prce != ppib->prceNewest );
	}


//  ================================================================
LOCAL VOID VERIDeleteRCEFromSessionList( PIB * const ppib, RCE * const prce )
//  ================================================================
//
//  Deletes the RCE from the session list of the given PIB. Must be
//  in the critical section of the PIB
//
//-
	{
	Assert( prce->TrxCommitted() == trxMax );	//  only uncommitted RCEs in the session list
	Assert( ppib == prce->Pfucb()->ppib );
	
	if ( prce == ppib->prceNewest )
		{
		//  we are at the head of the list
		PIBSetPrceNewest( ppib, prce->PrcePrevOfSession() );
		if ( prceNil != ppib->prceNewest )
			{
			Assert( prce == ppib->prceNewest->PrceNextOfSession() );
			ppib->prceNewest->SetPrceNextOfSession( prceNil );
			}
		}
	else
		{
		// we are in the middle/end
		Assert( prceNil != prce->PrceNextOfSession() );
		RCE * const prceNext = prce->PrceNextOfSession();
		RCE * const prcePrev = prce->PrcePrevOfSession();
		prceNext->SetPrcePrevOfSession( prcePrev );
		if ( prceNil != prcePrev )
			{
			prcePrev->SetPrceNextOfSession( prceNext );
			}
		}
	prce->SetPrceNextOfSession( prceNil );
	prce->SetPrcePrevOfSession( prceNil );
	}


//  ================================================================
LOCAL VOID VERIInsertRCEIntoFCBList( FCB * const pfcb, RCE * const prce )
//  ================================================================
//
//  Must be in critFCB
//
//-
	{
	Assert( pfcb->CritRCEList().FOwner() );
	pfcb->AttachRCE( prce );
	}

	
//  ================================================================
LOCAL VOID VERIDeleteRCEFromFCBList( FCB * const pfcb, RCE * const prce )
//  ================================================================
//
//  Must be in critFCB
//
//-
	{
	Assert( pfcb->CritRCEList().FOwner() );
	pfcb->DetachRCE( prce );
	}


//  ================================================================
VOID VERInsertRCEIntoLists(
	FUCB		*pfucbNode,		// cursor of session performing node operation
	CSR			*pcsr,
	RCE			*prce,
	const VERPROXY	*pverproxy )
//  ================================================================
	{
	Assert( prceNil != prce );
	
	FCB	* const pfcb = prce->Pfcb();
	Assert( pfcbNil != pfcb );

	LOG *plog = PinstFromIfmp( pfcb->Ifmp() )->m_plog;

	if ( prce->TrxCommitted() == trxMax )
		{
		FUCB	*pfucbVer = prce->Pfucb();	// cursor of session for whom the
											// RCE was created,
		Assert( pfucbNil != pfucbVer );
		Assert( pfcb == pfucbVer->u.pfcb );

#ifdef IGNORE_BAD_ATTACH
		if ( plog->m_fRecovering && prce->Rceid() != rceidNull )
			{
			//	Assert( plog->m_rceidLast <= prce->m_rceid );
			 plog->m_rceidLast = prce->Rceid();
			}
#endif // IGNORE_BAD_ATTACH

		// cursor performing the node operation may be different than cursor
		// for which the version was created if versioning by proxy
		Assert( pfucbNode == pfucbVer || pverproxy != NULL );
		
		if ( FVERIAddUndoInfo( prce ) )
			{
			Assert( pcsrNil != pcsr || plog->m_fRecovering );	// Allow Redo to override UndoInfo.
			if ( pcsrNil != pcsr )
				{
				Assert( !rgfmp[ pfcb->Ifmp() ].FLogOn() || !plog->m_fLogDisabled );
				if ( rgfmp[ pfcb->Ifmp() ].FLogOn() )
					{
					//	set up UndoInfo dependency 
					//	to make sure UndoInfo will be logged if the buffer
					//	is flushed before commit/rollback.
					BFAddUndoInfo( pcsr->Cpage().PBFLatch(), prce );
					}
				}
			}

		if ( NULL != pverproxy && proxyCreateIndex == pverproxy->proxy )
			{
			Assert( !plog->m_fRecovering );
			Assert( pfucbNode != pfucbVer );
			Assert( prce->Oper() == operInsert
				|| prce->Oper() == operReplace		// via FlagInsertAndReplaceData
				|| prce->Oper() == operFlagDelete );
			VERIInsertRCEIntoSessionList( pfucbVer->ppib, prce, pverproxy->prcePrimary );
			}
		else
			{
			Assert( pfucbNode == pfucbVer );
			if ( NULL != pverproxy )
				{
				Assert( plog->m_fRecovering );
				Assert( proxyRedo == pverproxy->proxy );
				Assert( prce->Level() == pverproxy->level );
				Assert( prce->Level() > 0 );
				Assert( prce->Level() <= pfucbVer->ppib->level );
				}
			VERIInsertRCEIntoSessionList( pfucbVer->ppib, prce );
			}
		}
	else
		{
		// Don't put committed RCE's into session list.
		// Only way to get a committed RCE is via concurrent create index.
		Assert( !plog->m_fRecovering );
		Assert( NULL != pverproxy );
		Assert( proxyCreateIndex == pverproxy->proxy );
		Assert( prceNil != pverproxy->prcePrimary );
		Assert( pverproxy->prcePrimary->TrxCommitted() == prce->TrxCommitted() );
		Assert( pfcb->FTypeSecondaryIndex() );
		Assert( pfcb->PfcbTable() == pfcbNil );
		Assert( prce->Oper() == operInsert
				|| prce->Oper() == operReplace		// via FlagInsertAndReplaceData
				|| prce->Oper() == operFlagDelete );
		}


	ENTERCRITICALSECTION enterCritFCBRCEList( &(pfcb->CritRCEList()) );
	VERIInsertRCEIntoFCBList( pfcb, prce );
	}


//  ================================================================
LOCAL VOID VERINullifyWaitLockRCE( RCE * const prce )
//  ================================================================
	{
	Assert( operWaitLock == prce->Oper() );
	VERWAITLOCK * pverwaitlock = (VERWAITLOCK *)prce->PbData();
	pverwaitlock->signal.~CManualResetSignal();
	}


//  ================================================================
LOCAL VOID VERINullifyUncommittedRCE( RCE * const prce )
//  ================================================================
//
//  Remove undo info from a RCE, remove it from the session list
//  FCB list and the hash table. The oper is then nullified
//
//-
	{
	Assert( prceInvalid != prce->PrceNextOfSession() );
	Assert( prceInvalid != prce->PrcePrevOfSession() );
	Assert( prceInvalid != prce->PrceNextOfFCB() );
	Assert( prceInvalid != prce->PrcePrevOfFCB() );
	Assert( trxMax == prce->TrxCommitted() );

	BFRemoveUndoInfo( prce );

	VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );
	VERIDeleteRCEFromSessionList( prce->Pfucb()->ppib, prce );
	
	const BOOL fInHash = prce->FOperInHashTable();
	if ( fInHash )
		{
		//  VERIDeleteRCEFromHash may call ASSERT_VALID so we do this deletion first,
		//  before we mess up the other linked lists
		VERIDeleteRCEFromHash( prce );
		}

	if( prce->Oper() == operWaitLock )
		{
		VERINullifyWaitLockRCE( prce );
		}
		
	prce->NullifyOper();
	}


//  ================================================================
LOCAL VOID VERINullifyCommittedRCE( RCE * const prce )
//  ================================================================
//
//  Remove a RCE from the Hash table (if necessary) and the FCB list.
//  The oper is then nullified
//
//-
	{
	Assert( prce->PgnoUndoInfo() == pgnoNull );
	Assert( prce->TrxCommitted() != trxMax );
	VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );
	
	const BOOL fInHash = prce->FOperInHashTable();
	if ( fInHash )
		{
		//  VERIDeleteRCEFromHash may call ASSERT_VALID so we do this deletion first,
		//  before we mess up the other linked lists
		VERIDeleteRCEFromHash( prce );
		}

	if( prce->Oper() == operWaitLock )
		{
		VERINullifyWaitLockRCE( prce );
		}

	prce->NullifyOper();
	}


//  ================================================================
INLINE VOID VERINullifyRCE( RCE *prce )
//  ================================================================
	{
	if ( prce->TrxCommitted() != trxMax )
		{
		VERINullifyCommittedRCE( prce );
		}
	else
		{
		VERINullifyUncommittedRCE( prce );
		}
	}


//  ================================================================
VOID VERNullifyFailedDMLRCE( RCE *prce )
//  ================================================================
	{
	ASSERT_VALID( prce );
	Assert( prceInvalid == prce->PrceNextOfSession() );
	Assert( prceInvalid == prce->PrcePrevOfSession() );
///	Assert( prceInvalid == prce->PrceNextOfFCB() );
///	Assert( prceInvalid == prce->PrcePrevOfFCB() );
	Assert( prce->TrxCommitted() == trxMax );

	BFRemoveUndoInfo( prce );

	Assert( prce->FOperInHashTable() );
	
		{
		ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );
		VERIDeleteRCEFromHash( prce );
		}
	
	prce->NullifyOper();
	}


//  ================================================================
VOID VERNullifyAllVersionsOnFCB( FCB * const pfcb )
//  ================================================================
//
//  This is used to nullify all RCEs on an FCB
//
//-
	{
	VER *pver = PverFromIfmp( pfcb->Ifmp() );
	LOG *plog = pver->m_pinst->m_plog;
	
	pfcb->CritRCEList().Enter();

	Assert( pfcb->FTypeTable()
		|| pfcb->FTypeSecondaryIndex()
		|| pfcb->FTypeTemporaryTable()
		|| pfcb->FTypeSentinel()
		|| pfcb->FTypeLV() );
	
	while ( prceNil != pfcb->PrceOldest() )
		{
		RCE	* const prce = pfcb->PrceOldest();

		Assert( prce->Pfcb() == pfcb );
		Assert( !prce->FOperNull() );

		Assert( prce->Ifmp() == pfcb->Ifmp() );
		Assert( prce->PgnoFDP() == pfcb->PgnoFDP() );

		//	during recovery, should not see any uncommitted RCE's
		Assert( prce->TrxCommitted() != trxMax || !plog->m_fRecovering );

		if ( prce->FOperInHashTable() )
			{
			const UINT	uiHash			= prce->UiHash();
			const BOOL	fNeedCritTrx	= ( prce->TrxCommitted() == trxMax
											&& rgfmp[ prce->Ifmp() ].Dbid() != dbidTemp );
			PIB			*ppib;

			Assert( !pfcb->FTypeTable()
				|| plog->m_fRecovering
				|| ( prce->FMoved() && operFlagDelete == prce->Oper() ) );
	
			if ( fNeedCritTrx )
				{
				// If uncommitted, must grab critTrx to ensure that
				// RCE does not commit or rollback on us while
				// nullifying.  Note that we don't need critTrx
				// if this is a temp table because we're the
				// only ones who have access to it.
				Assert( prce->Pfucb() != pfucbNil );
				Assert( prce->Pfucb()->ppib != ppibNil );
				ppib = prce->Pfucb()->ppib;
				}
				
			pfcb->CritRCEList().Leave();

			ENTERCRITICALSECTION	enterCritRCEClean( &pver->m_critRCEClean, plog->m_fRecovering );
			ENTERCRITICALSECTION	enterCritPIBTrx(
										fNeedCritTrx ? &ppib->critTrx : NULL,
										fNeedCritTrx );	//lint !e644
			ENTERCRITICALSECTION	enterCritHash( &( pver->CritRCEChain( uiHash ) ) );
			
			pfcb->CritRCEList().Enter();
			
			// Verify no one nullified the RCE while we were
			// switching critical sections.
			if ( pfcb->PrceOldest() == prce )
				{
				Assert( !prce->FOperNull() );
				VERINullifyRCE( prce );
				}
			}
		else
			{
			// If not hashable, nullification will be expensive,
			// because we have to grab m_critRCEClean.  Fortunately
			// this should be rare:
			// 	- the only non-hashable versioned operation
			//	  possible for a temporary table is CreateTable.
			//	- no non-hashable versioned operations possible
			//	  on secondary index.
			if ( !plog->m_fRecovering )
				{
				Assert( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp );
				Assert( prce->Pfcb()->FTypeTemporaryTable() );
				Assert( operCreateTable == prce->Oper() );
				}

			pfcb->CritRCEList().Leave();
			ENTERCRITICALSECTION	enterCritRCEClean( &pver->m_critRCEClean );
			pfcb->CritRCEList().Enter();

			// critTrx not needed, since we're the only ones
			// who should have access to this FCB.
			
			// Verify no one nullified the RCE while we were
			// switching critical sections.
			if ( pfcb->PrceOldest() == prce )
				{
				Assert( !prce->FOperNull() );
				VERINullifyRCE( prce );
				}
			}
		}

	pfcb->CritRCEList().Leave();
	}


//  ================================================================
VOID VERNullifyInactiveVersionsOnBM( const FUCB * pfucb, const BOOKMARK& bm )
//  ================================================================
//
//	Nullifies all RCE's for given bm. Used by online cleanup when 
//  a page is being cleaned.
//
//  All the RCE's should be inactive and thus don't need to be
//  removed from a session list
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bm );

	const TRX				trxOldest		= TrxOldest( PinstFromPfucb( pfucb ) );
	const UINT				uiHash			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	RCE	* prcePrev;
	RCE	* prce		= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	for ( ; prceNil != prce; prce = prcePrev )
		{
		prcePrev = prce->PrcePrevOfNode();

		if ( TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 )
			{
			FCB * const pfcb = prce->Pfcb();
			ENTERCRITICALSECTION enterCritFCBRCEList( &( pfcb->CritRCEList() ) );

			VERINullifyCommittedRCE( prce );
			}
		}
	}


	
//  ****************************************************************
//  VERSION LAYER
//  ****************************************************************

VOID VERSanitizeParameters
	(
	long	*plVerBucketsMax,
	long	*plVerBucketsPreferredMax
	)
	{
	//	Already been sanitized in JetSetSystemParameter()
	Assert( *plVerBucketsMax >= 1 );
	Assert( *plVerBucketsPreferredMax >= 1 );

	if ( *plVerBucketsPreferredMax > *plVerBucketsMax )
		{
		//	Note that we convert the bucket parameters to the 16K "pages"
		//	that the user is "familiar" with.
		TCHAR	szVerMax[ 32 ];
		_itoa( *plVerBucketsMax * ( cbBucket / ( 16 * 1024 ) ), szVerMax, 10 );
		TCHAR	szPreferredMax[ 32 ];
		_itoa( *plVerBucketsPreferredMax * ( cbBucket / ( 16 * 1024 ) ), szPreferredMax, 10 );

		//	SANITIZE
		*plVerBucketsPreferredMax = *plVerBucketsMax;
		
		const TCHAR	*rgsz[] = { szPreferredMax, szVerMax, szPreferredMax, szVerMax };
		::UtilReportEvent( eventWarning, SYSTEM_PARAMETER_CATEGORY,
			SYS_PARAM_VERPREFERREDPAGETOOBIG_ID, CArrayElements( rgsz ), rgsz );
		}

	Assert( *plVerBucketsMax >= 1 );
	Assert( *plVerBucketsPreferredMax >= 1 );
	Assert( *plVerBucketsPreferredMax <= *plVerBucketsMax );
	}


VOID VER::VERSignalCleanup()
	{
	//	check whether we already requested clean up of that version store
	if ( 0 == AtomicCompareExchange( (LONG *)&m_fVERCleanUpWait, 0, 1 ) )
		{
		//	be aware if we are in syncronous mode already, do not try to start a new task
		if ( m_fSyncronousTasks || JET_errSuccess > m_pinst->Taskmgr().ErrTMPost( VERIRCECleanProc, this ) )
			{
			LONG fStatus = AtomicCompareExchange( (LONG *)&m_fVERCleanUpWait, 1, 0 );
			if ( 2 == fStatus )
				{
				m_asigRCECleanDone.Set();
				}
			}
		}
	}


//  ================================================================
DWORD VER::VERIRCECleanProc( VOID *pvThis )
//  ================================================================
//
//	Go through all sessions, cleaning buckets as versions are
//	no longer needed.  Only those versions older than oldest
//	transaction are cleaned up.
//
//	Side Effects:
//		frees buckets.
//
//-
{
	VER *pver = (VER *)pvThis;

	pver->m_critRCEClean.Enter();
	pver->ErrVERIRCEClean();
	LONG fStatus = AtomicCompareExchange( (LONG *)&pver->m_fVERCleanUpWait, 1, 0 );
	pver->m_critRCEClean.Leave();
	if ( 2 == fStatus )
	{
		pver->m_asigRCECleanDone.Set();
	}
	return 0;
}


ERR VER::ErrVERSystemInit( )
{
	ERR     err = JET_errSuccess;
	
	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif

#ifdef GLOBAL_VERSTORE_MEMPOOL
	//	initialize resVerPagesGlobal
	//	reserve memory for version store
	//	number of buckets reserved must be a multiple of cbucketChunk
	Assert( NULL == g_pcresVERPool );
	g_pcresVERPool = new CRES( NULL, residVER, sizeof( BUCKET ), g_lVerPagesMin, &err );
	if ( JET_errSuccess > err )
		{
		delete g_pcresVERPool;
		g_pcresVERPool = NULL;
		}
	else if ( NULL == g_pcresVERPool )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	CallR ( err );
#endif
	// UNDONE: get rid of the magic number 4
	///	g_pcresVERPool->SetPreferredThreshold( 4 );

	return err;
}

VOID VER::VERSystemTerm( )
{
#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
#endif	
	
#ifdef GLOBAL_VERSTORE_MEMPOOL
	delete g_pcresVERPool ;
	g_pcresVERPool = NULL;
#endif	
}

#ifndef GLOBAL_VERSTORE_MEMPOOL

ERR VER::ErrVERMempoolInit( const ULONG cbucketMost )
{
	ERR     err = JET_errSuccess;
	
	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif
	//	initialize resVerPagesGlobal
	//	reserve memory for version store
	//	number of buckets reserved must be a multiple of cbucketChunk
	m_pcresVERPool = new CRES( m_pinst, residVER, sizeof( BUCKET ), cbucketMost, &err );
	if ( JET_errSuccess > err )
		{
		delete m_pcresVERPool;
		m_pcresVERPool = NULL;
		}
	else if ( NULL == m_pcresVERPool )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
		
	return err;
}

VOID VER::VERMempoolTerm( )
{
	Assert ( m_pcresVERPool );	
	delete m_pcresVERPool ;
}

#endif // GLOBAL_VERSTORE_MEMPOOL


//  ================================================================
ERR VER::ErrVERInit( INT cbucketMost, INT cbucketPreferred, INT cSessions )
//  ================================================================
//
//	Creates background version bucket clean up thread.
//
//-
	{
	ERR     err = JET_errSuccess;

#if defined( DEBUG ) && JET_cbPage == 8192
	//  double size of version store for 8K pages (LV replaces need more space)
	cbucketMost *= 2;
#endif

#ifdef VERPERF
	cVERcbucketAllocated.Clear( m_pinst );
	cVERcbucketDeleteAllocated.Clear( m_pinst );
	memset((VOID *)m_rgcRCEHashChainLengths, 0, sizeof(m_rgcRCEHashChainLengths[0])*crceheadGlobalHashTable );
	cVERcrceHashUsage.Clear( m_pinst );
	cVERcrceHashEntries.Clear( m_pinst );	//  number of RCEs that currently exist in hash table
	cVERcbBookmarkTotal.Clear( m_pinst );	//  amount of space used by bookmarks of existing RCEs
	cVERUnnecessaryCalls.Clear( m_pinst );	//  calls for a bookmark that doesn't exist
#endif	//  VERPERF

	AssertRTL( TrxCmp( trxMax, trxMax ) == 0 );
	AssertRTL( TrxCmp( trxMax, 0 ) > 0 );
	AssertRTL( TrxCmp( trxMax, 2 ) > 0 );
	AssertRTL( TrxCmp( trxMax, 2000000 ) > 0 );
	AssertRTL( TrxCmp( trxMax, trxMax - 1 ) > 0 );
	AssertRTL( TrxCmp( 0, trxMax ) < 0 );
	AssertRTL( TrxCmp( 2, trxMax ) < 0 );
	AssertRTL( TrxCmp( 2000000, trxMax ) < 0 );
	AssertRTL( TrxCmp( trxMax - 1, trxMax ) < 0 );
	AssertRTL( TrxCmp( 0xF0000000, 0xEFFFFFFE ) > 0 );
	AssertRTL( TrxCmp( 0xEFFFFFFF, 0xF0000000 ) < 0 );
	AssertRTL( TrxCmp( 0xfffffdbc, 0x000052e8 ) < 0 );
	AssertRTL( TrxCmp( 10, trxMax - 259 ) > 0 );
	AssertRTL( TrxCmp( trxMax - 251, 16 ) < 0 );
	AssertRTL( TrxCmp( trxMax - 257, trxMax - 513 ) > 0 );
	AssertRTL( TrxCmp( trxMax - 511, trxMax - 255 ) < 0 );
	
	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif

#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool ) ;
#else
	CallR ( ErrVERMempoolInit( cbucketMost ) );
#endif
	
	// pbucketGlobal{Head,Tail} should be NULL. If they aren't we probably
	// didn't terminate properly at some point
	Assert( pbucketNil == m_pbucketGlobalHead );
	Assert( pbucketNil == m_pbucketGlobalTail );
	m_pbucketGlobalHead = pbucketNil;
	m_pbucketGlobalTail = pbucketNil;

#ifdef DEBUG
///	VERCheckVER();	// call it here to make sure it is linked in
#endif	// DEBUG

	Assert( ppibNil == m_ppibRCEClean );
	Assert( ppibNil == m_ppibRCECleanCallback );
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		CallJ( ErrPIBBeginSession( m_pinst, &m_ppibRCEClean, procidNil, fFalse ), DeleteHash );
		CallJ( ErrPIBBeginSession( m_pinst, &m_ppibRCECleanCallback, procidNil, fFalse ), EndCleanSession );
		m_ppibRCEClean->grbitsCommitDefault = JET_bitCommitLazyFlush;
		m_ppibRCECleanCallback->SetFSystemCallback();
		}

	// sync tasks only during termination
	m_fSyncronousTasks = fFalse;

	m_fVERCleanUpWait = 0;

	//	set m_cbucketGlobalAllocMost
	m_critBucketGlobal.Enter();
	m_cbucketGlobalAllocMost = cbucketMost;
	Assert( cbucketPreferred >= 1 );
	m_cbucketGlobalAllocPreferred = cbucketPreferred;
	
	Assert( m_cbucketGlobalAllocPreferred <= m_cbucketGlobalAllocMost );
	Assert( m_cbucketGlobalAllocPreferred >= 1 );
	Assert( m_cbucketGlobalAllocMost >= 1 );

	m_critBucketGlobal.Leave();

	return err;
	
EndCleanSession:
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		PIBEndSession( m_ppibRCEClean );
		}

DeleteHash:
	
#ifndef GLOBAL_VERSTORE_MEMPOOL
	VERMempoolTerm();
#endif	
	return err;
	}


//  ================================================================
VOID VER::VERTerm( BOOL fNormal )
//  ================================================================
//
//	Terminates background thread and releases version store
//	resources.
//
//-
	{
	//  The TASKMGR has gone away by the time we are shutting down
	Assert ( m_fSyncronousTasks );
	
	//	be sure that the state will not change
	m_critRCEClean.Enter();
	LONG fStatus = AtomicExchange( (LONG *)&m_fVERCleanUpWait, 2 );
	m_critRCEClean.Leave();
	if ( 1 == fStatus )
		{
		m_asigRCECleanDone.Wait();
		}
	
	if ( fNormal )
		{
		Assert( trxMax == TrxOldest( m_pinst ) );
		ERR err = ErrVERRCEClean( );
		Assert( err != JET_wrnRemainingVersions );
		if ( err < JET_errSuccess )
			{
			fNormal = fFalse;
			AssertTracking();
			}
		}

	if ( ppibNil != m_ppibRCEClean )
		{
		PIBEndSession( m_ppibRCEClean );
		m_ppibRCEClean = ppibNil;
		}

	if ( ppibNil != m_ppibRCECleanCallback )
		{
#ifdef DEBUG
		for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			Assert( !FPIBUserOpenedDatabase( m_ppibRCECleanCallback, dbid ) );
			}
#endif	//	DEBUG
		PIBEndSession( m_ppibRCECleanCallback );
		m_ppibRCECleanCallback = ppibNil;
		}

	// pbucketGlobal{Head,Tail} should be NULL. If they aren't we probably
	// didn't cleanup properly
	Assert( pbucketNil == m_pbucketGlobalHead || !fNormal);
	Assert( pbucketNil == m_pbucketGlobalTail || !fNormal );
#ifdef GLOBAL_VERSTORE_MEMPOOL
	if ( fNormal )
		{
		//	Should be freed normally
		Assert( pbucketNil == m_pbucketGlobalHead );
		Assert( pbucketNil == m_pbucketGlobalTail );
		}
	else
		{
		//	Need to walk and free to the global version store
		//	Note that linked list goes from head -> prev -> prev -> prev -> nil
		BUCKET* pbucket = m_pbucketGlobalHead;
		while ( pbucketNil != pbucket )
			{
			BUCKET* const pbucketPrev = pbucket->hdr.pbucketPrev;
			VERIReleasePbucket( pbucket );
			pbucket = pbucketPrev;
			}
		}
#endif
	Assert( 0 == m_cbucketGlobalAlloc );
	m_pbucketGlobalHead = pbucketNil;
	m_pbucketGlobalTail = pbucketNil;
		
	//	deallocate resVerPagesGlobal ( buckets space )

#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
#else // GLOBAL_VERSTORE_MEMPOOL
	VERMempoolTerm();
#endif	

#ifdef VERPERF
	cVERcbucketAllocated.Clear( m_pinst );
	cVERcbucketDeleteAllocated.Clear( m_pinst );
	cVERcrceHashUsage.Clear( m_pinst );
	cVERcrceHashEntries.Clear( m_pinst );	//  number of RCEs that currently exist in hash table
	cVERcbBookmarkTotal.Clear( m_pinst );	//  amount of space used by bookmarks of existing RCEs
	cVERUnnecessaryCalls.Clear( m_pinst );	//  calls for a bookmark that doesn't exist
#endif // VERPERF
	}


//  ================================================================
ERR VER::ErrVERStatus( )
//  ================================================================
//
//	Returns JET_wrnIdleFull if version store more than half full.
//
//-
	{
	ENTERCRITICALSECTION	enterCritRCEBucketGlobal( &m_critBucketGlobal );
	ERR						err = ( m_cbucketGlobalAlloc > ( m_cbucketGlobalAllocPreferred * 0.6 ) ?
									ErrERRCheck( JET_wrnIdleFull ) :
									JET_errSuccess );
	return err;
	}

	
//  ================================================================
VS VsVERCheck( const FUCB * pfucb, CSR * pcsr, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, returns the version status
//
//	RETURN VALUE
//		vsCommitted
//		vsUncommittedByCaller
//		vsUncommittedByOther
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	
	const UINT 			uiHash 	= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const RCE * const 	prce 	= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	VS vs = vsNone;

	//	if no RCE for node then version bit in node header must
	//	have been orphaned due to crash.  Remove node bit.
	if ( prceNil == prce )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif
		if ( FFUCBUpdatable( pfucb ) )
			{
			NDDeferResetNodeVersion( pcsr );
			}
		vs = vsCommitted;
		}
	else if ( prce->TrxCommitted() != trxMax )
		{
		//	committed
		vs = vsCommitted;
		}
	else if ( prce->Pfucb()->ppib != pfucb->ppib )
		{
		//	not modified (uncommitted)
		vs = vsUncommittedByOther;
		}
	else
		{
		//	modifier (uncommitted)
		vs = vsUncommittedByCaller;
		}

	Assert( vsNone != vs );
	return vs;
	}


//  ================================================================
ERR ErrVERAccessNode( FUCB * pfucb, const BOOKMARK& bookmark, NS * pns )
//  ================================================================
//
//	Finds the correct version of a node.
//
//	PARAMETERS
//		pfucb			various fields used/returned.
//		pfucb->kdfCurr	the returned prce or NULL to tell caller to
//						use the node in the DB page.
//
//	RETURN VALUE
//		nsVersion
//		nsDatabase
//		nsInvalid
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	//	session with dirty cursor isolation model should never
	//	call NsVERAccessNode.	
	Assert( !FPIBDirty( pfucb->ppib ) );
	Assert( FPIBVersion( pfucb->ppib ) );
	Assert( !FFUCBUnique( pfucb ) || 0 == bookmark.data.Cb() );
	Assert( Pcsr( pfucb )->FLatched() );


	//  FAST PATH:  if there are no RCEs in this bucket, immediately bail with
	//  nsDatabase.  the assumption is that the version store is almost always
	//  nearly empty

	const UINT uiHash = UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	if ( prceNil == PverFromIfmp( pfucb->ifmp )->GetChain( uiHash ) )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif

		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}

		*pns = nsDatabase;
		return JET_errSuccess;
		}

	
	ERR	err	= JET_errSuccess;
	
	const TRX			trxSession	= TrxVERISession( pfucb );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	const RCE 			*prce 		= PrceFirstVersion( uiHash, pfucb, bookmark );

	while( 	prce
			&& trxMax != prce->TrxCommitted()
			&& operWaitLock != prce->Oper()
			&& prce->FDoesNotWriteConflict() )
		{
		
		//  skip past (committed) RCEs that we don't write conflict with
		
		prce = prce->PrcePrevOfNode();
		}

	NS nsStatus = nsNone;
	if ( prceNil == prce )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif
		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}
		nsStatus = nsDatabase;
		}
	else if ( prce->TrxCommitted() == trxMax &&
			  prce->Pfucb()->ppib == pfucb->ppib )
		{
		//	cannot be trying to access an RCE that we ourselves rolled back
		Assert( !prce->FRolledBack() );

		//	if caller is modifier of uncommitted version then database
		nsStatus = nsDatabase;
		}
	else if ( TrxCmp( prce->TrxCommitted(), trxSession ) < 0 )
		{
		//	if committed version younger than our transaction then database
		nsStatus = nsDatabase;
		}
	else
		{
		//  active version created by another session. look for before image
		Assert( prceNil != prce && TrxCmp( prce->TrxCommitted(), trxSession ) >= 0);
		//	loop will set prce to the non-delta RCE whose before image was committed
		//	before this transaction began. if all active RCE's are delta RCE's we set prce to
		//  the oldest uncommitted delta RCE 
		const RCE * prceLastNonDelta = prce;
		const RCE * prceLastReplace  = prceNil;
		for ( ; prceNil != prce && TrxCmp( prce->TrxCommitted(), trxSession ) >= 0; prce = prce->PrcePrevOfNode() )
			{
			if ( !prce->FRolledBack() )
				{
				switch( prce->Oper() )
					{
					case operDelta:
					case operReadLock:
					case operWriteLock:
					case operWaitLock:
					case operSLVSpace:
						break;
					case operReplace:
						prceLastReplace = prce;
						//  FALLTHRU to case below
					default:
						prceLastNonDelta = prce;
						break;
					}
				}
			}

		prce = prceLastNonDelta;
		
		switch( prce->Oper() )
			{
			case operReplace:
				nsStatus = ( prce->FRolledBack() ? nsDatabase : nsVersion );
				break;
			case operInsert:
				nsStatus = nsInvalid;
				break;
			case operFlagDelete:
				nsStatus = nsVerInDB;
				break;
			case operDelta:
			case operReadLock:
			case operWriteLock:
			case operPreInsert:
			case operWaitLock:
			case operSLVSpace:
				//  all the active versions are delta, Lock or SLV space versions. no before images
				nsStatus = nsDatabase;
				break;
			default:
				AssertSz( fFalse, "Illegal operation in RCE chain" );
				break;
			}

		if ( prceNil != prceLastReplace && nsInvalid != nsStatus )
			{
			Assert( prceLastReplace->CbData() >= cbReplaceRCEOverhead );
			Assert( !prceLastReplace->FRolledBack() );

			pfucb->kdfCurr.key.prefix.Nullify();
			pfucb->kdfCurr.key.suffix.SetPv( const_cast<BYTE *>( prceLastReplace->PbBookmark() ) );
			pfucb->kdfCurr.key.suffix.SetCb( prceLastReplace->CbBookmarkKey() );
			pfucb->kdfCurr.data.SetPv( const_cast<BYTE *>( prceLastReplace->PbData() ) + cbReplaceRCEOverhead );
			pfucb->kdfCurr.data.SetCb( prceLastReplace->CbData() - cbReplaceRCEOverhead );

			if ( 0 == pfucb->ppib->level )
				{
				//  Because we are at level 0 this RCE may disappear at any time after
				//  we leave the version store. We copy the before image into the FUCB
				//  to make sure we can always access it
				
				//  we should not be modifying the page at level 0
				Assert( Pcsr( pfucb )->Latch() == latchReadTouch
						|| Pcsr( pfucb )->Latch() == latchReadNoTouch ); 

				const INT cbRecord = pfucb->kdfCurr.key.Cb() + pfucb->kdfCurr.data.Cb();
				if ( NULL != pfucb->pvRCEBuffer )
					{
					OSMemoryHeapFree( pfucb->pvRCEBuffer );
					}
				pfucb->pvRCEBuffer = PvOSMemoryHeapAlloc( cbRecord );
				if ( NULL == pfucb->pvRCEBuffer )
					{
					Call( ErrERRCheck( JET_errOutOfMemory ) );
					}
				Assert( 0 == pfucb->kdfCurr.key.prefix.Cb() );
				memcpy( pfucb->pvRCEBuffer,
						pfucb->kdfCurr.key.suffix.Pv(),
						pfucb->kdfCurr.key.suffix.Cb() );
				memcpy( (BYTE *)(pfucb->pvRCEBuffer) + pfucb->kdfCurr.key.suffix.Cb(),
						pfucb->kdfCurr.data.Pv(),
						pfucb->kdfCurr.data.Cb() );
				pfucb->kdfCurr.key.suffix.SetPv( pfucb->pvRCEBuffer );
				pfucb->kdfCurr.data.SetPv( (BYTE *)pfucb->pvRCEBuffer + pfucb->kdfCurr.key.suffix.Cb() );
				}			
			ASSERT_VALID( &(pfucb->kdfCurr) );
			}
		}
	Assert( nsNone != nsStatus );

HandleError:

	*pns = nsStatus;
	
	Assert( JET_errSuccess == err || 0 == pfucb->ppib->level );
	return err;
	}


//  ================================================================
LOCAL BOOL FUpdateIsActive( const PIB * const ppib, const UPDATEID& updateid )
//  ================================================================
	{
	BOOL fUpdateIsActive = fFalse;

	const FUCB * pfucb;
	for ( pfucb = ppib->pfucbOfSession; pfucb != pfucbNil; pfucb = pfucb->pfucbNextOfSession )
		{
		if( FFUCBReplacePrepared( pfucb ) && pfucb->updateid == updateid )
			{
			fUpdateIsActive = fTrue;
			break;
			}
		}

	return fUpdateIsActive;
	}


//  ================================================================
LONG LDeltaVERGetDelta( const FUCB * pfucb, const BOOKMARK& bookmark, INT cbOffset )
//  ================================================================
//
//  Returns the correct compensating delta for this transaction on the given offset
//  of the bookmark. Collect the negation of all active delta versions not created
//  by our session.
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	// UNDONE: CIM support for updates is currently broken.
	Assert( FPIBVersion( pfucb->ppib ) );
 	const TRX			trxSession	= TrxVERISession( pfucb );
	const UINT			uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	
	LONG				lDelta		= 0;	
	
	const RCE 			*prce		= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		if ( operReplace == prce->Oper() )
			{
			if ( prce->FActiveNotByMe( pfucb->ppib, trxSession ) )
				{
				//	there is an outstanding replace (not by us) on this
				//	node -- must reset compensating delta count to begin
				//	from this point
				lDelta = 0;

				//	UNDONE: return a flag indicating potential write
				//	conflict so ErrRECAOSeparateLV() will not bother even
				//	even trying to do a replace on the LVROOT and instead
				//	burst immediately
				//	*pfPotentialWriteConflict = fTrue;
				}
			}
		else if ( operDelta == prce->Oper() )
			{
			Assert( !prce->FRolledBack() );		// Delta RCE's can never be flag-RolledBack

			const VERDELTA* const pverdelta = ( VERDELTA* )prce->PbData();
			if ( pverdelta->cbOffset == cbOffset
				&& 	( prce->FActiveNotByMe( pfucb->ppib, trxSession )
					|| ( 	trxMax == prce->TrxCommitted() 
							&& prce->Pfucb()->ppib == pfucb->ppib
							&& FUpdateIsActive( prce->Pfucb()->ppib, prce->Updateid() ) ) ) )
				{
				//  delta version created by a different session.
				//	the version is either uncommitted or created by
				//  a session that started after us
				//	or a delta done by a currently uncommitted replace
				lDelta += -pverdelta->lDelta;
				}
			}
		}

	return lDelta;
	}


//  ================================================================
BOOL FVERDeltaActiveNotByMe( const FUCB * pfucb, const BOOKMARK& bookmark, INT cbOffset )
//  ================================================================
//
//	get prce for node and look for uncommitted increment/decrement
//	versions created by another session.Used to determine if the delta
//  value we see may change because of a rollback.
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( 0 == cbOffset );	//  should only be used from LV

	const TRX			trxSession		= pfucb->ppib->trxBegin0;
	Assert( trxMax != trxSession );
	
	const UINT			uiHash 			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	BOOL 				fVersionExists	= fFalse;

	const RCE 			*prce 	= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		const VERDELTA* const pverdelta = ( VERDELTA* )prce->PbData();
		if ( operDelta == prce->Oper()
			&& pverdelta->cbOffset == cbOffset
			&& pverdelta->lDelta != 0 )
			{
			const TRX	trxCommitted	= prce->TrxCommitted();
			if ( ( trxMax == trxCommitted
					&& prce->Pfucb()->ppib != pfucb->ppib )
				|| ( trxMax != trxCommitted
					&& TrxCmp( trxCommitted, trxSession ) > 0 ) )
				{
				//  uncommitted delta version created by another session
				fVersionExists = fTrue;
				break;
				}
			}
		}

	return fVersionExists;
	}


//  ================================================================
INLINE BOOL FVERIGetReplaceInRangeByUs(
	const PIB		*ppib,
	const RCE		*prceLastBeforeEndOfRange,
	const RCEID		rceidFirst,
	const RCEID		rceidLast,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	const BOOL		fFindLastReplace,
	const RCE		**pprceReplaceInRange )
//  ================================================================
	{
	const RCE		*prce;
	BOOL			fSawCommittedByUs = fFalse;
	BOOL			fSawUncommittedByUs = fFalse;
	
	Assert( prceNil != prceLastBeforeEndOfRange );
	Assert( trxCommitted == trxMax ? ppibNil != ppib : ppibNil == ppib );

	Assert( PverFromIfmp( prceLastBeforeEndOfRange->Ifmp() )->CritRCEChain( prceLastBeforeEndOfRange->UiHash() ).FOwner() );
	
	// Initialize return value.
	*pprceReplaceInRange = prceNil;

	Assert( rceidNull != rceidLast );
	Assert( rceidFirst < rceidLast );
	if ( rceidNull == rceidFirst )
		{
		// If rceidFirst == NULL, then no updates were done in the range (this
		// will force retrieval of the after-image (since no updates were done
		// the before-image will be equivalent to the after-image).
		return fFalse;
		}

	//  go backwards through all RCEs in the range
	const RCE	*prceFirstReplaceInRange = prceNil;
	for ( prce = prceLastBeforeEndOfRange;
		prceNil != prce && rceidFirst < prce->Rceid();
		prce = prce->PrcePrevOfNode() )
		{
		Assert( prce->Rceid() < rceidLast );
		if ( prce->FOperReplace() )
			{
			if ( fSawCommittedByUs )
				{
				// If only looking for the last RCE, we would have exited
				// before finding a second one.
				Assert( !fFindLastReplace );
				
				Assert( trxMax != trxCommitted );
				Assert( ppibNil == ppib );
				
				// If one node in the range belogs to us, they must all belong
				// to us (otherwise a WriteConflict would have been generated).
				if ( fSawUncommittedByUs )
					{
					Assert( prce->TrxCommitted() == trxMax );
					Assert( prce->TrxBegin0() == trxBegin0 );
					}
				else if ( prce->TrxCommitted() == trxMax )
					{
					Assert( prce->TrxBegin0() == trxBegin0 );
					fSawUncommittedByUs = fTrue;
					}
				else
					{
					Assert( prce->TrxCommitted() == trxCommitted );
					}
				}
				
			else if ( fSawUncommittedByUs )
				{
				// If only looking for the last RCE, we would have exited
				// before finding a second one.
				Assert( !fFindLastReplace );
				
				// If one node in the range belogs to us, they must all belong
				// to us (otherwise a WriteConflict would have been generated).
				Assert( prce->TrxCommitted() == trxMax );
				Assert( prce->TrxBegin0() == trxBegin0 );
				if ( trxMax == trxCommitted )
					{
					Assert( ppibNil != ppib );
					Assert( prce->Pfucb()->ppib == ppib );
					}
				}
			else
				{
				if ( prce->TrxCommitted() == trxMax )
					{
					Assert( ppibNil != prce->Pfucb()->ppib );
					if ( prce->TrxBegin0() != trxBegin0 )
						{
						// The node is uncommitted, but it doesn't belong to us.
						Assert( trxCommitted != trxMax
							|| prce->Pfucb()->ppib != ppib );
						return fFalse;
						}

					//	if original RCE also uncommitted, assert same session.
					//	if original RCE already committed, can't assert anything.
					Assert( trxCommitted != trxMax
						|| prce->Pfucb()->ppib == ppib );
						
					fSawUncommittedByUs = fTrue;
					}
				else if ( prce->TrxCommitted() == trxCommitted )
					{
					// The node was committed by us.
					Assert( trxMax != trxCommitted );
					Assert( ppibNil == ppib );
					fSawCommittedByUs = fTrue;
					}
				else
					{
					// The node is committed, but not by us.
					Assert( prce->TrxBegin0() != trxBegin0 );
					return fFalse;
					}

				if ( fFindLastReplace )
					{
					// Only interested in the existence of a replace in the range,
					// don't really want the image.
					return fTrue;
					}
				}
				
			prceFirstReplaceInRange = prce;
			}
		}	// for
	Assert( prceNil == prce || rceidFirst >= prce->Rceid() );

	if ( prceNil != prceFirstReplaceInRange )
		{
		*pprceReplaceInRange = prceFirstReplaceInRange;
		return fTrue;
		}
	
	return fFalse;
	}
	

//  ================================================================
#ifdef DEBUG
LOCAL VOID VERDBGCheckReplaceByOthers(
	const RCE	* const prce,
	const PIB	* const ppib,
	const TRX	trxCommitted,
	const RCE	* const prceFirstActiveReplaceByOther,
	BOOL		*pfFoundUncommitted,
	BOOL		*pfFoundCommitted )
	{
	if ( prce->TrxCommitted() == trxMax )
		{
		Assert( ppibNil != prce->Pfucb()->ppib );
		Assert( ppib != prce->Pfucb()->ppib );
			
		if ( *pfFoundUncommitted )
			{
			// All uncommitted RCE's should be owned by the same session.
			Assert( prceNil != prceFirstActiveReplaceByOther );
			Assert( prceFirstActiveReplaceByOther->TrxCommitted() == trxMax );
			Assert( prceFirstActiveReplaceByOther->TrxBegin0() == prce->TrxBegin0() );
			Assert( prceFirstActiveReplaceByOther->Pfucb()->ppib == prce->Pfucb()->ppib );
			}
		else
			{
			if ( *pfFoundCommitted )
				{
				//	must belong to same session in the middle of committing to level 0
				Assert( prceNil != prceFirstActiveReplaceByOther );
				Assert( prceFirstActiveReplaceByOther->TrxBegin0() == prce->TrxBegin0() );
				}
			else
				{
				Assert( prceNil == prceFirstActiveReplaceByOther );
				}
				
			*pfFoundUncommitted = fTrue;
			}
		}
	else
		{
		Assert( prce->TrxCommitted() != trxCommitted );
		
		if ( !*pfFoundCommitted )
			{
			if ( *pfFoundUncommitted )
				{
				// If there's also an uncommitted RCEs on this node,
				// it must have started its transaction after this one committed.
				Assert( prceNil != prceFirstActiveReplaceByOther );
				Assert( prceFirstActiveReplaceByOther->TrxCommitted() == trxMax );
				Assert( prce->Rceid() < prceFirstActiveReplaceByOther->Rceid() );
				Assert( TrxCmp( prce->TrxCommitted(), prceFirstActiveReplaceByOther->TrxBegin0() ) < 0 );
				}
				
			//	Cannot be any uncommitted RCE's of any type
			//	before a committed replace RCE, except if the
			//	same session is in the middle of committing to level 0.
			const RCE	* prceT;
			for ( prceT = prce; prceNil != prceT; prceT = prceT->PrcePrevOfNode() )
				{
				if ( prceT->TrxCommitted() == trxMax )
					{
					Assert( !*pfFoundUncommitted );
					Assert( prceT->TrxBegin0() == prce->TrxBegin0() );
					}
				}
				
			*pfFoundCommitted = fTrue;
			}
		}
	}
#endif
//  ================================================================


//  ================================================================
BOOL FVERGetReplaceImage(
	const PIB		*ppib,
	const IFMP		ifmp,
	const PGNO		pgnoLVFDP,
	const BOOKMARK& bookmark,
	const RCEID 	rceidFirst,
	const RCEID		rceidLast,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	const BOOL		fAfterImage,
	const BYTE 		**ppb,
	ULONG 			* const pcbActual
	)
//  ================================================================
//
//  Extract the before image from the oldest replace RCE for the bookmark
//  that falls exclusively within the range (rceidFirst,rceidLast)
//
//-
	{
	const UINT	uiHash				= UiRCHashFunc( ifmp, pgnoLVFDP, bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	const RCE	*prce				= PrceRCEGet( uiHash, ifmp, pgnoLVFDP, bookmark );
	const RCE	*prceDesiredImage	= prceNil;
	const RCE	*prceFirstAfterRange= prceNil;

	Assert( rceidMax != rceidFirst );
	Assert( rceidMax != rceidLast );
	Assert( trxMax != trxBegin0 );

	// find the last RCE before the end of the range
	while ( prceNil != prce && prce->Rceid() >= rceidLast )
		{
		prceFirstAfterRange = prce;
		prce = prce->PrcePrevOfNode();
		}

	const RCE	* const prceLastBeforeEndOfRange	= prce;
	
	if ( prceNil == prceLastBeforeEndOfRange )
		{
		Assert( prceNil == prceDesiredImage );
		}
	else
		{
		Assert( prceNil == prceFirstAfterRange
			|| prceFirstAfterRange->Rceid() >= rceidLast );
		Assert( prceLastBeforeEndOfRange->Rceid() < rceidLast );
		Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );

		const BOOL fReplaceInRangeByUs = FVERIGetReplaceInRangeByUs(
													ppib,
													prceLastBeforeEndOfRange,
													rceidFirst,
													rceidLast,
													trxBegin0,
													trxCommitted,
													fAfterImage,
													&prceDesiredImage );
		if ( fReplaceInRangeByUs )
			{
			if ( fAfterImage )
				{
				// If looking for the after-image, it will be found in the
				// node's next replace RCE after the one found in the range.
				Assert( prceNil == prceDesiredImage );
				Assert( prceNil != prceLastBeforeEndOfRange );
				Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );
				}
			else
				{
				Assert( prceNil != prceDesiredImage );
				}
			}
			
		else if ( prceLastBeforeEndOfRange->TrxBegin0() == trxBegin0 )
			{
			//	If last operation before the end of the range belongs
			//	to us, then there will be no other active images on the
			//	node by other sessions (they would have write-conflicted).
			//	We can just fall through below to grab the proper image.
			
			if ( prceLastBeforeEndOfRange->TrxCommitted() != trxMax )
				{
				//	If the last RCE in the range has already committed,
				//	then the RCE on which the search was based must
				//	also have been committed at the same time.
				Assert( prceLastBeforeEndOfRange->TrxCommitted() == trxCommitted );
				Assert( ppibNil == ppib );
				}
			else if ( trxCommitted == trxMax )
				{
				//	Verify last RCE in the range belongs to same
				//	transaction.
				Assert( ppibNil != ppib );
				Assert( ppib == prceLastBeforeEndOfRange->Pfucb()->ppib );
				Assert( trxBegin0 == ppib->trxBegin0 );
				}
			else
				{
				//	This is the case where the RCE in the range is uncommitted,
				//	but the RCE on which the search was based has already
				//	committed.  This is a valid case (we could be looking at
				//	the RCE while the transaction is in the middle of being
				//	committed).  Can't really do anything to assert this except
				//	to check that the trxBegin0 is the same, which we've already
				//	done above.
				}

			// Force to look after the specified range for our image.
			Assert( prceNil == prceDesiredImage );
			}
			
		else
			{
			const RCE	*prceFirstActiveReplaceByOther = prceNil;
#ifdef DEBUG
			BOOL		fFoundUncommitted = fFalse;
			BOOL		fFoundCommitted = fFalse;
#endif		

			Assert( prceNil == prceDesiredImage );
			Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );
			
			// No replace RCE's by us between the specified range, or
			// any RCE's by us of any type before the end of the range.
			// Check active RCE's by others.
			const RCE	*prce;
			for ( prce = prceLastBeforeEndOfRange;
				prceNil != prce;
				prce = prce->PrcePrevOfNode() )
				{
				Assert( prce->Rceid() < rceidLast );
				//  For retrieving a LVROOT while getting a before image we may see deltas
				Assert( prce->TrxBegin0() != trxBegin0 || operDelta == prce->Oper() );

				if ( TrxCmp( prce->TrxCommitted(), trxBegin0 ) < 0 )
					break;	// No more active RCE's.
				
				if ( prce->FOperReplace() )
					{
#ifdef DEBUG					
					VERDBGCheckReplaceByOthers(
								prce,
								ppib,
								trxCommitted,
								prceFirstActiveReplaceByOther,
								&fFoundUncommitted,
								&fFoundCommitted );
#endif								

					// There may be multiple active RCE's on
					// the same node. We want the very first one.
					prceFirstActiveReplaceByOther = prce;
					}
					
				}	// for

				
			Assert( prceNil == prceDesiredImage );
			if ( prceNil != prceFirstActiveReplaceByOther )
				prceDesiredImage = prceFirstActiveReplaceByOther;
			}
		}

	// If no RCE's within range or before range, look after the range.
	if ( prceNil == prceDesiredImage )
		{
		for ( prce = prceFirstAfterRange;
			prceNil != prce;
			prce = prce->PrceNextOfNode() )
			{
			Assert( prce->Rceid() >= rceidLast );
			if ( prce->FOperReplace() )
				{
				prceDesiredImage = prce;
				break;
				}
			}
		}
		
	if ( prceNil != prceDesiredImage )
		{
		const VERREPLACE* pverreplace = (VERREPLACE*)prceDesiredImage->PbData();
		*pcbActual 	= prceDesiredImage->CbData() - cbReplaceRCEOverhead;
		*ppb 		= (BYTE *)pverreplace->rgbBeforeImage;
		return fTrue;
		}
		
	return fFalse;
	}


//  ================================================================
ERR VER::ErrVERICreateDMLRCE(
	FUCB			* pfucb,
	UPDATEID		updateid,
	const BOOKMARK&	bookmark,
	const UINT		uiHash,
	const OPER		oper,
	const BOOL		fDoesNotWriteConflict,
	const LEVEL		level,
	const BOOL		fProxy,
	RCE 			**pprce,
	RCEID			rceid
	)
//  ================================================================
//
//	Creates a DML RCE in a bucket
//
//-
	{
	Assert( pfucb->ppib->level > 0 );
	Assert( level > 0 );
	Assert( pfucb->u.pfcb != pfcbNil );
	Assert( FOperInHashTable( oper ) );
	
	ERR		err		= JET_errSuccess;
	RCE		*prce	= prceNil;

	//	calculate the length of the RCE in the bucket.
	//	if updating node, set cbData in RCE to length of data. (w/o the key).
	//	set cbNewRCE as well.
	const INT cbBookmark = bookmark.key.Cb() + bookmark.data.Cb();

	INT cbNewRCE = sizeof( RCE ) + cbBookmark;
	switch( oper )
		{
		case operReplace:
			Assert( !pfucb->kdfCurr.data.FNull() );
			cbNewRCE += cbReplaceRCEOverhead + pfucb->kdfCurr.data.Cb();
			break;
		case operDelta:
		  	cbNewRCE += sizeof( VERDELTA );
			break;
		case operSLVSpace:
			cbNewRCE += pfucb->kdfCurr.data.Cb();
			break;
		case operWaitLock:
			cbNewRCE += sizeof( VERWAITLOCK );
			break;
		case operInsert:
		case operFlagDelete:
		case operReadLock:
		case operWriteLock:
		case operPreInsert:
			break;
		default:
			Assert( fFalse );
			break;
		}

	//	Set up a skelton RCE. This holds m_critBucketGlobal, so do it
	//	first before filling the rest.
	Call( ErrVERICreateRCE(
			cbNewRCE,
			pfucb->u.pfcb,
			pfucb,
			updateid,
			pfucb->ppib->trxBegin0,
			level,
			bookmark.key.Cb(),
			bookmark.data.Cb(),
			oper,
			fDoesNotWriteConflict,
			uiHash,
			&prce,
			fProxy,
			rceid
			) );

	if ( FOperConcurrent( oper ) )
		{
		Assert( CritRCEChain( uiHash ).FOwner() );
		}
	else
		{
		Assert( CritRCEChain( uiHash ).FNotOwner() );
		}
	
	//  copy the bookmark
	prce->CopyBookmark( bookmark );
	
	Assert( pgnoNull == prce->PgnoUndoInfo( ) );
	Assert( prce->Oper() != operAllocExt );
	Assert( !prce->FOperDDL() );

	//	flag FUCB version
	FUCBSetVersioned( pfucb );

	CallS( err );

HandleError:
	if ( pprce )
		{
		*pprce = prce;
		}

	return err;
	}


//  ================================================================
LOCAL VOID VERISetupInsertedDMLRCE( const FUCB * pfucb, RCE * prce )
//  ================================================================
//
//  This copies the appropriate data from the FUCB into the RCE and
//  propagates the maximum node size. This must be called after
//  insertion so the maximum node size can be found (for operReplace)
//
//  This currently does not need to be called from VERModifyByProxy
//
//-
	{
	Assert( prce->TrxCommitted() == trxMax );
	//	If replacing node, rather than inserting or deleting node,
	//	copy the data to RCE for before image for version readers.
	//	Data size may be 0.
	switch( prce->Oper() )
		{
		case operReplace:
			{
			Assert( prce->CbData() >= cbReplaceRCEOverhead );

			VERREPLACE* const pverreplace	= (VERREPLACE*)prce->PbData();

			if ( pfucb->fUpdateSeparateLV )
				{
				//  we updated a separateLV store the begin time of the PrepareUpdate
				pverreplace->rceidBeginReplace = pfucb->rceidBeginUpdate;
				}
			else
				{
				pverreplace->rceidBeginReplace = rceidNull;
				}
			
			const RCE * const prcePrevReplace = PrceVERIGetPrevReplace( prce );
			if ( prceNil != prcePrevReplace )
				{
				//  a previous version exists. its max size is the max of the before- and
				//  after-images (the after-image is our before-image)
				Assert( !prcePrevReplace->FRolledBack() );
				const VERREPLACE* const pverreplacePrev = (VERREPLACE*)prcePrevReplace->PbData();
				Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering && fRecoveringUndo != PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecoveringMode || 
						pverreplacePrev->cbMaxSize >= (SHORT)pfucb->kdfCurr.data.Cb() );
				pverreplace->cbMaxSize = pverreplacePrev->cbMaxSize;
				}
			else
				{
				//  no previous replace. max size is the size of our before image
				pverreplace->cbMaxSize = (SHORT)pfucb->kdfCurr.data.Cb();
				}
				
			pverreplace->cbDelta = 0;

			Assert( prce->Oper() == operReplace );

			// move to data byte and copy old data (before image)
			UtilMemCpy( pverreplace->rgbBeforeImage, pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
			}
			break;

		case operDelta:
			{
			Assert( sizeof( VERDELTA ) == pfucb->kdfCurr.data.Cb() );
			*( VERDELTA* )prce->PbData() = *( VERDELTA* )pfucb->kdfCurr.data.Pv();
			}
			break;

		case operSLVSpace:
			{
			Assert( OffsetOf( VERSLVSPACE, wszFileName ) + sizeof( wchar_t ) <= pfucb->kdfCurr.data.Cb() );
			memcpy( prce->PbData(), pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
			}
			break;

		case operWaitLock:
			{
			//  initialize the signal
			VERWAITLOCK * const pverwaitlock = (VERWAITLOCK *)prce->PbData();
			Assert( PvAlignForAllPlatforms( pverwaitlock ) );
			new( &(pverwaitlock->signal) ) CManualResetSignal( CSyncBasicInfo( "VER::VERWAITLOCK::signal" ) );
			pverwaitlock->fInit = fTrue;
			}

		default:
			break;
		}
	}


//  ================================================================
LOCAL VOID VERWaitForWaitLock( RCE * const prce )
//  ================================================================
	{
	Assert( operWaitLock == prce->Oper() );
	
	const UINT uiHash = prce->UiHash();
	
	VERWAITLOCK * const pverwaitlock = (VERWAITLOCK *)prce->PbData();
	Assert( PvAlignForAllPlatforms( pverwaitlock ) );

	PverFromIfmp( prce->Ifmp() )->CritRCEChain( uiHash ).Leave();

	//  we are in a transaction so this lock cannot be cleaned up under us
	
	pverwaitlock->signal.Wait();

	PverFromIfmp( prce->Ifmp() )->CritRCEChain( uiHash ).Enter();	
	}

	
//  ================================================================
LOCAL BOOL FVERIWriteConflict(
	FUCB*			pfucb,
	const BOOKMARK&	bookmark,
	UINT			uiHash,		
	const OPER		oper
	)
//  ================================================================
	{
	BOOL			fWriteConflict	= fFalse;
	const TRX		trxSession		= TrxVERISession( pfucb );
	RCE*			prce 			= PrceRCEGet(
											uiHash,
											pfucb->ifmp,
											pfucb->u.pfcb->PgnoFDP(),
											bookmark );

	Assert( trxSession != trxMax );

	//	ignore non-conflicting RCEs and committed writeLock RCEs
	
	while( prceNil != prce )
		{
		if ( operWaitLock != prce->Oper() )
			{
			if ( prce->FDoesNotWriteConflict() )
				{
				//  skip past RCEs that we don't write conflict with
				prce = prce->PrcePrevOfNode();
				}
			else
				{
				//	need to check below for write-conflict
				break;
				}
			}
		else if ( trxMax == prce->TrxCommitted() )
			{
			Assert( !PinstFromIfmp( prce->Ifmp() )->FRecovering() );

			//	handling uncommitted waitlocks depends
			//	on whether we own the waitlock or not
			
			if ( prce->Pfucb()->ppib == pfucb->ppib )
				{
				//	OPTIMISATION: we already own a wait lock
				//	on the record, so we know there's no
				//	possible conflict
				return fFalse;
				}

			//  Someone else owns a wait lock on the record:
			//		leave the critical section
			//		wait on the signal
			//		re-enter the critical section
			//		try everything again
			VERWaitForWaitLock( prce );
			prce = PrceRCEGet(
						uiHash,
						pfucb->ifmp,
						pfucb->u.pfcb->PgnoFDP(),
						bookmark );
			}
		else
			{
		//	skip past any committed wait locks
			Assert( !PinstFromIfmp( prce->Ifmp() )->FRecovering() );
			prce = prce->PrcePrevOfNode();
			}

		}


	//  check for write conficts
	//  we can't use the pfucb of a committed transaction as the FUCB has been closed
	//  if a version is committed after we started however, it must have been
	//	created by another session
	if ( prce != prceNil )
		{
		if ( prce->FActiveNotByMe( pfucb->ppib, trxSession ) )
			{
			if ( operReadLock == oper
				|| operDelta == oper
				|| operSLVSpace == oper )
				{				
				//	these operations commute. i.e. two sessions can perform
				//	these operations without conflicting
 				//  we can only do this modification if all the active RCE's
				//	in the chain are of this type
				//  look at all active versions for an operation not of this type
				//  OPTIMIZATION:	if the session changes again (i.e. we get a
				//					_third_ session) we can stop looking as we 
				//					know that the second session commuted with
				//					the first, therefore the third will commute
				//					with the second and first (transitivity)
				const RCE	* prceT			= prce;
				for ( ;
					prceNil != prceT && TrxCmp( prceT->TrxCommitted(), trxSession ) > 0;
					prceT = prceT->PrcePrevOfNode() )
					{
					//  if all active RCEs have the same oper we are OK,
					//	else WriteConflict.
					if ( prceT->Oper() != oper )
						{
						Assert( operWaitLock != prceT->Oper() );
						Assert( !prceT->FDoesNotWriteConflict() );
						fWriteConflict = fTrue;
						break;
						}
					}
				}
			else
				{
				Assert( operWaitLock != prce->Oper() );
				Assert( !prce->FDoesNotWriteConflict() );
				fWriteConflict = fTrue;
				}
			}
			
		else 
			{
#ifdef DEBUG			
			if ( prce->TrxCommitted() == trxMax )
				{
				// Must be my uncommitted version, otherwise it would have been
				// caught by FActiveNotByMe().
				Assert( prce->Pfucb()->ppib == pfucb->ppib );
				Assert( prce->Level() <= pfucb->ppib->level
						|| PinstFromIfmp( pfucb->ifmp )->FRecovering() );		//	could be an RCE created by redo of UndoInfo
				}
			else
				{
				//	RCE exists, but it committed before our session began, so no
				//	danger of write conflict.
				//	Normally, this session's Begin0 cannot be equal to anyone else's Commit0,
				//	but because we only log an approximate trxCommit0, during recovery, we
				//	may find that this session's Begin0 is equal to someone else's Commit0
				Assert( TrxCmp( prce->TrxCommitted(), trxSession ) < 0
					|| ( prce->TrxCommitted() == trxSession && PinstFromIfmp( pfucb->ifmp )->FRecovering() ) );
				}
#endif				

			if ( prce->Oper() != oper && (  operReadLock == prce->Oper()
											|| operDelta == prce->Oper()
											|| operSLVSpace == prce->Oper() ) )
				{				
				//  these previous operation commuted. i.e. two sessions can perform
				//	these operations without conflicting
				//
				//	we are creating a different type of operation that does
				//	not commute 
				//
				//  therefore we must check all active versions to make sure
				//	that we are the only session that has created them
				//
				//  we can only do this modification if all the active RCE's
				//	in the chain created by us
				//
				//  look at all versions for a active versions for a different session
				//
				const RCE	* prceT			= prce;
				for ( ;
					 prceNil != prceT;
					 prceT = prceT->PrcePrevOfNode() )
					{
					if ( prceT->FActiveNotByMe( pfucb->ppib, trxSession ) )
						{
						Assert( operWaitLock != prce->Oper() );
						Assert( !prce->FDoesNotWriteConflict() );
						fWriteConflict = fTrue;
						break;
						}
					}
					
				Assert( fWriteConflict || prceNil == prceT );
				}
			}


#ifdef DEBUG
		if ( !fWriteConflict )
			{
			if ( prce->TrxCommitted() == trxMax )
				{
				Assert( prce->Pfucb()->ppib != pfucb->ppib
					|| prce->Level() <= pfucb->ppib->level
					|| PinstFromIfmp( prce->Pfucb()->ifmp )->FRecovering() );		//	could be an RCE created by redo of UndoInfo
				}

			if ( prce->Oper() == operFlagDelete )
				{
				//	normally, the only RCE that can follow a FlagDelete is an Insert.
				//	unless the RCE was moved, or if we're recovering, in which case we might
				//	create RCE's for UndoInfo
				Assert( operInsert == oper
					|| operPreInsert == oper
					|| operWriteLock == oper
					|| prce->FMoved()
					|| PinstFromIfmp( prce->Ifmp() )->FRecovering() );
				}
			}
#endif		
		}

	return fWriteConflict;
	}

BOOL FVERWriteConflict(
	FUCB			* pfucb,
	const BOOKMARK&	bookmark,
	const OPER		oper )
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	
	const UINT		uiHash	= UiRCHashFunc(
									pfucb->ifmp,
									pfucb->u.pfcb->PgnoFDP(),
									bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	return FVERIWriteConflict( pfucb, bookmark, uiHash, oper );
	}


//  ================================================================
INLINE ERR VER::ErrVERModifyCommitted(
	FCB				*pfcb,
	const BOOKMARK&	bookmark,
	const OPER		oper,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	RCE				**pprce
	)
//  ================================================================
//
//  Used by concurrent create index to create a RCE as though it was done
//  by another session. The trxCommitted of the RCE is set and no checks for
//  write conflicts are done
//
//-
	{
	ASSERT_VALID( &bookmark );
	Assert( pfcb->FTypeSecondaryIndex() );		// only called from concurrent create index.
	Assert( pfcb->PfcbTable() == pfcbNil );
	Assert( trxCommitted != trxMax );

	const UINT 			uiHash	= UiRCHashFunc( pfcb->Ifmp(), pfcb->PgnoFDP(), bookmark );

	ERR     			err 	= JET_errSuccess;
	RCE					*prce	= prceNil;

	//  assert default return value
	Assert( NULL != pprce );
	Assert( prceNil == *pprce );

		{
		//	calculate the length of the RCE in the bucket.
		const INT cbBookmark = bookmark.key.Cb() + bookmark.data.Cb();
		const INT cbNewRCE = sizeof( RCE ) + cbBookmark;
	
		//	Set up a skeleton RCE. This holds m_critBucketGlobal, so do it
		//	first before filling the rest.
		Call( ErrVERICreateRCE(
				cbNewRCE,
				pfcb,
				pfucbNil,
				updateidNil,
				trxBegin0,
				0,
				bookmark.key.Cb(),
				bookmark.data.Cb(),
				oper,
				fFalse,
				uiHash,
				&prce,
				fTrue
				) );
	
		//  copy the bookmark
		prce->CopyBookmark( bookmark );

		if( !prce->FOperConcurrent() )
			{
			ENTERCRITICALSECTION enterCritHash( &( CritRCEChain( uiHash ) ) );
			Call( ErrVERIInsertRCEIntoHash( prce ) );
			prce->SetCommittedByProxy( trxCommitted );
			}
		else
			{
			Call( ErrVERIInsertRCEIntoHash( prce ) );
			prce->SetCommittedByProxy( trxCommitted );
			CritRCEChain( uiHash ).Leave();
			}

		Assert( pgnoNull == prce->PgnoUndoInfo( ) );
		Assert( prce->Oper() == operWriteLock || prce->Oper() == operFlagDelete || prce->Oper() == operPreInsert );

		*pprce = prce;

		ASSERT_VALID( *pprce );
		}
	

HandleError:
	Assert( err < JET_errSuccess || prceNil != *pprce );
	return err;
	}


//  ================================================================
ERR VER::ErrVERModify(
	FUCB			* pfucb,
	const BOOKMARK&	bookmark,
	const OPER		oper,
	RCE				**pprce,
	const VERPROXY	* const pverproxy
	)
//  ================================================================
//
//	Create an RCE for a DML operation.
//
//  OPTIMIZATION:	combine delta/readLock/replace versions
//					remove redundant replace versions
//
//	RETURN VALUE
//		Jet_errWriteConflict for two cases:
//			-for any committed node, caller's transaction begin time
//			is less than node's level 0 commit time.
//			-for any uncommitted node except operDelta/operReadLock at all by another session
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( FOperInHashTable( oper ) );	//  not supposed to be in hash table? use VERFlag
	Assert( !bookmark.key.FNull() );
	Assert( !rgfmp[pfucb->ifmp].FVersioningOff() );
	Assert( !pfucb->ppib->FReadOnlyTrx() );

	//  set default return value
	Assert( NULL != pprce );
	*pprce = prceNil;

	ERR			err 			= JET_errSuccess;
	BOOL		fRCECreated		= fFalse;

	UPDATEID	updateid		= updateidNil;
	RCEID		rceid			= rceidNull;
	LEVEL		level;
	RCE			*prcePrimary	= prceNil;
	FUCB		*pfucbProxy		= pfucbNil;
	const BOOL	fProxy			= ( NULL != pverproxy );

	//  we never create an insert version at runtime. instead we create a writeLock version
	//  and use ChangeOper to change it into an insert
	Assert( m_pinst->m_plog->m_fRecovering || operInsert != oper );

	Assert( !m_pinst->m_plog->m_fRecovering || ( fProxy && proxyRedo == pverproxy->proxy ) );
	if ( fProxy )
		{
		if ( proxyCreateIndex == pverproxy->proxy )
			{
			Assert( !m_pinst->m_plog->m_fRecovering );
			Assert( oper == operWriteLock
				|| oper == operPreInsert
				|| oper == operReplace		// via FlagInsertAndReplaceData
				|| oper == operFlagDelete );
			Assert( prceNil != pverproxy->prcePrimary );
			prcePrimary = pverproxy->prcePrimary;

			if ( pverproxy->prcePrimary->TrxCommitted() != trxMax )
				{
				err = ErrVERModifyCommitted(
							pfucb->u.pfcb,
							bookmark,
							oper,
							prcePrimary->TrxBegin0(),
							prcePrimary->TrxCommitted(),
							pprce );
				return err;
				}
			else
				{
				Assert( prcePrimary->Pfucb()->ppib->critTrx.FOwner() );

				level = prcePrimary->Level();
				
				// Need to allocate an FUCB for the proxy, in case it rolls back.
				CallR( ErrDIROpenByProxy(
							prcePrimary->Pfucb()->ppib,
							pfucb->u.pfcb,
							&pfucbProxy,
							level ) );
				Assert( pfucbNil != pfucbProxy );

				//	force pfucbProxy to be defer-closed, so that it will
				//	not be released until the owning session commits
				//	or rolls back
				Assert( pfucbProxy->ppib->level > 0 );
				FUCBSetVersioned( pfucbProxy );

				// Use proxy FUCB for versioning.
				pfucb = pfucbProxy;
				}
			}
		else
			{
			Assert( proxyRedo == pverproxy->proxy );
			Assert( m_pinst->m_plog->m_fRecovering );
			Assert( rceidNull != pverproxy->rceid );
			rceid = pverproxy->rceid;
			level = LEVEL( pverproxy->level );
			}
		}
	else
		{
		updateid = UpdateidOfPpib( pfucb->ppib );
		level = pfucb->ppib->level;
		}

	const UINT 			uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	Call( ErrVERICreateDMLRCE(
			pfucb,
			updateid,
			bookmark,
			uiHash,
			oper,
			pfucb->ppib->FSessionOLDSLV(),
			level,
			fProxy,
			pprce,
			rceid ) );
	Assert( prceNil != *pprce );
	fRCECreated = fTrue;

	if ( FOperConcurrent( oper ) )
		{
		// For concurrent operations, we had to obtain critHash before
		// allocating an rceid, to ensure that rceid's are chained in order.
		Assert( CritRCEChain( uiHash ).FOwner() );
		}
	else
		{
		// For non-concurrent operations, rceid's won't be chained out
		// of order because all but one RCE will fail with write-conflict
		// below.
		CritRCEChain( uiHash ).Enter();
		}

	// UNDONE: CIM support for updates is currently broken -- only
	// used for dirty reads by concurrent create index.
	Assert( FPIBVersion( pfucb->ppib )
		|| ( prceNil != prcePrimary && prcePrimary->Pfucb()->ppib == pfucb->ppib ) );

	if ( !m_pinst->m_plog->m_fRecovering && FVERIWriteConflict( pfucb, bookmark, uiHash, oper ) )
		{
		Call( ErrERRCheck( JET_errWriteConflict ) );
		}

	Call( ErrVERIInsertRCEIntoHash( *pprce ) );

	VERISetupInsertedDMLRCE( pfucb, *pprce );
	
#ifdef DEBUG_VER
	{
	BOOKMARK bookmarkT;
	(*pprce)->GetBookmark( &bookmarkT );
	CallR( ErrCheckRCEChain(
		*PprceRCEChainGet( (*pprce)->UiHash(), (*pprce)->Ifmp(), (*pprce)->PgnoFDP(), bookmarkT ),
		(*pprce)->UiHash() ) );
	}
#endif	//	DEBUG_VER

	CritRCEChain( uiHash ).Leave();

	ASSERT_VALID( *pprce );
	
	CallS( err );

HandleError:
	if ( err < 0 && fRCECreated )
		{
		(*pprce)->NullifyOper();
		Assert( CritRCEChain( uiHash ).FOwner() );
		CritRCEChain( uiHash ).Leave();
		*pprce = prceNil;
		}

	if ( pfucbNil != pfucbProxy )
		{
		Assert( pfucbProxy->ppib->level > 0 );	// Ensure defer-closed, even on error
		Assert( FFUCBVersioned( pfucbProxy ) );
		DIRClose( pfucbProxy );
		}
		
	return err;
	}


//  ================================================================
ERR VER::ErrVERFlag( FUCB * pfucb, OPER oper, const VOID * pv, INT cb )
//  ================================================================
//
//  Creates a RCE for a DDL or space operation. The RCE is not put
//  in the hash table
//
//-
	{
#ifdef DEBUG
	ASSERT_VALID( pfucb );
	Assert( pfucb->ppib->level > 0 );
	Assert( cb >= 0 );
	Assert( !FOperInHashTable( oper ) );	//  supposed to be in hash table? use VERModify
	Ptls()->fAddColumn = operAddColumn == oper;
#endif	//	DEBUG

	ERR		err		= JET_errSuccess;
	RCE		*prce	= prceNil;	
	FCB 	*pfcb	= pfcbNil;

	if ( rgfmp[pfucb->ifmp].FVersioningOff() )
		{
		Assert( !rgfmp[pfucb->ifmp].FLogOn() );
		return JET_errSuccess;
		}

	pfcb = pfucb->u.pfcb;
	Assert( pfcb != NULL );
		
	//	Set up a skeleton RCE. This holds m_critBucketGlobal, so do it
	//	first before filling the rest.
	Call( ErrVERICreateRCE( 
			sizeof(RCE) + cb,
			pfucb->u.pfcb,
			pfucb,
			updateidNil,
			pfucb->ppib->trxBegin0,
			pfucb->ppib->level,
			0,
			0,
			oper,
			fFalse,
			uiHashInvalid,
			&prce
			) );

	UtilMemCpy( prce->PbData(), pv, cb );
	
	Assert( prce->TrxCommitted() == trxMax );
	VERInsertRCEIntoLists( pfucb, pcsrNil, prce, NULL );

	ASSERT_VALID( prce );

	FUCBSetVersioned( pfucb );

HandleError:
#ifdef DEBUG
	Ptls()->fAddColumn = fFalse;
#endif	//	DEBUG

	return err;
	}


//  ================================================================
VOID VERSetCbAdjust(
			CSR 		*pcsr,
	const 	RCE		 	*prce,
			INT 		cbDataNew,
			INT 		cbDataOld,
			UPDATEPAGE 	updatepage )
//  ================================================================
//
//  Sets the max size and delta fields in a replace RCE
//
//	WARNING:  The following comments explain how a Replace RCE's delta field
//	(ie. the second SHORT stored in rgbData) is used.  The semantics can get
//	pretty confusing, so PLEASE DO NOT REMOVE THESE COMMENTS.  -- JL
//
//	*psDelta records how much the operation contributes to deferred node
//	space reservation. A positive cbDelta here means the node is growing,
//	so we will use up space which may have been reserved (ie. *psDelta will
//	decrease).  A negative cbDelta here means the node is shrinking,
//	so we must add abs(cbDelta) to the *psDelta to reflect how much more node
//	space must be reserved.
//	
//	This is how to interpret the value of *psDelta:
//		- if *psDelta is positive, then *psDelta == reserved node space.  *psDelta can only
//		  be positive after a node shrinkage.
//		- if *psDelta is negative, then abs(*psDelta) is the reserved node space that									 
//		  was consumed during a node growth.  *psDelta can only become negative
//		  after a node shrinkage (which sets aside some reserved node space)
//		  followed by a node growth (which consumes some/all of that
//		  reserved node space).
//
//-
	{
	ASSERT_VALID( pcsr );
	Assert( pcsr->Latch() == latchWrite || fDoNotUpdatePage == updatepage );
	Assert( fDoNotUpdatePage == updatepage || pcsr->Cpage().FLeafPage() );
	Assert( prce->FOperReplace() );
	
	INT	cbDelta = cbDataNew - cbDataOld;

	Assert( cbDelta != 0 );

	VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
	INT	cbMax = pverreplace->cbMaxSize;
	Assert( pverreplace->cbMaxSize >= cbDataOld );
	
	//	set new node maximum size.
	if ( cbDataNew > cbMax )
		{
		//  this is the largest the node has ever been. set the max size and
		//  free all previously reserved space
		Assert( cbDelta > 0 );
		pverreplace->cbMaxSize	= SHORT( cbDataNew );
		cbDelta 				= cbMax - cbDataOld;
		}

	pverreplace->cbDelta = SHORT( pverreplace->cbDelta - cbDelta );

	if ( fDoUpdatePage == updatepage )
		{
		if ( cbDelta > 0 )
			{		
			// If, during this transaction, we've shrunk the node.  There will be
			// some uncommitted freed space.  Reclaim as much of this as needed to
			// satisfy the new node growth.  Note that we can update cbUncommittedFreed
			// in this fashion because the subsequent call to ErrPMReplace() is
			// guaranteed to succeed (ie. the node is guaranteed to grow).
			pcsr->Cpage().ReclaimUncommittedFreed( cbDelta );
			}
		else if ( cbDelta < 0 )
			{
			// Node has decreased in size.  The page header's cbFree has already
			// been increased to reflect this.  But we must also increase 
			// cbUncommittedFreed to indicate that the increase in cbFree is
			// contingent on commit of this operation.
			pcsr->Cpage().AddUncommittedFreed( -cbDelta );
			}
		}
#ifdef DEBUG
	else
		{
		Assert( fDoNotUpdatePage == updatepage );
		}
#endif	//	DEBUG
	}


//  ================================================================
LOCAL INT CbVERIGetNodeMax( const FUCB * pfucb, const BOOKMARK& bookmark, UINT uiHash )
//  ================================================================
//
//  This assumes nodeMax is propagated through the replace RCEs. Assumes
//  it is in the critical section to CbVERGetNodeReserverved can use it for
//  debugging
//
//-
	{
	INT			nodeMax = 0;	

	// Look for any replace RCE's.
	const RCE *prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce && trxMax == prce->TrxCommitted(); prce = prce->PrcePrevOfNode() )
		{
		if ( prce->FOperReplace() && !prce->FRolledBack() )
			{
			nodeMax = ((const VERREPLACE*)prce->PbData())->cbMaxSize;
			break;
			}
		}

	Assert( nodeMax >= 0 );
	return nodeMax;
	}


//  ================================================================
INT CbVERGetNodeMax( const FUCB * pfucb, const BOOKMARK& bookmark )
//  ================================================================
//
//  This enters the critical section and calls CbVERIGetNodeMax
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	const UINT				uiHash	= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const INT				nodeMax = CbVERIGetNodeMax( pfucb, bookmark, uiHash );
	
	Assert( nodeMax >= 0 );
	return nodeMax;
	}


//  ================================================================
INT CbVERGetNodeReserve( const PIB * ppib, const FUCB * pfucb, const BOOKMARK& bookmark, INT cbCurrentData )
//  ================================================================
	{
	Assert( ppibNil == ppib || (ASSERT_VALID( ppib ), fTrue) );
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( cbCurrentData >= 0 );

	const UINT				uiHash			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const BOOL				fIgnorePIB		= ( ppibNil == ppib );

	INT						cbNodeReserve	= 0;

	// Find all uncommitted RCE's for this node.
	const RCE * prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce && trxMax == prce->TrxCommitted(); prce = prce->PrcePrevOfNode() )
		{
		ASSERT_VALID( prce );
		if ( ( fIgnorePIB || prce->Pfucb()->ppib == ppib )
			&& prce->FOperReplace()
			&& !prce->FRolledBack() )
			{
			const VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
			cbNodeReserve += pverreplace->cbDelta;
			}
		}

	// The deltas should always net out to a non-negtative value.
	Assert( cbNodeReserve >= 0 );
	Assert(	cbNodeReserve == 0  ||
			cbNodeReserve == CbVERIGetNodeMax( pfucb, bookmark, uiHash ) - (INT)cbCurrentData );

	return cbNodeReserve;
	}


//  ================================================================
BOOL FVERCheckUncommittedFreedSpace(
	const FUCB	* pfucb,
	CSR			* const pcsr,
	const INT	cbReq,
	const BOOL	fPermitUpdateUncFree )
//  ================================================================
//
// This function is called after it has been determined that cbFree will satisfy
// cbReq. We now check that cbReq doesn't use up any uncommitted freed space.
//
//-
	{
	BOOL	fEnoughPageSpace	= fTrue;

	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );
	Assert( cbReq <= pcsr->Cpage().CbFree() );

	//	during recovery we would normally set cbUncommitted free to 0
	//	but if we didn't redo anything on the page and are now rolling 
	//	back this may not be the case
	if( PinstFromPfucb( pfucb )->FRecovering() && fPermitUpdateUncFree )
		{
		if( pcsr->Cpage().CbUncommittedFree() != 0 )
			{
			LATCH	latchOld;

			//	do this until we succeed. recovery is single threaded so we
			//	should only conflict with the buffer manager
			while ( pcsr->ErrUpgradeToWARLatch( &latchOld ) != JET_errSuccess )
				{
				UtilSleep( cmsecWaitGeneric );
				}

			pcsr->Cpage().SetCbUncommittedFree( 0 );
			BFDirty( pcsr->Cpage().PBFLatch(), bfdfUntidy );
			pcsr->DowngradeFromWARLatch( latchOld );
			}

		return fTrue;
		}


	// We should already have performed the check against cbFree only (in other
	// words, this function is only called from within FNDFreePageSpace(),
	// or something that simulates its function).  This tells us that if all
	// currently-uncommitted transactions eventually commit, we should have
	// enough space to satisfy this request.
	Assert( cbReq <= pcsr->Cpage().CbFree() );

	// The amount of space freed but possibly uncommitted should be a subset of
	// the total amount of free space for this page.
	Assert( pcsr->Cpage().CbUncommittedFree() >= 0 );
	Assert( pcsr->Cpage().CbFree() >= pcsr->Cpage().CbUncommittedFree() );

	// In the worst case, all transactions that freed space on this page will
	// rollback, causing the space freed to be reclaimed.  If the space
	// required can be satisfied even in the worst case, then we're okay;
	// otherwise, we have to do more checking.
	if ( cbReq > pcsr->Cpage().CbFree() - pcsr->Cpage().CbUncommittedFree() ) 
		{
		Assert( !FFUCBSpace( pfucb ) );
		Assert( !pcsr->Cpage().FSpaceTree() );

		//	UNDONE:	use the CbNDUncommittedFree call 
		//			to get rglineinfo for later split
		//			this will reduce CPU usage for RCE hashing
		//
		const INT	cbUncommittedFree = CbNDUncommittedFree( pfucb, pcsr );
		Assert( cbUncommittedFree <= pcsr->Cpage().CbUncommittedFree() );
		Assert( cbUncommittedFree >= 0 );

		if ( cbUncommittedFree == pcsr->Cpage().CbUncommittedFree() )
			{
			//	cbUncommittedFreed in page is correct
			//	return
			//
			fEnoughPageSpace = fFalse;
			}
		else
			{
			if ( fPermitUpdateUncFree )
				{
				// Try updating cbUncommittedFreed, in case some freed space was committed.
				LATCH latchOld;
				if ( pcsr->ErrUpgradeToWARLatch( &latchOld ) == JET_errSuccess )
					{
					pcsr->Cpage().SetCbUncommittedFree( cbUncommittedFree );
					BFDirty( pcsr->Cpage().PBFLatch(), bfdfUntidy );
					pcsr->DowngradeFromWARLatch( latchOld );
					}
				}

			// The amount of space freed but possibly uncommitted should be a subset of
			// the total amount of free space for this page.
			Assert( pcsr->Cpage().CbUncommittedFree() >= 0 );
			Assert( pcsr->Cpage().CbFree() >= pcsr->Cpage().CbUncommittedFree() );

			fEnoughPageSpace = ( cbReq <= ( pcsr->Cpage().CbFree() - cbUncommittedFree ) );
			}
		}

	return fEnoughPageSpace;
	}



//  ****************************************************************
//  RCE CLEANUP
//  ****************************************************************


//  ================================================================
LOCAL ERR VER::ErrVERIPossiblyDeleteLV( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( operDelta == prce->Oper() );

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );
	Assert( pverdelta->fDeferredDelete );
	Assert( !prce->Pfcb()->FDeleteCommitted() );
	Assert( !m_pinst->m_plog->m_fRecovering );
	Assert( !prce->Pfcb()->Ptdb() );	//  LV trees don't have TDB's

	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
	DELETELVTASK * const ptask = new DELETELVTASK( prce->PgnoFDP(), prce->Pfcb(), prce->Ifmp(), bookmark );
	if( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	if( m_fSyncronousTasks || rgfmp[prce->Ifmp()].FDetachingDB() )
		{
		TASK::Dispatch( m_ppibRCEClean, (ULONG_PTR)ptask );
		}
	else
		{
		err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
		if( err < JET_errSuccess )
			{
			//  The task was not enqued sucessfully.
			delete ptask;
			}
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR VER::ErrVERIPossiblyFinalize( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( operDelta == prce->Oper() );
	Assert( trxMax != prce->TrxCommitted() );
	Assert( TrxCmp( prce->TrxCommitted(), TrxOldest( m_pinst ) ) < 0 );

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );
	const IFMP ifmp = prce->Ifmp();
		
	Assert( pverdelta->fFinalize );
	Assert( !prce->Pfcb()->FDeleteCommitted() );
	Assert( !m_pinst->m_plog->m_fRecovering );
	Assert( prce->Pfcb()->Ptdb() );

	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
	FINALIZETASK * const ptask = new FINALIZETASK(
										prce->PgnoFDP(),
										prce->Pfcb(),
										prce->Ifmp(),
										bookmark,
										pverdelta->cbOffset );
	if( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	if( m_fSyncronousTasks || rgfmp[prce->Ifmp()].FDetachingDB() )
		{
		TASK::Dispatch( m_ppibRCECleanCallback, (ULONG_PTR)ptask );
		}
	else
		{
		err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
		if( err < JET_errSuccess )
			{
			//  The task was not enqued sucessfully.
			delete ptask;
			}
		}
		
	return err;
	}


//  ================================================================
LOCAL BOOL RCE::ErrGetCursorForDelete( PIB *ppib, FUCB **ppfucb, BOOKMARK * pbookmark ) const
//  ================================================================
	{
	ERR		err	= JET_errSuccess;
	Assert( PverFromPpib( ppib )->m_critRCEClean.FOwner() );

	*ppfucb = pfucbNil;

	//	since we have m_critRCEClean, we're guaranteed that the RCE will not go away,
	//	though it may get nullified (eg. VERNullifyInactiveVersionsOnBM().  For this reason,
	//	we cannot access members using methods, or else FAssertReadable() asserts
	//	may go off.
	const UINT				uiHash		= m_uiHash;
	ENTERCRITICALSECTION	enterCritRCEChain( &( PverFromIfmp( Ifmp() )->CritRCEChain( uiHash ) ) );

	if ( !FOperNull() )
		{
		Assert( operFlagDelete == Oper() );
	
		//	no cleanup for temporary tables or tables/indexes scheduled for deletion
		Assert( dbidTemp != rgfmp[ Ifmp() ].Dbid() );
		if ( !Pfcb()->FDeletePending() )
			{
			CallR( ErrBTOpen( ppib, Pfcb(), ppfucb ) );
			Assert( pfucbNil != *ppfucb );
			
			GetBookmark( pbookmark );
			}
		}


	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIDelete( PIB *ppib, const RCE * const prce )
//  ================================================================
	{
	ERR			err;
	FUCB		*pfucbT;
	BOOKMARK	bookmark;
	BOOL		fInTrx		= fFalse;
	
	Assert( ppibNil != ppib );
	Assert( 0 == ppib->level );

	//	since we have m_critRCEClean, we're guaranteed that the RCE will not go away,
	//	though it may get nullified (eg. VERNullifyInactiveVersionsOnBM()
	CallR( prce->ErrGetCursorForDelete( ppib, &pfucbT, &bookmark ) );

	//	a NULL cursor is returned if there is no cleanup to be done
	if ( pfucbNil == pfucbT )
		return JET_errSuccess;
		
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	err = ErrBTDelete( pfucbT, bookmark );
	switch( err )
		{
		case JET_errRecordNotDeleted:
		case JET_errNoCurrentRecord:
			err = JET_errSuccess;
			break;
		default:
			Call( err );
			break;
		}

	Assert( ppib == (PverFromPpib( ppib ))->m_ppibRCEClean );
	Call( ErrDIRCommitTransaction( ppib, 0 ) );
	fInTrx = fFalse;

HandleError:
	if ( fInTrx )
		{
		Assert( err < 0 );
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	BTClose( pfucbT );
	return err;
	}


//  ================================================================
LOCAL VOID VERIFreeExt( PIB * const ppib, FCB *pfcb, PGNO pgnoFirst, CPG cpg )
//  ================================================================
//	Throw away any errors encountered -- at worst, we just lose space.
	{
	Assert( pfcb );

	ERR     err;
	FUCB    *pfucb = pfucbNil;

	Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
	Assert( ppib->level > 0 );

	err = ErrDBOpenDatabaseByIfmp( ppib, pfcb->Ifmp() );
	if ( err < 0 )
		return;

	// Can't call DIROpen() because this function gets called
	// only during rollback AFTER the logic in DIRRollback()
	// which properly resets the navigation level of this
	// session's cursors.  Thus, when we close the cursor
	// below, the navigation level will not get properly reset.
	// Call( ErrDIROpen( ppib, pfcb, &pfucb ) );
	Call( ErrBTOpen( ppib, pfcb, &pfucb ) );

	Assert( !FFUCBSpace( pfucb ) );
	(VOID)ErrSPFreeExt( pfucb, pgnoFirst, cpg );

HandleError:
	if ( pfucbNil != pfucb )
		{
		Assert( !FFUCBDeferClosed( pfucb ) );
		FUCBAssertNoSearchKey( pfucb );
		Assert( !FFUCBCurrentSecondary( pfucb ) );
		BTClose( pfucb );
		}

	(VOID)ErrDBCloseDatabase( ppib, pfcb->Ifmp(), NO_GRBIT );
	}


#ifdef DEBUG
//  ================================================================
BOOL FIsRCECleanup()
//  ================================================================
//
//  DEBUG:  is the current thread a RCECleanup thread?
	{
	return Ptls()->fIsRCECleanup;
	}

//  ================================================================
BOOL FInCritBucket( VER *pver )
//  ================================================================
//
//  DEBUG:  is the current thread in m_critBucketGlobal
	{
	return pver->m_critBucketGlobal.FOwner();
	}
#endif	//	DEBUG


//  ================================================================
BOOL FPIBSessionRCEClean( PIB *ppib )
//  ================================================================
//
// Is the given PIB the one used by RCE clean?
//
//-
	{
	Assert( ppibNil != ppib );
	return ( (PverFromPpib( ppib ))->m_ppibRCEClean == ppib );
	}


//  ================================================================
INLINE VOID VERIUnlinkDefunctSecondaryIndex(
	PIB	* const ppib,
	FCB	* const pfcb )
//  ================================================================
	{
	Assert( pfcb->FTypeSecondaryIndex() );
	
	// Must unlink defunct FCB from all deferred-closed cursors.
	// The cursors themselves will be closed when the
	// owning session commits or rolls back.
	pfcb->Lock();
	
	while ( pfcb->Pfucb() != pfucbNil )
		{
		FUCB * const	pfucbT = pfcb->Pfucb();
		PIB	* const		ppibT = pfucbT->ppib;

		pfcb->Unlock();
		
		Assert( ppibNil != ppibT );
		if ( ppib == ppibT )
			{
			FCBUnlink( pfucbT );

			BTReleaseBM( pfucbT );
			// If cursor belongs to us, we can close
			// it right now.
			FUCBClose( pfucbT );
			}
		else
			{
			ppibT->critTrx.Enter();

			//	if undoing CreateIndex, we know other session must be
			//	in a transaction if it has a link to this FCB because
			//	the index is not visible yet.

			// FCB may have gotten unlinked if other session
			// committed or rolled back while we were switching
			// critical sections.
			if ( pfcb->Pfucb() == pfucbT )
				FCBUnlink( pfucbT );
					
			ppibT->critTrx.Leave();
			}
			
		pfcb->Lock();
		}
		
	pfcb->Unlock();
	}


//  ================================================================
INLINE VOID VERIUnlinkDefunctLV(
	PIB	* const ppib,
	FCB	* const pfcb )
//  ================================================================
//
//  If we are rolling back, only the owning session could have seen
//  the LV tree, because
//		-- the table was opened exclusively and the session that opened
//		the table created the LV tree
//		-- ppibLV created the LV tree and is rolling back before returning
//
//-
	{
	Assert( pfcb->FTypeLV() );
	
	pfcb->Lock();
	
	while ( pfcb->Pfucb() != pfucbNil )
		{
		FUCB * const	pfucbT = pfcb->Pfucb();
		PIB	* const		ppibT = pfucbT->ppib;

		pfcb->Unlock();
		
		Assert( ppib == ppibT );
		FCBUnlink( pfucbT );
		FUCBClose( pfucbT );
		
		pfcb->Lock();
		}
		
	pfcb->Unlock();
	}


//  ================================================================
ERR VER::ErrVERICleanDeltaRCE( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );
	if ( pverdelta->fDeferredDelete && !prce->Pfcb()->FDeleteCommitted() )
		{
		err = ErrVERIPossiblyDeleteLV( prce );
		}
	else if ( pverdelta->fFinalize && !prce->Pfcb()->FDeleteCommitted() )
		{
		err = ErrVERIPossiblyFinalize( prce );
		}
	return err;
	}


//  ================================================================
ERR VER::ErrVERICleanSLVSpaceRCE( const RCE * const prce )
//  ================================================================
	{
	ERR		err		= JET_errSuccess;
	TASK*	ptask	= NULL;

	//  this space was deleted and is now older than the oldest transaction

	const VERSLVSPACE* const pverslvspace = (VERSLVSPACE*)( prce->PbData() );
	if ( slvspaceoperCommittedToDeleted == pverslvspace->oper )
		{
		//  the SLV Provider is enabled
		
		if ( PinstFromIfmp( prce->Ifmp() )->FSLVProviderEnabled() )
			{
			//  register the space as deleted with the SLV Provider.  it will
			//  move the space to free when no SLV File handles are open on the
			//  space any longer

			BOOKMARK	bookmark;
			prce->GetBookmark( &bookmark );
			Assert( sizeof( PGNO ) == bookmark.key.Cb() );
			Assert( 0 == bookmark.data.Cb() );

			PGNO pgnoLastInExtent;
			LongFromKey( &pgnoLastInExtent, bookmark.key );
			Assert( pgnoLastInExtent != 0 );
			Assert( pgnoLastInExtent % cpgSLVExtent == 0 );
			PGNO pgnoFirst = ( pgnoLastInExtent - cpgSLVExtent + 1 ) + pverslvspace->ipage;
			QWORD ibLogical = OffsetOfPgno( pgnoFirst );
			QWORD cbSize = QWORD( pverslvspace->cpages ) * SLVPAGE_SIZE;
			
			ptask = new OSSLVDELETETASK(
							prce->Ifmp(),
							ibLogical,
							cbSize,
							CSLVInfo::FILEID( pverslvspace->fileid ),
							pverslvspace->cbAlloc,
							(const wchar_t*)pverslvspace->wszFileName );
			}

		//  the SLV Provider is not enabled
		
		else
			{
			//  move the space to free
			
			BOOKMARK	bookmark;
			prce->GetBookmark( &bookmark );
			Assert( sizeof( PGNO ) == bookmark.key.Cb() );
			Assert( 0 == bookmark.data.Cb() );
			ptask = new SLVSPACETASK(
							prce->PgnoFDP(),
							prce->Pfcb(),
							prce->Ifmp(),
							bookmark,
							slvspaceoperDeletedToFree,
							pverslvspace->ipage,
							pverslvspace->cpages );
			}

		//  execute the task

		if ( NULL == ptask )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
		if ( m_fSyncronousTasks || rgfmp[prce->Ifmp()].FDetachingDB() )
			{
			TASK::Dispatch( m_ppibRCEClean, (ULONG_PTR)ptask );
			}
		else
			{
			err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
			if ( err < JET_errSuccess )
				{
				//  The task was not enqued sucessfully.
				delete ptask;
				}
			}
		}

	return err;
	}


//  ================================================================
LOCAL VOID VERIRemoveCallback( const RCE * const prce )
//  ================================================================
//
//  Remove the callback from the list
//
//-
	{
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdescRemove = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	prce->Pfcb()->Ptdb()->UnregisterPcbdesc( pcbdescRemove );
	prce->Pfcb()->LeaveDDL();		
	delete pcbdescRemove;
	}


//  ================================================================
ERR VER::ErrVERICleanOneRCE( RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;
		
	Assert( m_critRCEClean.FOwner() );
	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( prce->TrxCommitted() != trxMax );
	Assert( !prce->FRolledBack() );
	
	switch( prce->Oper() )
		{
		case operCreateTable:
			// RCE list ensures FCB is still pinned
			Assert( pfcbNil != prce->Pfcb() );
			Assert( prce->Pfcb()->PrceOldest() != prceNil );
			if ( prce->Pfcb()->FTypeTable() )
				{
				if ( FCATHashActive( PinstFromIfmp( prce->Pfcb()->Ifmp() ) ) )
					{

					//	catalog hash is active so we need to insert this table
					
					CHAR szTable[JET_cbNameMost+1];

					//	read the table-name from the TDB
					
					prce->Pfcb()->EnterDML();
					strcpy( szTable, prce->Pfcb()->Ptdb()->SzTableName() );
					prce->Pfcb()->LeaveDML();

					//	insert the table into the catalog hash

					CATHashIInsert( prce->Pfcb(), szTable );
					}
				}
			else
				{
				Assert( prce->Pfcb()->FTypeTemporaryTable() );
				}
			break;
		
		case operAddColumn:
			{				
			// RCE list ensures FCB is still pinned
			Assert( prce->Pfcb()->PrceOldest() != prceNil );

			Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
			const JET_COLUMNID		columnid			= ( (VERADDCOLUMN*)prce->PbData() )->columnid;
			BYTE					* pbOldDefaultRec	= ( (VERADDCOLUMN*)prce->PbData() )->pbOldDefaultRec;
			FCB						* pfcbTable			= prce->Pfcb();

			pfcbTable->EnterDDL();

			TDB						* const ptdb		= pfcbTable->Ptdb();
			FIELD					* const pfield		= ptdb->Pfield( columnid );

			FIELDResetVersionedAdd( pfield->ffield );

			// Only reset the Versioned bit if a Delete
			// is not pending.
			if ( FFIELDVersioned( pfield->ffield ) && !FFIELDDeleted( pfield->ffield ) )
				{
				FIELDResetVersioned( pfield->ffield );
				}

			//	should be impossible for current default record to be same as old default record,
			//	but check anyways to be safe
			Assert( NULL == pbOldDefaultRec
				|| (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec );
			if ( NULL != pbOldDefaultRec
				&& (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec )
				{
				for ( RECDANGLING * precdangling = pfcbTable->Precdangling();
					;
					precdangling = precdangling->precdanglingNext )
					{
					if ( NULL == precdangling )
						{
						//	not in list, go ahead and free it
						OSMemoryHeapFree( pbOldDefaultRec );
						break;
						}
					else if ( (BYTE *)precdangling == pbOldDefaultRec )
						{
						//	pointer is already in the list, just get out
						AssertTracking();
						break;
						}
					}
				}

			pfcbTable->LeaveDDL();

			break;
			}
		
		case operDeleteColumn:
			{				
			// RCE list ensures FCB is still pinned
			Assert( prce->Pfcb()->PrceOldest() != prceNil );

			prce->Pfcb()->EnterDDL();

			Assert( prce->CbData() == sizeof(COLUMNID) );
			const COLUMNID	columnid		= *( (COLUMNID*)prce->PbData() );
			TDB				* const ptdb	= prce->Pfcb()->Ptdb();
			FIELD			* const pfield	= ptdb->Pfield( columnid );

			// If field was version-added, it would have been cleaned
			// up by now.
			Assert( pfield->coltyp != JET_coltypNil );

			// UNDONE: Don't reset coltyp to Nil, so that we can support
			// column access at level 0.
///					pfield->coltyp = JET_coltypNil;

			//	remove the column name from the TDB name space
			ptdb->MemPool().DeleteEntry( pfield->itagFieldName );
			
			// Reset version and autoinc fields.
			Assert( !( FFIELDVersion( pfield->ffield )
					 && FFIELDAutoincrement( pfield->ffield ) ) );
			if ( FFIELDVersion( pfield->ffield ) )
				{
				Assert( ptdb->FidVersion() == FidOfColumnid( columnid ) );
				ptdb->ResetFidVersion();
				}
			else if ( FFIELDAutoincrement( pfield->ffield ) )
				{
				Assert( ptdb->FidAutoincrement() == FidOfColumnid( columnid ) );
				ptdb->ResetFidAutoincrement();
				}
			
			Assert( !FFIELDVersionedAdd( pfield->ffield ) );
			Assert( FFIELDDeleted( pfield->ffield ) );
			FIELDResetVersioned( pfield->ffield );

			prce->Pfcb()->LeaveDDL();

			break;
			}
		
		case operCreateIndex:
			{
			//	pfcb of secondary index FCB or pfcbNil for primary
			//	index creation
			FCB						* const pfcbT = *(FCB **)prce->PbData();
			FCB						* const pfcbTable = prce->Pfcb();
			FCB						* const pfcbIndex = ( pfcbT == pfcbNil ? pfcbTable : pfcbT );
			IDB						* const pidb = pfcbIndex->Pidb();

			pfcbTable->EnterDDL();

			Assert( pidbNil != pidb );

			Assert( pfcbTable->FTypeTable() );

			if ( pfcbTable == pfcbIndex )
				{
				// VersionedCreate flag is reset at commit time for primary index.
				Assert( !pidb->FVersionedCreate() );
				Assert( !pidb->FDeleted() );
				pidb->ResetFVersioned();
				}
			else if ( pidb->FVersionedCreate() )
				{
				pidb->ResetFVersionedCreate();
				
				// If deleted, Versioned bit will be properly reset when
				// Delete commits or rolls back.
				if ( !pidb->FDeleted() )
					{
					pidb->ResetFVersioned();
					}
				}

			pfcbTable->LeaveDDL();
			
			break;
			}
			
		case operDeleteIndex:
			{
			FCB	* const pfcbIndex	= (*(FCB **)prce->PbData());
			FCB	* const pfcbTable	= prce->Pfcb();

			pfcbTable->SetIndexing();
			pfcbTable->EnterDDL();

			Assert( pfcbTable->FTypeTable() );
			Assert( pfcbIndex->FDeletePending() );
			Assert( pfcbIndex->FDeleteCommitted() );
			Assert( pfcbIndex->FTypeSecondaryIndex() );
			Assert( pfcbIndex != pfcbTable );
			Assert( pfcbIndex->PfcbTable() == pfcbTable );

			// Use dummy ppib because we lost the original one when the
			// transaction committed.
			Assert( pfcbIndex->FDomainDenyRead( m_ppibRCEClean ) );
			
			Assert( pfcbIndex->Pidb() != pidbNil );
			Assert( pfcbIndex->Pidb()->CrefCurrentIndex() == 0 );
			Assert( pfcbIndex->Pidb()->FDeleted() );
			Assert( !pfcbIndex->Pidb()->FVersioned() );

			pfcbTable->UnlinkSecondaryIndex( pfcbIndex );

			pfcbTable->LeaveDDL();
			pfcbTable->ResetIndexing();

			if ( pfcbIndex >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
				{

				//	the index FCB is above the threshold; thus, removing it may
				//	cause the table FCB to move below the threshold; if the
				//	table FCB is in the avail-above list, it must be moved
				//	to the avail-below list

				pfcbTable->UpdateAvailListPosition();
				}

			//	verify not called during recovery, which would be
			//	bad because VERNullifyAllVersionsOnFCB() enters
			//	m_critRCEClean, which we already have
			Assert( !PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );
			VERNullifyAllVersionsOnFCB( pfcbIndex );
			VERIUnlinkDefunctSecondaryIndex( ppibNil, pfcbIndex );

			//	prepare the FCB to be purged
			//	this removes the FCB from the hash-table among other things
			//		so that the following case cannot happen:
			//			we free the space for this FCB
			//			someone else allocates it
			//			someone else BTOpen's the space
			//			we try to purge the table and find that the refcnt
			//				is not zero and the state of the FCB says it is
			//				currently in use! 
			//			result --> CONCURRENCY HOLE

			pfcbIndex->PrepareForPurge( fFalse );

			//	if the parent (ie. the table) is pending deletion, we
			//	don't need to bother freeing the index space because
			//	it will be freed when the parent is freed
			//	Note that the DeleteCommitted flag is only ever set
			//	when the delete is guaranteed to be committed.  The
			//	flag NEVER gets reset, so there's no need to grab
			//	the FCB critical section to check it.
			
			if ( !pfcbTable->FDeleteCommitted() )
				{
				// Ignore lost space.
				(VOID)ErrSPFreeFDP(
						m_ppibRCEClean,
						pfcbIndex,
						pfcbTable->PgnoFDP() );
				}
				
			//	purge the FCB

			pfcbIndex->Purge();
			
			break;
			}
			
		case operDeleteTable:
			{
			INT			fState;
			const IFMP	ifmp				= prce->Ifmp();
			const PGNO	pgnoFDPTable		= *(PGNO*)prce->PbData();
			FCB			* const	pfcbTable	= FCB::PfcbFCBGet(
													ifmp,
													pgnoFDPTable,
													&fState );
			Assert( pfcbNil != pfcbTable );
			Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
			Assert( fFCBStateInitialized == fState
					|| fFCBStateSentinel == fState );

			//	verify VERNullifyAllVersionsOnFCB() not called during recovery,
			//	which would be bad because VERNullifyAllVersionsOnFCB() enters
			//	m_critRCEClean, which we already have
			Assert( !PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );

			// Remove all associated FCB's from hash table, so they will
			// be available for another file using the FDP that is about
			// about to be freed.
			for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
				{
				Assert( pfcbT->FDeletePending() );
				Assert( pfcbT->FDeleteCommitted() );

				//	wait for all tasks on the FCB to complete
				//	no new tasks should be created becayse there is a delete on the FCB

				CallS( pfcbT->ErrWaitForTasksToComplete() );

				// bugfix (#45382): May have outstanding moved RCE's
				Assert( pfcbT->PrceOldest() == prceNil
					|| ( pfcbT->PrceOldest()->Oper() == operFlagDelete
						&& pfcbT->PrceOldest()->FMoved() ) );
				VERNullifyAllVersionsOnFCB( pfcbT );

				pfcbT->PrepareForPurge( fFalse );
				}

			if ( pfcbTable->Ptdb() != ptdbNil )
				{
				Assert( fFCBStateInitialized == fState );
				FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
				if ( pfcbNil != pfcbLV )
					{
					Assert( pfcbLV->FDeletePending() );
					Assert( pfcbLV->FDeleteCommitted() );

					//	wait for all tasks on the FCB to complete
					//	no new tasks should be created becayse there is a delete on the FCB

					CallS( pfcbLV->ErrWaitForTasksToComplete() );

					// bugfix (#36315): processing of delta RCEs may have created flagDelete
					// RCEs after this RCE.
					Assert( pfcbLV->PrceOldest() == prceNil || pfcbLV->PrceOldest()->Oper() == operFlagDelete );
					VERNullifyAllVersionsOnFCB( pfcbLV );
					
					pfcbLV->PrepareForPurge( fFalse );
					}
				}
			else
				{
				Assert( fFCBStateSentinel == fState );
				}

			// Free table FDP (which implicitly frees child FDP's).
			// Ignore lost space.
			
			(VOID)ErrSPFreeFDP(
						m_ppibRCEClean,
						pfcbTable,
						pgnoSystemRoot );
						
			if ( fFCBStateInitialized == fState )
				{
				pfcbTable->Release();

				Assert( pfcbTable->PgnoFDP() == pgnoFDPTable );
				Assert( pfcbTable->FDeletePending() );
				Assert( pfcbTable->FDeleteCommitted() );
				
				// All transactions which were able to access this table
				// must have committed and been cleaned up by now.
				Assert( pfcbTable->PrceOldest() == prceNil );
				Assert( pfcbTable->PrceNewest() == prceNil );
				}
			else
				{
				Assert( fFCBStateSentinel == fState );
				}
			pfcbTable->Purge();

			break;
			}					

		case operRegisterCallback:
			{
			//  the callback is now visible to all transactions
			//  CONSIDER: unset the fVersioned flag if the callback has not been unregistered
			}
			break;

		case operUnregisterCallback:
			{
			//  the callback cannot be seen by any transaction. remove the callback from the list
			VERIRemoveCallback( prce );
			}
			break;
			
		case operFlagDelete:
			if ( FVERICleanWithoutIO()
				&& !rgfmp[prce->Ifmp()].FDetachingDB()		// if TRUE, it means we called RCE clean to detach one DB, so we MUST nullify all RCE's
				&& !prce->FMoved() )						//  if we have already moved it, try to delete
				{
				//  UNDONE: try to perform the delete without waiting for latches or IO
				
				err = ErrVERIMoveRCE( prce );
				CallSx( err, wrnVERRCEMoved );
				}

			else if ( FVERICleanDiscardDeletes() )
				{
#ifdef VERPERF
				m_crceDiscarded++;
#endif
				VERIReportDiscardedDeletes( prce );
				err = JET_errSuccess;
				}

			else if ( !PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering )
				{
#ifdef VERPERF
				if ( prce->FMoved() )
					{
					++m_crceMovedDeleted;
					}
#endif

				//	don't bother cleaning if there are future versions
				if ( !prce->FFutureVersionsOfNode() )
					{
					err = ErrVERIDelete( m_ppibRCEClean, prce );
					}
				}

			break;

		case operDelta:
			//  we may have to defer delete a LV
			if ( !PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering && !FVERICleanDiscardDeletes() )
				{
				err = ErrVERICleanDeltaRCE( prce );
				}
			break;

		case operSLVSpace:
			if ( !PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering )
				{
				err = ErrVERICleanSLVSpaceRCE( prce );
				}
			break;
			
		default:
			break;
		}
		
	return err;		
	}


//  ================================================================
ERR RCE::ErrPrepareToDeallocate( TRX trxOldest )
//  ================================================================
//
// Called by RCEClean to clean/nullify RCE before deallocation.
//
	{
	ERR			err		= JET_errSuccess;
	const OPER	oper 	= m_oper;
	const UINT	uiHash 	= m_uiHash;
	
	Assert( PinstFromIfmp( m_ifmp )->m_pver->m_critRCEClean.FOwner() );

#ifdef DEBUG
	const TRX	trxDBGOldest = TrxOldest( PinstFromIfmp( m_ifmp ) );
	Assert( TrxCommitted() != trxMax );
	Assert( TrxCmp( trxDBGOldest, trxOldest ) >= 0 || trxMax == trxOldest );
	Assert( dbidTemp != rgfmp[ m_ifmp ].Dbid() );
#endif

	VER *pver = PverFromIfmp( m_ifmp );
	CallR( pver->ErrVERICleanOneRCE( this ) );

	const BOOL	fInHash = ::FOperInHashTable( oper );
	ENTERCRITICALSECTION enterCritHash(
							fInHash ? &( PverFromIfmp( Ifmp() )->CritRCEChain( uiHash ) ) : NULL,
							fInHash );

	if ( FOperNull() || wrnVERRCEMoved == err )
		{
		//  RCE may have been nullified by VERNullifyAllVersionsOnFCB()
		//  (which is called while closing the temp table).
		//  or RCE may have been moved instead of being cleaned up
		}
	else
		{
		Assert( !FRolledBack() );
		Assert( !FOperNull() );

		//	Clean up the RCE. Also clean up any RCE of the same list to reduce
		//	Enter/leave critical section calls.

		RCE *prce = this;
		
		FCB * const pfcb = prce->Pfcb();
		ENTERCRITICALSECTION enterCritFCBRCEList( &( pfcb->CritRCEList() ) );

		do 
			{
			RCE *prceNext;
			if ( prce->FOperInHashTable() )
				prceNext = prce->PrceNextOfNode();
			else
				prceNext = prceNil;

			ASSERT_VALID( prce );
			Assert( !prce->FOperNull() );
			VERINullifyCommittedRCE( prce );

			prce = prceNext;
			} while (
				prce != prceNil &&
				TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 &&
				operFlagDelete != prce->Oper() &&		// Let RCE clean do the nullify for delete.
				operSLVSpace != prce->Oper() &&			// May need to move space from deleted to free
				operDelta != prce->Oper() );			// Delta is used to indicate if
														// it needs to do LV delete.
		}

	return JET_errSuccess;
	}


//  ================================================================
ERR VER::ErrVERRCEClean( const IFMP ifmp )
//  ================================================================
//	critical section wrapper
{
	//	clean PIB in critical section held across IO operations

	ENTERCRITICALSECTION	enterCritRCEClean( &m_critRCEClean );

	return ErrVERIRCEClean( ifmp );
}
//  ================================================================
ERR VER::ErrVERIRCEClean( const IFMP ifmp )
//  ================================================================
//
//	Cleans RCEs in bucket chain.
//	We only clean up the RCEs that has a commit timestamp older
//	that the oldest XactBegin of any user.
//	
//-
	{
	Assert( m_critRCEClean.FOwner() );

	const BOOL	fCleanOneDb = ( ifmp != ifmpMax );

	// Only time we do per-DB RCE Clean is just before we want to
	// detach the database.
	Assert( !fCleanOneDb || rgfmp[ifmp].FDetachingDB() );
	
	// keep the original value
	const BOOL fSyncronousTasks = m_fSyncronousTasks;

	//  override the default if we are cleaning one database
	m_fSyncronousTasks = fCleanOneDb ? fTrue : m_fSyncronousTasks;

#ifdef DEBUG
	Ptls()->fIsRCECleanup = fTrue;
#endif	//	DEBUG

#ifdef VERPERF
	//	UNDONE:	why do these get reset every RCEClean??
	m_cbucketCleaned	= 0;
	m_cbucketSeen		= 0;
	m_crceSeen		= 0;
	m_crceCleaned		= 0;
	qwStartTicks	= QwUtilHRTCount();
	m_crceFlagDelete	= 0;
	m_crceDeleteLV	= 0;
	m_crceMoved		= 0;
	m_crceDiscarded	= 0;
	m_crceMovedDeleted= 0;
#endif	//	VERPERF

	ERR     err = JET_errSuccess;

	//	get oldest bucket and clean RCEs from oldest to youngest

	m_critBucketGlobal.Enter();
	BUCKET * pbucket;
	if ( FVERICleanWithoutIO()
		&& !FVERICleanDiscardDeletes()
		&& pbucketNil != m_pbucketGlobalLastDelete
		&& !fCleanOneDb )
		{
		pbucket = m_pbucketGlobalLastDelete;
		}
	else
		{
		pbucket = PbucketVERIGetOldest( );
		}
	m_critBucketGlobal.Leave();

	TRX trxOldest = TrxOldest( m_pinst );
	//	loop through buckets, oldest to newest. stop when we run out of buckets
	//	or find an uncleanable RCE

	while ( pbucketNil != pbucket )
		{
#ifdef VERPERF
		INT	crceInBucketSeen 	= 0;
		INT	crceInBucketCleaned = 0;
#endif	//	VERPERF

		//	Check if need to get RCE within m_critBucketGlobal

		BOOL fNeedBeInCritBucketGlobal = ( pbucket->hdr.pbucketNext == pbucketNil );

		//	Only clean can change prceOldest and only one clean thread is active.

		Assert( m_critRCEClean.FOwner() );
		RCE * prce;
		BOOL	fSkippedRCEInBucket = fFalse;
		
		if ( pbucket->rgb != pbucket->hdr.pbLastDelete )
			{
			if ( !FVERICleanWithoutIO() || FVERICleanDiscardDeletes() || fCleanOneDb )
				{
				//  there are moved FlagDelete RCEs in this bucket. start with them
				//  we will either try to clean them or will discard them
				prce = reinterpret_cast<RCE *>( pbucket->rgb );
				}
			else
				{
				fSkippedRCEInBucket = fTrue;
				prce = pbucket->hdr.prceOldest;
				}
			}
		else
			{
			Assert( !fSkippedRCEInBucket );
			prce = pbucket->hdr.prceOldest;
			}

		forever
			{
#ifdef VERPERF
			++m_crceSeen;
			++crceInBucketSeen;
#endif

			//	verify RCE is within the bucket
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 )
				|| ( (BYTE *)prce == (BYTE *)( pbucket + 1 )
					&& prce == pbucket->hdr.prceNextNew ) );

			if ( fNeedBeInCritBucketGlobal )
				m_critBucketGlobal.Enter();

			if ( !fSkippedRCEInBucket
				&& reinterpret_cast<BYTE *>( prce ) >= pbucket->hdr.pbLastDelete )
				{
				pbucket->hdr.prceOldest = prce;
				}

			Assert( pbucket->rgb <= pbucket->hdr.pbLastDelete );
			Assert( pbucket->hdr.pbLastDelete <= reinterpret_cast<BYTE *>( pbucket->hdr.prceOldest ) );
			Assert( pbucket->hdr.prceOldest <= pbucket->hdr.prceNextNew );

			//	break to release the bucket
			if ( pbucket->hdr.prceNextNew == prce )
				break;

			if ( fNeedBeInCritBucketGlobal )
				m_critBucketGlobal.Leave();

			//	Save the size for use later
			const INT	cbRce = prce->CbRce();

			//	verify RCE is within the bucket
			Assert( cbRce > 0 );
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 ) );
			Assert( (BYTE *)prce + cbRce <= (BYTE *)( pbucket + 1 ) );

			if ( !prce->FOperNull() )
				{
#ifdef DEBUG
				const TRX	trxDBGOldest = TrxOldest( m_pinst );
#endif				
				const TRX	trxRCECommitted = prce->TrxCommitted();
				if ( trxMax == trxOldest && !m_pinst->m_plog->m_fRecovering )  // trxOldest is always trxMax when recovering
					{
					//  trxOldest may no longer be trxMax. if so we may not be able to 
					//  clean up this RCE after all. we retrieve the trxCommitted of the rce
					//  first to avoid a race condition
					trxOldest = TrxOldest( m_pinst );
					}
				
				if ( trxRCECommitted == trxMax
					|| TrxCmp( trxRCECommitted, trxOldest ) >= 0 )
					{
					if ( fCleanOneDb )
						{
						Assert( dbidTemp != rgfmp[ ifmp ].Dbid() );
						if ( prce->Ifmp() == ifmp )
							{
							if ( prce->TrxCommitted() == trxMax )
								{
								//	this can be caused by a task that is active
								//	stop here as we have cleaned up what we can
								err = ErrERRCheck( JET_wrnRemainingVersions );
								goto HandleError;
								}
							else
								{
								// Fall through and clean the RCE.
								}
							}
						else
							{
							// Skip uncleanable RCE's that don't belong to
							// the db we're trying to clean.
							fSkippedRCEInBucket = fTrue;
							goto NextRCE;
							}
						}
					else
						{
						Assert( pbucketNil != pbucket );
						Assert( !prce->FMoved() );
						err = ErrERRCheck( JET_wrnRemainingVersions );
						goto HandleError;
						}
					}

				Assert( prce->TrxCommitted() != trxMax );
				Assert( TrxCmp( prce->TrxCommitted(), trxDBGOldest ) < 0
						|| fCleanOneDb
						|| TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 );
				
#ifdef VERPERF
				if ( operFlagDelete == prce->Oper() )
					{
					++m_crceFlagDelete;
					}
				else if ( operDelta == prce->Oper() )
					{
					const VERDELTA* const pverdelta = reinterpret_cast<VERDELTA*>( prce->PbData() );
					if ( pverdelta->fDeferredDelete )
						{
						++m_crceDeleteLV;
						}
					}
#endif	//	DEBUG

				Call( prce->ErrPrepareToDeallocate( trxOldest ) );
				
#ifdef VERPERF
				++crceInBucketCleaned;
				++m_crceCleaned;
#endif	//	VERPERF
				}

			//	Set the oldest to next prce entry in the bucket
			//	Rce clean thread ( run within m_critRCEClean ) is
			//	the only one touch prceOldest
NextRCE:
			Assert( m_critRCEClean.FOwner() );


			const BYTE	*pbRce		= reinterpret_cast<BYTE *>( prce );
			const BYTE	*pbNextRce	= reinterpret_cast<BYTE *>( PvAlignForAllPlatforms( pbRce + cbRce ) );

#ifdef DEBUG
			//	verify we don't straddle pbLastDelete
			const BYTE	*pbLastDelete = reinterpret_cast<BYTE *>( pbucket->hdr.pbLastDelete );
			if ( pbRce < pbLastDelete )
				{
				Assert( pbNextRce <= pbLastDelete );
				}
			else
				{
				Assert( pbNextRce > pbLastDelete );
				}
#endif

			if ( pbNextRce != pbucket->hdr.pbLastDelete )
				{
				prce = (RCE *)pbNextRce;
				}
			else
				{
				//  we just looked at the last moved RCE. go to the first old RCE
				Assert( prce->FMoved() );
				prce = pbucket->hdr.prceOldest;
				--m_cbucketGlobalAllocDelete;
				if ( m_cbucketGlobalAllocDelete < 0 )
					{
					m_cbucketGlobalAllocDelete = 0;
					}
#ifdef VERPERF
				cVERcbucketDeleteAllocated.Dec( m_pinst );
#endif // VERPERF
				pbucket->hdr.pbLastDelete = pbucket->rgb;
				}
				
			//	verify RCE is within the bucket
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 )
				|| ( (BYTE *)prce == (BYTE *)( pbucket + 1 )
					&& prce == pbucket->hdr.prceNextNew ) );
			
			Assert( pbucket->hdr.prceOldest <= pbucket->hdr.prceNextNew );
			}

		//	all RCEs in bucket cleaned.  Now get next bucket and free
		//	cleaned bucket.

		if ( fNeedBeInCritBucketGlobal )
			Assert( m_critBucketGlobal.FOwner() );
		else
			m_critBucketGlobal.Enter();

#ifdef VERPERF
		++m_cbucketSeen;
#endif

		if ( fSkippedRCEInBucket || pbucket->rgb != pbucket->hdr.pbLastDelete )
			{
			pbucket = pbucket->hdr.pbucketNext;
			}
		else
			{
			Assert( pbucket->rgb == pbucket->hdr.pbLastDelete );
			pbucket = PbucketVERIFreeAndGetNextOldestBucket( pbucket );

#ifdef VERPERF
			++m_cbucketCleaned;
#endif
			}
			
		m_critBucketGlobal.Leave();
		}

	//	stop as soon as find RCE commit time younger than oldest
	//	transaction.  If bucket left then set ibOldestRCE and
	//	unlink back offset of last remaining RCE.
	//	If no error then set warning code if some buckets could
	//	not be cleaned.

	Assert( pbucketNil == pbucket );
	err = JET_errSuccess;
	
	// If only cleaning one db, we don't clean buckets because
	// there may be outstanding versions on other databases.
	if ( !fCleanOneDb )
		{
		m_critBucketGlobal.Enter();
		if ( pbucketNil != m_pbucketGlobalHead )
			{
			//	return warning if remaining versions
			Assert( pbucketNil != m_pbucketGlobalTail );
			err = ErrERRCheck( JET_wrnRemainingVersions );
			}
		else
			{
			Assert( pbucketNil == m_pbucketGlobalTail );
			}
		m_critBucketGlobal.Leave();
		}
	
	
HandleError:

#ifdef DEBUG_VER_EXPENSIVE
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		double	dblElapsedTime 	= DblUtilHRTElapsedTime( qwStartTicks );
		CHAR	szBuf[512];

		sprintf( szBuf, "RCEClean: "
						"elapsed time %10.10f seconds, "
						"saw %6.6d RCEs, "
						"( %6.6d flagDelete, "
						"%6.6d deleteLV ), "
						"cleaned %6.6d RCEs, "
						"moved %6.6d RCEs, "
						"discarded %6.6d RCEs, "
						"cleaned %6.6d previously moved RCEs, "
						"cleaned %4.4d buckets, "
						"%4.4d buckets left",
						dblElapsedTime,
						m_crceSeen,
						m_crceFlagDelete,
						m_crceDeleteLV,
						m_crceCleaned,
						m_crceMoved,
						m_crceDiscarded,
						m_crceMovedDeleted,
						m_cbucketCleaned,
						m_cbucketGlobalAlloc
					);
		(VOID)m_pinst->m_plog->ErrLGTrace( ppibNil, szBuf );
		}	
#endif	//	DEBUG_VER_EXPENSIVE
		
#ifdef DEBUG
	Ptls()->fIsRCECleanup = fFalse;
#endif	//	DEBUG

	// restore the original value
	m_fSyncronousTasks = fSyncronousTasks;

	return err;
	}


//  ================================================================
VOID VERICommitRegisterCallback( const RCE * const prce, const TRX trxCommit0 )
//  ================================================================
//
//  Set the trxRegisterCommit0 in the CBDESC
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax == pcbdesc->trxRegisterCommit0 );
	Assert( trxMax == pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxRegisterCommit0 = trxCommit0;
	prce->Pfcb()->LeaveDDL();
#endif	//	VERSIONED_CALLBACKS
	}


//  ================================================================
VOID VERICommitUnregisterCallback( const RCE * const prce, const TRX trxCommit0 )
//  ================================================================
//
//  Set the trxUnregisterCommit0 in the CBDESC
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax != pcbdesc->trxRegisterCommit0 );
	Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxUnregisterCommit0 = trxCommit0;
	prce->Pfcb()->LeaveDDL();
#endif	//	VERSIONED_CALLBACKS
	}


//  ================================================================
LOCAL VOID VERICommitTransactionToLevelGreaterThan0( const PIB * const ppib )
//  ================================================================
	{
	const LEVEL	level 	= ppib->level;
	Assert( level > 1 );

	//  we do not need to lock the RCEs as other transactions do not care about the level,
	//  only that they are uncommitted
	
	RCE 		*prce	= ppib->prceNewest;
	for ( ; prceNil != prce && prce->Level() == level; prce = prce->PrcePrevOfSession() )
		{
		Assert( prce->TrxCommitted() == trxMax );

		prce->SetLevel( LEVEL( level - 1 ) );
		}
	}


RCE * PIB::PrceOldest()
	{
	RCE		* prcePrev	= prceNil;

	for ( RCE * prceCurr = prceNewest;
		prceNil != prceCurr;
		prceCurr = prceCurr->PrcePrevOfSession() )
		{
		prcePrev = prceCurr;
		}

	return prcePrev;
	}

INLINE VOID VERICommitOneRCEToLevel0( PIB * const ppib, RCE * const prce )
	{
	Assert( prce->TrxCommitted() == trxMax );

	prce->SetLevel( 0 );

	VERIDeleteRCEFromSessionList( ppib, prce );
		
	prce->Pfcb()->CritRCEList().Enter();
	prce->SetTrxCommitted( ppib->trxCommit0 );
	prce->Pfcb()->CritRCEList().Leave();
	}

LOCAL VOID VERIReleaseWaitLockOnCommitToLevel0( PIB * const ppib )
	{
	RCE		* const prce	= ppib->prceNewest;
	Assert( ppib->FSessionOLDSLV() );
	Assert( 1 == ppib->level );

	Assert( prceNil != prce );
	Assert( operWaitLock == prce->Oper() );
	Assert( prceNil == prce->PrcePrevOfSession() );
	Assert( prceNil == prce->PrceNextOfSession() );
	Assert( pgnoNull == prce->PgnoUndoInfo() );

	Assert( prce->FOperInHashTable() );
	ENTERCRITICALSECTION enterCritHash( &( PverFromPpib( ppib )->CritRCEChain( prce->UiHash() ) ) );
	VERICommitOneRCEToLevel0( ppib, prce );
	Assert( prceNil == ppib->prceNewest );

	if ( operWaitLock == prce->Oper() )
		{
		VERWAITLOCK * const pverwaitlock = (VERWAITLOCK *)prce->PbData();
		Assert( PvAlignForAllPlatforms( pverwaitlock ) );
		pverwaitlock->signal.Set();
		}
	else
		{
		Assert( fFalse );
		}
	}

//  ================================================================
LOCAL VOID VERICommitTransactionToLevel0( PIB * const ppib )
//  ================================================================
	{
	//	BUG #143376: must commit from oldest to newest for OLDSLV otherwise
	//	there's a possibility we can see some of OLDSLV's RCE's committed
	//	and some not (normally it doesn't matter what order we commit the
	//	RCE's, but OLDSLV's special fDoesNotWriteConflict RCE's causes
	//	problems if we don't commit from oldest to newest)
	const BOOL	fOLDSLV				= ppib->FSessionOLDSLV();
	RCE 		* prce				= ( fOLDSLV ? ppib->PrceOldest() : ppib->prceNewest );

	Assert( 1 == ppib->level );

	//  because of some optimizations at the DIR/LOG level we cannot always assert this
///	Assert( TrxCmp( ppib->trxCommit0, ppib->trxBegin0 ) > 0 );

	RCE * prceNextToCommit;
#ifdef DEBUG
	prceNextToCommit = prceInvalid;
#endif	//	DEBUG
	for ( ; prceNil != prce; prce = prceNextToCommit )
		{
		prceNextToCommit 	= ( fOLDSLV ? prce->PrceNextOfSession() : prce->PrcePrevOfSession() );
		
		ASSERT_VALID( prce );
		Assert( !prce->FOperNull() );
		Assert( prce->TrxCommitted() == trxMax );
		Assert( 1 == prce->Level() || PinstFromPpib( ppib )->m_plog->m_fRecovering );
		Assert( prce->Pfucb()->ppib == ppib );

		Assert( prceInvalid != prce );

		if ( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp )
			{
			//  RCEs for temp tables are used only for rollback.
			//   - The table is not shared so there is no risk of write conflicts
			//   - The table is not recovered so no undo-info is needed
			//   - No committed RCEs exist on temp tables so RCEClean will never access the FCB
			//   - No concurrent-create-index on temp tables
			//  Thus, we simply nullify the RCE so that the FCB can be freed
			//
			//  UNDONE: RCEs for temp tables should not be inserted into the FCB list or hash table

			// DDL currently not supported on temp tables.
			Assert( operWaitLock != prce->Oper() );
			Assert( !prce->FOperNull() );
			Assert( !prce->FOperDDL() || operCreateTable == prce->Oper() );
			Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
			Assert( prce->TrxCommitted() == trxMax );
			Assert( prce->PgnoUndoInfo() == pgnoNull );	//	no logging on temp tables

			const BOOL fInHash = prce->FOperInHashTable();

			ENTERCRITICALSECTION enterCritHash( fInHash ? &( PverFromPpib( ppib )->CritRCEChain( prce->UiHash() ) ) : 0, fInHash );
			ENTERCRITICALSECTION enterCritFCBRCEList( &prce->Pfcb()->CritRCEList() );

			VERIDeleteRCEFromSessionList( ppib, prce );

			prce->SetLevel( 0 );
			prce->SetTrxCommitted( ppib->trxCommit0 );

			VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );	
			if ( fInHash )
				{
				VERIDeleteRCEFromHash( prce );
				}
			prce->NullifyOper();
			
			continue;
			}
		else if ( fOLDSLV && operWaitLock == prce->Oper() )
			{
			//	must defer releasing all waiters until this is the last RCE committed
			Assert( pgnoNull == prce->PgnoUndoInfo() );
			continue;
			}

		Assert( operWaitLock != prce->Oper() );

		//	Remove UndoInfo dependency if committing to level 0

		if ( prce->PgnoUndoInfo() != pgnoNull )
			BFRemoveUndoInfo( prce );

		ENTERCRITICALSECTION enterCritHash(
			prce->FOperInHashTable() ? &( PverFromPpib( ppib )->CritRCEChain( prce->UiHash() ) ) : 0,
			prce->FOperInHashTable()
			);
			
		//	if version for DDL operation then reset deny DDL
		//	and perform special handling
		if ( prce->FOperDDL() )
			{
			switch( prce->Oper() )
				{
				case operAddColumn:
					// RCE list ensures FCB is still pinned
					Assert( prce->Pfcb()->PrceOldest() != prceNil );
					Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
					break;

				case operDeleteColumn:
					// RCE list ensures FCB is still pinned
					Assert( prce->Pfcb()->PrceOldest() != prceNil );
					Assert( prce->CbData() == sizeof(COLUMNID) );
					break;

				case operCreateIndex:
					{
					const FCB	* const pfcbT		= *(FCB **)prce->PbData();
					if ( pfcbNil == pfcbT )
						{
						FCB		* const pfcbTable	= prce->Pfcb();
						
						pfcbTable->EnterDDL();
						Assert( pfcbTable->FPrimaryIndex() );
						Assert( pfcbTable->FTypeTable() );
						Assert( pfcbTable->Pidb() != pidbNil );
						Assert( pfcbTable->Pidb()->FPrimary() );
						
						// For primary index, must reset VersionedCreate()
						// flag at commit time so updates can occur
						// immediately once the primary index has been
						// committed (see ErrSetUpdatingAndEnterDML()).
						pfcbTable->Pidb()->ResetFVersionedCreate();

						pfcbTable->LeaveDDL();
						}
					else
						{
						Assert( pfcbT->FTypeSecondaryIndex() );
						Assert( pfcbT->Pidb() != pidbNil );
						Assert( !pfcbT->Pidb()->FPrimary() );
						Assert( pfcbT->PfcbTable() != pfcbNil );
						}
					break;
					}

				case operCreateLV:
					{
					//  no further action is required
					break;
					}

				case operDeleteIndex:
					{
					FCB * const pfcbIndex = (*(FCB **)prce->PbData());
					FCB * const pfcbTable = prce->Pfcb();
					
					Assert( pfcbTable->FTypeTable() );
					Assert( pfcbIndex->FDeletePending() );
					Assert( !pfcbIndex->FDeleteCommitted() );
					Assert( pfcbIndex->FTypeSecondaryIndex() );
					Assert( pfcbIndex != pfcbTable );
					Assert( pfcbIndex->PfcbTable() == pfcbTable );
					Assert( pfcbIndex->FDomainDenyReadByUs( prce->Pfucb()->ppib ) );
					
					pfcbTable->EnterDDL();
				
					//  free in-memory structure

					Assert( pfcbIndex->Pidb() != pidbNil );
					Assert( pfcbIndex->Pidb()->CrefCurrentIndex() == 0 );
					Assert( pfcbIndex->Pidb()->FDeleted() );
					pfcbIndex->Pidb()->ResetFVersioned();
					pfcbIndex->SetDeleteCommitted();
					
					//	update all index mask
					FILESetAllIndexMask( pfcbTable );

					pfcbTable->LeaveDDL();
					break;
					}

				case operDeleteTable:
					{
					FCB		*pfcbTable;
					INT		fState;

					//	pfcb should be found, even if it's a sentinel
					pfcbTable = FCB::PfcbFCBGet( prce->Ifmp(), *(PGNO*)prce->PbData(), &fState );
					Assert( pfcbTable != pfcbNil );
					Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
					Assert( fFCBStateInitialized == fState
						|| fFCBStateSentinel == fState );

					if ( pfcbTable->Ptdb() != ptdbNil )
						{
						Assert( fFCBStateInitialized == fState );

						pfcbTable->EnterDDL();

						// Nothing left to prevent access to this FCB except
						// for DeletePending.
						Assert( !pfcbTable->FDeleteCommitted() );
						for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
							{
							Assert( pfcbT->FDeletePending() );
							pfcbT->SetDeleteCommitted();
							}

						FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
						if ( pfcbNil != pfcbLV )
							{
							Assert( pfcbLV->FDeletePending() );
							pfcbLV->SetDeleteCommitted();
							}

						pfcbTable->LeaveDDL();
						
						// If regular FCB, decrement refcnt							
						pfcbTable->Release();
						}
					else
						{
						Assert( fFCBStateSentinel == fState );
						Assert( pfcbTable->PfcbNextIndex() == pfcbNil );
						pfcbTable->SetDeleteCommitted();
						}
					
					break;
					}
					
				case operCreateTable:
					break;

				case operRegisterCallback:
					VERICommitRegisterCallback( prce, ppib->trxCommit0 );
					break;

				case operUnregisterCallback:
					VERICommitUnregisterCallback( prce, ppib->trxCommit0 );
					break;
					
				default:
					Assert( fFalse );
					break;
				}
			}
#ifdef DEBUG
		else
			{
			//	the deferred before image chain of the prce should have been
			//	cleaned up in the beginning of this while block
			const PGNO	pgnoUndoInfo = prce->PgnoUndoInfo();
			Assert( pgnoNull == pgnoUndoInfo );
			}
#endif	//	DEBUG

		//  set level and trxCommitted
		VERICommitOneRCEToLevel0( ppib, prce );
		}	//	WHILE 


	Assert( prceNil == ppib->prceNewest || fOLDSLV );
	if ( fOLDSLV && prceNil != ppib->prceNewest )
		{
		VERIReleaseWaitLockOnCommitToLevel0( ppib );
		}

	//	UNDONE: prceNewest should already be prceNil
	Assert( prceNil == ppib->prceNewest );
	PIBSetPrceNewest( ppib, prceNil );
	}


//  ================================================================
VOID VERCommitTransaction( PIB * const ppib )
//  ================================================================
//
//  OPTIMIZATION:	combine delta/readLock/replace versions
//					remove redundant replace versions
//
//-
	{
	ASSERT_VALID( ppib );

	const LEVEL				level = ppib->level;

	//	must be in a transaction in order to commit
	Assert( level > 0 );
	Assert( PinstFromPpib( ppib )->m_plog->m_fRecovering || trxMax != TrxOldest( PinstFromPpib( ppib ) ) );
	Assert( PinstFromPpib( ppib )->m_plog->m_fRecovering || TrxCmp( ppib->trxBegin0, TrxOldest( PinstFromPpib( ppib ) ) ) >= 0 );

	//	handle commit to intermediate transaction level and
	//	commit to transaction level 0 differently.
	if ( level > 1 )
		{
		VERICommitTransactionToLevelGreaterThan0( ppib );
		}
	else
		{
		VERICommitTransactionToLevel0( ppib );
		}

	Assert( ppib->level > 0 );
	--ppib->level;
	}


//  ================================================================
LOCAL ERR ErrVERILogUndoInfo( RCE *prce, CSR* pcsr )
//  ================================================================
//
//	log undo information [if not in redo phase]
//	remove rce from before-image chain
//
	{
	ERR		err				= JET_errSuccess;
	LGPOS	lgpos			= lgposMin;
	LOG		* const plog	= PinstFromIfmp( prce->Ifmp() )->m_plog;
		
	Assert( pcsr->FLatched() );

	if ( plog->m_fRecoveringMode != fRecoveringRedo )
		{
		CallR( ErrLGUndoInfo( prce, &lgpos ) );
		}

	//	remove RCE from deferred BI chain
	//
	BFRemoveUndoInfo( prce, lgpos );

	return err;
	}



//  ================================================================
ERR ErrVERIUndoReplacePhysical( RCE * const prce, CSR *pcsr, const BOOKMARK& bm )
//  ================================================================
	{
	ERR		err;
	DATA  	data;
	FUCB	* const pfucb	= prce->Pfucb();
	LOG		*plog = PinstFromIfmp( prce->Ifmp() )->m_plog;
	
	if ( prce->PgnoUndoInfo() != pgnoNull )
		{
		CallR( ErrVERILogUndoInfo( prce, pcsr ) );
		}

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

 	//	replace should not fail since splits are avoided at undo
	//	time via deferred page space release.  This refers to space
	//	within a page and not pages freed when indexes and tables
	//	are deleted
	//
	Assert( prce->FOperReplace() );

	data.SetPv( prce->PbData() + cbReplaceRCEOverhead );
	data.SetCb( prce->CbData() - cbReplaceRCEOverhead );

	const VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
	const INT cbDelta = pverreplace->cbDelta;

	//  if we are recovering we don't need to track the cbUncommitted free
	//  (the version store will be empty at the end of recovery)
	if ( cbDelta > 0 )
		{
		//	Rolling back replace that shrunk the node.  To satisfy the rollback,
		//	we will consume the reserved node space, but first we must remove this
		//	reserved node space from the uncommitted freed count so that BTReplace()
		//	can see it.
		//	(This complements the call to AddUncommittedFreed() in SetCbAdjust()).
		pcsr->Cpage().ReclaimUncommittedFreed( cbDelta );
		}

	//  ND expects that fucb will contain bookmark of replaced node
	//
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering )
		{
		pfucb->bmCurr = bm;
		}
	else
		{
		Assert( CmpKeyData( pfucb->bmCurr, bm ) == 0 );
		}

	CallS( ErrNDReplace	( pfucb, pcsr, &data, fDIRUndo, rceidNull, prceNil ) );

	if ( cbDelta < 0 )
		{
		// Rolling back a replace that grew the node.  Add to uncommitted freed
		// count the amount of reserved node space, if any, that we must restore.
		// (This complements the call to ReclaimUncommittedFreed in SetCbAdjust()).
		pcsr->Cpage().AddUncommittedFreed( -cbDelta );
		}

	return err;
	}


//  ================================================================
ERR ErrVERIUndoInsertPhysical( RCE * const prce, CSR *pcsr )
//  ================================================================
//
//	set delete bit in node header and let RCE clean up
//	remove the node later
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();
	
	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	CallS( ErrNDFlagDelete( pfucb, pcsr, fDIRUndo, rceidNull, NULL ) );
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIUndoFlagDeletePhysical( RCE * prce, CSR *pcsr )
//  ================================================================
//
//	reset delete bit
//
//-
	{
	ERR		err;
#ifdef DEBUG
	FUCB	* const pfucb 	= prce->Pfucb();
#endif	//	DEBUG
	
	if ( prce->PgnoUndoInfo() != pgnoNull )
		{
		CallR( ErrVERILogUndoInfo( prce, pcsr ) );
		}
		
	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	NDResetFlagDelete( pcsr );
	return err;
	}


//  ================================================================
ERR ErrVERIUndoDeltaPhysical( RCE * const prce, CSR	*pcsr )
//  ================================================================
//
//	undo delta change. modifies the RCE by setting the lDelta to 0
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();
	LOG		*plog = PinstFromIfmp( prce->Ifmp() )->m_plog;
	
	VERDELTA* const 	pverdelta 	= (VERDELTA*)prce->PbData();
	const LONG 			lDelta 		= -pverdelta->lDelta;

	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//  NDDelta is dependant on the data that it is operating on. for this reason we use
	//  NDGet to get the real data from the database (DIRGet will get the versioned copy)
///	AssertNDGet( pfucb, pcsr );
///	NDGet( pfucb, pcsr );
	
	if ( pverdelta->lDelta < 0 && !plog->m_fRecovering )
		{
		ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );

		//  we are rolling back a decrement. we need to remove all the deferredDelete flags		
		RCE * prceT = prce;
		for ( ; prceNil != prceT->PrceNextOfNode(); prceT = prceT->PrceNextOfNode() )
			;
		for ( ; prceNil != prceT; prceT = prceT->PrcePrevOfNode() )
			{
			VERDELTA* const pverdeltaT = ( VERDELTA* )prceT->PbData();
			if ( operDelta == prceT->Oper() && pverdelta->cbOffset == pverdeltaT->cbOffset )
				{
				pverdeltaT->fDeferredDelete = fFalse;
				pverdeltaT->fFinalize = fFalse;
				}
			}
		}

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );


	LONG lPrev;
	CallS( ErrNDDelta(
			pfucb,
			pcsr,
			pverdelta->cbOffset,
			&lDelta,
			sizeof( lDelta ),
			&lPrev,
			sizeof( lPrev ),
			NULL,
			fDIRUndo,
			rceidNull ) );
	if ( 0 == ( lPrev + lDelta ) )
		{
		//  by undoing an increment delta we have reduced the refcount of a LV to zero
		//  UNDONE:  morph the RCE into a committed level 0 decrement with deferred delete
		//         	 the RCE must be removed from the list of RCE's on the pib
///		AssertSz( fFalse, "LV cleanup breakpoint. Call LaurionB" );
		}

	//  in order that the compensating delta is calculated properly, set the delta value to 0
	pverdelta->lDelta = 0;

	return err;
	}


//  ================================================================
ERR ErrVERIUndoSLVSpacePhysical( RCE * const prce, CSR *pcsr )
//  ================================================================
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();
	
	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//	dirty page and log operation
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	const VERSLVSPACE* const pverslvspace = (VERSLVSPACE*)( prce->PbData() );

	SLVSPACEOPER operUndo = slvspaceoperInvalid;
	switch( pverslvspace->oper )
		{
		case slvspaceoperFreeToReserved:
			operUndo = slvspaceoperFreeReserved;
			break;
		case slvspaceoperReservedToCommitted:
			operUndo = slvspaceoperCommittedToDeleted;
			break;
		case slvspaceoperFreeToCommitted:
			operUndo = slvspaceoperFree;
			break;
		case slvspaceoperCommittedToDeleted:
			operUndo = slvspaceoperDeletedToCommitted;
			break;
			
		//  These operations are only generated by a rollback and thus should not
		//  have an RCE
		case slvspaceoperFree:
		case slvspaceoperDeletedToCommitted:
		case slvspaceoperDeletedToFree:
			Assert( fFalse );
			break;
		}

	err = ErrNDMungeSLVSpace(
				pfucb,
				pcsr,
				operUndo,
				pverslvspace->ipage,
				pverslvspace->cpages,
				fDIRUndo,
				rceidNull );
	CallS( err );

	//  this space was moved from committed to deleted and the SLV Provider is
	//  enabled then issue a task to register this space for freeing with the
	//  SLV Provider.  we must do this because we cannot reuse the space until
	//  we know that all SLV File handles pointing to this space have been closed
	//
	//  NOTE:  if we get an error we must ignore it and leak the space because
	//  rollback cannot fail.  we will recover the space the next time we attach
	//  to the database

	Assert( !PinstFromIfmp( prce->Ifmp() )->m_pver->m_fSyncronousTasks );
	
	if (	slvspaceoperCommittedToDeleted == operUndo &&
			PinstFromIfmp( prce->Ifmp() )->FSLVProviderEnabled() )
		{
		BOOKMARK	bookmark;
		prce->GetBookmark( &bookmark );
		Assert( sizeof( PGNO ) == bookmark.key.Cb() );
		Assert( 0 == bookmark.data.Cb() );

		PGNO pgnoLastInExtent;
		LongFromKey( &pgnoLastInExtent, bookmark.key );
		Assert( pgnoLastInExtent != 0 );
		Assert( pgnoLastInExtent % cpgSLVExtent == 0 );
		PGNO pgnoFirst = ( pgnoLastInExtent - cpgSLVExtent + 1 ) + pverslvspace->ipage;
		QWORD ibLogical = OffsetOfPgno( pgnoFirst );
		QWORD cbSize = QWORD( pverslvspace->cpages ) * SLVPAGE_SIZE;
		
		OSSLVDELETETASK * const ptask = new OSSLVDELETETASK(
												prce->Ifmp(),
												ibLogical,
												cbSize,
												CSLVInfo::FILEID( pverslvspace->fileid ),
												pverslvspace->cbAlloc,
												(const wchar_t*)pverslvspace->wszFileName );

		if ( ptask )
			{
			if ( PinstFromIfmp( prce->Ifmp() )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) < JET_errSuccess )
				{
				//  The task was not enqued sucessfully.
				delete ptask;
				}
			}
		}

	return err;
	}


//  ================================================================
VOID VERRedoPhysicalUndo( INST *pinst, const LRUNDO *plrundo, FUCB *pfucb, CSR *pcsr, BOOL fRedoNeeded )
//  ================================================================
//	retrieve RCE to be undone
//	call corresponding physical undo
//
	{
	//	get RCE for operation
	//
	BOOKMARK	bm;
	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( (VOID *) plrundo->rgbBookmark );
	bm.key.suffix.SetCb( plrundo->CbBookmarkKey() );
	bm.data.SetPv( (BYTE *) plrundo->rgbBookmark + plrundo->CbBookmarkKey() );
	bm.data.SetCb( plrundo->CbBookmarkData() );

	const DBID				dbid 	= plrundo->dbid;
	IFMP 					ifmp = pinst->m_mpdbidifmp[ dbid ];
	
	const UINT				uiHash	= UiRCHashFunc( ifmp, plrundo->le_pgnoFDP, bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	RCE * prce = PrceRCEGet( uiHash, ifmp, plrundo->le_pgnoFDP, bm );
	Assert( !fRedoNeeded || prce != prceNil );

	for ( ; prce != prceNil ; prce = prce->PrcePrevOfNode() )
		{
		if ( prce->Rceid() == plrundo->le_rceid )
			{
			//	UNDONE:	use rceid instead of level and procid
			//			to identify RCE
			//
			Assert( prce->Pfucb() == pfucb );
			Assert( prce->Pfucb()->ppib == pfucb->ppib );
			Assert(	prce->Oper() == plrundo->le_oper );
			Assert( prce->TrxCommitted() == trxMax );

			//	UNDONE: the following assert will fire if
			//	the original node operation was created by proxy
			//	(ie. concurrent create index).
			Assert( prce->Level() == plrundo->level );
			
			if ( fRedoNeeded )
				{
				Assert( prce->FUndoableLoggedOper( ) );
				OPER oper = plrundo->le_oper;
				
				switch ( oper )
					{
					case operReplace:
						CallS( ErrVERIUndoReplacePhysical( prce, 
														   pcsr,
														   bm ) );
						break;

					case operInsert:
						CallS( ErrVERIUndoInsertPhysical( prce, pcsr ) );
						break;

					case operFlagDelete:
						CallS( ErrVERIUndoFlagDeletePhysical( prce, pcsr ) );
						break;

					case operDelta:
						CallS( ErrVERIUndoDeltaPhysical( prce, pcsr ) );
						break;

					case operSLVSpace:
						CallS( ErrVERIUndoSLVSpacePhysical( prce, pcsr ) );
						break;
						
					default:
						Assert( fFalse );
					}
				}

			ENTERCRITICALSECTION critRCEList( &(prce->Pfcb()->CritRCEList()) );
			VERINullifyUncommittedRCE( prce );
			break;
			}
		}

	Assert( !fRedoNeeded || prce != prceNil );

	return;
	}


LOCAL VOID VERINullifyRolledBackRCE(
	PIB				*ppib,
	RCE				* const prceToNullify,
	RCE				**pprceNextToNullify = NULL )
	{
	const BOOL			fOperInHashTable		= prceToNullify->FOperInHashTable();
	CCriticalSection	*pcritHash				= fOperInHashTable ?
													&( PverFromIfmp( prceToNullify->Ifmp() )->CritRCEChain( prceToNullify->UiHash() ) ) :
													NULL;
	CCriticalSection&	critRCEList				= prceToNullify->Pfcb()->CritRCEList();
	const BOOL			fPossibleSecondaryIndex	= prceToNullify->Pfcb()->FTypeTable()
													&& !prceToNullify->Pfcb()->FFixedDDL()
													&& prceToNullify->FOperAffectsSecondaryIndex();


	Assert( ppib->critTrx.FOwner() );

	if ( fOperInHashTable )
		{
		pcritHash->Enter();
		}
	critRCEList.Enter(); 

	prceToNullify->FlagRolledBack();

	// Take snapshot of count of people concurrently creating a secondary
	// index entry.  Since this count is only ever incremented within this
	// table's critRCEList (which we currently have) it doesn't matter
	// if value changes after the snapshot is taken, because it will
	// have been incremented for some other table.
	// Also, if this FCB is not a table or if it is but has fixed DDL, it
	// doesn't matter if others are doing concurrent create index -- we know 
	// they're not doing it on this FCB.
	// Finally, the only RCE types we have to lock are Insert, FlagDelete, and Replace,
	// because they are the only RCE's that concurrent CreateIndex acts upon.
	const LONG	crefCreateIndexLock		= ( fPossibleSecondaryIndex ? crefVERCreateIndexLock : 0 );
	Assert( crefVERCreateIndexLock >= 0 );
	if ( 0 == crefCreateIndexLock )
		{
		//	set return value before the RCE is nullified
		if ( NULL != pprceNextToNullify )
			*pprceNextToNullify = prceToNullify->PrcePrevOfSession();

		Assert( !prceToNullify->FOperNull() );
		Assert( prceToNullify->Level() <= ppib->level );
		Assert( prceToNullify->TrxCommitted() == trxMax );

		VERINullifyUncommittedRCE( prceToNullify );

		critRCEList.Leave(); 
		if ( fOperInHashTable )
			{
			pcritHash->Leave();
			}
		}
	else
		{
		Assert( crefCreateIndexLock > 0 );

		critRCEList.Leave(); 
		if ( fOperInHashTable )
			{
			pcritHash->Leave();
			}


		if ( NULL != pprceNextToNullify )
			{
			ppib->critTrx.Leave();
			UtilSleep( cmsecWaitGeneric );
			ppib->critTrx.Enter();

			//	restart RCE scan
			*pprceNextToNullify = ppib->prceNewest;
			}
		}
	}



//  ================================================================
LOCAL ERR ErrVERITryUndoSLVPageAppend( PIB *ppib, RCE * const prce )
//  ================================================================
	{
	ERR		err 			= JET_errSuccess;
	FUCB	* const pfucb	= prce->Pfucb();
	INT		crepeat 		= 0;
	LATCH	latch 			= latchReadNoTouch;
	BOOL	fGotoBookmark	= fFalse;

	Assert( pfucbNil != pfucb );
	ASSERT_VALID( pfucb );
	Assert( ppib == pfucb->ppib );
	Assert( prce->TrxCommitted() == trxMax );
	Assert ( operSLVPageAppend == prce->Oper() );

	//  we have sucessfully undone the operation
	//	must now set RolledBack flag before releasing page latch
	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );

	RCE::SLVPAGEAPPEND *pData;

	Assert ( prce->CbData() == sizeof(RCE::SLVPAGEAPPEND) );
	pData = (RCE::SLVPAGEAPPEND *) prce->PbData();
	Assert ( NULL != pData );
	
	IFMP	ifmpDb	= prce->Ifmp( );
	IFMP	ifmp	= ifmpDb | ifmpSLV;
	PGNO	pgno	= PgnoOfOffset( pData->ibLogical );
	DWORD	ib		= DWORD( pData->ibLogical % SLVPAGE_SIZE );
	DWORD cb 		= pData->cbData;

	// only in this care we will undo SLV page operations
	Assert ( !PinstFromIfmp( ifmpDb )->FSLVProviderEnabled() );

	Assert ( SLVPAGE_SIZE - ib >= cb );
	
	BFLatch bflPage;

	Call( ErrBFWriteLatchPage( &bflPage, ifmp, pgno , ib ? bflfDefault : bflfNew ) );
	
	// warnings like wrnBFCacheMiss are OK
	if ( err > JET_errSuccess )
		{
		err = JET_errSuccess;
		}
		
	BFDirty( &bflPage );
	memset ( (BYTE*)bflPage.pv + ib, 0, cb );
	
	BFWriteUnlatch( &bflPage );

	//	must nullify RCE while page is still latched, to avoid inconsistency
	//	between what's on the page and what's in the version store
	VERINullifyRolledBackRCE( ppib, prce );

	CallS( err );
	return JET_errSuccess;
	
HandleError:

	Assert( err < JET_errSuccess );	
	return err;
	}


//  ================================================================
LOCAL ERR ErrVERITryUndoLoggedOper( PIB *ppib, RCE * const prce )
//  ================================================================
//	seek to bookmark, upgrade latch
//	call corresponding physical undo
//
	{
	ERR				err;
	FUCB * const	pfucb		= prce->Pfucb();
	LATCH			latch 		= latchReadNoTouch;
	BOOKMARK		bm;
	BOOKMARK		bmSave;

	Assert( pfucbNil != pfucb );
	ASSERT_VALID( pfucb );
	Assert( ppib == pfucb->ppib );
	Assert( prce->FUndoableLoggedOper() );
	Assert( prce->TrxCommitted() == trxMax );

	// if Force Detach, just nullify the RCE
	if ( rgfmp[ prce->Ifmp() ].FForceDetaching() )
		{
		VERINullifyRolledBackRCE( ppib, prce );
		return JET_errSuccess;		
		}

	prce->GetBookmark( &bm );

	//  reset index range on this cursor
	//  we may be using a deferred-closed cursor or a cursor that
	//  had an index-range on it before the rollback
	DIRResetIndexRange( pfucb );
        
	//	save off cursor's current bookmark, set
	//	to bookmark of operation to be rolled back,
	//	then release any latches to force re-seek
	//	to bookmark
	bmSave = pfucb->bmCurr;
	pfucb->bmCurr = bm;

	BTUp( pfucb );
	
Refresh:
	err = ErrBTIRefresh( pfucb, latch );
	Assert( JET_errRecordDeleted != err );
	Call( err );

	//	upgrade latch on page
	//
	err = Pcsr( pfucb )->ErrUpgrade();
	if ( errBFLatchConflict == err )
		{
		Assert( !Pcsr( pfucb )->FLatched() );
		latch = latchRIW;
		goto Refresh;
		}
	Call( err );

	switch( prce->Oper() )
		{
		//	logged operations
		//
		case operReplace:
			Call( ErrVERIUndoReplacePhysical( prce, Pcsr( pfucb ), bm ) );
			break;
			
		case operInsert:
			Call( ErrVERIUndoInsertPhysical( prce, Pcsr( pfucb ) ) );
			break;			

		case operFlagDelete:
			Call( ErrVERIUndoFlagDeletePhysical( prce, Pcsr( pfucb ) ) );
			break;

		case operDelta:
			Call( ErrVERIUndoDeltaPhysical( prce, Pcsr( pfucb ) ) );
			break;

		case operSLVSpace:
			Call( ErrVERIUndoSLVSpacePhysical( prce, Pcsr( pfucb ) ) );
			break;
			
		default:
			Assert( fFalse );
		}

	//  we have sucessfully undone the operation
	//	must now set RolledBack flag before releasing page latch
	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );

	//	must nullify RCE while page is still latched, to avoid inconsistency
	//	between what's on the page and what's in the version store
	VERINullifyRolledBackRCE( ppib, prce );

	//	re-instate original bookmark
	pfucb->bmCurr = bmSave;
	BTUp( pfucb );

	CallS( err );
	return JET_errSuccess;
	
HandleError:
	//	re-instate original bookmark
	pfucb->bmCurr = bmSave;
	BTUp( pfucb );

	Assert( err < JET_errSuccess );
	Assert( JET_errDiskFull != err );

	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIUndoLoggedOper( PIB *ppib, RCE * const prce )
//  ================================================================
//	seek to bookmark, upgrade latch
//	call corresponding physical undo
//
	{
	ERR				err 	= JET_errSuccess;
	FUCB	* const pfucb	= prce->Pfucb();

	Assert( pfucbNil != pfucb );

	if ( operSLVPageAppend == prce->Oper() )
		{
		Assert ( !prce->FOperInHashTable() );

		Assert ( prce->CbData () == sizeof( RCE::SLVPAGEAPPEND ) );
		Call ( ErrVERITryUndoSLVPageAppend( ppib, prce ) );
		}
	else
		{
		Assert( ppib == prce->Pfucb()->ppib );
		Call ( ErrVERITryUndoLoggedOper( ppib, prce ) );
		}

	return JET_errSuccess;
	
HandleError:
	Assert( err < JET_errSuccess );
	
	//  if rollback fails due to an error, then we will disable
	//  logging in order to force the system down (we are in
	//  an uncontinuable state).  Recover to restart the database.

	//	We should never fail to rollback due to disk full

	Assert( JET_errDiskFull != err );

	switch ( err )
		{
		// failure with impact on the database only
		case JET_errDiskIO:
		case JET_errReadVerifyFailure:
		case JET_errOutOfBuffers:
		case JET_errOutOfMemory	:		// BF may hit OOM when trying to latch the page


#ifdef INDEPENDENT_DB_FAILURE
			// if we know that transactions are limited at
			// one database, we can put the database
			// in the "unusable" state and error out
			// (on error on TMP database fail the instance)
			if ( g_fOneDatabasePerSession && FUserIfmp( prce->Ifmp() ) )
				{

				// UNDONE: do we want to retry, at least on some errors
				// like OutOfMemory ?
				
				Assert ( prce );
				FMP::AssertVALIDIFMP( prce->Ifmp() );
				
				FMP * pfmp = &rgfmp[ prce->Ifmp() ];
				Assert ( pfmp );
				
				Assert ( !pfmp->FAllowForceDetach() );			
				pfmp->SetAllowForceDetach( ppib, err );
				Assert ( pfmp->FAllowForceDetach() );		

				// UNDONE: EventLog error during rollback

				// convert error to JET_errRollbackError
				err = ErrERRCheck ( JET_errRollbackError );
				break;
				}
#endif				
				
		// failure with impact on the instance
		case JET_errLogWriteFail:
		case JET_errLogDiskFull:
				{

				//	rollback failed -- log an event message

				CHAR		szRCEID[16];
				CHAR		szERROR[16];

				LOSSTRFormatA( szRCEID, "%d", prce->Rceid() );
				LOSSTRFormatA( szERROR, "%d", err );

				const CHAR *rgpsz[3] = { szRCEID, rgfmp[pfucb->ifmp & ifmpMask].SzDatabaseName(), szERROR };
				UtilReportEvent( eventError, LOGGING_RECOVERY_CATEGORY, UNDO_FAILED_ID, 3, rgpsz );

				//	REVIEW: (LaurionB) how can we get a logging failure if FLogOn() is
				//	not set?
				if ( rgfmp[pfucb->ifmp & ifmpMask].FLogOn() )
					{
					LOG		* const plog	= PinstFromPpib( ppib )->m_plog;
					Assert( plog );
					Assert( !plog->m_fLogDisabled );

					//  flush and halt the log

					plog->LGSignalFlush();
					UtilSleep( cmsecWaitLogFlush );
					plog->m_fLGNoMoreLogWrite = fTrue;
					}


				//	There may be an older version of this page which needs
				//	the undo-info from this RCE. Take down the instance and
				//	error out
				//
				//	(Yes, this could be optimized to only do this for RCEs
				//	with before-images, but why complicate the code to optimize
				//	a failure case)
					
				Assert( !prce->FOperNull() );
				Assert( !prce->FRolledBack() );

				PinstFromPpib( ppib )->SetFInstanceUnavailable();
				ppib->SetErrRollbackFailure( err );
				err = ErrERRCheck ( JET_errRollbackError );
				break;
				}

		default:
			//	error is non-fatal, so caller will attempt to redo rollback
			break;
		}

	return err;
	}


//  ================================================================
INLINE VOID VERINullifyForUndoCreateTable( PIB * const ppib, FCB * const pfcb )
//  ================================================================
//
//  This is used to nullify all RCEs on table FCB because CreateTable
//	was rolled back.
//
//-
	{
	Assert( pfcb->FTypeTable()
		|| pfcb->FTypeTemporaryTable() );
	
	// Because rollback is done in two phases, the RCE's on
	// this FCB are still outstanding -- they've already
	// been processed for rollback, they just need to be
	// nullified.
	while ( prceNil != pfcb->PrceNewest() )
		{
		RCE	* const prce = pfcb->PrceNewest();

		Assert( prce->Pfcb() == pfcb );
		Assert( !prce->FOperNull() );

		// Since we're rolling back CreateTable, all operations
		// on the table must also be uncommitted and rolled back
		Assert( prce->TrxCommitted() == trxMax );

		if ( !prce->FRolledBack() )
			{
			// The CreateTable RCE itself should be the only
			// RCE on this FCB that is not yet marked as
			// rolled back, because that's what we're in the 
			// midst of doing
			Assert( prce->Oper() == operCreateTable );
			Assert( pfcb->FTypeTable() || pfcb->FTypeTemporaryTable() );
			Assert( pfcb->PrceNewest() == prce );
			Assert( pfcb->PrceOldest() == prce );
			}
			
		Assert( prce->Pfucb() != pfucbNil );
		Assert( ppib == prce->Pfucb()->ppib );	// only one session should have access to the table

		ENTERCRITICALSECTION	enterCritHash(
									prce->FOperInHashTable() ? &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) : NULL,
									prce->FOperInHashTable() );
		ENTERCRITICALSECTION	enterCritFCBRCEList( &( pfcb->CritRCEList() ) );
		VERINullifyUncommittedRCE( prce );
		}
		
	// should be no more versions on the FCB
	Assert( pfcb->PrceOldest() == prceNil );
	Assert( pfcb->PrceNewest() == prceNil );
	}

//  ================================================================
INLINE VOID VERICleanupForUndoCreateTable( RCE * const prceCreateTable )
//  ================================================================
//
//  This is used to cleanup RCEs and deferred-closed cursors
//	on table FCB because CreateTable was rolled back.
//
//-
	{
	FCB	* const pfcbTable = prceCreateTable->Pfcb();

	Assert( pfcbTable->FInitialized() );
	Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeTemporaryTable() );
	Assert( pfcbTable->FPrimaryIndex() );

	// Last RCE left should be the CreateTable RCE (ie. this RCE).
	Assert( operCreateTable == prceCreateTable->Oper() );
	Assert( pfcbTable->PrceOldest() == prceCreateTable );

	Assert( prceCreateTable->Pfucb() != pfucbNil );
	PIB	* const ppib	= prceCreateTable->Pfucb()->ppib;
	Assert( ppibNil != ppib );

	Assert( pfcbTable->Ptdb() != ptdbNil );
	FCB	* pfcbT			= pfcbTable->Ptdb()->PfcbLV();

	// force-close any deferred closed cursors
	if ( pfcbNil != pfcbT )
		{
		Assert( pfcbT->FTypeLV() );

		//	all RCE's on the table's LV should have already been rolled back AND nullified
		//	(since we don't suppress nullification of LV RCE's during concurrent create-index
		//	like we do for Table RCE's)
		Assert( prceNil == pfcbT->PrceNewest() );
//		VERINullifyForUndoCreateTable( ppib, pfcbT );

		FUCBCloseAllCursorsOnFCB( ppib, pfcbT );
		}

	for ( pfcbT = pfcbTable->PfcbNextIndex(); pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
		{
		Assert( pfcbT->FTypeSecondaryIndex() );

		//	all RCE's on the table's indexes should have already been rolled back AND nullified
		//	(since we don't suppress nullification of Index RCE's during concurrent create-index
		//	like we do for Table RCE's)
		Assert( prceNil == pfcbT->PrceNewest() );
//		VERINullifyForUndoCreateTable( ppib, pfcbT );

		FUCBCloseAllCursorsOnFCB( ppib, pfcbT );
		}
		
	//	there may be rolled-back versions on the table that couldn't be nullified
	//	because concurrent create-index was in progress on another table (we
	//	suppress nullification of certain table RCE's if a CCI is in progress
	//	anywhere)
	VERINullifyForUndoCreateTable( ppib, pfcbTable );
	FUCBCloseAllCursorsOnFCB( ppib, pfcbTable );
	}

//  ================================================================
LOCAL VOID VERIUndoCreateTable( RCE * const prce )
//  ================================================================
//
//  Takes a non-const RCE as VersionDecrement sets the pfcb to NULL
//
//-
	{
	FUCB	* pfucb			= prce->Pfucb();
	FCB 	* const pfcb	= prce->Pfcb();
#ifdef DEBUG
	PIB		* const ppib	= pfucb->ppib;
#endif	//	DEBUG
	
	ASSERT_VALID( pfucb );
	ASSERT_VALID( ppib );
	Assert( prce->Oper() == operCreateTable );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( pfcb->FInitialized() );
	Assert( pfcb->FTypeTable() || pfcb->FTypeTemporaryTable() );
	Assert( pfcb->FPrimaryIndex() );

	//	close all cursors on this table
	while ( pfucbNil != pfucb )
		{
		ASSERT_VALID( pfucb );
		FUCB * const pfucbNext = pfucb->pfucbNextOfFile;

		// Since CreateTable is uncommitted, we should be the
		// only one who could have opened cursors on it.
		Assert( pfucb->ppib == ppib );

		//	if defer closed then continue
		if ( !FFUCBDeferClosed( pfucb ) )
			{
			if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
				{
				CallS( ErrDispCloseTable( ( JET_SESID )pfucb->ppib, ( JET_TABLEID )pfucb ) );
				}
			else
				{
				CallS( ErrFILECloseTable( pfucb->ppib, pfucb ) );
				}
			}

		pfucb = pfucbNext;
		}

	VERICleanupForUndoCreateTable( prce );
	
	pfcb->PrepareForPurge();
	pfcb->Purge();
	}


//  ================================================================
LOCAL VOID VERIUndoAddColumn( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operAddColumn );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
	Assert( prce->Pfcb() == prce->Pfucb()->u.pfcb );

	FCB			* const pfcbTable	= prce->Pfcb();

	pfcbTable->EnterDDL();

	// RCE list ensures FCB is still pinned
	Assert( pfcbTable->PrceOldest() != prceNil );

	TDB 			* const	ptdb 		= pfcbTable->Ptdb();
	const COLUMNID	columnid			= ( (VERADDCOLUMN*)prce->PbData() )->columnid;
	BYTE			* pbOldDefaultRec	= ( (VERADDCOLUMN*)prce->PbData() )->pbOldDefaultRec;
	FIELD			* const	pfield		= ptdb->Pfield( columnid );

	Assert( ( FCOLUMNIDTagged( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidTaggedLast() )
			|| ( FCOLUMNIDVar( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidVarLast() )
			|| ( FCOLUMNIDFixed( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidFixedLast() ) );

	//	rollback the added column by marking it as deleted
	Assert( !FFIELDDeleted( pfield->ffield ) );
	FIELDSetDeleted( pfield->ffield );

	FIELDResetVersioned( pfield->ffield );
	FIELDResetVersionedAdd( pfield->ffield );

	//	rollback version and autoinc fields, if set.
	Assert( !( FFIELDVersion( pfield->ffield ) && FFIELDAutoincrement( pfield->ffield ) ) );
	if ( FFIELDVersion( pfield->ffield ) )
		{
		Assert( ptdb->FidVersion() == FidOfColumnid( columnid ) );
		ptdb->ResetFidVersion();
		}
	else if ( FFIELDAutoincrement( pfield->ffield ) )
		{
		Assert( ptdb->FidAutoincrement() == FidOfColumnid( columnid ) );
		ptdb->ResetFidAutoincrement();
		}

	//	itag 0 in the TDB is reserved for the FIELD structures.  We
	//	cannibalise it for itags of field names to indicate that a name
	//	has not been added to the buffer.
	if ( 0 != pfield->itagFieldName )
		{
		//	remove the column name from the TDB name space
		ptdb->MemPool().DeleteEntry( pfield->itagFieldName );
		}

	//  UNDONE: remove the CBDESC for this from the TDB
	//  the columnid will not be re-used so the callback
	//  will not be called. The CBDESC will be freed when
	//  the table is closed so no memory will be lost
	//  Its just not pretty though...

	//	if we modified the default record, the changes will be invisible,
	//	since the field is now flagged as deleted (unversioned).  The
	//	space will be reclaimed by a subsequent call to AddColumn.

	//	if there was an old default record and we were successful in
	//	creating a new default record for the TDB, must get rid of
	//	the old default record.  However, we can't just free the
	//	memory because other threads could have stale pointers.
	//	So build a list hanging off the table FCB and free the
	//	memory when the table FCB is freed.
	if ( NULL != pbOldDefaultRec
		&& (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec )
		{
		//	user-defined defaults are not stored in the default
		//	record (ie. this AddColumn would not have caused
		//	use to rebuild the default record)
		Assert( !FFIELDUserDefinedDefault( pfield->ffield ) );

		for ( RECDANGLING * precdangling = pfcbTable->Precdangling();
			;
			precdangling = precdangling->precdanglingNext )
			{
			if ( NULL == precdangling )
				{
				//	not in list, so add it;
				//	assumes that the memory pointed to by pmemdangling is always at
				//	least sizeof(ULONG_PTR) bytes
				Assert( NULL == ( (RECDANGLING *)pbOldDefaultRec )->precdanglingNext );
				( (RECDANGLING *)pbOldDefaultRec )->precdanglingNext = pfcbTable->Precdangling();
				pfcbTable->SetPrecdangling( (RECDANGLING *)pbOldDefaultRec );
				break;
				}
			else if ( (BYTE *)precdangling == pbOldDefaultRec )
				{
				//	pointer is already in the list, just get out
				AssertTracking();
				break;
				}
			}
		}

	pfcbTable->LeaveDDL();
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteColumn( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operDeleteColumn );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( prce->CbData() == sizeof(COLUMNID) );
	Assert( prce->Pfcb() == prce->Pfucb()->u.pfcb );

	FCB			* const pfcbTable	= prce->Pfcb();

	pfcbTable->EnterDDL();

	// RCE list ensures FCB is still pinned
	Assert( pfcbTable->PrceOldest() != prceNil );

	TDB 			* const	ptdb 		= pfcbTable->Ptdb();
	const COLUMNID	columnid			= *( (COLUMNID*)prce->PbData() );
	FIELD			* const	pfield		= ptdb->Pfield( columnid );

	Assert( ( FCOLUMNIDTagged( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidTaggedLast() )
			|| ( FCOLUMNIDVar( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidVarLast() )
			|| ( FCOLUMNIDFixed( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidFixedLast() ) );

	Assert( pfield->coltyp != JET_coltypNil );
	Assert( FFIELDDeleted( pfield->ffield ) );
	FIELDResetDeleted( pfield->ffield );

	if ( FFIELDVersioned( pfield->ffield ) )
		{
		// UNDONE: Instead of the VersionedAdd flag, scan the version store
		// for other outstanding versions on this column (should either be
		// none or a single AddColumn version).
		if ( !FFIELDVersionedAdd( pfield->ffield ) )
			{
			FIELDResetVersioned( pfield->ffield );
			}
		}
	else
		{
		Assert( !FFIELDVersionedAdd( pfield->ffield ) );
		}

	pfcbTable->LeaveDDL();
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteTable( const RCE * const prce )
//  ================================================================
	{
	if ( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp )
		{
		// DeleteTable (ie. CloseTable) doesn't get rolled back for temp. tables.
		return;
		}

	Assert( prce->Oper() == operDeleteTable );
	Assert( prce->TrxCommitted() == trxMax );

	//	may be pfcbNil if sentinel
	INT	fState;
	FCB * const pfcbTable = FCB::PfcbFCBGet( prce->Ifmp(), *(PGNO*)prce->PbData(), &fState );
	Assert( pfcbTable != pfcbNil );
	Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
	Assert( fFCBStateInitialized == fState || fFCBStateSentinel == fState );

	// If regular FCB, decrement refcnt, else free sentinel.							
	if ( pfcbTable->Ptdb() != ptdbNil )
		{
		Assert( fFCBStateInitialized == fState );

		pfcbTable->EnterDDL();

		for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
			{
			Assert( pfcbT->FDeletePending() );
			Assert( !pfcbT->FDeleteCommitted() );
			pfcbT->ResetDeletePending();
			}

		FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
		if ( pfcbNil != pfcbLV )
			{
			Assert( pfcbLV->FDeletePending() );
			Assert( !pfcbLV->FDeleteCommitted() );
			pfcbLV->ResetDeletePending();
			}
			
		pfcbTable->LeaveDDL();

		pfcbTable->Release();
		}
	else
		{
		Assert( fFCBStateSentinel == fState );
		Assert( pfcbTable->FDeletePending() );
		Assert( !pfcbTable->FDeleteCommitted() );
		pfcbTable->ResetDeletePending();

		pfcbTable->PrepareForPurge();
		pfcbTable->Purge();
		}
	}


//  ================================================================
LOCAL VOID VERIUndoCreateLV( PIB *ppib, const RCE * const prce )
//  ================================================================
	{
	if ( pfcbNil != prce->Pfcb()->Ptdb()->PfcbLV() )
		{
		//  if we rollback the creation of the LV tree, unlink the LV FCB
		//  the FCB will be lost (memory leak)
		FCB * const pfcbLV = prce->Pfcb()->Ptdb()->PfcbLV();

		VERIUnlinkDefunctLV( prce->Pfucb()->ppib, pfcbLV );
		pfcbLV->PrepareForPurge( fFalse );
		pfcbLV->Purge();
		prce->Pfcb()->Ptdb()->SetPfcbLV( pfcbNil );

		if ( pfcbLV >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
			{

			//	the LV FCB is above the threshold; thus, removing it may
			//	cause the table FCB to move below the threshold; if the
			//	table FCB is in the avail-above list, it must be moved
			//	to the avail-below list

			prce->Pfcb()->UpdateAvailListPosition();
			}
		}
	}

	
//  ================================================================
LOCAL VOID VERIUndoCreateIndex( PIB *ppib, const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operCreateIndex );
	Assert( prce->TrxCommitted() == trxMax );
	Assert( prce->CbData() == sizeof(TDB *) );

	//	pfcb of secondary index FCB or pfcbNil for primary
	//	index creation
	FCB	* const pfcb = *(FCB **)prce->PbData();
	FCB	* const pfcbTable = prce->Pfucb()->u.pfcb;

	Assert( pfcbNil != pfcbTable );
	Assert( pfcbTable->FTypeTable() );
	Assert( pfcbTable->PrceOldest() != prceNil );	// This prevents the index FCB from being deallocated.

	//	if secondary index then close all cursors on index
	//	and purge index FCB, else free IDB for primary index.
	
	if ( pfcb != pfcbNil )
		{
		// This can't be the primary index, because we would not have allocated
		// an FCB for it.
		Assert( prce->Pfucb()->u.pfcb != pfcb );

		Assert( pfcb->FTypeSecondaryIndex() );
		Assert( pfcb->Pidb() != pidbNil );

		//	Normally, we grab the updating/indexing latch before we grab
		//	the ppib's critTrx, but in this case, we are already in the
		//	ppib's critTrx (because we are rolling back) and we need the
		//	updating/indexing latch.  We can guarantee that this will not
		//	cause a deadlock because the only person that grabs critTrx
		//	after grabbing the updating/indexing latch is concurrent create
		//	index, which quiesces all rollbacks before it begins.
		CLockDeadlockDetectionInfo::NextOwnershipIsNotADeadlock();

		pfcbTable->SetIndexing();
		pfcbTable->EnterDDL();

		Assert( !pfcb->Pidb()->FDeleted() );
		Assert( !pfcb->FDeletePending() );
		Assert( !pfcb->FDeleteCommitted() );
			
		// Mark as committed delete so no one else will attempt version check.
		pfcb->Pidb()->SetFDeleted();
		pfcb->Pidb()->ResetFVersioned();
			
		if ( pfcb->PfcbTable() == pfcbNil )
			{
			// Index FCB not yet linked into table's FCB list, but we did use
			// table's TDB memory pool to store some information for the IDB.
			pfcb->UnlinkIDB( pfcbTable );
			}
		else
			{
			Assert( pfcb->PfcbTable() == pfcbTable );
			
			// Unlink the FCB from the table's index list.
			// Note that the only way the FCB could have been
			// linked in is if the IDB was successfully created.
			pfcbTable->UnlinkSecondaryIndex( pfcb );
			
			//	update all index mask
			FILESetAllIndexMask( pfcbTable );
			}

		pfcbTable->LeaveDDL();
		pfcbTable->ResetIndexing();

		if ( pfcb >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
			{

			//	the index FCB is above the threshold; thus, removing it may
			//	cause the table FCB to move below the threshold; if the
			//	table FCB is in the avail-above list, it must be moved
			//	to the avail-below list

			pfcbTable->UpdateAvailListPosition();
			}

		//	must leave critTrx because we may enter the critTrx
		//	of other sessions when we try to remove their RCEs
		//	or cursors on this index
		ppib->critTrx.Leave();
		
		// Index FCB has been unlinked from table, so we're
		// guaranteed no further versions will occur on this
		// FCB.  Clean up remaining versions.
		VERNullifyAllVersionsOnFCB( pfcb );
		
		VERIUnlinkDefunctSecondaryIndex( prce->Pfucb()->ppib, pfcb );

		ppib->critTrx.Enter();
		
		// The table's version count will prevent the
		// table FCB (and thus this secondary index FCB)
		// from being deallocated before we can delete 
		// this index FCB.
		pfcb->PrepareForPurge( fFalse );
		pfcb->Purge();
		}
		
	else if ( pfcbTable->Pidb() != pidbNil )
		{
		pfcbTable->EnterDDL();

		IDB		*pidb	= pfcbTable->Pidb();

		Assert( !pidb->FDeleted() );
		Assert( !pfcbTable->FDeletePending() );
		Assert( !pfcbTable->FDeleteCommitted() );
		Assert( !pfcbTable->FSequentialIndex() );
			
		// Mark as committed delete so no one else will attempt version check.
		pidb->SetFDeleted();
		pidb->ResetFVersioned();
		pidb->ResetFVersionedCreate();
			
		pfcbTable->UnlinkIDB( pfcbTable );
		pfcbTable->SetPidb( pidbNil );
		pfcbTable->SetSequentialIndex();

		// UNDONE: Reset density to original value.
		pfcbTable->SetCbDensityFree( (SHORT)( ( ( 100 - ulFILEDefaultDensity ) * g_cbPage ) / 100 ) );
		
		//	update all index mask
		FILESetAllIndexMask( pfcbTable );

		pfcbTable->LeaveDDL();
		}
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteIndex( const RCE * const prce )
//  ================================================================
	{
	FCB	* const pfcbIndex = *(FCB **)prce->PbData();
	FCB	* const pfcbTable = prce->Pfcb();
	
	Assert( prce->Oper() == operDeleteIndex );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( pfcbTable->FTypeTable() );
	Assert( pfcbIndex->PfcbTable() == pfcbTable );
	Assert( prce->CbData() == sizeof(FCB *) );
	Assert( pfcbIndex->FTypeSecondaryIndex() );

	pfcbIndex->ResetDeleteIndex();
	}


//  ================================================================
INLINE VOID VERIUndoAllocExt( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->CbData() == sizeof(VEREXT) );
	Assert( prce->PgnoFDP() == ((VEREXT*)prce->PbData())->pgnoFDP );

	const VEREXT* const pverext = (const VEREXT*)(prce->PbData());

	VERIFreeExt( prce->Pfucb()->ppib, prce->Pfcb(),
		pverext->pgnoFirst,
		pverext->cpgSize );	
	}


//  ================================================================
INLINE VOID VERIUndoRegisterCallback( const RCE * const prce )
//  ================================================================
//
//  Remove the callback from the list
//
//-
	{
	VERIRemoveCallback( prce );
	}


//  ================================================================
VOID VERIUndoUnregisterCallback( const RCE * const prce )
//  ================================================================
//
//  Set the trxUnregisterBegin0 in the CBDESC to trxMax
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax != pcbdesc->trxRegisterCommit0 );
	Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxUnregisterBegin0 = trxMax;
#endif	//	VERSIONED_CALLBACKS
	prce->Pfcb()->LeaveDDL();
	}


//  ================================================================
INLINE VOID VERIUndoNonLoggedOper( PIB *ppib, RCE * const prce, RCE **pprceNextToUndo )
//  ================================================================
	{
	Assert( *pprceNextToUndo == prce->PrcePrevOfSession() );
	
	switch( prce->Oper() )
		{
		//	non-logged operations
		//
		case operAllocExt:
			VERIUndoAllocExt( prce );
			break;
		case operDeleteTable:
			VERIUndoDeleteTable( prce );
			break;
		case operAddColumn:
			VERIUndoAddColumn( prce );
			break;
		case operDeleteColumn:
			VERIUndoDeleteColumn( prce );
			break;
		case operCreateLV:
			VERIUndoCreateLV( ppib, prce );
			break;
		case operCreateIndex:
			VERIUndoCreateIndex( ppib, prce );
			//	refresh prceNextToUndo in case RCE list was
			//	updated when we lost critTrx
			*pprceNextToUndo = prce->PrcePrevOfSession();
			break;
		case operDeleteIndex:
			VERIUndoDeleteIndex( prce );
			break;
		case operRegisterCallback:
			VERIUndoRegisterCallback( prce );
			break;
		case operUnregisterCallback:
			VERIUndoUnregisterCallback( prce );
			break;
		case operReadLock:
		case operWriteLock:
		case operPreInsert:
		case operWaitLock:
			break;
		default:
			Assert( fFalse );
			break;
			
		case operCreateTable:
			VERIUndoCreateTable( prce );
			// For CreateTable only, the RCE is nullified, so no need
			// to set RolledBack flag -- get out immediately.
			Assert( prce->FOperNull() );	
			return;
		}
		
	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );
	
	//  we have sucessfully undone the operation
	VERINullifyRolledBackRCE( ppib, prce );
	}


//  ================================================================
ERR ErrVERRollback( PIB *ppib )
//  ================================================================
//
//	Rollback is done in 2 phase. 1st phase is to undo the versioned
//	operation and may involve IO. 2nd phase is nullify the undone
//	RCE. 2 phases are needed so that the version will be held till all
//	IO is done, then wipe them all. If it is mixed, then once we undo
//	a RCE, the record become writable to other session. This may mess
//	up recovery where we may get write conflict since we have not guarrantee
//	that the log for operations on undone record won't be logged before
//	Rollback record is logged.
//	UNDONE: rollback should behave the same as commit, and need two phase log.
//
//-
	{
	ASSERT_VALID( ppib );
	Assert( ppib->level > 0 );

	const 	LEVEL	level				= ppib->level;
#ifdef DEBUG
			INT 	cRepeat = 0;
#endif

	ERR err = JET_errSuccess;

	ppib->critTrx.Enter();

	RCE	*prceToUndo;
	RCE	*prceNextToUndo;
	RCE	*prceToNullify;
	for( prceToUndo = ppib->prceNewest;
		prceNil != prceToUndo && prceToUndo->Level() == level;
		prceToUndo = prceNextToUndo )
		{
		Assert( !prceToUndo->FOperNull() );
		Assert( prceToUndo->Level() <= level );
		Assert( prceToUndo->TrxCommitted() == trxMax );		
		Assert( prceToUndo->Pfcb() != pfcbNil );
		Assert( !prceToUndo->FRolledBack() );

#ifdef INDEPENDENT_DB_FAILURE
		// if we force detach
		// we have to check that we don't rollback operations
		// on other DBs.: it is not allowed to have operation on multiple databases
		if ( ifmpMax != ppib->m_ifmpForceDetach )
			{
			EnforceSz ( ppib->m_ifmpForceDetach == prceToUndo->Ifmp(),
				"Force detach not allowed if sessions are used cross-databases");
			}
#endif
			
		//  Save next RCE to process, because RCE will attempt to be
		//	nullified if undo is successful.
		prceNextToUndo = prceToUndo->PrcePrevOfSession();

		if ( prceToUndo->FUndoableLoggedOper() )
			{
			Assert( pfucbNil != prceToUndo->Pfucb() );
			Assert( ppib == prceToUndo->Pfucb()->ppib );
			Assert( JET_errSuccess == ppib->ErrRollbackFailure() );

			//	logged operations
			//
			
			err = ErrVERIUndoLoggedOper( ppib, prceToUndo );
			if ( err < JET_errSuccess )
				{
				// if due to an error we stopped a database usage
				// error out from the rollback
				// (the user will be able to ForceDetach the database
				if ( JET_errRollbackError == err )
					{
#ifdef INDEPENDENT_DB_FAILURE					
					Assert ( rgfmp[ prceToUndo->Ifmp() ].FAllowForceDetach() );
#else
					Assert( ppib->ErrRollbackFailure() < JET_errSuccess );
#endif					
					Call ( err );
					}
				else						
					{
					Assert( ++cRepeat < 100 );
					prceNextToUndo = prceToUndo;
					continue;		//	sidestep resetting of cRepeat
					}				
				Assert ( fFalse );
				}
			}
		
		else
			{
			// non-logged operations can never fail to rollback
			VERIUndoNonLoggedOper( ppib, prceToUndo, &prceNextToUndo );
			}

#ifdef DEBUG
		cRepeat = 0;
#endif		
		}
		
	//	must loop through again and catch any RCE's that couldn't be nullified
	//	during the first pass because of concurrent create index
	prceToNullify = ppib->prceNewest;
	while ( prceNil != prceToNullify && prceToNullify->Level() == level )
		{
		// Only time nullification should have failed during the first pass is
		// if the RCE was locked for concurrent create index.  This can only
		// occur on a non-catalog, non-fixed DDL table for an Insert,
		// FlagDelete, or Replace operation.
		Assert( pfucbNil != prceToNullify->Pfucb() );
		Assert( ppib == prceToNullify->Pfucb()->ppib );
		Assert( !prceToNullify->FOperNull() );
		Assert( prceToNullify->FOperAffectsSecondaryIndex() );
		Assert( prceToNullify->FRolledBack() );
		Assert( prceToNullify->Pfcb()->FTypeTable() );
		Assert( !prceToNullify->Pfcb()->FFixedDDL() );
		Assert( !FCATSystemTable( prceToNullify->PgnoFDP() ) );

		RCE	*prceNextToNullify;
		VERINullifyRolledBackRCE( ppib, prceToNullify, &prceNextToNullify );

		prceToNullify = prceNextToNullify;
		}

	// If this PIB has any RCE's remaining, they must be at a lower level.
	Assert( prceNil == ppib->prceNewest
		|| ppib->prceNewest->Level() <= level );

	ppib->critTrx.Leave();

	//	decrement session transaction level
	Assert( level == ppib->level );
	if ( 1 == ppib->level )
		{
		//  we should have processed all RCEs
		Assert( prceNil == ppib->prceNewest );
		PIBSetPrceNewest( ppib, prceNil );			//	safety measure, in case it's not NULL for whatever reason
		}
	Assert( ppib->level > 0 );
	--ppib->level;

	//	rollback always succeeds	
	return JET_errSuccess;
	
HandleError:
	ppib->critTrx.Leave();

	return err;
	}
					

//  ================================================================
ERR ErrVERRollback( PIB *ppib, UPDATEID updateid )
//  ================================================================
//
//  This is used to rollback the operations from one particular update
//  all levels are rolled back.
//
//	Rollback is done in 2 phase. 1st phase is to undo the versioned
//	operation and may involve IO. 2nd phase is nullify the undone
//	RCE. 2 phases are needed so that the version will be held till all
//	IO is done, then wipe them all. If it is mixed, then once we undo
//	a RCE, the record become writable to other session. This may mess
//	up recovery where we may get write conflict since we have not guarrantee
//	that the log for operations on undone record won't be logged before
//	Rollback record is logged.
//	UNDONE: rollback should behave the same as commit, and need two phase log.
//
//-
	{
	ASSERT_VALID( ppib );
	Assert( ppib->level > 0 );

	Assert( updateidNil != updateid );

#ifdef DEBUG
	const 	LEVEL	level	= ppib->level;
			INT 	cRepeat = 0;
#endif

	ERR err = JET_errSuccess;

	ppib->critTrx.Enter();

	RCE	*prceToUndo;
	RCE	*prceNextToUndo;

	for( prceToUndo = ppib->prceNewest;
		prceNil != prceToUndo && prceToUndo->TrxCommitted() == trxMax;
		prceToUndo = prceNextToUndo )
		{
		//  Save next RCE to process, because RCE will attemp to be nullified
		//	if undo is successful.
		prceNextToUndo = prceToUndo->PrcePrevOfSession();
		
		if ( prceToUndo->Updateid() == updateid )
			{
			Assert( pfucbNil != prceToUndo->Pfucb() );
			Assert( ppib == prceToUndo->Pfucb()->ppib );
			Assert( !prceToUndo->FOperNull() );
			Assert( prceToUndo->Pfcb() != pfcbNil );
			Assert( !prceToUndo->FRolledBack() );

			//	the only RCEs with an updateid should be DML RCE's.
			Assert( prceToUndo->FUndoableLoggedOper() );
			Assert( JET_errSuccess == ppib->ErrRollbackFailure() );
			
			err = ErrVERIUndoLoggedOper( ppib, prceToUndo );
			if ( err < JET_errSuccess )
				{
				// if due to an error we stopped a database usage
				// error out from the rollback
				// (the user will be able to ForceDetach the database
				if ( JET_errRollbackError == err )
					{
#ifdef INDEPENDENT_DB_FAILURE					
					Assert ( rgfmp[ prceToUndo->Ifmp() ].FAllowForceDetach() );
#else
					Assert( ppib->ErrRollbackFailure() < JET_errSuccess );
#endif
					Call( err );
					}
				else						
					{
					Assert( ++cRepeat < 100 );
					prceNextToUndo = prceToUndo;
					continue;		//	sidestep resetting of cRepeat
					}				
				Assert ( fFalse );
				}
			}
				
#ifdef DEBUG				
		cRepeat = 0;
#endif				
		}
		
		
	//	must loop through again and catch any RCE's that couldn't be nullified
	//	during the first pass because of concurrent create index
	RCE	*prceToNullify;
	prceToNullify = ppib->prceNewest;
	while ( prceNil != prceToNullify && prceToNullify->TrxCommitted() == trxMax )
		{
		if ( prceToNullify->Updateid() == updateid )
			{
			// Only time nullification should have failed during the first pass is
			// if the RCE was locked for concurrent create index.  This can only
			// occur on a non-catalog, non-fixed DDL table for an Insert,
			// FlagDelete, or Replace operation.
			Assert( pfucbNil != prceToNullify->Pfucb() );
			Assert( ppib == prceToNullify->Pfucb()->ppib );
			Assert( !prceToNullify->FOperNull() );
			Assert( prceToNullify->FOperAffectsSecondaryIndex() );
			Assert( prceToNullify->FRolledBack() );
			Assert( prceToNullify->Pfcb()->FTypeTable() );
			Assert( !prceToNullify->Pfcb()->FFixedDDL() );
			Assert( !FCATSystemTable( prceToNullify->PgnoFDP() ) );

			RCE	*prceNextToNullify;
			VERINullifyRolledBackRCE( ppib, prceToNullify, &prceNextToNullify );

			prceToNullify = prceNextToNullify;
			}
		else
			{
			prceToNullify = prceToNullify->PrcePrevOfSession();
			}
		}

	ppib->critTrx.Leave();

	return JET_errSuccess;
HandleError:
	ppib->critTrx.Leave();

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\ttmap.cxx ===
#include "std.hxx"

//  ================================================================
TTMAP::TTMAP( PIB * const ppib ) :
//  ================================================================
	m_tableid( JET_tableidNil ),
	m_sesid( reinterpret_cast<JET_SESID>( ppib ) ),
	m_columnidKey( 0 ),
	m_columnidValue( 0 ),
	m_crecords( 0 )
	{
	}


//  ================================================================
TTMAP::~TTMAP()
//  ================================================================
	{
	if( JET_tableidNil != m_tableid )
		{
		CallS( ErrDispCloseTable( m_sesid, m_tableid ) );
		m_tableid = JET_tableidNil;
		}
	}


//  ================================================================
ERR TTMAP::ErrInit( INST * const )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	JET_COLUMNDEF	rgcolumndef[2] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLong, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },  //  Key
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLong, 0, 0, 0, 0, 0, 0 },								//  Value
		};	
	JET_COLUMNID	rgcolumnid[sizeof(rgcolumndef)/sizeof(JET_COLUMNDEF)];
	
	CallR( ErrIsamOpenTempTable(
		m_sesid,
		rgcolumndef,
		sizeof(rgcolumndef)/sizeof(JET_COLUMNDEF),
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&m_tableid,
		rgcolumnid ) );

	m_columnidKey 		= rgcolumnid[0];
	m_columnidValue 	= rgcolumnid[1];

	return err;
	}


//  ================================================================
ERR TTMAP::ErrInsertKeyValue_( const ULONG ulKey, const ULONG ulValue )
//  ================================================================
	{
	ERR	err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}
	
	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepInsert ) );

	Call( ErrDispSetColumn(
				m_sesid, 
				m_tableid, 
				m_columnidKey,
				(BYTE *)&ulKey, 
				sizeof( ulKey ),
				0, 
				NULL ) );

	Call( ErrDispSetColumn(
				m_sesid, 
				m_tableid, 
				m_columnidValue,
				(BYTE *)&ulValue, 
				sizeof( ulValue ),
				0, 
				NULL ) );

	Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	++m_crecords;
	
	return err;

HandleError:
	(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}

	
//  ================================================================
ERR TTMAP::ErrRetrieveValue_( const ULONG ulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	CallS( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );

	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );

	if( err >= 0 && pulValue )
		{
		ULONG cbActual = 0;
		err = ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidValue,
				pulValue,
				sizeof( ULONG ),
				&cbActual,
				NO_GRBIT,
				NULL );
		}
		
	CallS( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrIncrementValue( const ULONG ulKey )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	BOOL	fUpdatePrepared	= fFalse;

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	Call( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );
	if( JET_errRecordNotFound == err )
		{
		Call( ErrInsertKeyValue_( ulKey, 1 ) );
		}
	else
		{
		ULONG	cbActual;
		ULONG	ulValue = 0;
		
		Call( err );
		Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepReplace ) );
		fUpdatePrepared = fTrue;

		Call( ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidValue,
				&ulValue,
				sizeof( ulValue ),
				&cbActual,
				JET_bitRetrieveCopy,
				NULL ) );
		Assert( sizeof( ulValue ) == cbActual );

		++ulValue;
		Call( ErrDispSetColumn(
					m_sesid, 
					m_tableid, 
					m_columnidValue,
					(BYTE *)&ulValue, 
					sizeof( ulValue ),
					NO_GRBIT, 
					NULL ) );
		Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
		}

	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	return err;

HandleError:
	if( fUpdatePrepared )
		{
		(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
		}
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrSetValue( const ULONG ulKey, const ULONG ulValue )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	BOOL	fUpdatePrepared	= fFalse;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	Call( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );
	if( JET_errRecordNotFound == err )
		{
		Call( ErrInsertKeyValue_( ulKey, ulValue ) );
		}
	else
		{
		Call( err );
		Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepReplace ) );
		fUpdatePrepared = fTrue;
		Call( ErrDispSetColumn(
					m_sesid, 
					m_tableid, 
					m_columnidValue,
					(BYTE *)&ulValue, 
					sizeof( ulValue ),
					NO_GRBIT, 
					NULL ) );
		Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
		}

	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	return err;

HandleError:
	if( fUpdatePrepared )
		{
		(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
		}
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrGetValue( const ULONG ulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	return ErrRetrieveValue_( ulKey, pulValue );
	}


//  ================================================================
ERR TTMAP::ErrGetCurrentKeyValue( ULONG * const pulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	ULONG cbActual;
	Call( ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidKey,
			pulKey,
			sizeof( ULONG ),
			&cbActual,
			NO_GRBIT,
			NULL ) );

	Call( ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidValue,
			pulValue,
			sizeof( ULONG ),
			&cbActual,
			NO_GRBIT,
			NULL ) );

HandleError:
	CallS( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrDeleteKey( const ULONG ulKey )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	CallS( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	Call( ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ ) );

	Call( ErrDispDelete( m_sesid, m_tableid ) );
	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	--m_crecords;
	
	return err;
	
HandleError:
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrMoveFirst()
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	FUCBSetSequential( (FUCB *)m_tableid );
	FUCBSetPrereadForward( (FUCB *)m_tableid, cpgPrereadSequential );
	return ErrDispMove( m_sesid, m_tableid, JET_MoveFirst, NO_GRBIT );
	}


//  ================================================================
ERR TTMAP::ErrMoveNext()
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	return ErrDispMove( m_sesid, m_tableid, JET_MoveNext, NO_GRBIT );
	}


//  ================================================================
ERR TTMAP::ErrFEmpty( BOOL * const pfEmpty ) const
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	ERR err = ErrDispMove( m_sesid, m_tableid, JET_MoveFirst, NO_GRBIT );

	*pfEmpty = fFalse;
	if( JET_errNoCurrentRecord == err )
		{
		*pfEmpty = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

#ifdef DEBUG
	if( !*pfEmpty )
		{
		ULONG ulKey;
		ULONG cbActual;
		const ERR errT = ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidKey,
			&ulKey,
			sizeof( ulKey ),
			&cbActual,
			NO_GRBIT,
			NULL );
		}
#endif	//	DEBUG

HandleError:
	return err;
	}


//  ================================================================
TTARRAY::TTARRAY( const ULONG culEntries, const ULONG ulDefault ) :
//  ================================================================
	m_culEntries( culEntries ),
	m_ulDefault( ulDefault ),
	m_ppib( ppibNil ),
	m_pfucb( pfucbNil ),
	m_pgnoFirst( pgnoNull ),
	m_rgbitInit( NULL )
	{
	}


//  ================================================================
TTARRAY::~TTARRAY()
//  ================================================================
	{
	//  delete our bit-map of init space if it exists

	if ( m_rgbitInit )
		{
		OSMemoryPageFree( m_rgbitInit );
		m_rgbitInit = NULL;
		}
		
	//  delete the dummy table if it exists
	
	if ( m_pfucb != pfucbNil )
		{
		CallS( ErrFILECloseTable( m_ppib, m_pfucb ) );
		m_pfucb = pfucbNil;
		}

	//  close our session

	if ( m_ppib != ppibNil )
		{
		PIBEndSession( m_ppib );
		m_ppib = ppibNil;
		}
	}


DWORD TTARRAY::s_cTTArray = 0;

//  ================================================================
ERR TTARRAY::ErrInit( INST * const pinst )
//  ================================================================
	{
	ERR					err						= JET_errSuccess;
	CHAR   				szName[JET_cbNameMost];
	JET_TABLECREATE2	tablecreate;

	//  get a session

	Call( ErrPIBBeginSession( pinst, &m_ppib, procidNil, fFalse ) );

	//	create a dummy table large enough to contain all the elements of the array
	//  as well as the FDP

	DWORD cpg;
	cpg = sizeof( DWORD ) * m_culEntries / g_cbPage + 1;

	sprintf( szName, "TTArray%d", AtomicIncrement( (long*)&s_cTTArray ) );

	tablecreate.cbStruct			= sizeof( JET_TABLECREATE2 );
	tablecreate.szTableName			= szName;
	tablecreate.szTemplateTableName	= NULL;
	tablecreate.ulPages				= cpg + 16;
	tablecreate.ulDensity			= 100;
	tablecreate.rgcolumncreate		= NULL;
	tablecreate.cColumns			= 0;
	tablecreate.rgindexcreate		= NULL;
	tablecreate.cIndexes			= 0;
	tablecreate.szCallback			= NULL;
	tablecreate.cbtyp				= JET_cbtypNull;
	tablecreate.grbit				= NO_GRBIT;
	tablecreate.tableid				= JET_TABLEID( pfucbNil );
	tablecreate.cCreated			= 0;

	Call( ErrFILECreateTable( m_ppib, pinst->m_mpdbidifmp[ dbidTemp ], &tablecreate ) );
	m_pfucb = (FUCB*)( tablecreate.tableid );
	Assert( m_pfucb != pfucbNil );
	Assert( tablecreate.cCreated == 1 );

	m_pgnoFirst	= m_pfucb->u.pfcb->PgnoFDP() + 16;

	//  issue a preread for up to the first 512 pages of the map file

	BFPrereadPageRange( m_pfucb->ifmp, m_pgnoFirst, min( 512, cpg ) );

	//  allocate a bit-map to represent init space in the array.  the array will
	//  be pre-initialized to zero by the OS
	//
	//  NOTE:  the largest possible amount of memory allocated will be 512KB

	if ( !( m_rgbitInit = (ULONG*)PvOSMemoryPageAlloc( cpg / 8 + 1, NULL ) ) )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

HandleError:
	return err;
	}


//  ================================================================
VOID TTARRAY::BeginRun( PIB* const ppib, RUN* const prun )
//  ================================================================
	{
	//  nop
	}

//  ================================================================
VOID TTARRAY::EndRun( PIB* const ppib, RUN* const prun )
//  ================================================================
	{
	//  unlatch existing page if any

	if ( prun->pgno != pgnoNull )
		{
		if ( prun->fWriteLatch )
			{
			BFWriteUnlatch( &prun->bfl );
			}
		else
			{
			BFRDWUnlatch( &prun->bfl );
			}
		prun->pgno = pgnoNull;
		}
	}

//  ================================================================
ERR TTARRAY::ErrSetValue( PIB * const ppib, const ULONG ulEntry, const ULONG ulValue, RUN* const prun )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( ulEntry >= m_culEntries )
		{
		return ErrERRCheck( JET_errRecordNotFound );
		}
		
	//  compute the pgno that this entry lives on

	const PGNO pgno = m_pgnoFirst + ulEntry / ( g_cbPage / sizeof( DWORD ) );

	//  we do not already have this page latched

	RUN runT;
	runT.pgno			= pgnoNull;
	runT.bfl.pv			= NULL;
	runT.bfl.dwContext	= NULL;
	runT.fWriteLatch 	= fFalse;
	RUN * const  prunT = prun ? prun : &runT;

	if ( prunT->pgno != pgno )
		{
		//  unlatch existing page if any

		if ( prunT->pgno != pgnoNull )
			{
			if ( prunT->fWriteLatch )
				{
				BFWriteUnlatch( &prunT->bfl );
				}
			else
				{
				BFRDWUnlatch( &prunT->bfl );
				}
			prunT->pgno 		= pgnoNull;
			prunT->fWriteLatch 	= fFalse;
			}

		//  latch the new page

		Call( ErrBFRDWLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
		prunT->pgno 		= pgno;
		prunT->fWriteLatch 	= fFalse;
		}

	if( fFalse == prunT->fWriteLatch )
		{
		CallS( ErrBFUpgradeRDWLatchToWriteLatch( &prunT->bfl ) );
		prunT->fWriteLatch 	= fTrue;
		}

	//  the page is not yet init

	if ( !FPageInit( pgno ) )
		{
		//  init the page

		DWORD iEntryMax;
		iEntryMax = g_cbPage / sizeof( DWORD );
		
		DWORD iEntry;
		for ( iEntry = 0; iEntry < iEntryMax; iEntry++ )
			{
			*( (ULONG*) prunT->bfl.pv + iEntry ) = m_ulDefault;
			}

		//  mark the page as init

		SetPageInit( pgno );
		}

	//  set the value on the page

	DWORD iEntry;
	iEntry = ulEntry % ( g_cbPage / sizeof( DWORD ) );
	
	*( (ULONG*) prunT->bfl.pv + iEntry ) = ulValue;

	BFDirty( &prunT->bfl );

HandleError:
	if ( runT.pgno != pgnoNull )
		{
		BFWriteUnlatch( &runT.bfl );
		}
	return err;
	}


//  ================================================================
ERR TTARRAY::ErrGetValue( PIB * const ppib, const ULONG ulEntry, ULONG * const pulValue, RUN* const prun ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( ulEntry >= m_culEntries )
		{
		*pulValue = m_ulDefault;
		return JET_errSuccess;
		}

	//  compute the pgno that this entry lives on

	const PGNO pgno = m_pgnoFirst + ulEntry / ( g_cbPage / sizeof( DWORD ) );

	//  we do not already have this page latched

	RUN runT;
	runT.pgno			= pgnoNull;
	runT.bfl.pv			= NULL;
	runT.bfl.dwContext	= NULL;
	runT.fWriteLatch 	= fFalse;
	RUN * const  prunT = prun ? prun : &runT;

	if ( prunT->pgno != pgno )
		{
		//  unlatch existing page if any

		if ( prunT->pgno != pgnoNull )
			{
			if ( prunT->fWriteLatch )
				{
				BFWriteUnlatch( &prunT->bfl );
				}
			else
				{
				BFRDWUnlatch( &prunT->bfl );
				}
			prunT->pgno 		= pgnoNull;
			prunT->fWriteLatch 	= fFalse;
			}

		//  latch the new page
		//  if we are not part of a run, just use a read-latch for speed and concurrency
		//  use we are part of a run use a RDW latch for consistency, but still allow reads

		if( prun )
			{
			Call( ErrBFRDWLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
			prunT->fWriteLatch 	= fFalse;
			}
		else
			{
			Assert( prunT == &runT );
			Call( ErrBFReadLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
			prunT->fWriteLatch 	= fFalse;
			}
			
		prunT->pgno = pgno;
		}

	//  this page is initialized

	if ( FPageInit( pgno ) )
		{
		//  retrieve the value from the page

		DWORD iEntry;
		iEntry = ulEntry % ( g_cbPage / sizeof( DWORD ) );
		
		*pulValue = *( (ULONG*) prunT->bfl.pv + iEntry );
		}

	//  this page is not initialized

	else
		{
		//  the answer is the default value

		*pulValue = m_ulDefault;
		}

HandleError:
	if ( runT.pgno != pgnoNull )
		{
		BFReadUnlatch( &runT.bfl );
		}
	return err;
	}


//  ================================================================
BOOL TTARRAY::FPageInit( const PGNO pgno ) const
//  ================================================================
	{
	const ULONG iPage 	= pgno - m_pgnoFirst;
	const ULONG iul		= iPage / 32;
	const ULONG ibit	= iPage % 32;
	
	return ( m_rgbitInit[ iul ] & ( 1 << ibit ) ) != 0;
	}

	
//  ================================================================
VOID TTARRAY::SetPageInit( const PGNO pgno )
//  ================================================================
	{
	Assert( !FPageInit( pgno ) );
	
	const ULONG iPage 	= pgno - m_pgnoFirst;
	const ULONG iul		= iPage / 32;
	const ULONG ibit	= iPage % 32;

	ULONG ulBIExpected;
	ULONG ulBI;
	ULONG ulAI;
	do	{
		ulBIExpected = m_rgbitInit[ iul ];
		ulAI = ulBIExpected | ( 1 << ibit );

		ulBI = AtomicCompareExchange( (long*)&m_rgbitInit[ iul ], ulBIExpected, ulAI );
		}
	while ( ulBI != ulBIExpected );

	Assert( FPageInit( pgno ) );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\server\makefile.inc ===
$(O)\esent_noalias.lib : esent.def $(IMPLIB_DEPEND) makefile.inc
    cl /nologo /c /DNO_DEF_ALIAS /EP /Tc esent.def > $(O)\esent_noalias.def
    lib -out:$@ -nod -nologo -def:$(O)\esent_noalias.def -machine:IX86 $(IMPLIB_DEPEND)
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\ds\ese98\src\ese\_log\log.cxx ===
#include "std.hxx"
#include <ctype.h>

//	Source Insight users:
//	------------------
//	If you get parse errors with this file, open up C.tom in your Source
//	Insight directory, comment out the "TRY try {" line with a semi-colon,
//	and then add a line with "PERSISTED" only.
//
//	If you don't use Source Insight, you are either a fool or else there is
//	a better editor by the time you read this.

//	D O C U M E N T A T I O N ++++++++++++++++++++++++++++
//
//	FASTFLUSH Physical Logging Overview
//	==========================
//	For information on how the FASTFLUSH Physical Logging works, please see
//	ese\doc\ESE Physical Logging.doc.
//
//	Asynchronous Log File Creation Overview
//	============================
//	Background
//	----------
//	Before asynchronous log file creation, ESE would stall in ErrLGNewLogFile()
//	when it would create and format a new 5MB log file (formatting means
//	applying a special signature pattern to the file which is used to determine
//	if corruption has occurred; log file size is settable via a Jet system
//	parameter). 5MB of I/O isn't that fast, especially considering that NTFS
//	makes extending I/Os synchronous (thus a maximum of 1 outstanding I/O),
//	plus each of the 1MB I/Os causes a write to the MFT.
//
//	Solution
//	-------
//	Once ErrLGNewLogFile() is about to return (since it has completed setting
//	up the next log file), we should create edbtmp.log immediately (or rename
//	an archived edb%05X.log file in the case of circular logging). Then we set
//	a "TRIGGER" for the first asynchronous 1MB formatting I/O to start once
//	we have logged 512K to the log buffer.
//
//	What is this TRIGGER about?
//	-------------------------
//	One way we could have done this is just to immediately start a 1MB
//	asynchronous formatting I/O, then once it finishes, issue another one
//	immediately. Unfortunately back-to-back 1MB I/Os basically consume
//	the disk and keep it busy -- thus no logging would be able to be done
//	to edb.log since we'd be busy using the disk to format edbtmp.log!
//	(I actually determined this experimentally with a prototype).
//
//	The trigger allows us to throttle I/O to the edbtmp.log that we're
//	formatting, so that we log 1MB of data to the in-memory log buffer, then
//	write 1MB to edbtmp.log, then... etc.
//
//	The trigger is handled such that once we pass the trigger, we will
//	issue the asynch formatting I/O, then once it completes AND we pass
//	the next trigger, we will issue the next, etc. If we reach ErrLGNewLogFile()
//	before edbtmp.log is completely formatted (or there is currently an
//	outstanding I/O), we will wait in ErrLGNewLogFile() for any asynch I/O
//	to complete, then we format the rest of the file if necessary.
//
//	Why is the policy based on how much we've logged to the log buffer
//	and not how much we've flushed to edb.log?
//	--------------------------------------------------------------
//	In the case of many lazy commit transactions, waiting for a log flush
//	may be a long time with a large log buffer (i.e. there are some
//	performance recommendations that Exchange Servers should
//	have log buffers set to `9000' which is 9000 * 512 bytes = ~4.4MB;
//	BTW, do not set the log buffers equal to the log file size or greater or
//	you will get some bizarre problems). So in this case, we should try to
//	format edbtmp.log while those lazy commit transactions are coming in,
//	especially since they're not causing edb.log to be used at all.
//
//	Why set the trigger to 512K instead of issuing the first I/O immediately?
//	----------------------------------------------------------------
//	In ErrLGNewLogFile() we're doing a lot with files, deleting unnecessary
//	log files in some circumstances, creating files, renaming files, etc. In other
//	words, this is going to take a while since we have to wait for NTFS to
//	update metadata by writing to the NTFS transaction log, at a minimum.
//	While we're waiting for NTFS to do this stuff, it is pretty likely that some
//	clients are doing transactions and they are now waiting for their commit
//	records to hit the disk -- in other words, they are waiting for their stuff
//	to be flushed to edb.log.
//
//	If we write 1MB to edbtmp.log now, those clients will have to wait until
//	the 1MB hits the disk before their records hit the disk.
//
//	Thus, this is why we wait for 512K to be logged to the log buffer -- as
//	a heuristic wait.
//
//	What about error handling with this asynch business?
//	-----------------------------------------------
//	If we get an error creating the new edbtmp.log, we'll handle the error
//	later when ErrLGNewLogFile() is next called. The same with errors from
//	any asynch I/O.
//
//	We handle all the weird errors with disk-full and using reserve log files, etc.



#ifdef LOGPATCH_UNIT_TEST

#error "LOGPATCH_UNIT_TEST is broken because of spenlow's asynch log file creation (Dec 2000)."

//	Should investigate using a RAM disk to reduce LOGPATCH_UNIT_TEST code.

#endif

//	constants

const LGPOS		lgposMax = { 0xffff, 0xffff, 0x7fffffff };
const LGPOS		lgposMin = { 0x0,  0x0,  0x0 };

#ifdef DEBUG
CCriticalSection g_critDBGPrint( CLockBasicInfo( CSyncBasicInfo( szDBGPrint ), rankDBGPrint, 0 ) );
#endif  //  DEBUG


//  monitoring statistics
//
PM_CEF_PROC LLGWriteCEFLPv;
PERFInstanceG<> cLGWrite;
long LLGWriteCEFLPv( long iInstance, void *pvBuf )
	{
	cLGWrite.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGBytesWrittenCEFLPv;
PERFInstanceGlobal<> cLGBytesWritten;
long LLGBytesWrittenCEFLPv( long iInstance, void *pvBuf )
	{
	cLGBytesWritten.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGUsersWaitingCEFLPv;
PERFInstanceG<> cLGUsersWaiting;
long LLGUsersWaitingCEFLPv( long iInstance, void *pvBuf )
	{
	cLGUsersWaiting.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGRecordCEFLPv;
PERFInstanceG<> cLGRecord;
long LLGRecordCEFLPv(long iInstance,void *pvBuf)
{
	cLGRecord.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCapacityFlushCEFLPv;
PERFInstanceG<> cLGCapacityFlush;
long LLGCapacityFlushCEFLPv(long iInstance,void *pvBuf)
{
	cLGCapacityFlush.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCommitFlushCEFLPv;
PERFInstanceG<> cLGCommitFlush;
long LLGCommitFlushCEFLPv(long iInstance,void *pvBuf)
{
	cLGCommitFlush.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGFlushCEFLPv;
long LLGFlushCEFLPv(long iInstance, void *pvBuf)
{
	if ( NULL != pvBuf )
		{
		*(LONG*)pvBuf = cLGCapacityFlush.Get( iInstance ) + cLGCommitFlush.Get( iInstance );
		}
	return 0;
}

PM_CEF_PROC LLGStallCEFLPv;
PERFInstanceG<> cLGStall;
long LLGStallCEFLPv(long iInstance,void *pvBuf)
{
	cLGStall.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCheckpointDepthCEFLPv;
PERFInstance<QWORD> cLGCheckpoint;
PERFInstance<QWORD> cLGRecordOffset;
LONG LLGCheckpointDepthCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	if ( pvBuf )
		{
		//	handle too deep checkpoint
		QWORD counter = cLGRecordOffset.Get( iInstance ) - cLGCheckpoint.Get( iInstance );
		if ( counter < QWORD(LONG_MAX) )
			{
			*(LONG*)pvBuf = (LONG)counter;
			}
		else
			{
			//	check the high bit
			*(LONG*)pvBuf = LONG_MAX;
			}
		}
	return 0;
	}
	
LOG::LOG( INST *pinst ) :
	m_pinst( pinst ),
	m_fLogInitialized( fFalse ),
	m_fLogDisabled( fFalse ),
	m_fLogDisabledDueToRecoveryFailure( fFalse ),
	m_fNewLogRecordAdded( fFalse ),
	m_fLGNoMoreLogWrite( fFalse ),
	m_ls( lsNormal ),
	m_fLogSequenceEnd( fFalse ),
	m_fRecovering( fFalse ),
	m_fRecoveringMode( fRecoveringNone ),
	m_fHardRestore( fFalse ),
	m_fRestoreMode( fRecoveringNone ),
	m_lgposSnapshotStart( lgposMin ),
	m_fSnapshotMode( fSnapshotNone ),
	m_fLGCircularLogging( g_fLGCircularLogging ),
	m_fLGFMPLoaded( fFalse ),
#ifdef UNLIMITED_DB
	m_pbLGDbListBuffer( NULL ),
	m_cbLGDbListBuffer( 0 ),
	m_cbLGDbListInUse( 0 ),
	m_cLGAttachments( 0 ),
	m_fLGNeedToLogDbList( fFalse ),
#endif
	m_fSignLogSet( fFalse ),
	m_fLGIgnoreVersion( g_fLGIgnoreVersion ),
	m_pfapiLog( NULL ),
	//	Asynchronous log file creation
	m_fCreateAsynchLogFile( g_fLogFileCreateAsynch ),
	m_pfapiJetTmpLog( NULL ),
	m_fCreateAsynchResUsed( fFalse ),
	m_errCreateAsynch( JET_errSuccess ),
	m_asigCreateAsynchIOCompleted( CSyncBasicInfo( _T( "LOG::m_asigCreateAsynchIOCompleted" ) ) ),
	m_ibJetTmpLog( 0 ),
	m_lgposCreateAsynchTrigger( lgposMax ),
	m_szLogCurrent( NULL ),
	m_plgfilehdr( NULL ),
	m_plgfilehdrT( NULL ),
	m_pbLGBufMin( NULL ),
	m_pbLGBufMax( NULL ),
	m_cbLGBuf( 0 ),
	m_pbEntry( NULL ),
	m_pbWrite( NULL ),
	m_isecWrite( 0 ),
	m_lgposLogRec( lgposMin ),
	m_lgposToFlush( lgposMin ),
	m_lgposMaxFlushPoint( lgposMin ),
	m_pbLGFileEnd( pbNil ),
	m_isecLGFileEnd( 0 ),
	m_lgposFullBackup( lgposMin ),
	m_lgposLastChecksum( lgposMin ),
	m_pbLastChecksum( pbNil ),
	// XXX
	// quick hack to always start writing in safe-safe mode
	// in regards to killing a shadow and killing the data sector.
	m_fHaveShadow( fTrue ),
	m_lgposIncBackup( lgposMin ),
	m_cbSec( 0 ),
	m_cbSecVolume( 0 ),
	m_csecHeader( 0 ),
	m_fLGFlushWait( 2 ),
	m_fLGFailedToPostFlushTask( fFalse ),
	m_asigLogFlushDone( CSyncBasicInfo( _T( "LOG::m_asigLogFlushDone" ) ) ),
	m_critLGFlush( CLockBasicInfo( CSyncBasicInfo( szLGFlush ), rankLGFlush, CLockDeadlockDetectionInfo::subrankNoDeadlock ) ),
	m_critLGBuf( CLockBasicInfo( CSyncBasicInfo( szLGBuf ), rankLGBuf, 0 ) ),
	m_critLGTrace( CLockBasicInfo( CSyncBasicInfo( szLGTrace ), rankLGTrace, 0 ) ),
	m_critLGWaitQ( CLockBasicInfo( CSyncBasicInfo( szLGWaitQ ), rankLGWaitQ, 0 ) ),
	m_critLGResFiles( CLockBasicInfo( CSyncBasicInfo( szLGResFiles ), rankLGResFiles, 0 ) ),
	m_ppibLGFlushQHead( ppibNil ),
	m_ppibLGFlushQTail( ppibNil ),
	m_cLGWrapAround( 0 ),
	m_pcheckpoint( NULL ),
	m_critCheckpoint( CLockBasicInfo( CSyncBasicInfo( szCheckpoint ), rankCheckpoint, CLockDeadlockDetectionInfo::subrankNoDeadlock ) ),
	m_fDisableCheckpoint( fTrue ),
	m_fReplayingReplicatedLogFiles( fFalse ),
#ifdef IGNORE_BAD_ATTACH
	m_fReplayingIgnoreMissingDB( fFalse ),
	m_rceidLast( rceidMin ),
#endif // IGNORE_BAD_ATTACH
	m_fAbruptEnd( fFalse ),
	m_plread( pNil ),
	m_lgposPbNextPreread( lgposMin ),
	m_lgposLastChecksumPreread( lgposMin ),
	m_cPagesPrereadInitial( 0 ),
	m_cPagesPrereadThreshold( 0 ),
	m_cPagesPrereadAmount( 0 ),
	m_cPageRefsConsumed( 0 ),
	m_fPreread( fFalse ),
	m_fAfterEndAllSessions( fFalse ),
	m_fLastLRIsShutdown( fFalse ),
	m_fNeedInitialDbList( fFalse ),
	m_rgcppib( NULL ),
	m_pcppibAvail( NULL ),
	m_ccppib( 0 ),
	m_ptablehfhash( NULL ),
	m_critBackupInProgress( CLockBasicInfo( CSyncBasicInfo( "BackupInProcess" ), rankBackupInProcess, 0 ) ),
	m_fBackupInProgress( fFalse ),
	m_fBackupStatus( backupStateNotStarted ),
	m_ppibBackup( ppibNil ),
	m_rgppatchlst( NULL ),
	m_rgrstmap( NULL ),
	m_irstmapMac( 0 ),
	m_fScrubDB( fFalse ),
	m_pscrubdb( NULL ),
	m_ulSecsStartScrub( 0 ),
	m_dbtimeLastScrubNew( 0 ),
	m_crhfMac( 0 ),
	m_cNOP( 0 ),
	m_fExternalRestore( fFalse ),
	m_lGenLowRestore( 0 ),
	m_lGenHighRestore( 0 ),
	m_lGenHighTargetInstance( 0 ),
	m_fDumppingLogs( fFalse ),
	m_fDeleteOldLogs( g_fDeleteOldLogs ),
	m_fDeleteOutOfRangeLogs ( g_fDeleteOutOfRangeLogs )
#ifdef DEBUG
	,
	m_lgposLastLogRec( lgposMin ),
	m_fDBGFreezeCheckpoint( fFalse ),
	m_fDBGTraceLog( fFalse ),
	m_fDBGTraceLogWrite( fFalse ),
	m_fDBGTraceRedo( fFalse ),
	m_fDBGTraceBR( fFalse ),
	m_fDBGNoLog( fFalse ),
	m_cbDBGCopied( 0 )
#endif
	,
	m_pcheckpointDeleted( NULL )
	{
	//	log perf counters
	cLGWrite.Clear( m_pinst );
	cLGUsersWaiting.Clear( m_pinst );
	cLGCapacityFlush.Clear( m_pinst );
	cLGCommitFlush.Clear( m_pinst );
	cLGStall.Clear( m_pinst );
	cLGRecord.Clear( m_pinst );
	cLGBytesWritten.Clear( m_pinst );
	cLGCheckpoint.Clear( m_pinst );
	cLGRecordOffset.Clear( m_pinst );
	
	INT	irhf;
	for ( irhf = 0; irhf < crhfMax; irhf++ )
		{
		m_rgrhf[irhf].fInUse = fFalse;
		m_rgrhf[irhf].pLogVerifier = pNil;
		m_rgrhf[irhf].pSLVVerifier = pNil;
		m_rgrhf[irhf].pfapi = NULL;
		}

	strcpy( m_szBaseName, szBaseName );
	strcpy( m_szJet, szJet );
	strcpy( m_szJetLog, szJetLog );
	strcpy( m_szJetLogNameTemplate, szJetLogNameTemplate );
	strcpy( m_szJetTmp, szJetTmp );
	strcpy( m_szJetTmpLog, szJetTmpLog );

	//	store absolute path if possible
	Assert( pinstNil != m_pinst );
	if ( NULL == m_pinst
		|| NULL == m_pinst->m_pfsapi
		|| m_pinst->m_pfsapi->ErrPathComplete( g_szLogFilePath, m_szLogFilePath ) < 0 )
		{
		strcpy( m_szLogFilePath, g_szLogFilePath );
		}

	strcpy( m_szLogFileFailoverPath, g_szLogFileFailoverPath );
	strcpy( m_szRecovery, g_szRecovery );
	m_szRestorePath[0] = '\0';
	m_szNewDestination[0] = '\0';
	m_szTargetInstanceLogPath[0] = '\0';

	//	perform unit test for checksum code, on LE only.
	
	AssertRTL( TestChecksumBytes() );
	}

LOG::~LOG()
	{
	//	log perf counters
	cLGWrite.Clear( m_pinst );
	cLGUsersWaiting.Clear( m_pinst );
	cLGCapacityFlush.Clear( m_pinst );
	cLGCommitFlush.Clear( m_pinst );
	cLGStall.Clear( m_pinst );
	cLGRecord.Clear( m_pinst );
	cLGBytesWritten.Clear( m_pinst );
	cLGCheckpoint.Clear( m_pinst );
	cLGRecordOffset.Clear( m_pinst );
	}


VOID LOG::LGMakeLogName( CHAR *szLogName, const CHAR *szFName )
	{
	CallS( m_pinst->m_pfsapi->ErrPathBuild( m_szLogCurrent, szFName, szLogExt, szLogName ) );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err, const _TCHAR* const pszLogFile )
	{
	_TCHAR			szError[ 64 ];
	const _TCHAR*	rgpsz[] = { pszLogFile, szError };

	_stprintf( szError, _T( "%i (0x%08x)" ), err, err );

	UtilReportEvent(	eventError,
						LOGGING_RECOVERY_CATEGORY,
						msgid,
						sizeof( rgpsz ) / sizeof( rgpsz[ 0 ] ),
						rgpsz,
						0,
						NULL,
						m_pinst );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err )
	{
	_TCHAR	szAbsPath[ IFileSystemAPI::cchPathMax ];
	
	if ( m_pinst->m_pfsapi->ErrPathComplete( m_szLogName, szAbsPath ) < JET_errSuccess )
		{
		_tcscpy( szAbsPath, m_szLogName );
		}
	LGReportError( msgid, err, szAbsPath );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err, IFileAPI* const pfapi )
	{
	_TCHAR		szAbsPath[ IFileSystemAPI::cchPathMax ];
	const ERR	errPath = pfapi->ErrPath( szAbsPath );
	if ( errPath < JET_errSuccess )
		{
		//	If we get an error here, this is the best we can do.

		_stprintf(	szAbsPath,
					_T( "%s [%i (0x%08x)]" ),
					m_szLogName,	// closest thing we have to pfapi's path
					errPath,
					errPath );
		}
	LGReportError( msgid, err, szAbsPath );
	}


VOID SIGGetSignature( SIGNATURE *psign )
	{
	INT cbComputerName;

	// init the rand seed with a per thread value, this will prevent
	// the generation of the same sequence in 2 different threads
	// (the seed is per thread if compiled with multithreaded libraries as we are)
	// Also put time in the seek to don't get the same random 3 numbers on all 
	// signature generation.
	srand( DwUtilThreadId() + TickOSTimeCurrent() );

	LGIGetDateTime( &psign->logtimeCreate );
	psign->le_ulRandom = rand() + rand() + rand() + TickOSTimeCurrent();
//	(VOID) GetComputerName( psign->szComputerName, &cbComputerName );
	cbComputerName = 0;
	memset( psign->szComputerName + cbComputerName,
		0,
		sizeof( psign->szComputerName ) - cbComputerName );
	}

BOOL FSIGSignSet( const SIGNATURE *psign )
	{
	SIGNATURE	signNull;

	memset( &signNull, 0, sizeof(SIGNATURE) );
	return ( 0 != memcmp( psign, &signNull, sizeof(SIGNATURE) ) );
	}


VOID LOG::GetLgpos( BYTE *pb, LGPOS *plgpos )
	{
	BYTE	*pbAligned;
	INT		csec;
	INT		isecCurrentFileEnd;

#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif

	//	m_pbWrite is always aligned
	//
	Assert( m_pbWrite != NULL );
	Assert( m_pbWrite == PbSecAligned( m_pbWrite ) );
	Assert( m_isecWrite >= m_csecHeader );

	// pb is a pointer into the log buffer, so it should be valid.
	Assert( pb >= m_pbLGBufMin );
	Assert( pb < m_pbLGBufMax );
	// m_pbWrite should also be valid since we're using it for comparisons here
	Assert( m_pbWrite >= m_pbLGBufMin );
	Assert( m_pbWrite < m_pbLGBufMax );

	pbAligned = PbSecAligned( pb );

	Assert( pbAligned >= m_pbLGBufMin );
	Assert( pbAligned < m_pbLGBufMax );

	plgpos->ib = USHORT( pb - pbAligned );
	if ( pbAligned < m_pbWrite )
		{
		csec = m_csecLGBuf - ULONG( m_pbWrite - pbAligned ) / m_cbSec;
		}
	else
		{
		csec = ULONG( pbAligned - m_pbWrite ) / m_cbSec;
		}

	plgpos->isec = USHORT( m_isecWrite + csec );

	isecCurrentFileEnd = m_isecLGFileEnd ? m_isecLGFileEnd : m_csecLGFile - 1;
	if ( plgpos->isec >= isecCurrentFileEnd )
		{
		plgpos->isec = (WORD) ( plgpos->isec - isecCurrentFileEnd + m_csecHeader );
		plgpos->lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration + 1;
		}
	else
		{
		plgpos->lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;
		}

	return;
	}



//********************* INIT/TERM **************************
//**********************************************************

ERR LOG::ErrLGInitSetInstanceWiseParameters( IFileSystemAPI *const pfsapi )
	{
	ERR		err;
	CHAR  	rgchFullName[IFileSystemAPI::cchPathMax];

	//	verify the log path

	if ( pfsapi->ErrPathComplete( m_szLogFilePath, rgchFullName ) == JET_errInvalidPath )
		{
		const CHAR *szPathT[1] = { m_szLogFilePath };

		UtilReportEvent( eventError, LOGGING_RECOVERY_CATEGORY, FILE_NOT_FOUND_ERROR_ID, 1, szPathT );

		return ErrERRCheck( JET_errFileNotFound );
		}

	//	get the atomic write size

	CallR( pfsapi->ErrFileAtomicWriteSize( rgchFullName, (DWORD*)&m_cbSecVolume ) );
	m_cbSec = m_cbSecVolume;
	Assert( m_cbSec >= 512 );
	Assert( sizeof( LGFILEHDR ) % m_cbSec == 0 );
	m_csecHeader = sizeof( LGFILEHDR ) / m_cbSec;

	//	log file size must be at least a
	//	header and at least 2 pages of log data.
	//	Prevent overflow by limiting parmater values.
	//
	m_csecLGFile = CsecLGIFromSize( m_pinst->m_lLogFileSize );
		
	return JET_errSuccess;
	}


BOOL LOG::FLGCheckParams()
	{
	const LONG	csecReserved	= m_csecHeader + 1 + 1;	// +1 for shadow sector, +1 for ultra-safety

	//	ErrSetSystemParameter() ensures a minimum log buffer size and logfile size
	Assert( m_pinst->m_lLogBuffers >= lLogBufferMin );
	Assert( m_csecLGFile * m_cbSec >= lLogFileSizeMin * 1024 );
	Assert( lLogBufferMin * m_cbSec < lLogFileSizeMin * 1024 );

	//	if user set log buffer size greater than logfile size, consider it an error
	if ( m_pinst->m_lLogBuffers > m_csecLGFile )
		{
		char szSize[32];
		_itoa( ( ( m_csecLGFile - csecReserved ) * m_cbSec ) / 1024, szSize, 10 );

		char szBuf[32];
		_itoa( m_pinst->m_lLogBuffers, szBuf, 10 );

		const char *rgsz[] = { szBuf, szSize };
		UtilReportEvent( eventError,
				SYSTEM_PARAMETER_CATEGORY,
				SYS_PARAM_LOGBUFFER_FILE_ERROR_ID, 
				2, 
				rgsz );

		return fFalse;
		}

	//	user specified a log buffer size less than or equal to the log file
	//	size, but we need to sanitize the value to ensure it meets the
	//	following requirements:
	//		- less than or equal to logfilesize-csecReserved (this is
	//		  because we must guarantee that a given log flush will never
	//		  span more than one log file, given that the most amount of
	//		  data we will write to a log file is logfilesize-csecReserved)
	//		- a multiple of the system allocation granularity
	//	NOTE: This fix supercedes the (incomplete) fix AFOXMAN_FIX_148537
	m_pinst->m_lLogBuffers = min( m_pinst->m_lLogBuffers, m_csecLGFile - csecReserved );
	Assert( m_pinst->m_lLogBuffers >= lLogBufferMin );

	UINT	csecAligned	= CsecUtilAllocGranularityAlign( m_pinst->m_lLogBuffers, m_cbSec );
	if ( csecAligned > m_pinst->m_lLogBuffers )
		{
		csecAligned -= ( OSMemoryPageReserveGranularity() / m_cbSec );
		Assert( CsecUtilAllocGranularityAlign( csecAligned, m_cbSec ) == csecAligned );
		Assert( csecAligned < m_pinst->m_lLogBuffers );
		Assert( csecAligned >= lLogBufferMin );
		m_pinst->m_lLogBuffers = csecAligned;
		}
	else
		{
		Assert( csecAligned == m_pinst->m_lLogBuffers );
		}

	return fTrue;
	}


// Rounds up the number of buffers (sectors in the log buffer) to make the log
// buffer be a multiple of the OS memory allocation granularity (currently 64K).
// *ASSUMES* that the OS memory allocation granularity is a multiple of the
// sector size.

ULONG LOG::CsecUtilAllocGranularityAlign( const LONG lBuffers, const LONG cbSec )
	{
	const DWORD		dwPageReserveGranT	= ::OSMemoryPageReserveGranularity();

	Assert( lBuffers > 0 );
	Assert( cbSec > 0 );

	Assert( dwPageReserveGranT % cbSec == 0 );

	return ( ( lBuffers * cbSec - 1 ) / dwPageReserveGranT + 1 ) * dwPageReserveGranT / cbSec;
	}

// Deallocates any storage for the log buffer, if necessary.

void LOG::LGTermLogBuffers()
	{
	if ( !COSMemoryMap::FCanMultiMap() )
		{
		if ( m_pbLGBufMin )
			{
			OSMemoryPageFree( m_pbLGBufMin );
			m_pbLGBufMax = m_pbLGBufMin = NULL;
			}
		}
	else
		{

		//	free the log buffer

		if ( m_pbLGBufMin )
			{
			Assert( m_pbLGBufMax );
			m_osmmLGBuf.OSMMPatternFree();
			m_pbLGBufMin = pbNil;
			m_pbLGBufMax = pbNil;
			}

		//	term the memory map

		m_osmmLGBuf.OSMMTerm();
		}		
	}


ERR LOG::ErrLGInitLogBuffers( LONG lIntendLogBuffers )
	{
	ERR err = JET_errSuccess;
	
	m_critLGBuf.Enter();

	m_csecLGBuf = lIntendLogBuffers;

	Assert( m_csecLGBuf > 4 );
	//	UNDONE: enforce log buffer > a data page

	//	reset log buffer
	//
	// Kill any existing log buffer.
	LGTermLogBuffers();

	// Round up number of sectors in log buffer to make log buffer
	// a multiple of system memory allocation granularity
	// (and thus a multiple of sector size).
	m_csecLGBuf = CsecUtilAllocGranularityAlign( m_csecLGBuf, m_cbSec );
	Assert( m_csecLGBuf > 0 );
	Assert( m_cbSec > 0 );
	m_cbLGBuf = m_csecLGBuf * m_cbSec;
	Assert( m_cbLGBuf > 0 );

	// Log buffer must be multiple of sector size
	Assert( 0 == m_cbLGBuf % m_cbSec );
	// Log buffer must be multiple of memory allocation granularity
	Assert( 0 == m_cbLGBuf % OSMemoryPageReserveGranularity() );

	if ( !COSMemoryMap::FCanMultiMap() )
		{
		if ( m_pbLGBufMin )
			{
			OSMemoryPageFree( m_pbLGBufMin );
			m_pbLGBufMin = NULL;
			}

		m_pbLGBufMin = (BYTE *) PvOSMemoryPageAlloc( m_cbLGBuf * 2, NULL );
		if ( m_pbLGBufMin == NULL )
			{
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}
		m_pbLGBufMax = m_pbLGBufMin + m_cbLGBuf;
		}
	else
		{
		Assert( pbNil == m_pbLGBufMin );
		Assert( pbNil == m_pbLGBufMax );

		//	init the memory map

		COSMemoryMap::ERR errOSMM;
		errOSMM = m_osmmLGBuf.ErrOSMMInit();
		if ( COSMemoryMap::errSuccess != errOSMM )
			{
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}

		//	allocate the buffer

		errOSMM = m_osmmLGBuf.ErrOSMMPatternAlloc( m_cbLGBuf, 2 * m_cbLGBuf, (void**)&m_pbLGBufMin );
		if ( COSMemoryMap::errSuccess != errOSMM )
			{
			AssertSz(	COSMemoryMap::errOutOfBackingStore == errOSMM ||
						COSMemoryMap::errOutOfAddressSpace == errOSMM ||
						COSMemoryMap::errOutOfMemory == errOSMM, 
						"unexpected error while allocating memory pattern" );
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}
		Assert( m_pbLGBufMin );
		m_pbLGBufMax = m_pbLGBufMin + m_cbLGBuf;
		}

HandleError:
	if ( err < 0 )
		{
		LGTermLogBuffers();
		}

	m_critLGBuf.Leave();
	return err;
	}


#ifdef UNLIMITED_DB
VOID LOG::LGIInitDbListBuffer()
	{
	if ( !m_fLogInitialized )
		return;

	//	place sentinel in ATTACHINFO buffer;

	LRDBLIST*	const plrdblist		= (LRDBLIST *)m_pbLGDbListBuffer;
	plrdblist->lrtyp = lrtypDbList;
	plrdblist->ResetFlags();
	plrdblist->SetCAttachments( 0 );
	plrdblist->SetCbAttachInfo( 1 );
	plrdblist->rgb[0] = 0;

	m_cbLGDbListInUse = sizeof(LRDBLIST) + 1;
	m_cLGAttachments = 0;
	m_fLGNeedToLogDbList = fFalse;
	}
#endif	

CTaskManager *g_plogtaskmanager = NULL;
VOID LOG::LGSignalFlush( void )
	{
	if ( 0 == AtomicCompareExchange( (LONG *)&m_fLGFlushWait, 0, 1 ) )
		{
		Assert( NULL != g_plogtaskmanager );
		if ( JET_errSuccess > g_plogtaskmanager->ErrTMPost( LGFlushLog, 0, (DWORD_PTR)this ) )
			{
			if ( !AtomicExchange( (LONG *)&m_fLGFailedToPostFlushTask, fTrue ) )
				{
				AtomicIncrement( (LONG *)&m_cLGFailedToPost );
				}
				
			if ( 2 == AtomicCompareExchange( (LONG *)&m_fLGFlushWait, 1, 0 ) )
				{
				m_asigLogFlushDone.Set();
				}
			}
		}
	}
	
ERR LOG::ErrLGSystemInit( void )
	{
	ERR err = JET_errSuccess;
	g_plogtaskmanager = new CTaskManager;

	if ( NULL == g_plogtaskmanager )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	else
		{
		err = g_plogtaskmanager->ErrTMInit( min( 8 * CUtilProcessProcessor(), 100 ), NULL, 2 * cmsecAsyncBackgroundCleanup, LGFlushAllLogs );
		if ( JET_errSuccess > err )
			{
			delete g_plogtaskmanager;
			g_plogtaskmanager = NULL;
			}
		}
	return err;
	}

VOID LOG::LGSystemTerm( void )
	{
	if ( NULL != g_plogtaskmanager )
		{
		g_plogtaskmanager->TMTerm();
		delete g_plogtaskmanager;
		}
	g_plogtaskmanager = NULL;
	}

VOID LOG::LGTasksTerm( void )
	{
Retry:
	LGSignalFlush();
	if ( 1 == AtomicExchange( (LONG *)&m_fLGFlushWait, 2 ) )
		{
		m_asigLogFlushDone.Wait();
		}
	m_msLGTaskExec.Partition();

	//	oops, not everything is flushed
	if ( m_fLGFailedToPostFlushTask )
		{
		LONG fStatus = AtomicExchange( (LONG *)&m_fLGFlushWait, 0 );
		Assert( 2 == fStatus );
		UtilSleep( cmsecWaitGeneric );
		goto Retry;
		}
	}

//
//  Initialize global variablas and threads for log manager.
//
ERR LOG::ErrLGInit( IFileSystemAPI *const pfsapi, BOOL *pfNewCheckpointFile, BOOL fCreateReserveLogs )
	{
	ERR		err;
	LONG	lGenMax;

	if ( m_fLogInitialized )
		return JET_errSuccess;

	Assert( m_fLogDisabled == fFalse );

	cLGUsersWaiting.Clear( m_pinst );

#ifdef DEBUG
	 m_lgposLastLogRec = lgposMin;
#endif // DEBUG

#ifdef DEBUG
	AssertLRSizesConsistent();
#endif
	
#ifdef DEBUG
	{
	CHAR	*sz;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACELOG" ) ) ) != NULL )
		{
		m_fDBGTraceLog = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceLog = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACELOGWRITE" ) ) ) != NULL )
		{
		m_fDBGTraceLogWrite = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceLogWrite = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "FREEZECHECKPOINT" ) ) ) != NULL )
		{
		m_fDBGFreezeCheckpoint = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGFreezeCheckpoint = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACEREDO" ) ) ) != NULL )
		{
		m_fDBGTraceRedo = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceRedo = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACEBR" ) ) ) != NULL )
		{
		m_fDBGTraceBR = atoi( sz );
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceBR = 0;

	if ( ( sz = GetDebugEnvValue ( _T( "IGNOREVERSION" ) ) ) != NULL )
		{
		m_fLGIgnoreVersion = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		g_fLGIgnoreVersion = fFalse;
	}
#endif

	//	assuming everything will work out
	//
	m_fLGNoMoreLogWrite = fFalse;

	//	log file header must be aligned on correct boundary for device;
	//	which is 16-byte for MIPS and 512-bytes for at least one NT
	//	platform.
	//
	if ( !( m_plgfilehdr = (LGFILEHDR *)PvOSMemoryPageAlloc( sizeof(LGFILEHDR) * 2, NULL ) ) )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	m_plgfilehdrT = m_plgfilehdr + 1;


#ifdef UNLIMITED_DB
	const ULONG		cbDbListBufferDefault	= OSMemoryPageCommitGranularity();
	if ( !( m_pbLGDbListBuffer = (BYTE *)PvOSMemoryPageAlloc( cbDbListBufferDefault, NULL ) ) )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	m_cbLGDbListBuffer = cbDbListBufferDefault;

	LGIInitDbListBuffer();
#endif	

	//	the log buffer MUST be SMALLER than any log file so that we only ever 
	//	have 2 log files open in the buffer at any given time;  the current
	//	log file manager only has one m_pbLGFileEnd to designate the end of 
	//	the current log in the buffer -- it cannot represent the ends of 2 log files

	Assert( m_pinst->m_lLogBuffers < m_csecLGFile );

	// always start with new buffer here!

	CallJ( ErrLGInitLogBuffers( m_pinst->m_lLogBuffers ), FreeLGFileHdr );

	//	Initialize trace mechanism

	m_pttFirst = NULL;
	m_pttLast = NULL;

#ifndef LOGPATCH_UNIT_TEST

	m_fLGFlushWait	= 0;

	CallJ( ErrLGICheckpointInit( pfsapi, pfNewCheckpointFile ), StopFlushThread );

#endif	//	!LOGPATCH_UNIT_TEST

	memset( &m_signLog, 0, sizeof( m_signLog ) );
	m_fSignLogSet = fFalse;

	if ( fCreateReserveLogs )
		{

		//	create the reserve logs and setup the reserve-log state

		m_critLGResFiles.Enter();
		CHAR *szT = m_szLogCurrent;
		m_szLogCurrent = m_szLogFilePath;
		CallS( ErrLGICreateReserveLogFiles( pfsapi, fTrue ) );
		m_szLogCurrent = szT;
		m_critLGResFiles.Leave();
		}

	//	determine if we are in "log sequence end" mode

	(void)ErrLGIGetGenerationRange( pfsapi, m_szLogFilePath, NULL, &lGenMax );
	lGenMax += 1;	//	assume edb.log exists (if not, who cares -- if they hit this, they have to shutdown and wipe the log anyway)
	if ( lGenMax >= lGenerationMax )
		{
		m_critLGResFiles.Enter();
		Assert( !m_fLogSequenceEnd );
		m_fLogSequenceEnd = fTrue;
		m_critLGResFiles.Leave();
		}

	m_fLogInitialized = fTrue;

	return err;

#ifndef LOGPATCH_UNIT_TEST
StopFlushThread:

	LGTasksTerm();
#endif	//	!LOGPATCH_UNIT_TEST

FreeLGFileHdr:
	if ( m_plgfilehdr )
		{
		OSMemoryPageFree( m_plgfilehdr );
		}

	LGTermLogBuffers();
	
	return err;
	}


//	Terminates update logging.	Adds quit record to in-memory log,
//	flushes log buffer to disk, updates checkpoint and closes active
//	log generation file.  Frees buffer memory.
//
//	RETURNS	   JET_errSuccess, or error code from failing routine
//
ERR LOG::ErrLGTerm( IFileSystemAPI *const pfsapi, const BOOL fLogQuitRec )
	{
	ERR			err				= JET_errSuccess;
	LE_LGPOS	le_lgposStart;

	//	if logging has been initialized, terminate it!
	//
	if ( !m_fLogInitialized )
		return JET_errSuccess;

	if ( !fLogQuitRec
		|| m_fLGNoMoreLogWrite
		|| !m_pfapiLog )
		goto FreeResources;

	Assert( !m_fRecovering );
		
	//	last written sector should have been written during final flush
	//
	le_lgposStart = m_lgposStart;
	Call( ErrLGQuit( this, &le_lgposStart ) );


	// Keep doing synchronous flushes until all log data
	// is definitely flushed to disk. With FASTFLUSH, a single call to
	// ErrLGFlushLog() may not flush everything in the log buffers.
	Call( ErrLGWaitAllFlushed( pfsapi ) );

#ifdef DEBUG
		{
		// verify that everything in the log buffers have been
		// flushed to disk.
		m_critLGFlush.Enter();
		m_critLGBuf.Enter();

		//	BUG X5:83888 
		//
		//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
		//		(we weren't seeing it because PbGetEndOfLogData() was pointing AT the LRCK instead of PAST it)
		//
		//		make sure we have flushed up to m_pbEntry (instead of PbGetEndOfLogData())

		LGPOS lgposEndOfData;
		GetLgpos( m_pbEntry, &lgposEndOfData );
		// Everything in the log buffer better be flushed out to disk,
		// otherwise we're hosed.
		Assert( CmpLgpos( &lgposEndOfData, &m_lgposToFlush ) <= 0 );

		m_critLGBuf.Leave();
		m_critLGFlush.Leave();
		}
#endif

	//	flush must have checkpoint log so no need to do checkpoint again
	//
//	Call( ErrLGWriteFileHdr( m_plgfilehdr ) );


	//	check for log-sequence-end
	//	(since we logged a term/rcvquit record, we know this is a clean shutdown)

	if ( err >= JET_errSuccess )
		{
		m_critLGResFiles.Enter();
		if ( m_fLogSequenceEnd )
			{
			err = ErrERRCheck( JET_errLogSequenceEndDatabasesConsistent );
			}
		m_critLGResFiles.Leave();
		Call( err );
		}

FreeResources:
HandleError:

#ifndef LOGPATCH_UNIT_TEST

	//	terminate log checkpoint
	//
	LGICheckpointTerm();

	LGTasksTerm();

#endif	//	!LOGPATCH_UNIT_TEST

	//	close the log file
	//
	delete m_pfapiLog;
	m_pfapiLog = NULL;

		{
		CHAR	szLogName[ IFileSystemAPI::cchPathMax ];

		//	If edbtmp.log is being written to, wait for it to complete and close file.

		LGCreateAsynchWaitForCompletion();
		
		//	Delete any existing edbtmp.log since we would need to recreate
		//	it at next startup anyway (we never trust any prepared log files,
		//	not edbtmp.log, not res1.log, nor res2.log).
		
		LGMakeLogName( szLogName, m_szJetTmp );
		(void) m_pinst->m_pfsapi->ErrFileDelete( szLogName );
		}
	
	//	clean up allocated resources
	//
	LGTermLogBuffers();

#ifdef UNLIMITED_DB
	OSMemoryPageFree( m_pbLGDbListBuffer );
	m_pbLGDbListBuffer = NULL;
	m_cbLGDbListBuffer = 0;
#endif	
	
	OSMemoryPageFree( m_plgfilehdr );
	m_plgfilehdr = NULL;

	OSMemoryPageFree ((void *) m_pcheckpointDeleted);
	m_pcheckpointDeleted = NULL;

	//	check for log-sequence-end

	if ( err >= JET_errSuccess )
		{
		m_critLGResFiles.Enter();
		if ( m_fLogSequenceEnd )
			{
			err = ErrERRCheck( JET_errLogSequenceEnd );
			}
		m_critLGResFiles.Leave();
		}

	m_fLogInitialized = fFalse;
	
	return err;
	}


//********************* LOGGING ****************************
//**********************************************************


// Log buffer utilities to make it easy to verify that we're
// using data in the log buffer that is "used" (allocated)
// (between m_pbWrite and m_pbEntry) and "free" (unallocated).
// Takes into consideration VM wrap-around and the circularity
// of the log buffer.


// We should not reference data > PbMaxEntry() when dealing
// with valid data in the log buffer.

const BYTE*	LOG::PbMaxEntry()
	{
	// need for access to m_pbWrite and m_pbEntry
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	if ( m_pbEntry > m_pbWrite )
		return m_pbEntry;
	else
		return m_pbEntry + m_cbLGBuf;
	}


// When adding log records to the log buffer, we should not copy data
// into the region past PbMaxWrite().

const BYTE*	LOG::PbMaxWrite()
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	if ( m_pbWrite >= m_pbEntry )
		return m_pbWrite;
	else
		return m_pbWrite + m_cbLGBuf;
	}

// Normalizes a pointer into the log buffer for use with
// comparisons to test whether the data the pointer points to
// is used.

const BYTE*	LOG::PbMaxPtrForUsed(const BYTE* const pb)
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	// below may need to be pb <= m_pbLGBufMax
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	if ( pb < m_pbEntry )
		return pb + m_cbLGBuf;
	else
		return pb;
	}

// Normalizes a pointer into the log buffer for use with
// comparisons to test whether the data the pointer points to
// is free.

const BYTE*	LOG::PbMaxPtrForFree(const BYTE* const pb)
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	// below may need to be pb <= m_pbLGBufMax
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	if ( pb < m_pbWrite )
		return pb + m_cbLGBuf;
	else
		return pb;
	}

// In use data in log buffer is between m_pbWrite and PbMaxEntry().

ULONG	LOG::CbLGUsed()
	{
	return ULONG( PbMaxEntry() - m_pbWrite );
	}

// Available space in log buffer is between the entry point
// and the start of the real data

ULONG	LOG::CbLGFree()
	{
	return ULONG( PbMaxWrite() - m_pbEntry );
	}

// Internal implementation function to determine whether
// cb bytes at pb are in the free portion 