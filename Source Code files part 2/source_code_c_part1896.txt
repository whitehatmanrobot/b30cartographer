 meaning of VirtualBias.
        //
        // Note if the system is booted with both /3GB & /USERVA, then system
        // PTEs will be allocated below virtual 3GB and that will end up being
        // the lowest system address the process creation needs to duplicate.
        //

        if (MmVirtualBias != 0) {
            MmVirtualBias = (ULONG_PTR)MmSessionBase - CODE_START;
        }
#endif

        //
        // Create the bitmap which represents valid memory.  Note the largest
        // possible size is used because (unlike the MmPhysicalMemoryBlock) we
        // don't want to free and reallocate this bitmap during hotadds because
        // we want callers to be able to reference it lock free.
        //

        ASSERT (MmHighestPossiblePhysicalPage + 1 < _4gb);

        Bitmap = ExAllocatePoolWithTag (
                       NonPagedPool,
                       (((MmHighestPossiblePhysicalPage + 1) + 31) / 32) * 4,
                       '  mM');

        if (Bitmap == NULL) {
            KeBugCheckEx (INSTALL_MORE_MEMORY,
                          MmNumberOfPhysicalPages,
                          MmLowestPhysicalPage,
                          MmHighestPhysicalPage,
                          0x101);
        }

        RtlInitializeBitMap (&MiPfnBitMap,
                             Bitmap,
                             (ULONG)(MmHighestPossiblePhysicalPage + 1));

        RtlClearAllBits (&MiPfnBitMap);

        for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

            Run = &MmPhysicalMemoryBlock->Run[i];

            if (Run->PageCount != 0) {
                RtlSetBits (&MiPfnBitMap,
                            (ULONG)Run->BasePage,
                            (ULONG)Run->PageCount);
            }
        }

        MiSyncCachedRanges ();

#if defined(_X86_) || defined(_AMD64_)
        MiAddHalIoMappings ();
#endif

#if !defined (_WIN64)
        if ((AutosizingFragment == TRUE) &&
            (NumberOfPages >= 256 * 1024)) {

            //
            // This is a system with at least 1GB of RAM.  Presumably it
            // will be used to cache many files.  Maybe we should factor in
            // pool size here and adjust it accordingly.
            //

            MmAllocationFragment;
        }
#endif

        //
        // Temporarily initialize resident available pages so large page
        // allocations can succeed if the memory exists.
        //

        MmResidentAvailablePages = MmAvailablePages - MM_FLUID_PHYSICAL_PAGES;

        MiInitializeLargePageSupport ();

        MiInitializeDriverLargePageList ();

        //
        // Relocate all the drivers so they can be paged (and protected) on
        // a per-page basis.
        //

        MiReloadBootLoadedDrivers (LoaderBlock);

#if defined (_MI_MORE_THAN_4GB_)
        if (MiNoLowMemory != 0) {
            MiRemoveLowPages (1);
        }
#endif
        MiInitializeVerifyingComponents (LoaderBlock);

        //
        // Setup the system size as small, medium, or large depending
        // on memory available.
        //
        // For internal MM tuning, the following applies
        //
        // 12Mb  is small
        // 12-19 is medium
        // > 19 is large
        //
        //
        // For all other external tuning,
        // < 19 is small
        // 19 - 31 is medium for workstation
        // 19 - 63 is medium for server
        // >= 32 is large for workstation
        // >= 64 is large for server
        //

        if (MmNumberOfPhysicalPages <= MM_SMALL_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 0;
            MmModifiedPageMaximum = 100;
            MmDataClusterSize = 0;
            MmCodeClusterSize = 1;
            MmReadClusterSize = 2;
            MmInPageSupportMinimum = 2;
        }
        else if (MmNumberOfPhysicalPages <= MM_MEDIUM_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 2;
            MmModifiedPageMaximum = 150;
            MmSystemCacheWsMinimum += 100;
            MmDataClusterSize = 1;
            MmCodeClusterSize = 2;
            MmReadClusterSize = 4;
            MmInPageSupportMinimum = 3;
        }
        else {
            MmSystemSize = MmMediumSystem;
            MmMaximumDeadKernelStacks = 5;
            MmModifiedPageMaximum = 300;
            MmSystemCacheWsMinimum += 400;
            MmDataClusterSize = 3;
            MmCodeClusterSize = 7;
            MmReadClusterSize = 7;
            MmInPageSupportMinimum = 4;
        }

        if (MmNumberOfPhysicalPages < ((24*1024*1024)/PAGE_SIZE)) {
            MmSystemCacheWsMinimum = 32;
        }

        if (MmNumberOfPhysicalPages >= ((32*1024*1024)/PAGE_SIZE)) {

            //
            // If we are on a workstation, 32Mb and above are considered
            // large systems.
            //

            if (MmProductType == 0x00690057) {
                MmSystemSize = MmLargeSystem;
            }
            else {

                //
                // For servers, 64Mb and greater is a large system
                //

                if (MmNumberOfPhysicalPages >= ((64*1024*1024)/PAGE_SIZE)) {
                    MmSystemSize = MmLargeSystem;
                }
            }
        }

        if (MmNumberOfPhysicalPages > ((33*1024*1024)/PAGE_SIZE)) {
            MmModifiedPageMaximum = 800;
            MmSystemCacheWsMinimum += 500;
            MmInPageSupportMinimum += 4;
        }

        if (NT_SUCCESS (MmIsVerifierEnabled (&VerifierFlags))) {

            //
            // The verifier is enabled so don't defer any MDL unlocks because
            // without state, debugging driver bugs in this area is very
            // difficult.
            //

            DeferredMdlEntries = 0;
        }
        else if (MmNumberOfPhysicalPages > ((255*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 32;
        }
        else if (MmNumberOfPhysicalPages > ((127*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 8;
        }
        else {
            DeferredMdlEntries = 4;
        }

#if defined(MI_MULTINODE)
        for (i = 0; i < KeNumberNodes; i += 1) {

            InitializeSListHead (&KeNodeBlock[i]->PfnDereferenceSListHead);
            KeNodeBlock[i]->PfnDeferredList = NULL;

            for (j = 0; j < DeferredMdlEntries; j += 1) {

                SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
                if (SingleListEntry != NULL) {
                    InterlockedPushEntrySList (&KeNodeBlock[i]->PfnDereferenceSListHead,
                                               SingleListEntry);
                }
            }
        }
#else
        InitializeSListHead (&MmPfnDereferenceSListHead);

        for (j = 0; j < DeferredMdlEntries; j += 1) {
            SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
            if (SingleListEntry != NULL) {
                InterlockedPushEntrySList (&MmPfnDereferenceSListHead,
                                           SingleListEntry);
            }
        }
#endif
        
        MmFreedExpansionPoolMaximum = 5;

        if (MmNumberOfPhysicalPages > ((1000*1024*1024)/PAGE_SIZE)) {
            MmFreedExpansionPoolMaximum = 300;
        }
        else if (MmNumberOfPhysicalPages > ((500*1024*1024)/PAGE_SIZE)) {
            MmFreedExpansionPoolMaximum = 100;
        }

        ASSERT (SharedUserData->NumberOfPhysicalPages == 0);

        SharedUserData->NumberOfPhysicalPages = (ULONG) MmNumberOfPhysicalPages;

        SharedUserData->LargePageMinimum = 0;

        //
        // Determine if we are on an AS system (Winnt is not AS).
        //

        if (MmProductType == 0x00690057) {
            SharedUserData->NtProductType = NtProductWinNt;
            MmProductType = 0;
            MmThrottleTop = 250;
            MmThrottleBottom = 30;

        }
        else {
            if (MmProductType == 0x0061004c) {
                SharedUserData->NtProductType = NtProductLanManNt;
            }
            else {
                SharedUserData->NtProductType = NtProductServer;
            }

            MmProductType = 1;
            MmThrottleTop = 450;
            MmThrottleBottom = 80;
            MmMinimumFreePages = 81;
            MmInPageSupportMinimum += 8;
        }

        MiAdjustWorkingSetManagerParameters ((LOGICAL)(MmProductType == 0 ? TRUE : FALSE));

        //
        // Set the ResidentAvailablePages to the number of available
        // pages minus the fluid value.
        //

        MmResidentAvailablePages = MmAvailablePages - MM_FLUID_PHYSICAL_PAGES;

        //
        // Subtract off the size of future nonpaged pool expansion
        // so that nonpaged pool will always be able to expand regardless of
        // prior system load activity.
        //

        MmResidentAvailablePages -= MiExpansionPoolPagesInitialCharge;

        //
        // Subtract off the size of the system cache working set.
        //

        MmResidentAvailablePages -= MmSystemCacheWsMinimum;
        MmResidentAvailableAtInit = MmResidentAvailablePages;

        if (MmResidentAvailablePages < 0) {
#if DBG
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                            "system cache working set too big\n");
#endif
            return FALSE;
        }

        //
        // Initialize spin lock for allowing working set expansion.
        //

        KeInitializeSpinLock (&MmExpansionLock);

        KeInitializeGuardedMutex (&MmPageFileCreationLock);

        //
        // Initialize resources for extending sections.
        //

        ExInitializeResourceLite (&MmSectionExtendResource);
        ExInitializeResourceLite (&MmSectionExtendSetResource);

        //
        // Build the system cache structures.
        //

        StartPde = MiGetPdeAddress (MmSystemCacheWorkingSetList);
        PointerPte = MiGetPteAddress (MmSystemCacheWorkingSetList);

#if (_MI_PAGING_LEVELS >= 3)

        TempPte = ValidKernelPte;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPdeAddress(StartPde);

        if (StartPxe->u.Hard.Valid == 0) {

            //
            // Map in a page directory parent page for the system cache working
            // set.  Note that we only populate one page table for this.
            //

            LOCK_PFN (OldIrql);
            DirectoryFrameIndex = MiRemoveZeroPage (
                MI_GET_PAGE_COLOR_FROM_PTE (StartPxe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

            MiInitializePfnAndMakePteValid (DirectoryFrameIndex, StartPxe, TempPte);

            MmResidentAvailablePages -= 1;
            UNLOCK_PFN (OldIrql);
        }
#endif

        StartPpe = MiGetPteAddress(StartPde);

        if (StartPpe->u.Hard.Valid == 0) {

            //
            // Map in a page directory page for the system cache working set.
            // Note that we only populate one page table for this.
            //

            LOCK_PFN (OldIrql);
            DirectoryFrameIndex = MiRemoveZeroPage (
                MI_GET_PAGE_COLOR_FROM_PTE (StartPpe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

            MiInitializePfnAndMakePteValid (DirectoryFrameIndex, StartPpe, TempPte);

            MmResidentAvailablePages -= 1;
            UNLOCK_PFN (OldIrql);
        }

#if (_MI_PAGING_LEVELS >= 4)

        //
        // The shared user data is already initialized and it shares the
        // page table page with the system cache working set list.
        //

        ASSERT (StartPde->u.Hard.Valid == 1);
#else

        //
        // Map in a page table page.
        //

        ASSERT (StartPde->u.Hard.Valid == 0);

        LOCK_PFN (OldIrql);
        PageFrameIndex = MiRemoveZeroPage(
                                MI_GET_PAGE_COLOR_FROM_PTE (StartPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        MiInitializePfnAndMakePteValid (PageFrameIndex, StartPde, TempPte);

        MmResidentAvailablePages -= 1;
        UNLOCK_PFN (OldIrql);
#endif

        StartPpe = MiGetPpeAddress (MmSystemCacheStart);
        StartPde = MiGetPdeAddress (MmSystemCacheStart);
        PointerPte = MiGetVirtualAddressMappedByPte (StartPde);

#else
#if !defined(_X86PAE_)
        ASSERT ((StartPde + 1) == MiGetPdeAddress (MmSystemCacheStart));
#endif
#endif

#if defined(_X86_)

        //
        // Make the system cache virtual size the maximum since 32 bit
        // address space limits us to far less than we'd want anyway.
        //

        MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                                MaximumSystemCacheSize * PAGE_SIZE) - 1);

        MmSizeOfSystemCacheInPages = MaximumSystemCacheSize;

        if (MiMaximumSystemCacheSizeExtra != 0) {
            MmSizeOfSystemCacheInPages += MiMaximumSystemCacheSizeExtra;
            ASSERT (MiSystemCacheStartExtra != 0);
            MiSystemCacheEndExtra = (PVOID)(((PCHAR) MiSystemCacheStartExtra +
                        (MiMaximumSystemCacheSizeExtra) * PAGE_SIZE) - 1);
        }
        else {
            MiSystemCacheStartExtra = MmSystemCacheStart;
            MiSystemCacheEndExtra = MmSystemCacheEnd;
        }
#else
        //
        // For NT64, size the system cache based on the amount of
        // physical memory so we don't go too far.  Note there is still
        // cost to for the preallocated page table pages so factor
        // that in as well (for smaller memory systems).
        //

        MmSizeOfSystemCacheInPages = (PFN_COUNT)((128*1024*1024) >> PAGE_SHIFT);

        i = (MmNumberOfPhysicalPages + 65) / 1024;

        if (i > 4) {

            //
            // System has at least 4032 pages.  Make the system
            // cache 128mb + 16mb for each additional 1024 pages.
            //

            MmSizeOfSystemCacheInPages +=
                            ((i - 4) * ((16*1024*1024) >> PAGE_SHIFT));

            if (MmSizeOfSystemCacheInPages > MaximumSystemCacheSize) {
                MmSizeOfSystemCacheInPages = MaximumSystemCacheSize;
            }
        }

        MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                                MmSizeOfSystemCacheInPages * PAGE_SIZE) - 1);

#endif

        EndPde = MiGetPdeAddress (MmSystemCacheEnd);

        TempPte = ValidKernelPte;
        Color = 0;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPxeAddress(MmSystemCacheStart);
        if (StartPxe->u.Hard.Valid == 0) {
            FirstPxe = TRUE;
            FirstPpe = TRUE;
        }
        else {
            FirstPxe = FALSE;
            FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
        }
#elif (_MI_PAGING_LEVELS >= 3)
        FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
#else
        DirectoryFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PDE_BASE));
#endif

        while (StartPde <= EndPde) {

#if (_MI_PAGING_LEVELS >= 4)
            if (FirstPxe == TRUE || MiIsPteOnPpeBoundary(StartPde)) {
                FirstPxe = FALSE;
                StartPxe = MiGetPdeAddress(StartPde);

                //
                // Map in a page directory parent page.
                //

                Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
                MI_SYSTEM_PAGE_COLOR++;

                LOCK_PFN (OldIrql);

                DirectoryFrameIndex = MiRemoveZeroPage (Color);

                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

                MiInitializePfnAndMakePteValid (DirectoryFrameIndex,
                                                StartPxe,
                                                TempPte);

                MmResidentAvailablePages -= 1;

                UNLOCK_PFN (OldIrql);
            }
#endif

#if (_MI_PAGING_LEVELS >= 3)
            if (FirstPpe == TRUE || MiIsPteOnPdeBoundary(StartPde)) {
                FirstPpe = FALSE;
                StartPpe = MiGetPteAddress(StartPde);

                //
                // Map in a page directory page.
                //

                Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
                MI_SYSTEM_PAGE_COLOR++;

                LOCK_PFN (OldIrql);

                DirectoryFrameIndex = MiRemoveZeroPage (Color);

                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

                MiInitializePfnAndMakePteValid (DirectoryFrameIndex,
                                                StartPpe,
                                                TempPte);

                MmResidentAvailablePages -= 1;

                UNLOCK_PFN (OldIrql);
            }
#endif

            ASSERT (StartPde->u.Hard.Valid == 0);

            //
            // Map in a page table page.
            //

            Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
            MI_SYSTEM_PAGE_COLOR++;

            LOCK_PFN (OldIrql);

            PageFrameIndex = MiRemoveZeroPage (Color);

            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

            MiInitializePfnAndMakePteValid (PageFrameIndex, StartPde, TempPte);

            MmResidentAvailablePages -= 1;

            UNLOCK_PFN (OldIrql);

            StartPde += 1;
        }

#if defined(_X86_)

        if (ReduceSystemCacheForPtes == TRUE) {

            PMMPTE ExtraPtesPointer;

            ASSERT (MmVirtualBias != 0);

            ReductionInPages = MaximumSystemCacheSize / 3;
            ReductionInPages = MI_ROUND_TO_SIZE (ReductionInPages, PTE_PER_PAGE);
            MaximumSystemCacheSize -= ReductionInPages;

            MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                                MaximumSystemCacheSize * PAGE_SIZE) - 1);

            ASSERT (MiSystemCacheStartExtra == MmSystemCacheStart);
            MiSystemCacheEndExtra = MmSystemCacheEnd;

            MmSizeOfSystemCacheInPages = MaximumSystemCacheSize;

            ExtraPtesPointer = MiGetPteAddress (((ULONG) MmSystemCacheEnd + 1));

            //
            // Increment the system PTEs (for autoconfiguration purposes) but
            // don't actually add the PTEs till later (to prevent
            // fragmentation).
            //

            MiIncrementSystemPtes (ReductionInPages);

            MiAddExtraSystemPteRanges (ExtraPtesPointer, ReductionInPages);
        }

#endif
        //
        // Initialize the system cache.
        //

        MiInitializeSystemCache ((ULONG) MmSystemCacheWsMinimum,
                                 (ULONG) MmAvailablePages);


        MmAttemptForCantExtend.Segment = NULL;
        MmAttemptForCantExtend.RequestedExpansionSize = 1;
        MmAttemptForCantExtend.ActualExpansion = 0;
        MmAttemptForCantExtend.InProgress = FALSE;
        MmAttemptForCantExtend.PageFileNumber = MI_EXTEND_ANY_PAGEFILE;

        KeInitializeEvent (&MmAttemptForCantExtend.Event,
                           NotificationEvent,
                           FALSE);

        //
        // Now that we have booted far enough, replace the temporary
        // commit limits with real ones: set the initial commit page
        // limit to the number of available pages.  This value is
        // updated as paging files are created.
        //

        MmTotalCommitLimit = MmAvailablePages;

        if (MmTotalCommitLimit > 1024) {
            MmTotalCommitLimit -= 1024;
        }

        MmTotalCommitLimitMaximum = MmTotalCommitLimit;

        //
        // Set maximum working set size to 512 pages less than the
        // total available memory.
        //

        MmMaximumWorkingSetSize = (WSLE_NUMBER)(MmAvailablePages - 512);

        if (MmMaximumWorkingSetSize > (MM_MAXIMUM_WORKING_SET - 5)) {
            MmMaximumWorkingSetSize = MM_MAXIMUM_WORKING_SET - 5;
        }

        //
        // Create the modified page writer event.
        //

        KeInitializeEvent (&MmModifiedPageWriterEvent, NotificationEvent, FALSE);

        //
        // Build paged pool.
        //

        MiBuildPagedPool ();

        //
        // Initialize the loaded module list.  This cannot be done until
        // paged pool has been built.
        //

        if (MiInitializeLoadedModuleList (LoaderBlock) == FALSE) {
#if DBG
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                            "Loaded module list initialization failed\n");
#endif
            return FALSE;
        }

        //
        // Initialize the handle for the PAGELK section now that all drivers
        // have been relocated to their final resting place and the loaded
        // module list has been initialized.
        //
    
        ExPageLockHandle = MmLockPageableCodeSection ((PVOID)(ULONG_PTR)MmShutdownSystem);
        MmUnlockPageableImageSection (ExPageLockHandle);

        //
        // Initialize the unused segment threshold.  Attempt to keep pool usage
        // below this percentage (by trimming the cache) if pool requests
        // can fail.
        //

        if (MmConsumedPoolPercentage == 0) {
            MmConsumedPoolPercentage = 80;
        }
        else if (MmConsumedPoolPercentage < 5) {
            MmConsumedPoolPercentage = 5;
        }
        else if (MmConsumedPoolPercentage > 100) {
            MmConsumedPoolPercentage = 100;
        }
    
        //
        // Add more system PTEs if this is a large memory system.
        // Note that 64 bit systems can determine the right value at the
        // beginning since there is no virtual address space crunch.
        //

#if !defined (_WIN64)
        if (MmNumberOfPhysicalPages > ((127*1024*1024) >> PAGE_SHIFT)) {

            PMMPTE StartingPte;

            PointerPde = MiGetPdeAddress ((PCHAR)MmPagedPoolEnd + 1);
            StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);
            j = 0;

            TempPte = ValidKernelPde;
            LOCK_PFN (OldIrql);
            while (PointerPde->u.Hard.Valid == 0) {

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_EXTRA_SYSTEM_PTES, 1);

                PageFrameIndex = MiRemoveZeroPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPde, TempPte);
                PointerPde += 1;
                StartingPte += PAGE_SIZE / sizeof(MMPTE);
                j += PAGE_SIZE / sizeof(MMPTE);
                MmResidentAvailablePages -= 1;
            }

            UNLOCK_PFN (OldIrql);

            if (j != 0) {
                StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);

                ASSERT (MiPteRanges[0].StartingVa == MmNonPagedSystemStart);
                MmNonPagedSystemStart = MiGetVirtualAddressMappedByPte (StartingPte);
                MiPteRanges[0].StartingVa = MmNonPagedSystemStart;
                MmNumberOfSystemPtes += j;
                MiAddSystemPtes (StartingPte, j, SystemPteSpace);
                MiIncrementSystemPtes (j);
            }
        }

        //
        // Snap a copy of the initial page directory so that when large page
        // system PTE mappings are deleted the proper values can be restored.
        //

        MiInitialSystemPageDirectory = ExAllocatePoolWithTag (
                                            NonPagedPool,
                                            PD_PER_SYSTEM * PAGE_SIZE,
                                            'dPmM');

        if (MiInitialSystemPageDirectory == NULL) { 
#if DBG
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                            "can't snap system page directory\n");
#endif
            return FALSE;
        }

        RtlCopyMemory (MiInitialSystemPageDirectory,
                       (PVOID) PDE_BASE,
                       PD_PER_SYSTEM * PAGE_SIZE);

#endif

        MmPageFaultNotifyRoutine = NULL;

        KeInitializeEvent (&MmWorkingSetManagerEvent,
                           SynchronizationEvent,
                           FALSE);

        return TRUE;
    }

    if (Phase == 1) {

#ifdef _X86_
        if (KeFeatureBits & KF_LARGE_PAGE)
#endif
            SharedUserData->LargePageMinimum = MM_MINIMUM_VA_FOR_LARGE_PAGE;

#if defined(_X86_) || defined(_AMD64_)
        MiInitMachineDependent (LoaderBlock);
#endif

        MiMapBBTMemory (LoaderBlock);

        if (!MiSectionInitialization ()) {
            return FALSE;
        }

        //
        // Create double mapped page between kernel and user mode.
        // The PTE is deliberately allocated from paged pool so that
        // it will always have a PTE itself instead of being superpaged.
        // This way, checks throughout the fault handler can assume that
        // the PTE can be checked without having to special case this.
        //

        MmSharedUserDataPte = ExAllocatePoolWithTag (PagedPool,
                                                     sizeof(MMPTE),
                                                     '  mM');

        if (MmSharedUserDataPte == NULL) {
            return FALSE;
        }

        PointerPte = MiGetPteAddress ((PVOID)KI_USER_SHARED_DATA);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                  PageFrameIndex,
                                  MM_READONLY,
                                  PointerPte);

        *MmSharedUserDataPte = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        LOCK_PFN (OldIrql);

        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        UNLOCK_PFN (OldIrql);

#ifdef _X86_
        if (MmHighestUserAddress < (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // Install the PTE mapping now as faults will not because the
            // shared user data is in the system portion of the address space.
            // Note the pagetable page has already been allocated and locked
            // down.
            //

            //
            // Make the mapping user accessible.
            //

            ASSERT (MmSharedUserDataPte->u.Hard.Owner == 0);
            MmSharedUserDataPte->u.Hard.Owner = 1;

            PointerPde = MiGetPdeAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPde->u.Hard.Owner == 0);
            PointerPde->u.Hard.Owner = 1;

            ASSERT (MiUseMaximumSystemSpace != 0);
            PointerPte = MiGetPteAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPte->u.Hard.Valid == 0);
            MI_WRITE_VALID_PTE (PointerPte, *MmSharedUserDataPte);
        }
#endif

        MiSessionWideInitializeAddresses ();
        MiInitializeSessionWsSupport ();
        MiInitializeSessionIds ();

        //
        // Start the modified page writer.
        //

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              MiModifiedPageWriter,
                                              NULL))) {
            return FALSE;
        }
        ZwClose (ThreadHandle);

        //
        // Initialize the low and high memory events.  This must be done
        // before starting the working set manager.
        //

        if (MiInitializeMemoryEvents () == FALSE) {
            return FALSE;
        }

        //
        // Start the balance set manager.
        //
        // The balance set manager performs stack swapping and working
        // set management and requires two threads.
        //

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              KeBalanceSetManager,
                                              NULL))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              KeSwapProcessOrStack,
                                              NULL))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

#if !defined(NT_UP)
        MiStartZeroPageWorkers ();
#endif

#if defined(_X86_)
        MiEnableKernelVerifier ();
#endif

        ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

        NextEntry = PsLoadedModuleList.Flink;

        for ( ; NextEntry != &PsLoadedModuleList; NextEntry = NextEntry->Flink) {

            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            NtHeaders = RtlImageNtHeader(DataTableEntry->DllBase);

            if ((NtHeaders != NULL) &&
                (NtHeaders->OptionalHeader.MajorOperatingSystemVersion >= 5) &&
                (NtHeaders->OptionalHeader.MajorImageVersion >= 5)) {
                DataTableEntry->Flags |= LDRP_ENTRY_NATIVE;
            }

            MiWriteProtectSystemImage (DataTableEntry->DllBase);
        }
        ExReleaseResourceLite (&PsLoadedModuleResource);

        InterlockedDecrement (&MiTrimInProgressCount);

        MiFullyInitialized = 1;

        return TRUE;
    }

    if (Phase == 2) {
        MiEnablePagingTheExecutive ();
        return TRUE;
    }

    return FALSE;
}

VOID
MiMapBBTMemory (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function walks through the loader block's memory descriptor list
    and maps memory reserved for the BBT buffer into the system.

    The mapped PTEs are PDE-aligned and made user accessible.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PVOID Va;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER NumberOfPagesMapped;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPde;
    MMPTE TempPte;
#if defined(_AMD64_)
    PVOID LastVa;
    PMMPTE PointerPpe;
    PMMPTE LastPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPxe;
#endif

    if (BBTPagesToReserve <= 0) {
        return;
    }

    //
    // Request enough PTEs such that protection can be applied to the PDEs.
    //

    NumberOfPages = (BBTPagesToReserve + (PTE_PER_PAGE - 1)) & ~(PTE_PER_PAGE - 1);

    PointerPte = MiReserveAlignedSystemPtes ((ULONG)NumberOfPages,
                                             SystemPteSpace,
                                             MM_VA_MAPPED_BY_PDE);

    if (PointerPte == NULL) {
        BBTPagesToReserve = 0;
        return;
    }

    //
    // Allow user access to the buffer.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    LastPde = MiGetPteAddress (PointerPte + NumberOfPages);

    ASSERT (LastPde != PointerPde);

    do {
        TempPte = *PointerPde;
        TempPte.u.Long |= MM_PTE_OWNER_MASK;
        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPde, TempPte);
        PointerPde += 1;
    } while (PointerPde < LastPde);

    Va = MiGetVirtualAddressMappedByPte (PointerPte);

#if defined(_AMD64_)

    LastVa = (PVOID) ((ULONG_PTR) Va + (BBTPagesToReserve * PAGE_SIZE) - 1);

    //
    // Set owner to user for PPEs.
    //

    PointerPpe = MiGetPpeAddress (Va);
    LastPpe = MiGetPpeAddress (LastVa);

    do {
        TempPte = *PointerPpe;
        TempPte.u.Long |= MM_PTE_OWNER_MASK;
        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPpe, TempPte);
        PointerPpe += 1;
    } while (PointerPpe <= LastPpe);

    //
    // Set owner to user for PXEs.
    //

    PointerPxe = MiGetPxeAddress (Va);
    LastPxe = MiGetPxeAddress (LastVa);

    do {
        TempPte = *PointerPxe;
        TempPte.u.Long |= MM_PTE_OWNER_MASK;
        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPxe, TempPte);
        PointerPxe += 1;
    } while (PointerPxe <= LastPxe);

#endif

    MI_FLUSH_ENTIRE_TB (8);

    TempPte = ValidUserPte;
    NumberOfPagesMapped = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType == LoaderBBTMemory) {

            PageFrameIndex = MemoryDescriptor->BasePage;
            NumberOfPages = MemoryDescriptor->PageCount;

            if (NumberOfPagesMapped + NumberOfPages > BBTPagesToReserve) {
                NumberOfPages = BBTPagesToReserve - NumberOfPagesMapped;
            }

            NumberOfPagesMapped += NumberOfPages;

            do {

                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                PointerPte += 1;
                PageFrameIndex += 1;
                NumberOfPages -= 1;
            } while (NumberOfPages);

            if (NumberOfPagesMapped == BBTPagesToReserve) {
                break;
            }
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    KeZeroPages (Va, BBTPagesToReserve << PAGE_SHIFT);

    //
    // Tell BBT_Init how many pages were allocated.
    //

    if (NumberOfPagesMapped < BBTPagesToReserve) {
        BBTPagesToReserve = (ULONG)NumberOfPagesMapped;
    }

    *(PULONG)Va = BBTPagesToReserve;

    //
    // At this point instrumentation code will detect the existence of
    // buffer and initialize the structures.
    //

    BBTBuffer = Va;
}


PPHYSICAL_MEMORY_DESCRIPTOR
MmInitializeMemoryLimits (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType,
    IN OUT PPHYSICAL_MEMORY_DESCRIPTOR InputMemory OPTIONAL
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and builds a list of contiguous physical
    memory blocks of the desired types.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in return.

    Memory - If non-NULL, supplies the physical memory blocks to place the
             search results in.  If NULL, pool is allocated to hold the
             returned search results in - the caller must free this pool.

Return Value:

    A pointer to the physical memory blocks for the requested search or NULL
    on failure.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PLIST_ENTRY NextMd;
    ULONG i;
    ULONG InitialAllocation;
    PFN_NUMBER NextPage;
    PFN_NUMBER TotalPages;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory2;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;

    InitialAllocation = 0;

    if (ARGUMENT_PRESENT (InputMemory)) {
        Memory = InputMemory;
    }
    else {

        //
        // The caller wants us to allocate the return result buffer.  Size it
        // by allocating the maximum possibly needed as this should not be
        // very big (relatively).  It is the caller's responsibility to free
        // this.  Obviously this option can only be requested after pool has
        // been initialized.
        //

        NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

        while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
            InitialAllocation += 1;
            MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                                 MEMORY_ALLOCATION_DESCRIPTOR,
                                                 ListEntry);
            NextMd = MemoryDescriptor->ListEntry.Flink;
        }

        Memory = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (InitialAllocation - 1),
                                        'lMmM');

        if (Memory == NULL) {
            return NULL;
        }

        Memory->NumberOfRuns = InitialAllocation;
    }

    //
    // Walk through the memory descriptors and build the physical memory list.
    //

    i = 0;
    TotalPages = 0;
    NextPage = (PFN_NUMBER) -1;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;

            //
            // Merge runs whenever possible.
            //

            if (MemoryDescriptor->BasePage == NextPage) {
                ASSERT (MemoryDescriptor->PageCount != 0);
                Memory->Run[i - 1].PageCount += MemoryDescriptor->PageCount;
                NextPage += MemoryDescriptor->PageCount;
            }
            else {
                Memory->Run[i].BasePage = MemoryDescriptor->BasePage;
                Memory->Run[i].PageCount = MemoryDescriptor->PageCount;
                NextPage = Memory->Run[i].BasePage + Memory->Run[i].PageCount;
                i += 1;
            }
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    ASSERT (i <= Memory->NumberOfRuns);

    if (i == 0) {

        //
        // Don't bother shrinking this as the caller will be freeing it
        // shortly as it is just an empty list.
        //

        Memory->Run[i].BasePage = 0;
        Memory->Run[i].PageCount = 0;
    }
    else if (!ARGUMENT_PRESENT (InputMemory)) {

        //
        // Shrink the buffer (if possible) now that the final size is known.
        //

        if (InitialAllocation > i) {
            Memory2 = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (i - 1),
                                            'lMmM');

            if (Memory2 != NULL) {
                RtlCopyMemory (Memory2->Run,
                               Memory->Run,
                               sizeof(PHYSICAL_MEMORY_RUN) * i);

                ExFreePool (Memory);
                Memory = Memory2;
            }
        }
    }

    Memory->NumberOfRuns = i;
    Memory->NumberOfPages = TotalPages;

    return Memory;
}


PFN_NUMBER
MiPagesInLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and returns the number of pages of the desired type.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in the returned count.

Return Value:

    The number of pages of the requested type in the loader block list.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER TotalPages;

    //
    // Walk through the memory descriptors counting pages.
    //

    TotalPages = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    return TotalPages;
}


typedef struct _REGISTRY_MEMORY_RUN {
    PFN_NUMBER BasePage;
    PFN_NUMBER PageCount;
    PMMPTE PointerPte;
} REGISTRY_MEMORY_RUN, *PREGISTRY_MEMORY_RUN;

PREGISTRY_MEMORY_RUN MiBootRegistryRuns;

VOID
MmFreeBootRegistry (
    VOID
    )
{
    PMMPTE PointerPte;
    PFN_NUMBER PagesFreed;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PREGISTRY_MEMORY_RUN Runs;

    //
    // Since systemwide commitment was determined early in Phase 0 and
    // excluded the ranges just freed, add them back in now.
    //

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    if (MiBootRegistryRuns != NULL) {

        PagesFreed = 0;

        for (Runs = MiBootRegistryRuns; Runs->BasePage != 0; Runs += 1) {

            PointerPte = Runs->PointerPte;
            PageFrameIndex = Runs->BasePage;
            NumberOfPages = Runs->PageCount;

            while (NumberOfPages != 0) {

                if (MI_IS_PHYSICAL_ADDRESS (
                     MiGetVirtualAddressMappedByPte (PointerPte))) {

                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    ASSERT (PointerPte == Pfn1->PteAddress);

                    LOCK_PFN (OldIrql);
                    MI_SET_PFN_DELETED (Pfn1);
                    MiDecrementShareCount (Pfn1, PageFrameIndex);
                    UNLOCK_PFN (OldIrql);

                    PagesFreed += 1;
                }
                else {

                    PagesFreed += MiDeleteSystemPageableVm (PointerPte,
                                                           1,
                                                           MI_DELETE_FLUSH_TB,
                                                           NULL);
                }
                NumberOfPages -= 1;
                PointerPte += 1;
                PageFrameIndex += 1;
            }
#if defined(_X86_)
            // LWFIX - pass bigger sizes to MiAddExtraSystemPtes, then rip out 
            // MmKeepBootRegistry (keep 0x1 behavior only)
#endif
        }

        if (PagesFreed != 0) {
            InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, PagesFreed);
            InterlockedExchangeAddSizeT (&MmTotalCommitLimit, PagesFreed);
        }

        ExFreePool (MiBootRegistryRuns);
        MiBootRegistryRuns = NULL;
    }

    return;
}


VOID
MmFreeLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function is called as the last routine in phase 1 initialization.
    It frees memory used by the OsLoader.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PVOID VirtualAddress;
    MMPTE TempPte;
    PMMPTE ExpectedPte;
    PETHREAD CurrentThread;
    PLIST_ENTRY NextMd;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    ULONG i;
    ULONG RegistryDescriptors;
    PFN_NUMBER NextPhysicalPage;
    PFN_NUMBER PagesFreed;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RUN RunBase;
    PPHYSICAL_MEMORY_RUN Runs;
    PREGISTRY_MEMORY_RUN RegistryRuns;

    i = 0;
    RegistryDescriptors = 0;
    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType == LoaderRegistryData) {
            RegistryDescriptors += 1;
        }

        i += 1;
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    RunBase = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(PHYSICAL_MEMORY_RUN) * i,
                                     'lMmM');

    if (RunBase == NULL) {
        return;
    }

    if ((MmKeepBootRegistry & 0x1) && (RegistryDescriptors != 0)) {

        MiBootRegistryRuns = ExAllocatePoolWithTag (NonPagedPool,
                                         sizeof(REGISTRY_MEMORY_RUN) * (RegistryDescriptors + 1),
                                         'lMmM');

        RegistryRuns = MiBootRegistryRuns;

        if (MiBootRegistryRuns != NULL) {

            //
            // Zero this allocation (and its NULL terminator) so it can be
            // walked later when the registry finally releases it.
            //

            RtlZeroMemory (RegistryRuns, sizeof(REGISTRY_MEMORY_RUN) * (RegistryDescriptors + 1));

            //
            // Walk through the memory descriptors and make the
            // boot-loaded registry ranges pageable.
            //
        
            NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;
        
            while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
        
                MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                                     MEMORY_ALLOCATION_DESCRIPTOR,
                                                     ListEntry);
        
        
                if ((MemoryDescriptor->MemoryType == LoaderRegistryData) &&
                    (MemoryDescriptor->PageCount != 0)) {
    
                    RegistryRuns->BasePage = MemoryDescriptor->BasePage;
                    RegistryRuns->PageCount = MemoryDescriptor->PageCount;
                    RegistryRuns += 1;
                }
        
                NextMd = MemoryDescriptor->ListEntry.Flink;
            }
        }
    
        if (RegistryRuns != MiBootRegistryRuns) {

            CurrentThread = PsGetCurrentThread ();
            RegistryRuns -= 1;

            do {
                i = (ULONG) RegistryRuns->PageCount;
                ASSERT (i != 0);
                NextPhysicalPage = RegistryRuns->BasePage;
    
#if defined (_MI_MORE_THAN_4GB_)
                if (MiNoLowMemory != 0) {
                    if (NextPhysicalPage < MiNoLowMemory) {
    
                        //
                        // Don't make this run pageable as it is below the
                        // memory threshold configured for this system.
                        //
                        // Zero the PageCount so it is not freed later.
                        //
    
                        RegistryRuns->PageCount = 0;
                        RegistryRuns -= 1;
                        continue;
                    }
                }
#endif
    
                Pfn1 = MI_PFN_ELEMENT (NextPhysicalPage);

                //
                // Stash the starting PTE so the
                // range can be easily released later.
                //

                RegistryRuns->PointerPte = Pfn1->PteAddress;
                ASSERT (RegistryRuns->BasePage != 0);

                ExpectedPte = Pfn1->PteAddress;
                VirtualAddress = MiGetVirtualAddressMappedByPte (ExpectedPte);

                LOCK_SYSTEM_WS (CurrentThread);

                LOCK_PFN (OldIrql);

                while (i != 0) {
    
                    ASSERT (Pfn1->u1.Flink == 0);
                    ASSERT (Pfn1->u2.ShareCount == 1);
                    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
                    ASSERT (Pfn1->OriginalPte.u.Long == 0);
                    ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);

                    ASSERT (Pfn1->PteAddress == ExpectedPte);

                    if (Pfn1->PteAddress != ExpectedPte) {

                        //
                        // The PTE for this PFN is not contiguous with the
                        // prior one so do not attempt to reclaim the VA
                        // space or physical pages later.
                        //

                        RegistryRuns->PageCount = 0;
                    }

                    //
                    // Make the page pageable and put the PTE into transition.
                    // If it is a PFN without a PTE then leave it as nonpaged
                    // until MmFreeBootRegistry is called.
                    //

                    if (!MI_IS_PHYSICAL_ADDRESS (VirtualAddress)) {

                        //
                        // Not a physical address so trim it.
                        //

                        MI_SET_MODIFIED (Pfn1, 1, 0x16);

                        Pfn1->u3.e1.CacheAttribute = MiCached;
                        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

                        TempPte = *(Pfn1->PteAddress);

                        ASSERT (TempPte.u.Hard.Valid == 1);

                        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

                        MI_WRITE_INVALID_PTE (Pfn1->PteAddress, TempPte);

                        MiDecrementShareCount (Pfn1, NextPhysicalPage);
                    }

                    Pfn1 += 1;
                    NextPhysicalPage += 1;
                    ExpectedPte += 1;
                    VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
                    i -= 1;
                }

                MI_FLUSH_ENTIRE_TB (9);

                UNLOCK_PFN (OldIrql);

                UNLOCK_SYSTEM_WS (CurrentThread);

                RegistryRuns -= 1;

            } while (RegistryRuns >= MiBootRegistryRuns);
        }
    }

    Runs = RunBase;

    //
    //
    // Walk through the memory descriptors and add pages to the
    // free list in the PFN database.
    //

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);


        switch (MemoryDescriptor->MemoryType) {
            case LoaderRegistryData:
                if (MmKeepBootRegistry & 0x1) {
                    break;
                }
            case LoaderOsloaderHeap:
            case LoaderNlsData:

                //
                // Capture the data to temporary storage so we won't
                // free memory we are referencing.
                //

                Runs->BasePage = MemoryDescriptor->BasePage;
                Runs->PageCount = MemoryDescriptor->PageCount;
                Runs += 1;

                break;

            default:

                break;
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    PagesFreed = 0;

    LOCK_PFN (OldIrql);

    if (Runs != RunBase) {
        Runs -= 1;
        do {
            i = (ULONG)Runs->PageCount;
            NextPhysicalPage = Runs->BasePage;

#if defined (_MI_MORE_THAN_4GB_)
            if (MiNoLowMemory != 0) {
                if (NextPhysicalPage < MiNoLowMemory) {

                    //
                    // Don't free this run as it is below the memory threshold
                    // configured for this system.
                    //

                    Runs -= 1;
                    continue;
                }
            }
#endif

            Pfn1 = MI_PFN_ELEMENT (NextPhysicalPage);
            while (i != 0) {

                if ((MmKeepBootRegistry & 0x1) && (Pfn1->u2.ShareCount > 1)) {

                    //
                    // Note LoaderOsloaderHeap is used to describe page table
                    // pages used to map the registry so care must be taken
                    // not to blindly free those.  Distinguish them via
                    // the share count check above.
                    //

                    NOTHING;
                }
                else if (Pfn1->u3.e2.ReferenceCount == 0) {
                    if (Pfn1->u1.Flink == 0) {

                        //
                        // Set the PTE address to the physical page for
                        // virtual address alignment checking.
                        //

                        Pfn1->PteAddress =
                                   (PMMPTE)(NextPhysicalPage << PTE_SHIFT);

                        MiDetermineNode (NextPhysicalPage, Pfn1);

                        MiInsertPageInFreeList (NextPhysicalPage);
                        PagesFreed += 1;
                    }
                }
                else {

                    if (NextPhysicalPage != 0) {

                        //
                        // Remove PTE and insert into the free list.  If it is
                        // a physical address within the PFN database, the PTE
                        // element does not exist and therefore cannot be
                        // updated.
                        //

                        if (!MI_IS_PHYSICAL_ADDRESS (
                                MiGetVirtualAddressMappedByPte (Pfn1->PteAddress))) {

                            //
                            // Not a physical address.
                            //

                            Pfn1->PteAddress->u.Long = 0;
                        }

                        MI_SET_PFN_DELETED (Pfn1);
                        MiDecrementShareCount (Pfn1, NextPhysicalPage);
                        PagesFreed += 1;
                    }
                }

                Pfn1 += 1;
                i -= 1;
                NextPhysicalPage += 1;
            }
            Runs -= 1;
        } while (Runs >= RunBase);
    }

#if defined(_X86_)

    if (MmVirtualBias != 0) {

        //
        // If the kernel has been biased to allow for 3gb of user
        // address space, the boot images are doubly mapped at KSEG0_BASE
        // and at ALTERNATE_BASE.  Therefore, the KSEG0_BASE entries must
        // be unmapped.
        //

        PMMPTE Pde;
        ULONG NumberOfPdes;

        NumberOfPdes = MmBootImageSize / MM_VA_MAPPED_BY_PDE;

        Pde = MiGetPdeAddress ((PVOID)KSEG0_BASE);

        for (i = 0; i < NumberOfPdes; i += 1) {
            MI_WRITE_ZERO_PTE (Pde);
            Pde += 1;
        }
    }

#endif

    UNLOCK_PFN (OldIrql);

    //
    // This virtual address space is being deleted and the system cannot be
    // accessing it (or it would be a serious bug) - thus it is safe to
    // flush the TB after releasing the PFN lock.
    //

    MI_FLUSH_ENTIRE_TB (0xA);

    ExFreePool (RunBase);

    //
    // Since systemwide commitment was determined early in Phase 0 and
    // excluded the ranges just freed, add them back in now.
    //

    if (PagesFreed != 0) {
        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, PagesFreed);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, PagesFreed);
    }

    if (MmKeepBootRegistry & 0x2) {
        MmFreeBootRegistry ();
    }

    return;
}

VOID
MiBuildPagedPool (
    VOID
    )

/*++

Routine Description:

    This function is called to build the structures required for paged
    pool and initialize the pool.  Once this routine is called, paged
    pool may be allocated.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    SIZE_T Size;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE PointerPde;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER ContainingFrame;
    SIZE_T AdditionalCommittedPages;
    KIRQL OldIrql;
    ULONG i;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PMMPTE PointerPxeEnd;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID LastVa;
    PMMPTE PointerPpe;
    PMMPTE PointerPpeEnd;
#else
    PMMPFN Pfn1;
#endif

    i = 0;
    AdditionalCommittedPages = 0;

#if (_MI_PAGING_LEVELS < 3)

    //
    // Double map system page directory page.
    //

    PointerPte = MiGetPteAddress (PDE_BASE);

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        MmSystemPageDirectory[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT(MmSystemPageDirectory[i]);
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
        PointerPte += 1;
    }

    //
    // Was not mapped physically, map it virtually in system space.
    //

    PointerPte = MiReserveSystemPtes (PD_PER_SYSTEM, SystemPteSpace);

    if (PointerPte == NULL) {
        MiIssueNoPtesBugcheck (PD_PER_SYSTEM, SystemPteSpace);
    }

    MmSystemPagePtes = (PMMPTE)MiGetVirtualAddressMappedByPte (PointerPte);

    TempPte = ValidKernelPde;

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        TempPte.u.Hard.PageFrameNumber = MmSystemPageDirectory[i];
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
    }

#endif

    if (MmPagedPoolMaximumDesired == TRUE) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }
    else if (MmSizeOfPagedPoolInBytes == 0) {

        //
        // A size of 0 means size the pool based on physical memory.
        //

        MmSizeOfPagedPoolInBytes = 2 * MmMaximumNonPagedPoolInBytes;
#if (_MI_PAGING_LEVELS >= 3)
        MmSizeOfPagedPoolInBytes *= 2;
#endif
    }

    if (MmIsThisAnNtAsSystem()) {
        if ((MmNumberOfPhysicalPages > ((24*1024*1024) >> PAGE_SHIFT)) &&
            (MmSizeOfPagedPoolInBytes < MM_MINIMUM_PAGED_POOL_NTAS)) {

            MmSizeOfPagedPoolInBytes = MM_MINIMUM_PAGED_POOL_NTAS;
        }
    }

    if (MmSizeOfPagedPoolInBytes >
              (ULONG_PTR)((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart)) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }

    Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);

    if (Size < MM_MIN_INITIAL_PAGED_POOL) {
        Size = MM_MIN_INITIAL_PAGED_POOL;
    }

    if (Size > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        Size = MM_MAX_PAGED_POOL >> PAGE_SHIFT;
    }

#if defined (_WIN64)

    //
    // NT64 places system mapped views directly after paged pool.  Ensure
    // enough VA space is available.
    //

    if (Size + (MmSystemViewSize >> PAGE_SHIFT) > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        ASSERT (MmSizeOfPagedPoolInBytes > 2 * MmSystemViewSize);
        MmSizeOfPagedPoolInBytes -= MmSystemViewSize;
        Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);
    }
#endif

    Size = (Size + (PTE_PER_PAGE - 1)) / PTE_PER_PAGE;
    MmSizeOfPagedPoolInBytes = (ULONG_PTR)Size * PAGE_SIZE * PTE_PER_PAGE;

    //
    // Set size to the number of pages in the pool.
    //

    Size = Size * PTE_PER_PAGE;

    //
    // If paged pool is really nonpageable then limit the size based
    // on how much physical memory is actually present.  Disable this
    // feature if not enough physical memory is present to do it.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        Size = MmSizeOfPagedPoolInBytes / PAGE_SIZE;

        if ((MI_NONPAGEABLE_MEMORY_AVAILABLE() < 2048) ||
            (MmAvailablePages < 2048)) {
                Size = 0;
        }
        else {
            if ((SPFN_NUMBER)(Size) > MI_NONPAGEABLE_MEMORY_AVAILABLE() - 2048) {
                Size = (MI_NONPAGEABLE_MEMORY_AVAILABLE() - 2048);
            }

            if (Size > MmAvailablePages - 2048) {
                Size = MmAvailablePages - 2048;
            }
        }

        Size = ((Size * PAGE_SIZE) / MM_VA_MAPPED_BY_PDE) * MM_VA_MAPPED_BY_PDE;

        if ((((Size / 5) * 4) >= MmSizeOfPagedPoolInBytes) &&
            (Size >= MM_MIN_INITIAL_PAGED_POOL)) {

            MmSizeOfPagedPoolInBytes = Size;
        }
        else {
            MmDisablePagingExecutive &= ~MM_PAGED_POOL_LOCKED_DOWN;
        }

        Size = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;
    }

    MmSizeOfPagedPoolInPages = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;

    ASSERT ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart) <=
            (PCHAR)MmNonPagedSystemStart);

    ASSERT64 ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart + MmSystemViewSize) <=
              (PCHAR)MmNonPagedSystemStart);

    MmPagedPoolEnd = (PVOID)(((PUCHAR)MmPagedPoolStart +
                            MmSizeOfPagedPoolInBytes) - 1);

    //
    // Build page table page for paged pool.
    //

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

    TempPte = ValidKernelPde;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Map in all the page directory pages to span all of paged pool.
    // This removes the need for a system lookup directory.
    //

    LastVa = (PVOID)((PCHAR)MmPagedPoolEnd + MmSystemViewSize);
    PointerPpe = MiGetPpeAddress (MmPagedPoolStart);
    PointerPpeEnd = MiGetPpeAddress (LastVa);

    MiSystemViewStart = (ULONG_PTR)MmPagedPoolEnd + 1;

    PointerPde = MiGetPdeAddress (MmPagedPoolEnd) + 1;
    LastPde = MiGetPdeAddress (LastVa);

    LOCK_PFN (OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
    PointerPxe = MiGetPxeAddress (MmPagedPoolStart);
    PointerPxeEnd = MiGetPxeAddress (LastVa);

    while (PointerPxe <= PointerPxeEnd) {

        if (PointerPxe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveZeroPage (
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPxe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

            MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPxe, TempPte);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPxe += 1;
    }
#endif

    while (PointerPpe <= PointerPpeEnd) {

        if (PointerPpe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveZeroPage (
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

            MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPpe, TempPte);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPpe += 1;
    }

    //
    // Initialize the system view page table pages.
    //

    MmResidentAvailablePages -= (LastPde - PointerPde + 1);
    AdditionalCommittedPages += (LastPde - PointerPde + 1);

    while (PointerPde <= LastPde) {

        ASSERT (PointerPde->u.Hard.Valid == 0);

        PageFrameIndex = MiRemoveZeroPage (
                            MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPde, TempPte);

        PointerPde += 1;
    }

    UNLOCK_PFN (OldIrql);

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

#endif

    PointerPte = MiGetPteAddress (MmPagedPoolStart);
    MmPagedPoolInfo.FirstPteForPagedPool = PointerPte;
    MmPagedPoolInfo.LastPteForPagedPool = MiGetPteAddress (MmPagedPoolEnd);

    MiZeroMemoryPte (PointerPde,
                     (1 + MiGetPdeAddress (MmPagedPoolEnd) - PointerPde));

    LOCK_PFN (OldIrql);

    //
    // Map in a page table page.
    //

    PageFrameIndex = MiRemoveZeroPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
    ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPpeAddress (MmPagedPoolStart));
#else
    ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

    MiInitializePfnForOtherProcess (PageFrameIndex,
                                    PointerPde,
                                    ContainingFrame);

    MmResidentAvailablePages -= 1;
    AdditionalCommittedPages += 1;

    UNLOCK_PFN (OldIrql);

    MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde + 1;

    //
    // Build bitmaps for paged pool.
    //

    MiCreateBitMap (&MmPagedPoolInfo.PagedPoolAllocationMap, Size, NonPagedPool);
    RtlSetAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

    //
    // Indicate first page worth of PTEs are available.
    //

    RtlClearBits (MmPagedPoolInfo.PagedPoolAllocationMap, 0, PTE_PER_PAGE);

    MiCreateBitMap (&MmPagedPoolInfo.EndOfPagedPoolBitmap, Size, NonPagedPool);
    RtlClearAllBits (MmPagedPoolInfo.EndOfPagedPoolBitmap);

    //
    // If verifier is present then build the verifier paged pool bitmap.
    //

    if (MmVerifyDriverBufferLength != (ULONG)-1) {
        MiCreateBitMap (&VerifierLargePagedPoolMap, Size, NonPagedPool);
        RtlClearAllBits (VerifierLargePagedPoolMap);
    }

    //
    // Initialize paged pool.
    //

    InitializePool (PagedPool, 0L);

    //
    // If paged pool is really nonpageable then allocate the memory now.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        PointerPde = MiGetPdeAddress (MmPagedPoolStart);
        PointerPde += 1;
        LastPde = MiGetPdeAddress (MmPagedPoolEnd);
        TempPte = ValidKernelPde;

        PointerPte = MiGetPteAddress (MmPagedPoolStart);
        LastPte = MiGetPteAddress (MmPagedPoolEnd);

        ASSERT (MmPagedPoolCommit == 0);
        MmPagedPoolCommit = (ULONG)(LastPte - PointerPte + 1);

        ASSERT (MmPagedPoolInfo.PagedPoolCommit == 0);
        MmPagedPoolInfo.PagedPoolCommit = MmPagedPoolCommit;

#if DBG
        //
        // Ensure no paged pool has been allocated yet.
        //

        for (i = 0; i < PTE_PER_PAGE; i += 1) {
            ASSERT (!RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
        }

        while (i < MmSizeOfPagedPoolInBytes / PAGE_SIZE) {
            ASSERT (RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
            i += 1;
        }
#endif

        RtlClearAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

        LOCK_PFN (OldIrql);

        //
        // Map in the page table pages.
        //

        MmResidentAvailablePages -= (LastPde - PointerPde + 1);
        AdditionalCommittedPages += (LastPde - PointerPde + 1);

        while (PointerPde <= LastPde) {

            ASSERT (PointerPde->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveZeroPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
            ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPteAddress (PointerPde));
#else
            ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

            MiInitializePfnForOtherProcess (PageFrameIndex,
                                            MiGetPteAddress (PointerPde),
                                            ContainingFrame);

            PointerPde += 1;
        }

        MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde;

        TempPte = ValidKernelPte;
        MI_SET_PTE_DIRTY (TempPte);

        ASSERT (MmAvailablePages > (PFN_COUNT)(LastPte - PointerPte + 1));
        ASSERT (MmResidentAvailablePages > (SPFN_NUMBER)(LastPte - PointerPte + 1));
        MmResidentAvailablePages -= (LastPte - PointerPte + 1);
        AdditionalCommittedPages += (LastPte - PointerPte + 1);

        while (PointerPte <= LastPte) {

            ASSERT (PointerPte->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

            MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPte, TempPte);

            PointerPte += 1;
        }

        UNLOCK_PFN (OldIrql);
    }

    //
    // Since the commitment return path is lock free, the total committed
    // page count must be atomically incremented.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommittedPages, AdditionalCommittedPages);

    MiInitializeSpecialPool (NonPagedPool);

    //
    // Initialize the default paged pool signaling thresholds.
    //

    MiLowPagedPoolThreshold = (30 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 5) < MiLowPagedPoolThreshold) {
        MiLowPagedPoolThreshold = Size / 5;
    }

    MiHighPagedPoolThreshold = (60 * 1024 * 1024) >> PAGE_SHIFT;

    if (((Size * 2) / 5) < MiHighPagedPoolThreshold) {
        MiHighPagedPoolThreshold = (Size * 2) / 5;
    }

    ASSERT (MiLowPagedPoolThreshold < MiHighPagedPoolThreshold);

    //
    // Allow mapping of views into system space.
    //

    MiInitializeSystemSpaceMap (NULL);

    return;
}


VOID
MiInitializeNonPagedPoolThresholds (
    VOID
    )

/*++

Routine Description:

    This function is called to initialize the default nonpaged pool
    signaling thresholds.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PFN_NUMBER Size;

    Size = MmMaximumNonPagedPoolInPages;

    //
    // Initialize the default nonpaged pool signaling thresholds.
    //

    MiLowNonPagedPoolThreshold = (8 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 3) < MiLowNonPagedPoolThreshold) {
        MiLowNonPagedPoolThreshold = Size / 3;
    }

    MiHighNonPagedPoolThreshold = (20 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 2) < MiHighNonPagedPoolThreshold) {
        MiHighNonPagedPoolThreshold = Size / 2;
    }

    ASSERT (MiLowNonPagedPoolThreshold < MiHighNonPagedPoolThreshold);

    return;
}


VOID
MiFindInitializationCode (
    OUT PVOID *StartVa,
    OUT PVOID *EndVa
    )

/*++

Routine Description:

    This function locates the start and end of the initialization code for
    each loaded module list entry.  This code resides in the INIT section
    of each image.

Arguments:

    StartVa - Returns the starting address of the init section.

    EndVa - Returns the ending address of the init section.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    ULONG Span;
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PVOID InitStart;
    PVOID InitEnd;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    PIMAGE_SECTION_HEADER LastDiscard;
    LONG i;
    LOGICAL DiscardSection;
    PVOID MiFindInitializationCodeAddress;
    PKTHREAD CurrentThread;
    UNICODE_STRING NameString;

    MiFindInitializationCodeAddress = MmGetProcedureAddress((PVOID)(ULONG_PTR)&MiFindInitializationCode);

    *StartVa = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    //
    // Acquire the load lock to ensure that we don't slice into a load
    // in progress (ie: fixups on INIT code may be ongoing) on a driver
    // already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {
        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->Flags & LDRP_MM_LOADED) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already
            // had its init section removed.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;
        NtHeader = RtlImageNtHeader (CurrentBase);

        if (NtHeader == NULL) {
            Next = Next->Flink;
            continue;
        }

        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the sections named 'INIT',
        // PAGEVRF* and PAGESPEC.  INIT always goes, the others go depending
        // on registry configuration.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        InitStart = NULL;
        while (i > 0) {

#if DBG
            if ((*(PULONG)SectionTableEntry->Name == 'tini') ||
                (*(PULONG)SectionTableEntry->Name == 'egap')) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                    "driver %wZ has lower case sections (init or pagexxx)\n",
                    &LdrDataTableEntry->FullDllName);
            }
#endif

            DiscardSection = FALSE;

            //
            // Free any INIT sections (or relocation sections that haven't
            // been already).  Note a driver may have a relocation section
            // but not have any INIT code.
            //

            if ((*(PULONG)SectionTableEntry->Name == 'TINI') ||
                ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0)) {
                DiscardSection = TRUE;
            }
            else if (*(PULONG)SectionTableEntry->Name == 'EGAP') {

                if ((SectionTableEntry->Name[4] == 'V') &&
                     (SectionTableEntry->Name[5] == 'R') &&
                     (SectionTableEntry->Name[6] == 'F')) {

                    //
                    // Discard PAGEVRF* if no drivers are being instrumented.
                    //

                    if (MmVerifyDriverBufferLength == (ULONG)-1) {
                        DiscardSection = TRUE;
                    }
                }
                else if (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS') {

                    //
                    // Discard PAGESPEC special pool code if it's not enabled.
                    //

                    if (MiSpecialPoolFirstPte == NULL) {
                        DiscardSection = TRUE;
                    }
                }
            }

            if (DiscardSection == TRUE) {

                InitStart = (PVOID)((PCHAR)CurrentBase + SectionTableEntry->VirtualAddress);
                //
                // Generally, SizeOfRawData is larger than VirtualSize for each
                // section because it includes the padding to get to the
                // subsection alignment boundary.  However, if the image is
                // linked with subsection alignment == native page alignment,
                // the linker will have VirtualSize be much larger than
                // SizeOfRawData because it will account for all the bss.
                //

                Span = SectionTableEntry->SizeOfRawData;

                if (Span < SectionTableEntry->Misc.VirtualSize) {
                    Span = SectionTableEntry->Misc.VirtualSize;
                }
                InitEnd = (PVOID)((PCHAR)InitStart + Span - 1);
                InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                        (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                InitStart = (PVOID)ROUND_TO_PAGES (InitStart);

                //
                // Check if more sections are discardable after this one so
                // even small INIT sections can be discarded.
                //

                if (i == 1) {
                    LastDiscard = SectionTableEntry;
                }
                else {
                    LastDiscard = NULL;
                    do {
                        i -= 1;
                        SectionTableEntry += 1;

                        if ((SectionTableEntry->Characteristics &
                             IMAGE_SCN_MEM_DISCARDABLE) != 0) {

                            //
                            // Discard this too.
                            //

                            LastDiscard = SectionTableEntry;
                        }
                        else {
                            break;
                        }
                    } while (i > 1);
                }

                if (LastDiscard) {
                    //
                    // Generally, SizeOfRawData is larger than VirtualSize for each
                    // section because it includes the padding to get to the subsection
                    // alignment boundary.  However, if the image is linked with
                    // subsection alignment == native page alignment, the linker will
                    // have VirtualSize be much larger than SizeOfRawData because it
                    // will account for all the bss.
                    //

                    Span = LastDiscard->SizeOfRawData;

                    if (Span < LastDiscard->Misc.VirtualSize) {
                        Span = LastDiscard->Misc.VirtualSize;
                    }

                    InitEnd = (PVOID)(((PCHAR)CurrentBase +
                                       LastDiscard->VirtualAddress) +
                                      (Span - 1));

                    //
                    // If this isn't the last section in the driver then the
                    // the next section is not discardable.  So the last
                    // section is not rounded down, but all others must be.
                    //

                    if (i != 1) {
                        InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                                                             (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                    }
                }

                if (InitEnd > (PVOID)((PCHAR)CurrentBase +
                                      LdrDataTableEntry->SizeOfImage)) {
                    InitEnd = (PVOID)(((ULONG_PTR)CurrentBase +
                                       (LdrDataTableEntry->SizeOfImage - 1)) |
                                      (PAGE_SIZE - 1));
                }

                if (InitStart <= InitEnd) {
                    if ((MiFindInitializationCodeAddress >= InitStart) &&
                        (MiFindInitializationCodeAddress <= InitEnd)) {

                        //
                        // This init section is in the kernel, don't free it
                        // now as it would free this code!
                        //

                        ASSERT (*StartVa == NULL);
                        *StartVa = InitStart;
                        *EndVa = InitEnd;
                    }
                    else {

                        //
                        // Don't free the INIT code for a driver mapped by
                        // large pages because if it unloads later, we'd have
                        // to deal with discontiguous ranges of pages to free.
                        // There would also be the issue of the frames getting
                        // given out and mapped with a conflicting TB attribute
                        // as well ...
                        //
                        // Make a special exception for the kernel & HAL
                        // since those never unload.
                        //

                        if (MI_IS_PHYSICAL_ADDRESS (InitStart)) {

                            NameString.Buffer = (const PUSHORT) KERNEL_NAME;
                            NameString.Length = sizeof (KERNEL_NAME) - sizeof (WCHAR);
                            NameString.MaximumLength = sizeof KERNEL_NAME;

                            if (!RtlEqualUnicodeString (&NameString,
                                                        &LdrDataTableEntry->BaseDllName,
                                                        TRUE)) {
                                MiFreeInitializationCode (InitStart, InitEnd);
                            }

                            NameString.Buffer = (const PUSHORT) HAL_NAME;
                            NameString.Length = sizeof (HAL_NAME) - sizeof (WCHAR);
                            NameString.MaximumLength = sizeof HAL_NAME;

                            if (!RtlEqualUnicodeString (&NameString,
                                                        &LdrDataTableEntry->BaseDllName,
                                                        TRUE)) {
                                MiFreeInitializationCode (InitStart, InitEnd);
                            }
                        }
                        else {
                            MiFreeInitializationCode (InitStart, InitEnd);
                        }
                    }
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }
        Next = Next->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (CurrentThread);

    return;
}


VOID
MiFreeInitializationCode (
    IN PVOID StartVa,
    IN PVOID EndVa
    )

/*++

Routine Description:

    This function is called to delete the initialization code for each
    loaded module list entry.

Arguments:

    StartVa - Supplies the starting address of the range to delete.

    EndVa - Supplies the ending address of the range to delete.

Return Value:

    None.

Environment:

    Kernel Mode Only.  Runs after system initialization.

--*/

{
    PMMPTE PointerPte;
    PFN_NUMBER PagesFreed;
    PFN_NUMBER PageFrameIndex;

    ASSERT (ExPageLockHandle);

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {

        //
        // Don't free this range as the kernel is always below the memory
        // threshold configured for this system.
        //

        return;
    }
#endif

    PointerPte = MiGetPteAddress (StartVa);

    PagesFreed = (PFN_NUMBER) (1 + MiGetPteAddress (EndVa) - PointerPte);

    if (MI_IS_PHYSICAL_ADDRESS (StartVa)) {

        //
        // Don't put these frames on the general purpose freelist because
        // they lie inside a range being mapped with a large page (or
        // otherwise locked down TB entry).  Thus we must ensure that
        // these are always mapped cached to avoid TB attribute conflict
        // problems.
        //

        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (StartVa);

        MiAddExpansionNonPagedPool (PageFrameIndex, PagesFreed, FALSE);
    }
    else {
        PagesFreed = MiDeleteSystemPageableVm (PointerPte,
                                              PagesFreed,
                                              MI_DELETE_FLUSH_TB,
                                              NULL);
    }

    if (PagesFreed != 0) {
        MiReturnCommitment (PagesFreed);
        MI_INCREMENT_RESIDENT_AVAILABLE (PagesFreed,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1);
    }

    return;
}


VOID
MiEnablePagingTheExecutive (
    VOID
    )

/*++

Routine Description:

    This function locates the start and end of the pageable code for
    each loaded module entry.  This code resides in the PAGE section of
    each image.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    ULONG Span;
    KIRQL OldIrql;
    PVOID StartVa;
    PETHREAD CurrentThread;
    PLONG SectionLockCountPointer;
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER StartSectionTableEntry;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    LONG i;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE SubsectionStartPte;
    PMMPTE SubsectionLastPte;
    LOGICAL PageSection;
    PVOID SectionBaseAddress;
    LOGICAL AlreadyLockedOnce;
    ULONG Waited;

    //
    // Don't page kernel mode code if customer does not want it paged or if
    // this is a diskless remote boot client.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

#if defined(REMOTE_BOOT)
    if (IoRemoteBootClient && IoCscInitializationFailed) {
        return;
    }
#endif

    //
    // Initializing LastPte is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    LastPte = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = PsGetCurrentThread ();

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    //
    // Acquire the load lock to ensure that we don't slice into a load
    // in progress (ie: fixups on INIT code may be ongoing) on a driver
    // already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {

        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->SectionPointer != NULL) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already paged.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;

        if ((MI_IS_PHYSICAL_ADDRESS (CurrentBase)) ||
            (MI_PDE_MAPS_LARGE_PAGE (MiGetPdeAddress (CurrentBase)))) {

            //
            // Mapped physically, can't be paged.
            //

            Next = Next->Flink;
            continue;
        }

        NtHeader = RtlImageNtHeader (CurrentBase);

        if (NtHeader == NULL) {
            Next = Next->Flink;
            continue;
        }

restart:

        StartSectionTableEntry = NULL;
        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the section named 'PAGE' or '.edata'.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        PointerPte = NULL;

        while (i > 0) {

            SectionBaseAddress = SECTION_BASE_ADDRESS(SectionTableEntry);

            if ((PUCHAR)SectionBaseAddress ==
                            ((PUCHAR)CurrentBase + SectionTableEntry->VirtualAddress)) {
                AlreadyLockedOnce = TRUE;

                //
                // This subsection has already been locked down (and possibly
                // unlocked as well) at least once.  If it is NOT locked down
                // right now and the pages are not in the system working set
                // then include it in the chunk to be paged.
                //

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (SectionTableEntry);

                if (*SectionLockCountPointer == 0) {

                    SubsectionStartPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));

                    //
                    // Generally, SizeOfRawData is larger than VirtualSize
                    // for each section because it includes the padding to
                    // get to the subsection alignment boundary.  However,
                    // if the image is linked with subsection alignment ==
                    // native page alignment, the linker will have VirtualSize
                    // be much larger than SizeOfRawData because it
                    // will account for all the bss.
                    //

                    Span = SectionTableEntry->SizeOfRawData;

                    if (Span < SectionTableEntry->Misc.VirtualSize) {
                        Span = SectionTableEntry->Misc.VirtualSize;
                    }

                    SubsectionLastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                                 SectionTableEntry->VirtualAddress +
                                 (NtHeader->OptionalHeader.SectionAlignment - 1) +
                                 Span -
                                 PAGE_SIZE));

                    if (SubsectionLastPte >= SubsectionStartPte) {
                        AlreadyLockedOnce = FALSE;
                    }
                }
            }
            else {
                AlreadyLockedOnce = FALSE;
            }

            PageSection = ((*(PULONG)SectionTableEntry->Name == 'EGAP') ||
                          (*(PULONG)SectionTableEntry->Name == 'ade.')) &&
                           (AlreadyLockedOnce == FALSE);

            if (*(PULONG)SectionTableEntry->Name == 'EGAP') {
                    
                if ((SectionTableEntry->Name[4] == 'K') &&
                    (SectionTableEntry->Name[5] == 'D')) {

                    //
                    // Only pageout PAGEKD if KdPitchDebugger is TRUE.
                    //

                    PageSection = KdPitchDebugger;
                }
                else if ((SectionTableEntry->Name[4] == 'V') &&
                         (SectionTableEntry->Name[5] == 'R') &&
                         (SectionTableEntry->Name[6] == 'F')) {

                    //
                    // Pageout PAGEVRF* if no drivers are being instrumented.
                    //

                    if (MmVerifyDriverBufferLength != (ULONG)-1) {
                        PageSection = FALSE;
                    }
                }
                else if (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS') {

                    //
                    // Pageout PAGESPEC special pool code if it's not enabled.
                    //

                    if (MiSpecialPoolFirstPte != NULL) {
                        PageSection = FALSE;
                    }
                }
            }

            if (PageSection) {

                //
                // This section is pageable, save away the start and end.
                //

                if (PointerPte == NULL) {

                     //
                     // Previous section was NOT pageable, get the start address.
                     //

                     ASSERT (StartSectionTableEntry == NULL);
                     StartSectionTableEntry = SectionTableEntry;
                     PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));
                }

                //
                // Generally, SizeOfRawData is larger than VirtualSize for each
                // section because it includes the padding to get to the
                // subsection alignment boundary.  However, if the image is
                // linked with subsection alignment == native page alignment,
                // the linker will have VirtualSize be much larger than
                // SizeOfRawData because it will account for all the bss.
                //

                Span = SectionTableEntry->SizeOfRawData;

                if (Span < SectionTableEntry->Misc.VirtualSize) {
                    Span = SectionTableEntry->Misc.VirtualSize;
                }

                LastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                             SectionTableEntry->VirtualAddress +
                             (NtHeader->OptionalHeader.SectionAlignment - 1) +
                             Span - PAGE_SIZE));
            }
            else {

                //
                // This section is not pageable, if the previous section was
                // pageable, enable it.
                //

                if (PointerPte != NULL) {

                    ASSERT (StartSectionTableEntry != NULL);
                    LOCK_SYSTEM_WS (CurrentThread);
                    LOCK_PFN (OldIrql);

                    StartVa = PAGE_ALIGN (StartSectionTableEntry);
                    while (StartVa < (PVOID) SectionTableEntry) {

                        Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa, OldIrql);

                        if (Waited != 0) {

                            //
                            // Restart at the top as the locks were released.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS (CurrentThread);
                            goto restart;
                        }
                        StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
                    }

                    //
                    // Now that we're holding the proper locks, rewalk all
                    // the sections to make sure they weren't locked down
                    // after we checked above.
                    //

                    while (StartSectionTableEntry < SectionTableEntry) {
                        SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                        SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                        if (((PUCHAR)SectionBaseAddress ==
                                        ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                        (*SectionLockCountPointer != 0)) {

                            //
                            // Restart at the top as the section has been
                            // explicitly locked by a driver since we first
                            // checked above.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS (CurrentThread);
                            goto restart;
                        }
                        StartSectionTableEntry += 1;
                    }

                    MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (CurrentThread);

                    PointerPte = NULL;
                    StartSectionTableEntry = NULL;
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }

        if (PointerPte != NULL) {
            ASSERT (StartSectionTableEntry != NULL);
            LOCK_SYSTEM_WS (CurrentThread);
            LOCK_PFN (OldIrql);

            StartVa = PAGE_ALIGN (StartSectionTableEntry);
            while (StartVa < (PVOID) SectionTableEntry) {

                Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa, OldIrql);

                if (Waited != 0) {

                    //
                    // Restart at the top as the locks were released.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (CurrentThread);
                    goto restart;
                }
                StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
            }

            //
            // Now that we're holding the proper locks, rewalk all
            // the sections to make sure they weren't locked down
            // after we checked above.
            //

            while (StartSectionTableEntry < SectionTableEntry) {
                SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                if (((PUCHAR)SectionBaseAddress ==
                                ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                (*SectionLockCountPointer != 0)) {

                    //
                    // Restart at the top as the section has been
                    // explicitly locked by a driver since we first
                    // checked above.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (CurrentThread);
                    goto restart;
                }
                StartSectionTableEntry += 1;
            }
            MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS (CurrentThread);
        }

        Next = Next->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    return;
}


VOID
MiEnablePagingOfDriverAtInit (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This routine marks the specified range of PTEs as pageable.

Arguments:

    PointerPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

Return Value:

    None.

Environment:

    Working set pushlock AND PFN lock held.

--*/

{
    PVOID Base;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    MMPTE TempPte;
    LOGICAL SessionAddress;
    ULONG FlushCount;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];

    FlushCount = 0;

    MM_PFN_LOCK_ASSERT();

    Base = MiGetVirtualAddressMappedByPte (PointerPte);
    SessionAddress = MI_IS_SESSION_PTE (PointerPte);

    while (PointerPte <= LastPte) {

        //
        // The PTE must be carefully checked as drivers may call MmPageEntire
        // during their DriverEntry yet faults may occur prior to this routine
        // running which cause pages to already be resident and in the working
        // set at this point.  So checks for validity and wsindex must be
        // applied.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn->u2.ShareCount == 1);

            if (Pfn->u1.WsIndex == 0) {

                //
                // Set the working set index to zero.  This allows page table
                // pages to be brought back in with the proper WSINDEX.
                //

                MI_ZERO_WSINDEX (Pfn);

                //
                // Original PTE may need to be set for drivers loaded via
                // ntldr.
                //

                if (Pfn->OriginalPte.u.Long == 0) {
                    Pfn->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
                    Pfn->OriginalPte.u.Soft.Protection |= MM_EXECUTE;
                }

                MI_SET_MODIFIED (Pfn, 1, 0x11);

                TempPte = *PointerPte;

                MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                              Pfn->OriginalPte.u.Soft.Protection);

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);

                if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
                    VaFlushList[FlushCount] = Base;
                    FlushCount += 1;
                }

                //
                // Flush the TB and decrement the number of valid PTEs
                // within the containing page table page.  Note that for a
                // private page, the page table page is still needed because
                // the page is in transition.
                //

                MiDecrementShareCount (Pfn, PageFrameIndex);

                MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_PAGE_DRIVER);

                MmTotalSystemCodePages += 1;
            }
            else {

                //
                // This would need to be taken out of the WSLEs so skip it for
                // now and let the normal paging algorithms remove it if we
                // run into memory pressure.
                //
            }

        }
        Base = (PVOID)((PCHAR)Base + PAGE_SIZE);
        PointerPte += 1;
    }

    if (FlushCount == 0) {
        NOTHING;
    }
    else if (FlushCount == 1) {
        MI_FLUSH_SINGLE_TB (VaFlushList[0], TRUE);
    }
    else if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (FlushCount, &VaFlushList[0], TRUE);
    }
    else {
        MI_FLUSH_ENTIRE_TB (0x15);
    }

    return;
}


MM_SYSTEMSIZE
MmQuerySystemSize (
    VOID
    )
{
    //
    // 12Mb  is small
    // 12-19 is medium
    // > 19 is large
    //
    return MmSystemSize;
}

NTKERNELAPI
BOOLEAN
MmIsThisAnNtAsSystem (
    VOID
    )
{
    return (BOOLEAN)MmProductType;
}

NTKERNELAPI
VOID
FASTCALL
MmSetPageFaultNotifyRoutine (
    PPAGE_FAULT_NOTIFY_ROUTINE NotifyRoutine
    )
{
    MmPageFaultNotifyRoutine = NotifyRoutine;
}

#define CONSTANT_UNICODE_STRING(s)   { sizeof( s ) - sizeof( WCHAR ), sizeof( s ), s }

VOID
MiNotifyMemoryEvents (
    VOID
    )

// PFN lock is held.
{
    if (MmAvailablePages < MmLowMemoryThreshold) {

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) == 0) {
            KeSetEvent (MiLowMemoryEvent, 0, FALSE);
        }
    }
    else if (MmAvailablePages < MmHighMemoryThreshold) {

        //
        // Gray zone, make sure both events are cleared.
        //

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }
    else {
        if (KeReadStateEvent (MiHighMemoryEvent) == 0) {
            KeSetEvent (MiHighMemoryEvent, 0, FALSE);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }

    return;
}

LOGICAL
MiInitializeMemoryEvents (
    VOID
    )
{
    KIRQL OldIrql;
    NTSTATUS Status;
    UNICODE_STRING LowMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowMemoryCondition");
    UNICODE_STRING HighMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighMemoryCondition");
    UNICODE_STRING LowPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowPagedPoolCondition");
    UNICODE_STRING HighPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighPagedPoolCondition");
    UNICODE_STRING LowNonPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowNonPagedPoolCondition");
    UNICODE_STRING HighNonPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighNonPagedPoolCondition");

    //
    // The thresholds may be set in the registry, if so, they are interpreted
    // in megabytes so convert them to pages now.
    //
    // If the user modifies the registry to introduce his own values, don't
    // bother error checking them as they can't hurt the system regardless (bad
    // values just may result in events not getting signaled or staying
    // signaled when they shouldn't, but that's not fatal).
    //

    if (MmLowMemoryThreshold != 0) {
        MmLowMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {

        //
        // Scale the threshold so on servers the low threshold is
        // approximately 32MB per 4GB, capping it at 64MB.
        //

        MmLowMemoryThreshold = MmPlentyFreePages;

        if (MmNumberOfPhysicalPages > 0x40000) {
            MmLowMemoryThreshold = (32 * 1024 * 1024) / PAGE_SIZE;
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x40000) >> 7);
        }
        else if (MmNumberOfPhysicalPages > 0x8000) {
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x8000) >> 5);
        }

        if (MmLowMemoryThreshold > (64 * 1024 * 1024) / PAGE_SIZE) {
            MmLowMemoryThreshold = (64 * 1024 * 1024) / PAGE_SIZE;
        }
    }

    if (MmHighMemoryThreshold != 0) {
        MmHighMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {
        MmHighMemoryThreshold = 3 * MmLowMemoryThreshold;
        ASSERT (MmHighMemoryThreshold > MmLowMemoryThreshold);
    }

    if (MmHighMemoryThreshold < MmLowMemoryThreshold) {
        MmHighMemoryThreshold = MmLowMemoryThreshold;
    }

    Status = MiCreateMemoryEvent (&LowMem, &MiLowMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighMem, &MiHighMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    //
    // Create the events for the pool thresholds.
    //

    Status = MiCreateMemoryEvent (&LowPagedPoolMem, &MiLowPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighPagedPoolMem, &MiHighPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&LowNonPagedPoolMem, &MiLowNonPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighNonPagedPoolMem, &MiHighNonPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    //
    // Initialize the pool threshold events based on the current system
    // values.
    //

    MiInitializePoolEvents ();

    //
    // Initialize the event values.
    //

    LOCK_PFN (OldIrql);

    MiNotifyMemoryEvents ();

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

extern POBJECT_TYPE ExEventObjectType;

NTSTATUS
MiCreateMemoryEvent (
    IN PUNICODE_STRING EventName,
    OUT PKEVENT *Event
    )
{
    PACL Dacl;
    HANDLE EventHandle;
    ULONG DaclLength;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES ObjectAttributes;
    SECURITY_DESCRIPTOR SecurityDescriptor;

    Status = RtlCreateSecurityDescriptor (&SecurityDescriptor,
                                          SECURITY_DESCRIPTOR_REVISION);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    DaclLength = sizeof (ACL) + sizeof (ACCESS_ALLOWED_ACE) * 3 +
                 RtlLengthSid (SeLocalSystemSid) +
                 RtlLengthSid (SeAliasAdminsSid) +
                 RtlLengthSid (SeWorldSid);

    Dacl = ExAllocatePoolWithTag (PagedPool, DaclLength, 'lcaD');

    if (Dacl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Status = RtlCreateAcl (Dacl, DaclLength, ACL_REVISION);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     SYNCHRONIZE|EVENT_QUERY_STATE|READ_CONTROL,
                                     SeWorldSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeAliasAdminsSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeLocalSystemSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

  
    Status = RtlSetDaclSecurityDescriptor (&SecurityDescriptor,
                                           TRUE,
                                           Dacl,
                                           FALSE);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }
  
    InitializeObjectAttributes (&ObjectAttributes,
                                EventName,
                                OBJ_KERNEL_HANDLE | OBJ_PERMANENT,
                                NULL,
                                &SecurityDescriptor);

    Status = ZwCreateEvent (&EventHandle,
                            EVENT_ALL_ACCESS,
                            &ObjectAttributes,
                            NotificationEvent,
                            FALSE);

    ExFreePool (Dacl);

    if (NT_SUCCESS (Status)) {
        Status = ObReferenceObjectByHandle (EventHandle,
                                            EVENT_MODIFY_STATE,
                                            ExEventObjectType,
                                            KernelMode,
                                            (PVOID *)Event,
                                            NULL);
    }

    ZwClose (EventHandle);

    return Status;
}

BOOLEAN MiAllMainMemoryMustBeCached = FALSE;

VOID
MiInitializeCacheOverrides (
    VOID
    )
{
#if defined (_WIN64)
    ULONG i;
    MMPTE PteContents;
    ULONG NumberOfBytes;
    NTSTATUS Status;
    HAL_PLATFORM_INFORMATION Information;

    //
    // Gather platform information from the HAL.
    //

    Status = HalQuerySystemInformation (HalPlatformInformation, 
                                        sizeof (Information),
                                        &Information,
                                        &NumberOfBytes);

    if (!NT_SUCCESS (Status)) {
        return;
    }

    //
    // Apply mapping modifications based on platform information flags.
    //
    // It would be better if the platform returned what the new cachetype
    // should be.
    //

    if (Information.PlatformFlags & HAL_PLATFORM_DISABLE_UC_MAIN_MEMORY) {

        //
        // The attribute table's noncached entries must be overridden so they
        // are equivalent to cached (for main system memory) since that's
        // all the underlying hardware supports.
        //

        MI_SET_CACHETYPE_TRANSLATION (MmNonCached, 0, MiCached);

        for (i = MM_NOCACHE + 1; i < MM_NOCACHE * 2; i += 1) {
            PteContents.u.Long = MmProtectToPteMask[i];
            MI_ENABLE_CACHING (PteContents);
            MmProtectToPteMask[i] = PteContents.u.Long;
        }
    }

    if (Information.PlatformFlags & HAL_PLATFORM_DISABLE_WRITE_COMBINING) {

        //
        // The attribute table's writecombine entries must be overridden so they
        // are equivalent to cached (for main system memory) since that's
        // all the underlying hardware supports.
        //

        MiAllMainMemoryMustBeCached = TRUE;

        MI_SET_CACHETYPE_TRANSLATION (MmWriteCombined, 0, MiCached);

        for (i = MM_WRITECOMBINE + 1; i < 32; i += 1) {
            PteContents.u.Long = MmProtectToPteMask[i];
            MI_ENABLE_CACHING (PteContents);
            MmProtectToPteMask[i] = PteContents.u.Long;
        }

        if ((Information.PlatformFlags & HAL_PLATFORM_ENABLE_WRITE_COMBINING_MMIO) == 0) {
            MI_SET_CACHETYPE_TRANSLATION (MmWriteCombined, 1, MiNonCached);
        }
    }
#endif

    return;
}

#if defined(_X86_) || defined(_AMD64_)

VOID
MiAddHalIoMappings (
    VOID
    )

/*++

Routine Description:

    This function scans the page directory and page tables for HAL I/O space
    mappings so they can be added to the page attribute table (to prevent
    any subsequent mappings from using a conflicting attribute).  This also
    lets the debugger automatically apply the correct attribute so !dd on
    any of these ranges just works.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 only.

--*/

{
    ULONG i;
    ULONG j;
    ULONG PdeCount;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER PageFrameIndex;
    PVOID VirtualAddress;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

#ifdef _X86_
    VirtualAddress = (PVOID) 0xFFC00000;
#elif defined(_AMD64_)
    VirtualAddress = (PVOID) HAL_VA_START;
#endif

    PointerPde = MiGetPdeAddress (VirtualAddress);

    ASSERT (MiGetPteOffset (VirtualAddress) == 0);

    PdeCount = PDE_PER_PAGE - MiGetPdeOffset (VirtualAddress);

    for (i = 0; i < PdeCount; i += 1) {

        if ((PointerPde->u.Hard.Valid == 1) &&
            (PointerPde->u.Hard.LargePage == 0)) {

            PointerPte = MiGetPteAddress (VirtualAddress);

            for (j = 0 ; j < PTE_PER_PAGE; j += 1) {

                PteContents = *PointerPte;

                if (PteContents.u.Hard.Valid == 1) {

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

                    if (!MI_IS_PFN (PageFrameIndex)) {

                        CacheAttribute = MiCached;

                        if ((MI_IS_CACHING_DISABLED (&PteContents)) &&
                            (PteContents.u.Hard.WriteThrough == 1)) {

                            CacheAttribute = MiNonCached;
                        }
                        else if ((MiWriteCombiningPtes == TRUE) &&
                                (PteContents.u.Hard.CacheDisable == 0) &&
                                (PteContents.u.Hard.WriteThrough == 1)) {

                            CacheAttribute = MiWriteCombined;
                        }
                        else if ((MiWriteCombiningPtes == FALSE) &&
                                (PteContents.u.Hard.CacheDisable == 1) &&
                                (PteContents.u.Hard.WriteThrough == 0)) {

                            CacheAttribute = MiWriteCombined;
                        }

                        MiInsertIoSpaceMap (VirtualAddress,
                                            PageFrameIndex,
                                            1,
                                            CacheAttribute);
                    }
                }

                VirtualAddress = (PVOID) ((PCHAR)VirtualAddress + PAGE_SIZE);
                PointerPte += 1;
            }
        }
        else {
            VirtualAddress = (PVOID) ((PCHAR)VirtualAddress + MM_VA_MAPPED_BY_PDE);
        }

        PointerPde += 1;
    }
}

#endif

__declspec(noinline)
PVOID
MiGetInstructionPointer (
    VOID
    )
{
    return (PVOID) _ReturnAddress ();
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\mmsup.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   mmsup.c

Abstract:

    This module contains the various routines for miscellaneous support
    operations for memory management.

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE, MmHibernateInformation)
#endif

//
// Data for is protection compatible.
//

WIN32_PROTECTION_MASK MmCompatibleProtectionMask[8] = {

    PAGE_NOACCESS,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY,

    PAGE_NOACCESS | PAGE_EXECUTE,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_EXECUTE | PAGE_EXECUTE_READ,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_READWRITE,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_READWRITE | PAGE_EXECUTE | PAGE_EXECUTE_READ | PAGE_EXECUTE_READWRITE | PAGE_EXECUTE_WRITECOPY,

    PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_EXECUTE | PAGE_EXECUTE_READ | PAGE_EXECUTE_WRITECOPY
};



LOGICAL
FASTCALL
MiIsProtectionCompatible (
    IN WIN32_PROTECTION_MASK OldWin32Protect,
    IN WIN32_PROTECTION_MASK NewWin32Protect
    )

/*++

Routine Description:

    This function takes two user supplied page protections and checks
    to see if the new protection is compatible with the old protection.

    protection        compatible protections

    NoAccess          NoAccess
    ReadOnly          NoAccess, ReadOnly, ReadWriteCopy
    ReadWriteCopy     NoAccess, ReadOnly, ReadWriteCopy
    ReadWrite         NoAccess, ReadOnly, ReadWriteCopy, ReadWrite
    Execute           NoAccess, Execute
    ExecuteRead       NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy
    ExecuteWrite      NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy, ReadWrite, ExecuteWrite
    ExecuteWriteCopy  NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy

Arguments:

    OldWin32Protect - Supplies the protection to be compatible with.

    NewWin32Protect - Supplies the protection to check out.

Return Value:

    Returns TRUE if the protection is compatible, FALSE if not.

Environment:

    Kernel Mode.

--*/

{
    MM_PROTECTION_MASK Mask;
    MM_PROTECTION_MASK PteProtection;
    WIN32_PROTECTION_MASK Win32ProtectMask;

    PteProtection = MiMakeProtectionMask (OldWin32Protect);

    if (PteProtection == MM_INVALID_PROTECTION) {
        return FALSE;
    }

    Mask = PteProtection & 0x7;

    Win32ProtectMask = MmCompatibleProtectionMask[Mask] | PAGE_GUARD | PAGE_NOCACHE | PAGE_WRITECOMBINE;

    if ((Win32ProtectMask | NewWin32Protect) != Win32ProtectMask) {
        return FALSE;
    }

    return TRUE;
}


LOGICAL
FASTCALL
MiIsPteProtectionCompatible (
    IN MM_PROTECTION_MASK PteProtection,
    IN WIN32_PROTECTION_MASK NewProtect
    )
{
    MM_PROTECTION_MASK Mask;
    WIN32_PROTECTION_MASK Win32ProtectMask;

    Mask = PteProtection & 0x7;

    Win32ProtectMask = MmCompatibleProtectionMask[Mask] | PAGE_GUARD | PAGE_NOCACHE | PAGE_WRITECOMBINE;

    if ((Win32ProtectMask | NewProtect) != Win32ProtectMask) {
        return FALSE;
    }
    return TRUE;
}


//
// Protection data for MiMakeProtectionMask
//

CCHAR MmUserProtectionToMask1[16] = {
                                 0,
                                 MM_NOACCESS,
                                 MM_READONLY,
                                 -1,
                                 MM_READWRITE,
                                 -1,
                                 -1,
                                 -1,
                                 MM_WRITECOPY,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1 };

CCHAR MmUserProtectionToMask2[16] = {
                                 0,
                                 MM_EXECUTE,
                                 MM_EXECUTE_READ,
                                 -1,
                                 MM_EXECUTE_READWRITE,
                                 -1,
                                 -1,
                                 -1,
                                 MM_EXECUTE_WRITECOPY,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1 };


MM_PROTECTION_MASK
FASTCALL
MiMakeProtectionMask (
    IN WIN32_PROTECTION_MASK Win32Protect
    )

/*++

Routine Description:

    This function takes a user supplied protection and converts it
    into a 5-bit protection code for the PTE.

Arguments:

    Win32Protect - Supplies the protection.

Return Value:

    Returns the protection code for use in the PTE.  Note that
    MM_INVALID_PROTECTION (-1) is returned for an invalid protection
    request.  Since valid PTE protections fit in 5 bits and are
    zero-extended, it's easy for callers to distinguish this.

Environment:

    Kernel Mode.

--*/

{
    ULONG Field1;
    ULONG Field2;
    MM_PROTECTION_MASK ProtectCode;

    if (Win32Protect >= (PAGE_WRITECOMBINE * 2)) {
        return MM_INVALID_PROTECTION;
    }

    Field1 = Win32Protect & 0xF;
    Field2 = (Win32Protect >> 4) & 0xF;

    //
    // Make sure at least one field is set.
    //

    if (Field1 == 0) {
        if (Field2 == 0) {

            //
            // Both fields are zero, return failure.
            //

            return MM_INVALID_PROTECTION;
        }
        ProtectCode = MmUserProtectionToMask2[Field2];
    }
    else {
        if (Field2 != 0) {
            //
            //  Both fields are non-zero, return failure.
            //

            return MM_INVALID_PROTECTION;
        }
        ProtectCode = MmUserProtectionToMask1[Field1];
    }

    if (ProtectCode == -1) {
        return MM_INVALID_PROTECTION;
    }

    if (Win32Protect & PAGE_GUARD) {

        if ((ProtectCode == MM_NOACCESS) ||
            (Win32Protect & (PAGE_NOCACHE | PAGE_WRITECOMBINE))) {

            //
            // Invalid protection -
            // guard and either no access, no cache or write combine.
            //

            return MM_INVALID_PROTECTION;
        }

        MI_ADD_GUARD (ProtectCode);
    }

    if (Win32Protect & PAGE_NOCACHE) {

        ASSERT ((Win32Protect & PAGE_GUARD) == 0);  // Already checked above

        if ((ProtectCode == MM_NOACCESS) ||
            (Win32Protect & PAGE_WRITECOMBINE)) {

            //
            // Invalid protection -
            // nocache and either no access or write combine.
            //

            return MM_INVALID_PROTECTION;
        }

        MI_ADD_NOCACHE (ProtectCode);
    }

    if (Win32Protect & PAGE_WRITECOMBINE) {

        ASSERT ((Win32Protect & (PAGE_GUARD|PAGE_NOACCESS)) == 0);  // Already checked above

        if (ProtectCode == MM_NOACCESS) {

            //
            // Invalid protection, no access and write combine.
            //

            return MM_INVALID_PROTECTION;
        }

        MI_ADD_WRITECOMBINE (ProtectCode);
    }

    return ProtectCode;
}


VOID
MiMakePdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine examines the specified Page Directory Parent Entry to
    determine if the page directory page mapped by the PPE exists.  If it does,
    then it examines the specified Page Directory Entry to determine if
    the page table page mapped by the PDE exists.

    If the page table page exists and is not currently in memory, the
    working set pushlock and, if held, the PFN lock are released and the
    page table page is faulted into the working set.  The pushlock and PFN
    lock are reacquired.

    If the PDE does not exist, a zero filled PTE is created and it
    too is brought into the working set.

Arguments:

    PointerPde - Supplies a pointer to the PDE to examine and bring
                 into the working set.

    TargetProcess - Supplies a pointer to the current process.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at or MM_NOIRQL
              if the caller does not hold the PFN lock.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set pushlock held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    PointerPpe = MiGetPteAddress (PointerPde);
    PointerPxe = MiGetPdeAddress (PointerPde);

    if ((PointerPxe->u.Hard.Valid == 1) &&
        (PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1)) {

        //
        // Already valid.
        //

        return;
    }

    if (OldIrql != MM_NOIRQL) {
        UNLOCK_PFN (OldIrql);
    }

    //
    // Page directory parent (or extended parent) entry not valid,
    // make it valid.
    //

    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

    do {

        ASSERT (KeAreAllApcsDisabled () == TRUE);

        //
        // Fault it in, this must be done one level at a time because the
        // fault handler checks for the preceding level already being valid
        // for system space addresses.
        //

        if (PointerPxe->u.Hard.Valid == 0) {
            MiMakeSystemAddressValid (PointerPpe, TargetProcess);
            ASSERT (PointerPxe->u.Hard.Valid == 1);
        }

        if (PointerPpe->u.Hard.Valid == 0) {
            MiMakeSystemAddressValid (PointerPde, TargetProcess);
            ASSERT (PointerPpe->u.Hard.Valid == 1);
        }

        MiMakeSystemAddressValid (PointerPte, TargetProcess);

        ASSERT (PointerPxe->u.Hard.Valid == 1);
        ASSERT (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);

    } while ((PointerPxe->u.Hard.Valid == 0) ||
             (PointerPpe->u.Hard.Valid == 0) ||
             (PointerPde->u.Hard.Valid == 0));

    if (OldIrql != MM_NOIRQL) {
        LOCK_PFN (OldIrql);
    }

    return;
}

ULONG
FASTCALL
MiMakeSystemAddressValid (
    IN PVOID PageTableVirtualAddress,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    PageTableVirtualAddress - Supplies the virtual address to make valid - this
                              MUST be a page table hierarchy address because
                              the process working set pushlock is used to
                              synchronize the validity check below.

    CurrentProcess - Supplies a pointer to the current process.

Return Value:

    Returns TRUE if the working set pushlock was released and wait performed,
    FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, working set pushlock held.

--*/

{
    NTSTATUS status;
    LOGICAL WsHeldSafe;
    LOGICAL WsHeldShared;
    ULONG Waited;
    PETHREAD Thread;

    Waited = FALSE;
    Thread = NULL;

    ASSERT (PageTableVirtualAddress > MM_HIGHEST_USER_ADDRESS);

    ASSERT ((PageTableVirtualAddress < MM_PAGED_POOL_START) ||
            (PageTableVirtualAddress > MmPagedPoolEnd));

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    while (!MiIsAddressValid (PageTableVirtualAddress, TRUE)) {

        //
        // The virtual address is not present.  Release
        // the working set pushlock and fault it in.
        //
        // The working set pushlock may have been acquired safely or unsafely
        // by our caller.  Handle both cases here and below.
        //

        if (Thread == NULL) {
            Thread = PsGetCurrentThread ();
        }

        UNLOCK_WS_REGARDLESS (Thread, CurrentProcess, WsHeldSafe, WsHeldShared);

        status = MmAccessFault (FALSE, PageTableVirtualAddress, KernelMode, NULL);

        if (!NT_SUCCESS (status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          1,
                          (ULONG)status,
                          (ULONG_PTR)CurrentProcess,
                          (ULONG_PTR)PageTableVirtualAddress);
        }

        LOCK_WS_REGARDLESS (Thread, CurrentProcess, WsHeldSafe, WsHeldShared);

        Waited = TRUE;
    }

    return Waited;
}


ULONG
FASTCALL
MiMakeSystemAddressValidPfnWs (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    CurrentProcess - Supplies a pointer to the current process (whose
                     working set pushlock is held).

    OldIrql - Supplies the IRQL the caller acquired the PFN lock.

Return Value:

    Returns TRUE if lock/pushlock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, PFN lock held, working set pushlock held.

--*/

{
    NTSTATUS status;
    ULONG Waited;
    LOGICAL WsHeldSafe;
    LOGICAL WsHeldShared;
    PETHREAD CurrentThread;

    ASSERT (OldIrql != MM_NOIRQL);
    Waited = FALSE;

    CurrentThread = PsGetCurrentThread ();

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    while (!MiIsAddressValid (VirtualAddress, FALSE)) {

        //
        // The virtual address is not present.  Release
        // the working set pushlock and PFN lock and fault it in.
        //
        // The working set pushlock may have been acquired safely
        // or unsafely by our caller.  Handle both cases here and below.
        //

        UNLOCK_PFN (OldIrql);

        UNLOCK_WS_REGARDLESS (CurrentThread, CurrentProcess, WsHeldSafe, WsHeldShared);

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, NULL);

        if (!NT_SUCCESS (status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          2,
                          (ULONG)status,
                          (ULONG_PTR)CurrentProcess,
                          (ULONG_PTR)VirtualAddress);
        }

        LOCK_WS_REGARDLESS (CurrentThread, CurrentProcess, WsHeldSafe, WsHeldShared);

        LOCK_PFN (OldIrql);

        Waited = TRUE;
    }
    return Waited;
}

ULONG
FASTCALL
MiMakeSystemAddressValidPfnSystemWs (
    IN PVOID VirtualAddress,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    Returns TRUE if lock/pushlock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, PFN lock held, working set pushlock held.

--*/

{
    PMMSUPPORT Ws;
    NTSTATUS status;
    PETHREAD Thread;

    ASSERT (OldIrql != MM_NOIRQL);

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    if (MiIsAddressValid (VirtualAddress, TRUE)) {
        return FALSE;
    }

    //
    // The virtual address is not present.  Release
    // the PFN lock and fault it in.
    //

    UNLOCK_PFN (OldIrql);

    if (MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress)) {
        Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
    }
    else {
        Ws = &MmSystemCacheWs;
    }

    Thread = PsGetCurrentThread ();

    do {

        //
        // The virtual address is not present.  Release
        // the working set pushlock and fault it in.
        //

        UNLOCK_WORKING_SET (Thread, Ws);

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, NULL);

        if (!NT_SUCCESS (status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          2,
                          (ULONG)status,
                          (ULONG_PTR)0,
                          (ULONG_PTR)VirtualAddress);
        }

        LOCK_WORKING_SET (Thread, Ws);

    } while (!MiIsAddressValid (VirtualAddress, TRUE));

    LOCK_PFN (OldIrql);

    return TRUE;
}

ULONG
FASTCALL
MiMakeSystemAddressValidPfn (
    IN PVOID VirtualAddress,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, only the PFN lock held.

--*/

{
    NTSTATUS status;

    ULONG Waited = FALSE;

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    while (!MiIsAddressValid (VirtualAddress, FALSE)) {

        //
        // The virtual address is not present.  Release
        // the PFN lock and fault it in.
        //

        UNLOCK_PFN (OldIrql);

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, NULL);

        if (!NT_SUCCESS (status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          3,
                          (ULONG)status,
                          (ULONG_PTR)VirtualAddress,
                          0);
        }

        LOCK_PFN (OldIrql);

        Waited = TRUE;
    }

    return Waited;
}

VOID
FASTCALL
MiLockPagedAddress (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode.

--*/

{

    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPTE PointerPte;

    PointerPte = MiGetPteAddress (VirtualAddress);

    //
    // The address must be within paged pool.
    //

    LOCK_PFN (OldIrql);

    if (PointerPte->u.Hard.Valid == 0) {
        MiMakeSystemAddressValidPfn (VirtualAddress, OldIrql);
    }

    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

    MI_ADD_LOCKED_PAGE_CHARGE (Pfn1);

    UNLOCK_PFN (OldIrql);

    return;
}


VOID
FASTCALL
MiUnlockPagedAddress (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine unlocks a previously locked paged pool address.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPFN Pfn1;
    MMPTE PteContents;
    PMMPTE PointerPte;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;

    PointerPte = MiGetPteAddress (VirtualAddress);

    //
    // Address must be within paged pool.
    //

    PteContents = *PointerPte;
    ASSERT (PteContents.u.Hard.Valid == 1);
    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    LOCK_PFN2 (OldIrql);

    ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);

    UNLOCK_PFN2 (OldIrql);

    return;
}

VOID
FASTCALL
MiZeroPhysicalPage (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and fills the page with zeros.

Arguments:

    PageFrameIndex - Supplies the physical page number to fill with zeroes.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    PVOID VirtualAddress;
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();

    VirtualAddress = MiMapPageInHyperSpace (Process, PageFrameIndex, &OldIrql);
    KeZeroSinglePage (VirtualAddress);
    MiUnmapPageInHyperSpace (Process, VirtualAddress, OldIrql);

    return;
}

VOID
FASTCALL
MiRestoreTransitionPte (
    IN PMMPFN Pfn1
    )

/*++

Routine Description:

    This procedure restores the original contents into the PTE (which could
    be a prototype PTE) referred to by the PFN database for the specified
    physical page.  It also updates all necessary data structures to
    reflect the fact that the referenced PTE is no longer in transition.

    The physical address of the referenced PTE is mapped into hyper space
    of the current process and the PTE is then updated.

Arguments:

    Pfn1 - Supplies the PFN element which refers to a transition PTE.

Return Value:

    none.

Environment:

    Must be holding the PFN lock.

--*/

{
    PMMPFN Pfn2;
    PMMPTE PointerPte;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PEPROCESS Process;
    PFN_NUMBER PageTableFrameIndex;

    Process = NULL;

    ASSERT (Pfn1->u3.e1.PageLocation == StandbyPageList);

    if (Pfn1->u3.e1.PrototypePte) {

        if (MiIsProtoAddressValid (Pfn1->PteAddress)) {
            PointerPte = Pfn1->PteAddress;
        }
        else {

            //
            // The page containing the prototype PTE is not valid,
            // map the page into hyperspace and reference it that way.
            //

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == MI_PFN_ELEMENT_TO_INDEX (Pfn1)) &&
                 (PointerPte->u.Hard.Valid == 0));

        //
        // This page is referenced by a prototype PTE.  The
        // segment structures need to be updated when the page
        // is removed from the transition state.
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype) {

            //
            // The prototype PTE is in subsection format, calculate the
            // address of the control area for the subsection and decrement
            // the number of PFN references to the control area.
            //
            // Calculate address of subsection for this prototype PTE.
            //

            Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
            ControlArea = Subsection->ControlArea;
            ControlArea->NumberOfPfnReferences -= 1;
            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

            MiCheckForControlAreaDeletion (ControlArea);
        }

    }
    else {

        //
        // The page points to a page or page table page which may not be
        // for the current process.  Map the page into hyperspace and
        // reference it through hyperspace.  If the page resides in
        // system space (but not session space), it does not need to be
        // mapped as all PTEs for system space must be resident.  Session
        // space PTEs are only mapped per session so access to them must
        // also go through hyperspace.
        //

        PointerPte = Pfn1->PteAddress;

        if (PointerPte < MiGetPteAddress ((PVOID)MM_SYSTEM_SPACE_START) ||
	       MI_IS_SESSION_PTE (PointerPte)) {

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                       MiGetByteOffset(Pfn1->PteAddress));
        }
        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == MI_PFN_ELEMENT_TO_INDEX (Pfn1)) &&
                 (PointerPte->u.Hard.Valid == 0));

        MI_CAPTURE_USED_PAGETABLE_ENTRIES (Pfn1);
    }

    ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);
    ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
             (Pfn1->OriginalPte.u.Soft.Transition == 1)));

    MI_WRITE_INVALID_PTE_WITHOUT_WS (PointerPte, Pfn1->OriginalPte);

    if (Process != NULL) {
        MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
    }

    //
    // The page is being reused, set the PFN priority back to the default.
    //

    MI_RESET_PFN_PRIORITY (Pfn1);

    //
    // The PTE has been restored to its original contents and is
    // no longer in transition.  Decrement the share count on
    // the page table page which contains the PTE.
    //

    PageTableFrameIndex = Pfn1->u4.PteFrame;
    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

    return;
}

PSUBSECTION
MiGetSubsectionAndProtoFromPte (
    IN PMMPTE PointerPte,
    OUT PMMPTE *ProtoPte
    )

/*++

Routine Description:

    This routine examines the contents of the supplied PTE (which must
    map a page within a section) and determines the address of the
    subsection in which the PTE is contained.

Arguments:

    PointerPte - Supplies a pointer to the PTE.

    ProtoPte - Supplies a pointer to a PMMPTE which receives the
               address of the prototype PTE which is mapped by the supplied
               PointerPte.

Return Value:

    Returns the pointer to the subsection for this PTE.

Environment:

    Kernel mode - Must be holding the PFN lock and
                  working set pushlock (acquired safely) with APCs disabled.

--*/

{
    PMMPTE PointerProto;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PSUBSECTION Subsection;

    LOCK_PFN (OldIrql);

    if (PointerPte->u.Hard.Valid == 1) {
        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
        *ProtoPte = Pfn1->PteAddress;
        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
        UNLOCK_PFN (OldIrql);
        return Subsection;
    }

    PointerProto = MiPteToProto (PointerPte);
    *ProtoPte = PointerProto;

    if (MiGetPteAddress (PointerProto)->u.Hard.Valid == 0) {
        MiMakeSystemAddressValidPfn (PointerProto, OldIrql);
    }

    if (PointerProto->u.Hard.Valid == 1) {

        //
        // Prototype PTE is valid.
        //

        Pfn1 = MI_PFN_ELEMENT (PointerProto->u.Hard.PageFrameNumber);
        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
        UNLOCK_PFN (OldIrql);
        return Subsection;
    }

    if ((PointerProto->u.Soft.Transition == 1) &&
         (PointerProto->u.Soft.Prototype == 0)) {

        //
        // Prototype PTE is in transition.
        //

        Pfn1 = MI_PFN_ELEMENT (PointerProto->u.Trans.PageFrameNumber);
        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
        UNLOCK_PFN (OldIrql);
        return Subsection;
    }

    ASSERT (PointerProto->u.Soft.Prototype == 1);
    Subsection = MiGetSubsectionAddress (PointerProto);
    UNLOCK_PFN (OldIrql);

    return Subsection;
}

BOOLEAN
MmIsNonPagedSystemAddressValid (
    __in PVOID VirtualAddress
    )

/*++

Routine Description:

    For a given virtual address this function returns TRUE if the address
    is within the non-pageable portion of the system's address space,
    FALSE otherwise.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if the address is within the non-pageable portion of the system
    address space, FALSE otherwise.

Environment:

    Kernel mode.

--*/

{
    //
    // Return TRUE if address is within the non-pageable portion
    // of the system.  Check limits for paged pool and if not within
    // those limits, return TRUE.
    //

    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress <= MmPagedPoolEnd)) {
        return FALSE;
    }

    //
    // Check special pool before checking session space because on NT64
    // nonpaged session pool exists in session space (on NT32, nonpaged
    // session requests are satisfied from systemwide nonpaged pool instead).
    //

    if (MmIsSpecialPoolAddress (VirtualAddress)) {
        if (MmQuerySpecialPoolBlockType (VirtualAddress) & PagedPool) {
            return FALSE;
        }
        return TRUE;
    }

    if ((VirtualAddress >= (PVOID) MmSessionBase) &&
        (VirtualAddress < (PVOID) MiSessionSpaceEnd)) {
        return FALSE;
    }

    return TRUE;
}

VOID
MmHibernateInformation (
    IN PVOID    MemoryMap,
    OUT PULONG_PTR  HiberVa,
    OUT PPHYSICAL_ADDRESS HiberPte
    )
{
    //
    // Mark PTE page where the 16 dump PTEs reside as needing cloning.
    //

    PoSetHiberRange (MemoryMap, PO_MEM_CLONE, MmCrashDumpPte, 1, ' etP');

    //
    // Return the dump PTEs to the loader (as it needs to use them
    // to map it's relocation code into the kernel space on the
    // final bit of restoring memory).
    //

    *HiberVa = (ULONG_PTR) MiGetVirtualAddressMappedByPte(MmCrashDumpPte);
    *HiberPte = MmGetPhysicalAddress(MmCrashDumpPte);
}

#if defined (_WIN64)

PVOID
MmGetMaxWowAddress (
    VOID
    )

/*++

Routine Description:

    This function returns the WOW usermode address boundary.

Arguments:

    None.

Return Value:

    The highest Wow usermode address boundary.

Environment:

    The calling process must be the relevant wow64 process as each process
    can have a different limit (based on its PE header, etc).

--*/

{
    if (PsGetCurrentProcess()->Wow64Process == NULL) {
        return NULL;
    }

    ASSERT (MmWorkingSetList->HighestUserAddress != NULL);

    return MmWorkingSetList->HighestUserAddress;
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\modwrite.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

    modwrite.c

Abstract:

    This module contains the modified page writer for memory management.

--*/

#include "mi.h"

typedef enum _MODIFIED_WRITER_OBJECT {
    NormalCase,
    MappedPagesNeedWriting,
    ModifiedWriterMaximumObject
} MODIFIED_WRITER_OBJECT;

typedef struct _MM_WRITE_CLUSTER {
    ULONG Count;
    ULONG StartIndex;
    ULONG Cluster[2 * (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE) + 1];
} MM_WRITE_CLUSTER, *PMM_WRITE_CLUSTER;

LONG MmWriteAllModifiedPages;
LOGICAL MiFirstPageFileCreatedAndReady = FALSE;

LOGICAL MiDrainingMappedWrites = FALSE;

ULONG MmNumberOfMappedMdls;
#if DBG
ULONG MmNumberOfMappedMdlsInUse;
ULONG MmNumberOfMappedMdlsInUsePeak;

typedef struct _MM_MODWRITE_ERRORS {
    NTSTATUS Status;
    ULONG Count;
} MM_MODWRITE_ERRORS, *PMM_MODWRITE_ERRORS;

#define MM_MAX_MODWRITE_ERRORS  8
MM_MODWRITE_ERRORS MiModwriteErrors[MM_MAX_MODWRITE_ERRORS];
#endif

LONG MiClusterWritesDisabled;

#define MI_SLOW_CLUSTER_WRITES   10

#define ONEMB_IN_PAGES  ((1024 * 1024) / PAGE_SIZE)

NTSTATUS MiLastModifiedWriteError;
NTSTATUS MiLastMappedWriteError;

//
// Keep separate counters for the mapped and modified writer threads.  This
// way they can both be read and updated without locks.
//

#define MI_MAXIMUM_PRIORITY_BURST   32

ULONG MiMappedWriteBurstCount;
ULONG MiModifiedWriteBurstCount;

VOID
MiClusterWritePages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex,
    IN PMM_WRITE_CLUSTER WriteCluster,
    IN ULONG Size
    );

VOID
MiExtendPagingFileMaximum (
    IN ULONG PageFileNumber,
    IN PRTL_BITMAP NewBitmap
    );

SIZE_T
MiAttemptPageFileExtension (
    IN ULONG PageFileNumber,
    IN SIZE_T SizeNeeded,
    IN LOGICAL Maximum
    );

NTSTATUS
MiZeroPageFileFirstPage (
    IN PFILE_OBJECT File
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtCreatePagingFile)
#pragma alloc_text(PAGE,MmGetPageFileInformation)
#pragma alloc_text(PAGE,MmGetSystemPageFile)
#pragma alloc_text(PAGE,MiLdwPopupWorker)
#pragma alloc_text(PAGE,MiAttemptPageFileExtension)
#pragma alloc_text(PAGE,MiExtendPagingFiles)
#pragma alloc_text(PAGE,MiZeroPageFileFirstPage)
#pragma alloc_text(PAGELK,MiModifiedPageWriter)
#endif


extern POBJECT_TYPE IoFileObjectType;

extern SIZE_T MmSystemCommitReserve;

LIST_ENTRY MmMappedPageWriterList;

KEVENT MmMappedPageWriterEvent;

KEVENT MmMappedFileIoComplete;

ULONG MmSystemShutdown;

BOOLEAN MmSystemPageFileLocated;

NTSTATUS
MiCheckPageFileMapping (
    IN PFILE_OBJECT File
    );

VOID
MiInsertPageFileInList (
    VOID
    );

VOID
MiGatherMappedPages (
    IN KIRQL OldIrql
    );

VOID
MiGatherPagefilePages (
    IN PMMMOD_WRITER_MDL_ENTRY ModWriterEntry,
    IN PRTL_BITMAP Bitmap
    );

VOID
MiPageFileFull (
    VOID
    );

#if DBG
ULONG_PTR MmPagingFileDebug[8192];
#endif

#define MINIMUM_PAGE_FILE_SIZE ((ULONG)(256*PAGE_SIZE))

//
// Log pagefile writes so that scheduling, filesystem and storage stack
// problems can be tracked down.
//

#define MI_TRACK_PAGEFILE_WRITES 0x100

typedef struct _MI_PAGEFILE_TRACES {

    NTSTATUS Status;
    UCHAR Priority;
    UCHAR IrpPriority;
    LARGE_INTEGER CurrentTime;

    PFN_NUMBER AvailablePages;
    PFN_NUMBER ModifiedPagesTotal;
    PFN_NUMBER ModifiedPagefilePages;
    PFN_NUMBER ModifiedNoWritePages;

    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];

} MI_PAGEFILE_TRACES, *PMI_PAGEFILE_TRACES;

LONG MiPageFileTraceIndex;

MI_PAGEFILE_TRACES MiPageFileTraces[MI_TRACK_PAGEFILE_WRITES];

VOID
FORCEINLINE
MiSnapPagefileWrite (
    IN PMMMOD_WRITER_MDL_ENTRY ModWriterEntry,
    IN PLARGE_INTEGER CurrentTime,
    IN IO_PAGING_PRIORITY IrpPriority,
    IN NTSTATUS Status
    )
{
    PMI_PAGEFILE_TRACES Information;
    ULONG Index;

    Index = InterlockedIncrement (&MiPageFileTraceIndex);
    Index &= (MI_TRACK_PAGEFILE_WRITES - 1);
    Information = &MiPageFileTraces[Index];

    Information->Status = Status;
    Information->Priority = (UCHAR) KeGetCurrentThread()->Priority;
    Information->IrpPriority = (UCHAR) IrpPriority;
    Information->CurrentTime = *CurrentTime;

    Information->AvailablePages = MmAvailablePages;
    Information->ModifiedPagesTotal = MmModifiedPageListHead.Total;
    Information->ModifiedPagefilePages = MmTotalPagesForPagingFile;
    Information->ModifiedNoWritePages = MmModifiedNoWritePageListHead.Total;

    RtlCopyMemory (Information->MdlHack,
                   &ModWriterEntry->Mdl,
                   sizeof (Information->MdlHack));
}

#if MI_TRACK_PAGEFILE_WRITES
#define MI_PAGEFILE_WRITE(ModWriterEntry,CurrentTime,IrpPriority,Status) \
            MiSnapPagefileWrite(ModWriterEntry,CurrentTime,IrpPriority,Status)
#else
#define MI_PAGEFILE_WRITE(ModWriterEntry,CurrentTime,IrpPriority,Status)
#endif

VOID
MiModifiedPageWriterWorker (
    VOID
    );


VOID
MiReleaseModifiedWriter (
    VOID
    )

/*++

Routine Description:

    Non-pageable wrapper to signal the modified writer when the first pagefile
    creation has completely finished.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    MiFirstPageFileCreatedAndReady = TRUE;
    UNLOCK_PFN (OldIrql);
}

NTSTATUS
MiZeroPageFileFirstPage (
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    This routine zeroes the first page of the newly created paging file
    to ensure no stale crashdump signatures get to live on.

Arguments:

    File - Supplies a pointer to the file object for the paging file.

Return Value:

    NTSTATUS.

--*/

{
    PMDL Mdl;
    LARGE_INTEGER Offset = {0};
    PULONG Block;
    IO_STATUS_BLOCK IoStatus;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    KEVENT Event;

    Mdl = (PMDL)&MdlHack[0];

    MmCreateMdl (Mdl, NULL, PAGE_SIZE);

    Mdl->MdlFlags |= MDL_PAGES_LOCKED;

    Page = (PPFN_NUMBER)(Mdl + 1);

    *Page = MiGetPageForHeader (FALSE);

    Block = MmGetSystemAddressForMdlSafe (Mdl, HighPagePriority);

    if (Block != NULL) {
        KeZeroSinglePage (Block);
    }
    else {
        MiZeroPhysicalPage (*Page);
    }

    KeInitializeEvent (&Event, NotificationEvent, FALSE);

    Status = IoSynchronousPageWrite (File,
                                     Mdl,
                                     &Offset,
                                     &Event,
                                     &IoStatus);

    if (NT_SUCCESS (Status)) {

        KeWaitForSingleObject (&Event,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               NULL);

        Status = IoStatus.Status;
    }

    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
    }

    MiRemoveImageHeaderPage (*Page);

    return Status;
}


NTSTATUS
NtCreatePagingFile (
    __in PUNICODE_STRING PageFileName,
    __in PLARGE_INTEGER MinimumSize,
    __in PLARGE_INTEGER MaximumSize,
    __in ULONG Priority
    )

/*++

Routine Description:

    This routine opens the specified file, attempts to write a page
    to the specified file, and creates the necessary structures to
    use the file as a paging file.

    If this file is the first paging file, the modified page writer
    is started.

    This system service requires the caller to have SeCreatePagefilePrivilege.

Arguments:

    PageFileName - Supplies the fully qualified file name.

    MinimumSize - Supplies the starting size of the paging file.
                  This value is rounded up to the host page size.

    MaximumSize - Supplies the maximum number of bytes to write to the file.
                  This value is rounded up to the host page size.

    Priority - Supplies the relative priority of this paging file.

--*/

{
    ULONG i;
    PFILE_OBJECT File;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES PagingFileAttributes;
    HANDLE FileHandle;
    IO_STATUS_BLOCK IoStatus;
    UNICODE_STRING CapturedName;
    PWSTR CapturedBuffer;
    LARGE_INTEGER CapturedMaximumSize;
    LARGE_INTEGER CapturedMinimumSize;
    FILE_END_OF_FILE_INFORMATION EndOfFileInformation;
    KPROCESSOR_MODE PreviousMode;
    FILE_FS_DEVICE_INFORMATION FileDeviceInfo;
    ULONG ReturnedLength;
    ULONG PageFileNumber;
    ULONG NewMaxSizeInPages;
    ULONG NewMinSizeInPages;
    PMMPAGING_FILE FoundExisting;
    PMMPAGING_FILE NewPagingFile;
    PRTL_BITMAP NewBitmap;
    PDEVICE_OBJECT deviceObject;
    MMPAGE_FILE_EXPANSION PageExtend;
    SECURITY_DESCRIPTOR SecurityDescriptor;
    ULONG DaclLength;
    PACL Dacl;


    DBG_UNREFERENCED_PARAMETER (Priority);

    PAGED_CODE();

    CapturedBuffer = NULL;
    Dacl = NULL;

    if (MmNumberOfPagingFiles == MAX_PAGE_FILES) {

        //
        // The maximum number of paging files is already in use.
        //

        return STATUS_TOO_MANY_PAGING_FILES;
    }

    PreviousMode = KeGetPreviousMode();

    if (PreviousMode != KernelMode) {

        //
        // Make sure the caller has the proper privilege for this.
        //

        if (!SeSinglePrivilegeCheck (SeCreatePagefilePrivilege, PreviousMode)) {
            return STATUS_PRIVILEGE_NOT_HELD;
        }

        //
        // Probe arguments.
        //

        try {

#if !defined (_WIN64)

            //
            // Note we only probe for byte alignment because early releases
            // of NT did and we don't want to break user apps
            // that had bad alignment if they worked before.
            //

            ProbeForReadSmallStructure (PageFileName,
                                        sizeof(*PageFileName),
                                        sizeof(UCHAR));
#else
            ProbeForReadSmallStructure (PageFileName,
                                        sizeof(*PageFileName),
                                        PROBE_ALIGNMENT (UNICODE_STRING));
#endif

            ProbeForReadSmallStructure (MaximumSize,
                                        sizeof(LARGE_INTEGER),
                                        PROBE_ALIGNMENT (LARGE_INTEGER));

            ProbeForReadSmallStructure (MinimumSize,
                                        sizeof(LARGE_INTEGER),
                                        PROBE_ALIGNMENT (LARGE_INTEGER));

            //
            // Capture arguments.
            //

            CapturedMinimumSize = *MinimumSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {

        //
        // Capture arguments.
        //

        CapturedMinimumSize = *MinimumSize;
    }

    if ((CapturedMinimumSize.QuadPart > MI_MAXIMUM_PAGEFILE_SIZE) ||
        (CapturedMinimumSize.LowPart < MINIMUM_PAGE_FILE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    if (PreviousMode != KernelMode) {

        try {
            CapturedMaximumSize = *MaximumSize;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {
        CapturedMaximumSize = *MaximumSize;
    }

    if (CapturedMaximumSize.QuadPart > MI_MAXIMUM_PAGEFILE_SIZE) {
        return STATUS_INVALID_PARAMETER_3;
    }

    if (CapturedMinimumSize.QuadPart > CapturedMaximumSize.QuadPart) {
        return STATUS_INVALID_PARAMETER_3;
    }

    if (PreviousMode != KernelMode) {
        try {
            CapturedName = *PageFileName;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {
        CapturedName = *PageFileName;
    }

    CapturedName.MaximumLength = CapturedName.Length;

    if ((CapturedName.Length == 0) ||
        (CapturedName.Length > MAXIMUM_FILENAME_LENGTH )) {
        return STATUS_OBJECT_NAME_INVALID;
    }

    CapturedBuffer = ExAllocatePoolWithTag (PagedPool,
                                            (ULONG)CapturedName.Length,
                                            '  mM');

    if (CapturedBuffer == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (PreviousMode != KernelMode) {
        try {

            ProbeForRead (CapturedName.Buffer,
                          CapturedName.Length,
                          sizeof (UCHAR));

            //
            // Copy the string to the allocated buffer.
            //

            RtlCopyMemory (CapturedBuffer,
                           CapturedName.Buffer,
                           CapturedName.Length);

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            ExFreePool (CapturedBuffer);

            return GetExceptionCode();
        }
    }
    else {

        //
        // Copy the string to the allocated buffer.
        //

        RtlCopyMemory (CapturedBuffer,
                       CapturedName.Buffer,
                       CapturedName.Length);
    }

    //
    // Point the buffer to the string that was just copied.
    //

    CapturedName.Buffer = CapturedBuffer;

    //
    // Create a security descriptor to protect the pagefile
    //
    Status = RtlCreateSecurityDescriptor (&SecurityDescriptor,
                                          SECURITY_DESCRIPTOR_REVISION);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
    DaclLength = sizeof (ACL) + sizeof (ACCESS_ALLOWED_ACE) * 2 +
                 RtlLengthSid (SeLocalSystemSid) +
                 RtlLengthSid (SeAliasAdminsSid);

    Dacl = ExAllocatePoolWithTag (PagedPool, DaclLength, 'lcaD');

    if (Dacl == NULL) {
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn1;
    }

    Status = RtlCreateAcl (Dacl, DaclLength, ACL_REVISION);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     FILE_ALL_ACCESS,
                                     SeAliasAdminsSid);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     FILE_ALL_ACCESS,
                                     SeLocalSystemSid);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
  
    Status = RtlSetDaclSecurityDescriptor (&SecurityDescriptor,
                                           TRUE,
                                           Dacl,
                                           FALSE);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
  

    //
    // Open a paging file and get the size.
    //

    InitializeObjectAttributes (&PagingFileAttributes,
                                &CapturedName,
                                (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                NULL,
                                &SecurityDescriptor);

//
// Note this macro cannot use ULONG_PTR as it must also work on PAE.
//

#define ROUND64_TO_PAGES(Size)  (((ULONG64)(Size) + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))

    EndOfFileInformation.EndOfFile.QuadPart =
                                ROUND64_TO_PAGES (CapturedMinimumSize.QuadPart);

    Status = IoCreateFile (&FileHandle,
                           FILE_READ_DATA | FILE_WRITE_DATA | WRITE_DAC | SYNCHRONIZE,
                           &PagingFileAttributes,
                           &IoStatus,
                           &CapturedMinimumSize,
                           FILE_ATTRIBUTE_HIDDEN | FILE_ATTRIBUTE_SYSTEM,
                           FILE_SHARE_WRITE,
                           FILE_SUPERSEDE,
                           FILE_NO_INTERMEDIATE_BUFFERING | FILE_NO_COMPRESSION | FILE_DELETE_ON_CLOSE,
                           NULL,
                           0L,
                           CreateFileTypeNone,
                           NULL,
                           IO_OPEN_PAGING_FILE | IO_NO_PARAMETER_CHECKING);

    if (NT_SUCCESS(Status)) {

        //
        // Update the DACL in case there was a pre-existing regular file named
        // pagefile.sys (even supersede above does not do this).
        //

        if (NT_SUCCESS(IoStatus.Status)) {

            Status = ZwSetSecurityObject (FileHandle,
                                          DACL_SECURITY_INFORMATION,
                                          &SecurityDescriptor);

            if (!NT_SUCCESS(Status)) {
                goto ErrorReturn2;
            }
        }
    }
    else {

        //
        // Treat this as an extension of an existing pagefile maximum -
        // and try to open rather than create the paging file specified.
        //

        Status = IoCreateFile (&FileHandle,
                           FILE_WRITE_DATA | SYNCHRONIZE,
                           &PagingFileAttributes,
                           &IoStatus,
                           &CapturedMinimumSize,
                           FILE_ATTRIBUTE_HIDDEN | FILE_ATTRIBUTE_SYSTEM,
                           FILE_SHARE_READ | FILE_SHARE_WRITE,
                           FILE_OPEN,
                           FILE_NO_INTERMEDIATE_BUFFERING | FILE_NO_COMPRESSION,
                           (PVOID) NULL,
                           0L,
                           CreateFileTypeNone,
                           (PVOID) NULL,
                           IO_OPEN_PAGING_FILE | IO_NO_PARAMETER_CHECKING);

        if (!NT_SUCCESS(Status)) {

#if DBG
            if (Status != STATUS_DISK_FULL) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                    "MM MODWRITE: unable to open paging file %wZ - status = %X \n", &CapturedName, Status);
            }
#endif

            goto ErrorReturn1;
        }

        Status = ObReferenceObjectByHandle (FileHandle,
                                            FILE_READ_DATA | FILE_WRITE_DATA,
                                            IoFileObjectType,
                                            KernelMode,
                                            (PVOID *)&File,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            goto ErrorReturn2;
        }

        FoundExisting = NULL;

        KeAcquireGuardedMutex (&MmPageFileCreationLock);

        for (PageFileNumber = 0; PageFileNumber < MmNumberOfPagingFiles; PageFileNumber += 1) {
            if (MmPagingFile[PageFileNumber]->File->SectionObjectPointer == File->SectionObjectPointer) {
                FoundExisting = MmPagingFile[PageFileNumber];
                break;
            }
        }

        if (FoundExisting == NULL) {
            Status = STATUS_NOT_FOUND;
            goto ErrorReturn4;
        }

        //
        // Check for increases in the minimum or the maximum paging file sizes.
        // Decreasing either paging file size on the fly is not allowed.
        //

        NewMaxSizeInPages = (ULONG)(CapturedMaximumSize.QuadPart >> PAGE_SHIFT);
        NewMinSizeInPages = (ULONG)(CapturedMinimumSize.QuadPart >> PAGE_SHIFT);

        if (FoundExisting->MinimumSize > NewMinSizeInPages) {
            Status = STATUS_INVALID_PARAMETER_2;
            goto ErrorReturn4;
        }

        if (FoundExisting->MaximumSize > NewMaxSizeInPages) {
            Status = STATUS_INVALID_PARAMETER_3;
            goto ErrorReturn4;
        }

        if (NewMaxSizeInPages > FoundExisting->MaximumSize) {

            //
            // Make sure that the pagefile increase doesn't cause the commit
            // limit (in pages) to wrap.  Currently this can only happen on
            // PAE systems where 16 pagefiles of 16TB (==256TB) is greater
            // than the 32-bit commit variable (max is 16TB).
            //

            if (MmTotalCommitLimitMaximum + (NewMaxSizeInPages - FoundExisting->MaximumSize) <= MmTotalCommitLimitMaximum) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn4;
            }

            //
            // Handle the increase to the maximum paging file size.
            //

            MiCreateBitMap (&NewBitmap, NewMaxSizeInPages, NonPagedPool);

            if (NewBitmap == NULL) {
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn4;
            }

            MiExtendPagingFileMaximum (PageFileNumber, NewBitmap);

            //
            // We may be low on commitment and/or may have put a temporary
            // stopgate on things.  Clear up the logjam now by forcing an
            // extension and immediately returning it.
            //

            if (MmTotalCommittedPages + 100 > MmTotalCommitLimit) {
                if (MiChargeCommitment (200, NULL) == TRUE) {
                    MiReturnCommitment (200);
                }
            }
        }

        if (NewMinSizeInPages > FoundExisting->MinimumSize) {

            //
            // Handle the increase to the minimum paging file size.
            //

            if (NewMinSizeInPages > FoundExisting->Size) {

                //
                // Queue a message to the segment dereferencing / pagefile
                // extending thread to see if the page file can be extended.
                //

                PageExtend.InProgress = 1;
                PageExtend.ActualExpansion = 0;
                PageExtend.RequestedExpansionSize = NewMinSizeInPages - FoundExisting->Size;
                PageExtend.Segment = NULL;
                PageExtend.PageFileNumber = PageFileNumber;
                KeInitializeEvent (&PageExtend.Event, NotificationEvent, FALSE);

                MiIssuePageExtendRequest (&PageExtend);
            }

            //
            // The current size is now greater than the new desired minimum.
            // Ensure subsequent contractions obey this new minimum.
            //

            if (FoundExisting->Size >= NewMinSizeInPages) {
                ASSERT (FoundExisting->Size >= FoundExisting->MinimumSize);
                ASSERT (NewMinSizeInPages >= FoundExisting->MinimumSize);
                FoundExisting->MinimumSize = NewMinSizeInPages;
            }
            else {

                //
                // The pagefile could not be expanded to handle the new minimum.
                // No easy way to undo any maximum raising that may have been
                // done as the space may have already been used, so just set
                // Status so our caller knows it didn't all go perfectly.
                //

                Status = STATUS_INSUFFICIENT_RESOURCES;
            }
        }

        goto ErrorReturn4;
    }

    //
    // Free the DACL as it's no longer needed.
    //

    ExFreePool (Dacl);
    Dacl = NULL;

    if (!NT_SUCCESS(IoStatus.Status)) {
        KdPrint(("MM MODWRITE: unable to open paging file %wZ - iosb %lx\n", &CapturedName, IoStatus.Status));
        Status = IoStatus.Status;
        goto ErrorReturn1;
    }

    //
    // Make sure that the pagefile increase doesn't cause the commit
    // limit (in pages) to wrap.  Currently this can only happen on
    // PAE systems where 16 pagefiles of 16TB (==256TB) is greater
    // than the 32-bit commit variable (max is 16TB).
    //

    if (MmTotalCommitLimitMaximum + (CapturedMaximumSize.QuadPart >> PAGE_SHIFT)
        <= MmTotalCommitLimitMaximum) {
        Status = STATUS_INVALID_PARAMETER_3;
        goto ErrorReturn2;
    }

    Status = ZwSetInformationFile (FileHandle,
                                   &IoStatus,
                                   &EndOfFileInformation,
                                   sizeof(EndOfFileInformation),
                                   FileEndOfFileInformation);

    if (!NT_SUCCESS(Status)) {
        KdPrint(("MM MODWRITE: unable to set length of paging file %wZ status = %X \n",
                 &CapturedName, Status));
        goto ErrorReturn2;
    }

    if (!NT_SUCCESS(IoStatus.Status)) {
        KdPrint(("MM MODWRITE: unable to set length of paging file %wZ - iosb %lx\n",
                &CapturedName, IoStatus.Status));
        Status = IoStatus.Status;
        goto ErrorReturn2;
    }

    Status = ObReferenceObjectByHandle ( FileHandle,
                                         FILE_READ_DATA | FILE_WRITE_DATA,
                                         IoFileObjectType,
                                         KernelMode,
                                         (PVOID *)&File,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        KdPrint(("MM MODWRITE: Unable to reference paging file - %wZ\n",
                 &CapturedName));
        goto ErrorReturn2;
    }

    //
    // Get the address of the target device object and ensure
    // the specified file is of a suitable type.
    //

    deviceObject = IoGetRelatedDeviceObject (File);

    if ((deviceObject->DeviceType != FILE_DEVICE_DISK_FILE_SYSTEM) &&
        (deviceObject->DeviceType != FILE_DEVICE_NETWORK_FILE_SYSTEM) &&
        (deviceObject->DeviceType != FILE_DEVICE_DFS_VOLUME) &&
        (deviceObject->DeviceType != FILE_DEVICE_DFS_FILE_SYSTEM)) {
            KdPrint(("MM MODWRITE: Invalid paging file type - %x\n",
                     deviceObject->DeviceType));
            Status = STATUS_UNRECOGNIZED_VOLUME;
            goto ErrorReturn3;
    }

    //
    // Make sure the specified file is not currently being used
    // as a mapped data file.
    //

    Status = MiCheckPageFileMapping (File);
    if (!NT_SUCCESS(Status)) {
        goto ErrorReturn3;
    }

    //
    // Make sure the volume is not a floppy disk.
    //

    Status = IoQueryVolumeInformation ( File,
                                        FileFsDeviceInformation,
                                        sizeof(FILE_FS_DEVICE_INFORMATION),
                                        &FileDeviceInfo,
                                        &ReturnedLength
                                      );

    if (FILE_FLOPPY_DISKETTE & FileDeviceInfo.Characteristics) {
        Status = STATUS_FLOPPY_VOLUME;
        goto ErrorReturn3;
    }

    //
    // Check with all of the drivers along the path to the file to ensure
    // that they are willing to follow the rules required of them and to
    // give them a chance to lock down code and data that needs to be locked.
    // If any of the drivers along the path refuses to participate, fail the
    // pagefile creation.
    //

    Status = PpPagePathAssign (File);

    if (!NT_SUCCESS(Status)) {
        KdPrint(( "PpPagePathAssign(%wZ) FAILED: %x\n", &CapturedName, Status ));
        //
        // Fail the pagefile creation if the storage stack tells us to.
        //

        goto ErrorReturn3;
    }

    NewPagingFile = ExAllocatePoolWithTag (NonPagedPool,
                                           sizeof(MMPAGING_FILE),
                                           '  mM');

    if (NewPagingFile == NULL) {

        //
        // Allocate pool failed.
        //

        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    RtlZeroMemory (NewPagingFile, sizeof(MMPAGING_FILE));

    NewPagingFile->File = File;
    NewPagingFile->FileHandle = FileHandle;
    NewPagingFile->Size = (PFN_NUMBER)(CapturedMinimumSize.QuadPart >> PAGE_SHIFT);
    NewPagingFile->MinimumSize = NewPagingFile->Size;
    NewPagingFile->FreeSpace = NewPagingFile->Size - 1;

    NewPagingFile->MaximumSize = (PFN_NUMBER)(CapturedMaximumSize.QuadPart >>
                                                PAGE_SHIFT);

    for (i = 0; i < MM_PAGING_FILE_MDLS; i += 1) {

        NewPagingFile->Entry[i] = ExAllocatePoolWithTag (NonPagedPool,
                                            sizeof(MMMOD_WRITER_MDL_ENTRY) +
                                            MmModifiedWriteClusterSize *
                                            sizeof(PFN_NUMBER),
                                            '  mM');

        if (NewPagingFile->Entry[i] == NULL) {

            //
            // Allocate pool failed.
            //

            while (i != 0) {
                i -= 1;
                ExFreePool (NewPagingFile->Entry[i]);
            }

            ExFreePool (NewPagingFile);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn3;
        }

        RtlZeroMemory (NewPagingFile->Entry[i], sizeof(MMMOD_WRITER_MDL_ENTRY));

        NewPagingFile->Entry[i]->PagingListHead = &MmPagingFileHeader;

        NewPagingFile->Entry[i]->PagingFile = NewPagingFile;
    }

    NewPagingFile->PageFileName = CapturedName;

    MiCreateBitMap (&NewPagingFile->Bitmap,
                    NewPagingFile->MaximumSize,
                    NonPagedPool);

    if (NewPagingFile->Bitmap == NULL) {

        //
        // Allocate pool failed.
        //

        ExFreePool (NewPagingFile->Entry[0]);
        ExFreePool (NewPagingFile->Entry[1]);
        ExFreePool (NewPagingFile);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    Status = MiZeroPageFileFirstPage (File);

    if (!NT_SUCCESS (Status)) {

        //
        // The storage stack could not zero the first page of the file.
        // This means an old crashdump signature could still be around so
        // fail the create.
        //

        for (i = 0; i < MM_PAGING_FILE_MDLS; i += 1) {
            ExFreePool (NewPagingFile->Entry[i]);
        }
        ExFreePool (NewPagingFile);
        MiRemoveBitMap (&NewPagingFile->Bitmap);
        goto ErrorReturn3;
    }

    RtlSetAllBits (NewPagingFile->Bitmap);

    //
    // Set the first bit as 0 is an invalid page location, clear the
    // following bits.
    //

    RtlClearBits (NewPagingFile->Bitmap,
                  1,
                  (ULONG)(NewPagingFile->Size - 1));

    //
    // See if this pagefile is on the boot partition, and if so, mark it
    // so we can find it later if someone enables crashdump.
    //

    if (File->DeviceObject->Flags & DO_SYSTEM_BOOT_PARTITION) {
        NewPagingFile->BootPartition = 1;
    }
    else {
        NewPagingFile->BootPartition = 0;
    }

    //
    // Acquire the global page file creation mutex.
    //

    KeAcquireGuardedMutex (&MmPageFileCreationLock);

    PageFileNumber = MmNumberOfPagingFiles;

    MmPagingFile[PageFileNumber] = NewPagingFile;

    NewPagingFile->PageFileNumber = PageFileNumber;

    MiInsertPageFileInList ();

    if (PageFileNumber == 0) {

        //
        // The first paging file has been created and reservation of any
        // crashdump pages has completed, signal the modified
        // page writer.
        //

        MiReleaseModifiedWriter ();
    }

    KeReleaseGuardedMutex (&MmPageFileCreationLock);

    //
    // Note that the file handle (a kernel handle) is not closed during the
    // create path (it IS duped and closed in the pagefile size extending path)
    // to prevent the paging file from being deleted or opened again.  It is
    // also kept open so that extensions of existing pagefiles can be detected
    // because successive IoCreateFile calls will fail.
    //

    if ((!MmSystemPageFileLocated) &&
        (File->DeviceObject->Flags & DO_SYSTEM_BOOT_PARTITION)) {
        MmSystemPageFileLocated = IoInitializeCrashDump (FileHandle);
    }

    return STATUS_SUCCESS;

    //
    // Error returns:
    //

ErrorReturn4:
    KeReleaseGuardedMutex (&MmPageFileCreationLock);

ErrorReturn3:
    ObDereferenceObject (File);

ErrorReturn2:
    ZwClose (FileHandle);

ErrorReturn1:
    if (Dacl != NULL) {
        ExFreePool (Dacl);
    }

    ExFreePool (CapturedBuffer);

    return Status;
}


HANDLE
MmGetSystemPageFile (
    VOID
    )
/*++

Routine Description:

    Returns a filehandle to the paging file on the system boot partition.
    This is used by crashdump to enable crashdump after the system has
    already booted.

Arguments:

    None.

Return Value:

    A file handle to the paging file on the system boot partition,
    NULL if no such pagefile exists.

--*/

{
    HANDLE FileHandle;
    ULONG PageFileNumber;

    PAGED_CODE();

    FileHandle = NULL;

    KeAcquireGuardedMutex (&MmPageFileCreationLock);
    for (PageFileNumber = 0; PageFileNumber < MmNumberOfPagingFiles; PageFileNumber += 1) {
        if (MmPagingFile[PageFileNumber]->BootPartition) {
            FileHandle = MmPagingFile[PageFileNumber]->FileHandle;
        }
    }
    KeReleaseGuardedMutex (&MmPageFileCreationLock);

    return FileHandle;
}


LOGICAL
MmIsFileObjectAPagingFile (
    IN PFILE_OBJECT FileObject
    )
/*++

Routine Description:

    Returns TRUE if the file object refers to a paging file, FALSE if not.

Arguments:

    FileObject - Supplies the file object in question.

Return Value:

    Returns TRUE if the file object refers to a paging file, FALSE if not.

    Note this routine is called both at DISPATCH_LEVEL by drivers in their
    completion routines and it is called in the path to satisfy pagefile reads,
    so it cannot be made pageable.

--*/

{
    PMMPAGING_FILE PageFile;
    PMMPAGING_FILE *PagingFile;
    PMMPAGING_FILE *PagingFileEnd;

    //
    // It's ok to check without synchronization.
    //

    PagingFile = MmPagingFile;
    PagingFileEnd = PagingFile + MmNumberOfPagingFiles;

    while (PagingFile < PagingFileEnd) {
        PageFile = *PagingFile;
        if (PageFile->File == FileObject) {
            return TRUE;
        }
        PagingFile += 1;
    }

    return FALSE;
}


VOID
MiExtendPagingFileMaximum (
    IN ULONG PageFileNumber,
    IN PRTL_BITMAP NewBitmap
    )

/*++

Routine Description:

    This routine switches from the old bitmap to the new (larger) bitmap.

Arguments:

    PageFileNumber - Supplies the paging file number to be extended.

    NewBitmap - Supplies the new bitmap to use.

Return Value:

    Returns a non-NULL value for the caller to free to pool

Environment:

    Kernel mode, APC_LEVEL, MmPageFileCreationLock held.

--*/

{
    KIRQL OldIrql;
    PRTL_BITMAP OldBitmap;
    SIZE_T Delta;

    OldBitmap = MmPagingFile[PageFileNumber]->Bitmap;

    RtlSetAllBits (NewBitmap);

    LOCK_PFN (OldIrql);

    //
    // Copy the bits from the existing map.
    //

    RtlCopyMemory (NewBitmap->Buffer,
                   OldBitmap->Buffer,
                   ((OldBitmap->SizeOfBitMap + 31) / 32) * sizeof (ULONG));

    Delta = NewBitmap->SizeOfBitMap - OldBitmap->SizeOfBitMap;

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, Delta);

    MmPagingFile[PageFileNumber]->MaximumSize = NewBitmap->SizeOfBitMap;

    MmPagingFile[PageFileNumber]->Bitmap = NewBitmap;

    //
    // If any MDLs are waiting for space, get them up now.
    //

    if (!IsListEmpty (&MmFreePagingSpaceLow)) {
        MiUpdateModifiedWriterMdls (PageFileNumber);
    }

    //
    // The modified writer may be scanning the old map without holding any
    // locks - if so, he will have reference counted the old bitmap, so only
    // free it if this isn't in progress.  If it is in progress, the modified
    // writer will free the old bitmap.
    //

    if (MmPagingFile[PageFileNumber]->ReferenceCount != 0) {
        ASSERT (MmPagingFile[PageFileNumber]->ReferenceCount == 1);
        OldBitmap = NULL;
    }

    UNLOCK_PFN (OldIrql);

    if (OldBitmap != NULL) {
        MiRemoveBitMap (&OldBitmap);
    }
}


VOID
MiFinishPageFileExtension (
    IN ULONG PageFileNumber,
    IN PFN_NUMBER AdditionalAllocation
    )

/*++

Routine Description:

    This routine finishes the specified page file extension.

Arguments:

    PageFileNumber - Supplies the page file number to attempt to extend.

    SizeNeeded - Supplies the number of pages to extend the file by.

    Maximum - Supplies TRUE if the page file should be extended
              by the maximum size possible, but not to exceed
              SizeNeeded.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPAGING_FILE PagingFile;

    //
    // Clear bits within the paging file bitmap to allow the extension
    // to take effect.
    //

    PagingFile = MmPagingFile[PageFileNumber];

    LOCK_PFN (OldIrql);

    ASSERT (RtlCheckBit (PagingFile->Bitmap, (ULONG) PagingFile->Size) == 1);

    RtlClearBits (PagingFile->Bitmap,
                  (ULONG)PagingFile->Size,
                  (ULONG)AdditionalAllocation);

    PagingFile->Size += AdditionalAllocation;
    PagingFile->FreeSpace += AdditionalAllocation;

    MiUpdateModifiedWriterMdls (PageFileNumber);

    UNLOCK_PFN (OldIrql);

    return;
}


SIZE_T
MiAttemptPageFileExtension (
    IN ULONG PageFileNumber,
    IN SIZE_T SizeNeeded,
    IN LOGICAL Maximum
    )

/*++

Routine Description:

    This routine attempts to extend the specified page file by SizeNeeded.

Arguments:

    PageFileNumber - Supplies the page file number to attempt to extend.

    SizeNeeded - Supplies the number of pages to extend the file by.

    Maximum - Supplies TRUE if the page file should be extended
              by the maximum size possible, but not to exceed
              SizeNeeded.

Return Value:

    Returns the size of the extension.  Zero if the page file cannot
    be extended.

--*/

{

    NTSTATUS status;
    FILE_FS_SIZE_INFORMATION FileInfo;
    FILE_END_OF_FILE_INFORMATION EndOfFileInformation;
    ULONG AllocSize;
    ULONG ReturnedLength;
    PFN_NUMBER PagesAvailable;
    SIZE_T SizeToExtend;
    SIZE_T MinimumExtension;
    LARGE_INTEGER BytesAvailable;

    //
    // Check to see if this page file is at the maximum.
    //

    if (MmPagingFile[PageFileNumber]->Size ==
                                    MmPagingFile[PageFileNumber]->MaximumSize) {
        return 0;
    }

    //
    // Find out how much free space is on this volume.
    //

    status = IoQueryVolumeInformation (MmPagingFile[PageFileNumber]->File,
                                       FileFsSizeInformation,
                                       sizeof(FileInfo),
                                       &FileInfo,
                                       &ReturnedLength);

    if (!NT_SUCCESS (status)) {

        //
        // The volume query did not succeed - return 0 indicating
        // the paging file was not extended.
        //

        return 0;
    }

    //
    // Attempt to extend by at least 16 megabytes, if that fails then attempt
    // for at least a megabyte.
    //

    MinimumExtension = MmPageFileExtension << 4;

retry:

    SizeToExtend = SizeNeeded;

    if (SizeNeeded < MinimumExtension) {
        SizeToExtend = MinimumExtension;
    }
    else {
        MinimumExtension = MmPageFileExtension;
    }

    //
    // Don't go over the maximum size for the paging file.
    //

    ASSERT (MmPagingFile[PageFileNumber]->MaximumSize >= MmPagingFile[PageFileNumber]->Size);

    PagesAvailable = MmPagingFile[PageFileNumber]->MaximumSize -
                     MmPagingFile[PageFileNumber]->Size;

    if (SizeToExtend > PagesAvailable) {
        SizeToExtend = PagesAvailable;

        if ((SizeToExtend < SizeNeeded) && (Maximum == FALSE)) {

            //
            // Can't meet the requested (mandatory) requirement.
            //

            return 0;
        }
    }

    //
    // See if there is enough space on the volume for the extension.
    //

    AllocSize = FileInfo.SectorsPerAllocationUnit * FileInfo.BytesPerSector;

    BytesAvailable = RtlExtendedIntegerMultiply (
                        FileInfo.AvailableAllocationUnits,
                        AllocSize);

    if ((UINT64)BytesAvailable.QuadPart > (UINT64)MmMinimumFreeDiskSpace) {

        BytesAvailable.QuadPart = BytesAvailable.QuadPart -
                                    (LONGLONG)MmMinimumFreeDiskSpace;

        if ((UINT64)BytesAvailable.QuadPart > (((UINT64)SizeToExtend) << PAGE_SHIFT)) {
            BytesAvailable.QuadPart = (((LONGLONG)SizeToExtend) << PAGE_SHIFT);
        }

        PagesAvailable = (PFN_NUMBER)(BytesAvailable.QuadPart >> PAGE_SHIFT);

        if ((Maximum == FALSE) && (PagesAvailable < SizeNeeded)) {

            //
            // Can't meet the requested (mandatory) requirement.
            //

            return 0;
        }

    }
    else {

        //
        // Not enough space is available period.
        //

        return 0;
    }

#if defined (_WIN64) || defined (_X86PAE_)
    EndOfFileInformation.EndOfFile.QuadPart =
              ((ULONG64)MmPagingFile[PageFileNumber]->Size + PagesAvailable) * PAGE_SIZE;
#else
    EndOfFileInformation.EndOfFile.LowPart =
              (MmPagingFile[PageFileNumber]->Size + PagesAvailable) * PAGE_SIZE;

    //
    // Set high part to zero as paging files are limited to 4GB.
    //

    EndOfFileInformation.EndOfFile.HighPart = 0;
#endif

    //
    // Attempt to extend the file by setting the end-of-file position.
    //

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    status = IoSetInformation (MmPagingFile[PageFileNumber]->File,
                               FileEndOfFileInformation,
                               sizeof(FILE_END_OF_FILE_INFORMATION),
                               &EndOfFileInformation);

    if (status != STATUS_SUCCESS) {
        KdPrint(("MM MODWRITE: page file extension failed %p %lx\n",PagesAvailable,status));

        if (MinimumExtension != MmPageFileExtension) {
            MinimumExtension = MmPageFileExtension;
            goto retry;
        }

        return 0;
    }

    MiFinishPageFileExtension (PageFileNumber, PagesAvailable);

    return PagesAvailable;
}

SIZE_T
MiExtendPagingFiles (
    IN PMMPAGE_FILE_EXPANSION PageExpand
    )

/*++

Routine Description:

    This routine attempts to extend the paging files to provide
    SizeNeeded bytes.

    Note - Page file expansion and page file reduction are synchronized
           because a single thread is responsible for performing the
           operation.  Hence, while expansion is occurring, a reduction
           request will be queued to the thread.

Arguments:

    PageFileNumber - Supplies the page file number to extend.
                     MI_EXTEND_ANY_PAGFILE indicates to extend any page file.

Return Value:

    Returns the size of the extension.  Zero if the page file(s) cannot
    be extended.

--*/

{
    SIZE_T DesiredQuota;
    ULONG PageFileNumber;
    SIZE_T ExtendedSize;
    SIZE_T SizeNeeded;
    ULONG i;
    SIZE_T CommitLimit;
    SIZE_T CommittedPages;

    DesiredQuota = PageExpand->RequestedExpansionSize;
    PageFileNumber = PageExpand->PageFileNumber;

    ASSERT (PageExpand->ActualExpansion == 0);

    ASSERT (PageFileNumber < MmNumberOfPagingFiles || PageFileNumber == MI_EXTEND_ANY_PAGEFILE);

    if (MmNumberOfPagingFiles == 0) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    if (PageFileNumber < MmNumberOfPagingFiles) {
        i = PageFileNumber;
        ExtendedSize = MmPagingFile[i]->MaximumSize - MmPagingFile[i]->Size;
        if (ExtendedSize < DesiredQuota) {
            InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
            return 0;
        }

        ExtendedSize = MiAttemptPageFileExtension (i, DesiredQuota, FALSE);
        goto alldone;
    }

    //
    // Snap the globals into locals so calculations will be consistent from
    // step to step.  It is ok to snap the globals unsynchronized with respect
    // to each other as even when pagefile expansion occurs, the expansion
    // space is not reserved for the caller - any process could consume
    // the expansion prior to this routine returning.
    //

    CommittedPages = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    SizeNeeded = CommittedPages + DesiredQuota + MmSystemCommitReserve;

    //
    // Check to make sure the request does not wrap.
    //

    if (SizeNeeded < CommittedPages) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Check to see if ample space already exists.
    //

    if (SizeNeeded <= CommitLimit) {
        PageExpand->ActualExpansion = 1;
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 1;
    }

    //
    // Calculate the additional pages needed.
    //

    SizeNeeded -= CommitLimit;
    if (SizeNeeded > MmSystemCommitReserve) {
        SizeNeeded -= MmSystemCommitReserve;
    }

    //
    // Make sure ample space exists within the paging files.
    //

    i = 0;
    ExtendedSize = 0;

    do {
        ExtendedSize += MmPagingFile[i]->MaximumSize - MmPagingFile[i]->Size;
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    if (ExtendedSize < SizeNeeded) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Attempt to extend only one of the paging files.
    //

    i = 0;
    do {
        ExtendedSize = MiAttemptPageFileExtension (i, SizeNeeded, FALSE);
        if (ExtendedSize != 0) {
            goto alldone;
        }
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    ASSERT (ExtendedSize == 0);

    if (MmNumberOfPagingFiles == 1) {

        //
        // If the attempt didn't succeed for one (not enough disk space free) -
        // don't try to set it to the maximum size.
        //

        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Attempt to extend all paging files.
    //

    i = 0;
    do {
        ASSERT (SizeNeeded > ExtendedSize);
        ExtendedSize += MiAttemptPageFileExtension (i,
                                                    SizeNeeded - ExtendedSize,
                                                    TRUE);
        if (ExtendedSize >= SizeNeeded) {
            goto alldone;
        }
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    //
    // Not enough space is available.
    //

    InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
    return 0;

alldone:

    ASSERT (ExtendedSize != 0);

    PageExpand->ActualExpansion = ExtendedSize;

    //
    // Increase the systemwide commit limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, ExtendedSize);

    //
    // Clear the in progress flag - if this is the global cantexpand structure
    // it is possible for it to be immediately reused.
    //

    InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);

    return ExtendedSize;
}

MMPAGE_FILE_EXPANSION MiPageFileContract;


VOID
MiContractPagingFiles (
    VOID
    )

/*++

Routine Description:

    This routine checks to see if ample space is no longer committed
    and if so, does enough free space exist in any paging file.

    IFF the answer to both these is affirmative, a reduction in the
    paging file size(s) is attempted.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PMMPAGE_FILE_EXPANSION PageReduce;

    //
    // This is an unsynchronized check but that's ok.  The real check is
    // made when the packet below is processed by the dereference thread.
    //

    if (MmTotalCommittedPages >= ((MmTotalCommitLimit/10)*8)) {
        return;
    }

    if ((MmTotalCommitLimit - MmMinimumPageFileReduction) <=
                                                       MmTotalCommittedPages) {
        return;
    }

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        if (MmPagingFile[i]->Size != MmPagingFile[i]->MinimumSize) {
            if (MmPagingFile[i]->FreeSpace > MmMinimumPageFileReduction) {
                break;
            }
        }
    }

    if (i == MmNumberOfPagingFiles) {
        return;
    }

    PageReduce = &MiPageFileContract;

    //
    // See if the page file contraction item is already queued up, if so then
    // nothing more needs to be done.
    //

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    if (PageReduce->RequestedExpansionSize == MI_CONTRACT_PAGEFILES) {

        ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

        return;
    }

    PageReduce->Segment = NULL;
    PageReduce->RequestedExpansionSize = MI_CONTRACT_PAGEFILES;

    InsertTailList (&MmDereferenceSegmentHeader.ListHead,
                    &PageReduce->DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore, 0L, 1L, FALSE);
    return;
}

VOID
MiAttemptPageFileReduction (
    VOID
    )

/*++

Routine Description:

    This routine attempts to reduce the size of the paging files to
    their minimum levels.

    Note - Page file expansion and page file reduction are synchronized
           because a single thread is responsible for performing the
           operation.  Hence, while expansion is occurring, a reduction
           request will be queued to the thread.

Arguments:

    None.

Return Value:

    None.

--*/

{
    SIZE_T CommitLimit;
    SIZE_T CommittedPages;
    SIZE_T SafetyMargin;
    KIRQL OldIrql;
    ULONG i;
    ULONG j;
    PFN_NUMBER StartReduction;
    PFN_NUMBER ReductionSize;
    PFN_NUMBER TryBit;
    PFN_NUMBER TryReduction;
    PMMPAGING_FILE PagingFile;
    SIZE_T MaxReduce;
    FILE_ALLOCATION_INFORMATION FileAllocationInfo;
    NTSTATUS status;

    //
    // Mark the global pagefile contraction item as being processed so other
    // threads will know to reset it if another contraction is desired.
    //

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    ASSERT (MiPageFileContract.RequestedExpansionSize == MI_CONTRACT_PAGEFILES);

    MiPageFileContract.RequestedExpansionSize = ~MI_CONTRACT_PAGEFILES;

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    //
    // Snap the globals into locals so calculations will be consistent from
    // step to step.  It is ok to snap the globals unsynchronized with respect
    // to each other.
    //

    CommittedPages = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    //
    // Make sure the commit limit is significantly greater than the number
    // of committed pages to avoid thrashing.
    //

    SafetyMargin = 2 * MmMinimumPageFileReduction;

    if (CommittedPages + SafetyMargin >= ((CommitLimit/10)*8)) {
        return;
    }

    MaxReduce = ((CommitLimit/10)*8) - (CommittedPages + SafetyMargin);

    ASSERT ((SSIZE_T)MaxReduce > 0);
    ASSERT ((LONG_PTR)MaxReduce >= 0);

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {

        if (MaxReduce < MmMinimumPageFileReduction) {

            //
            // Don't reduce any more paging files.
            //

            break;
        }

        PagingFile = MmPagingFile[i];

        if (PagingFile->Size == PagingFile->MinimumSize) {
            continue;
        }

        //
        // This unsynchronized check is ok because a synchronized check is
        // made later.
        //

        if (PagingFile->FreeSpace < MmMinimumPageFileReduction) {
            continue;
        }

        //
        // Lock the PFN database and check to see if ample pages
        // are free at the end of the paging file.
        //

        TryBit = PagingFile->Size - MmMinimumPageFileReduction;
        TryReduction = MmMinimumPageFileReduction;

        if (TryBit <= PagingFile->MinimumSize) {
            TryBit = PagingFile->MinimumSize;
            TryReduction = PagingFile->Size - PagingFile->MinimumSize;
        }

        StartReduction = 0;
        ReductionSize = 0;

        LOCK_PFN (OldIrql);

        do {

            //
            // Try to reduce.
            //

            if ((ReductionSize + TryReduction) > MaxReduce) {

                //
                // The reduction attempt would remove more
                // than MaxReduce pages.
                //

                break;
            }

            if (RtlAreBitsClear (PagingFile->Bitmap,
                                 (ULONG)TryBit,
                                 (ULONG)TryReduction)) {

                //
                // Can reduce it by TryReduction, see if it can
                // be made smaller.
                //

                StartReduction = TryBit;
                ReductionSize += TryReduction;

                if (StartReduction == PagingFile->MinimumSize) {
                    break;
                }

                TryBit = StartReduction - MmMinimumPageFileReduction;

                if (TryBit <= PagingFile->MinimumSize) {
                    TryReduction -= PagingFile->MinimumSize - TryBit;
                    TryBit = PagingFile->MinimumSize;
                }
                else {
                    TryReduction = MmMinimumPageFileReduction;
                }
            }
            else {

                //
                // Reduction has failed.
                //

                break;
            }

        } while (TRUE);

        //
        // Make sure there are no outstanding writes to
        // pages within the start reduction range.
        //

        if (StartReduction != 0) {

            //
            // There is an outstanding write past where the
            // new end of the paging file should be.  This
            // is a very rare condition, so just punt shrinking
            // the file.
            //

            for (j = 0; j < MM_PAGING_FILE_MDLS; j += 1) {
                if (PagingFile->Entry[j]->LastPageToWrite > StartReduction) {
                    StartReduction = 0;
                    break;
                }
            }
        }

        //
        // If there are no pages to remove, march on to the next pagefile.
        //

        if (StartReduction == 0) {
            UNLOCK_PFN (OldIrql);
            continue;
        }

        //
        // Reduce the paging file's size and free space.
        //

        ASSERT (ReductionSize == (PagingFile->Size - StartReduction));

        PagingFile->Size = StartReduction;
        PagingFile->FreeSpace -= ReductionSize;

        RtlSetBits (PagingFile->Bitmap,
                    (ULONG)StartReduction,
                    (ULONG)ReductionSize );

        //
        // Release the PFN lock now that the size info
        // has been updated.
        //

        UNLOCK_PFN (OldIrql);

        MaxReduce -= ReductionSize;
        ASSERT ((LONG)MaxReduce >= 0);

        //
        // Change the commit limit to reflect the returned page file space.
        // First try to charge the reduction amount to confirm that the
        // reduction is still a sensible thing to do.
        //

        if (MiChargeTemporaryCommitmentForReduction (ReductionSize + SafetyMargin) == FALSE) {

            LOCK_PFN (OldIrql);

            PagingFile->Size = StartReduction + ReductionSize;
            PagingFile->FreeSpace += ReductionSize;

            RtlClearBits (PagingFile->Bitmap,
                          (ULONG)StartReduction,
                          (ULONG)ReductionSize );

            UNLOCK_PFN (OldIrql);

            ASSERT ((LONG)(MaxReduce + ReductionSize) >= 0);

            break;
        }

        //
        // Reduce the systemwide commit limit - note this is carefully done
        // *PRIOR* to returning this commitment so no one else (including a DPC
        // in this very thread) can consume past the limit.
        //

        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, 0 - ReductionSize);

        //
        // Now that the systemwide commit limit has been lowered, the amount
        // we have removed can be safely returned.
        //

        MiReturnCommitment (ReductionSize + SafetyMargin);

#if defined (_WIN64) || defined (_X86PAE_)
        FileAllocationInfo.AllocationSize.QuadPart =
                                       ((ULONG64)StartReduction << PAGE_SHIFT);

#else
        FileAllocationInfo.AllocationSize.LowPart = StartReduction * PAGE_SIZE;

        //
        // Set high part to zero, paging files are limited to 4gb.
        //

        FileAllocationInfo.AllocationSize.HighPart = 0;
#endif

        //
        // Reduce the allocated size of the paging file
        // thereby actually freeing the space and
        // setting a new end of file.
        //

        ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

        status = IoSetInformation (PagingFile->File,
                                   FileAllocationInformation,
                                   sizeof(FILE_ALLOCATION_INFORMATION),
                                   &FileAllocationInfo);
#if DBG
        //
        // Ignore errors on truncating the paging file
        // as we can always have less space in the bitmap
        // than the pagefile holds.
        //

        if (status != STATUS_SUCCESS) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                "MM: pagefile truncate status %lx\n", status);
        }
#endif
    }

    return;
}


VOID
MiLdwPopupWorker (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine is the worker routine to send a lost delayed write data popup
    for a given control area/file.

Arguments:

    Context - Supplies a pointer to the MM_LDW_WORK_CONTEXT for the failed I/O.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    NTSTATUS Status;
    PFILE_OBJECT FileObject;
    PMM_LDW_WORK_CONTEXT LdwContext;
    POBJECT_NAME_INFORMATION FileNameInfo;

    PAGED_CODE();

    LdwContext = (PMM_LDW_WORK_CONTEXT) Context;
    FileObject = LdwContext->FileObject;
    FileNameInfo = NULL;

    if (MiDereferenceLastChanceLdw (LdwContext) == FALSE) {
        ExFreePool (LdwContext);
    }

    //
    // Throw the popup with the user-friendly form, if possible.
    // If everything fails, the user probably couldn't have figured
    // out what failed either.
    //

    Status = IoQueryFileDosDeviceName (FileObject, &FileNameInfo);

    if (Status == STATUS_SUCCESS) {

        IoRaiseInformationalHardError (STATUS_LOST_WRITEBEHIND_DATA,
                                       &FileNameInfo->Name,
                                       NULL);

    }
    else {
        if ((FileObject->FileName.Length) &&
            (FileObject->FileName.MaximumLength) &&
            (FileObject->FileName.Buffer)) {

            IoRaiseInformationalHardError (STATUS_LOST_WRITEBEHIND_DATA,
                                           &FileObject->FileName,
                                           NULL);
        }
    }

    //
    // Now drop the reference to the file object and clean up.
    //

    ObDereferenceObject (FileObject);

    if (FileNameInfo != NULL) {
        ExFreePool (FileNameInfo);
    }
}


VOID
MiWriteComplete (
    IN PVOID Context,
    IN PIO_STATUS_BLOCK IoStatus,
    IN ULONG Reserved
    )

/*++

Routine Description:

    This routine is the APC write completion procedure.  It is invoked
    at APC_LEVEL when a page write operation is completed.

Arguments:

    Context - Supplies a pointer to the MOD_WRITER_MDL_ENTRY which was
              used for this I/O.

    IoStatus - Supplies a pointer to the IO_STATUS_BLOCK which was used
               for this I/O.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL.

--*/

{
    PKEVENT Event;
    LONG ClusterWritesDisabled;
    LONG OldClusterWritesDisabled;
    PMM_LDW_WORK_CONTEXT LdwContext;
    PMMMOD_WRITER_MDL_ENTRY WriterEntry;
    PMMMOD_WRITER_MDL_ENTRY NextWriterEntry;
    PPFN_NUMBER Page;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    LONG ByteCount;
    NTSTATUS status;
    PCONTROL_AREA ControlArea;
    LOGICAL FailAllIo;
    LOGICAL MarkDirty;
    PFILE_OBJECT FileObject;
    PERESOURCE FileResource;

    UNREFERENCED_PARAMETER (Reserved);

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    Event = NULL;
    FailAllIo = FALSE;
    MarkDirty = FALSE;

    //
    // A page write has completed, at this time the pages are not
    // on any lists, write-in-progress is set in the PFN database,
    // and the reference count was incremented.
    //

    WriterEntry = (PMMMOD_WRITER_MDL_ENTRY)Context;
    ByteCount = (LONG) WriterEntry->Mdl.ByteCount;
    Page = &WriterEntry->Page[0];

    if (WriterEntry->Mdl.MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
        MmUnmapLockedPages (WriterEntry->Mdl.MappedSystemVa,
                            &WriterEntry->Mdl);
    }

    status = IoStatus->Status;
    ControlArea = WriterEntry->ControlArea;

    //
    // Do as much work as possible before acquiring the PFN lock.
    //

    while (ByteCount > 0) {

        Pfn1 = MI_PFN_ELEMENT (*Page);
        *Page = (PFN_NUMBER) Pfn1;
        Page += 1;
        ByteCount -= (LONG)PAGE_SIZE;
    }

    ByteCount = (LONG) WriterEntry->Mdl.ByteCount;
    Page = &WriterEntry->Page[0];

    FileResource = WriterEntry->FileResource;

    if (FileResource != NULL) {
        FileObject = WriterEntry->File;
        FsRtlReleaseFileForModWrite (FileObject, FileResource);
    }
    else if (ControlArea == NULL) {
        LARGE_INTEGER CurrentTime;
        KeQuerySystemTime (&CurrentTime);
        MI_PAGEFILE_WRITE (WriterEntry, &CurrentTime, 0, status);
    }

    if (!NT_SUCCESS (status)) {

        //
        // If the file object is over the network, assume that this
        // I/O operation can never complete and mark the pages as
        // clean and indicate in the control area all I/O should fail.
        // Note that the modified bit in the PFN database is not set.
        //
        // If the user changes the protection on the volume containing the
        // file to readonly, this puts us in a problematic situation.  We
        // cannot just keep retrying the writes because if there are no
        // other pages that can be written, not writing these can cause the
        // system to run out of pages, ie: bugcheck 4D.  So throw away
        // these pages just as if they were on a network that has
        // disappeared.
        //

        if (((status != STATUS_FILE_LOCK_CONFLICT) &&
            (ControlArea != NULL) &&
            (ControlArea->u.Flags.Networked == 1))
                        ||
            (status == STATUS_FILE_INVALID)
                        ||
            ((status == STATUS_MEDIA_WRITE_PROTECTED) &&
             (ControlArea != NULL))) {

            if (ControlArea->u.Flags.FailAllIo == 0) {
                ControlArea->u.Flags.FailAllIo = 1;
                FailAllIo = TRUE;

                KdPrint(("MM MODWRITE: failing all io, controlarea %p status %lx\n",
                      ControlArea, status));
            }
        }
        else {

            //
            // The modified write operation failed, SET the modified bit
            // for each page which was written and free the page file
            // space.
            //

#if DBG
            ULONG i;

            //
            // Save error status information to ease debugging.  Since this
            // doesn't need to be exact, don't bother lock-synchronizing.
            //

            for (i = 0; i < MM_MAX_MODWRITE_ERRORS - 1; i += 1) {

                if (MiModwriteErrors[i].Status == 0) {
                    MiModwriteErrors[i].Status = status;
                    MiModwriteErrors[i].Count += 1;
                    break;
                }
                else if (MiModwriteErrors[i].Status == status) {
                    MiModwriteErrors[i].Count += 1;
                    break;
                }
            }

            if (i == MM_MAX_MODWRITE_ERRORS - 1) {
                MiModwriteErrors[i].Count += 1;
            }
#endif

            MarkDirty = TRUE;
        }

        //
        // Save the last error for debugging purposes.
        //

        if (ControlArea == NULL) {
            MiLastModifiedWriteError = status;
        }
        else {
            MiLastMappedWriteError = status;
        }
    }

    //
    // Get the PFN lock so the PFN database can be manipulated.
    //

    LOCK_PFN (OldIrql);

    //
    // Indicate that the write is complete.
    //

    WriterEntry->LastPageToWrite = 0;

    while (ByteCount > 0) {

        Pfn1 = (PMMPFN) *Page;
        ASSERT (Pfn1->u3.e1.WriteInProgress == 1);
#if DBG
#if !defined (_WIN64)
        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            ULONG Offset;

            Offset = GET_PAGING_FILE_OFFSET(Pfn1->OriginalPte);

            if ((Offset < 8192) &&
                (GET_PAGING_FILE_NUMBER(Pfn1->OriginalPte) == 0)) {

                ASSERT ((MmPagingFileDebug[Offset] & 1) != 0);

                if ((!MI_IS_PFN_DELETED (Pfn1)) &&
                    ((MmPagingFileDebug[Offset] & ~0x1f) !=
                                   ((ULONG_PTR)Pfn1->PteAddress << 3)) &&
                    (Pfn1->PteAddress != MiGetPteAddress(PDE_BASE))) {

                    //
                    // Make sure this isn't a PTE that was forked
                    // during the I/O.
                    //

                    if ((Pfn1->PteAddress < (PMMPTE)PTE_TOP) ||
                        ((Pfn1->OriginalPte.u.Soft.Protection &
                                MM_COPY_ON_WRITE_MASK) ==
                                    MM_PROTECTION_WRITE_MASK)) {
                        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                            "MMWRITE: Mismatch Pfn1 %p Offset %lx info %p\n",
                                 Pfn1,
                                 Offset,
                                 MmPagingFileDebug[Offset]);

                        DbgBreakPoint ();
                    }
                    else {
                        MmPagingFileDebug[Offset] &= 0x1f;
                        MmPagingFileDebug[Offset] |=
                            ((ULONG_PTR)Pfn1->PteAddress << 3);
                    }
                }
            }
        }
#endif
#endif //DBG

        Pfn1->u3.e1.WriteInProgress = 0;

        if (MarkDirty == TRUE) {

            //
            // The modified write operation failed, SET the modified bit
            // for each page which was written and free the page file
            // space.
            //

            MI_SET_MODIFIED (Pfn1, 1, 0x9);
        }

        if ((Pfn1->u3.e1.Modified == 1) &&
            (Pfn1->OriginalPte.u.Soft.Prototype == 0)) {

            //
            // This page was modified since the write was done,
            // release the page file space.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);

#if DBG
        *Page = 0xF0FFFFFF;
#endif

        Page += 1;
        ByteCount -= (LONG)PAGE_SIZE;
    }

    //
    // Choose which list to insert this entry into depending on
    // the amount of free space left in the paging file.
    //

    if ((WriterEntry->PagingFile != NULL) &&
        (WriterEntry->PagingFile->FreeSpace < MM_USABLE_PAGES_FREE)) {

        InsertTailList (&MmFreePagingSpaceLow, &WriterEntry->Links);
        WriterEntry->CurrentList = &MmFreePagingSpaceLow;
        MmNumberOfActiveMdlEntries -= 1;

        if (MmNumberOfActiveMdlEntries == 0) {

            //
            // If we leave this entry on the list, there will be
            // no more paging.  Locate all entries which are non
            // zero and pull them from the list.
            //

            WriterEntry = (PMMMOD_WRITER_MDL_ENTRY)MmFreePagingSpaceLow.Flink;

            while ((PLIST_ENTRY)WriterEntry != &MmFreePagingSpaceLow) {

                NextWriterEntry =
                            (PMMMOD_WRITER_MDL_ENTRY)WriterEntry->Links.Flink;

                if (WriterEntry->PagingFile->FreeSpace != 0) {

                    RemoveEntryList (&WriterEntry->Links);

                    //
                    // Insert this into the active list.
                    //

                    if (IsListEmpty (&WriterEntry->PagingListHead->ListHead)) {
                        KeSetEvent (&WriterEntry->PagingListHead->Event,
                                    0,
                                    FALSE);
                    }

                    InsertTailList (&WriterEntry->PagingListHead->ListHead,
                                    &WriterEntry->Links);
                    WriterEntry->CurrentList = &MmPagingFileHeader.ListHead;
                    MmNumberOfActiveMdlEntries += 1;
                }

                WriterEntry = NextWriterEntry;
            }

        }
    }
    else {

#if DBG
        if (WriterEntry->PagingFile == NULL) {
            MmNumberOfMappedMdlsInUse -= 1;
        }
#endif
        //
        // Ample space exists, put this on the active list.
        //

        if (IsListEmpty (&WriterEntry->PagingListHead->ListHead)) {
            Event = &WriterEntry->PagingListHead->Event;
        }

        InsertTailList (&WriterEntry->PagingListHead->ListHead,
                        &WriterEntry->Links);
    }

    ASSERT (((ULONG_PTR)WriterEntry->Links.Flink & 1) == 0);

    if (Event != NULL) {
        KeSetEvent (Event, 0, FALSE);
    }

    if (ControlArea != NULL) {

        if (FailAllIo == TRUE) {
    
            //
            // Reference our fileobject and queue the popup.  The DOS
            // name translation must occur at PASSIVE_LEVEL - we're at APC.
            //
    
            UNLOCK_PFN (OldIrql);

            LdwContext = ExAllocatePoolWithTag (NonPagedPool,
                                                sizeof(MM_LDW_WORK_CONTEXT),
                                                'pdmM');
    
            if (LdwContext != NULL) {
                LdwContext->FileObject = ControlArea->FilePointer;
                ObReferenceObject (LdwContext->FileObject);
    
                ExInitializeWorkItem (&LdwContext->WorkItem,
                                      MiLdwPopupWorker,
                                      (PVOID)LdwContext);
    
                ExQueueWorkItem (&LdwContext->WorkItem, DelayedWorkQueue);
            }

            LOCK_PFN (OldIrql);
        }
    
        //
        // A write to a mapped file just completed, check to see if
        // there are any waiters on the completion of this i/o.
        //

        ControlArea->ModifiedWriteCount -= 1;
        ASSERT ((SHORT)ControlArea->ModifiedWriteCount >= 0);

        if (ControlArea->u.Flags.SetMappedFileIoComplete != 0) {
            KePulseEvent (&MmMappedFileIoComplete, 0, FALSE);
        }

        if (MiDrainingMappedWrites == TRUE) {
            if (MmModifiedPageListHead.Flink != MM_EMPTY_LIST) {

                //
                // Note the TimerPending flag and the event must be set while
                // holding the PFN lock - otherwise the other thread (modified
                // or mapped writer) can clear TimerPending before we set
                // the event here.
                //

                MiTimerPending = TRUE;
                KeSetEvent (&MiMappedPagesTooOldEvent, 0, FALSE);
            }
            else {
                MiDrainingMappedWrites = FALSE;
            }
        }

        ControlArea->NumberOfPfnReferences -= 1;

        if (ControlArea->NumberOfPfnReferences == 0) {

            //
            // This routine returns with the PFN lock released!
            //

            MiCheckControlArea (ControlArea, OldIrql);
        }
        else {
            UNLOCK_PFN (OldIrql);
        }
    }
    else {
        UNLOCK_PFN (OldIrql);
    }

    if (!NT_SUCCESS(status)) {

        //
        // Wait for a short time so other processing can continue.
        //

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&Mm30Milliseconds);

        if (MmIsRetryIoStatus (status)) {

            //
            // Low resource scenarios are a chicken and egg problem.  The
            // mapped and modified writers must make forward progress to
            // alleviate low memory situations.  If these threads are
            // unable to write data due to resource problems in the driver
            // stack then temporarily fall back to single page I/Os as
            // the stack guarantees forward progress with those.  This
            // causes the low memory situation to persist slightly longer
            // but ensures that it won't become terminal either.
            //

            do {

                OldClusterWritesDisabled = ReadForWriteAccess (&MiClusterWritesDisabled);

                ClusterWritesDisabled = InterlockedCompareExchange (
                                                    &MiClusterWritesDisabled,
                                                    MI_SLOW_CLUSTER_WRITES,
                                                    OldClusterWritesDisabled);

            } while (OldClusterWritesDisabled != ClusterWritesDisabled);
        }
    }
    else {

        do {

            OldClusterWritesDisabled = MiClusterWritesDisabled;

            if (OldClusterWritesDisabled == 0) {
                break;
            }

            ClusterWritesDisabled = InterlockedCompareExchange (
                                                &MiClusterWritesDisabled,
                                                OldClusterWritesDisabled - 1,
                                                OldClusterWritesDisabled);

        } while (OldClusterWritesDisabled != ClusterWritesDisabled);
    }

    return;
}

LOGICAL
MiCancelWriteOfMappedPfn (
    IN PFN_NUMBER PageToStop,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine attempts to stop a pending mapped page writer write for the
    specified PFN.  Note that if the write can be stopped, any other pages
    that may be clustered with the write are also stopped.

Arguments:

    PageToStop - Supplies the frame number that the caller wants to stop.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    TRUE if the write was stopped, FALSE if not.

Environment:

    Kernel mode, PFN lock held.  The PFN lock is released and reacquired if
    the write was stopped.

    N.B.  No other locks may be held as IRQL is lowered to APC_LEVEL here.

--*/

{
    ULONG i;
    ULONG PageCount;
    PPFN_NUMBER Page;
    PLIST_ENTRY NextEntry;
    PMDL MemoryDescriptorList;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    //
    // Walk the MmMappedPageWriterList looking for an MDL which contains
    // the argument page.  If found, remove it and cancel the write.
    //

    NextEntry = MmMappedPageWriterList.Flink;
    while (NextEntry != &MmMappedPageWriterList) {

        ModWriterEntry = CONTAINING_RECORD(NextEntry,
                                           MMMOD_WRITER_MDL_ENTRY,
                                           Links);

        MemoryDescriptorList = &ModWriterEntry->Mdl;
        PageCount = (MemoryDescriptorList->ByteCount >> PAGE_SHIFT);
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        for (i = 0; i < PageCount; i += 1) {
            if (*Page == PageToStop) {
                RemoveEntryList (NextEntry);
                goto CancelWrite;
            }
            Page += 1;
        }

        NextEntry = NextEntry->Flink;
    }

    return FALSE;

CancelWrite:

    UNLOCK_PFN (OldIrql);

    //
    // File lock conflict to indicate an error has occurred,
    // but that future I/Os should be allowed.  Keep APCs disabled and
    // call the write completion routine.
    //

    ModWriterEntry->u.IoStatus.Status = STATUS_FILE_LOCK_CONFLICT;
    ModWriterEntry->u.IoStatus.Information = 0;

    KeRaiseIrql (APC_LEVEL, &OldIrql);
    MiWriteComplete ((PVOID)ModWriterEntry,
                     &ModWriterEntry->u.IoStatus,
                     0);
    KeLowerIrql (OldIrql);

    LOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MiModifiedPageWriter (
    IN PVOID StartContext
    )

/*++

Routine Description:

    Implements the NT modified page writer thread.  When the modified
    page threshold is reached, or memory becomes overcommitted the
    modified page writer event is set, and this thread becomes active.

Arguments:

    StartContext - not used.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    HANDLE ThreadHandle;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES ObjectAttributes;
    PMMMOD_WRITER_MDL_ENTRY ModWriteEntry;

    PAGED_CODE();

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Initialize listheads as empty.
    //

    MmSystemShutdown = 0;
    KeInitializeEvent (&MmPagingFileHeader.Event, NotificationEvent, FALSE);
    KeInitializeEvent (&MmMappedFileHeader.Event, NotificationEvent, FALSE);

    InitializeListHead(&MmPagingFileHeader.ListHead);
    InitializeListHead(&MmMappedFileHeader.ListHead);
    InitializeListHead(&MmFreePagingSpaceLow);

    //
    // Allocate enough MDLs such that 2% of system memory can be pending
    // at any point in time in mapped writes.  Even smaller memory systems
    // get 20 MDLs as the minimum.
    //

    MmNumberOfMappedMdls = MmNumberOfPhysicalPages / (32 * 1024);

    if (MmNumberOfMappedMdls < 20) {
        MmNumberOfMappedMdls = 20;
    }

    for (i = 0; i < MmNumberOfMappedMdls; i += 1) {
        ModWriteEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MMMOD_WRITER_MDL_ENTRY) +
                                                MmModifiedWriteClusterSize *
                                                    sizeof(PFN_NUMBER),
                                                'eWmM');

        if (ModWriteEntry == NULL) {
            break;
        }

        ModWriteEntry->PagingFile = NULL;
        ModWriteEntry->PagingListHead = &MmMappedFileHeader;

        InsertTailList (&MmMappedFileHeader.ListHead, &ModWriteEntry->Links);
    }

    MmNumberOfMappedMdls = i;

    //
    // Make this a real time thread.
    //

    KeSetPriorityThread (KeGetCurrentThread (), LOW_REALTIME_PRIORITY + 1);

    //
    // Start a secondary thread for writing mapped file pages.  This
    // is required as the writing of mapped file pages could cause
    // page faults resulting in requests for free pages.  But there
    // could be no free pages - hence a dead lock.  Rather than deadlock
    // the whole system waiting on the modified page writer, creating
    // a secondary thread allows that thread to block without affecting
    // on going page file writes.
    //

    KeInitializeEvent (&MmMappedPageWriterEvent, NotificationEvent, FALSE);
    InitializeListHead (&MmMappedPageWriterList);
    InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

    Status = PsCreateSystemThread (&ThreadHandle,
                                   THREAD_ALL_ACCESS,
                                   &ObjectAttributes,
                                   0L,
                                   NULL,
                                   MiMappedPageWriter,
                                   NULL);

    if (!NT_SUCCESS(Status)) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x41288,
                      Status,
                      0,
                      0);
    }

    ZwClose (ThreadHandle);

    MiModifiedPageWriterWorker ();

    //
    // Shutdown in progress, wait forever.
    //

    {
        LARGE_INTEGER Forever;

        //
        // System has shutdown, go into LONG wait.
        //

        Forever.LowPart = 0;
        Forever.HighPart = 0xF000000;
        KeDelayExecutionThread (KernelMode, FALSE, &Forever);
    }

    return;
}


VOID
MiModifiedPageWriterTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )

/*++

Routine Description:

    This routine is executed whenever modified mapped pages are waiting to
    be written.  Its job is to signal the Modified Page Writer to write
    these out.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    DeferredContext - Optional deferred context;  not used.

    SystemArgument1 - Optional argument 1;  not used.

    SystemArgument2 - Optional argument 2;  not used.

Return Value:

    None.

--*/

{
    UNREFERENCED_PARAMETER (Dpc);
    UNREFERENCED_PARAMETER (DeferredContext);
    UNREFERENCED_PARAMETER (SystemArgument1);
    UNREFERENCED_PARAMETER (SystemArgument2);

    LOCK_PFN_AT_DPC ();

    MiTimerPending = TRUE;
    KeSetEvent (&MiMappedPagesTooOldEvent, 0, FALSE);

    UNLOCK_PFN_FROM_DPC ();
}


VOID
MiModifiedPageWriterWorker (
    VOID
    )

/*++

Routine Description:

    Implements the NT modified page writer thread.  When the modified
    page threshold is reached, or memory becomes overcommitted the
    modified page writer event is set, and this thread becomes active.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PRTL_BITMAP Bitmap;
    KIRQL OldIrql;
    static KWAIT_BLOCK WaitBlockArray[ModifiedWriterMaximumObject];
    PVOID WaitObjects[ModifiedWriterMaximumObject];
    NTSTATUS WakeupStatus;
    LOGICAL MappedPage;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    PsGetCurrentThread()->MemoryMaker = 1;

    //
    // Wait for the modified page writer event or the mapped pages event.
    //

    WaitObjects[NormalCase] = (PVOID)&MmModifiedPageWriterEvent;
    WaitObjects[MappedPagesNeedWriting] = (PVOID)&MiMappedPagesTooOldEvent;

    for (;;) {

        WakeupStatus = KeWaitForMultipleObjects (ModifiedWriterMaximumObject,
                                                 &WaitObjects[0],
                                                 WaitAny,
                                                 WrFreePage,
                                                 KernelMode,
                                                 FALSE,
                                                 NULL,
                                                 &WaitBlockArray[0]);

        LOCK_PFN (OldIrql);

        for (;;) {

            //
            // Modified page writer was signaled.
            //

            if (MmModifiedPageListHead.Total == 0) {

                //
                // No more pages, clear the event(s) and wait again...
                // Note we can clear both events regardless of why we woke up
                // since no modified pages of any type exist.
                //

                MiTimerPending = FALSE;
                KeClearEvent (&MiMappedPagesTooOldEvent);

                UNLOCK_PFN (OldIrql);

                KeClearEvent (&MmModifiedPageWriterEvent);

                break;
            }

            //
            // If we didn't wake up explicitly to deal with mapped pages,
            // then determine which type of pages are the most popular:
            // page file backed pages, or mapped file backed pages.
            //

            if ((WakeupStatus == MappedPagesNeedWriting) ||
                (MmTotalPagesForPagingFile <
                MmModifiedPageListHead.Total - MmTotalPagesForPagingFile)) {

                //
                // More pages are destined for mapped files.
                //

                if (MmModifiedPageListHead.Flink != MM_EMPTY_LIST) {

                    if (WakeupStatus == MappedPagesNeedWriting) {

                        //
                        // Our mapped pages DPC went off, only deal with
                        // those pages.  Write all the mapped pages (ONLY),
                        // then clear the flag and come back to the top.
                        //

                        MiDrainingMappedWrites = TRUE;
                    }

                    MappedPage = TRUE;

                    if (IsListEmpty (&MmMappedFileHeader.ListHead)) {

                        //
                        // Make sure page is destined for paging file as there
                        // are no MDLs for mapped writes free.
                        //

                        if (WakeupStatus != MappedPagesNeedWriting) {

                            //
                            // No MDLs are available for writing mapped pages,
                            // try for a page destined for a pagefile.
                            //

                            if (MmTotalPagesForPagingFile != 0) {
                                MappedPage = FALSE;
                            }
                        }
                    }
                }
                else {

                    //
                    // No more modified mapped pages (there may still be
                    // modified pagefile-destined pages), so clear only the
                    // mapped pages event and check for directions at the top
                    // again.
                    //

                    if (WakeupStatus == MappedPagesNeedWriting) {

                        MiTimerPending = FALSE;
                        KeClearEvent (&MiMappedPagesTooOldEvent);
                        UNLOCK_PFN (OldIrql);

                        break;
                    }
                    MappedPage = FALSE;
                }
            }
            else {

                //
                // More pages are destined for the paging file.
                //

                MappedPage = FALSE;

                if (((IsListEmpty(&MmPagingFileHeader.ListHead)) ||
                    (MiFirstPageFileCreatedAndReady == FALSE))) {

                    //
                    // Try for a dirty section-backed page as no paging
                    // file MDLs are available.
                    //

                    if (MmModifiedPageListHead.Flink != MM_EMPTY_LIST) {
                        MappedPage = TRUE;
                    }
                    else {
                        ASSERT (MmTotalPagesForPagingFile == MmModifiedPageListHead.Total);
                        if ((MiFirstPageFileCreatedAndReady == FALSE) &&
                            (MmNumberOfPagingFiles != 0)) {

                            //
                            // The first paging has been created but the
                            // reservation checking for crashdumps has not
                            // finished yet.  Delay a bit as this will finish
                            // shortly and then restart.
                            //

                            UNLOCK_PFN (OldIrql);
                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                            LOCK_PFN (OldIrql);
                            continue;
                        }
                    }
                }
            }

            if (MappedPage == TRUE) {

                if (IsListEmpty(&MmMappedFileHeader.ListHead)) {

                    if (WakeupStatus == MappedPagesNeedWriting) {

                        //
                        // Since we woke up only to take care of mapped pages,
                        // don't wait for an MDL below because drivers may take
                        // an inordinate amount of time processing the
                        // outstanding ones.  We might have to wait too long,
                        // resulting in the system running out of pages.
                        //

                        if (MiTimerPending == TRUE) {

                            //
                            // This should be normal case - the reason we must
                            // first check timer pending above is for the rare
                            // case - when this thread first ran for normal
                            // modified page processing and took
                            // care of all the pages including the mapped ones.
                            // Then this thread woke up again for the mapped
                            // reason and here we are.
                            //

                            MiTimerPending = FALSE;
                            KeClearEvent (&MiMappedPagesTooOldEvent);
                        }

                        MiTimerPending = TRUE;

                        KeSetTimerEx (&MiModifiedPageWriterTimer,
                                      MiModifiedPageLife,
                                      0,
                                      &MiModifiedPageWriterTimerDpc);

                        UNLOCK_PFN (OldIrql);
                        break;
                    }

                    //
                    // Reset the event indicating no mapped files in
                    // the list, drop the PFN lock and wait for an
                    // I/O operation to complete with a one second
                    // timeout.
                    //

                    KeClearEvent (&MmMappedFileHeader.Event);

                    UNLOCK_PFN (OldIrql);

                    KeWaitForSingleObject (&MmMappedFileHeader.Event,
                                           WrPageOut,
                                           KernelMode,
                                           FALSE,
                                           (PLARGE_INTEGER)&Mm30Milliseconds);

                    //
                    // Don't go on as the old PageFrameIndex at the
                    // top of the ModifiedList may have changed states.
                    //

                    LOCK_PFN (OldIrql);
                }
                else {

                    //
                    // This routine returns with the PFN lock released !
                    //

                    MiGatherMappedPages (OldIrql);

                    //
                    // Nothing magic here, just give other processors a turn at
                    // the PFN lock (and allow DPCs a chance to execute).
                    //

                    LOCK_PFN (OldIrql);
                }
            }
            else {

                if (IsListEmpty(&MmPagingFileHeader.ListHead)) {

                    //
                    // Reset the event indicating no paging files MDLs in
                    // the list, drop the PFN lock and wait for an
                    // I/O operation to complete.
                    //

                    KeClearEvent (&MmPagingFileHeader.Event);
                    UNLOCK_PFN (OldIrql);
                    KeWaitForSingleObject (&MmPagingFileHeader.Event,
                                           WrPageOut,
                                           KernelMode,
                                           FALSE,
                                           (PLARGE_INTEGER)&Mm30Milliseconds);

                    //
                    // Don't go on as the old PageFrameIndex at the
                    // top of the ModifiedList may have changed states.
                    //
                }
                else {

                    ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                            &MmPagingFileHeader.ListHead);

#if DBG
                    ModWriterEntry->Links.Flink = MM_IO_IN_PROGRESS;
#endif

                    //
                    // Increment the reference count under PFN lock protection.
                    //

                    ASSERT (ModWriterEntry->PagingFile->ReferenceCount == 0);
                    ModWriterEntry->PagingFile->ReferenceCount += 1;

                    Bitmap = ModWriterEntry->PagingFile->Bitmap;

                    UNLOCK_PFN (OldIrql);

                    MiGatherPagefilePages (ModWriterEntry, Bitmap);
                }

                LOCK_PFN (OldIrql);
            }

            if (MmSystemShutdown) {

                //
                // Shutdown has returned.  Stop the modified page writer.
                //

                UNLOCK_PFN (OldIrql);
                return;
            }

            //
            // If this is a mapped page timer, then keep on writing till there's
            // nothing left.
            //

            if (WakeupStatus == MappedPagesNeedWriting) {
                continue;
            }

            //
            // If this is a request to write all modified pages, then keep on
            // writing.
            //

            if (MmWriteAllModifiedPages) {
                continue;
            }

            if (MmModifiedPageListHead.Total < MmModifiedWriteClusterSize) {

                //
                // There are ample pages, clear the event and wait again...
                //

                UNLOCK_PFN (OldIrql);

                KeClearEvent (&MmModifiedPageWriterEvent);
                break;
            }

        } // end for

    } // end for
}

VOID
MiGatherMappedPages (
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine processes the specified modified page by examining
    the prototype PTE for that page and the adjacent prototype PTEs
    building a cluster of modified pages destined for a mapped file.
    Once the cluster is built, it is sent to the mapped writer thread
    to be processed.

Arguments:

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    The number of pages in the attempted write.  The PFN lock is released
    prior to returning.

Environment:

    PFN lock held.

--*/

{
    PMMPFN Pfn2;
    PFN_NUMBER PagesWritten;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PPFN_NUMBER Page;
    PMMPTE LastPte;
    PMMPTE BasePte;
    PMMPTE NextPte;
    PMMPTE PointerPte;
    PMMPTE StartingPte;
    MMPTE PteContents;
    PVOID HyperMapped;
    PEPROCESS Process;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PKPRCB Prcb;

    PageFrameIndex = MmModifiedPageListHead.Flink;
    ASSERT (PageFrameIndex != MM_EMPTY_LIST);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // This page is destined for a mapped file, check to see if
    // there are any physically adjacent pages are also in the
    // modified page list and write them out at the same time.
    //

    Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
    ControlArea = Subsection->ControlArea;

    if (ControlArea->u.Flags.NoModifiedWriting) {

        //
        // This page should not be written out, add it to the
        // tail of the modified NO WRITE list and get the next page.
        //

        MiUnlinkPageFromList (Pfn1);
        MiInsertPageInList (&MmModifiedNoWritePageListHead,
                            PageFrameIndex);
        UNLOCK_PFN (OldIrql);
        return;
    }

    if (ControlArea->u.Flags.Image) {

        //
        // This is an image section, writes are not
        // allowed to an image section.
        //

        //
        // Change page contents to look like it's a demand zero
        // page and put it back into the modified list.
        //

        //
        // Decrement the count for PfnReferences to the
        // segment as paging file pages are not counted as
        // "image" references.
        //

        ControlArea->NumberOfPfnReferences -= 1;
        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
        MiUnlinkPageFromList (Pfn1);

        Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        Pfn1->OriginalPte.u.Soft.Prototype = 0;
        Pfn1->OriginalPte.u.Soft.Transition = 0;

        //
        // Insert the page at the tail of the list and get
        // color update performed.
        //

        MiInsertPageInList (&MmModifiedPageListHead, PageFrameIndex);
        UNLOCK_PFN (OldIrql);
        return;
    }

    //
    // Look at backwards at previous prototype PTEs to see if
    // this can be clustered into a larger write operation.
    //

    PointerPte = Pfn1->PteAddress;
    NextPte = PointerPte - (MmModifiedWriteClusterSize - 1);

    //
    // Make sure NextPte is in the same page.
    //

    if (NextPte < (PMMPTE)PAGE_ALIGN (PointerPte)) {
        NextPte = (PMMPTE)PAGE_ALIGN (PointerPte);
    }

    //
    // Make sure NextPte is within the subsection.
    //

    if (NextPte < Subsection->SubsectionBase) {
        NextPte = Subsection->SubsectionBase;
    }

    //
    // If the prototype PTEs are not currently mapped,
    // map them via hyperspace.  BasePte refers to the
    // prototype PTEs for nonfaulting references.
    //

    if (MiIsProtoAddressValid (PointerPte)) {
        Process = NULL;
        HyperMapped = NULL;
        BasePte = PointerPte;
    }
    else {
        Process = PsGetCurrentProcess ();
        HyperMapped = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
        BasePte = (PMMPTE)((PCHAR)HyperMapped + BYTE_OFFSET (PointerPte));
    }

    ASSERT (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (BasePte) == PageFrameIndex);

    PointerPte -= 1;
    BasePte -= 1;

    if (MiClusterWritesDisabled != 0) {
        NextPte = PointerPte + 1;
    }

    //
    // Don't go before the start of the subsection nor cross
    // a page boundary.
    //

    while (PointerPte >= NextPte) {

        PteContents = *BasePte;

        //
        // If the page is not in transition, exit loop.
        //

        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Transition == 0) ||
            (PteContents.u.Soft.Prototype == 1)) {

            break;
        }

        Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        //
        // Make sure page is modified and on the modified list.
        //

        if ((Pfn2->u3.e1.Modified == 0 ) ||
            (Pfn2->u3.e2.ReferenceCount != 0)) {
            break;
        }
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        PointerPte -= 1;
        BasePte -= 1;
    }

    StartingPte = PointerPte + 1;
    BasePte = BasePte + 1;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (StartingPte == Pfn1->PteAddress);
    MiUnlinkPageFromList (Pfn1);

    //
    // Get an entry from the list and fill it in.
    //

    ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                    &MmMappedFileHeader.ListHead);

#if DBG
    MmNumberOfMappedMdlsInUse += 1;
    if (MmNumberOfMappedMdlsInUse > MmNumberOfMappedMdlsInUsePeak) {
        MmNumberOfMappedMdlsInUsePeak = MmNumberOfMappedMdlsInUse;
    }
#endif

    ModWriterEntry->File = ControlArea->FilePointer;
    ModWriterEntry->ControlArea = ControlArea;

    //
    // Calculate the offset to read into the file.
    //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
    //

    ModWriterEntry->WriteOffset.QuadPart = MiStartingOffset (Subsection,
                                                             Pfn1->PteAddress);

    MmInitializeMdl(&ModWriterEntry->Mdl, NULL, PAGE_SIZE);

    ModWriterEntry->Mdl.MdlFlags |= MDL_PAGES_LOCKED;

    ModWriterEntry->Mdl.Size = (CSHORT)(sizeof(MDL) +
                      (sizeof(PFN_NUMBER) * MmModifiedWriteClusterSize));

    Page = &ModWriterEntry->Page[0];

    //
    // Up the reference count for the physical page as there
    // is I/O in progress.
    //

    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

    //
    // Clear the modified bit for the page and set the write
    // in progress bit.
    //

    MI_SET_MODIFIED (Pfn1, 0, 0x23);

    Pfn1->u3.e1.WriteInProgress = 1;

    //
    // Put this physical page into the MDL.
    //

    *Page = PageFrameIndex;

    //
    // See if any adjacent pages are also modified and in
    // the transition state and if so, write them out at
    // the same time.
    //

    LastPte = StartingPte + MmModifiedWriteClusterSize;

    //
    // Look at the last PTE, ensuring a page boundary is not crossed.
    //
    // If LastPte is not in the same page as the StartingPte,
    // set LastPte so the cluster will not cross.
    //

    if (StartingPte < (PMMPTE)PAGE_ALIGN(LastPte)) {
        LastPte = (PMMPTE)PAGE_ALIGN(LastPte);
    }

    //
    // Make sure LastPte is within the subsection.
    //

    if (LastPte > &Subsection->SubsectionBase[Subsection->PtesInSubsection]) {
        LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
    }

    //
    // Look forwards.
    //

    NextPte = BasePte + 1;
    PointerPte = StartingPte + 1;

    if (MiClusterWritesDisabled != 0) {
        LastPte = PointerPte;
    }

    //
    // Loop until an MDL is filled, the end of a subsection
    // is reached, or a page boundary is reached.
    // Note, PointerPte points to the PTE. NextPte points
    // to where it is mapped in hyperspace (if required).
    //

    while (PointerPte < LastPte) {

        PteContents = *NextPte;

        //
        // If the page is not in transition, exit loop.
        //

        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Transition == 0) ||
            (PteContents.u.Soft.Prototype == 1)) {

            break;
        }

        Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        if ((Pfn2->u3.e1.Modified == 0 ) ||
            (Pfn2->u3.e2.ReferenceCount != 0)) {

            //
            // Page is not dirty or not on the modified list,
            // end clustering operation.
            //

            break;
        }
        Page += 1;

        //
        // Add physical page to MDL.
        //

        *Page = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        ASSERT (PointerPte == Pfn2->PteAddress);
        MiUnlinkPageFromList (Pfn2);

        //
        // Up the reference count for the physical page as there
        // is I/O in progress.
        //

        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn2);

        //
        // Clear the modified bit for the page and set the
        // write in progress bit.
        //

        MI_SET_MODIFIED (Pfn2, 0, 0x24);

        Pfn2->u3.e1.WriteInProgress = 1;

        ModWriterEntry->Mdl.ByteCount += PAGE_SIZE;

        NextPte += 1;
        PointerPte += 1;
    }

    if (HyperMapped != NULL) {
        MiUnmapPageInHyperSpaceFromDpc (Process, HyperMapped);
    }

    ASSERT (BYTES_TO_PAGES (ModWriterEntry->Mdl.ByteCount) <= MmModifiedWriteClusterSize);

    ModWriterEntry->u.LastByte.QuadPart = ModWriterEntry->WriteOffset.QuadPart +
                        ModWriterEntry->Mdl.ByteCount;

    ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

#if DBG
    if ((ULONG)ModWriterEntry->Mdl.ByteCount >
                                ((1+MmModifiedWriteClusterSize)*PAGE_SIZE)) {
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "Mdl %p, MDL End Offset %lx %lx Subsection %p\n",
                    ModWriterEntry->Mdl,
                    ModWriterEntry->u.LastByte.LowPart,
                    ModWriterEntry->u.LastByte.HighPart,
                    Subsection);
        DbgBreakPoint ();
    }
#endif

    PagesWritten = (ModWriterEntry->Mdl.ByteCount >> PAGE_SHIFT);

    Prcb = KeGetCurrentPrcb ();
    Prcb->MmMappedWriteIoCount += 1;
    Prcb->MmMappedPagesWriteCount += (ULONG)PagesWritten;

    //
    // Increment the count of modified page writes outstanding
    // in the control area.
    //

    ControlArea->ModifiedWriteCount += 1;

    //
    // Increment the number of PFN references.  This allows the file
    // system to purge (i.e. call MmPurgeSection) modified writes.
    //

    ControlArea->NumberOfPfnReferences += 1;

    ModWriterEntry->FileResource = NULL;

    if (ControlArea->u.Flags.BeingPurged == 1) {
        UNLOCK_PFN (OldIrql);
        ModWriterEntry->u.IoStatus.Status = STATUS_FILE_LOCK_CONFLICT;
        ModWriterEntry->u.IoStatus.Information = 0;
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        MiWriteComplete ((PVOID)ModWriterEntry,
                         &ModWriterEntry->u.IoStatus,
                         0);
        KeLowerIrql (OldIrql);
        return;
    }

    //
    // Send the entry to the MappedPageWriter.
    //

    InsertTailList (&MmMappedPageWriterList, &ModWriterEntry->Links);

    KeSetEvent (&MmMappedPageWriterEvent, 0, FALSE);

    UNLOCK_PFN (OldIrql);

    return;
}

VOID
MiGatherPagefilePages (
    IN PMMMOD_WRITER_MDL_ENTRY ModWriterEntry,
    IN PRTL_BITMAP Bitmap
    )

/*++

Routine Description:

    This routine processes the specified modified page by getting
    that page and gather any other pages on the modified list destined
    for the paging file in a large write cluster.  This cluster is
    then written to the paging file.

Arguments:

    ModWriterEntry - Supplies the modified writer entry to use for the write.

    Bitmap - Supplies the captured bitmap to scan for free pagefile blocks.
             This address was captured under the PFN lock by the caller - this
             routine must free the bitmap pool if it detects that a new bitmap
             (ie: the pagefile has been extended) while the lock free scan was
             underway.

Return Value:

    None.

Environment:

    No locks held.

--*/

{
    PFILE_OBJECT File;
    IO_PAGING_PRIORITY IrpPriority;
    PMMPAGING_FILE CurrentPagingFile;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    ULONG StartBit;
    LARGE_INTEGER StartingOffset;
    PFN_NUMBER ClusterSize;
    PFN_NUMBER ThisCluster;
    MMPTE LongPte;
    KIRQL OldIrql;
    ULONG NextColor;
    LOGICAL PageFileFull;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PKPRCB Prcb;

    //
    // Page is destined for the paging file.
    //

    OldIrql = PASSIVE_LEVEL;
    CurrentPagingFile = ModWriterEntry->PagingFile;

    File = CurrentPagingFile->File;

    if (MiClusterWritesDisabled == 0) {
        ThisCluster = MmModifiedWriteClusterSize;
    }
    else {
        ThisCluster = 1;
    }

    PageFileFull = FALSE;

    MmInitializeMdl (&ModWriterEntry->Mdl, NULL, PAGE_SIZE);

    ModWriterEntry->Mdl.MdlFlags |= MDL_PAGES_LOCKED;

    ModWriterEntry->Mdl.Size = (CSHORT)(sizeof(MDL) +
                    sizeof(PFN_NUMBER) * MmModifiedWriteClusterSize);

    Page = &ModWriterEntry->Page[0];

    ClusterSize = 0;

    //
    // As scan durations may be long, first scan for a possible hit without
    // holding the PFN lock.
    //

    do {

        //
        // Attempt to cluster MmModifiedWriteClusterSize pages
        // together.  Reduce by one page until we succeed or
        // can't find a single page free in the paging file.
        //

        StartBit = RtlFindClearBits (Bitmap,
                                     (ULONG)ThisCluster,
                                     0);

        if (StartBit != NO_BITS_FOUND) {

            LOCK_PFN (OldIrql);

            //
            // Note that the current bitmap may be different from the one
            // that was passed in.
            //

            StartBit = RtlFindClearBitsAndSet (CurrentPagingFile->Bitmap,
                                               (ULONG)ThisCluster,
                                               StartBit);

            if (StartBit != NO_BITS_FOUND) {

                //
                // The bits have been set and the PFN lock is still held.
                //

                CurrentPagingFile->ReferenceCount -= 1;
                ASSERT (CurrentPagingFile->ReferenceCount == 0);

                if (Bitmap == CurrentPagingFile->Bitmap) {

                    //
                    // The paging file has not been extended so don't free the
                    // existing bitmap.
                    //

                    Bitmap = NULL;
                }

                break;
            }

            UNLOCK_PFN (OldIrql);
        }
        else {
            ThisCluster -= 1;
            PageFileFull = TRUE;
        }

    } while (ThisCluster != 0);

    if (StartBit == NO_BITS_FOUND) {

        if (MiClusterWritesDisabled == 0) {
            ThisCluster = MmModifiedWriteClusterSize;
        }
        else {
            ThisCluster = 1;
        }

        LOCK_PFN (OldIrql);

        CurrentPagingFile->ReferenceCount -= 1;
        ASSERT (CurrentPagingFile->ReferenceCount == 0);

        do {

            //
            // Attempt to cluster MmModifiedWriteClusterSize pages
            // together.  Since we hold the PFN lock, reduce by one
            // half (instead of one page) until we succeed or
            // can't find a single page free in the paging file.
            //

            StartBit = RtlFindClearBitsAndSet (CurrentPagingFile->Bitmap,
                                               (ULONG)ThisCluster,
                                               0);

            if (StartBit != NO_BITS_FOUND) {
                break;
            }

            ThisCluster = ThisCluster >> 1;
            PageFileFull = TRUE;

        } while (ThisCluster != 0);

        if (StartBit == NO_BITS_FOUND) {

            //
            // Paging file must be full.
            //

            KdPrint(("MM MODWRITE: page file full\n"));
            ASSERT(CurrentPagingFile->FreeSpace == 0);

            //
            // Move this entry to the not enough space list,
            // and try again.
            //

            InsertTailList (&MmFreePagingSpaceLow, &ModWriterEntry->Links);
            ModWriterEntry->CurrentList = &MmFreePagingSpaceLow;
            MmNumberOfActiveMdlEntries -= 1;

            if (Bitmap == CurrentPagingFile->Bitmap) {

                //
                // The paging file has not been extended so don't free the
                // existing bitmap.
                //

                Bitmap = NULL;
            }

            UNLOCK_PFN (OldIrql);

            if (Bitmap != NULL) {
                MiRemoveBitMap (&Bitmap);
            }

            MiPageFileFull ();
            return;
        }
    }

    CurrentPagingFile->FreeSpace -= ThisCluster;
    CurrentPagingFile->CurrentUsage += ThisCluster;

    if (CurrentPagingFile->FreeSpace < 32) {
        PageFileFull = TRUE;
    }

    StartingOffset.QuadPart = (UINT64)StartBit << PAGE_SHIFT;

    if (MmTotalPagesForPagingFile == 0) {

        //
        // No modified pages left - other threads may have put them back into
        // working sets, flushed them or deleted them.
        //

        ASSERT (ThisCluster != 0);
        ClusterSize = 0;
        goto bail;
    }

#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
    NextColor will need to be selected round-robin.
#else
    NextColor = 0;
#endif

    MI_GET_MODIFIED_PAGE_ANY_COLOR (PageFrameIndex, NextColor);

    ASSERT (PageFrameIndex != MM_EMPTY_LIST);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Search through the modified page list looking for other
    // pages destined for the paging file and build a cluster.
    //

    while (ClusterSize != ThisCluster) {

        //
        // Is this page destined for a paging file?
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            *Page = PageFrameIndex;

            //
            // Remove the page from the modified list. Note that
            // write-in-progress marks the state.
            //
            // Unlink the page so the same page won't be found
            // on the modified page list by color.
            //

            MiUnlinkPageFromList (Pfn1);
            NextColor = MI_GET_NEXT_COLOR(NextColor);

            MI_GET_MODIFIED_PAGE_BY_COLOR (PageFrameIndex,
                                           NextColor);

            //
            // Up the reference count for the physical page as there
            // is I/O in progress.
            //

            MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

            //
            // Clear the modified bit for the page and set the
            // write in progress bit.
            //

            MI_SET_MODIFIED (Pfn1, 0, 0x25);

            Pfn1->u3.e1.WriteInProgress = 1;
            ASSERT (Pfn1->OriginalPte.u.Soft.PageFileHigh == 0);

            MI_SET_PAGING_FILE_INFO (LongPte,
                                     Pfn1->OriginalPte,
                                     CurrentPagingFile->PageFileNumber,
                                     StartBit);

#if DBG
            if ((StartBit < 8192) &&
                (CurrentPagingFile->PageFileNumber == 0)) {
                ASSERT ((MmPagingFileDebug[StartBit] & 1) == 0);
                MmPagingFileDebug[StartBit] =
                    (((ULONG_PTR)Pfn1->PteAddress << 3) |
                        ((ClusterSize & 0xf) << 1) | 1);
            }
#endif

            //
            // Change the original PTE contents to refer to
            // the paging file offset where this was written.
            //

            Pfn1->OriginalPte = LongPte;

            ClusterSize += 1;
            Page += 1;
            StartBit += 1;
        }
        else {

            //
            // This page was not destined for a paging file,
            // get another page.
            //
            // Get a page of the same color as the one which
            // was not usable.
            //

            MI_GET_MODIFIED_PAGE_BY_COLOR (PageFrameIndex,
                                           NextColor);
        }

        if (PageFrameIndex == MM_EMPTY_LIST) {
            break;
        }

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    } //end while

    if (ClusterSize != ThisCluster) {

bail:

        //
        // A complete cluster could not be located, free the
        // excess page file space that was reserved and adjust
        // the size of the packet.
        //

        RtlClearBits (CurrentPagingFile->Bitmap,
                      StartBit,
                      (ULONG)(ThisCluster - ClusterSize));

        CurrentPagingFile->FreeSpace += ThisCluster - ClusterSize;
        CurrentPagingFile->CurrentUsage -= ThisCluster - ClusterSize;

        //
        // If there are no pages to write, don't issue a write
        // request and restart the scan loop.
        //

        if (ClusterSize == 0) {

            //
            // No pages to write.  Insert the entry back in the list.
            //

            if (IsListEmpty (&ModWriterEntry->PagingListHead->ListHead)) {
                KeSetEvent (&ModWriterEntry->PagingListHead->Event,
                            0,
                            FALSE);
            }

            InsertTailList (&ModWriterEntry->PagingListHead->ListHead,
                            &ModWriterEntry->Links);

            if (Bitmap == CurrentPagingFile->Bitmap) {

                //
                // The paging file has not been extended so don't free the
                // existing bitmap.
                //

                Bitmap = NULL;
            }

            UNLOCK_PFN (OldIrql);

            if (Bitmap != NULL) {
                MiRemoveBitMap (&Bitmap);
            }

            if (PageFileFull == TRUE) {
                MiPageFileFull ();
            }
            return;
        }
    }

    if (CurrentPagingFile->PeakUsage <
                                CurrentPagingFile->CurrentUsage) {
        CurrentPagingFile->PeakUsage =
                                CurrentPagingFile->CurrentUsage;
    }

    ModWriterEntry->LastPageToWrite = StartBit - 1;

    //
    // Release the PFN lock and wait for the write to complete.
    //

    UNLOCK_PFN (OldIrql);

    Prcb = KeGetCurrentPrcb ();
    InterlockedIncrement (&Prcb->MmDirtyWriteIoCount);

    InterlockedExchangeAdd (&Prcb->MmDirtyPagesWriteCount,
                            (LONG) ClusterSize);

    ModWriterEntry->Mdl.ByteCount = (ULONG)(ClusterSize * PAGE_SIZE);

    KeQuerySystemTime (&ModWriterEntry->IssueTime);

    IrpPriority = IoPagingPriorityNormal;

    if (MiModifiedWriteBurstCount != 0) {
        if (MmAvailablePages < MM_PLENTY_FREE_LIMIT) {
            MiModifiedWriteBurstCount -= 1;
            IrpPriority = IoPagingPriorityHigh;
        }
        else {
            MiModifiedWriteBurstCount = 0;
        }
    }
    else if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiModifiedWriteBurstCount = MI_MAXIMUM_PRIORITY_BURST;
        IrpPriority = IoPagingPriorityHigh;
    }
    else if (MmAvailablePages < MM_TIGHT_LIMIT) {
        MiModifiedWriteBurstCount = MI_MAXIMUM_PRIORITY_BURST / 4;
        IrpPriority = IoPagingPriorityHigh;
    }

    MI_PAGEFILE_WRITE (ModWriterEntry,
                       &ModWriterEntry->IssueTime,
                       IrpPriority,
                       (NTSTATUS)-1);

    //
    // Issue the write request.
    //

    Status = IoAsynchronousPageWrite (File,
                                      &ModWriterEntry->Mdl,
                                      &StartingOffset,
                                      MiWriteComplete,
                                      (PVOID)ModWriterEntry,
                                      IrpPriority,
                                      &ModWriterEntry->u.IoStatus,
                                      &ModWriterEntry->Irp);

    if (NT_ERROR (Status)) {
        KdPrint(("MM MODWRITE: modified page write failed %lx\n", Status));

        //
        // An error has occurred, disable APCs and
        // call the write completion routine.
        //

        ModWriterEntry->u.IoStatus.Status = Status;
        ModWriterEntry->u.IoStatus.Information = 0;
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        MiWriteComplete ((PVOID)ModWriterEntry,
                         &ModWriterEntry->u.IoStatus,
                         0);
        KeLowerIrql (OldIrql);
    }

    if ((Bitmap != NULL) && (Bitmap != CurrentPagingFile->Bitmap)) {

        //
        // The paging file has been extended so free the entry bitmap.
        //

        MiRemoveBitMap (&Bitmap);
    }

    if (PageFileFull == TRUE) {
        MiPageFileFull ();
    }

    return;
}

VOID
MiMappedPageWriter (
    IN PVOID StartContext
    )

/*++

Routine Description:

    Implements the NT secondary modified page writer thread.
    Requests for writes to mapped files are sent to this thread.
    This is required as the writing of mapped file pages could cause
    page faults resulting in requests for free pages.  But there
    could be no free pages - hence a deadlock.  Rather than deadlock
    the whole system waiting on the modified page writer, creating
    a secondary thread allows that thread to block without affecting
    ongoing page file writes.

Arguments:

    StartContext - not used.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    NTSTATUS Status;
    KEVENT TempEvent;
    PETHREAD CurrentThread;
    IO_PAGING_PRIORITY IrpPriority;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Make this a real time thread.
    //

    CurrentThread = PsGetCurrentThread ();

    KeSetPriorityThread (&CurrentThread->Tcb, LOW_REALTIME_PRIORITY + 1);

    CurrentThread->MemoryMaker = 1;

    //
    // Let the file system know that we are getting resources.
    //

    FsRtlSetTopLevelIrpForModWriter ();

    KeInitializeEvent (&TempEvent, NotificationEvent, FALSE);

    while (TRUE) {

        KeWaitForSingleObject (&MmMappedPageWriterEvent,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               NULL);

        LOCK_PFN (OldIrql);

        if (IsListEmpty (&MmMappedPageWriterList)) {
            KeClearEvent (&MmMappedPageWriterEvent);
            UNLOCK_PFN (OldIrql);
        }
        else {

            ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                                &MmMappedPageWriterList);

            UNLOCK_PFN (OldIrql);

            if (ModWriterEntry->ControlArea->u.Flags.FailAllIo == 1) {
                Status = STATUS_UNSUCCESSFUL;
            }
            else {
                Status = FsRtlAcquireFileForModWriteEx (ModWriterEntry->File,
                                                        &ModWriterEntry->u.LastByte,
                                                        &ModWriterEntry->FileResource);
                if (NT_SUCCESS(Status)) {

                    //
                    // Issue the write request.
                    //

                    IrpPriority = IoPagingPriorityNormal;

                    if (MiMappedWriteBurstCount != 0) {
                        if (MmAvailablePages < MM_PLENTY_FREE_LIMIT) {
                            MiMappedWriteBurstCount -= 1;
                            IrpPriority = IoPagingPriorityHigh;
                        }
                        else {
                            MiMappedWriteBurstCount = 0;
                        }
                    }
                    else if (MmAvailablePages < MM_HIGH_LIMIT) {
                        MiMappedWriteBurstCount = MI_MAXIMUM_PRIORITY_BURST;
                        IrpPriority = IoPagingPriorityHigh;
                    }
                    else if (MmAvailablePages < MM_TIGHT_LIMIT) {
                        MiMappedWriteBurstCount = MI_MAXIMUM_PRIORITY_BURST / 4;
                        IrpPriority = IoPagingPriorityHigh;
                    }

                    Status = IoAsynchronousPageWrite (ModWriterEntry->File,
                                                      &ModWriterEntry->Mdl,
                                                      &ModWriterEntry->WriteOffset,
                                                      MiWriteComplete,
                                                      (PVOID) ModWriterEntry,
                                                      IrpPriority,
                                                      &ModWriterEntry->u.IoStatus,
                                                      &ModWriterEntry->Irp);
                }
                else {

                    //
                    // Unable to get the file system resources, set error status
                    // to lock conflict (ignored by MiWriteComplete) so the APC
                    // routine is explicitly called.
                    //

                    Status = STATUS_FILE_LOCK_CONFLICT;
                }
            }

            if (NT_ERROR (Status)) {

                //
                // An error has occurred, disable APCs and
                // call the write completion routine.
                //

                ModWriterEntry->u.IoStatus.Status = Status;
                ModWriterEntry->u.IoStatus.Information = 0;
                KeRaiseIrql (APC_LEVEL, &OldIrql);
                MiWriteComplete ((PVOID)ModWriterEntry,
                                 &ModWriterEntry->u.IoStatus,
                                 0);
                KeLowerIrql (OldIrql);
            }
        }
    }
}


BOOLEAN
MmDisableModifiedWriteOfSection (
    __in PSECTION_OBJECT_POINTERS SectionObjectPointer
    )

/*++

Routine Description:

    This function disables page writing by the modified page writer for
    the section which is mapped by the specified file object pointer.

    This should only be used for files which CANNOT be mapped by user
    programs, e.g., volume files, directory files, etc.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects


Return Value:

    Returns TRUE if the operation was a success, FALSE if either
    the there is no section or the section already has a view.

--*/

{
    KIRQL OldIrql;
    BOOLEAN state;
    PCONTROL_AREA ControlArea;

    state = TRUE;

    LOCK_PFN (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    if (ControlArea != NULL) {
        if (ControlArea->NumberOfMappedViews == 0) {

            //
            // There are no views to this section, indicate no modified
            // page writing is allowed.
            //

            ControlArea->u.Flags.NoModifiedWriting = 1;
        }
        else {

            //
            // Return the current modified page writing state.
            //

            state = (BOOLEAN)ControlArea->u.Flags.NoModifiedWriting;
        }
    }
    else {

        //
        // This file no longer has an associated segment.
        //

        state = 0;
    }

    UNLOCK_PFN (OldIrql);
    return state;
}


BOOLEAN
MmEnableModifiedWriteOfSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer
    )

/*++

Routine Description:

    This function enables page writing by the modified page writer for
    the section which is mapped by the specified file object pointer.

    This should only be used for files which have previously been disabled.
    Normal sections are created allowing modified write.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects


Return Value:

    Returns TRUE if the operation was a success, FALSE if not.

--*/

{
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PMMPFN Pfn1;
    PSUBSECTION Subsection;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NextPageFrameIndex;

    LOCK_PFN2 (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    if ((ControlArea != NULL) && (ControlArea->u.Flags.NoModifiedWriting)) {

        //
        // Indicate modified page writing is now allowed.
        //

        ControlArea->u.Flags.NoModifiedWriting = 0;

        //
        // Move any straggling pages on the modnowrite list back onto the
        // modified list otherwise they would be orphaned and this could
        // cause us to run out of pages.
        //

        if (MmModifiedNoWritePageListHead.Total != 0) {

            PageFrameIndex = MmModifiedNoWritePageListHead.Flink;
    
            do {

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                NextPageFrameIndex = Pfn1->u1.Flink;

                Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);

                if (ControlArea == Subsection->ControlArea) {
    
                    //
                    // This page must be moved to the modified list.
                    //
    
                    MiUnlinkPageFromList (Pfn1);

                    MiInsertPageInList (&MmModifiedPageListHead,
                                        PageFrameIndex);
                }

                PageFrameIndex = NextPageFrameIndex;

            } while (PageFrameIndex != MM_EMPTY_LIST);
        }
    }

    UNLOCK_PFN2 (OldIrql);

    return TRUE;
}


#define ROUND_UP(VALUE,ROUND) ((ULONG)(((ULONG)VALUE + \
                               ((ULONG)ROUND - 1L)) & (~((ULONG)ROUND - 1L))))
NTSTATUS
MmGetPageFileInformation (
    OUT PVOID SystemInformation,
    IN ULONG SystemInformationLength,
    OUT PULONG Length
    )

/*++

Routine Description:

    This routine returns information about the currently active paging
    files.

Arguments:

    SystemInformation - Returns the paging file information.

    SystemInformationLength - Supplies the length of the SystemInformation
                              buffer.

    Length - Returns the length of the paging file information placed in the
             buffer.

Return Value:

    Returns the status of the operation.

--*/

{
    PSYSTEM_PAGEFILE_INFORMATION PageFileInfo;
    ULONG NextEntryOffset = 0;
    ULONG TotalSize = 0;
    ULONG i;
    UNICODE_STRING UserBufferPageFileName;

    PAGED_CODE();

    *Length = 0;
    PageFileInfo = (PSYSTEM_PAGEFILE_INFORMATION)SystemInformation;

    PageFileInfo->TotalSize = 0;

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        PageFileInfo = (PSYSTEM_PAGEFILE_INFORMATION)(
                                (PUCHAR)PageFileInfo + NextEntryOffset);
        NextEntryOffset = sizeof(SYSTEM_PAGEFILE_INFORMATION);
        TotalSize += sizeof(SYSTEM_PAGEFILE_INFORMATION);

        if (TotalSize > SystemInformationLength) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }

        PageFileInfo->TotalSize = (ULONG)MmPagingFile[i]->Size;
        PageFileInfo->TotalInUse = (ULONG)MmPagingFile[i]->CurrentUsage;
        PageFileInfo->PeakUsage = (ULONG)MmPagingFile[i]->PeakUsage;

        //
        // The PageFileName portion of the UserBuffer must be saved locally
        // to protect against a malicious thread changing the contents.  This
        // is because we will reference the contents ourselves when the actual
        // string is copied out carefully below.
        //

        UserBufferPageFileName.Length = MmPagingFile[i]->PageFileName.Length;
        UserBufferPageFileName.MaximumLength = (USHORT)(MmPagingFile[i]->PageFileName.Length + sizeof(WCHAR));
        UserBufferPageFileName.Buffer = (PWCHAR)(PageFileInfo + 1);

        PageFileInfo->PageFileName = UserBufferPageFileName;

        TotalSize += ROUND_UP (UserBufferPageFileName.MaximumLength,
                               sizeof(ULONG));
        NextEntryOffset += ROUND_UP (UserBufferPageFileName.MaximumLength,
                                     sizeof(ULONG));

        if (TotalSize > SystemInformationLength) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }

        //
        // Carefully reference the user buffer here.
        //

        RtlCopyMemory(UserBufferPageFileName.Buffer,
                      MmPagingFile[i]->PageFileName.Buffer,
                      MmPagingFile[i]->PageFileName.Length);
        UserBufferPageFileName.Buffer[
                    MmPagingFile[i]->PageFileName.Length/sizeof(WCHAR)] = UNICODE_NULL;
        PageFileInfo->NextEntryOffset = NextEntryOffset;
    }
    PageFileInfo->NextEntryOffset = 0;
    *Length = TotalSize;
    return STATUS_SUCCESS;
}


NTSTATUS
MiCheckPageFileMapping (
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    Non-pageable routine to check to see if a given file has
    no sections and therefore is eligible to become a paging file.

Arguments:

    File - Supplies a pointer to the file object.

Return Value:

    Returns STATUS_SUCCESS if the file can be used as a paging file.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    if (File->SectionObjectPointer == NULL) {
        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    if ((File->SectionObjectPointer->DataSectionObject != NULL) ||
        (File->SectionObjectPointer->ImageSectionObject != NULL)) {

        UNLOCK_PFN (OldIrql);
        return STATUS_INCOMPATIBLE_FILE_MAP;
    }
    UNLOCK_PFN (OldIrql);
    return STATUS_SUCCESS;

}


VOID
MiInsertPageFileInList (
    VOID
    )

/*++

Routine Description:

    Non-pageable routine to add a page file into the list
    of system wide page files.

Arguments:

    None, implicitly found through page file structures.

Return Value:

    None.  Operation cannot fail.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    SIZE_T FreeSpace;
    SIZE_T MaximumSize;

    LOCK_PFN (OldIrql);

    MmNumberOfPagingFiles += 1;

    if (IsListEmpty (&MmPagingFileHeader.ListHead)) {
        KeSetEvent (&MmPagingFileHeader.Event, 0, FALSE);
    }

    for (i = 0; i < MM_PAGING_FILE_MDLS; i += 1) {

        InsertTailList (&MmPagingFileHeader.ListHead,
                     &MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[i]->Links);

        MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[i]->CurrentList =
                                                &MmPagingFileHeader.ListHead;
    }

    FreeSpace = MmPagingFile[MmNumberOfPagingFiles - 1]->FreeSpace;
    MaximumSize = MmPagingFile[MmNumberOfPagingFiles - 1]->MaximumSize;

    MmPagingFile[MmNumberOfPagingFiles - 1]->ReferenceCount = 0;

    MmNumberOfActiveMdlEntries += 2;

    UNLOCK_PFN (OldIrql);

    //
    // Increase the systemwide commit limit maximum first.  Then increase
    // the current limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, MaximumSize);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, FreeSpace);

    return;
}

VOID
MiPageFileFull (
    VOID
    )

/*++

Routine Description:

    This routine is called when no space can be found in a paging file.
    It looks through all the paging files to see if ample space is
    available and if not, tries to expand the paging files.

    If more than 90% of all the paging files are in use, the commitment limit
    is set to the total and then 100 pages are added.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG i;
    PFN_NUMBER Total;
    PFN_NUMBER Free;
    SIZE_T QuotaCharge;

    if (MmNumberOfPagingFiles == 0) {
        return;
    }

    Total = 0;
    Free = 0;

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        Total += MmPagingFile[i]->Size;
        Free += MmPagingFile[i]->FreeSpace;
    }

    //
    // Check to see if more than 90% of the total space has been used.
    //

    if (((Total >> 5) + (Total >> 4)) >= Free) {

        //
        // Try to expand the paging files.
        //
        // Check (unsynchronized is ok) commit limits of each pagefile.
        // If all the pagefiles are already at their maximums, then don't
        // make things worse by setting commit to the maximum - this gives
        // systems with lots of memory a longer lease on life when they have
        // small pagefiles.
        //

        i = 0;

        do {
            if (MmPagingFile[i]->MaximumSize > MmPagingFile[i]->Size) {
                break;
            }
            i += 1;
        } while (i < MmNumberOfPagingFiles);

        if (i == MmNumberOfPagingFiles) {

            //
            // No pagefiles can be extended,
            // display a popup if we haven't before.
            //

            MiCauseOverCommitPopup ();

            return;
        }

        QuotaCharge = MmTotalCommitLimit - MmTotalCommittedPages;

        //
        // IFF the total number of committed pages is less than the limit,
        // or in any event, no more than 50 pages past the limit,
        // then charge pages against the commitment to trigger pagefile
        // expansion.
        //
        // If the total commit is more than 50 past the limit, then don't
        // bother trying to extend the pagefile.
        //

        if ((SSIZE_T)QuotaCharge + 50 > 0) {

            if ((SSIZE_T)QuotaCharge < 50) {
                QuotaCharge = 50;
            }

            MiChargeCommitmentCantExpand (QuotaCharge, TRUE);

            MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGEFILE_FULL, QuotaCharge);

            //
            // Display a popup if we haven't before.
            //

            MiCauseOverCommitPopup ();

            MiReturnCommitment (QuotaCharge);

            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PAGEFILE_FULL, QuotaCharge);
        }
    }
    return;
}

VOID
MiFlushAllPages (
    VOID
    )

/*++

Routine Description:

    Forces a write of all modified pages.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  APC_LEVEL or less.

--*/

{
    ULONG j;

    //
    // If there are no paging files, then no sense in waiting for
    // modified writes to complete.
    //

    if (MmNumberOfPagingFiles == 0) {
        return;
    }

    InterlockedIncrement (&MmWriteAllModifiedPages);
    KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);

    j = 0xff;

    do {
        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
        j -= 1;
    } while ((MmModifiedPageListHead.Total > 50) && (j > 0));

    InterlockedDecrement (&MmWriteAllModifiedPages);
    return;
}


LOGICAL
MiIssuePageExtendRequest (
    IN PMMPAGE_FILE_EXPANSION PageExtend
    )

/*++

Routine Description:

    Queue a message to the segment dereferencing / pagefile extending
    thread to see if the page file can be extended.  Extension is done
    in the context of a system thread due to mutexes which the current
    thread may be holding.

Arguments:

    PageExtend - Supplies a pointer to the page file extension request.

Return Value:

    TRUE indicates the request completed.  FALSE indicates the request timed
    out and was removed.

Environment:

    Kernel mode.  No locks held.  APC_LEVEL or below.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    NTSTATUS status;
    PLIST_ENTRY NextEntry;
    PETHREAD Thread;

    Thread = PsGetCurrentThread ();

    //
    // The segment dereference thread cannot wait for itself !
    //

    if (Thread->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread) {
        return FALSE;
    }

    //
    // Avoid repeatedly waking the segment dereference thread just for it to
    // perform a no-op by first checking whether the pagefile(s) are already
    // at their maximum (or don't exist) - if so, just return a failure.
    //

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        if (MmPagingFile[i]->Size < MmPagingFile[i]->MaximumSize) {
            break;
        }
    }

    if (i == MmNumberOfPagingFiles) {
        return FALSE;
    }

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    InsertHeadList (&MmDereferenceSegmentHeader.ListHead,
                    &PageExtend->DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                        0L,
                        1L,
                        TRUE);

    //
    // Wait for the thread to extend the paging file.
    //

    status = KeWaitForSingleObject (&PageExtend->Event,
                                    Executive,
                                    KernelMode,
                                    FALSE,
                                    (PageExtend->RequestedExpansionSize < 10) ?
                                        (PLARGE_INTEGER)&MmOneSecond : (PLARGE_INTEGER)&MmTwentySeconds);

    if (status == STATUS_TIMEOUT) {

        //
        // The wait has timed out, if this request has not
        // been processed, remove it from the list and check
        // to see if we should allow this request to succeed.
        // This prevents a deadlock between the file system
        // trying to allocate memory in the FSP and the
        // segment dereferencing thread trying to close a
        // file object, and waiting in the file system.
        //

        KdPrint(("MiIssuePageExtendRequest: wait timed out, page-extend= %p, quota = %lx\n",
                   PageExtend, PageExtend->RequestedExpansionSize));

        ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

        NextEntry = MmDereferenceSegmentHeader.ListHead.Flink;

        while (NextEntry != &MmDereferenceSegmentHeader.ListHead) {

            //
            // Check to see if this is the entry we are waiting for.
            //

            if (NextEntry == &PageExtend->DereferenceList) {

                //
                // Remove this entry.
                //

                RemoveEntryList (&PageExtend->DereferenceList);
                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);
                return FALSE;
            }
            NextEntry = NextEntry->Flink;
        }

        ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

        //
        // Entry is being processed, wait for completion.
        //

        KdPrint (("MiIssuePageExtendRequest: rewaiting...\n"));

        KeWaitForSingleObject (&PageExtend->Event,
                               Executive,
                               KernelMode,
                               FALSE,
                               NULL);
    }

    return TRUE;
}


VOID
MiIssuePageExtendRequestNoWait (
    IN PFN_NUMBER SizeInPages
    )

/*++

Routine Description:

    Queue a message to the segment dereferencing / pagefile extending
    thread to see if the page file can be extended.  Extension is done
    in the context of a system thread due to mutexes which the current
    thread may be holding.

Arguments:

    SizeInPages - Supplies the size in pages to increase the page file(s) by.
                  This is rounded up to a 1MB multiple by this routine.

Return Value:

    TRUE indicates the request completed.  FALSE indicates the request timed
    out and was removed.

Environment:

    Kernel mode.  No locks held.  DISPATCH_LEVEL or less.

    Note this routine must be very careful to not use any paged
    pool as the only reason it is being called is because pool is depleted.

--*/

{
    KIRQL OldIrql;
    LONG OriginalInProgress;

    OriginalInProgress = InterlockedCompareExchange (
                            &MmAttemptForCantExtend.InProgress, 1, 0);

    if (OriginalInProgress != 0) {

        //
        // An expansion request is already in progress, assume
        // it will help enough (another can always be issued later) and
        // that it will succeed.
        //

        return;
    }

    ASSERT (MmAttemptForCantExtend.InProgress == 1);

    SizeInPages = (SizeInPages + ONEMB_IN_PAGES - 1) & ~(ONEMB_IN_PAGES - 1);

    MmAttemptForCantExtend.ActualExpansion = 0;
    MmAttemptForCantExtend.RequestedExpansionSize = SizeInPages;

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    InsertHeadList (&MmDereferenceSegmentHeader.ListHead,
                    &MmAttemptForCantExtend.DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                        0L,
                        1L,
                        FALSE);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\pagfault.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.

Module Name:

   pagfault.c

Abstract:

    This module contains the pager for memory management.

--*/

#include "mi.h"

#define STATUS_PTE_CHANGED      0x87303000
#define STATUS_REFAULT          0xC7303001

ULONG MmInPageSupportMinimum = 4;

ULONG MiInPageSinglePages;

extern PMMPTE MmSharedUserDataPte;

extern PVOID MmSpecialPoolStart;
extern PVOID MmSpecialPoolEnd;

ULONG MiFaultRetries;
ULONG MiUserFaultRetries;

ULONG MmClusterPageFileReads;

#define MI_PROTOTYPE_WSINDEX    ((ULONG)-1)

NTSTATUS
MiResolvePageFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    OUT PMMPTE CapturedPteContents,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    );

NTSTATUS
MiResolveProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN OUT PMMPFN *LockedProtoPfn,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    OUT PMMPTE CapturedPteContents,
    IN PEPROCESS Process,
    IN KIRQL OldIrql,
    IN PVOID TrapInformation
    );

VOID
MiHandleBankedSection (
    IN PVOID VirtualAddress,
    IN PMMVAD Vad
    );

NTSTATUS
MiResolveMappedFileFault (
    IN PMMPTE PointerPte,
    IN PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    );

NTSTATUS
MiResolveTransitionFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN KIRQL OldIrql,
    OUT PMMINPAGE_SUPPORT *InPageBlock
    );

NTSTATUS
MiCompleteProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN KIRQL OldIrql,
    IN OUT PMMPFN *LockedProtoPfn
    );

ULONG MmMaxTransitionCluster = 8;


NTSTATUS
MiDispatchFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN LOGICAL RecheckAccess,
    IN PEPROCESS Process,
    IN PVOID TrapInformation,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This routine dispatches a page fault to the appropriate
    routine to complete the fault.

Arguments:

    FaultStatus - Supplies fault status information bits.

    VirtualAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in,
                      NULL if no prototype PTE exists.

    RecheckAccess - Supplies TRUE if the prototype PTE needs to be checked for
                    access permissions - this is only an issue for forked
                    prototype PTEs that are no-access.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.
              If this parameter is HYDRA_PROCESS, then the fault is for session
              space and the process's working set lock is not held - rather
              the session space's working set lock is held.

    Vad - Supplies the VAD used for sections.  May optionally be NULL even
          for section-based faults, so use purely as an opportunistic hint.

    TrapInformation - Supplies the trap information.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, working set mutex held.

--*/

{
#if DBG
    KIRQL EntryIrql;
    MMWSLE ProtoProtect2;
#endif

#define VARIOUS_FLAGS_DIRTY_PTE                 0x01
#define VARIOUS_FLAGS_PFN_HELD                  0x02
#define VARIOUS_FLAGS_ACCESS_CHECK_NEEDED       0x04
#define VARIOUS_FLAGS_LOG_HARD_FAULT            0x08
#define VARIOUS_FLAGS_ENTERED_CRITICAL_REGION   0x10

    //
    // Miscellaneous unrelated flags kept here in one ULONG to save stack space.
    //

    ULONG VariousFlags;
    MMPTE OriginalPte;
    MMPTE TempPte;
    MMPTE RealPteContents;
    MMPTE NewPteContents;
    MM_PROTECTION_MASK Protection;
    ULONG FreeBit;
    MMWSLE ProtoProtect;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;
    PSUBSECTION Subsection;
    ULONG Flags;
    PVOID UsedPageTableHandle;
    ULONG_PTR i;
    ULONG_PTR NumberOfProtos;
    ULONG_PTR MaxProtos;
    ULONG_PTR ProtosProcessed;
    NTSTATUS status;
    PMMINPAGE_SUPPORT ReadBlock;
    PMMINPAGE_SUPPORT CapturedEvent;
    KIRQL OldIrql;
    PPFN_NUMBER Page;
    PFN_NUMBER PageFrameIndex;
    LONG NumberOfBytes;
    PMMPTE CheckPte;
    PMMPTE ReadPte;
    PMMPFN PfnClusterPage;
    PMMPFN Pfn1;
    PMMSUPPORT SessionWs;
    PETHREAD WsThread;
    PERFINFO_HARDPAGEFAULT_INFORMATION HardFaultEvent;
    LARGE_INTEGER IoStartTime;
    ULONG_PTR StoreInstruction;
    PMMPFN LockedProtoPfn;
    WSLE_NUMBER WorkingSetIndex;
    PMMPFN Pfn2;
    PMMPTE ContainingPageTablePointer;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    PCONTROL_AREA ControlArea;
    PMMCLONE_DESCRIPTOR CloneDescriptor;

#if DBG
    EntryIrql = KeGetCurrentIrql ();
    ASSERT (EntryIrql <= APC_LEVEL);
    ASSERT (KeAreAllApcsDisabled () == TRUE);
    SATISFY_OVERZEALOUS_COMPILER (OriginalPte.u.Long = (ULONG_PTR)-1);
#endif

    VariousFlags = 0;
    LockedProtoPfn = NULL;
    SessionWs = NULL;
    StoreInstruction = MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus);
    WsThread = PsGetCurrentThread ();

    //
    // Initializing ReadBlock & ReadPte is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    OldIrql = MM_NOIRQL;
    ReadPte = NULL;
    ReadBlock = NULL;
    ProtoProtect.u1.Long = 0;

    if (PointerProtoPte != NULL) {

        ASSERT (!MI_IS_PHYSICAL_ADDRESS(PointerProtoPte));

        CheckPte = MiGetPteAddress (PointerProtoPte);

        if (VirtualAddress < MmSystemRangeStart) {

            NumberOfProtos = 1;
            SATISFY_OVERZEALOUS_COMPILER (UsedPageTableHandle = NULL);
            SATISFY_OVERZEALOUS_COMPILER (Pfn2 = NULL);
            SATISFY_OVERZEALOUS_COMPILER (Protection = 0);
            SATISFY_OVERZEALOUS_COMPILER (RealPteContents.u.Long = 0);

            if ((PointerPte->u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) &&
                (PointerPte->u.Proto.ReadOnly == 0)) {

                //
                // Kernel mode access must be verified, go the long way below.
                //

                VariousFlags |= VARIOUS_FLAGS_ACCESS_CHECK_NEEDED;
            }
            else {

                //
                // Opportunistically cluster the transition faults as needed.
                // When the Vad is non-NULL, the proper access checks have
                // already been applied across the range (as long as the
                // PTEs are zero).
                //
    
                if ((Vad != NULL) &&
                    (Vad->u.VadFlags.VadType != VadImageMap) &&
                    (Vad->u2.VadFlags2.ExtendableFile == 0) &&
                    (MmAvailablePages > MM_ENORMOUS_LIMIT) &&
                    ((Process->Vm.Flags.MaximumWorkingSetHard == 0) ||
                     (Process->Vm.WorkingSetSize + MmMaxTransitionCluster <= Process->Vm.MaximumWorkingSetSize)) &&
                    (RecheckAccess == FALSE)) {
    
                    NumberOfProtos = MmMaxTransitionCluster;
    
                    //
                    // Ensure the cluster doesn't cross the VAD contiguous PTE
                    // limits.
                    //

                    MaxProtos = Vad->LastContiguousPte - PointerProtoPte + 1;
    
                    if (NumberOfProtos > MaxProtos) {
                        NumberOfProtos = MaxProtos;
                    }
    
                    //
                    // Ensure the cluster doesn't cross the page containing
                    // the real page table page as we only locked down the
                    // single page.
                    //

                    MaxProtos = (PAGE_SIZE - BYTE_OFFSET (PointerPte)) / sizeof (MMPTE);
                    if (NumberOfProtos > MaxProtos) {
                        NumberOfProtos = MaxProtos;
                    }
    
                    //
                    // Ensure the cluster doesn't cross the page containing
                    // the prototype PTEs as we only locked down the single
                    // page.
                    //

                    MaxProtos = (PAGE_SIZE - BYTE_OFFSET (PointerProtoPte)) / sizeof (MMPTE);
                    if (NumberOfProtos > MaxProtos) {
                        NumberOfProtos = MaxProtos;
                    }
    
                    //
                    // Ensure the cluster doesn't cross the VAD limits.
                    //

                    MaxProtos = Vad->EndingVpn - MI_VA_TO_VPN (VirtualAddress) + 1;
    
                    if (NumberOfProtos > MaxProtos) {
                        NumberOfProtos = MaxProtos;
                    }
    
                    //
                    // Ensure enough WSLEs are available so we cannot fail to
                    // insert the cluster later.
                    //

                    MaxProtos = 1;
                    WorkingSetIndex = MmWorkingSetList->FirstFree;

                    if ((NumberOfProtos > 1) &&
                        (WorkingSetIndex != WSLE_NULL_INDEX)) {

                        do {
                            if (MmWsle[WorkingSetIndex].u1.Long == (WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT)) {
                                break;
                            }
                            MaxProtos += 1;
                            WorkingSetIndex = (WSLE_NUMBER) (MmWsle[WorkingSetIndex].u1.Long >> MM_FREE_WSLE_SHIFT);
                        } while (MaxProtos < NumberOfProtos);
                    }

                    if (NumberOfProtos > MaxProtos) {
                        NumberOfProtos = MaxProtos;
                    }

                    //
                    // We have computed the maximum cluster size.  Fill the PTEs
                    // and increment the use counts on the page table page for
                    // each PTE we fill (regardless of whether the prototype
                    // cluster pages are already in transition).
                    //

                    ASSERT (VirtualAddress <= MM_HIGHEST_USER_ADDRESS);

                    for (i = 1; i < NumberOfProtos; i += 1) {
                        if ((PointerPte + i)->u.Long != MM_ZERO_PTE) {
                            break;
                        }
                        MI_WRITE_INVALID_PTE (PointerPte + i, *PointerPte);
                    }

                    NumberOfProtos = i;

                    if (NumberOfProtos > 1) {
                        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (VirtualAddress);
                        MI_INCREMENT_USED_PTES_BY_HANDLE_CLUSTER (UsedPageTableHandle, NumberOfProtos - 1);

                        //
                        // The protection code for the real PTE comes from
                        // the real PTE as it was placed there earlier during
                        // the handling of this fault.
                        //

                        ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (PointerPte);
                        //
                        // Build up a valid PTE so that when the PFN lock is
                        // held, the only additional update to it is for the
                        // actual PFN.
                        //
    
                        MI_MAKE_VALID_USER_PTE (RealPteContents,
                                                0,
                                                ProtoProtect.u1.e1.Protection,
                                                PointerPte);

                        if ((StoreInstruction != 0) &&
                            ((ProtoProtect.u1.e1.Protection & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK)) {
                            MI_SET_PTE_DIRTY (RealPteContents);
                            VariousFlags |= VARIOUS_FLAGS_DIRTY_PTE;
                        }

                        ContainingPageTablePointer = MiGetPteAddress (PointerPte);
                        Pfn2 = MI_PFN_ELEMENT (ContainingPageTablePointer->u.Hard.PageFrameNumber);
                    }
                }
            }

            ProtosProcessed = 0;

            //
            // Acquire the PFN lock to synchronize access to prototype PTEs.
            // This is required as the working set mutex does not prevent
            // multiple processes from operating on the same prototype PTE.
            //

            VariousFlags |= VARIOUS_FLAGS_PFN_HELD;
            LOCK_PFN (OldIrql);

            if (CheckPte->u.Hard.Valid == 0) {

                //
                // The working set pushlock must be released here because this
                // process' working set may be large, but the number of
                // available pages may be low.  In this case, this thread
                // could end up needing a page to read the prototype PTE
                // into and would thus wait in MiEnsureAvailablePageOrWait,
                // but since this would be as part of handling the a "new"
                // fault, it would not release the process working set.
                //
                // Increment the number of mapped views so the control area
                // (and the prototype PTEs) cannot be immediately deleted since
                // the working set pushlock is going to be released and
                // reacquired.
                //

                CloneDescriptor = NULL;
                ControlArea = NULL;

                if ((Vad == NULL) &&
                    ((PAGE_ALIGN (VirtualAddress) != (PVOID) MM_SHARED_USER_DATA_VA))) {
                    Vad = MiLocateAddress (VirtualAddress);
                    ASSERT (Vad != NULL);
                }

                if (Vad != NULL) {
                    if (Vad->u.VadFlags.PrivateMemory == 0) {
                        ControlArea = Vad->ControlArea;
                        ASSERT (ControlArea->NumberOfMappedViews >= 1);
                        ControlArea->NumberOfMappedViews += 1;
                    }
                    else {

                        //
                        // Forked private VADs are prototype PTE-backed but
                        // have no control areas.  Reference their clone
                        // descriptor instead.
                        //

                        ASSERT (Process->CloneRoot != NULL);
                        CloneDescriptor = MiLocateCloneAddress (Process,
                                                                PointerProtoPte);

                        ASSERT (CloneDescriptor != NULL);
                        ASSERT (CloneDescriptor->NumberOfReferences >= 1);
                        CloneDescriptor->NumberOfReferences += 1;
                        CloneDescriptor->FinalNumberOfReferences += 1;
                    }
                }

                UNLOCK_PFN (OldIrql);
                UNLOCK_WS (WsThread, Process);

                MmAccessFault (FALSE, PointerProtoPte, KernelMode, NULL);

                //
                // Note that the VAD may no longer exist at this point !
                //

                if (ControlArea != NULL) {

                    LOCK_PFN (OldIrql);
    
                    ASSERT (ControlArea->NumberOfMappedViews >= 1);
                    ControlArea->NumberOfMappedViews -= 1;

                    //
                    // See if the control area should be deleted - this
                    // returns with the PFN lock released.
                    //

                    MiCheckControlArea (ControlArea, OldIrql);
                }
                else if (CloneDescriptor != NULL) {

                    LOCK_WS (WsThread, Process);
                    LOCK_PFN (OldIrql);

                    MiDecrementCloneBlockReference (CloneDescriptor,
                                                    NULL,
                                                    Process,
                                                    NULL,
                                                    OldIrql);
                    UNLOCK_PFN (OldIrql);

                    ASSERT (EntryIrql == KeGetCurrentIrql ());
                    ASSERT (KeAreAllApcsDisabled () == TRUE);

                    return STATUS_SUCCESS;
                }

                LOCK_WS (WsThread, Process);

                ASSERT (EntryIrql == KeGetCurrentIrql ());
                ASSERT (KeAreAllApcsDisabled () == TRUE);

                //
                // Reissue the faulting instruction since while the working set
                // pushlock was released, the address space may have changed.
                //

                return STATUS_SUCCESS;
            }

            TempPte = *PointerProtoPte;

            if (RecheckAccess == TRUE) {

                //
                // This is a forked process so shared prototype PTEs
                // may actually be fork clone prototypes.  These have
                // the protection within the fork clone yet the
                // hardware PTEs always share it.  This must be
                // checked here for the case where the NO_ACCESS
                // permission has been put into the fork clone because
                // it would not necessarily be in the hardware PTEs like
                // it is for normal prototypes.
                //
                // First make sure the proto is in transition or paged
                // out as only these states can be no access.
                //

                if ((TempPte.u.Hard.Valid == 0) &&
                    (TempPte.u.Soft.Prototype == 0)) {

                    ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (&TempPte);
                    if (ProtoProtect.u1.e1.Protection == MM_NOACCESS) {
                        ASSERT (MiLocateCloneAddress (Process, PointerProtoPte) != NULL);
                        UNLOCK_PFN (OldIrql);
                        return STATUS_ACCESS_VIOLATION;
                    }
                }
            }

            //
            // If the fault can be handled inline (prototype transition or
            // valid for example), then process it here (eliminating
            // locked page charges, etc) to reduce PFN hold times.
            //

            if ((VariousFlags & VARIOUS_FLAGS_ACCESS_CHECK_NEEDED) == 0) {

                while (TRUE) {
    
                    if (TempPte.u.Hard.Valid == 1) {

                        //
                        // Prototype PTE is valid.
                        //

                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
                        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
                        Pfn1->u2.ShareCount += 1;
    
                    }
                    else if ((TempPte.u.Soft.Prototype == 0) &&
                             (TempPte.u.Soft.Transition == 1)) {
    
                        //
                        // This is a fault on a PTE which ultimately
                        // decodes to a prototype PTE referencing a
                        // page already in the cache.
                        //
                        // Optimize this path as every cycle counts.
                        //
        
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        
                        ASSERT (Pfn1->u3.e1.PageLocation != ActiveAndValid);
        
                        if ((Pfn1->u3.e1.ReadInProgress == 1) ||
                            (Pfn1->u4.InPageError == 1) ||
                            (MmAvailablePages < MM_HIGH_LIMIT)) {
        
                            break;
                        }
        
                        MiUnlinkPageFromList (Pfn1);
        
                        //
                        // Update the PFN database - the reference count
                        // must be incremented as the share count is
                        // going to go from zero to 1.
                        //
        
                        ASSERT (Pfn1->u2.ShareCount == 0);
        
                        //
                        // The PFN reference count will be 1 already
                        // here if the modified writer has begun a write
                        // of this page.  Otherwise it's ordinarily 0.
                        //
                        // Note there is no need to apply locked page
                        // charges for this page as we know that the share
                        // count is zero (and the reference count is
                        // unknown).  But since we are incrementing both
                        // the share and reference counts by one, the
                        // page will retain its current locked charge
                        // regardless of whether or not it is currently
                        // set.
                        //
        
                        InterlockedIncrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);
        
                        //
                        // Update the transition PTE.
                        //
        
                        Pfn1->u2.ShareCount += 1;
                        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        
                        MI_MAKE_TRANSITION_PROTOPTE_VALID (TempPte,
                                                           PointerProtoPte);
        
                        //
                        // If the modified field is set in the PFN database
                        // and this page is not copy on write, then set
                        // the dirty bit.  This can be done as the modified
                        // page will not be written to the paging file
                        // until this PTE is made invalid.
                        //
        
                        if ((Pfn1->u3.e1.Modified) &&
                            (TempPte.u.Hard.Write) &&
                            (TempPte.u.Hard.CopyOnWrite == 0)) {
        
                            MI_SET_PTE_DIRTY (TempPte);
                        }
                        else {
                            MI_SET_PTE_CLEAN (TempPte);
                        }
        
                        MI_WRITE_VALID_PTE (PointerProtoPte, TempPte);
        
                        ASSERT (PointerPte->u.Hard.Valid == 0);
                    }
                    else {
                        break;
                    }
    
                    ProtosProcessed += 1;
    
                    if (ProtosProcessed == NumberOfProtos) {
    
                        //
                        // This is the last (or only) PFN so use
                        // MiCompleteProtoPteFault so the PFN lock is released 
                        // as quickly as possible.
                        //
    
                        MiCompleteProtoPteFault (StoreInstruction,
                                                 VirtualAddress,
                                                 PointerPte,
                                                 PointerProtoPte,
                                                 OldIrql,
                                                 &LockedProtoPfn);
    
                        VariousFlags &= ~VARIOUS_FLAGS_PFN_HELD;
                        break;
                    }
    
                    //
                    // Just finish the PFN work here but not the working
                    // set or prefetcher actions as the PFN lock is held
                    // and we want to minimize PFN hold time.
                    //
    
                    ASSERT (PointerProtoPte->u.Hard.Valid == 1);

                    Pfn1->u3.e1.PrototypePte = 1;

                    //
                    // Prototype PTE is now valid, make the PTE valid.
                    //
                    // A PTE just went from not present, not transition to
                    // present.  The share count and valid count must be
                    // updated in the page table page which contains this PTE.
                    //

                    Pfn2->u2.ShareCount += 1;

                    //
                    // Ensure the user's attributes do not conflict with
                    // the PFN attributes.
                    //

                    NewPteContents.u.Long = RealPteContents.u.Long;
                    ASSERT (NewPteContents.u.Long != 0);

                    if (Pfn1->u3.e1.CacheAttribute == MiCached) {
                        NOTHING;
                    }
                    else if (Pfn1->u3.e1.CacheAttribute == MiNonCached) {

                        Protection = (MM_PROTECTION_MASK) ProtoProtect.u1.e1.Protection;
                        Protection &= ~(MM_NOCACHE | MM_WRITECOMBINE);

                        Protection |= MM_NOCACHE;
                        NewPteContents.u.Long = 0;
                    }
                    else if (Pfn1->u3.e1.CacheAttribute == MiWriteCombined) {
                        Protection = (MM_PROTECTION_MASK) ProtoProtect.u1.e1.Protection;
                        Protection &= ~(MM_NOCACHE | MM_WRITECOMBINE);
                        Protection |= MM_WRITECOMBINE;
                        NewPteContents.u.Long = 0;
                    }

                    if (NewPteContents.u.Long == 0) {
                        MI_MAKE_VALID_USER_PTE (NewPteContents,
                                                0,
                                                Protection,
                                                PointerPte);

                        if ((StoreInstruction != 0) &&
                            ((Protection & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK)) {
                            MI_SET_PTE_DIRTY (NewPteContents);
                        }
                    }

                    NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;

#if DBG

                    //
                    // The protection code for the real PTE comes from
                    // the real PTE as it was placed there above.
                    //

                    ProtoProtect2.u1.Long = 0;
                    ASSERT (PointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED);
                    ProtoProtect2.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(PointerPte);

                    MI_MAKE_VALID_PTE (OriginalPte,
                                       PageFrameIndex,
                                       ProtoProtect2.u1.e1.Protection,
                                       PointerPte);

                    if ((StoreInstruction != 0) &&
                        ((ProtoProtect2.u1.e1.Protection & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK)) {
                        MI_SET_PTE_DIRTY (OriginalPte);
                    }

                    {
                        MMPTE RealPte2;

                        RealPte2.u.Long = RealPteContents.u.Long;
                        RealPte2.u.Hard.PageFrameNumber = PageFrameIndex;
                        ASSERT (OriginalPte.u.Long == RealPte2.u.Long);
                    }
#endif

                    MI_SNAP_DATA (Pfn1, PointerProtoPte, 6);

                    //
                    // If this is a store instruction and the page is not
                    // copy on write, then set the modified bit in the PFN
                    // database and the dirty bit in the PTE.  The PTE is
                    // not set dirty even if the modified bit is set so
                    // writes to the page can be tracked for FlushVirtualMemory.
                    //

                    if (VariousFlags & VARIOUS_FLAGS_DIRTY_PTE) {

                        OriginalPte = Pfn1->OriginalPte;

#if DBG
                        if (OriginalPte.u.Soft.Prototype == 1) {

                            PCONTROL_AREA ControlArea;

                            Subsection = MiGetSubsectionAddress (&OriginalPte);
                            ControlArea = Subsection->ControlArea;

                            if (ControlArea->DereferenceList.Flink != NULL) {
                                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                                    "MM: page fault completing to dereferenced CA %p %p %p\n",
                                                ControlArea, Pfn1, PointerPte);
                                DbgBreakPoint ();
                            }
                        }
#endif

                        MI_SET_MODIFIED (Pfn1, 1, 0xA);

                        if ((OriginalPte.u.Soft.Prototype == 0) &&
                            (Pfn1->u3.e1.WriteInProgress == 0)) {

                            FreeBit = GET_PAGING_FILE_OFFSET (OriginalPte);

                            if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                                MiReleaseConfirmedPageFileSpace (OriginalPte);
                            }

                            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                        }
                    }

                    ASSERT (PointerPte == MiGetPteAddress (VirtualAddress));

                    MI_WRITE_VALID_PTE (PointerPte, NewPteContents);

                    PointerProtoPte += 1;
                    TempPte = *PointerProtoPte;
                    PointerPte += 1;
                    VirtualAddress = (PVOID)((ULONG_PTR)VirtualAddress + PAGE_SIZE);
                }
            }

            if (ProtosProcessed != 0) {

                //
                // At least the first VA was handled and that was the one
                // that caused the fault so just return now as any other
                // VAs were purely optional.
                //

                if (VariousFlags & VARIOUS_FLAGS_PFN_HELD) {

                    //
                    // The last speculative VA was not made valid and the PFN
                    // lock is still held.  Release the PFN lock now.
                    //

                    UNLOCK_PFN (OldIrql);
                    InterlockedExchangeAdd (&KeGetCurrentPrcb ()->MmTransitionCount,
                                            (LONG) ProtosProcessed);
                }
                else {

                    //
                    // The last speculative VA was made valid and was also
                    // inserted into the working set list.  Subtract one from
                    // the count of protos that need working set insertions
                    // below.
                    //

                    InterlockedExchangeAdd (&KeGetCurrentPrcb ()->MmTransitionCount,
                                            (LONG) ProtosProcessed);

                    ProtosProcessed -= 1;
                }

                //
                // Back the locals up to the last "made-valid" VA where
                // the working set insertions need to begin.
                //
                // Add working set entries for the cluster of addresses.
                //
                // Note because we checked the WSLE list above prior
                // to clustering (and the working set mutex has never been
                // released), we are guaranteed that the working set list
                // insertions below cannot fail.
                //

                Subsection = NULL;
                SATISFY_OVERZEALOUS_COMPILER (FileObject = NULL);
                SATISFY_OVERZEALOUS_COMPILER (FileOffset = 0);
                SATISFY_OVERZEALOUS_COMPILER (Flags = 0);

                while (ProtosProcessed != 0) {

                    PointerProtoPte -= 1;
                    PointerPte -= 1;
                    VirtualAddress = (PVOID)((ULONG_PTR)VirtualAddress - PAGE_SIZE);
                    ProtosProcessed -= 1;

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                    ASSERT (ProtoProtect.u1.e1.Protection != MM_ZERO_ACCESS);

                    ASSERT (MI_IS_PAGE_TABLE_ADDRESS(PointerPte));
                    ASSERT (PointerPte->u.Hard.Valid == 1);

                    WorkingSetIndex = MiAllocateWsle (&Process->Vm,
                                                      PointerPte,
                                                      Pfn1,
                                                      ProtoProtect.u1.Long);

                    ASSERT (WorkingSetIndex != 0);

                    //
                    // Log prefetch fault information.
                    //
                    // Note that the process' working set mutex is still
                    // held so any other faults or operations on user
                    // addresses by other threads in this process
                    // will block for the duration of this call.
                    //

                    if ((Subsection == NULL) &&
                        (CCPF_IS_PREFETCHER_ACTIVE()) &&
                        (Pfn1->OriginalPte.u.Soft.Prototype == 1)) {

                        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);

                        FileObject = Subsection->ControlArea->FilePointer;
                        FileOffset = MiStartingOffset (Subsection, PointerProtoPte);

                        Flags = 0;

                        //
                        // Image pages are not speculatively transition
                        // clustered so this must be a data page we are telling
                        // the prefetcher about.
                        //

                        ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

                        if (Subsection->ControlArea->u.Flags.Rom) {
                            Flags |= CCPF_TYPE_ROM;
                        }
                    }

                    if (Subsection != NULL) {
                        CcPfLogPageFault (FileObject, FileOffset, Flags);
                    }
                }

                ASSERT (EntryIrql == KeGetCurrentIrql ());
                ASSERT (EntryIrql <= APC_LEVEL);
                ASSERT (KeAreAllApcsDisabled () == TRUE);

                return STATUS_PAGE_FAULT_TRANSITION;
            }

            ASSERT (VariousFlags & VARIOUS_FLAGS_PFN_HELD);

            LockedProtoPfn = MI_PFN_ELEMENT (CheckPte->u.Hard.PageFrameNumber);
            MI_ADD_LOCKED_PAGE_CHARGE (LockedProtoPfn);
            ASSERT (LockedProtoPfn->u3.e2.ReferenceCount > 1);

            ASSERT (PointerPte->u.Hard.Valid == 0);
        }
        else {
            LOCK_PFN (OldIrql);

            if (CheckPte->u.Hard.Valid == 0) {

                //
                // Make sure the prototype PTEs are in memory.  If not, since
                // this is a system address, just convert the fault as though
                // it happened on the prototype PTE instead.
                //

                ASSERT ((Process == NULL) || (Process == HYDRA_PROCESS));

                UNLOCK_PFN (OldIrql);

                VirtualAddress = PointerProtoPte;
                PointerPte = CheckPte;
                PointerProtoPte = NULL;

                //
                // The page that contains the prototype PTE is not in memory.
                //

                if (Process == HYDRA_PROCESS) {

                    //
                    // We were called while holding this session space's
                    // working set lock.  But we need to fault in a
                    // prototype PTE which is in system paged pool. This
                    // must be done under the system working set lock.
                    //
                    // So we release the session space WSL lock and get
                    // the system working set lock.  When done
                    // we return STATUS_MORE_PROCESSING_REQUIRED
                    // so our caller will call us again to handle the
                    // actual prototype PTE fault.
                    //

                    ASSERT (MI_IS_SESSION_ADDRESS (VirtualAddress) == FALSE);

                    SessionWs = &MmSessionSpace->GlobalVirtualAddress->Vm;

                    UNLOCK_WORKING_SET (WsThread, SessionWs);

                    //
                    // Clear Process as the system working set is now held.
                    //

                    Process = NULL;

                    LOCK_SYSTEM_WS (WsThread);

                    if (PointerPte->u.Hard.Valid != 0) {

                        //
                        // The prototype PTE was made valid by some other
                        // thread while we waited for the system working
                        // set pushlock, so just return and reprocess the fault.
                        //

                        UNLOCK_SYSTEM_WS (WsThread);

                        LOCK_WORKING_SET (WsThread, SessionWs);

                        ASSERT (LockedProtoPfn == NULL);
                        ASSERT (EntryIrql == KeGetCurrentIrql ());
                        ASSERT (KeAreAllApcsDisabled () == TRUE);

                        return STATUS_SUCCESS;
                    }
                }

                goto NonProtoFault;
            }
            else if (PointerPte->u.Hard.Valid == 1) {

                //
                // PTE was already made valid by the cache manager support
                // routines.
                //

                UNLOCK_PFN (OldIrql);

                return STATUS_SUCCESS;
            }
        }

        status = MiResolveProtoPteFault (StoreInstruction,
                                         VirtualAddress,
                                         PointerPte,
                                         PointerProtoPte,
                                         &LockedProtoPfn,
                                         &ReadBlock,
                                         &OriginalPte,
                                         Process,
                                         OldIrql,
                                         TrapInformation);

        //
        // Returns with PFN lock released.
        //

        ReadPte = PointerProtoPte;

        ASSERT (KeGetCurrentIrql() <= APC_LEVEL);
        ASSERT (KeAreAllApcsDisabled () == TRUE);
    }
    else {

NonProtoFault:

        TempPte = *PointerPte;
        ASSERT (TempPte.u.Hard.Valid == 0);
        ASSERT (TempPte.u.Soft.Prototype == 0);
        ASSERT (TempPte.u.Long != 0);

        if (TempPte.u.Soft.Transition != 0) {

            //
            // This is a transition page.
            //

            CapturedEvent = NULL;
            status = MiResolveTransitionFault (VirtualAddress,
                                               PointerPte,
                                               Process,
                                               MM_NOIRQL,
                                               &CapturedEvent);
            if (CapturedEvent != NULL) {
                MiFreeInPageSupportBlock (CapturedEvent);
            }

        }
        else if (TempPte.u.Soft.PageFileHigh == 0) {

            //
            // Demand zero fault.
            //

            status = MiResolveDemandZeroFault (VirtualAddress,
                                               PointerPte,
                                               Process,
                                               MM_NOIRQL);
        }
        else {

            //
            // Page resides in paging file.
            //

            ReadPte = PointerPte;
            LOCK_PFN (OldIrql);

            TempPte = *PointerPte;
            ASSERT (TempPte.u.Long != 0);

            if ((TempPte.u.Hard.Valid == 0) &&
                (TempPte.u.Soft.Prototype == 0) &&
                (TempPte.u.Soft.Transition == 0)) {

                status = MiResolvePageFileFault (VirtualAddress,
                                                 PointerPte,
                                                 &OriginalPte,
                                                 &ReadBlock,
                                                 Process,
                                                 OldIrql);
            }
            else {
                UNLOCK_PFN (OldIrql);
                status = STATUS_REFAULT;
            }
        }
    }

    //
    // Issue the I/O and/or finish completing the soft fault.
    //

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    if (NT_SUCCESS(status)) {

        if (LockedProtoPfn != NULL) {

            //
            // Unlock page containing prototype PTEs.
            //

            ASSERT (PointerProtoPte != NULL);
            LOCK_PFN (OldIrql);

            //
            // The reference count on the prototype PTE page will
            // always be greater than 1 if it is a genuine prototype
            // PTE pool allocation.  However, if it is a fork
            // prototype PTE allocation, it is possible the pool has
            // already been deallocated and in this case, the LockedProtoPfn
            // frame below will be in transition limbo with a share
            // count of 0 and a reference count of 1 awaiting our
            // final dereference below which will put it on the free list.
            //

            ASSERT (LockedProtoPfn->u3.e2.ReferenceCount >= 1);
            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (LockedProtoPfn);
            UNLOCK_PFN (OldIrql);
        }

        if (SessionWs != NULL) {
            UNLOCK_SYSTEM_WS (WsThread);
            LOCK_WORKING_SET (WsThread, SessionWs);
        }

        ASSERT (EntryIrql == KeGetCurrentIrql ());
        ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

        return status;
    }

    if (status == STATUS_ISSUE_PAGING_IO) {

        ASSERT (ReadPte != NULL);
        ASSERT (ReadBlock != NULL);

        if (PointerProtoPte == NULL) {
            OriginalPte = *ReadPte;
        }
        else {

            //
            // Since we may be holding a process or session working set
            // pushlock, we cannot reference the prototype PTE here as it
            // may have been trimmed already (this is possible when the PFN
            // lock was released) - so our callees have already captured
            // it for us.
            //

            ASSERT (OriginalPte.u.Hard.Valid == 0);
            ASSERT (OriginalPte.u.Soft.Prototype == 0);
            ASSERT (OriginalPte.u.Soft.Transition == 1);
        }

        CapturedEvent = (PMMINPAGE_SUPPORT)ReadBlock->Pfn->u1.Event;

        WsThread->ActiveFaultCount += 1;

        if (Process == HYDRA_PROCESS) {
            UNLOCK_WORKING_SET (WsThread,
                                &MmSessionSpace->GlobalVirtualAddress->Vm);
            ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
            ASSERT (KeAreAllApcsDisabled () == TRUE);
        }
        else if (Process != NULL) {

            //
            // APCs must be explicitly disabled to prevent suspend APCs from
            // interrupting this thread before the I/O has been issued.
            // Otherwise a shared page I/O can stop any other thread that
            // references it indefinitely until the suspend is released.
            //

            KeEnterCriticalRegionThread (&WsThread->Tcb);

            UNLOCK_WS (WsThread, Process);

            VariousFlags |= VARIOUS_FLAGS_ENTERED_CRITICAL_REGION;
        }
        else {
            UNLOCK_SYSTEM_WS (WsThread);
            ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
            ASSERT (KeAreAllApcsDisabled () == TRUE);
        }

        if (PERFINFO_IS_GROUP_ON (PERF_FILE_IO)) {
            VariousFlags |= VARIOUS_FLAGS_LOG_HARD_FAULT;

            PerfTimeStamp (IoStartTime);
        }
        else {
            SATISFY_OVERZEALOUS_COMPILER (IoStartTime.QuadPart = 0);
        }

        //
        // Assert no reads issued here are marked as prefetched.
        //

        ASSERT (ReadBlock->u1.e1.PrefetchMdlHighBits == 0);

        //
        // Issue the read request.
        //

        status = IoPageRead (ReadBlock->FilePointer,
                             &ReadBlock->Mdl,
                             &ReadBlock->ReadOffset,
                             &ReadBlock->Event,
                             &ReadBlock->IoStatus);

        if (!NT_SUCCESS(status)) {

            //
            // Set the event as the I/O system doesn't set it on errors.
            //

            ReadBlock->IoStatus.Status = status;
            ReadBlock->IoStatus.Information = 0;
            KeSetEvent (&ReadBlock->Event, 0, FALSE);
        }

        //
        // Initializing PageFrameIndex is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        PageFrameIndex = (PFN_NUMBER)-1;

        //
        // Wait for the I/O operation.
        //

        status = MiWaitForInPageComplete (ReadBlock->Pfn,
                                          ReadPte,
                                          VirtualAddress,
                                          &OriginalPte,
                                          CapturedEvent,
                                          Process);

        WsThread->ActiveFaultCount -= 1;

        if (VariousFlags & VARIOUS_FLAGS_ENTERED_CRITICAL_REGION) {
            KeLeaveCriticalRegionThread (&WsThread->Tcb);
        }

        if (VariousFlags & VARIOUS_FLAGS_LOG_HARD_FAULT) {
            PerfTimeStamp (HardFaultEvent.IoTime);
            HardFaultEvent.IoTime.QuadPart -= IoStartTime.QuadPart;
        }

        //
        // MiWaitForInPageComplete RETURNS WITH THE WORKING SET LOCK
        // AND PFN LOCK HELD!!!
        //

        //
        // This is the thread which owns the event, clear the event field
        // in the PFN database.
        //

        Pfn1 = ReadBlock->Pfn;
        Page = &ReadBlock->Page[0];
        NumberOfBytes = (LONG)ReadBlock->Mdl.ByteCount;
        CheckPte = ReadBlock->BasePte;

        while (NumberOfBytes > 0) {

            //
            // Don't remove the page we just brought in to
            // satisfy this page fault.
            //

            if (CheckPte != ReadPte) {
                PfnClusterPage = MI_PFN_ELEMENT (*Page);
                MI_SNAP_DATA (PfnClusterPage, PfnClusterPage->PteAddress, 0xB);
                ASSERT (PfnClusterPage->u4.PteFrame == Pfn1->u4.PteFrame);
#if DBG
                if (PfnClusterPage->u4.InPageError) {
                    ASSERT (status != STATUS_SUCCESS);
                }
#endif
                if (PfnClusterPage->u3.e1.ReadInProgress != 0) {

                    ASSERT (PfnClusterPage->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                    PfnClusterPage->u3.e1.ReadInProgress = 0;

                    if (PfnClusterPage->u4.InPageError == 0) {
                        PfnClusterPage->u1.Event = NULL;
                    }
                }
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (PfnClusterPage);
            }
            else {
                PageFrameIndex = *Page;
                MI_SNAP_DATA (MI_PFN_ELEMENT (PageFrameIndex),
                              MI_PFN_ELEMENT (PageFrameIndex)->PteAddress,
                              0xC);
            }

            CheckPte += 1;
            Page += 1;
            NumberOfBytes -= PAGE_SIZE;
        }

        if (status != STATUS_SUCCESS) {

            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (MI_PFN_ELEMENT(PageFrameIndex));

            if (status != STATUS_PTE_CHANGED) {

                //
                // An I/O error occurred during the page read
                // operation.  All the pages which were just
                // put into transition should be put onto the
                // free list if InPageError is set, and their
                // PTEs restored to the proper contents.
                //
    
                Page = &ReadBlock->Page[0];
    
                NumberOfBytes = ReadBlock->Mdl.ByteCount;
    
                while (NumberOfBytes > 0) {
    
                    PfnClusterPage = MI_PFN_ELEMENT (*Page);
    
                    if ((PfnClusterPage->u4.InPageError == 1) &&
                        (PfnClusterPage->u3.e2.ReferenceCount == 0)) {
    
                        PfnClusterPage->u4.InPageError = 0;

                        //
                        // Only restore the transition PTE if the address
                        // space still exists.  Another thread may have
                        // deleted the VAD while this thread waited for the
                        // fault to complete - in this case, the frame
                        // will be marked as free already.
                        //

                        if (PfnClusterPage->u3.e1.PageLocation != FreePageList) {
                            ASSERT (PfnClusterPage->u3.e1.PageLocation ==
                                                            StandbyPageList);
                            MiUnlinkPageFromList (PfnClusterPage);
                            ASSERT (PfnClusterPage->u3.e2.ReferenceCount == 0);
                            MiRestoreTransitionPte (PfnClusterPage);
                            MiInsertPageInFreeList (*Page);
                        }
                    }
                    Page += 1;
                    NumberOfBytes -= PAGE_SIZE;
                }
            }

            if (LockedProtoPfn != NULL) {

                //
                // Unlock page containing prototype PTEs.
                //

                ASSERT (PointerProtoPte != NULL);

                //
                // The reference count on the prototype PTE page will
                // always be greater than 1 if it is a genuine prototype
                // PTE pool allocation.  However, if it is a fork
                // prototype PTE allocation, it is possible the pool has
                // already been deallocated and in this case, the LockedProtoPfn
                // frame below will be in transition limbo with a share
                // count of 0 and a reference count of 1 awaiting our
                // final dereference below which will put it on the free list.
                //

                ASSERT (LockedProtoPfn->u3.e2.ReferenceCount >= 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (LockedProtoPfn);
            }

            UNLOCK_PFN (OldIrql);

            if (SessionWs != NULL) {
                UNLOCK_SYSTEM_WS (WsThread);
                LOCK_WORKING_SET (WsThread, SessionWs);
            }

            MiFreeInPageSupportBlock (CapturedEvent);

            if (status == STATUS_PTE_CHANGED) {

                //
                // State of PTE changed during I/O operation, just
                // return success and refault.
                //

                status = STATUS_SUCCESS; 
            }
            else if (status == STATUS_REFAULT) {

                //
                // The I/O operation to bring in a system page failed
                // due to insufficient resources.  Set the status to one
                // of the MmIsRetryIoStatus codes so our caller will
                // delay and retry.
                //

                status = STATUS_NO_MEMORY;
            }

            ASSERT (EntryIrql == KeGetCurrentIrql ());
            return status;
        }

        //
        // PTE is still in transition state, same protection, etc.
        //

        ASSERT (Pfn1->u4.InPageError == 0);

        if (Pfn1->u2.ShareCount == 0) {
            MI_REMOVE_LOCKED_PAGE_CHARGE (Pfn1);
        }

        Pfn1->u2.ShareCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;

        //
        // Ensure the proper cache attribute is used both in the PTE and
        // recorded in the PFN.
        //

        CacheAttribute = MiCached;
        Protection = (MM_PROTECTION_MASK) ReadPte->u.Soft.Protection;
        if (MI_IS_WRITECOMBINE (Protection)) {
            CacheAttribute = MI_TRANSLATE_CACHETYPE (MiWriteCombined, 0);
        }
        else if (MI_IS_NOCACHE (Protection)) {
            CacheAttribute = MI_TRANSLATE_CACHETYPE (MiNonCached, 0);
        }

        if (Pfn1->u3.e1.CacheAttribute != CacheAttribute) {
            MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageFrameIndex,
                                                         CacheAttribute);
            Pfn1->u3.e1.CacheAttribute = CacheAttribute;
        }

        //
        // Fill the PTE.
        //

        MI_MAKE_TRANSITION_PTE_VALID (TempPte, ReadPte);
        if (StoreInstruction && TempPte.u.Hard.Write) {
            MI_SET_PTE_DIRTY (TempPte);
        }
        MI_WRITE_VALID_PTE (ReadPte, TempPte);

        if (PointerProtoPte != NULL) {

            //
            // The prototype PTE has been made valid, now make the
            // original PTE valid.  The original PTE must still be invalid
            // otherwise MiWaitForInPageComplete would have returned
            // a collision status.
            //

            ASSERT (PointerPte->u.Hard.Valid == 0);

            //
            // PTE is not valid, continue with operation.
            //

            status = MiCompleteProtoPteFault (StoreInstruction,
                                              VirtualAddress,
                                              PointerPte,
                                              PointerProtoPte,
                                              OldIrql,
                                              &LockedProtoPfn);

            //
            // Returns with PFN lock released!
            //

            ASSERT (KeAreAllApcsDisabled () == TRUE);
        }
        else {

            ASSERT (LockedProtoPfn == NULL);

            ASSERT (Pfn1->u3.e1.PrototypePte == 0);

            UNLOCK_PFN (OldIrql);

            WorkingSetIndex = MiAddValidPageToWorkingSet (VirtualAddress,
                                                          ReadPte,
                                                          Pfn1,
                                                          0);

            if (WorkingSetIndex == 0) {

                //
                // Trim the page since we couldn't add it to the working
                // set list at this time.
                //

                MiTrimPte (VirtualAddress,
                           ReadPte,
                           Pfn1,
                           Process,
                           ZeroPte);

                status = STATUS_NO_MEMORY;
            }

            ASSERT (KeAreAllApcsDisabled () == TRUE);
        }

        if (VariousFlags & VARIOUS_FLAGS_LOG_HARD_FAULT) {

            HardFaultEvent.ReadOffset = ReadBlock->ReadOffset;
            HardFaultEvent.VirtualAddress = VirtualAddress;
            HardFaultEvent.FileObject = ReadBlock->FilePointer;
            HardFaultEvent.ThreadId = HandleToUlong (WsThread->Cid.UniqueThread);
            HardFaultEvent.ByteCount = ReadBlock->Mdl.ByteCount;

            PerfInfoLogBytes (PERFINFO_LOG_TYPE_HARDFAULT, 
                              &HardFaultEvent, 
                              sizeof(HardFaultEvent));
        }

        MiFreeInPageSupportBlock (CapturedEvent);

        if (status == STATUS_SUCCESS) {
            status = STATUS_PAGE_FAULT_PAGING_FILE;
        }
    }

    if ((status == STATUS_REFAULT) || (status == STATUS_PTE_CHANGED)) {
        status = STATUS_SUCCESS;
    }

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    if (SessionWs != NULL) {
        UNLOCK_SYSTEM_WS (WsThread);
        ASSERT (KeAreAllApcsDisabled () == TRUE);
        LOCK_WORKING_SET (WsThread, SessionWs);
    }

    if (LockedProtoPfn != NULL) {

        //
        // Unlock page containing prototype PTEs.
        //

        ASSERT (PointerProtoPte != NULL);
        LOCK_PFN (OldIrql);

        //
        // The reference count on the prototype PTE page will
        // always be greater than 1 if it is a genuine prototype
        // PTE pool allocation.  However, if it is a fork
        // prototype PTE allocation, it is possible the pool has
        // already been deallocated and in this case, the LockedProtoPfn
        // frame below will be in transition limbo with a share
        // count of 0 and a reference count of 1 awaiting our
        // final dereference below which will put it on the free list.
        //

        ASSERT (LockedProtoPfn->u3.e2.ReferenceCount >= 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (LockedProtoPfn);
        UNLOCK_PFN (OldIrql);
    }

    ASSERT (EntryIrql == KeGetCurrentIrql ());
    ASSERT (KeAreAllApcsDisabled () == TRUE);

    return status;
}


NTSTATUS
MiResolveDemandZeroFault (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine resolves a demand zero page fault.

Arguments:

    VirtualAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at (MM_NOIRQL
              if the caller does not hold the PFN lock).  If the caller holds
              the PFN lock, the lock cannot be dropped, and the page should
              not be added to the working set at this time.


Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PFN lock held conditionally.

--*/


{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    MMPTE TempPte;
    ULONG PageColor;
    LOGICAL NeedToZero;
    LOGICAL BarrierNeeded;
    ULONG BarrierStamp;
    WSLE_NUMBER WorkingSetIndex;
    LOGICAL ZeroPageNeeded;
    LOGICAL CallerHeldPfn;

    NeedToZero = FALSE;
    BarrierNeeded = FALSE;
    CallerHeldPfn = TRUE;

    //
    // Initializing BarrierStamp is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BarrierStamp = 0;

    //
    // Initialize variables assuming the operation will succeed.
    // If it fails (lack of pages or whatever), it's ok that the
    // process' NextPageColor got bumped anyway.  The goal is to do
    // as much as possible without holding the PFN lock.
    //

    if ((Process > HYDRA_PROCESS) && (OldIrql == MM_NOIRQL)) {

        ASSERT (MI_IS_PAGE_TABLE_ADDRESS (PointerPte));

        //
        // If a fork operation is in progress and the faulting thread
        // is not the thread performing the fork operation, block until
        // the fork is completed.
        //

        if (Process->ForkInProgress != NULL) {
            if (MiWaitForForkToComplete (Process) == TRUE) {
                return STATUS_REFAULT;
            }
        }

        PageColor = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                              &Process->NextPageColor);

        ASSERT (PageColor != 0xFFFFFFFF);
        ZeroPageNeeded = TRUE;
    }
    else {
        if (OldIrql != MM_NOIRQL) {
            ZeroPageNeeded = TRUE;
        }
        else {
            ZeroPageNeeded = FALSE;

            //
            // For session space, the BSS of an image is typically mapped
            // directly as an image, but in the case of images that have
            // outstanding user references at the time of section creation,
            // the image is copied to a pagefile backed section and then
            // mapped in session view space (the destination is mapped in
            // system view space).  See MiSessionWideReserveImageAddress.
            //

            if ((Process == HYDRA_PROCESS) &&
                ((MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress)) ||
                 ((VirtualAddress >= (PVOID) MiSessionViewStart) &&
                  (VirtualAddress < (PVOID) MiSessionSpaceWs)))) {

                ZeroPageNeeded = TRUE;
            }
        }

        PageColor = 0xFFFFFFFF;
    }

    if (OldIrql == MM_NOIRQL) {
        CallerHeldPfn = FALSE;
        LOCK_PFN (OldIrql);
    }

    MM_PFN_LOCK_ASSERT();

    ASSERT (PointerPte->u.Hard.Valid == 0);

    //
    // Check to see if a page is available, if a wait is
    // returned, do not continue, just return success.
    //

    if ((MmAvailablePages >= MM_HIGH_LIMIT) ||
        (!MiEnsureAvailablePageOrWait (Process, OldIrql))) {

        if (PageColor != 0xFFFFFFFF) {

            //
            // This page is for a user process and so must be zeroed.
            //

            PageFrameIndex = MiRemoveZeroPageIfAny (PageColor);

            if (PageFrameIndex) {

                //
                // This barrier check is needed after zeroing the page
                // and before setting the PTE valid.  Note since the PFN
                // database entry is used to hold the sequence timestamp,
                // it must be captured now.  Check it at the last possible
                // moment.
                //

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                BarrierStamp = (ULONG)Pfn1->u4.PteFrame;
            }
            else {
                PageFrameIndex = MiRemoveAnyPage (PageColor);
                NeedToZero = TRUE;
            }
        }
        else {

            //
            // As this is a system page, there is no need to
            // remove a page of zeroes, it must be initialized by
            // the system before being used.
            //

            PageColor = MI_GET_PAGE_COLOR_FROM_VA (VirtualAddress);

            if (ZeroPageNeeded) {
                PageFrameIndex = MiRemoveZeroPage (PageColor);
            }
            else {
                PageFrameIndex = MiRemoveAnyPage (PageColor);
            }
        }

        MiInitializePfn (PageFrameIndex, PointerPte, 1);

        if (CallerHeldPfn == FALSE) {
            UNLOCK_PFN (OldIrql);
            if (Process > HYDRA_PROCESS) {
                Process->NumberOfPrivatePages += 1;
                BarrierNeeded = TRUE;
            }
        }

        InterlockedIncrement (&KeGetCurrentPrcb ()->MmDemandZeroCount);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if (NeedToZero) {

            PMMPTE ZeroPte;
            PVOID ZeroAddress;

            ASSERT (CallerHeldPfn == FALSE);

            ZeroPte = MiReserveSystemPtes (1, SystemPteSpace);

            if (ZeroPte != NULL) {

                TempPte = ValidKernelPte;
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

                if (Pfn1->u3.e1.CacheAttribute == MiWriteCombined) {
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                }
                else if (Pfn1->u3.e1.CacheAttribute == MiNonCached) {
                    MI_DISABLE_CACHING (TempPte);
                }

                MI_WRITE_VALID_PTE (ZeroPte, TempPte);

                ZeroAddress = MiGetVirtualAddressMappedByPte (ZeroPte);

                KeZeroSinglePage (ZeroAddress);

                MiReleaseSystemPtes (ZeroPte, 1, SystemPteSpace);
            }
            else {
                MiZeroPhysicalPage (PageFrameIndex);
            }

            //
            // Note the stamping must occur after the page is zeroed.
            //

            MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
        }

        //
        // As this page is demand zero, set the modified bit in the
        // PFN database element and set the dirty bit in the PTE.
        //

        MI_SNAP_DATA (Pfn1, PointerPte, 5);

        if (PointerPte <= MiHighestUserPte) {
            MI_MAKE_VALID_USER_PTE (TempPte,
                                    PageFrameIndex,
                                    PointerPte->u.Soft.Protection,
                                    PointerPte);
        }
        else {

            //
            // Might be system or session or user page directories,
            // so go the long way to find out.
            //

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               PointerPte->u.Soft.Protection,
                               PointerPte);
        }

        if (TempPte.u.Hard.Write != 0) {
            MI_SET_PTE_DIRTY (TempPte);
        }

        if (BarrierNeeded) {
            MI_BARRIER_SYNCHRONIZE (BarrierStamp);
        }

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        if (CallerHeldPfn == FALSE) {

            ASSERT (Pfn1->u1.Event == 0);

            ASSERT (Pfn1->u3.e1.PrototypePte == 0);

            WorkingSetIndex = MiAddValidPageToWorkingSet (VirtualAddress,
                                                          PointerPte,
                                                          Pfn1,
                                                          0);
            if (WorkingSetIndex == 0) {

                //
                // Trim the page since we couldn't add it to the working
                // set list at this time.
                //

                MiTrimPte (VirtualAddress,
                           PointerPte,
                           Pfn1,
                           Process,
                           ZeroPte);

                return STATUS_NO_MEMORY;
            }
        }
        return STATUS_PAGE_FAULT_DEMAND_ZERO;
    }

    if (CallerHeldPfn == FALSE) {
        UNLOCK_PFN (OldIrql);
    }
    return STATUS_REFAULT;
}


NTSTATUS
MiResolveTransitionFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS CurrentProcess,
    IN KIRQL OldIrql,
    OUT PMMINPAGE_SUPPORT *InPageBlock
    )

/*++

Routine Description:

    This routine resolves a transition page fault.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    CurrentProcess - Supplies a pointer to the process object.  If this
                     parameter is NULL, then the fault is for system
                     space and the process's working set lock is not held.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

    InPageBlock - Supplies a pointer to an inpage block pointer.  The caller
                  must initialize this to NULL on entry.  This routine
                  sets this to a non-NULL value to signify an inpage block
                  the caller must free when the caller releases the PFN lock.

Return Value:

    status, either STATUS_SUCCESS, STATUS_REFAULT or an I/O status
    code.

Environment:

    Kernel mode, PFN lock may optionally be held.

--*/

{
    MM_PROTECTION_MASK Protection;
    MMPFNENTRY PfnFlags;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    MMPTE TempPte;
    MMPTE TempPte2;
    NTSTATUS status;
    NTSTATUS PfnStatus;
    PMMINPAGE_SUPPORT CapturedEvent;
    PETHREAD CurrentThread;
    PMMPTE PointerToPteForProtoPage;
    WSLE_NUMBER WorkingSetIndex;
    ULONG PfnLockHeld;
    LOGICAL EnteredCritical;

    //
    // ***********************************************************
    //      Transition PTE.
    // ***********************************************************
    //

    //
    // A transition PTE is either on the free or modified list,
    // on neither list because of its ReferenceCount
    // or currently being read in from the disk (read in progress).
    // If the page is read in progress, this is a collided page
    // and must be handled accordingly.
    //

    ASSERT (*InPageBlock == NULL);

    if (OldIrql == MM_NOIRQL) {

        PfnLockHeld = FALSE;

        //
        // Read the PTE now without the PFN lock so that the PFN entry
        // calculations, etc can be done in advance.  If it turns out the PTE
        // changed after the lock is acquired (should be rare), then
        // recalculate.
        //

        TempPte2 = *PointerPte;

        PageFrameIndex = (PFN_NUMBER) TempPte2.u.Hard.PageFrameNumber;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (OldIrql == MM_NOIRQL);
        LOCK_PFN (OldIrql);

        TempPte = *PointerPte;

        if ((TempPte.u.Soft.Valid == 0) &&
            (TempPte.u.Soft.Prototype == 0) &&
            (TempPte.u.Soft.Transition == 1)) {

            if (TempPte2.u.Long != TempPte.u.Long) {
                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            }

            NOTHING;
        }
        else {
            UNLOCK_PFN (OldIrql);
            return STATUS_REFAULT;
        }
    }
    else {

        PfnLockHeld = TRUE;
        ASSERT (OldIrql != MM_NOIRQL);
        TempPte = *PointerPte;

        ASSERT ((TempPte.u.Soft.Valid == 0) &&
                (TempPte.u.Soft.Prototype == 0) &&
                (TempPte.u.Soft.Transition == 1));

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    }

    //
    // Still in transition format.
    //

    InterlockedIncrement (&KeGetCurrentPrcb ()->MmTransitionCount);

    if (Pfn1->u4.InPageError) {

        //
        // There was an in-page read error and there are other
        // threads colliding for this page, delay to let the
        // other threads complete and return.  Snap relevant PFN fields
        // before releasing the lock as the page may immediately get 
        // reused.
        //

        PfnFlags = Pfn1->u3.e1;
        status = Pfn1->u1.ReadStatus;

        if (!PfnLockHeld) {
            UNLOCK_PFN (OldIrql);
        }

        if (PfnFlags.ReadInProgress) {

            //
            // This only occurs when the page is being reclaimed by the
            // compression reaper.  In this case, the page is still on the
            // transition list (so the ReadStatus is really a flink) so
            // substitute a retry status which will induce a delay so the
            // compression reaper can finish taking the page (and PTE).
            //

            return STATUS_NO_MEMORY;
        }

        ASSERT (!NT_SUCCESS(status));

        return status;
    }

    if (Pfn1->u3.e1.ReadInProgress) {

        //
        // Collided page fault.
        //

        CapturedEvent = (PMMINPAGE_SUPPORT)Pfn1->u1.Event;

        CurrentThread = PsGetCurrentThread ();

        if (CapturedEvent->Thread == CurrentThread) {

            //
            // This detects when the Io APC completion routine accesses
            // the same user page (ie: during an overlapped I/O) that
            // the user thread has already faulted on.
            //
            // This can result in a fatal deadlock and so must
            // be detected here.  Return a unique status code so the
            // (legitimate) callers know this has happened so it can be
            // handled properly, ie: Io must request a callback from
            // the Mm once the first fault has completed.
            //
            // Note that non-legitimate callers must get back a failure
            // status so the thread can be terminated.
            //

#if DBG

#if defined (_AMD64_)

            {
                //
                // Do not assert if there are stack walks in progress since
                // this can be ok. The stack walking code will deal with
                // the in page error raised due to this situation.
                //

                extern LONG RtlpStackWalksInProgress;

                if (RtlpStackWalksInProgress == 0) {
                    ASSERT (CurrentThread->ActiveFaultCount >= 1);
                }
            }

#else
            ASSERT (CurrentThread->ActiveFaultCount >= 1);

#endif

#endif

            CurrentThread->ApcNeeded = 1;

            if (!PfnLockHeld) {
                UNLOCK_PFN (OldIrql);
            }
            return STATUS_MULTIPLE_FAULT_VIOLATION;
        }

        //
        // Increment the reference count for the page so it won't be
        // reused until all collisions have been completed.
        //

        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

        InterlockedIncrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);

        //
        // Careful synchronization is applied to the WaitCount field so
        // that freeing of the inpage block can occur lock-free.  Note
        // that the ReadInProgress bit on each PFN is set and cleared while
        // holding the PFN lock.  Inpage blocks are always (and must be)
        // freed _AFTER_ the ReadInProgress bit is cleared.
        //

        InterlockedIncrement (&CapturedEvent->WaitCount);

        UNLOCK_PFN (OldIrql);

        CurrentThread->ActiveFaultCount += 1;
        EnteredCritical = FALSE;

        if (CurrentProcess == HYDRA_PROCESS) {
            UNLOCK_WORKING_SET (CurrentThread,
                                &MmSessionSpace->GlobalVirtualAddress->Vm);
        }
        else if (CurrentProcess != NULL) {

            //
            // APCs must be explicitly disabled to prevent suspend APCs from
            // interrupting this thread before the wait has been issued.
            // Otherwise the APC can result in this page being locked
            // indefinitely until the suspend is released.
            //

            KeEnterCriticalRegionThread (&CurrentThread->Tcb);

            UNLOCK_WS (CurrentThread, CurrentProcess);
            EnteredCritical = TRUE;
        }
        else {
            UNLOCK_SYSTEM_WS (CurrentThread);
        }

        //
        // Set the inpage block address as the waitcount was incremented
        // above and therefore the free must be done by our caller.
        //

        *InPageBlock = CapturedEvent;

        status = MiWaitForInPageComplete (Pfn1,
                                          PointerPte,
                                          FaultingAddress,
                                          &TempPte,
                                          CapturedEvent,
                                          CurrentProcess);

        //
        // MiWaitForInPageComplete RETURNS WITH THE WORKING SET LOCK
        // AND PFN LOCK HELD!!!
        //

        CurrentThread->ActiveFaultCount -= 1;

        if (EnteredCritical == TRUE) {
            KeLeaveCriticalRegionThread (&CurrentThread->Tcb);
        }

        ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

        if (status != STATUS_SUCCESS) {
            PfnStatus = Pfn1->u1.ReadStatus;
            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);

            //
            // Check to see if an I/O error occurred on this page.
            // If so, try to free the physical page, wait a
            // half second and return a status of PTE_CHANGED.
            // This will result in success being returned to
            // the user and the fault will occur again and should
            // not be a transition fault this time.
            //

            if (Pfn1->u4.InPageError == 1) {
                ASSERT (!NT_SUCCESS(PfnStatus));
                status = PfnStatus;
                if (Pfn1->u3.e2.ReferenceCount == 0) {

                    Pfn1->u4.InPageError = 0;

                    //
                    // Only restore the transition PTE if the address
                    // space still exists.  Another thread may have
                    // deleted the VAD while this thread waited for the
                    // fault to complete - in this case, the frame
                    // will be marked as free already.
                    //

                    if (Pfn1->u3.e1.PageLocation != FreePageList) {
                        ASSERT (Pfn1->u3.e1.PageLocation ==
                                                        StandbyPageList);
                        MiUnlinkPageFromList (Pfn1);
                        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
                        MiRestoreTransitionPte (Pfn1);
                        MiInsertPageInFreeList (PageFrameIndex);
                    }
                }
            }

            if (!PfnLockHeld) {
                UNLOCK_PFN (OldIrql);
            }

            //
            // Instead of returning status, always return STATUS_REFAULT.
            // This is to support filesystems that save state in the
            // ETHREAD of the thread that serviced the fault !  Since
            // collided threads never enter the filesystem, their ETHREADs
            // haven't been hacked up.  Since this only matters when
            // errors occur (specifically STATUS_VERIFY_REQUIRED today),
            // retry any failed I/O in the context of each collider
            // to give the filesystems ample opportunity.
            //

            return STATUS_REFAULT;
        }
    }
    else {

        //
        // PTE refers to a normal transition PTE.
        //

        ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);

        //
        // Check available pages so that a machine which is low on memory
        // can stop this thread from gobbling up the pages from every modified
        // write that completes because that would starve waiting threads.
        //
        // Another scenario is if the system is utilizing a hardware
        // compression cache.  Checking ensures that only a safe amount
        // of the compressed virtual cache is directly mapped so that
        // if the hardware gets into trouble, we can bail it out.
        //

        if ((MmAvailablePages < MM_HIGH_LIMIT) &&
            ((MmAvailablePages == 0) ||
             (PsGetCurrentThread()->MemoryMaker == 0) &&
             (MiEnsureAvailablePageOrWait (CurrentProcess, OldIrql)))) {

            //
            // A wait operation was performed which dropped the locks,
            // repeat this fault.
            //

            if (!PfnLockHeld) {
                UNLOCK_PFN (OldIrql);
            }

            //
            // Note our caller will delay execution after releasing the
            // working set mutex in order to make pages available.
            //

            return STATUS_NO_MEMORY;
        }

        ASSERT (Pfn1->u4.InPageError == 0);
        if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

            //
            // This page must contain an MmSt allocation of prototype PTEs.
            // Because these types of pages reside in paged pool (or special
            // pool) and are part of the system working set, they can be
            // trimmed at any time regardless of the share count.  However,
            // if the share count is nonzero, then the page state will
            // remain active and the page will remain in memory - but the
            // PTE will be set to the transition state.  Make the page
            // valid without incrementing the reference count, but
            // increment the share count.
            //

            ASSERT (((Pfn1->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                    (Pfn1->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                    ((Pfn1->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                    (Pfn1->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

            //
            // Don't increment the valid PTE count for the
            // page table page.
            //

            ASSERT (Pfn1->u2.ShareCount != 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
        }
        else {

            MiUnlinkPageFromList (Pfn1);

            //
            // Update the PFN database - the reference count must be
            // incremented as the share count is going to go from zero to 1.
            //

            ASSERT (Pfn1->u2.ShareCount == 0);

            //
            // The PFN reference count will be 1 already here if the
            // modified writer has begun a write of this page.  Otherwise
            // it's ordinarily 0.
            //

            MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);
        }
    }

    //
    // Join with collided page fault code to handle updating
    // the transition PTE.
    //

    ASSERT (Pfn1->u4.InPageError == 0);

    if (Pfn1->u2.ShareCount == 0) {
        MI_REMOVE_LOCKED_PAGE_CHARGE (Pfn1);
    }

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    //
    // Paged pool is trimmed without regard to sharecounts.
    // This means a paged pool PTE can be in transition while
    // the page is still marked active.
    //
    // Note this check only needs to be done for system space addresses
    // as user space address faults lock down the page containing the
    // prototype PTE entries before processing the fault.
    //
    // One example is a system cache fault - the FaultingAddress is a
    // system cache virtual address, the PointerPte points at the pool
    // allocation containing the relevant prototype PTEs.  This page
    // may have been trimmed because it isn't locked down during
    // processing of system space virtual address faults.
    //

    if (FaultingAddress >= MmSystemRangeStart) {

        PointerToPteForProtoPage = MiGetPteAddress (PointerPte);

        TempPte = *PointerToPteForProtoPage;

        if ((TempPte.u.Hard.Valid == 0) &&
            (TempPte.u.Soft.Transition == 1)) {

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
            Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT ((Pfn2->u3.e1.ReadInProgress == 0) &&
                (Pfn2->u4.InPageError));

            ASSERT (Pfn2->u3.e1.PageLocation == ActiveAndValid);

            ASSERT (((Pfn2->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                    (Pfn2->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                    ((Pfn2->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                    (Pfn2->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

            //
            // Don't increment the valid PTE count for the
            // paged pool page.
            //

            ASSERT (Pfn2->u2.ShareCount != 0);
            ASSERT (Pfn2->u3.e2.ReferenceCount != 0);
            ASSERT (Pfn2->u3.e1.CacheAttribute == MiCached);

            //
            // Ensure the PTE mapping does not conflict with the PFN attributes.
            //

            Protection = (MM_PROTECTION_MASK) Pfn2->OriginalPte.u.Soft.Protection;
            Protection &= ~(MM_NOCACHE | MM_WRITECOMBINE);

            if (Pfn2->u3.e1.CacheAttribute == MiCached) {
                NOTHING;
            }
            else if (Pfn2->u3.e1.CacheAttribute == MiNonCached) {
                Protection |= MM_NOCACHE;
            }
            else if (Pfn2->u3.e1.CacheAttribute == MiWriteCombined) {
                Protection |= MM_WRITECOMBINE;
            }

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               Protection,
                               PointerToPteForProtoPage);

            MI_WRITE_VALID_PTE (PointerToPteForProtoPage, TempPte);
        }
    }

    MI_MAKE_TRANSITION_PTE_VALID (TempPte, PointerPte);

    //
    // If the modified field is set in the PFN database and this
    // page is not copy on modify, then set the dirty bit.
    // This can be done as the modified page will not be
    // written to the paging file until this PTE is made invalid.
    //

    if ((Pfn1->u3.e1.Modified && TempPte.u.Hard.Write) &&
        (TempPte.u.Hard.CopyOnWrite == 0)) {

        MI_SET_PTE_DIRTY (TempPte);
    }
    else {
        MI_SET_PTE_CLEAN (TempPte);
    }

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    if (!PfnLockHeld) {

        ASSERT (Pfn1->u3.e1.PrototypePte == 0);

        UNLOCK_PFN (OldIrql);

        WorkingSetIndex = MiAddValidPageToWorkingSet (FaultingAddress,
                                                      PointerPte,
                                                      Pfn1,
                                                      0);

        if (WorkingSetIndex == 0) {

            //
            // Trim the page since we couldn't add it to the working
            // set list at this time.
            //

            MiTrimPte (FaultingAddress,
                       PointerPte,
                       Pfn1,
                       CurrentProcess,
                       ZeroPte);


            return STATUS_NO_MEMORY;
        }
    }
    return STATUS_PAGE_FAULT_TRANSITION;
}


NTSTATUS
MiResolvePageFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    OUT PMMPTE CapturedPteContents,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine builds the MDL and other structures to allow a
    read operation on a page file for a page fault.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    CapturedPteContents - The contents of the prototype PTE on exit are
                          captured here.  This is because the caller needs
                          to compare it later to ensure things haven't changed,
                          but the caller is unable to capture it on return
                          because the caller may be holding the pushlock of a
                          process or session working set (not system) and thus
                          cannot access the prototype PTE since it can be
                          paged as soon as the PFN lock is released by us.

                          Note this only has meaning if the caller is actually
                          going to issue the I/O.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    status.  A status value of STATUS_ISSUE_PAGING_IO is returned
    if this function completes successfully.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    PMDL Mdl;
    ULONG i;
    PMMPTE BasePte;
    PMMPTE FaultingPte;
    PMMPTE CheckPte;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PSUBSECTION Subsection;
    ULONG ReadSize;
    LARGE_INTEGER StartingOffset;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER MdlPage;
    ULONG PageFileNumber;
    ULONG ClusterSize;
    ULONG BackwardPageCount;
    ULONG ForwardPageCount;
    ULONG MaxForwardPageCount;
    ULONG MaxBackwardPageCount;
    WSLE_NUMBER WorkingSetIndex;
    ULONG PageColor;
    MMPTE TempPte;
    MMPTE ComparePte;
    PMMINPAGE_SUPPORT ReadBlockLocal;
    PETHREAD CurrentThread;
    PMMVAD Vad;
    NTSTATUS Status;
    PKPRCB Prcb;

    // **************************************************
    //    Page File Read
    // **************************************************

    //
    // Calculate the VBN for the in-page operation.
    //

    TempPte = *PointerPte;

    ASSERT (TempPte.u.Hard.Valid == 0);
    ASSERT (TempPte.u.Soft.Prototype == 0);
    ASSERT (TempPte.u.Soft.Transition == 0);

    MM_PFN_LOCK_ASSERT();

    if ((MmAvailablePages < MM_HIGH_LIMIT) &&
        (MiEnsureAvailablePageOrWait (Process, OldIrql))) {

        //
        // A wait operation was performed which dropped the locks,
        // repeat this fault.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_REFAULT;
    }

    ReadBlockLocal = MiGetInPageSupportBlock (OldIrql, &Status);

    if (ReadBlockLocal == NULL) {

        UNLOCK_PFN (OldIrql);

        ASSERT (!NT_SUCCESS (Status));

        return Status;
    }

    //
    // Transition collisions rely on the entire PFN (including the event field)
    // being initialized, the ReadBlockLocal's event being not-signaled,
    // and the ReadBlockLocal's thread and waitcount being initialized.
    //
    // All of this has been done by MiGetInPageSupportBlock already except
    // the PFN settings.  The PFN lock can be safely released once
    // this is done.
    //

    ReadSize = 1;
    BasePte = NULL;
    FaultingPte = PointerPte;

    if (MI_IS_PAGE_TABLE_ADDRESS (PointerPte)) {
        WorkingSetIndex = 1;
    }
    else {
        WorkingSetIndex = MI_PROTOTYPE_WSINDEX;
    }

    //
    // Capture the desired cluster size.
    //

    ClusterSize = MmClusterPageFileReads;
    ASSERT (ClusterSize <= MM_MAXIMUM_READ_CLUSTER_SIZE);

    if (MiInPageSinglePages != 0) {
        MiInPageSinglePages -= 1;
    }
    else if ((ClusterSize > 1) && (MmAvailablePages > MM_PLENTY_FREE_LIMIT)) {

        //
        // Maybe this condition should be only on free+zeroed pages (ie: don't
        // include standby).  Maybe it should look at the recycle rate of
        // the standby list, etc, etc.
        //

        ASSERT (ClusterSize <= MmAvailablePages);

        //
        // Attempt to cluster ahead and behind.
        //

        MaxForwardPageCount = PTE_PER_PAGE - (BYTE_OFFSET (PointerPte) / sizeof (MMPTE));
        ASSERT (MaxForwardPageCount != 0);
        MaxBackwardPageCount = PTE_PER_PAGE - MaxForwardPageCount;
        MaxForwardPageCount -= 1;

        if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {

            //
            // This is a pagefile read for a shared memory (prototype PTE)
            // backed section.   Stay within the prototype PTE pool allocation.
            //
            // The prototype PTE pool start and end must be carefully
            // calculated (remember the user's view may be smaller or larger
            // than this).  Don't bother walking the entire VAD tree if it is
            // very large as this can take a significant amount of time.
            //

            if ((FaultingAddress <= MM_HIGHEST_USER_ADDRESS) &&
                (Process->VadRoot.NumberGenericTableElements < 128)) {

                Vad = MiLocateAddress (FaultingAddress);

                if (Vad != NULL) {
                    Subsection = MiLocateSubsection (Vad,
                                            MI_VA_TO_VPN(FaultingAddress));

                    if (Subsection != NULL) {
                        FirstPte = &Subsection->SubsectionBase[0];
                        LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                        if ((ULONG)(LastPte - PointerPte - 1) < MaxForwardPageCount) {
                            MaxForwardPageCount = (ULONG)(LastPte - PointerPte - 1);
                        }

                        if ((ULONG)(PointerPte - FirstPte) < MaxBackwardPageCount) {
                            MaxBackwardPageCount = (ULONG)(PointerPte - FirstPte);
                        }
                    }
                    else {
                        ClusterSize = 0;
                    }
                }
                else {
                    ClusterSize = 0;
                }
            }
            else {
                ClusterSize = 0;
            }
        }

        CurrentThread = PsGetCurrentThread();

        if (CurrentThread->ForwardClusterOnly) {

            MaxBackwardPageCount = 0;

            if (MaxForwardPageCount == 0) {

                //
                // This PTE is the last one in the page table page and
                // no backwards clustering is enabled for this thread so
                // no clustering can be done.
                //

                ClusterSize = 0;
            }
        }

        if (ClusterSize != 0) {

            if (MaxForwardPageCount > ClusterSize) {
                MaxForwardPageCount = ClusterSize;
            }

            ComparePte = TempPte;
            CheckPte = PointerPte + 1;
            ForwardPageCount = MaxForwardPageCount;

            //
            // Try to cluster forward within the page of PTEs.
            //

            while (ForwardPageCount != 0) {

                ASSERT (MiIsPteOnPdeBoundary (CheckPte) == 0);

                ComparePte.u.Soft.PageFileHigh += 1;

                if (CheckPte->u.Long != ComparePte.u.Long) {
                    break;
                }

                ForwardPageCount -= 1;
                CheckPte += 1;
            }

            ReadSize += (MaxForwardPageCount - ForwardPageCount);

            //
            // Try to cluster backward within the page of PTEs.  Donate
            // any unused forward cluster space to the backwards gathering
            // but keep the entire transfer within the MDL.
            //

            ClusterSize -= (MaxForwardPageCount - ForwardPageCount);

            if (MaxBackwardPageCount > ClusterSize) {
                MaxBackwardPageCount = ClusterSize;
            }

            ComparePte = TempPte;
            BasePte = PointerPte;
            CheckPte = PointerPte;
            BackwardPageCount = MaxBackwardPageCount;

            while (BackwardPageCount != 0) {

                ASSERT (MiIsPteOnPdeBoundary(CheckPte) == 0);

                CheckPte -= 1;
                ComparePte.u.Soft.PageFileHigh -= 1;

                if (CheckPte->u.Long != ComparePte.u.Long) {
                    break;
                }

                BackwardPageCount -= 1;
            }

            ReadSize += (MaxBackwardPageCount - BackwardPageCount);
            BasePte -= (MaxBackwardPageCount - BackwardPageCount);
        }
    }

    if (ReadSize == 1) {

        //
        // Get a page and put the PTE into the transition state with the
        // read-in-progress flag set.
        //

        if (Process == HYDRA_PROCESS) {
            PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
        }
        else if (Process == NULL) {
            PageColor = MI_GET_PAGE_COLOR_FROM_VA(FaultingAddress);
        }
        else {
            PageColor = MI_PAGE_COLOR_VA_PROCESS (FaultingAddress,
                                                  &Process->NextPageColor);
        }

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        MiInitializeReadInProgressSinglePfn (PageFrameIndex,
                                             PointerPte,
                                             &ReadBlockLocal->Event,
                                             WorkingSetIndex);

        MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE (ReadBlockLocal, &TempPte);
    }
    else {

        Mdl = &ReadBlockLocal->Mdl;
        MdlPage = &ReadBlockLocal->Page[0];

        ASSERT (ReadSize <= MmAvailablePages);

        for (i = 0; i < ReadSize; i += 1) {

            //
            // Get a page and put the PTE into the transition state with the
            // read-in-progress flag set.
            //

            if (Process == HYDRA_PROCESS) {
                PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
            }
            else if (Process == NULL) {
                PageColor = MI_GET_PAGE_COLOR_FROM_VA(FaultingAddress);
            }
            else {
                PageColor = MI_PAGE_COLOR_VA_PROCESS (FaultingAddress,
                                                      &Process->NextPageColor);
            }

            *MdlPage = MiRemoveAnyPage (PageColor);
            MdlPage += 1;
        }

        ReadSize *= PAGE_SIZE;

        //
        // Note PageFrameIndex is the actual frame that was requested by
        // this caller.  All the other frames will be put in transition
        // when the inpage completes (provided there are no colliding threads).
        //

        MdlPage = &ReadBlockLocal->Page[0];
        PageFrameIndex = *(MdlPage + (PointerPte - BasePte));

        //
        // Initialize the MDL for this request.
        //

        MmInitializeMdl (Mdl,
                         MiGetVirtualAddressMappedByPte (BasePte),
                         ReadSize);

        Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

        //
        // Set PointerPte and TempPte to the base of the cluster so the
        // correct starting offset can be calculated below.  Note this must
        // be done before MiInitializeReadInProgressPfn overwrites the PTEs.
        //

        PointerPte = BasePte;
        TempPte = *PointerPte;
        ASSERT (TempPte.u.Soft.Prototype == 0);
        ASSERT (TempPte.u.Soft.Transition == 0);

        //
        // Put the PTEs into the transition state with the
        // read-in-progress flag set.
        //

        MiInitializeReadInProgressPfn (Mdl,
                                       BasePte,
                                       &ReadBlockLocal->Event,
                                       WorkingSetIndex);

        MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(ReadBlockLocal);
    }

    //
    // Capture the faulting PTE's contents (it will be in transition now)
    // prior to releasing the PFN lock.  The contents of this prototype PTE
    // are used by the caller to compare it later to ensure things haven't
    // changed, but the caller is unable to capture it on return because
    // the caller may be holding the pushlock of a process or session working
    // set (not system) and thus cannot access the prototype PTE since it can be
    // paged as soon as the PFN lock is released here.
    //
    // Note this only has meaning if the caller is actually going to issue
    // the I/O, ie: we must return STATUS_ISSUE_PAGING_IO here.
    //

    *CapturedPteContents = *FaultingPte;

    ASSERT (CapturedPteContents->u.Hard.Valid == 0);
    ASSERT (CapturedPteContents->u.Soft.Prototype == 0);
    ASSERT (CapturedPteContents->u.Soft.Transition == 1);

    UNLOCK_PFN (OldIrql);

    Prcb = KeGetCurrentPrcb ();
    InterlockedExchangeAdd (&Prcb->MmPageReadCount,
                            (LONG) (ReadSize >> PAGE_SHIFT));
    InterlockedIncrement (&Prcb->MmPageReadIoCount);

    *ReadBlock = ReadBlockLocal;

    PageFileNumber = GET_PAGING_FILE_NUMBER (TempPte);
    StartingOffset.LowPart = GET_PAGING_FILE_OFFSET (TempPte);

    ASSERT (StartingOffset.LowPart <= MmPagingFile[PageFileNumber]->Size);

    StartingOffset.HighPart = 0;
    StartingOffset.QuadPart = StartingOffset.QuadPart << PAGE_SHIFT;

    ReadBlockLocal->FilePointer = MmPagingFile[PageFileNumber]->File;

#if DBG

    if (((StartingOffset.QuadPart >> PAGE_SHIFT) < 8192) && (PageFileNumber == 0)) {
        if ((MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT] & ~0x1f) !=
               ((ULONG_PTR)PointerPte << 3)) {
            if ((MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT] & ~0x1f) !=
                  ((ULONG_PTR)(MiGetPteAddress(FaultingAddress)) << 3)) {

                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                    "MMINPAGE: Mismatch PointerPte %p Offset %I64X info %p\n",
                         PointerPte,
                         StartingOffset.QuadPart >> PAGE_SHIFT,
                         MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT]);

                DbgBreakPoint ();
            }
        }
    }

#endif //DBG

    ReadBlockLocal->ReadOffset = StartingOffset;
    ReadBlockLocal->BasePte = PointerPte;

    //
    // Build a single page MDL for the request unless it was a cluster -
    // clustered MDLs have already been constructed.
    //

    if (ReadSize == 1) {
        MmInitializeMdl (&ReadBlockLocal->Mdl, PAGE_ALIGN(FaultingAddress), PAGE_SIZE);
        ReadBlockLocal->Mdl.MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);
        ReadBlockLocal->Page[0] = PageFrameIndex;
    }

    ReadBlockLocal->Pfn = MI_PFN_ELEMENT (PageFrameIndex);

    return STATUS_ISSUE_PAGING_IO;
}

NTSTATUS
MiResolveProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN OUT PMMPFN *LockedProtoPfn,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    OUT PMMPTE CapturedPteContents,
    IN PEPROCESS Process,
    IN KIRQL OldIrql,
    IN PVOID TrapInformation
    )

/*++

Routine Description:

    This routine resolves a prototype PTE fault.

Arguments:

    StoreInstruction - Supplies nonzero if the instruction is trying
                       to modify the faulting address (i.e. write
                       access required).

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in.

    LockedProtoPfn - Supplies a non-NULL pointer to the prototype PTE's PFN
                     that was locked down by the caller, or NULL if the caller
                     did not lock down any PFN.  This routine may unlock
                     the PFN - if so, it must also clear this pointer.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    CapturedPteContents - The contents of the prototype PTE on exit are
                          captured here.  This is because the caller needs
                          to compare it later to ensure things haven't changed,
                          but the caller is unable to capture it on return
                          because the caller may be holding the pushlock of a
                          process or session working set (not system) and thus
                          cannot access the prototype PTE since it can be
                          paged as soon as the PFN lock is released by us.

                          Note this only has meaning if the caller is actually
                          going to issue the I/O (ie, this routine must be
                          returning STATUS_ISSUE_PAGING_IO).

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

    TrapInformation - Supplies the trap information.

Return Value:

    NTSTATUS: STATUS_SUCCESS, STATUS_REFAULT, or an I/O status code.

Environment:

    Kernel mode, PFN lock held.

--*/
{
    MMPTE TempPte;
    MMPTE RealPte;
    MMPTE NewPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS status;
    ULONG CopyOnWrite;
    LOGICAL PfnHeld;
    PMMINPAGE_SUPPORT CapturedEvent;

    //
    // Note the PFN lock must be held as the routine to locate a working
    // set entry decrements the share count of PFN elements.
    //

    MM_PFN_LOCK_ASSERT ();

    ASSERT (PointerPte->u.Hard.Valid == 0);
    ASSERT (PointerPte->u.Soft.Prototype == 1);

    TempPte = *PointerProtoPte;

    //
    // The page containing the prototype PTE is resident,
    // handle the fault referring to the prototype PTE.
    // If the prototype PTE is already valid, make this
    // PTE valid and up the share count etc.
    //

    if (TempPte.u.Hard.Valid) {

        //
        // Prototype PTE is valid, count this as a transition fault.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn1->u2.ShareCount += 1;

        InterlockedIncrement (&KeGetCurrentPrcb ()->MmTransitionCount);

        return MiCompleteProtoPteFault (StoreInstruction,
                                        FaultingAddress,
                                        PointerPte,
                                        PointerProtoPte,
                                        OldIrql,
                                        LockedProtoPfn);
    }

    //
    // Check to make sure the prototype PTE is committed.
    //

    if (TempPte.u.Long == 0) {
        MI_BREAK_ON_AV (FaultingAddress, 0xA);
        UNLOCK_PFN (OldIrql);
        return STATUS_ACCESS_VIOLATION;
    }

    CapturedEvent = NULL;

    //
    // If the PTE indicates that the protection field to be
    // checked is in the prototype PTE, check it now.
    //

    CopyOnWrite = FALSE;

    RealPte = *PointerPte;

    if (RealPte.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {
        if (RealPte.u.Proto.ReadOnly == 0) {

            //
            // Check for kernel mode access, we have already verified
            // that the user has access to the virtual address.
            //

            status = MiAccessCheck (PointerProtoPte,
                                    StoreInstruction,
                                    KernelMode,
                                    MI_GET_PROTECTION_FROM_SOFT_PTE (&TempPte),
                                    TrapInformation,
                                    TRUE);

            if (status != STATUS_SUCCESS) {

                if ((StoreInstruction) &&
                    (MI_IS_SESSION_ADDRESS (FaultingAddress)) &&
                    (MmSessionSpace->ImageLoadingCount != 0)) {
        
                    PLIST_ENTRY NextEntry;
                    PIMAGE_ENTRY_IN_SESSION Image;

                    NextEntry = MmSessionSpace->ImageList.Flink;

                    while (NextEntry != &MmSessionSpace->ImageList) {

                        Image = CONTAINING_RECORD (NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

                        if ((FaultingAddress >= Image->Address) &&
                            (FaultingAddress <= Image->LastAddress)) {

                            if (Image->ImageLoading) {

                                //
                                // Temporarily allow the write so that
                                // relocations and import snaps can be
                                // completed.
                                //
                                // Even though the page's current backing
                                // is the image file, the modified writer
                                // will convert it to pagefile backing
                                // when it notices the change later.
                                //

                                goto done;
                            }
                            break;
                        }
                        NextEntry = NextEntry->Flink;
                    }
                }

                MI_BREAK_ON_AV (FaultingAddress, 9);
                UNLOCK_PFN (OldIrql);
                return status;
            }
            if ((TempPte.u.Soft.Protection & MM_COPY_ON_WRITE_MASK) ==
                 MM_COPY_ON_WRITE_MASK) {
                CopyOnWrite = TRUE;
            }
        }
    }
    else {
        if ((RealPte.u.Soft.Protection & MM_COPY_ON_WRITE_MASK) ==
             MM_COPY_ON_WRITE_MASK) {
            CopyOnWrite = TRUE;
        }
    }

    //
    // If the fault is in user space and this is a cloned process then
    // don't take the optimized demand zero copy on write path as we
    // want to go the long way so the clone block can be properly
    // decremented in MiCopyOnWrite.
    //

    if ((PointerPte <= MiHighestUserPte) &&
        (Process > PREFETCH_PROCESS) &&
        (Process->CloneRoot != NULL)) {

        CopyOnWrite = FALSE;
    }

done:

    if ((!IS_PTE_NOT_DEMAND_ZERO (TempPte)) && (CopyOnWrite)) {

        //
        // The prototype PTE is demand zero and copy on
        // write.  Make this PTE a private demand zero PTE.
        //

        ASSERT (Process != NULL);

        UNLOCK_PFN (OldIrql);

        NewPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        ASSERT (RealPte.u.Hard.Valid == 0);

        if (RealPte.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {

            //
            // Get the protection from the prototype PTE.
            //

            if (TempPte.u.Soft.Protection & MM_EXECUTE) {
                NewPte.u.Long = (MM_EXECUTE_READWRITE << MM_PROTECT_FIELD_SHIFT);
            }
        }
        else {

            //
            // The prototype PTE protection has been overridden by the
            // protection in the real PTE.
            //

            if (RealPte.u.Soft.Protection & MM_EXECUTE) {
                NewPte.u.Long = (MM_EXECUTE_READWRITE << MM_PROTECT_FIELD_SHIFT);
            }
        }

        MI_WRITE_INVALID_PTE (PointerPte, NewPte);

        status = MiResolveDemandZeroFault (FaultingAddress,
                                           PointerPte,
                                           Process,
                                           MM_NOIRQL);
        return status;
    }

    //
    // Make the prototype PTE valid, the prototype PTE is in
    // one of these 4 states:
    //
    //   demand zero
    //   transition
    //   paging file
    //   mapped file
    //

    if (TempPte.u.Soft.Prototype == 1) {

        //
        // Mapped File.
        //

        status = MiResolveMappedFileFault (PointerProtoPte,
                                           ReadBlock,
                                           Process,
                                           OldIrql);

        if (status == STATUS_ISSUE_PAGING_IO) {

            *CapturedPteContents = *PointerProtoPte;

            ASSERT (CapturedPteContents->u.Hard.Valid == 0);
            ASSERT (CapturedPteContents->u.Soft.Prototype == 0);
            ASSERT (CapturedPteContents->u.Soft.Transition == 1);
        }

        //
        // Returns with PFN lock held.
        //

        PfnHeld = TRUE;
    }
    else if (TempPte.u.Soft.Transition == 1) {

        //
        // Transition.
        //

        ASSERT (OldIrql != MM_NOIRQL);

        status = MiResolveTransitionFault (FaultingAddress,
                                           PointerProtoPte,
                                           Process,
                                           OldIrql,
                                           &CapturedEvent);
        //
        // Returns with PFN lock held.
        //

        PfnHeld = TRUE;
    }
    else if (TempPte.u.Soft.PageFileHigh == 0) {

        //
        // Demand Zero.
        //

        ASSERT (OldIrql != MM_NOIRQL);

        status = MiResolveDemandZeroFault (FaultingAddress,
                                           PointerProtoPte,
                                           Process,
                                           OldIrql);

        //
        // Returns with PFN lock held.
        //

        PfnHeld = TRUE;
    }
    else {

        //
        // Paging file.
        //

        status = MiResolvePageFileFault (FaultingAddress,
                                         PointerProtoPte,
                                         CapturedPteContents,
                                         ReadBlock,
                                         Process,
                                         OldIrql);

#if DBG
        if (status == STATUS_ISSUE_PAGING_IO) {
            ASSERT (CapturedPteContents->u.Hard.Valid == 0);
            ASSERT (CapturedPteContents->u.Soft.Prototype == 0);
            ASSERT (CapturedPteContents->u.Soft.Transition == 1);
        }
#endif

        //
        // Returns with PFN lock released.
        //

        ASSERT (KeAreAllApcsDisabled () == TRUE);
        PfnHeld = FALSE;
    }

    if (NT_SUCCESS (status)) {

        ASSERT (PointerPte->u.Hard.Valid == 0);

        status = MiCompleteProtoPteFault (StoreInstruction,
                                          FaultingAddress,
                                          PointerPte,
                                          PointerProtoPte,
                                          OldIrql,
                                          LockedProtoPfn);

        if (CapturedEvent != NULL) {
            MiFreeInPageSupportBlock (CapturedEvent);
        }
    }
    else {

        if (PfnHeld == TRUE) {
            UNLOCK_PFN (OldIrql);
        }

        ASSERT (KeAreAllApcsDisabled () == TRUE);

        if (CapturedEvent != NULL) {
            MiFreeInPageSupportBlock (CapturedEvent);
        }
    }

    return status;
}



NTSTATUS
MiCompleteProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN KIRQL OldIrql,
    IN OUT PMMPFN *LockedProtoPfn
    )

/*++

Routine Description:

    This routine completes a prototype PTE fault.  It is invoked
    after a read operation has completed bringing the data into
    memory.

Arguments:

    StoreInstruction - Supplies nonzero if the instruction is trying
                       to modify the faulting address (i.e. write
                       access required).

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in,
                      NULL if no prototype PTE exists.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

    LockedProtoPfn - Supplies a pointer to a non-NULL prototype PTE's PFN
                     pointer that was locked down by the caller, or a
                     pointer to NULL if the caller did not lock down any
                     PFN.  This routine may unlock the PFN - if so, it
                     must also clear this pointer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PFN lock held.

--*/
{
    NTSTATUS Status;
    ULONG FreeBit;
    MMPTE TempPte;
    MMPTE ProtoPteContents;
    MMPTE OriginalPte;
    MMWSLE ProtoProtect;
    LOGICAL SameProtectAsProto;
    ULONG MarkPageDirty;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE ContainingPageTablePointer;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;
    PSUBSECTION Subsection;
    MMSECTION_FLAGS ControlAreaFlags;
    ULONG Flags;
    WSLE_NUMBER WorkingSetIndex;
    PEPROCESS CurrentProcess;
    MM_PROTECTION_MASK Protection;

    MM_PFN_LOCK_ASSERT();

    ASSERT (PointerProtoPte->u.Hard.Valid == 1);

    ProtoPteContents.u.Long = PointerProtoPte->u.Long;

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->u3.e1.PrototypePte = 1;

    //
    // Capture prefetch fault information.
    //

    OriginalPte = Pfn1->OriginalPte;

    //
    // Prototype PTE is now valid, make the PTE valid.
    //
    // A PTE just went from not present, not transition to
    // present.  The share count and valid count must be
    // updated in the page table page which contains this PTE.
    //

    ContainingPageTablePointer = MiGetPteAddress (PointerPte);
    Pfn2 = MI_PFN_ELEMENT (ContainingPageTablePointer->u.Hard.PageFrameNumber);
    Pfn2->u2.ShareCount += 1;

    ProtoProtect.u1.Long = 0;
    if (PointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

        //
        // The protection code for the real PTE comes from the real PTE as
        // it was placed there earlier during the handling of this fault.
        //

        ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (PointerPte);
        SameProtectAsProto = FALSE;
    }
    else {

        //
        // Use the protection in the prototype PTE to initialize the real PTE.
        //

        ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (&OriginalPte);
        SameProtectAsProto = TRUE;

        MI_ASSERT_NOT_SESSION_DATA (PointerPte);

        if ((StoreInstruction != 0) &&
            ((ProtoProtect.u1.e1.Protection & MM_PROTECTION_WRITE_MASK) == 0)) {

            //
            // This is the errant case where the user is trying to write
            // to a readonly subsection in the image.  Since we're more than
            // halfway through the fault, take the easy way to clean this up -
            // treat the access as a read for the rest of this trip through
            // the fault.  We'll then immediately refault when the instruction
            // is rerun (because it's really a write), and then we'll notice
            // that the user's PTE is not copy-on-write (or even writable!)
            // and return a clean access violation.
            //

            StoreInstruction = 0;
        }
    }

    MI_SNAP_DATA (Pfn1, PointerProtoPte, 0xD);

    MarkPageDirty = 0;

    //
    // If this is a store instruction and the page is not copy on
    // write, then set the modified bit in the PFN database and
    // the dirty bit in the PTE.  The PTE is not set dirty even
    // if the modified bit is set so writes to the page can be
    // tracked for FlushVirtualMemory.
    //

    if ((StoreInstruction != 0) &&
        ((ProtoProtect.u1.e1.Protection & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK)) {

        MarkPageDirty = 1;

#if DBG
        if (OriginalPte.u.Soft.Prototype == 1) {

            PCONTROL_AREA ControlArea;

            Subsection = MiGetSubsectionAddress (&OriginalPte);
            ControlArea = Subsection->ControlArea;

            if (ControlArea->DereferenceList.Flink != NULL) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                    "MM: page fault completing to dereferenced CA %p %p %p\n",
                                ControlArea, Pfn1, PointerPte);
                DbgBreakPoint ();
            }
        }
#endif

        MI_SET_MODIFIED (Pfn1, 1, 0xA);

        if ((OriginalPte.u.Soft.Prototype == 0) &&
            (Pfn1->u3.e1.WriteInProgress == 0)) {

            FreeBit = GET_PAGING_FILE_OFFSET (OriginalPte);

            if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                MiReleaseConfirmedPageFileSpace (OriginalPte);
            }

            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }
    }

    if (*LockedProtoPfn != NULL) {

        //
        // Unlock page containing prototype PTEs.
        //
        // The reference count on the prototype PTE page will
        // always be greater than 1 if it is a genuine prototype
        // PTE pool allocation.  However, if it is a fork
        // prototype PTE allocation, it is possible the pool has
        // already been deallocated and in this case, the LockedProtoPfn
        // frame below will be in transition limbo with a share
        // count of 0 and a reference count of 1 awaiting our
        // final dereference below which will put it on the free list.
        //

        ASSERT ((*LockedProtoPfn)->u3.e2.ReferenceCount >= 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (*LockedProtoPfn);

        //
        // Tell our caller we've unlocked it for him.
        //

        *LockedProtoPfn = NULL;
    }

    UNLOCK_PFN (OldIrql);

    //
    // Ensure the user's attributes do not conflict with the PFN attributes.
    //

    Protection = (MM_PROTECTION_MASK) ProtoProtect.u1.e1.Protection;
    Protection &= ~(MM_NOCACHE | MM_WRITECOMBINE);

    if (Pfn1->u3.e1.CacheAttribute == MiCached) {
        NOTHING;
    }
    else if (Pfn1->u3.e1.CacheAttribute == MiNonCached) {
        Protection |= MM_NOCACHE;
    }
    else if (Pfn1->u3.e1.CacheAttribute == MiWriteCombined) {
        Protection |= MM_WRITECOMBINE;
    }

    if (FaultingAddress < MmSystemRangeStart) {
        MI_MAKE_VALID_USER_PTE (TempPte,
                                PageFrameIndex,
                                Protection,
                                PointerPte);
    }
    else {

        //
        // Might be system or session, so go the long way to find out.
        //

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Protection,
                           PointerPte);
    }

    if (MarkPageDirty != 0) {
        MI_SET_PTE_DIRTY (TempPte);
    }

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    if (SameProtectAsProto == TRUE) {
        ProtoProtect.u1.e1.Protection = MM_ZERO_ACCESS;
    }

    WorkingSetIndex = MiAddValidPageToWorkingSet (FaultingAddress,
                                                  PointerPte,
                                                  Pfn1,
                                                  (ULONG) ProtoProtect.u1.Long);

    if (WorkingSetIndex == 0) {

        if (SameProtectAsProto == FALSE) {

            //
            // The protection for the prototype PTE is in the WSLE.
            //

            ASSERT (ProtoProtect.u1.e1.Protection != MM_ZERO_ACCESS);

            TempPte.u.Long = 0;
            TempPte.u.Soft.Protection =
                MI_GET_PROTECTION_FROM_WSLE (&ProtoProtect);
            TempPte.u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;
        }
        else {

            //
            // The protection is in the prototype PTE.
            //

            TempPte.u.Long = MiProtoAddressForPte (Pfn1->PteAddress);
        }

        TempPte.u.Proto.Prototype = 1;

        //
        // Trim the page since we couldn't add it to the working
        // set list at this time.
        //

        if (FaultingAddress < MmSystemRangeStart) {
            CurrentProcess = PsGetCurrentProcess ();
        }
        else if ((MI_IS_SESSION_ADDRESS (FaultingAddress)) ||
                 (MI_IS_SESSION_PTE (FaultingAddress))) {
            CurrentProcess = HYDRA_PROCESS;
        }
        else {
            CurrentProcess = NULL;
        }

        MiTrimPte (FaultingAddress,
                   PointerPte,
                   Pfn1,
                   CurrentProcess,
                   TempPte);

        Status = STATUS_NO_MEMORY;
    }
    else {
        Status = STATUS_SUCCESS;
    }

    //
    // Log prefetch fault information now that the PFN lock has been released
    // and the PTE has been made valid.  This minimizes PFN lock contention,
    // allows CcPfLogPageFault to allocate (and fault on) pool, and allows other
    // threads in this process to execute without faulting on this address.
    //
    // Note that the process' working set mutex is still held so any other
    // faults or operations on user addresses by other threads in this process
    // will block for the duration of this call.
    //

    if ((CCPF_IS_PREFETCHER_ACTIVE()) && (OriginalPte.u.Soft.Prototype == 1)) {

        Subsection = MiGetSubsectionAddress (&OriginalPte);

        FileObject = Subsection->ControlArea->FilePointer;
        FileOffset = MiStartingOffset (Subsection, PointerProtoPte);
        ControlAreaFlags = Subsection->ControlArea->u.Flags;

        Flags = 0;
        if (ControlAreaFlags.Image) {
            Flags |= CCPF_TYPE_IMAGE;
        }
        if (ControlAreaFlags.Rom) {
            Flags |= CCPF_TYPE_ROM;
        }
        CcPfLogPageFault (FileObject, FileOffset, Flags);
    }

    ASSERT (PointerPte == MiGetPteAddress (FaultingAddress));

    return Status;
}


NTSTATUS
MiResolveMappedFileFault (
    IN PMMPTE PointerPte,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine builds the MDL and other structures to allow a
    read operation on a mapped file for a page fault.

Arguments:

    PointerPte - Supplies the PTE for the faulting address.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    status.  A status value of STATUS_ISSUE_PAGING_IO is returned
    if this function completes successfully.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PMDL Mdl;
    ULONG ReadSize;
    PETHREAD CurrentThread;
    PPFN_NUMBER Page;
    PPFN_NUMBER EndPage;
    PMMPTE BasePte;
    PMMPTE CheckPte;
    LARGE_INTEGER StartingOffset;
    LARGE_INTEGER TempOffset;
    PPFN_NUMBER FirstMdlPage;
    PMMINPAGE_SUPPORT ReadBlockLocal;
    ULONG PageColor;
    ULONG ClusterSize;
    PFN_NUMBER AvailablePages;
    NTSTATUS Status;
    PKPRCB Prcb;

    ClusterSize = 0;

    ASSERT (PointerPte->u.Soft.Prototype == 1);

    // *********************************************
    //   Mapped File (subsection format)
    // *********************************************

    if ((MmAvailablePages < MM_HIGH_LIMIT) &&
        (MiEnsureAvailablePageOrWait (Process, OldIrql))) {

        //
        // A wait operation was performed which dropped the locks,
        // repeat this fault.
        //

        return STATUS_REFAULT;
    }

    //
    // Calculate address of subsection for this prototype PTE.
    //

    Subsection = MiGetSubsectionAddress (PointerPte);

    ControlArea = Subsection->ControlArea;

    if (ControlArea->u.Flags.FailAllIo) {
        return STATUS_IN_PAGE_ERROR;
    }

    if (PointerPte >= &Subsection->SubsectionBase[Subsection->PtesInSubsection]) {

        //
        // Attempt to read past the end of this subsection.
        //

        return STATUS_ACCESS_VIOLATION;
    }

    ASSERT (ControlArea->u.Flags.Rom != 1);

    CurrentThread = PsGetCurrentThread ();

    ReadBlockLocal = MiGetInPageSupportBlock (OldIrql, &Status);

    if (ReadBlockLocal == NULL) {
        ASSERT (!NT_SUCCESS (Status));
        return Status;
    }

    *ReadBlock = ReadBlockLocal;

    //
    // Build an MDL for the request.
    //

    Mdl = &ReadBlockLocal->Mdl;

    FirstMdlPage = &ReadBlockLocal->Page[0];
    Page = FirstMdlPage;

#if DBG
    RtlFillMemoryUlong (Page, (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof(PFN_NUMBER), 0xf1f1f1f1);
#endif //DBG

    ReadSize = PAGE_SIZE;
    BasePte = PointerPte;

    //
    // Should we attempt to perform page fault clustering?
    //

    AvailablePages = MmAvailablePages;

    if (MiInPageSinglePages != 0) {
        AvailablePages = 0;
        MiInPageSinglePages -= 1;
    }

    if ((!CurrentThread->DisablePageFaultClustering) &&
        (ControlArea->u.Flags.NoModifiedWriting == 0)) {

        if ((AvailablePages > (MmFreeGoal * 2))
                 ||
         (((ControlArea->u.Flags.Image != 0) ||
            (CurrentThread->ForwardClusterOnly)) &&
         (AvailablePages > MM_HIGH_LIMIT))) {

            //
            // Cluster up to n pages.  This one + n-1.
            //

            ASSERT (MM_HIGH_LIMIT > MM_MAXIMUM_READ_CLUSTER_SIZE + 16);
            ASSERT (AvailablePages > MM_MAXIMUM_READ_CLUSTER_SIZE + 16);

            if (ControlArea->u.Flags.Image == 0) {
                ASSERT (CurrentThread->ReadClusterSize <=
                            MM_MAXIMUM_READ_CLUSTER_SIZE);
                ClusterSize = CurrentThread->ReadClusterSize;
            }
            else {
                ClusterSize = MmDataClusterSize;
                if (Subsection->u.SubsectionFlags.Protection &
                                            MM_PROTECTION_EXECUTE_MASK ) {
                    ClusterSize = MmCodeClusterSize;
                }
            }
            EndPage = Page + ClusterSize;

            CheckPte = PointerPte + 1;

            //
            // Try to cluster within the page of PTEs.
            //

            while ((MiIsPteOnPdeBoundary(CheckPte) == 0) &&
               (Page < EndPage) &&
               (CheckPte <
                 &Subsection->SubsectionBase[Subsection->PtesInSubsection])
                      && (CheckPte->u.Long == BasePte->u.Long)) {

                ControlArea->NumberOfPfnReferences += 1;
                ReadSize += PAGE_SIZE;
                Page += 1;
                CheckPte += 1;
            }

            if ((Page < EndPage) && (!CurrentThread->ForwardClusterOnly)) {

                //
                // Attempt to cluster going backwards from the PTE.
                //

                CheckPte = PointerPte - 1;

                while ((((ULONG_PTR)CheckPte & (PAGE_SIZE - 1)) !=
                                            (PAGE_SIZE - sizeof(MMPTE))) &&
                        (Page < EndPage) &&
                         (CheckPte >= Subsection->SubsectionBase) &&
                         (CheckPte->u.Long == BasePte->u.Long)) {

                    ControlArea->NumberOfPfnReferences += 1;
                    ReadSize += PAGE_SIZE;
                    Page += 1;
                    CheckPte -= 1;
                }
                BasePte = CheckPte + 1;
            }
        }
    }

    //
    //
    // Calculate the offset to read into the file.
    //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
    //

    StartingOffset.QuadPart = MiStartingOffset (Subsection, BasePte);

    TempOffset = MiEndingOffset (Subsection);

    ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

    //
    // Remove pages to fill in the MDL.  This is done here as the
    // base PTE has been determined and can be used for virtual
    // aliasing checks.
    //

    EndPage = FirstMdlPage;
    CheckPte = BasePte;

    while (EndPage < Page) {
        if (Process == HYDRA_PROCESS) {
            PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
        }
        else if (Process == NULL) {
            PageColor = MI_GET_PAGE_COLOR_FROM_PTE (CheckPte);
        }
        else {
            PageColor = MI_PAGE_COLOR_PTE_PROCESS (CheckPte,
                                                   &Process->NextPageColor);
        }
        *EndPage = MiRemoveAnyPage (PageColor);

        EndPage += 1;
        CheckPte += 1;
    }

    if (Process == HYDRA_PROCESS) {
        PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
    }
    else if (Process == NULL) {
        PageColor = MI_GET_PAGE_COLOR_FROM_PTE (CheckPte);
    }
    else {
        PageColor = MI_PAGE_COLOR_PTE_PROCESS (CheckPte,
                                               &Process->NextPageColor);
    }

    //
    // Check to see if the read will go past the end of the file,
    // If so, correct the read size and get a zeroed page.
    //

    Prcb = KeGetCurrentPrcb ();
    InterlockedIncrement (&Prcb->MmPageReadIoCount);

    InterlockedExchangeAdd (&Prcb->MmPageReadCount,
                            (LONG) (ReadSize >> PAGE_SHIFT));

    if ((ControlArea->u.Flags.Image) &&
        (((UINT64)StartingOffset.QuadPart + ReadSize) > (UINT64)TempOffset.QuadPart)) {

        ASSERT ((ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart)
                > (ReadSize - PAGE_SIZE));

        ReadSize = (ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart);

        //
        // Round the offset to a 512-byte offset as this will help filesystems
        // optimize the transfer.  Note that filesystems will always zero fill
        // the remainder between VDL and the next 512-byte multiple and we have
        // already zeroed the whole page.
        //

        ReadSize = ((ReadSize + MMSECTOR_MASK) & ~MMSECTOR_MASK);

        PageFrameIndex = MiRemoveZeroPage (PageColor);
    }
    else {

        //
        // We are reading a complete page, no need to get a zeroed page.
        //

        PageFrameIndex = MiRemoveAnyPage (PageColor);
    }

    //
    // Increment the PFN reference count in the control area for
    // the subsection (the PFN lock is required to modify this field).
    //

    ControlArea->NumberOfPfnReferences += 1;
    *Page = PageFrameIndex;

    PageFrameIndex = *(FirstMdlPage + (PointerPte - BasePte));

    //
    // Get a page and put the PTE into the transition state with the
    // read-in-progress flag set.
    //

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Initialize MDL for request.
    //

    MmInitializeMdl (Mdl,
                     MiGetVirtualAddressMappedByPte (BasePte),
                     ReadSize);
    Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

#if DBG
    if (ReadSize > ((ClusterSize + 1) << PAGE_SHIFT)) {
        KeBugCheckEx (MEMORY_MANAGEMENT, 0x777,(ULONG_PTR)Mdl, (ULONG_PTR)Subsection,
                        (ULONG)TempOffset.LowPart);
    }
#endif //DBG

    MiInitializeReadInProgressPfn (Mdl,
                                   BasePte,
                                   &ReadBlockLocal->Event,
                                   MI_PROTOTYPE_WSINDEX);

    MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(ReadBlockLocal);

    ReadBlockLocal->ReadOffset = StartingOffset;
    ReadBlockLocal->FilePointer = ControlArea->FilePointer;
    ReadBlockLocal->BasePte = BasePte;
    ReadBlockLocal->Pfn = Pfn1;

    return STATUS_ISSUE_PAGING_IO;
}

NTSTATUS
MiWaitForInPageComplete (
    IN PMMPFN Pfn2,
    IN PMMPTE PointerPte,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPteContents,
    IN PMMINPAGE_SUPPORT InPageSupport,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    Waits for a page read to complete.

Arguments:

    Pfn - Supplies a pointer to the PFN element for the page being read.

    PointerPte - Supplies a pointer to the PTE that is in the transition
                 state.  This can be a prototype PTE address.

    FaultingAddress - Supplies the faulting address.

    PointerPteContents - Supplies the contents of the PTE before the
                         working set lock was released.

    InPageSupport - Supplies a pointer to the inpage support structure
                    for this read operation.

Return Value:

    Returns the status of the inpage operation.

    Note that the working set mutex and PFN lock are held upon return !!!

Environment:

    Kernel mode, APCs disabled.  Neither the working set lock nor
    the PFN lock may be held.

--*/

{
    PMMVAD ProtoVad;
    PMMPTE NewPointerPte;
    PMMPTE ProtoPte;
    PMMPFN Pfn1;
    PMMPFN Pfn;
    PULONG Va;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    ULONG Offset;
    ULONG Protection;
    PMDL Mdl;
    KIRQL OldIrql;
    NTSTATUS status;
    NTSTATUS status2;
    PEPROCESS Process;
    PETHREAD Thread;

    Thread = PsGetCurrentThread ();

    //
    // Wait for the I/O to complete.  Note that we can't wait for all
    // the objects simultaneously as other threads/processes could be
    // waiting for the same event.  The first thread which completes
    // the wait and gets the PFN lock may reuse the event for another
    // fault before this thread completes its wait.
    //

    KeWaitForSingleObject( &InPageSupport->Event,
                           WrPageIn,
                           KernelMode,
                           FALSE,
                           NULL);

    if (CurrentProcess == HYDRA_PROCESS) {
        LOCK_WORKING_SET (Thread, &MmSessionSpace->GlobalVirtualAddress->Vm);
    }
    else if (CurrentProcess == PREFETCH_PROCESS) {
        NOTHING;
    }
    else if (CurrentProcess != NULL) {
        LOCK_WS (Thread, CurrentProcess);
    }
    else {
        LOCK_SYSTEM_WS (Thread);
    }

    LOCK_PFN (OldIrql);

    ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

    //
    // Check to see if this is the first thread to complete the in-page
    // operation.
    //

    Pfn = InPageSupport->Pfn;
    if (Pfn2 != Pfn) {
        ASSERT (Pfn2->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        Pfn2->u3.e1.ReadInProgress = 0;
    }

    //
    // Another thread has already serviced the read, check the
    // io-error flag in the PFN database to ensure the in-page
    // was successful.
    //

    if (Pfn2->u4.InPageError == 1) {
        ASSERT (!NT_SUCCESS(Pfn2->u1.ReadStatus));

        if (MmIsRetryIoStatus(Pfn2->u1.ReadStatus)) {
            return STATUS_REFAULT;
        }
        return Pfn2->u1.ReadStatus;
    }

    if (InPageSupport->u1.e1.Completed == 0) {

        //
        // The ReadInProgress bit for the dummy page is constantly cleared
        // below as there are generally multiple inpage blocks pointing to
        // the same dummy page.
        //

        ASSERT ((Pfn->u3.e1.ReadInProgress == 1) ||
                (Pfn->PteAddress == MI_PF_DUMMY_PAGE_PTE));

        InPageSupport->u1.e1.Completed = 1;

        Mdl = &InPageSupport->Mdl;

        if (InPageSupport->u1.e1.PrefetchMdlHighBits != 0) {

            //
            // This is a prefetcher-issued read.
            //

            Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        ASSERT (Pfn->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        Pfn->u3.e1.ReadInProgress = 0;
        Pfn->u1.Event = NULL;

#if defined (_WIN64)
        //
        // Page directory and page table pages are never clustered,
        // ensure this is never violated as only one UsedPageTableEntries
        // is kept in the inpage support block.
        //

        if (InPageSupport->UsedPageTableEntries) {
            Page = (PPFN_NUMBER)(Mdl + 1);
            LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);
            ASSERT (Page == LastPage);
        }
#endif

        MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(Pfn, InPageSupport);

        //
        // Check the IO_STATUS_BLOCK to ensure the in-page completed successfully.
        //

        if (!NT_SUCCESS(InPageSupport->IoStatus.Status)) {

            if (InPageSupport->IoStatus.Status == STATUS_END_OF_FILE) {

                //
                // An attempt was made to read past the end of file
                // zero all the remaining bytes in the read.
                //

                if (Pfn->OriginalPte.u.Soft.Prototype == 0) {

                    //
                    // Something is very wrong for this to happen on a pagefile
                    // read.
                    //

                    KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                                  3,
                                  (ULONG_PTR) FaultingAddress,
                                  (ULONG_PTR) InPageSupport,
                                  0);
                }

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);

                while (Page <= LastPage) {
                    MiZeroPhysicalPage (*Page);

                    MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(MI_PFN_ELEMENT(*Page));

                    Page += 1;
                }

            }
            else {

                //
                // In page io error occurred.
                //

                status = InPageSupport->IoStatus.Status;
                status2 = InPageSupport->IoStatus.Status;

                if (status != STATUS_VERIFY_REQUIRED) {

                    LOGICAL Retry;

                    Retry = FALSE;
#if DBG
                    DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                                    "MM: inpage I/O error %X\n",
                                    InPageSupport->IoStatus.Status);
#endif

                    //
                    // If this page is for paged pool or for paged
                    // kernel code or page table pages, bugcheck.
                    //

                    if ((FaultingAddress > MM_HIGHEST_USER_ADDRESS) &&
                        (!MI_IS_SYSTEM_CACHE_ADDRESS(FaultingAddress))) {

                        if (MmIsRetryIoStatus(status)) {

                            if (MiInPageSinglePages == 0) {
                                MiInPageSinglePages = 30;
                            }

                            MiFaultRetries -= 1;
                            if (MiFaultRetries & MiFaultRetryMask) {
                                Retry = TRUE;
                            }
                        }

                        if (Retry == FALSE) {

                            ULONG_PTR PteContents;

                            //
                            // The prototype PTE resides in paged pool which may
                            // not be resident at this point.  Check first.
                            //

                            if (MiIsAddressValid (PointerPte, FALSE) == TRUE) {
                                PteContents = *(PULONG_PTR)PointerPte;
                            }
                            else {
                                PteContents = (ULONG_PTR)-1;
                            }

                            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                                          (ULONG_PTR)PointerPte,
                                          status,
                                          (ULONG_PTR)FaultingAddress,
                                          PteContents);
                        }
                        status2 = STATUS_REFAULT;
                    }
                    else {

                        if (MmIsRetryIoStatus(status)) {

                            if (MiInPageSinglePages == 0) {
                                MiInPageSinglePages = 30;
                            }

                            MiUserFaultRetries -= 1;
                            if (MiUserFaultRetries & MiUserFaultRetryMask) {
                                Retry = TRUE;
                            }
                        }

                        if (Retry == TRUE) {
                            status2 = STATUS_REFAULT;
                        }
                    }
                }

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);

#if DBG
                Process = PsGetCurrentProcess ();
#endif
                while (Page <= LastPage) {
                    Pfn1 = MI_PFN_ELEMENT (*Page);
                    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
                    Pfn1->u4.InPageError = 1;
                    Pfn1->u1.ReadStatus = status;

#if DBG
                    Va = (PULONG)MiMapPageInHyperSpaceAtDpc (Process, *Page);
                    RtlFillMemoryUlong (Va, PAGE_SIZE, 0x50444142);
                    MiUnmapPageInHyperSpaceFromDpc (Process, Va);
#endif
                    Page += 1;
                }

                return status2;
            }
        }
        else {

            MiFaultRetries = 0;
            MiUserFaultRetries = 0;

            if (InPageSupport->IoStatus.Information != Mdl->ByteCount) {

                ASSERT (InPageSupport->IoStatus.Information != 0);

                //
                // Less than a full page was read - zero the remainder
                // of the page if the page is filesystem backed (this can
                // be legitimate as we are reading whole pages and the
                // end of file may not be page aligned.
                //
                // However, if the page is pagefile backed, then this
                // underreading is a fatal driver error - and it would
                // would be a mistake to zero trailing portions of pages
                // to hide that.
                //

                if (Pfn->OriginalPte.u.Soft.Prototype == 0) {
                    KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                                  4,
                                  (ULONG_PTR) FaultingAddress,
                                  (ULONG_PTR) InPageSupport,
                                  0);
                }

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);
                Page += ((InPageSupport->IoStatus.Information - 1) >> PAGE_SHIFT);

                Offset = BYTE_OFFSET (InPageSupport->IoStatus.Information);

                if (Offset != 0) {
                    Process = PsGetCurrentProcess ();
                    Va = (PULONG)((PCHAR)MiMapPageInHyperSpaceAtDpc (Process, *Page)
                                + Offset);

                    RtlZeroMemory (Va, PAGE_SIZE - Offset);
                    MiUnmapPageInHyperSpaceFromDpc (Process, Va);
                }

                //
                // Zero any remaining pages within the MDL.
                //

                Page += 1;

                while (Page <= LastPage) {
                    MiZeroPhysicalPage (*Page);
                    Page += 1;
                }
            }

            //
            // If any filesystem return non-zeroed data for any slop
            // after the VDL but before the next 512-byte offset then this
            // non-zeroed data will overwrite our zeroed page.  This would
            // need to be checked for and cleaned up here.  Note that the only
            // reason Mm even rounds the MDL request up to a 512-byte offset
            // is so filesystems receive a transfer they can handle optimally,
            // but any transfer size has always worked (although non-512 byte
            // multiples end up getting posted by the filesystem).
            //
        }
    }

    //
    // Prefetcher-issued reads only put prototype PTEs into transition and
    // never fill actual hardware PTEs so these can be returned now.
    //

    if (CurrentProcess == PREFETCH_PROCESS) {
        return STATUS_SUCCESS;
    }

    //
    // Check to see if the faulting PTE has changed.
    //

    NewPointerPte = MiFindActualFaultingPte (FaultingAddress);

    //
    // If this PTE is in prototype PTE format, make the pointer to the
    // PTE point to the prototype PTE.
    //

    if (NewPointerPte == NULL) {
        return STATUS_PTE_CHANGED;
    }

    if (NewPointerPte != PointerPte) {

        //
        // Check to make sure the NewPointerPte is not a prototype PTE
        // which refers to the page being made valid.
        //

        if (NewPointerPte->u.Soft.Prototype == 1) {
            if (NewPointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

                ProtoPte = MiCheckVirtualAddress (FaultingAddress,
                                                  &Protection,
                                                  &ProtoVad);

            }
            else {
                ProtoPte = MiPteToProto (NewPointerPte);
            }

            //
            // Make sure the prototype PTE refers to the PTE made valid.
            //

            if (ProtoPte != PointerPte) {
                return STATUS_PTE_CHANGED;
            }

            //
            // If the only difference is the owner mask, everything is okay.
            //

            if (ProtoPte->u.Long != PointerPteContents->u.Long) {
                return STATUS_PTE_CHANGED;
            }
        }
        else {
            return STATUS_PTE_CHANGED;
        }
    }
    else {

        if (NewPointerPte->u.Long != PointerPteContents->u.Long) {
            return STATUS_PTE_CHANGED;
        }
    }
    return STATUS_SUCCESS;
}

PMMPTE
MiFindActualFaultingPte (
    IN PVOID FaultingAddress
    )

/*++

Routine Description:

    This routine locates the actual PTE which must be made resident in order
    to complete this fault.  Note that for certain cases multiple faults
    are required to make the final page resident.

Arguments:

    FaultingAddress - Supplies the virtual address which caused the fault.

Return Value:

    The PTE to be made valid to finish the fault, NULL if the fault should
    be retried.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    PMMVAD ProtoVad;
    PMMPTE ProtoPteAddress;
    PMMPTE PointerPte;
    PMMPTE PointerFaultingPte;
    ULONG Protection;

    if (MI_IS_PHYSICAL_ADDRESS (FaultingAddress)) {
        return NULL;
    }

#if (_MI_PAGING_LEVELS >= 4)

    PointerPte = MiGetPxeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page directory parent page is not valid.
        //

        return PointerPte;
    }

#endif

#if (_MI_PAGING_LEVELS >= 3)

    PointerPte = MiGetPpeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page directory page is not valid.
        //

        return PointerPte;
    }

#endif

    PointerPte = MiGetPdeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page table page is not valid.
        //

        return PointerPte;
    }

    PointerPte = MiGetPteAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 1) {

        //
        // Page is already valid, no need to fault it in.
        //

        return NULL;
    }

    if (PointerPte->u.Soft.Prototype == 0) {

        //
        // Page is not a prototype PTE, make this PTE valid.
        //

        return PointerPte;
    }

    //
    // Check to see if the PTE which maps the prototype PTE is valid.
    //

    if (PointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

        //
        // Protection is here, PTE must be located in VAD.
        //

        ProtoPteAddress = MiCheckVirtualAddress (FaultingAddress,
                                                 &Protection,
                                                 &ProtoVad);

        if (ProtoPteAddress == NULL) {

            //
            // No prototype PTE means another thread has deleted the VAD while
            // this thread waited for the inpage to complete.  Certainly NULL
            // must be returned so a stale PTE is not modified - the instruction
            // will then be reexecuted and an access violation delivered.
            //

            return NULL;
        }

    }
    else {

        //
        // Protection is in ProtoPte.
        //

        ProtoPteAddress = MiPteToProto (PointerPte);
    }

    PointerFaultingPte = MiFindActualFaultingPte (ProtoPteAddress);

    if (PointerFaultingPte == NULL) {
        return PointerPte;
    }

    return PointerFaultingPte;
}

PMMPTE
MiCheckVirtualAddress (
    IN PVOID VirtualAddress,
    OUT PULONG ProtectCode,
    OUT PMMVAD *VadOut
    )

/*++

Routine Description:

    This function examines the virtual address descriptors to see
    if the specified virtual address is contained within any of
    the descriptors.  If a virtual address descriptor is found
    which contains the specified virtual address, a PTE is built
    from information within the virtual address descriptor and
    returned to the caller.

Arguments:

    VirtualAddress - Supplies the virtual address to locate within
                     a virtual address descriptor.

    ProtectCode - Supplies a pointer to a variable that will receive the
                  protection to insert the actual PTE.

    Vad - Supplies a pointer to a variable that will receive the pointer
          to the VAD that was used for validation (or NULL if no VAD was
          used).

Return Value:

    Returns the PTE which corresponds to the supplied virtual address.
    If no virtual address descriptor is found, a zero PTE is returned.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    PMMVAD Vad;
    PMMPTE PointerPte;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;

    *VadOut = NULL;

    if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {

        if (PAGE_ALIGN(VirtualAddress) == (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // This is the page that is double mapped between
            // user mode and kernel mode.  Map it as read only.
            //

            *ProtectCode = MM_READONLY;

            return MmSharedUserDataPte;
        }

        Vad = MiLocateAddress (VirtualAddress);

        if (Vad == NULL) {
            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        //
        // A virtual address descriptor which contains the virtual address
        // has been located.  Build the PTE from the information within
        // the virtual address descriptor.
        //

        if (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory) {

            //
            // This is definitely a banked section.
            //

            MiHandleBankedSection (VirtualAddress, Vad);
            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        if (Vad->u.VadFlags.PrivateMemory == 1) {

            //
            // This is a private region of memory.  Check to make
            // sure the virtual address has been committed.  Note that
            // addresses are dense from the bottom up.
            //

            if (Vad->u.VadFlags.VadType == VadAwe) {

                //
                // These mappings only fault if the access is bad.
                //

                *ProtectCode = MM_NOACCESS;
                return NULL;
            }

            if (Vad->u.VadFlags.MemCommit == 1) {
                *ProtectCode = MI_GET_PROTECTION_FROM_VAD(Vad);
                return NULL;
            }

            //
            // The address is reserved but not committed.
            //

            *ProtectCode = MM_NOACCESS;
            return NULL;
        }
        else {

            //
            // This virtual address descriptor refers to a
            // section, calculate the address of the prototype PTE
            // and construct a pointer to the PTE.
            //
            //*******************************************************
            //*******************************************************
            // well here's an interesting problem, how do we know
            // how to set the attributes on the PTE we are creating
            // when we can't look at the prototype PTE without
            // potentially incurring a page fault.  In this case
            // PteTemplate would be zero.
            //*******************************************************
            //*******************************************************
            //

            if (Vad->u.VadFlags.VadType == VadImageMap) {

                //
                // PTE and proto PTEs have the same protection for images.
                //

                *ProtectCode = MM_UNKNOWN_PROTECTION;
            }
            else {
                *ProtectCode = MI_GET_PROTECTION_FROM_VAD(Vad);

                //
                // Opportunistic clustering can use the identical protections
                // so give our caller the green light.
                //

                if (Vad->u2.VadFlags2.ExtendableFile == 0) {
                    *VadOut = Vad;
                }
            }
            PointerPte = (PMMPTE)MiGetProtoPteAddress(Vad,
                                                MI_VA_TO_VPN (VirtualAddress));
            if (PointerPte == NULL) {
                *ProtectCode = MM_NOACCESS;
            }
            if (Vad->u2.VadFlags2.ExtendableFile) {

                //
                // Make sure the data has been committed.
                //

                if ((MI_VA_TO_VPN (VirtualAddress) - Vad->StartingVpn) >
                    (ULONG_PTR)((((PMMVAD_LONG)Vad)->u4.ExtendedInfo->CommittedSize - 1)
                                                 >> PAGE_SHIFT)) {
                    *ProtectCode = MM_NOACCESS;
                }
            }
            return PointerPte;
        }

    }
    else if (MI_IS_PAGE_TABLE_ADDRESS(VirtualAddress)) {

        //
        // The virtual address is within the space occupied by PDEs,
        // make the PDE valid.
        //

        if (((PMMPTE)VirtualAddress >= MiGetPteAddress (MM_PAGED_POOL_START)) &&
            ((PMMPTE)VirtualAddress <= MmPagedPoolInfo.LastPteForPagedPool)) {

            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        *ProtectCode = MM_READWRITE;
        return NULL;
    }
    else if (MI_IS_SESSION_ADDRESS (VirtualAddress) == TRUE) {

        //
        // See if the session space address is copy on write.
        //

        MM_SESSION_SPACE_WS_LOCK_ASSERT ();

        PointerPte = NULL;
        *ProtectCode = MM_NOACCESS;

        NextEntry = MmSessionSpace->ImageList.Flink;

        while (NextEntry != &MmSessionSpace->ImageList) {

            Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

            if ((VirtualAddress >= Image->Address) && (VirtualAddress <= Image->LastAddress)) {
                PointerPte = Image->PrototypePtes +
                    (((PCHAR)VirtualAddress - (PCHAR)Image->Address) >> PAGE_SHIFT);
                *ProtectCode = MM_EXECUTE_WRITECOPY;
                break;
            }

            NextEntry = NextEntry->Flink;
        }

        return PointerPte;
    }

    //
    // Address is in system space.
    //

    *ProtectCode = MM_NOACCESS;
    return NULL;
}

#if (_MI_PAGING_LEVELS < 3)

NTSTATUS
FASTCALL
MiCheckPdeForPagedPool (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function copies the Page Table Entry for the corresponding
    virtual address from the system process's page directory.

    This allows page table pages to be lazily evaluated for things
    like paged pool and per-session mappings.

Arguments:

    VirtualAddress - Supplies the virtual address in question.

Return Value:

    Either success or access violation.

Environment:

    Kernel mode, DISPATCH level or below.

--*/

{
    PMMPTE PointerPde;
    NTSTATUS status;

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) == TRUE) {

         //
         // Virtual address in the session space range.
         //

         return MiCheckPdeForSessionSpace (VirtualAddress);
    }

    if (MI_IS_SESSION_PTE (VirtualAddress) == TRUE) {

         //
         // PTE for the session space range.
         //

         return MiCheckPdeForSessionSpace (VirtualAddress);
    }

    status = STATUS_SUCCESS;

    if (MI_IS_KERNEL_PAGE_TABLE_ADDRESS(VirtualAddress)) {

        //
        // PTE for paged pool.
        //

        PointerPde = MiGetPteAddress (VirtualAddress);
        status = STATUS_WAIT_1;
    }
    else if (VirtualAddress < MmSystemRangeStart) {

        return STATUS_ACCESS_VIOLATION;

    }
    else {

        //
        // Virtual address in paged pool range.
        //

        PointerPde = MiGetPdeAddress (VirtualAddress);
    }

    //
    // Locate the PDE for this page and make it valid.
    //

    if (PointerPde->u.Hard.Valid == 0) {

        //
        // The MI_WRITE_VALID_PTE macro cannot be used here because
        // its ASSERTs could mistakenly fire as multiple processors
        // may execute the instruction below without synchronization.
        //

        InterlockedExchangePte (PointerPde,
                        MmSystemPagePtes [((ULONG_PTR)PointerPde &
                               (PD_PER_SYSTEM * (sizeof(MMPTE) * PDE_PER_PAGE) - 1)) / sizeof(MMPTE)].u.Long);
    }
    return status;
}


NTSTATUS
FASTCALL
MiCheckPdeForSessionSpace (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function copies the Page Table Entry for the corresponding
    session virtual address from the current session's data structures.

    This allows page table pages to be lazily evaluated for session mappings.
    The caller must check for the current process having a session space.

Arguments:

    VirtualAddress - Supplies the virtual address in question.

Return Value:

    STATUS_WAIT_1 - The mapping has been made valid, retry the fault.

    STATUS_SUCCESS - Did not handle the fault, continue further processing.

    !STATUS_SUCCESS - An access violation has occurred - raise an exception.

Environment:

    Kernel mode, DISPATCH level or below.

--*/

{
    MMPTE TempPde;
    PMMPTE PointerPde;
    PVOID  SessionVirtualAddress;
    ULONG  Index;

    //
    // First check whether the reference was to a page table page which maps
    // session space.  If so, the PDE is retrieved from the session space
    // data structure and made valid.
    //

    if (MI_IS_SESSION_PTE (VirtualAddress) == TRUE) {

        //
        // Verify that the current process has a session space.
        //

        PointerPde = MiGetPdeAddress (MmSessionSpace);

        if (PointerPde->u.Hard.Valid == 0) {

#if DBG
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                "MiCheckPdeForSessionSpace: No current session for PTE %p\n",
                VirtualAddress);

            DbgBreakPoint ();
#endif
            return STATUS_ACCESS_VIOLATION;
        }

        SessionVirtualAddress = MiGetVirtualAddressMappedByPte ((PMMPTE) VirtualAddress);

        PointerPde = MiGetPteAddress (VirtualAddress);

        if (PointerPde->u.Hard.Valid == 1) {

            //
            // The PDE is already valid - another thread must have
            // won the race.  Just return.
            //

            return STATUS_WAIT_1;
        }

        //
        // Calculate the session space PDE index and load the
        // PDE from the session space table for this session.
        //

        Index = MiGetPdeSessionIndex (SessionVirtualAddress);

        TempPde.u.Long = MmSessionSpace->PageTables[Index].u.Long;

        if (TempPde.u.Hard.Valid == 1) {

            //
            // The MI_WRITE_VALID_PTE macro cannot be used here because
            // its ASSERTs could mistakenly fire as multiple processors
            // may execute the instruction below without synchronization.
            //

            InterlockedExchangePte (PointerPde, TempPde.u.Long);
            return STATUS_WAIT_1;
        }

#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MiCheckPdeForSessionSpace: No Session PDE for PTE %p, %p\n",
            PointerPde->u.Long, SessionVirtualAddress);

        DbgBreakPoint ();
#endif
        return STATUS_ACCESS_VIOLATION;
    }

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) == FALSE) {

        //
        // Not a session space fault - tell the caller to try other handlers.
        //

        return STATUS_SUCCESS;
    }

    //
    // Handle PDE faults for references in the session space.
    // Verify that the current process has a session space.
    //

    PointerPde = MiGetPdeAddress (MmSessionSpace);

    if (PointerPde->u.Hard.Valid == 0) {

#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MiCheckPdeForSessionSpace: No current session for VA %p\n",
            VirtualAddress);

        DbgBreakPoint ();
#endif
        return STATUS_ACCESS_VIOLATION;
    }

    PointerPde = MiGetPdeAddress (VirtualAddress);

    if (PointerPde->u.Hard.Valid == 0) {

        //
        // Calculate the session space PDE index and load the
        // PDE from the session space table for this session.
        //

        Index = MiGetPdeSessionIndex (VirtualAddress);

        PointerPde->u.Long = MmSessionSpace->PageTables[Index].u.Long;

        if (PointerPde->u.Hard.Valid == 1) {
            return STATUS_WAIT_1;
        }

#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MiCheckPdeForSessionSpace: No Session PDE for VA %p, %p\n",
            PointerPde->u.Long, VirtualAddress);

        DbgBreakPoint ();
#endif

        return STATUS_ACCESS_VIOLATION;
    }

    //
    // Tell the caller to continue with other fault handlers.
    //

    return STATUS_SUCCESS;
}
#endif


VOID
MiInitializePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN ULONG ModifiedState
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state.

Arguments:

    PageFrameIndex - Supplies the page frame number to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the page fault.

    ModifiedState - Supplies the state to set the modified field in the PFN
                    element for this page, either 0 or 1.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MM_PROTECTION_MASK Protection;
    MI_PFN_CACHE_ATTRIBUTE NewCacheAttribute;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;
    NewCacheAttribute = MiCached;

    //
    // If the PTE is currently valid, an address space is being built,
    // just make the original PTE demand zero.
    //

    if (PointerPte->u.Hard.Valid == 1) {

        //
        // The only time this should happen is for the initial process pages
        // created (and initialized) out of context.  They must be in order
        // to attach and then initialize the process, and
        // MmCreateProcessAddressSpace assumes responsibility for detecting
        // and preventing TB attribute conflicts for this case, so we don't
        // need to here.
        //

        ASSERT (PsGetCurrentProcess()->Vm.WorkingSetSize == 0);

        ASSERT (!MI_IS_WRITE_COMBINE_ENABLED (PointerPte));
        ASSERT (!MI_IS_CACHING_DISABLED (PointerPte));
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

#if defined (_AMD64_)
        if (PointerPte->u.Hard.NoExecute == 0) {
            Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
        }
#endif

#if defined(_X86PAE_)
        if (MmPaeMask != 0) {
            if ((PointerPte->u.Long & MmPaeMask) == 0) {
                Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
            }
        }
#endif
    }
    else {

        Pfn1->OriginalPte = *PointerPte;

        ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                    (Pfn1->OriginalPte.u.Soft.Transition == 1)));

        Protection = (MM_PROTECTION_MASK) PointerPte->u.Soft.Protection;

        if (MI_IS_WRITECOMBINE (Protection)) {
            NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiWriteCombined, 0);
        }
        else if (MI_IS_NOCACHE (Protection)) {
            NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiNonCached, 0);
        }

        if (Pfn1->u3.e1.CacheAttribute != NewCacheAttribute) {
            MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageFrameIndex,
                                                         NewCacheAttribute);
            Pfn1->u3.e1.CacheAttribute = NewCacheAttribute;
        }
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount += 1;

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    if (ModifiedState == 1) {
        MI_SET_MODIFIED (Pfn1, 1, 0xB);
    }
    else {
        MI_SET_MODIFIED (Pfn1, 0, 0x26);
    }

#if defined (_WIN64)
    Pfn1->UsedPageTableEntries = 0;
#endif

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress (PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }
    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    ASSERT (PteFramePage != 0);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Increment the share count for the page table page containing
    // this PTE.
    //

    Pfn2 = MI_PFN_ELEMENT (PteFramePage);

    Pfn2->u2.ShareCount += 1;

    return;
}

VOID
MiInitializePfnAndMakePteValid (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN MMPTE NewPteContents
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state.  After initializing the PFN, the PTE is
    then immediately made valid.  Note the order is important because
    the TB may need to be flushed to avoid a prior stale system PTE
    mapping (these are lazy-flushed) from causing a TB attribute conflict.

Arguments:

    PageFrameIndex - Supplies the page frame number to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the page fault.

    NewPteContents - Supplies the contents to insert in the PTE.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MI_PFN_CACHE_ATTRIBUTE NewCacheAttribute;

    MM_PFN_LOCK_ASSERT();

    ASSERT (PointerPte->u.Hard.Valid == 0);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;

    //
    // An address space is being built, just make the original PTE demand zero.
    //

    Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

#if defined (_AMD64_)
    if (NewPteContents.u.Hard.NoExecute == 0) {
        Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
    }
#endif

#if defined(_X86PAE_)
    if (MmPaeMask != 0) {
        if ((NewPteContents.u.Long & MmPaeMask) == 0) {
            Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
        }
    }
#endif

    NewCacheAttribute = MiCached;

    if (MI_IS_WRITE_COMBINE_ENABLED (&NewPteContents)) {
        Pfn1->OriginalPte.u.Soft.Protection |= MM_WRITECOMBINE;
        NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiWriteCombined, 0);
    }
    else if (MI_IS_CACHING_DISABLED (&NewPteContents)) {
        Pfn1->OriginalPte.u.Soft.Protection |= MM_NOCACHE;
        NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiNonCached, 0);
    }

    //
    // Flush the TB if the attribute for this PFN is changing.
    //

    if (Pfn1->u3.e1.CacheAttribute != NewCacheAttribute) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageFrameIndex,
                                                     NewCacheAttribute);
        Pfn1->u3.e1.CacheAttribute = NewCacheAttribute;
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount += 1;

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    MI_SET_MODIFIED (Pfn1, 1, 0xB);

#if defined (_WIN64)
    Pfn1->UsedPageTableEntries = 0;
#endif

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress (PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }
    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    ASSERT (PteFramePage != 0);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Increment the share count for the page table page containing
    // this PTE.
    //

    Pfn2 = MI_PFN_ELEMENT (PteFramePage);

    Pfn2->u2.ShareCount += 1;

    MI_WRITE_VALID_PTE (PointerPte, NewPteContents);

    return;
}

VOID
MiInitializeReadInProgressSinglePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition / read-in-progress state for an in-page operation.

Arguments:

    PageFrameIndex - Supplies the page frame to initialize.

    BasePte - Supplies the pointer to the PTE for the page frame.

    Event - Supplies the event which is to be set when the I/O operation
            completes.

    WorkingSetIndex - Supplies the working set index flag, a value of
                      -1 indicates no WSLE is required because
                      this is a prototype PTE.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;
    MM_PROTECTION_MASK Protection;
    MI_PFN_CACHE_ATTRIBUTE NewCacheAttribute;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->u1.Event = Event;
    Pfn1->PteAddress = BasePte;
    Pfn1->OriginalPte = *BasePte;
    if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {
        Pfn1->u3.e1.PrototypePte = 1;
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

    Pfn1->u2.ShareCount = 0;
    Pfn1->u3.e1.ReadInProgress = 1;

    Protection = (MM_PROTECTION_MASK) BasePte->u.Soft.Protection;

    NewCacheAttribute = MiCached;

    if (MI_IS_WRITECOMBINE (Protection)) {
        NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiWriteCombined, 0);
    }
    else if (MI_IS_NOCACHE (Protection)) {
        NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiNonCached, 0);
    }

    if (Pfn1->u3.e1.CacheAttribute != NewCacheAttribute) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageFrameIndex,
                                                     NewCacheAttribute);
        Pfn1->u3.e1.CacheAttribute = NewCacheAttribute;
    }

    Pfn1->u4.InPageError = 0;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress (BasePte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (BasePte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)BasePte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(BasePte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Put the PTE into the transition state, no cache flush needed as
    // PTE is still not valid.
    //

    MI_MAKE_TRANSITION_PTE (TempPte,
                            PageFrameIndex,
                            BasePte->u.Soft.Protection,
                            BasePte);

    MI_WRITE_INVALID_PTE (BasePte, TempPte);

    //
    // Increment the share count for the page table page containing
    // this PTE as the PTE just went into the transition state.
    //

    ASSERT (PteFramePage != 0);

    Pfn1 = MI_PFN_ELEMENT (PteFramePage);
    Pfn1->u2.ShareCount += 1;

    return;
}

VOID
MiInitializeReadInProgressPfn (
    IN PMDL Mdl,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition / read-in-progress state for an in-page operation.


Arguments:

    Mdl - Supplies a pointer to the MDL.

    BasePte - Supplies the pointer to the PTE which the first page in
              the MDL maps.

    Event - Supplies the event which is to be set when the I/O operation
            completes.

    WorkingSetIndex - Supplies the working set index flag, a value of
                      -1 indicates no WSLE is required because
                      this is a prototype PTE.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;
    LONG NumberOfBytes;
    PPFN_NUMBER Page;
    MM_PROTECTION_MASK Protection;
    MI_PFN_CACHE_ATTRIBUTE NewCacheAttribute;
    LOGICAL TbFlushNeeded;

    TbFlushNeeded = FALSE;

    PteFramePage = 0;
    PteFramePointer = 0;
    SATISFY_OVERZEALOUS_COMPILER (NewCacheAttribute = MiCached);

    MM_PFN_LOCK_ASSERT();

    Page = (PPFN_NUMBER)(Mdl + 1);

    NumberOfBytes = Mdl->ByteCount;

    while (NumberOfBytes > 0) {

        Pfn1 = MI_PFN_ELEMENT (*Page);
        Pfn1->u1.Event = Event;
        Pfn1->PteAddress = BasePte;
        Pfn1->OriginalPte = *BasePte;
        if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {
            Pfn1->u3.e1.PrototypePte = 1;
        }
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

        Pfn1->u2.ShareCount = 0;
        Pfn1->u3.e1.ReadInProgress = 1;

        Protection = (MM_PROTECTION_MASK) BasePte->u.Soft.Protection;

        NewCacheAttribute = MiCached;

        if (MI_IS_WRITECOMBINE (Protection)) {
            NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiWriteCombined, 0);
        }
        else if (MI_IS_NOCACHE (Protection)) {
            NewCacheAttribute = MI_TRANSLATE_CACHETYPE (MiNonCached, 0);
        }

        if (Pfn1->u3.e1.CacheAttribute != NewCacheAttribute) {
            TbFlushNeeded = TRUE;
            Pfn1->u3.e1.CacheAttribute = NewCacheAttribute;
        }

        Pfn1->u4.InPageError = 0;

        if ((PteFramePage == 0) || (MiIsPteOnPdeBoundary (BasePte))) {

            //
            // Determine the page frame number of the page table page which
            // contains this PTE.
            //

            if (PteFramePage == 0) {
                PteFramePointer = MiGetPteAddress (BasePte);
            }
            else {
                PteFramePointer += 1;
                ASSERT (PteFramePointer == MiGetPteAddress (BasePte));
            }

            if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (BasePte))) {
#endif
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                                  0x61940,
                                  (ULONG_PTR)BasePte,
                                  (ULONG_PTR)PteFramePointer->u.Long,
                                  (ULONG_PTR)MiGetVirtualAddressMappedByPte(BasePte));
#if (_MI_PAGING_LEVELS < 3)
                }
#endif
            }

            PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
            ASSERT (PteFramePage != 0);
        }

        Pfn1->u4.PteFrame = PteFramePage;

        //
        // Put the PTE into the transition state, no TB flush needed as the
        // existing PTE is not valid.
        //

        MI_MAKE_TRANSITION_PTE (TempPte,
                                *Page,
                                BasePte->u.Soft.Protection,
                                BasePte);

        MI_WRITE_INVALID_PTE (BasePte, TempPte);

        //
        // Increment the share count for the page table page containing
        // this PTE as the PTE just went into the transition state.
        //

        ASSERT (PteFramePage != 0);
        Pfn2 = MI_PFN_ELEMENT (PteFramePage);
        Pfn2->u2.ShareCount += 1;

        NumberOfBytes -= PAGE_SIZE;
        Page += 1;
        BasePte += 1;
    }

    if (TbFlushNeeded == TRUE) {
        MI_FLUSH_ENTIRE_TB_FOR_ATTRIBUTE_CHANGE (NewCacheAttribute);
    }

    return;
}

VOID
MiInitializeTransitionPfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition state.  Main use is by MapImageFile to make the
    page which contains the image header transition in the
    prototype PTEs.

Arguments:

    PageFrameIndex - Supplies the page frame index to be initialized.

    PointerPte - Supplies an invalid, non-transition PTE to initialize.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;

    MM_PFN_LOCK_ASSERT();
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->u1.Event = NULL;
    Pfn1->PteAddress = PointerPte;
    Pfn1->OriginalPte = *PointerPte;
    ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
              (Pfn1->OriginalPte.u.Soft.Transition == 1)));

    //
    // Don't change the reference count (it should already be 1).
    //

    Pfn1->u2.ShareCount = 0;

    //
    // No WSLE is required because this is a prototype PTE.
    //

    Pfn1->u3.e1.PrototypePte = 1;

    Pfn1->u3.e1.PageLocation = TransitionPage;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress (PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Put the PTE into the transition state, no cache flush needed as
    // PTE is still not valid.
    //

    MI_MAKE_TRANSITION_PTE (TempPte,
                            PageFrameIndex,
                            PointerPte->u.Soft.Protection,
                            PointerPte);

    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

    //
    // Increment the share count for the page table page containing
    // this PTE as the PTE just went into the transition state.
    //

    Pfn2 = MI_PFN_ELEMENT (PteFramePage);
    ASSERT (PteFramePage != 0);
    Pfn2->u2.ShareCount += 1;

    return;
}

VOID
MiInitializeCopyOnWritePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMWSL WorkingSetList
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state for a copy on write operation.

    In this case the page table page which contains the PTE has
    the proper ShareCount.

Arguments:

    PageFrameIndex - Supplies the page frame number to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    WorkingSetIndex - Supplies the working set index for the corresponding
                      virtual address.

    WorkingSetList - Supplies the relevant working set list.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set pushlock AND PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMWSLE Wsle;
    PMMPTE PteFramePointer;
    PFN_NUMBER OldPageFrameIndex;
    PMMPFN OldPfn;
    PFN_NUMBER PteFramePage;
    PVOID VirtualAddress;
    MM_PROTECTION_MASK ProtectionMask;

    ASSERT (PointerPte->u.Hard.Valid == 1);
    OldPageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    OldPfn = MI_PFN_ELEMENT (OldPageFrameIndex);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;

    //
    // Get the protection for the page.
    //

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    Pfn1->OriginalPte.u.Long = 0;

    Wsle = &WorkingSetList->Wsle[WorkingSetIndex];

    ProtectionMask = (MM_PROTECTION_MASK) Wsle->u1.e1.Protection;

    if (ProtectionMask == MM_ZERO_ACCESS) {
        ProtectionMask = MI_GET_PROTECTION_FROM_SOFT_PTE (&OldPfn->OriginalPte);
    }

    Pfn1->OriginalPte.u.Soft.Protection =
            MI_MAKE_PROTECT_NOT_WRITE_COPY (ProtectionMask);

    Wsle->u1.e1.Protection = (MM_PROTECTION_MASK) Pfn1->OriginalPte.u.Soft.Protection;

    //
    // Now initialize the rest of the new PFN entry.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount += 1;
    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    if (Pfn1->u3.e1.CacheAttribute != OldPfn->u3.e1.CacheAttribute) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageFrameIndex,
                                                     OldPfn->u3.e1.CacheAttribute);
        Pfn1->u3.e1.CacheAttribute = OldPfn->u3.e1.CacheAttribute;
    }

    Pfn1->u1.WsIndex = WorkingSetIndex;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress (PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    ASSERT (PteFramePage != 0);

    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Set the modified flag in the PFN database as we are writing
    // into this page and the dirty bit is already set in the PTE.
    //

    MI_SET_MODIFIED (Pfn1, 1, 0xC);

    return;
}

BOOLEAN
MiIsAddressValid (
    IN PVOID VirtualAddress,
    IN LOGICAL UseForceIfPossible
    )

/*++

Routine Description:

    For a given virtual address this function returns TRUE if no page fault
    will occur for a read operation on the address, FALSE otherwise.

    Note that after this routine was called, if appropriate locks are not
    held, a non-faulting address could fault.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

    UseForceIfPossible - Supplies TRUE if the address should be forced valid
                         if possible.

Return Value:

    TRUE if no page fault would be generated reading the virtual address,
    FALSE otherwise.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;
    UNREFERENCED_PARAMETER (UseForceIfPossible);

#if defined (_AMD64_)

    //
    // If this is within the physical addressing range, just return TRUE.
    //

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {

        PFN_NUMBER PageFrameIndex;

        //
        // Only bound with MmHighestPhysicalPage once Mm has initialized.
        //

        if (MmHighestPhysicalPage != 0) {

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN(VirtualAddress);

            if (PageFrameIndex > MmHighestPhysicalPage) {
                return FALSE;
            }
        }

        return TRUE;
    }

#endif

    //
    // If the address is not canonical then return FALSE as the caller (which
    // may be the kernel debugger) is not expecting to get an unimplemented
    // address bit fault.
    //

    if (MI_RESERVED_BITS_CANONICAL(VirtualAddress) == FALSE) {
        return FALSE;
    }

#if (_MI_PAGING_LEVELS >= 4)
    PointerPte = MiGetPxeAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }
#endif

#if (_MI_PAGING_LEVELS >= 3)
    PointerPte = MiGetPpeAddress (VirtualAddress);

    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }
#endif

    PointerPte = MiGetPdeAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }

    if (MI_PDE_MAPS_LARGE_PAGE (PointerPte)) {
        return TRUE;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }

    //
    // Make sure we're not treating a page directory as a page table here for
    // the case where the page directory is mapping a large page.  This is
    // because the large page bit is valid in PDE formats, but reserved in
    // PTE formats and will cause a trap.  A virtual address like c0200000 (on
    // x86) triggers this case.
    //

    if (MI_PDE_MAPS_LARGE_PAGE (PointerPte)) {
        return FALSE;
    }

    return TRUE;
}

BOOLEAN
MmIsAddressValid (
    __in PVOID VirtualAddress
    )

/*++

Routine Description:

    For a given virtual address this function returns TRUE if no page fault
    will occur for a read operation on the address, FALSE otherwise.

    Note that after this routine was called, if appropriate locks are not
    held, a non-faulting address could fault.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if no page fault would be generated reading the virtual address,
    FALSE otherwise.

Environment:

    Kernel mode.

--*/

{
    return MiIsAddressValid (VirtualAddress, FALSE);
}

VOID
MiInitializePfnForOtherProcess (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN PFN_NUMBER ContainingPageFrame
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state with the dirty bit on in the PTE and
    the PFN database marked as modified.

    As this PTE is not visible from the current process, the containing
    page frame must be supplied at the PTE contents field for the
    PFN database element are set to demand zero.

Arguments:

    PageFrameIndex - Supplies the page frame number of which to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    ContainingPageFrame - Supplies the page frame number of the page
                          table page which contains this PTE.
                          If the ContainingPageFrame is 0, then
                          the ShareCount for the
                          containing page is not incremented.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;

    //
    // Note that pages allocated this way are for the kernel and thus
    // never have split permissions in the PTE or the PFN.
    //

    Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount = 1;

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    //
    // Set the page attribute to cached even though it isn't really mapped
    // into a TB entry yet - it will be when the I/O completes and in the
    // future, may get paged in and out multiple times and will be marked
    // as cached in those transactions also.  If in fact the driver stack
    // wants to map it some other way for the transfer, the correct mapping
    // will get used regardless.
    //

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_CACHED_ATTRIBUTE ();
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

    MI_SET_MODIFIED (Pfn1, 1, 0xD);

    Pfn1->u4.InPageError = 0;

    //
    // Increment the share count for the page table page containing
    // this PTE.
    //

    if (ContainingPageFrame != 0) {
        Pfn1->u4.PteFrame = ContainingPageFrame;
        Pfn2 = MI_PFN_ELEMENT (ContainingPageFrame);
        Pfn2->u2.ShareCount += 1;
    }
    return;
}

WSLE_NUMBER
MiAddValidPageToWorkingSet (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG WsleMask
    )

/*++

Routine Description:

    This routine adds the specified virtual address into the
    appropriate working set list.

Arguments:

    VirtualAddress - Supplies the address to add to the working set list.

    PointerPte - Supplies a pointer to the PTE that is now valid.

    Pfn1 - Supplies the PFN database element for the physical page
           mapped by the virtual address.

    WsleMask - Supplies a mask (protection and flags) to OR into the
               working set list entry.

Return Value:

    Non-zero on success, 0 on failure.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    WSLE_NUMBER WorkingSetIndex;
    PEPROCESS Process;
    PMMSUPPORT WsInfo;

#if !DBG
    UNREFERENCED_PARAMETER (PointerPte);
#endif

    ASSERT (MI_IS_PAGE_TABLE_ADDRESS(PointerPte));
    ASSERT (PointerPte->u.Hard.Valid == 1);

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) || MI_IS_SESSION_PTE (VirtualAddress)) {
        //
        // Current process's session space working set.
        //

        WsInfo = &MmSessionSpace->Vm;
    }
    else if (MI_IS_PROCESS_SPACE_ADDRESS(VirtualAddress)) {

        //
        // Per process working set.
        //

        Process = PsGetCurrentProcess();
        WsInfo = &Process->Vm;
    }
    else {

        //
        // System cache working set.
        //

        WsInfo = &MmSystemCacheWs;
    }

    WorkingSetIndex = MiAllocateWsle (WsInfo, PointerPte, Pfn1, WsleMask);

    return WorkingSetIndex;
}

VOID
MiTrimPte (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN PEPROCESS CurrentProcess,
    IN MMPTE NewPteContents
    )

/*++

Routine Description:

    This routine removes the specified virtual address from a page table
    page.   Note there is no working set list entry for this address.

Arguments:

    VirtualAddress - Supplies the address to remove.

    PointerPte - Supplies a pointer to the PTE that is now valid.

    Pfn1 - Supplies the PFN database element for the physical page
           mapped by the virtual address.

    CurrentProcess - Supplies NULL (ie: system cache), HYDRA_PROCESS (ie:
                     a session) or anything else (ie: process).

    NewPteContents - Supplies the new PTE contents to place in the PTE.  This
                     is only used for prototype PTEs - private PTEs are
                     always encoded with the Pfn's OriginalPte information.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    KIRQL OldIrql;
    MMPTE TempPte;
    MMPTE PreviousPte;
    PMMPTE ContainingPageTablePage;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PETHREAD CurrentThread;

    CurrentThread = PsGetCurrentThread ();

    PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);

    // Working set mutex must be held.
    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
    ASSERT (KeAreAllApcsDisabled () == TRUE);

    if (Pfn1->u3.e1.PrototypePte) {

        ASSERT (MI_IS_PFN_DELETED (Pfn1) == 0);

        //
        // This is a prototype PTE.  The PFN database does not contain
        // the contents of this PTE it contains the contents of the
        // prototype PTE.  This PTE must be reconstructed to contain
        // a pointer to the prototype PTE.
        //
        // The working set list entry contains information about
        // how to reconstruct the PTE.
        //

        TempPte = NewPteContents;
        ASSERT (NewPteContents.u.Proto.Prototype == 1);

        //
        // Decrement the share count of the containing page table
        // page as the PTE for the removed page is no longer valid
        // or in transition.
        //

        ContainingPageTablePage = MiGetPteAddress (PointerPte);
#if (_MI_PAGING_LEVELS >= 3)
        ASSERT (ContainingPageTablePage->u.Hard.Valid == 1);
#else
        if (ContainingPageTablePage->u.Hard.Valid == 0) {
            if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x61940, 
                              (ULONG_PTR) PointerPte,
                              (ULONG_PTR) ContainingPageTablePage->u.Long,
                              (ULONG_PTR) VirtualAddress);
            }
        }
#endif
        PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPageTablePage);
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        LOCK_PFN (OldIrql);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
    }
    else {

        //
        // This is a private page, make it transition.
        //
        // Assert that the share count is 1 for all user mode pages.
        //

        ASSERT ((Pfn1->u2.ShareCount == 1) ||
                (VirtualAddress > (PVOID)MM_HIGHEST_USER_ADDRESS));

        if (MI_IS_PFN_DELETED (Pfn1)) {
            TempPte.u.Long = 0;
            Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
            LOCK_PFN (OldIrql);
            MiDecrementShareCountInline (Pfn2, Pfn1->u4.PteFrame);
        }
        else {
            TempPte = *PointerPte;

            MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                          Pfn1->OriginalPte.u.Soft.Protection);
            LOCK_PFN (OldIrql);
        }
    }

    PreviousPte = *PointerPte;

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

    MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);

    if ((CurrentProcess == NULL) || (CurrentProcess == HYDRA_PROCESS)) {
        MI_FLUSH_SINGLE_TB (VirtualAddress, TRUE);
    }
    else {

        MI_FLUSH_SINGLE_TB (VirtualAddress, FALSE);

        if ((Pfn1->u3.e1.PrototypePte == 0) &&
            (MI_IS_PTE_DIRTY(PreviousPte)) &&
            (CurrentProcess->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH)) {

            //
            // This process has (or had) write watch VADs.
            // Search now for a write watch region encapsulating
            // the PTE being invalidated.
            //

            MiCaptureWriteWatchDirtyBit (CurrentProcess, VirtualAddress);
        }
    }

    MiDecrementShareCountInline (Pfn1, PageFrameIndex);

    UNLOCK_PFN (OldIrql);
}

PMMINPAGE_SUPPORT
MiGetInPageSupportBlock (
    IN KIRQL OldIrql,
    OUT PNTSTATUS Status
    )

/*++

Routine Description:

    This routine acquires an inpage support block.  If none are available,
    the PFN lock will be released and reacquired to add an entry to the list.
    NULL will then be returned.

Arguments:

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at (or
              MM_NOIRQL if the caller did not acquire the PFN at all).

    Status - Supplies a pointer to a status to return (valid only if the
             PFN lock had to be released, ie: NULL was returned).

Return Value:

    A non-null pointer to an inpage block if one is already available.
    The PFN lock is not released in this path.

    NULL is returned if no inpage blocks were available.  In this path, the
    PFN lock is released and an entry is added - but NULL is still returned
    so the caller is aware that the state has changed due to the lock release
    and reacquisition.

Environment:

    Kernel mode, PFN lock may optionally be held.

--*/

{
    PMMINPAGE_SUPPORT Support;
    PSLIST_ENTRY SingleListEntry;

#if DBG
    if (OldIrql != MM_NOIRQL) {
        MM_PFN_LOCK_ASSERT();
    }
    else {
        ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    }
#endif

    if (ExQueryDepthSList (&MmInPageSupportSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmInPageSupportSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         MMINPAGE_SUPPORT,
                                         ListEntry);

returnok:
            ASSERT (Support->WaitCount == 1);
            ASSERT (Support->u1.e1.PrefetchMdlHighBits == 0);
            ASSERT (Support->u1.LongFlags == 0);
            ASSERT (KeReadStateEvent (&Support->Event) == 0);
            ASSERT64 (Support->UsedPageTableEntries == 0);

            Support->Thread = PsGetCurrentThread();
#if DBG
            Support->ListEntry.Next = NULL;
#endif

            return Support;
        }
    }

    if (OldIrql != MM_NOIRQL) {
        UNLOCK_PFN (OldIrql);
    }

    Support = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(MMINPAGE_SUPPORT),
                                     'nImM');

    if (Support != NULL) {

        KeInitializeEvent (&Support->Event, NotificationEvent, FALSE);

        Support->WaitCount = 1;
        Support->u1.LongFlags = 0;
        ASSERT (Support->u1.PrefetchMdl == NULL);
        ASSERT (KeReadStateEvent (&Support->Event) == 0);

#if defined (_WIN64)
        Support->UsedPageTableEntries = 0;
#endif
#if DBG
        Support->Thread = NULL;
#endif

        if (OldIrql == MM_NOIRQL) {
            goto returnok;
        }

        InterlockedPushEntrySList (&MmInPageSupportSListHead,
                                   (PSLIST_ENTRY)&Support->ListEntry);

        //
        // Pool had to be allocated for this inpage block hence the PFN
        // lock had to be released.  No need to delay, but the fault must
        // be retried due to the PFN lock release.  Return a status such
        // that this will occur quickly.
        //

        *Status = STATUS_REFAULT;
    }
    else {

        //
        // No pool is available - don't let a high priority thread consume
        // the machine in a continuous refault stream.  A delay allows
        // other system threads to run which will try to free up more pool.
        // Return a status that will introduce a delay above us in an effort
        // to make pool available.  The advantage of our caller executing the
        // delay is because he'll do this after releasing the relevant
        // working set mutex (if any) so the current process can be trimmed
        // for pages also.
        //

        *Status = STATUS_INSUFFICIENT_RESOURCES;
    }

    if (OldIrql != MM_NOIRQL) {
        LOCK_PFN (OldIrql);
    }

    return NULL;

}

VOID
MiFreeInPageSupportBlock (
    IN PMMINPAGE_SUPPORT Support
    )

/*++

Routine Description:

    This routine returns the inpage support block to a list of freed blocks.

Arguments:

    Support - Supplies the inpage support block to put on the free list.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    ASSERT (Support->Thread != NULL);
    ASSERT (Support->WaitCount != 0);

    ASSERT ((Support->ListEntry.Next == NULL) ||
            (Support->u1.e1.PrefetchMdlHighBits != 0));

    //
    // An interlock is needed for the WaitCount decrement as an inpage support
    // block can be simultaneously freed by any number of threads.
    //
    // Careful synchronization is applied to the WaitCount field so
    // that freeing of the inpage block can occur lock-free.  Note
    // that the ReadInProgress bit on each PFN is set and cleared while
    // holding the PFN lock.  Inpage blocks are always (and must be)
    // freed _AFTER_ the ReadInProgress bit is cleared.
    //

    if (InterlockedDecrement (&Support->WaitCount) == 0) {

        if (Support->u1.e1.PrefetchMdlHighBits != 0) {
            PMDL Mdl;
            Mdl = MI_EXTRACT_PREFETCH_MDL (Support);
            if (Mdl != &Support->Mdl) {
                ExFreePool (Mdl);
            }
        }

        if (ExQueryDepthSList (&MmInPageSupportSListHead) < MmInPageSupportMinimum) {
            Support->WaitCount = 1;
            Support->u1.LongFlags = 0;
            KeClearEvent (&Support->Event);
#if defined (_WIN64)
            Support->UsedPageTableEntries = 0;
#endif
#if DBG
            Support->Thread = NULL;
#endif

            InterlockedPushEntrySList (&MmInPageSupportSListHead,
                                       (PSLIST_ENTRY)&Support->ListEntry);
            return;
        }
        ExFreePool (Support);
    }

    return;
}

VOID
MiHandleBankedSection (
    IN PVOID VirtualAddress,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This routine invalidates a bank of video memory, calls out to the
    video driver and then enables the next bank of video memory.

Arguments:

    VirtualAddress - Supplies the address of the faulting page.

    Vad - Supplies the VAD which maps the range.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    PMMBANKED_SECTION Bank;
    PMMPTE PointerPte;
    ULONG BankNumber;
    ULONG size;

    Bank = ((PMMVAD_LONG) Vad)->u4.Banked;
    size = Bank->BankSize;

    MiZeroMemoryPte (Bank->CurrentMappedPte, size >> PAGE_SHIFT);

    //
    // Flush the TB as we have invalidated all the PTEs in this range.
    //

    MI_FLUSH_PROCESS_TB (FALSE);

    //
    // Calculate new bank address and bank number.
    //

    PointerPte = MiGetPteAddress (
                        (PVOID)((ULONG_PTR)VirtualAddress & ~((LONG)size - 1)));
    Bank->CurrentMappedPte = PointerPte;

    BankNumber = (ULONG)(((PCHAR)PointerPte - (PCHAR)Bank->BasedPte) >> Bank->BankShift);

    (Bank->BankedRoutine) (BankNumber, BankNumber, Bank->Context);

    //
    // Set the new range valid.
    //

    RtlCopyMemory (PointerPte,
                   &Bank->BankTemplate[0],
                   size >> (PAGE_SHIFT - PTE_SHIFT));

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\physical.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   physical.c

Abstract:

    This module contains the routines to manipulate physical memory from
    user space.

    There are restrictions on how user controlled physical memory can be used.
    Realize that all this memory is nonpaged and hence applications should
    allocate this with care as it represents a very real system resource.

    Virtual memory which maps user controlled physical memory pages must be :

    1.  Private memory only (ie: cannot be shared between processes).

    2.  The same physical page cannot be mapped at 2 different virtual
        addresses.

    3.  Callers must have LOCK_VM privilege to create these VADs.

    4.  Device drivers cannot call MmSecureVirtualMemory on it - this means
        that applications should not expect to use this memory for win32k.sys
        calls.

    5.  NtProtectVirtualMemory only allows read-write protection on this
        memory.  No other protection (no access, guard pages, readonly, etc)
        are allowed.

    6.  NtFreeVirtualMemory allows only MEM_RELEASE and NOT MEM_DECOMMIT on
        these VADs.  Even MEM_RELEASE is only allowed on entire VAD ranges -
        that is, splitting of these VADs is not allowed.

    7.  fork() style child processes don't inherit physical VADs.

    8.  The physical pages in these VADs are not subject to job limits.

--*/

#include "mi.h"

#if defined (_WIN64)
#define InterlockedZeroPointer(P)                               \
        {                                                       \
            LONG64 NewPointer = 0;                              \
            InterlockedExchange64 ((PLONG64)&(P), NewPointer);  \
        }
#else
#define InterlockedZeroPointer(P)                               \
        {                                                       \
            InterlockedAnd ((PLONG)&(P), 0);                    \
        }
#endif

NTSTATUS
MiCaptureUlongPtrArray (
    OUT PVOID Destination,
    IN PVOID Source,
    IN ULONG_PTR ArraySize
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtAllocateUserPhysicalPages)
#pragma alloc_text(PAGE,NtFreeUserPhysicalPages)
#pragma alloc_text(PAGE,NtMapUserPhysicalPages)
#pragma alloc_text(PAGE,NtMapUserPhysicalPagesScatter)
#pragma alloc_text(PAGE,MiCaptureUlongPtrArray)
#pragma alloc_text(PAGE,MiRemoveUserPhysicalPagesVad)
#pragma alloc_text(PAGE,MiCleanPhysicalProcessPages)
#pragma alloc_text(PAGE,MiAllocateAweInfo)
#pragma alloc_text(PAGE,MiFreeAweInfo)
#pragma alloc_text(PAGE,MiInsertAweInfo)
#pragma alloc_text(PAGE,MiAweViewInserter)
#pragma alloc_text(PAGE,MiAweViewRemover)
#pragma alloc_text(PAGE,MiReleasePhysicalCharges)
#pragma alloc_text(PAGE,MmSetPhysicalPagesLimit)
#pragma alloc_text(PAGELK,MiAllocateLargeZeroPages)
#endif

//
// This local stack size definition is deliberately large as ISVs have told
// us they expect to typically do up to this amount.
//

#define COPY_STACK_SIZE             1024

#define SMALL_COPY_STACK_SIZE        (COPY_STACK_SIZE / 2)
#define VERY_SMALL_COPY_STACK_SIZE    64

#define BITS_IN_ULONG ((sizeof (ULONG)) * 8)
    
#define LOWEST_USABLE_PHYSICAL_ADDRESS    (16 * 1024 * 1024)
#define LOWEST_USABLE_PHYSICAL_PAGE       (LOWEST_USABLE_PHYSICAL_ADDRESS >> PAGE_SHIFT)

#define LOWEST_BITMAP_PHYSICAL_PAGE       0
#define MI_FRAME_TO_BITMAP_INDEX(x)       ((ULONG)(x))
#define MI_BITMAP_INDEX_TO_FRAME(x)       ((ULONG)(x))

PFN_NUMBER MmVadPhysicalPages;

#if DBG
LOGICAL MiUsingLowPagesForAwe = FALSE;
extern ULONG MiShowStuckPages;
#endif

#define MI_WRITE_ZERO_PTE_NO_LOGGING(PointerPte)    PointerPte->u.Long = 0

NTSTATUS
NtMapUserPhysicalPages (
    __in PVOID VirtualAddress,
    __in ULONG_PTR NumberOfPages,
    __in_ecount_opt(NumberOfPages) PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function maps the specified nonpaged physical pages into the specified
    user address range.

    Note no WSLEs are maintained for this range as it is all nonpaged.

Arguments:

    VirtualAddress - Supplies a user virtual address within a UserPhysicalPages
                     Vad.
        
    NumberOfPages - Supplies the number of pages to map.
        
    UserPfnArray - Supplies a pointer to the page frame numbers to map in.
                   If this is zero, then the virtual addresses are set to
                   NO_ACCESS.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PMMPTE OldPte;
    ULONG Processor;
    PAWEINFO AweInfo;
    PULONG BitBuffer;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PVOID EndAddress;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS Status;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID PoolArea;
    PVOID PoolAreaEnd;
    PPFN_NUMBER FrameList;
    ULONG_PTR StackArray[COPY_STACK_SIZE];
    MMPTE OldPteContents;
    MMPTE NewPteContents;
    ULONG_PTR NumberOfBytes;
    ULONG_PTR SizeOfBitMap;
    PRTL_BITMAP BitMap;
    PMI_PHYSICAL_VIEW PhysicalView;
    PEX_PUSH_LOCK PushLock;
    PETHREAD CurrentThread;
    TABLE_SEARCH_RESULT SearchResult;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (NumberOfPages > (MAXULONG_PTR / PAGE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    VirtualAddress = PAGE_ALIGN(VirtualAddress);
    EndAddress = (PVOID)((PCHAR)VirtualAddress + (NumberOfPages << PAGE_SHIFT) -1);

    if (EndAddress <= VirtualAddress) {
        return STATUS_INVALID_PARAMETER_2;
    }

    //
    // Carefully probe and capture all user parameters.
    //

    FrameList = NULL;
    PoolArea = (PVOID)&StackArray[0];

    if (ARGUMENT_PRESENT (UserPfnArray)) {

        //
        // Check for zero pages here so the loops further down can be optimized
        // taking into account this can never happen.
        //

        if (NumberOfPages == 0) {
            return STATUS_SUCCESS;
        }

        NumberOfBytes = NumberOfPages * sizeof(ULONG_PTR);

        if (NumberOfPages > COPY_STACK_SIZE) {
            PoolArea = ExAllocatePoolWithTag (NonPagedPool,
                                              NumberOfBytes,
                                              'wRmM');
    
            if (PoolArea == NULL) {
                return STATUS_INSUFFICIENT_RESOURCES;
            }
        }
    
        //
        // Capture the specified page frame numbers.
        //

        Status = MiCaptureUlongPtrArray (PoolArea,
                                         UserPfnArray,
                                         NumberOfPages);

        if (!NT_SUCCESS (Status)) {
            if (PoolArea != (PVOID)&StackArray[0]) {
                ExFreePool (PoolArea);
            }
            return Status;
        }

        FrameList = (PPFN_NUMBER)PoolArea;
    }

    PoolAreaEnd = (PVOID)((PULONG_PTR)PoolArea + NumberOfPages);

    PointerPte = MiGetPteAddress (VirtualAddress);
    LastPte = PointerPte + NumberOfPages;

    CurrentThread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (CurrentThread);

    PageFrameIndex = 0;

    //
    // Initialize as much as possible before acquiring any locks.
    // Note we deliberately pass MiHighestUserPte here to the macro
    // because the user may have passed a kernel virtual address.  We
    // don't check the virtual address till element lookup below but
    // don't want the PTE construction macro to assert.
    //

    MI_MAKE_VALID_USER_PTE (NewPteContents,
                            PageFrameIndex,
                            MM_READWRITE,
                            MiHighestUserPte);

    MI_SET_PTE_DIRTY (NewPteContents);

    PteFlushList.Count = 0;

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        if (PoolArea != (PVOID)&StackArray[0]) {
            ExFreePool (PoolArea);
        }
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Block APCs to prevent recursive pushlock scenarios as this is not
    // supported.
    //

    KeEnterGuardedRegionThread (&CurrentThread->Tcb);

    //
    // Pushlock protection protects insertion/removal of Vads into each process'
    // AweVadList.  It also protects creation/deletion and adds/removes
    // of the VadPhysicalPagesBitMap.  Finally, it protects the PFN
    // modifications for pages in the bitmap.
    //

    PushLock = ExAcquireCacheAwarePushLockShared (AweInfo->PushLock);

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    Processor = KeGetCurrentProcessorNumber ();
    PhysicalView = AweInfo->PhysicalViewHint[Processor];

    if ((PhysicalView != NULL) &&
        (PhysicalView->VadType == VadAwe) &&
        (VirtualAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
        (EndAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

        NOTHING;
    }
    else {

        //
        // Lookup the element and save the result.
        //
        // Note that the pushlock is sufficient to traverse this list.
        //

        SearchResult = MiFindNodeOrParent (&AweInfo->AweVadRoot,
                                           MI_VA_TO_VPN (VirtualAddress),
                                           (PMMADDRESS_NODE *) &PhysicalView);

        if ((SearchResult == TableFoundNode) &&
            (PhysicalView->VadType == VadAwe) &&
            (VirtualAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
            (EndAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

            AweInfo->PhysicalViewHint[Processor] = PhysicalView;
        }
        else {
            Status = STATUS_INVALID_PARAMETER_1;
            goto ErrorReturn;
        }
    }

    //
    // Ensure the PFN element corresponding to each specified page is owned
    // by the specified VAD.
    //
    // Since this ownership can only be changed while holding this process'
    // working set lock, the PFN can be scanned here without holding the PFN
    // lock.
    //
    // Note the PFN lock is not needed because any race with MmProbeAndLockPages
    // can only result in the I/O going to the old page or the new page.
    // If the user breaks the rules, the PFN database (and any pages being
    // windowed here) are still protected because of the reference counts
    // on the pages with inprogress I/O.  This is possible because NO pages
    // are actually freed here - they are just windowed.
    //

    if (ARGUMENT_PRESENT (UserPfnArray)) {

        //
        // By keeping the PFN bitmap in the VAD (instead of in the PFN
        // database itself), a few benefits are realized:
        //
        // 1. No need to acquire the PFN lock here.
        // 2. Faster handling of PFN databases with holes.
        // 3. Transparent support for dynamic PFN database growth.
        // 4. Less nonpaged memory is used (for the bitmap vs adding a
        //    field to the PFN) on systems with no unused pack space in
        //    the PFN database, presuming not many of these VADs get
        //    allocated.
        //

#if defined(_AMD64_)

        //
        // Perform a prefetchw of the PFN database cachelines that will
        // be updated later.
        //
        // Note that at this point the page frame numbers haven't been
        // validated and may in fact be completely bogus.  Prefetch
        // semantics allow this.
        //

        do {

            PageFrameIndex = *FrameList;
            if (PageFrameIndex != 0) {
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                PrefetchForWrite (&Pfn1->PteAddress);
            }

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        FrameList = (PPFN_NUMBER) PoolArea;

#endif

        //
        // The first pass here ensures all the frames are secure.
        //

        //
        // N.B.  This implies that PFN_NUMBER is always ULONG_PTR in width
        //       as PFN_NUMBER is not exposed to application code today.
        //

        SizeOfBitMap = BitMap->SizeOfBitMap;
        BitBuffer = BitMap->Buffer;

        do {
            
            PageFrameIndex = *FrameList;

            //
            // Frames past the end of the bitmap are not allowed.
            //
            // Ensure the frame is a 32-bit number.
            //

            if (PageFrameIndex >= SizeOfBitMap) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // Frames not in the bitmap are not allowed.
            //

            if (MI_CHECK_BIT (BitBuffer, PageFrameIndex) == 0) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // The frame must not be already mapped anywhere.
            // Or be passed in twice in different spots in the array.
            // Also guard against the malicious user issuing more than
            // one remap request for all or portions of the same region
            // simultaneously.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (MI_PFN_IS_AWE (Pfn1));
            ASSERT (Pfn1->u2.ShareCount == 1);

            OldPte = InterlockedCompareExchangePointer (&Pfn1->PteAddress,
                                                        PointerPte,
                                                        NULL);
                                                                 
            if (OldPte != NULL) {

                //
                // This frame is already mapped so fail the request.
                //

                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            PointerPte += 1;
            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        //
        // Even though we have already validated the new PFNs are not mapped
        // anywhere, an interlocked sequence must still be used on the
        // target PTE to prevent a concurrent thread trying to map a different
        // PFN at this address from corrupting things.
        //

        PointerPte -= NumberOfPages;
        FrameList = (PPFN_NUMBER) PoolArea;

        do {

            PageFrameIndex = *FrameList;

            NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;

            ASSERT (MI_PFN_ELEMENT(PageFrameIndex)->PteAddress == PointerPte);

            OldPteContents.u.Long =
                InterlockedExchangePte (PointerPte, NewPteContents.u.Long);

            //
            // The PTE is now pointing at the new frame.  Note that another
            // thread can immediately access the page contents via this PTE
            // even though they're not supposed to until this API returns.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't really still or yet own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);

                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 1);
                ASSERT (MI_PFN_IS_AWE (Pfn1));

                InterlockedZeroPointer (Pfn1->PteAddress);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.Count += 1;
                }
            }
            else {
                ASSERT (OldPteContents.u.Long == 0);
            }

            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            PointerPte += 1;

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);
    }
    else {

        //
        // Set the specified virtual address range to no access.
        //

        while (PointerPte < LastPte) {

#if defined(_X86PAE_)
            OldPteContents.u.Long = InterlockedExchangePte (PointerPte,
                                                            ZeroPte.u.Long);
#else
            OldPteContents.u.Long = InterlockedExchangePte (PointerPte,
                                                            0);
#endif

            //
            // The PTE has been cleared.  Note that another thread can still
            // be accessing the page contents via the stale PTE until the TB
            // entry is flushed even though they're not supposed to.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't still own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);
                ASSERT (MI_PFN_IS_AWE (Pfn1));
                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 1);

                InterlockedZeroPointer (Pfn1->PteAddress);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.Count += 1;
                }
            }

            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            PointerPte += 1;
        }
    }

    ExReleaseCacheAwarePushLockShared (PushLock);

    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    //
    // Flush the TB entries for any relevant pages.  Note this can be done
    // without holding the AWE pushlock because the PTEs have already been
    // filled so any concurrent (bogus) map/unmap call will see the right
    // entries.  AND any free of the physical pages will also see the right
    // entries (although the free must do a TB flush while holding the AWE
    // pushlock exclusive to ensure no thread gets to continue using a
    // stale mapping to the page being freed prior to the flush below).
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return STATUS_SUCCESS;

ErrorReturn0:

    while (FrameList > (PPFN_NUMBER)PoolArea) {
        FrameList -= 1;
        PageFrameIndex = *FrameList;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (MI_PFN_IS_AWE (Pfn1));
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->PteAddress != NULL);
        InterlockedZeroPointer (Pfn1->PteAddress);
    }

ErrorReturn:

    ExReleaseCacheAwarePushLockShared (PushLock);

    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return Status;
}

NTSTATUS
NtMapUserPhysicalPagesScatter (
    __in_ecount(NumberOfPages) PVOID *VirtualAddresses,
    __in ULONG_PTR NumberOfPages,
    __in_ecount_opt(NumberOfPages) PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function maps the specified nonpaged physical pages into the specified
    user address range.

    Note no WSLEs are maintained for this range as it is all nonpaged.

Arguments:

    VirtualAddresses - Supplies a pointer to an array of user virtual addresses
                       within UserPhysicalPages Vads.  Each array entry is
                       presumed to map a single page.
        
    NumberOfPages - Supplies the number of pages to map.
        
    UserPfnArray - Supplies a pointer to the page frame numbers to map in.
                   If this is zero, then the virtual addresses are set to
                   NO_ACCESS.  If the array entry is zero then just the
                   corresponding virtual address is set to NO_ACCESS.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PMMPTE OldPte;
    ULONG Processor;
    PULONG BitBuffer;
    PAWEINFO AweInfo;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS Status;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID PoolArea;
    PVOID PoolAreaEnd;
    PVOID *PoolVirtualArea;
    PVOID *PoolVirtualAreaBase;
    PVOID *PoolVirtualAreaEnd;
    PPFN_NUMBER FrameList;
    PVOID StackVirtualArray[SMALL_COPY_STACK_SIZE];
    ULONG_PTR StackArray[SMALL_COPY_STACK_SIZE];
    MMPTE OldPteContents;
    MMPTE NewPteContents0;
    MMPTE NewPteContents;
    SIZE_T NumberOfBytes;
    SIZE_T NumberOfPoolBytes;
    PRTL_BITMAP BitMap;
    PMI_PHYSICAL_VIEW PhysicalView;
    PMI_PHYSICAL_VIEW LocalPhysicalView;
    PMI_PHYSICAL_VIEW NewPhysicalViewHint;
    PVOID VirtualAddress;
    ULONG_PTR SizeOfBitMap;
    PEX_PUSH_LOCK PushLock;
    PETHREAD CurrentThread;
    TABLE_SEARCH_RESULT SearchResult;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (NumberOfPages > (MAXULONG_PTR / PAGE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    //
    // Carefully probe and capture the user virtual address array.
    //

    PoolArea = (PVOID)&StackArray[0];
    PoolVirtualAreaBase = (PVOID)&StackVirtualArray[0];

    NumberOfPoolBytes = NumberOfPages * sizeof(PVOID);
    NumberOfBytes = NumberOfPoolBytes;

    if (NumberOfPages > SMALL_COPY_STACK_SIZE) {

        if (ARGUMENT_PRESENT(UserPfnArray)) {
            NumberOfPoolBytes *= 2;
        }

        PoolVirtualAreaBase = ExAllocatePoolWithTag (NonPagedPool,
                                                     NumberOfPoolBytes,
                                                     'wRmM');

        if (PoolVirtualAreaBase == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    PoolVirtualArea = PoolVirtualAreaBase;

    Status = MiCaptureUlongPtrArray (PoolVirtualArea,
                                     VirtualAddresses,
                                     NumberOfPages);
    if (!NT_SUCCESS(Status)) {
        goto ErrorReturn;
    }

    //
    // Check for zero pages here so the loops further down can be optimized
    // taking into account this can never happen.
    //

    if (NumberOfPages == 0) {
        return STATUS_SUCCESS;
    }

    //
    // Carefully probe and capture the user PFN array.
    //

    if (ARGUMENT_PRESENT (UserPfnArray)) {

        ASSERT (NumberOfBytes == NumberOfPages * sizeof(ULONG_PTR));

        if (NumberOfPages > SMALL_COPY_STACK_SIZE) {
            PoolArea = PoolVirtualAreaBase + NumberOfPages;
        }
    
        //
        // Capture the specified page frame numbers.
        //

        Status = MiCaptureUlongPtrArray (PoolArea,
                                         UserPfnArray,
                                         NumberOfPages);
        if (!NT_SUCCESS (Status)) {
            goto ErrorReturn;
        }
    }

    PoolAreaEnd = (PVOID)((PULONG_PTR)PoolArea + NumberOfPages);

    CurrentThread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (CurrentThread);

    //
    // Initialize as much as possible before acquiring any locks.
    //

    PageFrameIndex = 0;

    PhysicalView = NULL;

    PteFlushList.Count = 0;

    FrameList = (PPFN_NUMBER)PoolArea;

    ASSERT (NumberOfPages != 0);

    PoolVirtualAreaEnd = PoolVirtualAreaBase + NumberOfPages;

    //
    // Note we deliberately pass MiHighestUserPte here to the macro
    // because the user may have passed a kernel virtual address.  We
    // don't check the virtual address till element lookup below but
    // don't want the PTE construction macro to assert.
    //

    MI_MAKE_VALID_USER_PTE (NewPteContents0,
                            PageFrameIndex,
                            MM_READWRITE,
                            MiHighestUserPte);

    MI_SET_PTE_DIRTY (NewPteContents0);

    Status = STATUS_SUCCESS;

    NewPhysicalViewHint = NULL;

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    //
    // Block APCs to prevent recursive pushlock scenarios as this is not
    // supported.
    //

    KeEnterGuardedRegionThread (&CurrentThread->Tcb);

    //
    // Pushlock protection protects insertion/removal of Vads into each process'
    // AweVadList.  It also protects creation/deletion and adds/removes
    // of the VadPhysicalPagesBitMap.  Finally, it protects the PFN
    // modifications for pages in the bitmap.
    //

    PushLock = ExAcquireCacheAwarePushLockShared (AweInfo->PushLock);

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    //
    // Note that the PFN lock is not needed to traverse this list (even though
    // MmProbeAndLockPages uses it), because the pushlock has been acquired.
    //

    Processor = KeGetCurrentProcessorNumber ();
    LocalPhysicalView = AweInfo->PhysicalViewHint[Processor];

    if ((LocalPhysicalView != NULL) &&
        (LocalPhysicalView->VadType != VadAwe)) {

        LocalPhysicalView = NULL;
    }

    do {

        VirtualAddress = *PoolVirtualArea;

        //
        // First check the last physical view this processor used.
        //

        if (LocalPhysicalView != NULL) {

            ASSERT (LocalPhysicalView->VadType == VadAwe);
            ASSERT (LocalPhysicalView->Vad->u.VadFlags.VadType == VadAwe);

            if ((VirtualAddress >= MI_VPN_TO_VA (LocalPhysicalView->StartingVpn)) &&
                (VirtualAddress <= MI_VPN_TO_VA_ENDING (LocalPhysicalView->EndingVpn))) {

                //
                // The virtual address is within the hint so it's good.
                //

                PoolVirtualArea += 1;
                NewPhysicalViewHint = LocalPhysicalView;
                continue;
            }
        }

        //
        // Check the last physical view this loop used.
        //

        if (PhysicalView != NULL) {

            ASSERT (PhysicalView->VadType == VadAwe);
            ASSERT (PhysicalView->Vad->u.VadFlags.VadType == VadAwe);

            if ((VirtualAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
                (VirtualAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

                //
                // The virtual address is within the hint so it's good.
                //

                PoolVirtualArea += 1;
                NewPhysicalViewHint = PhysicalView;
                continue;
            }
        }

        //
        // Lookup the element and save the result.
        //
        // Note that the pushlock is sufficient to traverse this list.
        //

        SearchResult = MiFindNodeOrParent (&AweInfo->AweVadRoot,
                                           MI_VA_TO_VPN (VirtualAddress),
                                           (PMMADDRESS_NODE *) &PhysicalView);

        if ((SearchResult == TableFoundNode) &&
            (PhysicalView->VadType == VadAwe) &&
            (VirtualAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
            (VirtualAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

            NewPhysicalViewHint = PhysicalView;
        }
        else {
            //
            // No virtual address is reserved at the specified base address,
            // return an error.
            //

            ExReleaseCacheAwarePushLockShared (PushLock);
            KeLeaveGuardedRegionThread (&CurrentThread->Tcb);
            Status = STATUS_INVALID_PARAMETER_1;
            goto ErrorReturn;
        }

        PoolVirtualArea += 1;

    } while (PoolVirtualArea < PoolVirtualAreaEnd);

    ASSERT (NewPhysicalViewHint != NULL);

    if (AweInfo->PhysicalViewHint[Processor] != NewPhysicalViewHint) {
        AweInfo->PhysicalViewHint[Processor] = NewPhysicalViewHint;
    }

    //
    // Ensure the PFN element corresponding to each specified page is owned
    // by the specified VAD.
    //
    // Since this ownership can only be changed while holding this process'
    // working set lock, the PFN can be scanned here without holding the PFN
    // lock.
    //
    // Note the PFN lock is not needed because any race with MmProbeAndLockPages
    // can only result in the I/O going to the old page or the new page.
    // If the user breaks the rules, the PFN database (and any pages being
    // windowed here) are still protected because of the reference counts
    // on the pages with inprogress I/O.  This is possible because NO pages
    // are actually freed here - they are just windowed.
    //

    PoolVirtualArea = PoolVirtualAreaBase;

    if (ARGUMENT_PRESENT (UserPfnArray)) {

        //
        // By keeping the PFN bitmap in the process (instead of in the PFN
        // database itself), a few benefits are realized:
        //
        // 1. No need to acquire the PFN lock here.
        // 2. Faster handling of PFN databases with holes.
        // 3. Transparent support for dynamic PFN database growth.
        // 4. Less nonpaged memory is used (for the bitmap vs adding a
        //    field to the PFN) on systems with no unused pack space in
        //    the PFN database.
        //

#if defined(_AMD64_)

        //
        // Perform a prefetchw of the PFN database cachelines that will
        // be updated later.
        //
        // Note that at this point the page frame numbers haven't been
        // validated and may in fact be completely bogus.  Prefetch
        // semantics allow this.
        //

        do {

            PageFrameIndex = *FrameList;
            if (PageFrameIndex != 0) {
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                PrefetchForWrite (&Pfn1->PteAddress);
            }

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        FrameList = (PPFN_NUMBER) PoolArea;

#endif

        //
        // The first pass here ensures all the frames are secure.
        //

        //
        // N.B.  This implies that PFN_NUMBER is always ULONG_PTR in width
        //       as PFN_NUMBER is not exposed to application code today.
        //

        SizeOfBitMap = BitMap->SizeOfBitMap;
        BitBuffer = BitMap->Buffer;

        do {

            PageFrameIndex = *FrameList;
            FrameList += 1;

            //
            // Zero entries are treated as a command to unmap.
            //

            if (PageFrameIndex == 0) {
                PoolVirtualArea += 1;
                continue;
            }

            //
            // Frames past the end of the bitmap are not allowed.
            //
            // Ensure the frame is a 32-bit number.
            //

            if (PageFrameIndex >= SizeOfBitMap) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // Frames not in the bitmap are not allowed.
            //

            if (MI_CHECK_BIT (BitBuffer, PageFrameIndex) == 0) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // The frame must not be already mapped anywhere.
            // Or be passed in twice in different spots in the array.
            // Also guard against the malicious user issuing more than
            // one remap request for all or portions of the same region
            // simultaneously.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            ASSERT (MI_PFN_IS_AWE (Pfn1));
            ASSERT (Pfn1->u2.ShareCount == 1);

            VirtualAddress = *PoolVirtualArea;
            PointerPte = MiGetPteAddress (VirtualAddress);

            OldPte = InterlockedCompareExchangePointer (&Pfn1->PteAddress,
                                                        PointerPte,
                                                        NULL);
                                                                 
            if (OldPte != NULL) {

                //
                // This frame is already mapped so fail the request.
                //

                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            PoolVirtualArea += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        //
        // This pass actually inserts them all into the page table pages and
        // the TBs now that we know the frames are good.  Check the PTEs and
        // PFNs carefully as a malicious user may issue more than one remap
        // request for all or portions of the same region simultaneously.
        //

        FrameList = (PPFN_NUMBER) PoolArea;
        PoolVirtualArea = PoolVirtualAreaBase;

        do {

            PageFrameIndex = *FrameList;

            if (PageFrameIndex != 0) {
                NewPteContents = NewPteContents0;
                NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;
            }
            else {
                NewPteContents.u.Long = 0;
            }

            VirtualAddress = *PoolVirtualArea;
            PoolVirtualArea += 1;

            PointerPte = MiGetPteAddress (VirtualAddress);

#if DBG
            if (PageFrameIndex != 0) {
                ASSERT (MI_PFN_ELEMENT(PageFrameIndex)->PteAddress == PointerPte);
            }
#endif

            OldPteContents.u.Long =
                InterlockedExchangePte (PointerPte, NewPteContents.u.Long);

            //
            // The PTE is now pointing at the new frame.  Note that another
            // thread can immediately access the page contents via this PTE
            // even though they're not supposed to until this API returns.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't really still or yet own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);

                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 1);
                ASSERT (MI_PFN_IS_AWE (Pfn1));

                InterlockedZeroPointer (Pfn1->PteAddress);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.Count += 1;
                }
            }

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);
    }
    else {

        //
        // Set the specified virtual address range to no access.
        //

        do {

            VirtualAddress = *PoolVirtualArea;
            PointerPte = MiGetPteAddress (VirtualAddress);
    
#if defined(_X86PAE_)
            OldPteContents.u.Long = InterlockedExchangePte (PointerPte, ZeroPte.u.Long);
#else
            OldPteContents.u.Long = InterlockedExchangePte (PointerPte, 0);
#endif

            //
            // The PTE is now zeroed.  Note that another thread can still
            // maliciously dirty data in the old frame until the TB flush
            // completes, so don't allow frame reuse till then (although
            // allowing remapping within this process is ok) to prevent
            // the app from corrupting frames it doesn't really still own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);

                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 1);
                ASSERT (MI_PFN_IS_AWE (Pfn1));

                InterlockedZeroPointer (Pfn1->PteAddress);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.Count += 1;
                }
            }

            PoolVirtualArea += 1;

        } while (PoolVirtualArea < PoolVirtualAreaEnd);
    }

    ExReleaseCacheAwarePushLockShared (PushLock);
    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    //
    // Flush the TB entries for any relevant pages.  Note this can be done
    // without holding the AWE pushlock because the PTEs have already been
    // filled so any concurrent (bogus) map/unmap call will see the right
    // entries.  AND any free of the physical pages will also see the right
    // entries (although the free must do a TB flush while holding the AWE
    // pushlock exclusive to ensure no thread gets to continue using a
    // stale mapping to the page being freed prior to the flush below).
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

ErrorReturn:

    if (PoolVirtualAreaBase != (PVOID)&StackVirtualArray[0]) {
        ExFreePool (PoolVirtualAreaBase);
    }

    return Status;

ErrorReturn0:

    FrameList -= 1;
    while (FrameList > (PPFN_NUMBER)PoolArea) {
        FrameList -= 1;
        PageFrameIndex = *FrameList;
        if (PageFrameIndex != 0) {
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (MI_PFN_IS_AWE (Pfn1));
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->PteAddress != NULL);
            InterlockedZeroPointer (Pfn1->PteAddress);
        }
    }

    ExReleaseCacheAwarePushLockShared (PushLock);
    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    goto ErrorReturn;
}

PVOID
MiAllocateAweInfo (
    VOID
    )

/*++

Routine Description:

    This function allocates an AWE structure for the current process.  Note
    this structure is never destroyed while the process is alive in order to
    allow various checks to occur lock free.

Arguments:

    None.

Return Value:

    A non-NULL AweInfo pointer on success, NULL on failure.

Environment:

    Kernel mode, APC_LEVEL or below, address space mutex (or no locks at all)
    held.

--*/

{
    PAWEINFO AweInfo;

    AweInfo = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof (AWEINFO),
                                     'wAmM');

    if (AweInfo != NULL) {

        AweInfo->VadPhysicalPagesBitMap = NULL;
        AweInfo->VadPhysicalPages = 0;
        AweInfo->VadPhysicalPagesLimit = 0;

        RtlZeroMemory (&AweInfo->PhysicalViewHint,
                       MAXIMUM_PROCESSORS * sizeof(PMI_PHYSICAL_VIEW));

        RtlZeroMemory (&AweInfo->AweVadRoot,
                       sizeof(MM_AVL_TABLE));

        ASSERT (AweInfo->AweVadRoot.NumberGenericTableElements == 0);

        AweInfo->AweVadRoot.BalancedRoot.u1.Parent = &AweInfo->AweVadRoot.BalancedRoot;

        AweInfo->PushLock = ExAllocateCacheAwarePushLock ();
        if (AweInfo->PushLock == NULL) {
            ExFreePool (AweInfo);
            return NULL;
        }
    }

    return (PVOID) AweInfo;
}

VOID
MiFreeAweInfo (
    IN PAWEINFO AweInfo
    )

/*++

Routine Description:

    This function releases the argument AWE info structure.

Arguments:

    AweInfo - Supplies the AweInfo to release.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below, address space mutex (or no locks at all)
    held.

--*/
{
    ExFreeCacheAwarePushLock (AweInfo->PushLock);
    ExFreePool (AweInfo);

    return;
}

PVOID
MiInsertAweInfo (
    IN PAWEINFO AweInfo
    )

/*++

Routine Description:

    This function inserts the argument AWE structure for the current process.
    Note this structure is never destroyed while the process is alive in
    order to allow various checks to occur lock free.

Arguments:

    AweInfo - Supplies the AweInfo to insert.

Return Value:

    The AweInfo pointer the caller should use.

Environment:

    Kernel mode, APC_LEVEL or below, address space mutex held.

--*/
{
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();

    //
    // A memory barrier is needed to ensure the writes initializing the
    // AweInfo fields are visible prior to setting the EPROCESS AweInfo
    // pointer.  This is because the reads from these fields are done
    // lock free for improved performance.  There is no need to explicitly
    // add one here as the InterlockedCompare already has one.
    //

    if (InterlockedCompareExchangePointer (&Process->AweInfo,
                                           AweInfo,
                                           NULL) != NULL) {
        
        //
        // Another thread has already inserted the AWE info structure so
        // free our caller's.
        //

        ExFreeCacheAwarePushLock (AweInfo->PushLock);

        ExFreePool (AweInfo);
        AweInfo = Process->AweInfo;
        ASSERT (AweInfo != NULL);
    }

    return (PVOID) AweInfo;
}


NTSTATUS
NtAllocateUserPhysicalPages(
    __in HANDLE ProcessHandle,
    __inout PULONG_PTR NumberOfPages,
    __out_ecount(*NumberOfPages) PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function allocates nonpaged physical pages for the specified
    subject process.

    No WSLEs are maintained for this range.

    The caller must check the NumberOfPages returned to determine how many
    pages were actually allocated (this number may be less than the requested
    amount).

    On success, the user array is filled with the allocated physical page
    frame numbers (only up to the returned NumberOfPages is filled in).

    No PTEs are filled here - this gives the application the flexibility
    to order the address space with no metadata structure imposed by the Mm.
    Applications do this via NtMapUserPhysicalPages - ie:

        - Each physical page allocated is set in the process's bitmap.
          This provides remap, free and unmap a way to validate and rundown
          these frames.

          Unmaps may result in a walk of the entire bitmap, but that's ok as
          unmaps should be less frequent.  The win is it saves us from
          using up system virtual address space to manage these frames.

        - Note that the same physical frame may NOT be mapped at two different
          virtual addresses in the process.  This makes frees and unmaps
          substantially faster as no checks for aliasing need be performed.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    NumberOfPages - Supplies a pointer to a variable that supplies the
                    desired size in pages of the allocation.  This is filled
                    with the actual number of pages allocated.
        
    UserPfnArray - Supplies a pointer to user memory to store the allocated
                   frame numbers into.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PAWEINFO AweInfo;
    PAWEINFO NewAweInfo;
    ULONG i;
    KAPC_STATE ApcState;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    LOGICAL Attached;
    ULONG_PTR CapturedNumberOfPages;
    ULONG_PTR AllocatedPages;
    ULONG_PTR MdlRequestInPages;
    ULONG_PTR TotalAllocatedPages;
    PMDL MemoryDescriptorList;
    PMDL MemoryDescriptorList2;
    PMDL MemoryDescriptorHead;
    PPFN_NUMBER MdlPage;
    PRTL_BITMAP BitMap;
    ULONG BitMapSize;
    ULONG BitMapIndex;
    PMMPFN Pfn1;
    PHYSICAL_ADDRESS LowAddress;
    PHYSICAL_ADDRESS HighAddress;
    PHYSICAL_ADDRESS SkipBytes;
    ULONG SizeOfBitMap;
    PFN_NUMBER HighestPossiblePhysicalPage;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    Attached = FALSE;

    //
    // Check the allocation type field.
    //

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        //
        // Capture the number of pages.
        //

        if (PreviousMode != KernelMode) {

            ProbeForWriteUlong_ptr (NumberOfPages);

            CapturedNumberOfPages = *NumberOfPages;

            if (CapturedNumberOfPages == 0) {
                return STATUS_SUCCESS;
            }

            if (CapturedNumberOfPages > (MAXULONG_PTR / sizeof(ULONG_PTR))) {
                return STATUS_INVALID_PARAMETER_2;
            }

            ProbeForWrite (UserPfnArray,
                           CapturedNumberOfPages * sizeof (ULONG_PTR),
                           sizeof(PULONG_PTR));

        }
        else {
            CapturedNumberOfPages = *NumberOfPages;
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    //
    // LockMemory privilege is required.
    //

    if (!SeSinglePrivilegeCheck (SeLockMemoryPrivilege, PreviousMode)) {
        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        return STATUS_PRIVILEGE_NOT_HELD;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    BitMapSize = 0;
    TotalAllocatedPages = 0;

    NewAweInfo = NULL;
    AweInfo = Process->AweInfo;

    if (AweInfo == NULL) {

        NewAweInfo = (PAWEINFO) MiAllocateAweInfo ();

        if (NewAweInfo == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn2;
        }
    }

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted. If so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        UNLOCK_ADDRESS_SPACE (Process);
        Status = STATUS_PROCESS_IS_TERMINATING;
        if (NewAweInfo != NULL) {
            MiFreeAweInfo (NewAweInfo);
        }
        goto ErrorReturn;
    }

    if (NewAweInfo != NULL) {
        AweInfo = MiInsertAweInfo (NewAweInfo);
    }

    //
    // Get the working set mutex to synchronize.  This also blocks APCs so
    // an APC which takes a page fault does not corrupt various structures.
    //

    if (AweInfo->VadPhysicalPagesLimit != 0) {

        if (AweInfo->VadPhysicalPages >= AweInfo->VadPhysicalPagesLimit) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_COMMITMENT_LIMIT;
            goto ErrorReturn;
        }

        if (CapturedNumberOfPages > AweInfo->VadPhysicalPagesLimit - AweInfo->VadPhysicalPages) {
            CapturedNumberOfPages = AweInfo->VadPhysicalPagesLimit - AweInfo->VadPhysicalPages;
        }
    }

    //
    // Create the physical pages bitmap if it does not already exist.
    //

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    if (BitMap == NULL) {

        HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;

#if defined (_WIN64)
        //
        // Force a 32-bit maximum on any page allocation because the bitmap
        // package is currently 32-bit.
        //

        if (HighestPossiblePhysicalPage + 1 >= _4gb) {
            HighestPossiblePhysicalPage = _4gb - 2;
        }
#endif

        BitMapSize = sizeof(RTL_BITMAP) + (ULONG)((((HighestPossiblePhysicalPage + 1) + 31) / 32) * 4);

        BitMap = ExAllocatePoolWithTag (NonPagedPool, BitMapSize, 'LdaV');

        if (BitMap == NULL) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn;
        }

        RtlInitializeBitMap (BitMap,
                             (PULONG)(BitMap + 1),
                             (ULONG)(HighestPossiblePhysicalPage + 1));

        RtlClearAllBits (BitMap);

        //
        // Charge quota for the nonpaged pool for the bitmap.  This is
        // done here rather than by using ExAllocatePoolWithQuota
        // so the process object is not referenced by the quota charge.
        //

        Status = PsChargeProcessNonPagedPoolQuota (Process, BitMapSize);

        if (!NT_SUCCESS(Status)) {
            UNLOCK_ADDRESS_SPACE (Process);
            ExFreePool (BitMap);
            goto ErrorReturn;
        }

        AweInfo->VadPhysicalPagesBitMap = BitMap;

        SizeOfBitMap = BitMap->SizeOfBitMap;
    }
    else {
        SizeOfBitMap = AweInfo->VadPhysicalPagesBitMap->SizeOfBitMap;
    }

    UNLOCK_ADDRESS_SPACE (Process);

    AllocatedPages = 0;
    MemoryDescriptorHead = NULL;

    SkipBytes.QuadPart = 0;

    //
    // Don't use the low 16mb of memory so that at least some low pages are left
    // for 32/24-bit device drivers.  Just under 4gb is the maximum allocation
    // per MDL so the ByteCount field does not overflow.
    //

    HighAddress.QuadPart = ((ULONGLONG)(SizeOfBitMap - 1)) << PAGE_SHIFT;

    LowAddress.QuadPart = LOWEST_USABLE_PHYSICAL_ADDRESS;

    if (LowAddress.QuadPart >= HighAddress.QuadPart) {

        //
        // If there's less than 16mb of RAM, just take pages from anywhere.
        //

#if DBG
        MiUsingLowPagesForAwe = TRUE;
#endif
        LowAddress.QuadPart = 0;
    }

    Status = STATUS_SUCCESS;

    do {

        MdlRequestInPages = CapturedNumberOfPages - TotalAllocatedPages;

        if (MdlRequestInPages > (ULONG_PTR)((MAXULONG - PAGE_SIZE) >> PAGE_SHIFT)) {
            MdlRequestInPages = (ULONG_PTR)((MAXULONG - PAGE_SIZE) >> PAGE_SHIFT);
        }

        //
        // Note this allocation returns zeroed pages.
        //

        MemoryDescriptorList = MiAllocatePagesForMdl (LowAddress,
                                                      HighAddress,
                                                      SkipBytes,
                                                      MdlRequestInPages << PAGE_SHIFT,
                                                      MiCached,
                                                      MI_ALLOCATION_IS_AWE);

        if (MemoryDescriptorList == NULL) {

            //
            // No (more) pages available.  If this becomes a common situation,
            // all the working sets could be flushed here.
            //
            // Make do with what we've gotten so far.
            //

            if (TotalAllocatedPages == 0) {
                Status = STATUS_INSUFFICIENT_RESOURCES;
            }

            break;
        }

        AllocatedPages = MemoryDescriptorList->ByteCount >> PAGE_SHIFT;

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted. If so, return an error.
        // Note any prior MDLs allocated in this loop have already had their
        // pages freed by the exiting thread, but this thread is still
        // responsible for freeing the pool containing the MDLs themselves.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {

            UNLOCK_ADDRESS_SPACE (Process);

            MmFreePagesFromMdl (MemoryDescriptorList);
            ExFreePool (MemoryDescriptorList);

            Status = STATUS_PROCESS_IS_TERMINATING;

            break;
        }

        //
        // Recheck the process and job limits as they may have changed
        // when the address space mutex was released above.
        //

        if (AweInfo->VadPhysicalPagesLimit != 0) {

            if ((AweInfo->VadPhysicalPages >= AweInfo->VadPhysicalPagesLimit) ||
                (AllocatedPages > AweInfo->VadPhysicalPagesLimit - AweInfo->VadPhysicalPages)) {

                UNLOCK_ADDRESS_SPACE (Process);

                MmFreePagesFromMdl (MemoryDescriptorList);
                ExFreePool (MemoryDescriptorList);

                if (TotalAllocatedPages == 0) {
                    Status = STATUS_COMMITMENT_LIMIT;
                }

                break;
            }
        }

        if (Process->JobStatus & PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES) {

            if (PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                        AllocatedPages) == FALSE) {

                UNLOCK_ADDRESS_SPACE (Process);

                MmFreePagesFromMdl (MemoryDescriptorList);
                ExFreePool (MemoryDescriptorList);

                if (TotalAllocatedPages == 0) {
                    Status = STATUS_COMMITMENT_LIMIT;
                }

                break;
            }
        }

        ASSERT ((AweInfo->VadPhysicalPages + AllocatedPages <= AweInfo->VadPhysicalPagesLimit) || (AweInfo->VadPhysicalPagesLimit == 0));

        AweInfo->VadPhysicalPages += AllocatedPages;

        //
        // Update the allocation bitmap for each allocated frame.
        // Note the PFN lock is not needed to modify the PFN fields below
        // as these are brand new pages owned only by this thread.
        //

        MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

        for (i = 0; i < AllocatedPages; i += 1) {

            ASSERT ((*MdlPage >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                    (MiUsingLowPagesForAwe == TRUE));

            BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(*MdlPage);

            ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
            ASSERT (MI_CHECK_BIT (BitMap->Buffer, BitMapIndex) == 0);

            ASSERT64 (*MdlPage < _4gb);

            Pfn1 = MI_PFN_ELEMENT (*MdlPage);
            ASSERT (MI_PFN_IS_AWE (Pfn1));
            Pfn1->PteAddress = NULL;
            Pfn1->AweReferenceCount = 1;
            ASSERT (Pfn1->u4.AweAllocation == 0);
            Pfn1->u4.AweAllocation = 1;
            ASSERT (Pfn1->u2.ShareCount == 1);

            //
            // Once this bit is set (and the mutex released below), a rogue
            // thread that is passing random frame numbers to
            // NtFreeUserPhysicalPages can free this frame.  This means NO
            // references can be made to it by this routine after this point
            // without first re-checking the bitmap.
            //

            MI_SET_BIT (BitMap->Buffer, BitMapIndex);

            MdlPage += 1;
        }

        ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

        UNLOCK_ADDRESS_SPACE (Process);

        MemoryDescriptorList->Next = MemoryDescriptorHead;
        MemoryDescriptorHead = MemoryDescriptorList;

        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, AllocatedPages);

        TotalAllocatedPages += AllocatedPages;

        ASSERT (TotalAllocatedPages <= CapturedNumberOfPages);

        if (TotalAllocatedPages == CapturedNumberOfPages) {
            break;
        }

        //
        // Try the same memory range again - there might be more pages
        // left in it that can be claimed as a truncated MDL had to be
        // used for the last request.
        //

    } while (TRUE);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    //
    // Establish an exception handler and carefully write out the
    // number of pages and the frame numbers.
    //

    try {

        ASSERT (TotalAllocatedPages <= CapturedNumberOfPages);

        //
        // Deliberately only write out the number of pages if the operation
        // succeeded.  This is because this was the behavior on Windows 2000.
        // And an app may be calling like this:
        //
        // PagesNo = SOMETHING_BIG;
        //
        // do
        // {
        //     Success = AllocateUserPhysicalPages (&PagesNo);
        //         
        //     if (Success == TRUE) {
        //         break;
        //     }
        //
        //     PagesNo = PagesNo / 2;
        //     continue;
        // } while (PagesNo > 0);
        //

        if (NT_SUCCESS (Status)) {
            *NumberOfPages = TotalAllocatedPages;
        }

        MemoryDescriptorList = MemoryDescriptorHead;

        while (MemoryDescriptorList != NULL) {

            MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            AllocatedPages = MemoryDescriptorList->ByteCount >> PAGE_SHIFT;

            RtlCopyMemory ((PVOID) UserPfnArray,
                           MdlPage,
                           AllocatedPages * sizeof (PFN_NUMBER));

            UserPfnArray += AllocatedPages;

            MemoryDescriptorList = MemoryDescriptorList->Next;
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If anything went wrong communicating the pages back to the user
        // then the user has really hurt himself because these addresses
        // passed the probe tests at the beginning of the service.  Rather
        // than carrying around extensive recovery code, just return back
        // success as this scenario is the same as if the user scribbled
        // over the output parameters after the service returned anyway.
        // You can't stop someone who's determined to lose their values !
        //
        // Fall through...
        //
    }

    //
    // Free the space consumed by the MDLs now that the page frame numbers
    // have been saved in the bitmap and copied to the user.
    //

    MemoryDescriptorList = MemoryDescriptorHead;
    while (MemoryDescriptorList != NULL) {
        MemoryDescriptorList2 = MemoryDescriptorList->Next;
        ExFreePool (MemoryDescriptorList);
        MemoryDescriptorList = MemoryDescriptorList2;
    }

ErrorReturn:

    ASSERT (TotalAllocatedPages <= CapturedNumberOfPages);

ErrorReturn2:

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    return Status;
}


NTSTATUS
NtFreeUserPhysicalPages(
    __in HANDLE ProcessHandle,
    __inout PULONG_PTR NumberOfPages,
    __in_ecount(*NumberOfPages) PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function frees the nonpaged physical pages for the specified
    subject process.  Any PTEs referencing these pages are also invalidated.

    Note there is no need to walk the entire VAD tree to clear the PTEs that
    match each page as each physical page can only be mapped at a single
    virtual address (alias addresses within the VAD are not allowed).

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    NumberOfPages - Supplies the size in pages of the allocation to delete.
                    Returns the actual number of pages deleted.
        
    UserPfnArray - Supplies a pointer to memory to retrieve the page frame
                   numbers from.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PAWEINFO AweInfo;
    PULONG BitBuffer;
    KAPC_STATE ApcState;
    ULONG_PTR CapturedNumberOfPages;
    PMDL MemoryDescriptorList;
    PPFN_NUMBER MdlPage;
    PPFN_NUMBER LastMdlPage;
    PFN_NUMBER PagesInMdl;
    PFN_NUMBER PageFrameIndex;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    ULONG_PTR PagesProcessed;
    PFN_NUMBER MdlHack[(sizeof(MDL) / sizeof(PFN_NUMBER)) + VERY_SMALL_COPY_STACK_SIZE];
    ULONG_PTR MdlPages;
    ULONG_PTR NumberOfBytes;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    LOGICAL Attached;
    PMMPFN Pfn1;
    LOGICAL OnePassComplete;
    LOGICAL ProcessReferenced;
    MMPTE_FLUSH_LIST PteFlushList;
    PMMPTE PointerPte;
    MMPTE OldPteContents;
    PETHREAD CurrentThread;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // Establish an exception handler, probe the specified addresses
    // for read access and capture the page frame numbers.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread (&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        try {

            ProbeForWriteUlong_ptr (NumberOfPages);

            CapturedNumberOfPages = *NumberOfPages;

            //
            // Initialize the NumberOfPages freed to zero so the user can be
            // reasonably informed about errors that occur midway through
            // the transaction.
            //

            *NumberOfPages = 0;

        } except (ExSystemExceptionFilter()) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //
    
            return GetExceptionCode();
        }
    }
    else {
        CapturedNumberOfPages = *NumberOfPages;
    }

    if (CapturedNumberOfPages == 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

    OnePassComplete = FALSE;
    PagesProcessed = 0;
    MemoryDescriptorList = NULL;
    SATISFY_OVERZEALOUS_COMPILER (MdlPages = 0);

    if (CapturedNumberOfPages > VERY_SMALL_COPY_STACK_SIZE) {

        //
        // Ensure the number of pages can fit into an MDL's ByteCount.
        //

        if (CapturedNumberOfPages > ((ULONG)MAXULONG >> PAGE_SHIFT)) {
            MdlPages = (ULONG_PTR)((ULONG)MAXULONG >> PAGE_SHIFT);
        }
        else {
            MdlPages = CapturedNumberOfPages;
        }

        while (MdlPages > VERY_SMALL_COPY_STACK_SIZE) {
            MemoryDescriptorList = MmCreateMdl (NULL,
                                                0,
                                                MdlPages << PAGE_SHIFT);
    
            if (MemoryDescriptorList != NULL) {
                break;
            }

            MdlPages >>= 1;
        }
    }

    if (MemoryDescriptorList == NULL) {
        MdlPages = VERY_SMALL_COPY_STACK_SIZE;
        MemoryDescriptorList = (PMDL)&MdlHack[0];
    }

    ProcessReferenced = FALSE;

    Process = PsGetCurrentProcessByThread (CurrentThread);

repeat:

    if (CapturedNumberOfPages < MdlPages) {
        MdlPages = CapturedNumberOfPages;
    }

    MmInitializeMdl (MemoryDescriptorList, 0, MdlPages << PAGE_SHIFT);

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    NumberOfBytes = MdlPages * sizeof(ULONG_PTR);

    Attached = FALSE;

    //
    // Establish an exception handler, probe the specified addresses
    // for read access and capture the page frame numbers.
    //

    if (PreviousMode != KernelMode) {
        
        Status = MiCaptureUlongPtrArray ((PVOID)MdlPage,
                                         UserPfnArray,
                                         MdlPages);
        if (!NT_SUCCESS (Status)) {
            goto ErrorReturn;
        }

    }
    else {
        RtlCopyMemory ((PVOID)MdlPage,
                       UserPfnArray,
                       NumberOfBytes);
    }

    if (OnePassComplete == FALSE) {

        //
        // Reference the specified process handle for VM_OPERATION access.
        //
    
        if (ProcessHandle == NtCurrentProcess()) {
            Process = PsGetCurrentProcessByThread(CurrentThread);
        }
        else {
            Status = ObReferenceObjectByHandle ( ProcessHandle,
                                                 PROCESS_VM_OPERATION,
                                                 PsProcessType,
                                                 PreviousMode,
                                                 (PVOID *)&Process,
                                                 NULL );
    
            if (!NT_SUCCESS(Status)) {
                goto ErrorReturn;
            }
            ProcessReferenced = TRUE;
        }
    }
    
    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (PsGetCurrentProcessByThread(CurrentThread) != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    PteFlushList.Count = 0;
    Status = STATUS_SUCCESS;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        UNLOCK_ADDRESS_SPACE (Process);
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    BitBuffer = BitMap->Buffer;

    LastMdlPage = MdlPage + MdlPages;

    //
    // Flush the entire TB for this process while holding its AWE push lock
    // exclusive so that if this free is occurring prior to any pending
    // flushes at the end of an in-progress map/unmap, the app is not left
    // with a stale TB entry that would allow him to corrupt pages that no
    // longer belong to him.
    //

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    MI_FLUSH_PROCESS_TB (FALSE);

    while (MdlPage < LastMdlPage) {

        PageFrameIndex = *MdlPage;
        BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(PageFrameIndex);

#if defined (_WIN64)
        //
        // Ensure the frame is a 32-bit number.
        //

        if (BitMapIndex != PageFrameIndex) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }
#endif
            
        //
        // Frames past the end of the bitmap are not allowed.
        //

        if (BitMapIndex >= BitMap->SizeOfBitMap) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }

        //
        // Frames not in the bitmap are not allowed.
        //

        if (MI_CHECK_BIT (BitBuffer, BitMapIndex) == 0) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }

        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));

        PagesProcessed += 1;

        ASSERT64 (PageFrameIndex < _4gb);

        MI_CLEAR_BIT (BitBuffer, BitMapIndex);

        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

        ASSERT (MI_PFN_IS_AWE (Pfn1));
        ASSERT (Pfn1->u4.AweAllocation == 1);
        ASSERT (Pfn1->u2.ShareCount == 1);

        //
        // If the frame is currently mapped in a Vad then the PTE must
        // be cleared and the TB entry flushed.
        //

        PointerPte = Pfn1->PteAddress;

        if (PointerPte != NULL) {

            //
            // Note the exclusive hold of the AWE pushlock prevents
            // any other concurrent threads from mapping or unmapping
            // right now.  This also eliminates the need to update the PFN
            // PteAddress with an interlocked sequence as well.
            //

            Pfn1->PteAddress = NULL;

            OldPteContents = *PointerPte;
    
            ASSERT (OldPteContents.u.Hard.Valid == 1);

            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] =
                    MiGetVirtualAddressMappedByPte (PointerPte);
                PteFlushList.Count += 1;
            }

            MI_WRITE_ZERO_PTE_NO_LOGGING (PointerPte);
        }

        MI_SET_PFN_DELETED (Pfn1);

        MdlPage += 1;
    }

    //
    // Flush the TB entries for any relevant pages.
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

    //
    // Free the actual pages (this may be a partially filled MDL).
    //

    PagesInMdl = MdlPage - (PPFN_NUMBER)(MemoryDescriptorList + 1);

    //
    // Set the ByteCount to the actual number of validated pages - the caller
    // may have lied and we have to sync up here to account for any bogus
    // frames.
    //

    MemoryDescriptorList->ByteCount = (ULONG)(PagesInMdl << PAGE_SHIFT);

    if (PagesInMdl != 0) {

        AweInfo->VadPhysicalPages -= PagesInMdl;

        UNLOCK_ADDRESS_SPACE (Process);

        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, 0 - PagesInMdl);

        MmFreePagesFromMdl (MemoryDescriptorList);

        if (Process->JobStatus & PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                    -(SSIZE_T)PagesInMdl);
        }
    }
    else {
        UNLOCK_ADDRESS_SPACE (Process);
    }

    CapturedNumberOfPages -= PagesInMdl;

    if ((Status == STATUS_SUCCESS) && (CapturedNumberOfPages != 0)) {

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            Attached = FALSE;
        }

        OnePassComplete = TRUE;
        ASSERT (MdlPages == PagesInMdl);
#if defined(_AMD64_)
        if (PsGetCurrentProcess()->Wow64Process != NULL)
            UserPfnArray = (PULONG_PTR)((PULONG)UserPfnArray + MdlPages);
        else
#endif
            UserPfnArray += MdlPages;

        //
        // Do it all again until all the pages are freed or an error occurs.
        //

        goto repeat;
    }

    //
    // Fall through.
    //

ErrorReturn:

    //
    // Free any pool acquired for holding MDLs.
    //

    if (MemoryDescriptorList != (PMDL)&MdlHack[0]) {
        ExFreePool (MemoryDescriptorList);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    //
    // Establish an exception handler and carefully write out the
    // number of pages actually processed.
    //

    try {

        *NumberOfPages = PagesProcessed;

    } except (EXCEPTION_EXECUTE_HANDLER) {

        //
        // Return success at this point even if the results
        // cannot be written.
        //

        NOTHING;
    }

    if (ProcessReferenced == TRUE) {
        ObDereferenceObject (Process);
    }

    return Status;
}


VOID
MiRemoveUserPhysicalPagesVad (
    IN PMMVAD_SHORT Vad
    )

/*++

Routine Description:

    This function removes the user-physical-pages mapped region from the
    current process's address space.  This mapped region is private memory.

    The physical pages of this Vad are unmapped here, but not freed.

    Pagetable pages are freed and their use/commitment counts/quotas are
    managed by our caller.

Arguments:

    Vad - Supplies the VAD which manages the address space.

Return Value:

    None.

Environment:

    APC level, address creation mutex held.

--*/

{
    PMMPFN Pfn1;
    PEPROCESS Process;
    PFN_NUMBER PageFrameIndex;
    MMPTE_FLUSH_LIST PteFlushList;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PMMPTE EndingPte;
    PAWEINFO AweInfo;
    PKTHREAD CurrentThread;
#if DBG
    ULONG_PTR ActualPages;
    ULONG_PTR ExpectedPages;
    PMI_PHYSICAL_VIEW PhysicalView;
    PVOID RestartKey;
#endif

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    ASSERT (Vad->u.VadFlags.VadType == VadAwe);

    Process = PsGetCurrentProcess ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo != NULL);

    //
    // If the physical pages count is zero, nothing needs to be done.
    // On checked systems, verify the list anyway.
    //

#if DBG
    ActualPages = 0;
    ExpectedPages = AweInfo->VadPhysicalPages;
#else
    if (AweInfo->VadPhysicalPages == 0) {
        return;
    }
#endif

    PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
    EndingPte = MiGetPteAddress (MI_VPN_TO_VA_ENDING (Vad->EndingVpn));

    PteFlushList.Count = 0;
    
    //
    // The caller must have removed this Vad from the physical view list,
    // otherwise another thread could immediately remap pages back into this
    // same Vad.
    //

    CurrentThread = KeGetCurrentThread ();

    KeEnterGuardedRegionThread (CurrentThread);

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

#if DBG

    RestartKey = NULL;

    do {

        PhysicalView = (PMI_PHYSICAL_VIEW) MiEnumerateGenericTableWithoutSplayingAvl (&AweInfo->AweVadRoot, &RestartKey);

        if (PhysicalView == NULL) {
            break;
        }

        ASSERT (PhysicalView->Vad != (PMMVAD)Vad);

    } while (TRUE);

#endif

    while (PointerPte <= EndingPte) {
        PteContents = *PointerPte;
        if (PteContents.u.Hard.Valid == 0) {
            PointerPte += 1;
            continue;
        }

        //
        // The frame is currently mapped in this Vad so the PTE must
        // be cleared and the TB entry flushed.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));

        ASSERT (ExpectedPages != 0);

        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

        ASSERT (MI_PFN_IS_AWE (Pfn1));
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->PteAddress == PointerPte);

        //
        // Note the PFN lock is not needed here because we have acquired
        // the pushlock exclusive so no one can be mapping or unmapping
        // right now.  In fact, the PFN PteAddress doesn't even have to be
        // updated with an interlocked sequence because the pushlock is held
        // exclusive.
        //

        Pfn1->PteAddress = NULL;

        if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushVa[PteFlushList.Count] =
                MiGetVirtualAddressMappedByPte (PointerPte);
            PteFlushList.Count += 1;
        }

        MI_WRITE_ZERO_PTE_NO_LOGGING (PointerPte);

        PointerPte += 1;
#if DBG
        ActualPages += 1;
#endif
        ASSERT (ActualPages <= ExpectedPages);
    }

    //
    // Flush the TB entries for any relevant pages.
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

    KeLeaveGuardedRegionThread (CurrentThread);

    return;
}

VOID
MiCleanPhysicalProcessPages (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine frees the VadPhysicalBitMap, any remaining physical pages (as
    they may not have been currently mapped into any Vads) and returns the
    bitmap quota.

Arguments:

    Process - Supplies the process to clean.

Return Value:

    None.

Environment:

    Kernel mode, APC level, address space mutex held.
    Called only on process exit, so the AWE push lock is not needed here.

--*/

{
    PMMPFN Pfn1;
    PAWEINFO AweInfo;
    ULONG BitMapSize;
    ULONG BitMapIndex;
    ULONG BitMapHint;
    PRTL_BITMAP BitMap;
    PPFN_NUMBER MdlPage;
    PFN_NUMBER MdlHack[(sizeof(MDL) / sizeof(PFN_NUMBER)) + VERY_SMALL_COPY_STACK_SIZE];
    ULONG_PTR MdlPages;
    ULONG_PTR NumberOfPages;
    ULONG_PTR TotalFreedPages;
    PMDL MemoryDescriptorList;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER HighestPossiblePhysicalPage;
#if DBG
    ULONG_PTR ActualPages = 0;
    ULONG_PTR ExpectedPages = 0;
#endif

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo != NULL);

    TotalFreedPages = 0;
    BitMap = AweInfo->VadPhysicalPagesBitMap;

    if (BitMap == NULL) {
        goto Finish;
    }

#if DBG
    ExpectedPages = AweInfo->VadPhysicalPages;
#else
    if (AweInfo->VadPhysicalPages == 0) {
        goto Finish;
    }
#endif

    MdlPages = VERY_SMALL_COPY_STACK_SIZE;
    MemoryDescriptorList = (PMDL)&MdlHack[0];

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = 0;
    
    BitMapHint = 0;

    while (TRUE) {

        BitMapIndex = RtlFindSetBits (BitMap, 1, BitMapHint);

        if (BitMapIndex < BitMapHint) {
            break;
        }

        if (BitMapIndex == NO_BITS_FOUND) {
            break;
        }

        PageFrameIndex = MI_BITMAP_INDEX_TO_FRAME(BitMapIndex);

        ASSERT64 (PageFrameIndex < _4gb);

        //
        // The bitmap search wraps, so handle it here.
        // Note PFN 0 is illegal.
        //

        ASSERT (PageFrameIndex != 0);
        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));

        ASSERT (ExpectedPages != 0);
        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
        ASSERT (Pfn1->u4.AweAllocation == 1);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->PteAddress == NULL);

        ASSERT (MI_PFN_IS_AWE (Pfn1));

        MI_SET_PFN_DELETED(Pfn1);

        *MdlPage = PageFrameIndex;
        MdlPage += 1;
        NumberOfPages += 1;
#if DBG
        ActualPages += 1;
#endif

        if (NumberOfPages == VERY_SMALL_COPY_STACK_SIZE) {

            //
            // Free the pages in the full MDL.
            //

            MmInitializeMdl (MemoryDescriptorList,
                             0,
                             NumberOfPages << PAGE_SHIFT);

            MmFreePagesFromMdl (MemoryDescriptorList);

            MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            AweInfo->VadPhysicalPages -= NumberOfPages;
            TotalFreedPages += NumberOfPages;
            NumberOfPages = 0;
        }

        BitMapHint = BitMapIndex + 1;
        if (BitMapHint >= BitMap->SizeOfBitMap) {
            break;
        }
    }

    //
    // Free any straggling MDL pages here.
    //

    if (NumberOfPages != 0) {
        MmInitializeMdl (MemoryDescriptorList,
                         0,
                         NumberOfPages << PAGE_SHIFT);

        MmFreePagesFromMdl (MemoryDescriptorList);
        AweInfo->VadPhysicalPages -= NumberOfPages;
        TotalFreedPages += NumberOfPages;
    }

Finish:

    ASSERT (ExpectedPages == ActualPages);

    HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;

#if defined (_WIN64)
    //
    // Force a 32-bit maximum on any page allocation because the bitmap
    // package is currently 32-bit.
    //

    if (HighestPossiblePhysicalPage + 1 >= _4gb) {
        HighestPossiblePhysicalPage = _4gb - 2;
    }
#endif

    ASSERT (AweInfo->VadPhysicalPages == 0);

    if (BitMap != NULL) {
        BitMapSize = sizeof(RTL_BITMAP) + (ULONG)((((HighestPossiblePhysicalPage + 1) + 31) / 32) * 4);

        ExFreePool (BitMap);
        PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
    }

    ExFreeCacheAwarePushLock (AweInfo->PushLock);
    ExFreePool (AweInfo);

    Process->AweInfo = NULL;

    ASSERT (ExpectedPages == ActualPages);

    if (TotalFreedPages != 0) {
        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, 0 - TotalFreedPages);
        if (Process->JobStatus & PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                    -(SSIZE_T)TotalFreedPages);
        }
    }

    return;
}

VOID
MiAweViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This function inserts a new AWE or large page view into the specified
    process' AWE chain.

Arguments:

    Process - Supplies the process to add the AWE VAD to.

    PhysicalView - Supplies the physical view data to link in.

Return Value:

    TRUE if the view was inserted, FALSE if not.

Environment:

    Kernel mode.  APC_LEVEL, address space mutex held.

--*/

{
    PAWEINFO AweInfo;

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo != NULL);

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    MiInsertNode ((PMMADDRESS_NODE)PhysicalView, &AweInfo->AweVadRoot);

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);
}

VOID
MiAweViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function removes an AWE or large page Vad from the specified
    process' AWE chain.

Arguments:

    Process - Supplies the process to remove the AWE VAD from.

    Vad - Supplies the Vad to remove.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, address space mutex held.

--*/

{
    PAWEINFO AweInfo;
    PMI_PHYSICAL_VIEW AweView;
    TABLE_SEARCH_RESULT SearchResult;

    AweInfo = (PAWEINFO) Process->AweInfo;
    ASSERT (AweInfo != NULL);

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    //
    // Lookup the element and save the result.
    //

    SearchResult = MiFindNodeOrParent (&AweInfo->AweVadRoot,
                                       Vad->StartingVpn,
                                       (PMMADDRESS_NODE *) &AweView);

    ASSERT (SearchResult == TableFoundNode);
    ASSERT (AweView->Vad == Vad);

    MiRemoveNode ((PMMADDRESS_NODE)AweView, &AweInfo->AweVadRoot);

    if ((AweView->VadType == VadAwe) ||
        (AweView->VadType == VadLargePages)) {

        RtlZeroMemory (&AweInfo->PhysicalViewHint,
                       MAXIMUM_PROCESSORS * sizeof(PMI_PHYSICAL_VIEW));
    }

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

    ExFreePool (AweView);

    return;
}

VOID
MiReturnLargePages (
    IN PMI_LARGEPAGE_MEMORY_RUN LargePageListHead
    )
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER NewPage;
    PFN_NUMBER ChunkSize;
    PMI_LARGEPAGE_MEMORY_RUN LargePageInfo;
    LOGICAL FlushTbNeeded;

    while (LargePageListHead != NULL) {

        LargePageInfo = LargePageListHead;

        LargePageListHead = LargePageListHead->Next;

        NewPage = LargePageInfo->BasePage;
        ChunkSize = LargePageInfo->PageCount;
        ASSERT (ChunkSize != 0);

        FlushTbNeeded = FALSE;

        Pfn1 = MI_PFN_ELEMENT (LargePageInfo->BasePage);
        LOCK_PFN (OldIrql);

        MI_INCREMENT_RESIDENT_AVAILABLE (ChunkSize, MM_RESAVAIL_FREE_LARGE_PAGES);

        do {
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);
            ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
            ASSERT (Pfn1->u3.e1.PrototypePte == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
            ASSERT (Pfn1->u4.VerifierAllocation == 0);
            ASSERT (Pfn1->u4.AweAllocation == 1);

            //
            // The most likely case is that the pages will be reused with
            // a fully cached attribute.  Convert the pages now (and flush
            // the TB) to avoid having to flush the TB as each page gets
            // reallocated.
            //

            if (Pfn1->u3.e1.CacheAttribute != MiCached) {
                FlushTbNeeded = TRUE;
            }

            Pfn1->u3.e1.CacheAttribute = MiCached;
            Pfn1->u3.e1.StartOfAllocation = 0;
            Pfn1->u3.e1.EndOfAllocation = 0;

            Pfn1->u3.e2.ReferenceCount = 0;

#if DBG
            Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif

            MiInsertPageInFreeList (NewPage);

            Pfn1 += 1;
            NewPage += 1;
            ChunkSize -= 1;

        } while (ChunkSize != 0);

        if (FlushTbNeeded == TRUE) {
            MI_FLUSH_TB_FOR_CACHED_ATTRIBUTE ();
            FlushTbNeeded = FALSE;
        }

        UNLOCK_PFN (OldIrql);

        ExFreePool (LargePageInfo);
    }
    return;
}

PMI_LARGEPAGE_MEMORY_RUN
MiAllocateLargeZeroPages (
    IN PFN_NUMBER NumberOfPages,
    IN MM_PROTECTION_MASK ProtectionMask
    )

/*++

Routine Description:

    This routine allocates contiguous physical memory and ensures it is zero
    filled on return.  The caller must map it into the relevant virtual
    address space.

Arguments:

    NumberOfPages - Supplies the number of pages to allocate.

    ProtectionMask - Supplies the protection mask the caller will map the pages 
                     with.

Return Value:

    A pointer to a list of large page ranges on success, NULL on failure.

Environment:

    Kernel mode, AddressCreation mutex MAY be held.

--*/

{
    PMI_LARGEPAGE_MEMORY_RUN LargePageInfo;
    PMI_LARGEPAGE_MEMORY_RUN LargePageListHead;
    PFN_NUMBER ZeroCount;
    PFN_NUMBER ZeroSize;
    ULONG Color;
    PCOLORED_PAGE_INFO ColoredPageInfoBase;
    ULONG i;
    PFN_NUMBER PageFrameIndexLarge;
    PFN_NUMBER ChunkSize;
    PFN_NUMBER PagesSoFar;
    PFN_NUMBER PagesLeft;

    i = 3;
    ChunkSize = NumberOfPages;
    PagesSoFar = 0;
    LargePageInfo = NULL;
    LargePageListHead = NULL;
    ZeroCount = 0;

    //
    // Allocate a list of colored anchors.
    //

    ColoredPageInfoBase = (PCOLORED_PAGE_INFO) ExAllocatePoolWithTag (
                                NonPagedPool,
                                MmSecondaryColors * sizeof (COLORED_PAGE_INFO),
                                'ldmM');

    if (ColoredPageInfoBase == NULL) {
        return NULL;
    }

    for (Color = 0; Color < MmSecondaryColors; Color += 1) {
        ColoredPageInfoBase[Color].PagesQueued = 0;
        ColoredPageInfoBase[Color].PfnAllocation = (PMMPFN) MM_EMPTY_LIST;
        ColoredPageInfoBase[Color].PagesQueued = 0;
    }

    //
    // Try for the actual contiguous memory.
    //

    MmLockPageableSectionByHandle (ExPageLockHandle);

    InterlockedIncrement (&MiDelayPageFaults);

    do {
        
        ASSERT (i <= 3);

        if (LargePageInfo == NULL) {
            LargePageInfo = ExAllocatePoolWithTag (NonPagedPool,
                                                   sizeof (MI_LARGEPAGE_MEMORY_RUN),
                                                   'lLmM');
    
            if (LargePageInfo == NULL) {
                PageFrameIndexLarge = 0;
                break;
            }
        }

        PageFrameIndexLarge = MiFindLargePageMemory (ColoredPageInfoBase,
                                                     ChunkSize,
                                                     ProtectionMask,
                                                     &ZeroSize);

        if (PageFrameIndexLarge != 0) {

            //
            // Save the start and length of each run for subsequent
            // zeroing and PDE filling.
            //

            LargePageInfo->BasePage = PageFrameIndexLarge;
            LargePageInfo->PageCount = ChunkSize;

            LargePageInfo->Next = LargePageListHead;
            LargePageListHead = LargePageInfo;

            LargePageInfo = NULL;

            ASSERT (ZeroSize <= ChunkSize);
            ZeroCount += ZeroSize;

            ASSERT ((ChunkSize == NumberOfPages) || (i == 0));

            PagesSoFar += ChunkSize;

            if (PagesSoFar == NumberOfPages) {
                break;
            }
            else { 
                ASSERT (NumberOfPages > PagesSoFar);
                PagesLeft = NumberOfPages - PagesSoFar;
                
                if (ChunkSize > PagesLeft) {
                    ChunkSize = PagesLeft;
                }
            }

            continue;
        }

        switch (i) {

            case 3:

                MmEmptyAllWorkingSets ();
#if DBG
                if (MiShowStuckPages != 0) {
                    MiFlushAllPages ();
                    KeDelayExecutionThread (KernelMode,
                                            FALSE,
                                            (PLARGE_INTEGER)&MmHalfSecond);
                }
#endif
                i -= 1;
                break;

            case 2:
#if DBG
                if (MiShowStuckPages != 0) {
                    MmEmptyAllWorkingSets ();
                }
#endif
                MiFlushAllPages ();
                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&MmHalfSecond);
                i -= 1;
                break;

            case 1:
                MmEmptyAllWorkingSets ();
                MiFlushAllPages ();
                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&MmOneSecond);
                i -= 1;
                break;

            case 0:

                //
                // Halve the request size.  If needed, then round down
                // to the next page directory multiple.  Then retry.
                //

                ChunkSize >>= 1;
                ChunkSize &= ~((MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT) - 1);

                break;
        }

        if (ChunkSize < (MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT)) {
            ASSERT (i == 0);
            break;
        }

    } while (TRUE);

    InterlockedDecrement (&MiDelayPageFaults);

    if (LargePageInfo != NULL) {
        ExFreePool (LargePageInfo);
        LargePageInfo = NULL;
    }

    if (PageFrameIndexLarge == 0) {

        //
        // The entire region could not be allocated.
        // Free any large page subchunks that might have been allocated.
        //

        MiReturnLargePages (LargePageListHead);
        LargePageListHead = NULL;
    }
    else if (ZeroCount != 0) {

        //
        // Zero all the free & standby pages, fanning out the work.  This
        // is done even on UP machines because the worker thread code maps
        // large MDLs and is thus better performing than zeroing a single
        // page at a time.
        //

        MiZeroInParallel (ColoredPageInfoBase);

        //
        // Denote that no pages are left to be zeroed because in addition
        // to zeroing them, we have reset all their OriginalPte fields
        // to demand zero so they cannot be walked by the zeroing loop
        // below.
        //

        ZeroCount = 0;
    }

    //
    // Return the now zeroed pages to the caller.
    //

    ExFreePool (ColoredPageInfoBase);

    MmUnlockPageableImageSection (ExPageLockHandle);

    return LargePageListHead;
}
NTSTATUS
MiAllocateLargePages (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN MM_PROTECTION_MASK ProtectionMask,
    IN LOGICAL CallerHasPages
    )

/*++

Routine Description:

    This routine allocates contiguous physical memory and then initializes
    page directory and page table pages to map it with large pages.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    ProtectionMask - Supplies the protection mask the caller will map the
                     pages with.

    CallerHasPages - Supplies TRUE if the caller already has pages and will
                     fill in the page directory entries on return.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APCs disabled, AddressCreation mutex held.

--*/

{
    PFN_NUMBER PdeFrame;
    PMI_LARGEPAGE_MEMORY_RUN LargePageInfo;
    PMI_LARGEPAGE_MEMORY_RUN LargePageListHead;
    PFN_NUMBER ZeroCount;
    PMMPFN Pfn1;
    PMMPFN EndPfn;
    LOGICAL ChargedJob;
    ULONG i;
    PAWEINFO AweInfo;
    MMPTE TempPde;
    PETHREAD Thread;
    PEPROCESS Process;
    SIZE_T NumberOfBytes;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER ChunkSize;
    PFN_NUMBER PagesSoFar;
    PMMPTE LastPde;
    PMMPTE LastPpe;
    PMMPTE LastPxe;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
#if (_MI_PAGING_LEVELS >= 3)
    KIRQL OldIrql;
    PFN_NUMBER PagesNeeded;
    MMPTE PteContents;
    PVOID UsedPageTableHandle;
#endif

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    NumberOfBytes = (PCHAR)EndingAddress + 1 - (PCHAR)StartingAddress;

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    ChargedJob = FALSE;

    Thread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (Thread);

    AweInfo = (PAWEINFO) Process->AweInfo;

    if (CallerHasPages == FALSE) {

        if (AweInfo->VadPhysicalPagesLimit != 0) {

            if (AweInfo->VadPhysicalPages >= AweInfo->VadPhysicalPagesLimit) {
                return STATUS_COMMITMENT_LIMIT;
            }
    
            if (NumberOfPages > AweInfo->VadPhysicalPagesLimit - AweInfo->VadPhysicalPages) {
                return STATUS_COMMITMENT_LIMIT;
            }
    
            ASSERT ((AweInfo->VadPhysicalPages + NumberOfPages <= AweInfo->VadPhysicalPagesLimit) || (AweInfo->VadPhysicalPagesLimit == 0));
    
            if (Process->JobStatus & PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES) {
    
                if (PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                            NumberOfPages) == FALSE) {
    
                    return STATUS_COMMITMENT_LIMIT;
                }
                ChargedJob = TRUE;
            }
        }

        AweInfo->VadPhysicalPages += NumberOfPages;
    }

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    LastPxe = MiGetPxeAddress (EndingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Charge resident available pages for all of the page directory
    // pages as they will not be paged until the VAD is freed.
    //
    // Note that commitment is not charged here because the VAD insertion
    // charges commit for the entire paging hierarchy (including the
    // nonexistent page tables).
    //

    PagesNeeded = LastPpe - PointerPpe + 1;

#if (_MI_PAGING_LEVELS >= 4)
    PagesNeeded += LastPxe - PointerPxe + 1;
#endif

    ASSERT (PagesNeeded != 0);

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)PagesNeeded > MI_NONPAGEABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);

        if (CallerHasPages == FALSE) {
            ASSERT (AweInfo->VadPhysicalPages >= NumberOfPages);
            AweInfo->VadPhysicalPages -= NumberOfPages;

            if (ChargedJob == TRUE) {
                PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                        -(SSIZE_T)NumberOfPages);
            }
        }

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_ALLOCATE_USER_PAGE_TABLE);

    UNLOCK_PFN (OldIrql);

#endif

    //
    // Try for the actual contiguous memory.
    //

    SATISFY_OVERZEALOUS_COMPILER (LargePageListHead = NULL);

    if (CallerHasPages == FALSE) {

        i = 3;
        ChunkSize = NumberOfPages;
        PagesSoFar = 0;
        LargePageInfo = NULL;
        ZeroCount = 0;

        LargePageListHead = MiAllocateLargeZeroPages (NumberOfPages, ProtectionMask);
    
        if (LargePageListHead == NULL) {
    
            //
            // The entire region could not be allocated.
            //
    
#if (_MI_PAGING_LEVELS >= 3)
            if (PagesNeeded != 0) {
                MI_INCREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_FREE_USER_PAGE_TABLE);
            }
#endif
            ASSERT (AweInfo->VadPhysicalPages >= NumberOfPages);
            AweInfo->VadPhysicalPages -= NumberOfPages;
    
            if (ChargedJob == TRUE) {
                PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                        -(SSIZE_T)NumberOfPages);
            }
    
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

#if (_MI_PAGING_LEVELS >= 3)

    LOCK_WS_UNSAFE (Thread, Process);

    while (PointerPpe <= LastPpe) {

        //
        // Pointing to the next page directory page, make
        // it exist and make it valid.
        //
        // Note this ripples sharecounts through the paging hierarchy so
        // there is no need to up sharecounts to prevent trimming of the
        // page directory parent page as making the page directory
        // valid below does this automatically.
        //

        MiMakePdeExistAndMakeValid (PointerPpe, Process, MM_NOIRQL);

        //
        // Up the sharecount so the page directory page will not get
        // trimmed even if it has no currently valid entries.
        //

        PteContents = *PointerPpe;
        ASSERT (PteContents.u.Hard.Valid == 1);
        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
        LOCK_PFN (OldIrql);
        Pfn1->u2.ShareCount += 1;
        UNLOCK_PFN (OldIrql);

        UsedPageTableHandle = (PVOID) Pfn1;

        //
        // Increment the count of non-zero page directory entries
        // for this page directory - even though this entry is still zero,
        // this is a special case.
        //

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        PointerPpe += 1;
    }

    UNLOCK_WS_UNSAFE (Thread, Process);

#endif

    if (CallerHasPages == FALSE) {

        //
        // Map the now zeroed pages into the caller's user address space.
        //
    
        MI_MAKE_VALID_PTE (TempPde,
                           0,
                           ProtectionMask,
                           MiGetPteAddress (StartingAddress));
    
        MI_SET_PTE_DIRTY (TempPde);
        MI_SET_ACCESSED_IN_PTE (&TempPde, 1);
    
        MI_MAKE_PDE_MAP_LARGE_PAGE (&TempPde);
    
        while (LargePageListHead != NULL) {
    
            LargePageInfo = LargePageListHead;
            LargePageListHead = LargePageListHead->Next;
    
            TempPde.u.Hard.PageFrameNumber = LargePageInfo->BasePage;
    
            ChunkSize = LargePageInfo->PageCount;
            ASSERT (ChunkSize != 0);
    
            //
            // Initialize each page directory page.  Acquire the
            // working set pushlock exclusive to prevent races with
            // concurrent MmProbeAndLockPages calls.
            //
        
            LastPde = PointerPde + (ChunkSize / (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT));
    
            Pfn1 = MI_PFN_ELEMENT (LargePageInfo->BasePage);
            EndPfn = Pfn1 + ChunkSize;
    
            ASSERT (MiGetPteAddress (PointerPde)->u.Hard.Valid == 1);
            PdeFrame = (PFN_NUMBER) (MiGetPteAddress (PointerPde)->u.Hard.PageFrameNumber);
    
            LOCK_WS_UNSAFE (Thread, Process);
        
            do {
                ASSERT (Pfn1->u4.AweAllocation == 1);
                Pfn1->AweReferenceCount = 1;
                Pfn1->PteAddress = PointerPde;      // Point at the allocation base
                MI_SET_PFN_DELETED (Pfn1);
                Pfn1->u4.PteFrame = PdeFrame;       // Point at the allocation base
                Pfn1 += 1;
            } while (Pfn1 < EndPfn);
    
    
            while (PointerPde < LastPde) {
        
                ASSERT (PointerPde->u.Long == 0);
        
                MI_WRITE_VALID_PTE (PointerPde, TempPde);
        
                TempPde.u.Hard.PageFrameNumber += (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);
        
                PointerPde += 1;
            }
    
            UNLOCK_WS_UNSAFE (Thread, Process);
    
            ExFreePool (LargePageInfo);
        }
    }

    return STATUS_SUCCESS;
}

VOID
MiReleasePhysicalCharges (
    IN SIZE_T NumberOfPages,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine releases AWE and job charges.

Arguments:

    NumberOfPages - Supplies the number of pages being removed.

    Process - Supplies the owning process.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, AddressCreation mutex held.

--*/

{
    PAWEINFO AweInfo;

    ASSERT (Process == PsGetCurrentProcess ());

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo->VadPhysicalPages >= NumberOfPages);

    AweInfo->VadPhysicalPages -= NumberOfPages;

    if (Process->JobStatus & PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES) {
        PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_PHYSICAL_PAGE_CHANGES,
                                -(SSIZE_T)NumberOfPages);
    }

    return;
}


VOID
MiFreeLargePages (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN LOGICAL CallerHadPages
    )

/*++

Routine Description:

    This routine deletes page directory and page table pages for a
    user-controlled large page range.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    CallerHadPages - Supplies TRUE if the caller already had pages and
                     filled in the page directory entries himself.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPde;
    PMMPTE LastPde;
    MMPTE PteContents;
    PEPROCESS CurrentProcess;
    PVOID UsedPageTableHandle;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPFN Pfn1;
    PMMPTE LastPpe;
    PMMPTE LastPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PFN_NUMBER PagesNeeded;
    PVOID TempVa;
#endif

    CurrentProcess = PsGetCurrentProcess ();

    PointerPde = MiGetPdeAddress (StartingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);

#if (_MI_PAGING_LEVELS >= 3)

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    LastPxe = MiGetPxeAddress (EndingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);

    //
    // Return resident available pages for all of the page directory
    // pages as they can now be paged again.
    //
    // Note that commitment is not returned here because the VAD release
    // returns commit for the entire paging hierarchy (including the
    // nonexistent page tables).
    //

    PagesNeeded = LastPpe - PointerPpe + 1;

#if (_MI_PAGING_LEVELS >= 4)
    PagesNeeded += LastPxe - PointerPxe + 1;
#endif

    ASSERT (PagesNeeded != 0);

#endif

    //
    // Delete the range mapped by each page directory page.
    //

    while (PointerPde <= LastPde) {

        PteContents = *PointerPde;

        ASSERT (PteContents.u.Hard.Valid == 1);
        ASSERT (MI_PDE_MAPS_LARGE_PAGE (&PteContents) == 1);

        PageFrameIndex = (PFN_NUMBER) PteContents.u.Hard.PageFrameNumber;

        ASSERT ((PageFrameIndex != 0) || (CallerHadPages == TRUE));

        LOCK_PFN (OldIrql);

        MI_WRITE_ZERO_PTE (PointerPde);

        UNLOCK_PFN (OldIrql);

        //
        // Flush the mapping so the pages can be immediately reused
        // without any possibility of conflicting TB entries.
        //

        MI_FLUSH_PROCESS_TB (FALSE);

        if (CallerHadPages == FALSE) {
            MiFreeLargePageMemory (PageFrameIndex,
                                   MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);
        }

        PointerPde += 1;
    }

#if (_MI_PAGING_LEVELS >= 3)

    LOCK_PFN (OldIrql);

    do {

        //
        // Down the sharecount on the finished page directory page.
        //

        PteContents = *PointerPpe;
        ASSERT (PteContents.u.Hard.Valid == 1);
        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
        ASSERT (Pfn1->u2.ShareCount > 1);
        Pfn1->u2.ShareCount -= 1;

        UsedPageTableHandle = (PVOID) Pfn1;

        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        PointerPpe += 1;

        //
        // If all the entries have been eliminated from the previous
        // page directory page, delete the page directory page itself.
        //

        if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {

            ASSERT ((PointerPpe - 1)->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 4)
            UsedPageTableHandle = (PVOID) MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
            MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

            TempVa = MiGetVirtualAddressMappedByPte (PointerPpe - 1);
            MiDeletePte (PointerPpe - 1,
                         TempVa,
                         FALSE,
                         CurrentProcess,
                         NULL,
                         NULL,
                         OldIrql);

#if (_MI_PAGING_LEVELS >= 4)

            if ((MiIsPteOnPdeBoundary(PointerPpe)) || (PointerPpe > LastPpe)) {

                if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {

                    PointerPxe = MiGetPteAddress (PointerPpe - 1);
                    ASSERT (PointerPxe->u.Long != 0);
                    TempVa = MiGetVirtualAddressMappedByPte (PointerPxe);

                    MiDeletePte (PointerPxe,
                                 TempVa,
                                 FALSE,
                                 CurrentProcess,
                                 NULL,
                                 NULL,
                                 OldIrql);
                }
            }
#endif    
        }

    } while (PointerPpe <= LastPpe);

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_FREE_USER_PAGE_TABLE);

#endif

    return;
}

PFN_NUMBER
MmSetPhysicalPagesLimit (
    IN PFN_NUMBER NewPhysicalPagesLimit
    )

/*++

Routine Description:

    This routine sets a physical page allocation limit for the current process.
    This is the limit of AWE and large page allocations.

    Note the process may already be over the new limit at the time this routine
    is called.  If so, no new AWE or large page allocations will succeed until
    existing allocations are freed such that the process satisfies the
    new limit.

Arguments:

    NewPhysicalPagesLimit - Supplies the new limit to be enforced or zero if the
                            caller is simply querying for an existing limit.

Return Value:

    The physical pages limit in effect upon return from this routine.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PAWEINFO AweInfo;
    PETHREAD Thread;
    PEPROCESS Process;

    Thread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (Thread);

    PAGED_CODE ();

    LOCK_ADDRESS_SPACE (Process);

    AweInfo = (PAWEINFO) Process->AweInfo;

    if (AweInfo != NULL) {
        if (NewPhysicalPagesLimit != 0) {
            AweInfo->VadPhysicalPagesLimit = NewPhysicalPagesLimit;
        }
        else {
            NewPhysicalPagesLimit = AweInfo->VadPhysicalPagesLimit;
        }
    }
    else {
        NewPhysicalPagesLimit = 0;
    }

    UNLOCK_ADDRESS_SPACE (Process);

    return NewPhysicalPagesLimit;
}



NTSTATUS
MiCaptureUlongPtrArray (
    OUT PVOID Destination,
    IN PVOID Source,
    IN ULONG_PTR ArraySize
    )

/*++

Routine Description:

    This routine captures a user-supplied array of ULONG_PTR elements into
    a system-supplied buffer.  In the case of Wow64, the user buffer may
    be supplied as an array of ULONG elements, in which case an unsigned
    conversion to ULONG_PTR is performed for each element.

Arguments:

    Destination - Supplies the system buffer to contain the captured array.

    Source - Supplies the user buffer from which to perform the capture. On
             some platforms, this is an array of ULONG if called from within
             a Wow64 process.

    ArraySize - Supplies the number of elements in the arrays.

Return Value:

    The final status of the operation.

Environment:

    Kernel mode.

--*/

{
    NTSTATUS status;

    status = STATUS_SUCCESS;
    try {

#if defined(_AMD64_)

        if (PsGetCurrentProcess()->Wow64Process != NULL) {

            ULONG_PTR *dst;
            ULONG64 index;
            ULONG_PTR remainingElements;
            ULONG *src;

            dst = (ULONG_PTR *)Destination;
            src = (ULONG *)Source;

            ProbeForRead (src, ArraySize * sizeof(ULONG), sizeof(ULONG));

            //
            // Unroll the copy operation.  The compiler is hesitant to perform
            // much optimization within a try block so help it out as much
            // as possible.
            // 

            index = 0;
            remainingElements = ArraySize & ~7;
            if (remainingElements != 0) {
                do {
                    dst[index + 0]  = src[index + 0];
                    dst[index + 1]  = src[index + 1];
                    dst[index + 2]  = src[index + 2];
                    dst[index + 3]  = src[index + 3];
                    dst[index + 4]  = src[index + 4];
                    dst[index + 5]  = src[index + 5];
                    dst[index + 6]  = src[index + 6];
                    dst[index + 7]  = src[index + 7];
    
                    index += 8;
                } while (index < remainingElements);
            }
    
            if ((ArraySize & 7) != 0) {
                do {
                    dst[index] = src[index];
    
                    index += 1;
                } while (index < ArraySize);
            }

        } else
#endif
        {
            ProbeForRead (Source,
                          ArraySize * sizeof(ULONG_PTR),
                          sizeof(ULONG_PTR));

            RtlCopyMemory (Destination,
                           Source,
                           ArraySize * sizeof(ULONG_PTR));
        }

    } except(EXCEPTION_EXECUTE_HANDLER) {
        status = GetExceptionCode();
    }

    return status;
}


WIN32_PROTECTION_MASK
MiProtectAweRegion (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN MM_PROTECTION_MASK ProtectionMask
    )

/*++

Routine Description:

    This function applies the specified protection on the specified
    AWE user address range.

Arguments:

    StartingAddress - Supplies the starting user virtual address within a
                      UserPhysicalPages Vad.
        
    EndingAddress - Supplies the ending user virtual address within a
                    UserPhysicalPages Vad.
        
    ProtectionMask - Supplies the new protection mask to apply.

Return Value:

    The Win32 protection mask of the argument starting address.

Environment:

    Kernel mode, APC_LEVEL, address space pushlock held.

--*/

{
    PAWEINFO AweInfo;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE NewPteContents;
    PETHREAD CurrentThread;
    WIN32_PROTECTION_MASK CapturedOldProtect;

    //
    // Initialize as much as possible before acquiring any locks.
    //

    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    CurrentThread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (CurrentThread);

    PageFrameIndex = 0;

    MI_MAKE_VALID_USER_PTE (NewPteContents,
                            PageFrameIndex,
                            ProtectionMask,
                            PointerPte);

    if (ProtectionMask == MM_NOACCESS) {
        MI_SET_OWNER_IN_PTE (&NewPteContents, MI_PTE_OWNER_KERNEL);
    }
    else if (ProtectionMask == MM_READWRITE) {
        MI_SET_PTE_DIRTY (NewPteContents);
    }

    PteFlushList.Count = 0;

    //
    // No memory barrier is needed to read the EPROCESS AweInfo field
    // here because the caller has already ensured that this region is
    // a valid AWE region and the address space pushlock is held.
    //

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    ASSERT ((AweInfo != NULL) && (AweInfo->VadPhysicalPagesBitMap != NULL));

    CapturedOldProtect = PAGE_NOACCESS;

    //
    // Pushlock protection protects insertion/removal of Vads into each process'
    // AweVadList.  It also protects creation/deletion and adds/removes
    // of the VadPhysicalPagesBitMap.  Finally, it protects the PFN
    // modifications for pages in the bitmap.
    //

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid) {
        if (PteContents.u.Hard.Owner == MI_PTE_OWNER_USER) {
            if (PteContents.u.Hard.Write == 1) {
                CapturedOldProtect = PAGE_READWRITE;
            }
            else {
                CapturedOldProtect = PAGE_READONLY;
            }
        }
    }

    //
    // Note the PFN lock is not needed because any race with MmProbeAndLockPages
    // can only result in the I/O going to the old page or the new page.
    // If the user breaks the rules, the PFN database (and any pages being
    // windowed here) are still protected because of the reference counts
    // on the pages with inprogress I/O.  This is possible because NO pages
    // are actually freed here - they are just windowed.
    //

    while (PointerPte <= LastPte) {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid) {

            //
            // The frame is currently mapped in this VAD so the TB entry
            // must be flushed.
            //
    
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    
            NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;
    
            //
            // Note the PFN lock is not needed here because we have acquired
            // the pushlock exclusive so no one can be mapping or unmapping
            // right now.
            //
    
            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] =
                    MiGetVirtualAddressMappedByPte (PointerPte);
                PteFlushList.Count += 1;
            }
    
            *PointerPte = NewPteContents;
        }
    
        PointerPte += 1;
    }

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

    //
    // Flush the TB entries for any relevant pages.  Note this can be done
    // without holding the AWE pushlock because the PTEs have already been
    // filled so any concurrent (bogus) map/unmap call will see the right
    // entries.  AND any free of the physical pages will also see the right
    // entries (although the free must do a TB flush while holding the AWE
    // pushlock exclusive to ensure no thread gets to continue using a
    // stale mapping to the page being freed prior to the flush below).
    //

    MiFlushPteList (&PteFlushList);

    return CapturedOldProtect;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\protect.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   protect.c

Abstract:

    This module contains the routines which implement the
    NtProtectVirtualMemory service.

--*/

#include "mi.h"

#if defined (_WIN64)
#define _MI_RESET_USER_STACK_LIMIT 1
#endif

VOID
MiFlushTbAndCapture (
    IN PMMVAD FoundVad,
    IN PMMPTE PtePointer,
    IN ULONG ProtectionMask,
    IN PMMPFN Pfn1,
    IN LOGICAL CaptureDirtyBit
    );

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

extern CCHAR MmReadWrite[32];

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtProtectVirtualMemory)
#pragma alloc_text(PAGE,MiCheckSecuredVad)
#endif


NTSTATUS
NtProtectVirtualMemory(
    __in HANDLE ProcessHandle,
    __inout PVOID *BaseAddress,
    __inout PSIZE_T RegionSize,
    __in WIN32_PROTECTION_MASK NewProtectWin32,
    __out PULONG OldProtect
    )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

    Note if a virtual address is locked in the working set and the
    protection is changed to no access, the page is removed from the
    working set since valid pages can't be no access.

Arguments:

     ProcessHandle - An open handle to a process object.

     BaseAddress - The base address of the region of pages
                   whose protection is to be changed. This value is
                   rounded down to the next host page address
                   boundary.

     RegionSize - A pointer to a variable that will receive
                  the actual size in bytes of the protected region
                  of pages. The initial value of this argument is
                  rounded up to the next host page size boundary.

     NewProtectWin32 - The new protection desired for the specified region of pages.

     Protect Values

          PAGE_NOACCESS - No access to the specified region
                          of pages is allowed. An attempt to read,
                          write, or execute the specified region
                          results in an access violation.

          PAGE_EXECUTE - Execute access to the specified
                         region of pages is allowed. An attempt to
                         read or write the specified region results in
                         an access violation.

          PAGE_READONLY - Read only and execute access to the
                          specified region of pages is allowed. An
                          attempt to write the specified region results
                          in an access violation.

          PAGE_READWRITE - Read, write, and execute access to
                           the specified region of pages is allowed. If
                           write access to the underlying section is
                           allowed, then a single copy of the pages are
                           shared. Otherwise the pages are shared read
                           only/copy on write.

          PAGE_GUARD - Read, write, and execute access to the
                       specified region of pages is allowed,
                       however, access to the region causes a "guard
                       region entered" condition to be raised in the
                       subject process. If write access to the
                       underlying section is allowed, then a single
                       copy of the pages are shared. Otherwise the
                       pages are shared read only/copy on write.

          PAGE_NOCACHE - The page should be treated as uncached.
                         This is only valid for non-shared pages.

          PAGE_WRITECOMBINE - The page should be treated as write-combined.
                              This is only valid for non-shared pages.

     OldProtect - A pointer to a variable that will receive
                  the old protection of the first page within the
                  specified region of pages.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    KAPC_STATE ApcState;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    ULONG Attached = FALSE;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    ULONG ProtectionMask;
    ULONG LastProtect;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (NewProtectWin32);

    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread (&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        //
        // Capture the region size and base address under an exception handler.
        //

        try {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
            ProbeForWriteUlong (OldProtect);

            //
            // Capture the region size and base address.
            //

            CapturedBase = *BaseAddress;
            CapturedRegionSize = *RegionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {

        //
        // Capture the region size and base address.
        //

        CapturedRegionSize = *RegionSize;
        CapturedBase = *BaseAddress;
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                  CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER_3;
    }

    Status = ObReferenceObjectByHandle (ProcessHandle,
                                        PROCESS_VM_OPERATION,
                                        PsProcessType,
                                        PreviousMode,
                                        (PVOID *)&Process,
                                        NULL);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Status = MiProtectVirtualMemory (Process,
                                     &CapturedBase,
                                     &CapturedRegionSize,
                                     NewProtectWin32,
                                     &LastProtect);


    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    ObDereferenceObject (Process);

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *RegionSize = CapturedRegionSize;
        *BaseAddress = CapturedBase;
        *OldProtect = LastProtect;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return Status;
}


NTSTATUS
MiProtectVirtualMemory (
    IN PEPROCESS Process,
    IN PVOID *BaseAddress,
    IN PSIZE_T RegionSize,
    IN WIN32_PROTECTION_MASK NewProtectWin32,
    IN PWIN32_PROTECTION_MASK LastProtect
    )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

Arguments:

    Process - Supplies a pointer to the current process.

    BaseAddress - Supplies the starting address to protect.

    RegionsSize - Supplies the size of the region to protect.

    NewProtectWin32 - Supplies the new protection to set.

    LastProtect - Supplies the address of a kernel owned pointer to
                  store (without probing) the old protection into.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PETHREAD Thread;
    PMMVAD FoundVad;
    PVOID VirtualAddress;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    NTSTATUS Status;
    ULONG Attached;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerProtoPte;
    PMMPTE LastProtoPte;
    PMMPFN Pfn1;
    WIN32_PROTECTION_MASK CapturedOldProtect;
    ULONG ProtectionMask;
    MMPTE PteContents;
    ULONG Locked;
    PVOID Va;
    ULONG DoAgain;
    PVOID UsedPageTableHandle;
    WIN32_PROTECTION_MASK OriginalProtect;
    LOGICAL WsHeld;
#if defined (_MI_RESET_USER_STACK_LIMIT)
    PTEB Teb;
    PVOID StackLimit;
#endif
    Attached = FALSE;
    Locked = FALSE;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time.
    // Get the working set mutex so PTEs can be modified.
    // Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    CapturedBase = *BaseAddress;
    CapturedRegionSize = *RegionSize;
    OriginalProtect = NewProtectWin32;

    //
    // Note the NO_CACHE, write combined and GUARD attributes are mutually
    // exclusive.  MiMakeProtectionMask enforces this.
    //

    ProtectionMask = MiMakeProtectionMask (NewProtectWin32);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    EndingAddress = (PVOID)(((ULONG_PTR)CapturedBase +
                                CapturedRegionSize - 1L) | (PAGE_SIZE - 1L));

    StartingAddress = (PVOID)PAGE_ALIGN(CapturedBase);

#if defined (_MI_RESET_USER_STACK_LIMIT)

    //
    // If a guard page is being set above the current stack limit in the
    // caller's thread stack then he may be trying to recover from a
    // user initiated stack probe growth (ie, inside his exception handler).
    // Reset the stack limit for him automatically so all the broken usermode
    // code doesn't have to be updated to do so.
    //
    // Capture the stack limit (it's in user space) before acquiring any
    // mutexes.
    //

    Teb = NULL;
    StackLimit = 0;

    if ((MI_IS_GUARD (ProtectionMask)) &&
        (!KeIsAttachedProcess ()) &&
        (Process->Wow64Process == NULL)) {

        Teb = KeGetCurrentThread()->Teb;
        try {
            StackLimit = Teb->NtTib.StackLimit;
        } except (EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }

        StackLimit = PAGE_ALIGN (StackLimit);
    }

#endif

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorFound;
    }

    FoundVad = MiCheckForConflictingVad (Process, StartingAddress, EndingAddress);

    if (FoundVad == NULL) {

        //
        // No virtual address is reserved at the specified base address,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    //
    // Ensure that the starting and ending addresses are all within
    // the same virtual address descriptor.
    //

    if ((MI_VA_TO_VPN (StartingAddress) < FoundVad->StartingVpn) ||
        (MI_VA_TO_VPN (EndingAddress) > FoundVad->EndingVpn)) {

        //
        // Not within the section virtual address descriptor,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.VadType == VadLargePages) {

        //
        // These regions are always the same protection throughout.
        //

        if (ProtectionMask == FoundVad->u.VadFlags.Protection) {

            UNLOCK_ADDRESS_SPACE (Process);

            *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
            *BaseAddress = StartingAddress;
            *LastProtect = MI_CONVERT_FROM_PTE_PROTECTION (ProtectionMask);

            return STATUS_SUCCESS;
        }

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.VadType == VadAwe) {

        if ((ProtectionMask == MM_NOACCESS) ||
            (ProtectionMask == MM_READONLY) ||
            (ProtectionMask == MM_READWRITE)) {

            CapturedOldProtect = MiProtectAweRegion (StartingAddress,
                                                     EndingAddress,
                                                     ProtectionMask);

            UNLOCK_ADDRESS_SPACE (Process);

            *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
            *BaseAddress = StartingAddress;
            *LastProtect = CapturedOldProtect;

            return STATUS_SUCCESS;
        }

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.VadType == VadDevicePhysicalMemory) {

        //
        // Setting the protection of a physically mapped section is
        // not allowed as there is no corresponding PFN database element.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is made at changing the protection
        // of a secured VAD, check to see if the address range
        // to change allows the change.
        //

        Status = MiCheckSecuredVad (FoundVad,
                                    CapturedBase,
                                    CapturedRegionSize,
                                    ProtectionMask);

        if (!NT_SUCCESS (Status)) {
            goto ErrorFound;
        }
    }

    if (FoundVad->u.VadFlags.PrivateMemory == 0) {

        if (FoundVad->u.VadFlags.VadType == VadLargePageSection) {

            //
            // For now, these regions do not allow protection changes since
            // they are mapped by large pages.  We can consider whether we
            // want to split the pages into small mappings to provide individual
            // protections.
            //

            if (ProtectionMask == FoundVad->u.VadFlags.Protection) {

                UNLOCK_ADDRESS_SPACE (Process);

                *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
                *BaseAddress = StartingAddress;
                *LastProtect = MI_CONVERT_FROM_PTE_PROTECTION (ProtectionMask);

                return STATUS_SUCCESS;
            }

            Status = STATUS_CONFLICTING_ADDRESSES;
            goto ErrorFound;
        }

        //
        // For mapped sections, the NO_CACHE and write combined attributes
        // are not allowed.
        //

        //
        // PAGE_WRITECOPY is not valid for private pages and rotate physical
        // sections are private pages from the application's viewpoint.
        //

        if (FoundVad->u.VadFlags.VadType == VadRotatePhysical) {

            if ((NewProtectWin32 & PAGE_WRITECOPY) ||
                (NewProtectWin32 & PAGE_EXECUTE_WRITECOPY) ||
                (NewProtectWin32 & PAGE_NOACCESS) ||
                (NewProtectWin32 & PAGE_GUARD)) {

                Status = STATUS_INVALID_PAGE_PROTECTION;
                goto ErrorFound;
            }
        }
        else {
                
            if (NewProtectWin32 & (PAGE_NOCACHE | PAGE_WRITECOMBINE)) {

                //
                // Not allowed.
                //

                Status = STATUS_INVALID_PARAMETER_4;
                goto ErrorFound;
            }

            //
            // Make sure the section page protection is compatible with
            // the specified page protection.
            //
    
            if ((FoundVad->ControlArea->u.Flags.Image == 0) &&
                (!MiIsPteProtectionCompatible ((MM_PROTECTION_MASK)FoundVad->u.VadFlags.Protection,
                                               OriginalProtect))) {
                Status = STATUS_SECTION_PROTECTION;
                goto ErrorFound;
            }
        }

        //
        // If this is a file mapping, then all pages must be
        // committed as there can be no sparse file maps. Images
        // can have non-committed pages if the alignment is greater
        // than the page size.
        //

        if ((FoundVad->ControlArea->u.Flags.File == 0) ||
            (FoundVad->ControlArea->u.Flags.Image == 1)) {

            VirtualAddress = MI_VPN_TO_VA (FoundVad->StartingVpn);

            if ((FoundVad->u.VadFlags.VadType == VadImageMap)
#if (_MI_PAGING_LEVELS>=4)
                && (MiGetPxeAddress(VirtualAddress)->u.Hard.Valid == 1)
#endif
#if (_MI_PAGING_LEVELS>=3)
                && (MiGetPpeAddress(VirtualAddress)->u.Hard.Valid == 1)
#endif
                && (MiGetPdeAddress(VirtualAddress)->u.Hard.Valid == 1)) {

                //
                // Handle this specially if it is an image view using
                // large pages.
                //

                PointerPde = MiGetPdeAddress (VirtualAddress);

                if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {

                    if ((NewProtectWin32 == PAGE_EXECUTE_READWRITE) ||
                        (NewProtectWin32 == PAGE_READWRITE)) {

                        UNLOCK_ADDRESS_SPACE (Process);

                        *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
                        *BaseAddress = StartingAddress;
                        *LastProtect = MI_CONVERT_FROM_PTE_PROTECTION (ProtectionMask);

                        return STATUS_SUCCESS;
                    }
                    Status = STATUS_SECTION_PROTECTION;
                    goto ErrorFound;
                }
            }

            PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                                MI_VA_TO_VPN (StartingAddress));
            LastProtoPte = MiGetProtoPteAddress (FoundVad,
                                        MI_VA_TO_VPN (EndingAddress));

            //
            // Release the working set mutex and acquire the section
            // commit mutex.  Check all the prototype PTEs described by
            // the virtual address range to ensure they are committed.
            //

            KeAcquireGuardedMutexUnsafe (&MmSectionCommitMutex);

            while (PointerProtoPte <= LastProtoPte) {

                //
                // Check to see if the prototype PTE is committed, if
                // not return an error.
                //

                if (PointerProtoPte->u.Long == 0) {

                    //
                    // Error, this prototype PTE is not committed.
                    //

                    KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);
                    Status = STATUS_NOT_COMMITTED;
                    goto ErrorFound;
                }
                PointerProtoPte += 1;
            }

            //
            // The range is committed, release the section commitment
            // mutex, acquire the working set mutex and update the local PTEs.
            //

            KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);
        }

        //
        // Set the protection on the section pages.
        //

        Status = MiSetProtectionOnSection (Process,
                                           FoundVad,
                                           StartingAddress,
                                           EndingAddress,
                                           NewProtectWin32,
                                           &CapturedOldProtect,
                                           FALSE,
                                           &Locked);

        //
        //      ***  WARNING ***
        //
        // The alternate PTE support routines called by MiSetProtectionOnSection
        // may have deleted the old (small) VAD and replaced it with a different
        // (large) VAD - if so, the old VAD is freed and cannot be referenced.
        //

        if (!NT_SUCCESS (Status)) {
            goto ErrorFound;
        }
    }
    else {

        //
        // Not a section, private.
        // For private pages, the WRITECOPY attribute is not allowed.
        //

        if ((NewProtectWin32 & PAGE_WRITECOPY) ||
            (NewProtectWin32 & PAGE_EXECUTE_WRITECOPY)) {

            //
            // Not allowed.
            //

            Status = STATUS_INVALID_PARAMETER_4;
            goto ErrorFound;
        }

        Thread = PsGetCurrentThread ();

        LOCK_WS_UNSAFE (Thread, Process);

        //
        // Ensure all of the pages are already committed as described
        // in the virtual address descriptor.
        //

        if ( !MiIsEntireRangeCommitted (StartingAddress,
                                        EndingAddress,
                                        FoundVad,
                                        Process)) {

            //
            // Previously reserved pages have been decommitted, or an error
            // occurred, release mutex and return status.
            //

            UNLOCK_WS_UNSAFE (Thread, Process);
            Status = STATUS_NOT_COMMITTED;
            goto ErrorFound;
        }

        //
        // The address range is committed, change the protection.
        //

        PointerPde = MiGetPdeAddress (StartingAddress);
        PointerPte = MiGetPteAddress (StartingAddress);
        LastPte = MiGetPteAddress (EndingAddress);

        MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

        //
        // Capture the protection for the first page.
        //

        if (PointerPte->u.Long != 0) {

            CapturedOldProtect = MiGetPageProtection (PointerPte);

            //
            // Make sure the page directory & table pages are still resident.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
        }
        else {

            //
            // Get the protection from the VAD.
            //

            CapturedOldProtect =
               MI_CONVERT_FROM_PTE_PROTECTION (FoundVad->u.VadFlags.Protection);
        }

        //
        // For all the PTEs in the specified address range, set the
        // protection depending on the state of the PTE.
        //

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);

                MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
            }

            PteContents = *PointerPte;

            if (PteContents.u.Long == 0) {

                //
                // Increment the count of non-zero page table entries
                // for this page table and the number of private pages
                // for the process.  The protection will be set as
                // if the PTE was demand zero.
                //

                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));

                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }

            if (PteContents.u.Hard.Valid == 1) {

                //
                // Set the protection into both the PTE and the original PTE
                // in the PFN database.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                if (Pfn1->u3.e1.PrototypePte == 1) {

                    //
                    // This PTE refers to a fork prototype PTE, make it
                    // private.
                    //

                    MiCopyOnWrite (MiGetVirtualAddressMappedByPte (PointerPte),
                                   PointerPte);

                    //
                    // This may have released the working set mutex and
                    // the page directory and table pages may no longer be
                    // in memory.
                    //

                    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

                    //
                    // Do the loop again for the same PTE.
                    //

                    continue;
                }

                //
                // The PTE is a private page which is valid, if the
                // specified protection is no-access or guard page
                // remove the PTE from the working set.
                //

                if ((NewProtectWin32 & PAGE_NOACCESS) || (NewProtectWin32 & PAGE_GUARD)) {

                    //
                    // Remove the page from the working set.
                    //

                    Locked = MiRemovePageFromWorkingSet (PointerPte,
                                                         Pfn1,
                                                         &Process->Vm);

                    continue;
                }

                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

                //
                // Change the protection of the valid PTE and flush the TB.
                //

                MiFlushTbAndCapture (FoundVad,
                                     PointerPte,
                                     ProtectionMask,
                                     Pfn1,
                                     TRUE);
            }
            else if (PteContents.u.Soft.Prototype == 1) {

                //
                // This PTE refers to a fork prototype PTE, make the
                // page private.  This is accomplished by releasing
                // the working set mutex, reading the page thereby
                // causing a fault, and re-executing the loop. Hopefully,
                // this time, we'll find the page present and we'll
                // turn it into a private page.
                //
                // Note, that a TRY is used to catch guard
                // page exceptions and no-access exceptions.
                //

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                DoAgain = TRUE;

                while (PteContents.u.Hard.Valid == 0) {

                    UNLOCK_WS_UNSAFE (Thread, Process);
                    WsHeld = FALSE;

                    try {

                        *(volatile ULONG *)Va;

                    } except (EXCEPTION_EXECUTE_HANDLER) {

                        if (GetExceptionCode() == STATUS_ACCESS_VIOLATION) {

                            //
                            // The prototype PTE must be noaccess.
                            //

                            WsHeld = TRUE;
                            LOCK_WS_UNSAFE (Thread, Process);
                            MiMakePdeExistAndMakeValid (PointerPde,
                                                        Process,
                                                        MM_NOIRQL);

                            if (MiChangeNoAccessForkPte (PointerPte, ProtectionMask) == TRUE) {
                                DoAgain = FALSE;
                            }
                        }
                        else if (GetExceptionCode() == STATUS_IN_PAGE_ERROR) {

                            //
                            // Ignore this page and go on to the next one.
                            //

                            PointerPte += 1;
                            DoAgain = TRUE;

                            WsHeld = TRUE;
                            LOCK_WS_UNSAFE (Thread, Process);
                            break;
                        }
                    }

                    if (WsHeld == FALSE) {
                        LOCK_WS_UNSAFE (Thread, Process);
                    }

                    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

                    PteContents = *PointerPte;
                }

                if (DoAgain) {
                    continue;
                }
            }
            else if (PteContents.u.Soft.Transition == 1) {

                if (MiSetProtectionOnTransitionPte (PointerPte,
                                                    ProtectionMask)) {
                    continue;
                }
            }
            else {

                //
                // Must be page file space or demand zero.
                //

                PointerPte->u.Soft.Protection = ProtectionMask;
                ASSERT (PointerPte->u.Long != 0);
            }

            PointerPte += 1;

        } //end while

        UNLOCK_WS_UNSAFE (Thread, Process);

#if defined (_MI_RESET_USER_STACK_LIMIT)

        //
        // If a guard page is being set above the current stack limit in the
        // caller's thread stack then he may be trying to recover from a
        // user initiated stack probe growth (ie, inside his exception handler).
        // Reset the stack limit for him automatically so all the broken
        // usermode code doesn't have to be updated to do so.
        //

        if (StackLimit != 0) {
    
            if ((StackLimit < EndingAddress) &&
                (MI_VA_TO_VPN (StackLimit) >= FoundVad->StartingVpn) &&
                (MI_VA_TO_VPN (StackLimit) <= FoundVad->EndingVpn)) {
    
                StackLimit = (PVOID)((ULONG_PTR) EndingAddress + 1);
    
                if (MI_VA_TO_VPN (StackLimit) <= FoundVad->EndingVpn) {
    
                    //
                    // Stack limit is still within the VAD, so update the TEB.
                    //
    
                    try {
                        Teb->NtTib.StackLimit = StackLimit;
                    } except (EXCEPTION_EXECUTE_HANDLER) {
                        NOTHING;
                    }
                }
            }
        }

#endif

    }

    UNLOCK_ADDRESS_SPACE (Process);

    //
    // Common completion code.
    //

    *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    *BaseAddress = StartingAddress;
    *LastProtect = CapturedOldProtect;

    if (Locked) {
        return STATUS_WAS_UNLOCKED;
    }

    return STATUS_SUCCESS;

ErrorFound:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}


NTSTATUS
MiSetProtectionOnSection (
    IN PEPROCESS Process,
    IN PMMVAD FoundVad,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN ULONG NewProtect,
    OUT PULONG CapturedOldProtect,
    IN ULONG DontCharge,
    OUT PULONG Locked
    )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

Arguments:

    Process - Supplies a pointer to the current process.

    FoundVad - Supplies a pointer to the VAD containing the range to protect.

    StartingAddress - Supplies the starting address to protect.

    EndingAddress - Supplies the ending address to protect.

    NewProtect - Supplies the new protection to set.

    CapturedOldProtect - Supplies the address of a kernel owned pointer to
                store (without probing) the old protection into.

    DontCharge - Supplies TRUE if no quota or commitment should be charged.

    Locked - Receives TRUE if a locked page was removed from the working
             set (protection was guard page or no-access), FALSE otherwise.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, address creation mutex held, APCs disabled.

--*/

{
    KIRQL OldIrql;
    LOGICAL WsHeld;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE PointerProtoPte;
    PFN_NUMBER PageFrameIndex;
    PETHREAD Thread;
    PMMPFN Pfn1;
    MMPTE TempPte;
    ULONG ProtectionMask;
    ULONG ProtectionMaskNotCopy;
    ULONG NewProtectionMask;
    MMPTE PteContents;
    WSLE_NUMBER Index;
    PULONG Va;
    ULONG WriteCopy;
    ULONG DoAgain;
    ULONG Waited;
    SIZE_T QuotaCharge;
    PVOID UsedPageTableHandle;
    NTSTATUS Status;
    LOGICAL CaptureDirtyBit;

#if DBG

#define PTES_TRACKED 0x10

    ULONG PteIndex = 0;
    MMPTE PteTracker[PTES_TRACKED];
    MMPFN PfnTracker[PTES_TRACKED];
    SIZE_T PteQuotaCharge;
#endif

    PAGED_CODE();

    *Locked = FALSE;
    WriteCopy = FALSE;
    QuotaCharge = 0;

    //
    // Make the protection field.
    //

    ASSERT (FoundVad->u.VadFlags.PrivateMemory == 0);

    if ((FoundVad->u.VadFlags.VadType == VadImageMap) ||
        (FoundVad->u2.VadFlags2.CopyOnWrite == 1)) {

        if (NewProtect & PAGE_READWRITE) {
            NewProtect &= ~PAGE_READWRITE;
            NewProtect |= PAGE_WRITECOPY;
        }

        if (NewProtect & PAGE_EXECUTE_READWRITE) {
            NewProtect &= ~PAGE_EXECUTE_READWRITE;
            NewProtect |= PAGE_EXECUTE_WRITECOPY;
        }
    }

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {

        //
        // Return the error.
        //

        return STATUS_INVALID_PAGE_PROTECTION;
    }

    //
    // Determine if copy on write is being set.
    //

    ProtectionMaskNotCopy = ProtectionMask;
    if ((ProtectionMask & MM_COPY_ON_WRITE_MASK) == MM_COPY_ON_WRITE_MASK) {
        WriteCopy = TRUE;
        ProtectionMaskNotCopy &= ~MM_PROTECTION_COPY_MASK;
    }

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    Thread = PsGetCurrentThread ();

    LOCK_WS_UNSAFE (Thread, Process);

    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

    //
    // Capture the protection for the first page.
    //

    if (PointerPte->u.Long != 0) {

        if ((FoundVad->u.VadFlags.VadType == VadRotatePhysical) &&
             (PointerPte->u.Hard.Valid == 1)) {

            PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                    MI_VA_TO_VPN (
                                    MiGetVirtualAddressMappedByPte (PointerPte)));

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if ((!MI_IS_PFN (PageFrameIndex)) ||
                (Pfn1->PteAddress != PointerProtoPte)) {

                //
                // This address in the view is currently pointing at
                // the frame buffer (video RAM or AGP), protection is
                // in the prototype (which may itself be paged out - so
                // release the process working set pushlock before
                // accessing it).
                //

                UNLOCK_WS_UNSAFE (Thread, Process);

                TempPte = *PointerProtoPte;

                ASSERT (TempPte.u.Hard.Valid == 0);
                ASSERT (TempPte.u.Soft.Prototype == 0);

                //
                // PTE is either demand zero, pagefile or transition, in all
                // cases protection is in the prototype PTE.
                //

                *CapturedOldProtect =
                    MI_CONVERT_FROM_PTE_PROTECTION (TempPte.u.Soft.Protection);

                LOCK_WS_UNSAFE (Thread, Process);

                //
                // Ensure the PDE (and any table above it) are still resident.
                //

                MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
            }
            else {
                ASSERT (Pfn1->u3.e1.PrototypePte == 1);
                *CapturedOldProtect = MI_CONVERT_FROM_PTE_PROTECTION (
                                        Pfn1->OriginalPte.u.Soft.Protection);
            }
        }
        else {
            *CapturedOldProtect = MiGetPageProtection (PointerPte);
        }

        //
        // Ensure the PDE (and any table above it) are still resident.
        //

        MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
    }
    else {

        //
        // Get the protection from the VAD, unless image file.
        //

        if (FoundVad->u.VadFlags.VadType != VadImageMap) {

            //
            // This is not an image file, the protection is in the VAD.
            //

            *CapturedOldProtect =
                MI_CONVERT_FROM_PTE_PROTECTION(FoundVad->u.VadFlags.Protection);
        }
        else {

            //
            // This is an image file, the protection is in the
            // prototype PTE.
            //

            PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                    MI_VA_TO_VPN (
                                    MiGetVirtualAddressMappedByPte (PointerPte)));

            //
            // The prototype PTE may be paged out so release the process
            // working set pushlock before referencing the proto.
            //

            UNLOCK_WS_UNSAFE (Thread, Process);

            TempPte = *PointerProtoPte;

            if (TempPte.u.Hard.Valid == 0) {
                *CapturedOldProtect = MI_CONVERT_FROM_PTE_PROTECTION (TempPte.u.Soft.Protection);
            }
            else {

                //
                // The prototype PTE is valid but not in this
                // process (at least not before we released the
                // working set pushlock above).  So the PFN's
                // OriginalPte field contains the correct
                // protection value, but it may get trimmed at any
                // time.  Acquire the PFN lock so we can examine
                // it safely.
                //

                LOCK_PFN (OldIrql);

                if (MiGetPteAddress (PointerProtoPte)->u.Hard.Valid == 0) {
                    MiMakeSystemAddressValidPfn (PointerProtoPte, OldIrql);
                }

                TempPte = *PointerProtoPte;

                ASSERT (TempPte.u.Long != 0);

                if (TempPte.u.Hard.Valid) {
                    Pfn1 = MI_PFN_ELEMENT (TempPte.u.Hard.PageFrameNumber);

                    *CapturedOldProtect = MI_CONVERT_FROM_PTE_PROTECTION (
                                           Pfn1->OriginalPte.u.Soft.Protection);
                }
                else {
                    *CapturedOldProtect = MI_CONVERT_FROM_PTE_PROTECTION (TempPte.u.Soft.Protection);
                }
                UNLOCK_PFN (OldIrql);
            }

            LOCK_WS_UNSAFE (Thread, Process);

            //
            // Ensure the PDE (and any table above it) are still resident.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
        }
    }

    //
    // If the page protection is being changed to be copy-on-write, the
    // commitment and page file quota for the potentially dirty private pages
    // must be calculated and charged.  This must be done before any
    // protections are changed as the changes cannot be undone.
    //

    if (WriteCopy) {

        //
        // Calculate the charges.  If the page is shared and not write copy
        // it is counted as a charged page.
        //

        ASSERT (FoundVad->u.VadFlags.VadType != VadRotatePhysical);

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif
                do {

                    while (!MiDoesPxeExistAndMakeValid(PointerPxe, Process, MM_NOIRQL, &Waited)) {

                        //
                        // No PXE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PXE.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerProtoPte = PointerPte;
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }
                        QuotaCharge += PointerPte - PointerProtoPte;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif

                    while (!MiDoesPpeExistAndMakeValid(PointerPpe, Process, MM_NOIRQL, &Waited)) {

                        //
                        // No PPE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PPE.
                        //

                        PointerPpe += 1;
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerProtoPte = PointerPte;
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }

#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPdeBoundary (PointerPpe)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            goto retry;
                        }
#endif
                        QuotaCharge += PointerPte - PointerProtoPte;
                    }

#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif

                    while (!MiDoesPdeExistAndMakeValid(PointerPde, Process, MM_NOIRQL, &Waited)) {

                        //
                        // No PDE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PDE.
                        //

                        PointerPde += 1;
                        PointerProtoPte = PointerPte;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }
                        QuotaCharge += PointerPte - PointerProtoPte;
#if (_MI_PAGING_LEVELS >= 3)
                        if (MiIsPteOnPdeBoundary (PointerPde)) {
                            Waited = 1;
                            break;
                        }
#endif
                    }
                } while (Waited != 0);
            }

            PteContents = *PointerPte;

            if (PteContents.u.Long == 0) {

                //
                // The PTE has not been evaluated, assume copy on write.
                //

                QuotaCharge += 1;

            }
            else if (PteContents.u.Hard.Valid == 1) {
                if (PteContents.u.Hard.CopyOnWrite == 0) {

                    //
                    // See if this is a prototype PTE, if so charge it.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                    if (Pfn1->u3.e1.PrototypePte == 1) {
                        QuotaCharge += 1;
                    }
                }
            }
            else {

                if (PteContents.u.Soft.Prototype == 1) {

                    //
                    // This is a prototype PTE.  Charge if it is not
                    // in copy on write format.
                    //

                    if (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {
                        //
                        // Page protection is within the PTE.
                        //

                        if (!MI_IS_PTE_PROTECTION_COPY_WRITE(PteContents.u.Soft.Protection)) {
                            QuotaCharge += 1;
                        }
                    }
                    else {

                        //
                        // The PTE references the prototype directly, therefore
                        // it can't be copy on write.  Charge.
                        //

                        QuotaCharge += 1;
                    }
                }
            }
            PointerPte += 1;
        }

Done:

        //
        // If any quota is required, charge for it now.
        //
        // Note that the working set pushlock  must be released before charging.
        //

        if ((!DontCharge) && (QuotaCharge != 0)) {

            UNLOCK_WS_UNSAFE (Thread, Process);

            Status = PsChargeProcessPageFileQuota (Process, QuotaCharge);

            if (!NT_SUCCESS (Status)) {
                return STATUS_PAGEFILE_QUOTA_EXCEEDED;
            }

            if (Process->CommitChargeLimit) {
                if (Process->CommitCharge + QuotaCharge > Process->CommitChargeLimit) {
                    PsReturnProcessPageFileQuota (Process, QuotaCharge);
                    if (Process->Job) {
                        PsReportProcessMemoryLimitViolation ();
                    }
                    return STATUS_COMMITMENT_LIMIT;
                }
            }
            if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {

                if (PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, QuotaCharge) == FALSE) {
                    PsReturnProcessPageFileQuota (Process, QuotaCharge);
                    return STATUS_COMMITMENT_LIMIT;
                }
            }

            if (MiChargeCommitment (QuotaCharge, NULL) == FALSE) {
                if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                    PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)QuotaCharge);
                }
                PsReturnProcessPageFileQuota (Process, QuotaCharge);
                return STATUS_COMMITMENT_LIMIT;
            }

            //
            // Add the quota into the charge to the VAD.
            //

            MM_TRACK_COMMIT (MM_DBG_COMMIT_SET_PROTECTION, QuotaCharge);
            FoundVad->u.VadFlags.CommitCharge += QuotaCharge;
            Process->CommitCharge += QuotaCharge;
            if (Process->CommitCharge > Process->CommitChargePeak) {
                Process->CommitChargePeak = Process->CommitCharge;
            }
            MI_INCREMENT_TOTAL_PROCESS_COMMIT (QuotaCharge);

            LOCK_WS_UNSAFE (Thread, Process);
        }
    }

#if DBG
    PteQuotaCharge = QuotaCharge;
#endif

    //
    // For all the PTEs in the specified address range, set the
    // protection depending on the state of the PTE.
    //

    //
    // If the PTE was copy on write (but not written) and the
    // new protection is NOT copy-on-write, return page file quota
    // and commitment.
    //

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);

    //
    // Ensure the PDE (and any table above it) are still resident.
    //

    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

    QuotaCharge = 0;

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {
            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPdeAddress (PointerPte);
            PointerPxe = MiGetPpeAddress (PointerPte);

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
        }

        PteContents = *PointerPte;

        if (PteContents.u.Long == 0) {

            //
            // Increment the count of non-zero page table entries
            // for this page table and the number of private pages
            // for the process.
            //

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            //
            // The PTE is zero, set it into prototype PTE format
            // with the protection in the prototype PTE.
            //

            TempPte = PrototypePte;
            TempPte.u.Soft.Protection = ProtectionMask;
            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
        }
        else if (PteContents.u.Hard.Valid == 1) {

            //
            // Set the protection into both the PTE and the original PTE
            // in the PFN database for private pages only.
            //

            NewProtectionMask = ProtectionMask;

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

#if DBG
            if (PteIndex < PTES_TRACKED) {
                PteTracker[PteIndex] = PteContents;
                PfnTracker[PteIndex] = *Pfn1;
                PteIndex += 1;
            }
#endif

            if ((NewProtect & PAGE_NOACCESS) ||
                (NewProtect & PAGE_GUARD)) {

                ASSERT (FoundVad->u.VadFlags.VadType != VadRotatePhysical);

                *Locked = MiRemovePageFromWorkingSet (PointerPte,
                                                      Pfn1,
                                                      &Process->Vm);
                continue;
            }

            CaptureDirtyBit = TRUE;

            SATISFY_OVERZEALOUS_COMPILER (PointerProtoPte = NULL);

            if (FoundVad->u.VadFlags.VadType == VadRotatePhysical) {

                //
                // Regardless of whether view is currently pointing at the
                // frame buffer or regular memory, the prototype PTE must
                // be updated here as well.  This is so the view rotation
                // (which is much more common than a protection change on
                // one of these ranges) can be fast.
                //

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                                        MI_VA_TO_VPN (Va));

                if (!MI_IS_PFN (PageFrameIndex)) {
                    Pfn1 = NULL;
                }

                if ((Pfn1 == NULL) || (Pfn1->PteAddress != PointerProtoPte)) {

                    //
                    // This address in the view is currently pointing at
                    // the frame buffer (video RAM or AGP), protection is
                    // in the prototype PTE.
                    //
                    // No WSLE to update since the regular memory equivalent is
                    // not valid, and the active framebuffer doesn't have one.
                    //

                    CaptureDirtyBit = FALSE;
                }
                else {

                    ASSERT (Pfn1->u3.e1.PrototypePte == 1);

                    //
                    // The true protection cannot be in the WSLE because for
                    // these types of regions we always update both the WSLE
                    // and the prototype PTE.
                    //

#if DBG
                    Index = MiLocateWsle ((PVOID)Va, 
                                          MmWorkingSetList,
                                          Pfn1->u1.WsIndex,
                                          FALSE);

                    ASSERT (MmWsle[Index].u1.e1.Protection == MM_ZERO_ACCESS);
#endif
                }
            }
            else if (Pfn1->u3.e1.PrototypePte == 1) {

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                //
                // Check to see if this is a prototype PTE.  This
                // is done by comparing the PTE address in the
                // PFN database to the PTE address indicated by the VAD.
                // If they are not equal, this is a fork prototype PTE.
                //

                if (Pfn1->PteAddress != MiGetProtoPteAddress (FoundVad,
                                                    MI_VA_TO_VPN ((PVOID)Va))) {

                    //
                    // This PTE refers to a fork prototype PTE, make it
                    // private.  But don't charge quota for it if the PTE
                    // was already copy on write (because it's already
                    // been charged for this case).
                    //

                    if (MiCopyOnWrite ((PVOID)Va, PointerPte) == TRUE) {

                        if ((WriteCopy) && (PteContents.u.Hard.CopyOnWrite == 0)) {
                            QuotaCharge += 1;
                        }
                    }

                    //
                    // Ensure the PDE (and any table above it) are still
                    // resident (they may have been trimmed when the working
                    // set mutex was released above).
                    //

                    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

                    //
                    // Do the loop again.
                    //

                    continue;
                }

                //
                // Update the protection field in the WSLE and the PTE.
                //
                // If the PTE is copy on write uncharge the
                // previously charged quota.
                //

                if ((!WriteCopy) && (PteContents.u.Hard.CopyOnWrite == 1)) {
                    QuotaCharge += 1;
                }

                //
                // The true protection may be in the WSLE, locate it.
                //

                Index = MiLocateWsle ((PVOID)Va, 
                                      MmWorkingSetList,
                                      Pfn1->u1.WsIndex,
                                      FALSE);

                MmWsle[Index].u1.e1.Protection = ProtectionMask;
            }
            else {

                //
                // Page is private (copy on written), protection mask
                // is stored in the original PTE field.
                //

                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMaskNotCopy;

                NewProtectionMask = ProtectionMaskNotCopy;
            }

            MI_SNAP_DATA (Pfn1, PointerPte, 7);

            //
            // Change the protection of the valid PTE and flush the TB.
            //

            MiFlushTbAndCapture (FoundVad,
                                 PointerPte,
                                 NewProtectionMask,
                                 Pfn1,
                                 CaptureDirtyBit);

            if (FoundVad->u.VadFlags.VadType == VadRotatePhysical) {

                //
                // Release the working set pushlock as the prototype PTE we
                // are about to update may be paged out and we don't want
                // to deadlock (this process may have all the trimmable pages).
                //
                // Because the address space mutex is still held, the section
                // cannot go away and thus the prototype PTE cannot go away.
                // The system working set pushlock is acquired to prevent it
                // from changing.
                //

                UNLOCK_WS_UNSAFE (Thread, Process);

                LOCK_SYSTEM_WS (Thread);
                LOCK_PFN (OldIrql);

                MiMakeSystemAddressValidPfnSystemWs (PointerProtoPte, OldIrql);

                //
                // Note the prototype PTE may be in any state since the
                // process working set pushlock was released.  Handle all of
                // them here.
                //

                ASSERT (PointerProtoPte->u.Long != 0);

                if (PointerProtoPte->u.Hard.Valid == 1) {
                    Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerProtoPte));
                    Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                }
                else {
                    PointerProtoPte->u.Soft.Protection = ProtectionMask;
                }

                UNLOCK_PFN (OldIrql);
                UNLOCK_SYSTEM_WS (Thread);

                LOCK_WS_UNSAFE (Thread, Process);

                MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
            }
        }
        else if (PteContents.u.Soft.Prototype == 1) {

#if DBG
            if (PteIndex < PTES_TRACKED) {
                PteTracker[PteIndex] = PteContents;
                *(PULONG)(&PfnTracker[PteIndex]) = 0x88;
                PteIndex += 1;
            }
#endif

            //
            // The PTE is in prototype PTE format.
            //
            // Is it a fork prototype PTE?
            //

            Va = (PULONG)MiGetVirtualAddressMappedByPte (PointerPte);

            if ((PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) &&
               (MiPteToProto (PointerPte) !=
                                 MiGetProtoPteAddress (FoundVad,
                                     MI_VA_TO_VPN ((PVOID)Va)))) {

                //
                // This PTE refers to a fork prototype PTE, make the
                // page private.  This is accomplished by releasing
                // the working set mutex, reading the page thereby
                // causing a fault, and re-executing the loop, hopefully,
                // this time, we'll find the page present and will
                // turn it into a private page.
                //
                // Note, that page with prototype = 1 cannot be
                // no-access.
                //

                DoAgain = TRUE;

                while (PteContents.u.Hard.Valid == 0) {

                    UNLOCK_WS_UNSAFE (Thread, Process);

                    WsHeld = FALSE;

                    try {

                        *(volatile ULONG *)Va;
                    } except (EXCEPTION_EXECUTE_HANDLER) {

                        if (GetExceptionCode() != STATUS_GUARD_PAGE_VIOLATION) {

                            //
                            // The prototype PTE must be noaccess.
                            //

                            WsHeld = TRUE;
                            LOCK_WS_UNSAFE (Thread, Process);
                            MiMakePdeExistAndMakeValid (PointerPde,
                                                        Process,
                                                        MM_NOIRQL);
                            if (MiChangeNoAccessForkPte (PointerPte, ProtectionMask) == TRUE) {
                                DoAgain = FALSE;
                            }
                        }
                    }

                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);

                    if (WsHeld == FALSE) {
                        LOCK_WS_UNSAFE (Thread, Process);
                    }

                    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

                    PteContents = *PointerPte;
                }

                if (DoAgain) {
                    continue;
                }

            }
            else {

                //
                // If the new protection is not write-copy, the PTE
                // protection is not in the prototype PTE (can't be
                // write copy for sections), and the protection in
                // the PTE is write-copy, release the page file
                // quota and commitment for this page.
                //

                if ((!WriteCopy) &&
                    (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED)) {
                    if (MI_IS_PTE_PROTECTION_COPY_WRITE(PteContents.u.Soft.Protection)) {
                        QuotaCharge += 1;
                    }

                }

                //
                // The PTE is a prototype PTE.  Make the high part
                // of the PTE indicate that the protection field
                // is in the PTE itself.
                //

                MI_WRITE_INVALID_PTE (PointerPte, PrototypePte);
                PointerPte->u.Soft.Protection = ProtectionMask;
            }
        }
        else if (PteContents.u.Soft.Transition == 1) {

            ASSERT (FoundVad->u.VadFlags.VadType != VadRotatePhysical);
#if DBG
            if (PteIndex < PTES_TRACKED) {
                PteTracker[PteIndex] = PteContents;
                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
                PfnTracker[PteIndex] = *Pfn1;
                PteIndex += 1;
            }
#endif

            //
            // This is a transition PTE. (Page is private)
            //

            if (MiSetProtectionOnTransitionPte (
                                        PointerPte,
                                        ProtectionMaskNotCopy)) {
                continue;
            }
        }
        else {

#if DBG
            if (PteIndex < PTES_TRACKED) {
                PteTracker[PteIndex] = PteContents;
                *(PULONG)(&PfnTracker[PteIndex]) = 0x99;
                PteIndex += 1;
            }
#endif

            //
            // Must be page file space or demand zero.
            //

            PointerPte->u.Soft.Protection = ProtectionMaskNotCopy;
        }

        PointerPte += 1;
    }

    UNLOCK_WS_UNSAFE (Thread, Process);

    //
    // Return the quota charge and the commitment, if any.
    //

    if ((QuotaCharge > 0) && (!DontCharge)) {

        MiReturnCommitment (QuotaCharge);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROTECTION, QuotaCharge);
        PsReturnProcessPageFileQuota (Process, QuotaCharge);

#if DBG
        if (QuotaCharge > FoundVad->u.VadFlags.CommitCharge) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                "MMPROTECT QUOTA FAILURE: %p %p %x %p\n",
                PteTracker, PfnTracker, PteIndex, PteQuotaCharge);
            DbgBreakPoint ();
        }
#endif

        FoundVad->u.VadFlags.CommitCharge -= QuotaCharge;
        if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)QuotaCharge);
        }
        Process->CommitCharge -= QuotaCharge;

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - QuotaCharge);
    }

    return STATUS_SUCCESS;
}

WIN32_PROTECTION_MASK
MiGetPageProtection (
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This routine returns the page protection of a non-zero PTE.
    It may release and reacquire the working set pushlock.

Arguments:

    PointerPte - Supplies a pointer to a non-zero PTE.

Return Value:

    Returns the protection code.

Environment:

    Kernel mode, working set and address creation pushlocks held.
    Note that the address creation mutex does not need to be held
    if the working set pushlock does not need to be released in the
    case of a prototype PTE.

--*/

{
    KIRQL OldIrql;
    PEPROCESS Process;
    LOGICAL WsHeldSafe;
    LOGICAL WsHeldShared;
    MMPTE PteContents;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PMMPFN Pfn1;
    PVOID Va;
    WSLE_NUMBER Index;
    PETHREAD Thread;
    MM_PROTECTION_MASK ProtectionMask;
    WIN32_PROTECTION_MASK ReturnProtection;

    PAGED_CODE ();

    ASSERT (!MI_IS_PTE_PROTOTYPE (PointerPte));

    PteContents = *PointerPte;

    ASSERT (PteContents.u.Long != 0);

    if ((PteContents.u.Soft.Valid == 0) && (PteContents.u.Soft.Prototype == 1)) {

        //
        // This PTE is in prototype format, the protection is
        // stored in the prototype PTE.
        //

        if (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

            //
            // The protection is within this PTE.
            //

            return MI_CONVERT_FROM_PTE_PROTECTION (
                                            PteContents.u.Soft.Protection);
        }

        //
        // Capture the prototype PTE contents.
        //
        // Note the prototype PTE itself may be paged out - so
        // release the process working set pushlock before accessing it.
        //

        Thread = PsGetCurrentThread ();
        Process = PsGetCurrentProcessByThread (Thread);

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Handle both cases here and below.
        //

        UNLOCK_WS_REGARDLESS (Thread, Process, WsHeldSafe, WsHeldShared);

        ProtoPte = MiPteToProto (&PteContents);

        PteContents = *ProtoPte;

        //
        // The working set pushlock may have been released and the
        // page may no longer be in prototype format, but it doesn't
        // matter to us - we are going to return the protection from
        // the prototype PTE regardless (the protection cannot be
        // changed by the app since the address space pushlock is still
        // held.
        //

        if (PteContents.u.Hard.Valid) {

            //
            // The prototype PTE is valid but not in this
            // process (at least not before we released the
            // working set pushlock above).  So the PFN's
            // OriginalPte field contains the correct
            // protection value, but it may get trimmed at any
            // time.  Acquire the PFN lock so we can examine
            // it safely.
            //

            PointerPde = MiGetPteAddress (ProtoPte);

            LOCK_PFN (OldIrql);

            if (PointerPde->u.Hard.Valid == 0) {
                MiMakeSystemAddressValidPfn (ProtoPte, OldIrql);
            }

            PteContents = *ProtoPte;

            ASSERT (PteContents.u.Long != 0);

            if (PteContents.u.Hard.Valid) {
                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                ReturnProtection = MI_CONVERT_FROM_PTE_PROTECTION (
                                          Pfn1->OriginalPte.u.Soft.Protection);
            }
            else {
                ReturnProtection = MI_CONVERT_FROM_PTE_PROTECTION (PteContents.u.Soft.Protection);
            }
            UNLOCK_PFN (OldIrql);
        }
        else {

            //
            // The prototype PTE is not valid, return the
            // protection from the PTE.
            //

            ReturnProtection = MI_CONVERT_FROM_PTE_PROTECTION (PteContents.u.Soft.Protection);
        }

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Reacquire it in the same manner our caller did.
        //

        LOCK_WS_REGARDLESS (Thread, Process, WsHeldSafe, WsHeldShared);

        return ReturnProtection;
    }

    if (PteContents.u.Hard.Valid) {

        //
        // The page is valid, the protection field is either in the
        // PFN database original PTE element or the WSLE.  If
        // the page is private, get it from the PFN original PTE
        // element, else use the WSLE.
        //

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

        if (Pfn1->u3.e1.PrototypePte == 0) {

            if (Pfn1->u4.AweAllocation == 0) {

                //
                // This is a private PTE or the PTE address is that of a
                // prototype PTE, hence the protection is in the original PTE.
                //

                return MI_CONVERT_FROM_PTE_PROTECTION (
                                      Pfn1->OriginalPte.u.Soft.Protection);
            }

            //
            // This is an AWE frame - the original PTE field in the PFN
            // actually contains the AweReferenceCount.  These pages are
            // either noaccess, readonly or readwrite.
            //

            if (PteContents.u.Hard.Owner == MI_PTE_OWNER_KERNEL) {
                return PAGE_NOACCESS;
            }

            if (PteContents.u.Hard.Write == 1) {
                return PAGE_READWRITE;
            }

            return PAGE_READONLY;
        }

        //
        // The PTE is a hardware PTE, get the protection from the WSLE.
        //

        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        Process = PsGetCurrentProcess ();

        Index = MiLocateWsle (Va,
                              MmWorkingSetList,
                              Pfn1->u1.WsIndex,
                              FALSE);

        if (MmWsle[Index].u1.e1.Protection == MM_ZERO_ACCESS) {
            ProtectionMask = MI_GET_PROTECTION_FROM_SOFT_PTE (&Pfn1->OriginalPte);
        }
        else {
            ProtectionMask = (MM_PROTECTION_MASK) MmWsle[Index].u1.e1.Protection;
        }

        return MI_CONVERT_FROM_PTE_PROTECTION (ProtectionMask);
    }

    //
    // PTE is either demand zero or transition, in either
    // case protection is in PTE.
    //

    return MI_CONVERT_FROM_PTE_PROTECTION (PteContents.u.Soft.Protection);
}

ULONG
MiChangeNoAccessForkPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:

Arguments:

    PointerPte - Supplies a pointer to the current PTE.

    ProtectionMask - Supplies the protection mask to set.

Return Value:

    TRUE if the loop does NOT need to be repeated for this PTE, FALSE
    if it does need retrying.

Environment:

    Kernel mode, address creation mutex held, APCs disabled.

--*/

{
    PAGED_CODE();

    if (ProtectionMask == MM_NOACCESS) {

        //
        // No need to change the page protection.
        //

        return TRUE;
    }

    PointerPte->u.Proto.ReadOnly = 1;

    return FALSE;
}

VOID
MiFlushTbAndCapture (
    IN PMMVAD FoundVad,
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask,
    IN PMMPFN Pfn1,
    IN LOGICAL CaptureDirtyBit
    )

/*++

Routine Description:

    Non-pageable helper routine to change a PTE & flush the relevant TB entry.

Arguments:

    FoundVad - Supplies a writewatch VAD to update or NULL.

    PointerPte - Supplies the PTE to update.

    ProtectionMask - Supplies the new protection mask to use.

    Pfn1 - Supplies the PFN database entry to update.  NULL if there is no
           entry to update (ie: it's a PTE that points at dualport memory, etc).

    CaptureDirtyBit - Supplies TRUE if the dirty bit should be captured and
                      pagefile space released, etc.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    MMPTE TempPte;
    MMPTE PreviousPte;
    KIRQL OldIrql;
    PVOID VirtualAddress;
    LOGICAL RecomputePte;
    PFN_NUMBER PageFrameIndex;
    MEMORY_CACHING_TYPE CacheType;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    WSLE_NUMBER WorkingSetIndex;

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    //
    // Opportunistically compute the new PTE without the PFN lock.  If a
    // TB attribute conflict is detected, this will be redone while holding
    // the PFN lock below.
    //

    RecomputePte = FALSE;

    PreviousPte = *PointerPte;

    PageFrameIndex = (PFN_NUMBER) PreviousPte.u.Hard.PageFrameNumber;

    MI_MAKE_VALID_USER_PTE (TempPte,
                            PageFrameIndex,
                            ProtectionMask,
                            PointerPte);

    WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PreviousPte);
    MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);

    LOCK_PFN (OldIrql);

    PreviousPte = *PointerPte;

    if (Pfn1 != NULL) {

        //
        // Ensure the requested attributes do not conflict with the
        // PFN attributes.
        //

        if (Pfn1->u3.e1.CacheAttribute == MiCached) {
            if (ProtectionMask & (MM_NOCACHE | MM_WRITECOMBINE)) {
                RecomputePte = TRUE;
                ProtectionMask &= ~(MM_NOCACHE | MM_WRITECOMBINE);
            }
        }
        else if (Pfn1->u3.e1.CacheAttribute == MiNonCached) {
            if ((ProtectionMask & (MM_NOCACHE | MM_WRITECOMBINE)) != MM_NOCACHE) {
                RecomputePte = TRUE;
                ProtectionMask &= ~(MM_NOCACHE | MM_WRITECOMBINE);
                ProtectionMask |= MM_NOCACHE;
            }
        }
        else if (Pfn1->u3.e1.CacheAttribute == MiWriteCombined) {
            if ((ProtectionMask & (MM_NOCACHE | MM_WRITECOMBINE)) != MM_WRITECOMBINE) {
                RecomputePte = TRUE;
                ProtectionMask &= ~(MM_NOCACHE | MM_WRITECOMBINE);
                ProtectionMask |= MM_WRITECOMBINE;
            }
        }

        if (RecomputePte == TRUE) {
            MI_MAKE_VALID_USER_PTE (TempPte,
                                    PageFrameIndex,
                                    ProtectionMask,
                                    PointerPte);
    
            WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PreviousPte);
            MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);
        }
    }
    else {

        //
        // This mapping is to an I/O space page (ie, not system
        // memory or AGP), so recompute the attributes factoring in
        // potential platform overrides.  This is because the
        // MI_MAKE_VALID_USER_PTE call assumes the memory is system -
        // the PTE masks were updated when we booted for this.
        //

        CacheType = MmCached;
        if (MI_IS_NOCACHE (ProtectionMask)) {
            CacheType = MmNonCached;
        }
        else if (MI_IS_WRITECOMBINE (ProtectionMask)) {
            CacheType = MmWriteCombined;
        }

        CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 1);

        switch (CacheAttribute) {
            case MiCached:
                MI_ENABLE_CACHING (TempPte);
                break;
            case MiNonCached:
                MI_DISABLE_CACHING (TempPte);
                break;
            case MiWriteCombined:
                MI_SET_PTE_WRITE_COMBINE (TempPte);
                break;
            default:
                break;
        }

        //
        // If the protection indicates write privilege, then mark
        // the PTE as dirty and for x86/AMD64 MP Writable as well.
        //

        if (ProtectionMask & MM_PROTECTION_WRITE_MASK) {
            MI_SET_PTE_DIRTY (TempPte);
#if !defined(NT_UP)
#if defined(_X86_) || defined(_AMD64_)
            TempPte.u.Hard.Writable = 1;
#endif
#endif
        }
    }

    MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    //
    // Flush the TB as we have changed the protection of a valid PTE.
    //

    MI_FLUSH_SINGLE_TB (VirtualAddress, FALSE);

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    //
    // A page protection is being changed, on certain
    // hardware the dirty bit should be ORed into the
    // modify bit in the PFN element.
    //

    if (CaptureDirtyBit == TRUE) {
        ASSERT (Pfn1 != NULL);
        MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);
    }

    //
    // If the PTE indicates the page has been modified (this is different
    // from the PFN indicating this), then ripple it back to the write watch
    // bitmap now since we are still in the correct process context.
    //

    if (FoundVad->u.VadFlags.VadType == VadWriteWatch) {

        ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH));
        ASSERT (Pfn1->u3.e1.PrototypePte == 0);

        if (MI_IS_PTE_DIRTY(PreviousPte)) {

            //
            // This process has (or had) write watch VADs.  Update the
            // bitmap now.
            //

            MiCaptureWriteWatchDirtyBit (PsGetCurrentProcess(), VirtualAddress);
        }
    }

    UNLOCK_PFN (OldIrql);
    return;
}

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:

    Non-pageable helper routine to change the protection of a transition PTE.

Arguments:

    PointerPte - Supplies a pointer to the PTE.

    ProtectionMask - Supplies the new protection mask.

Return Value:

    TRUE if the caller needs to retry, FALSE if the protection was successfully
    changed.

Environment:

    Kernel mode.  The PFN lock is needed to ensure the (private) page
    doesn't become non-transition.

--*/

{
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPFN Pfn1;

    LOCK_PFN (OldIrql);

    //
    // Make sure the page is still a transition page.
    //

    PteContents = *PointerPte;

    if ((PteContents.u.Soft.Prototype == 0) &&
        (PointerPte->u.Soft.Transition == 1)) {

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
        PointerPte->u.Soft.Protection = ProtectionMask;

        UNLOCK_PFN (OldIrql);

        return FALSE;
    }

    //
    // Do this loop again for the same PTE.
    //

    UNLOCK_PFN (OldIrql);
    return TRUE;
}

NTSTATUS
MiCheckSecuredVad (
    IN PMMVAD Vad,
    IN PVOID Base,
    IN SIZE_T Size,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:

    This routine checks to see if the specified VAD is secured in such
    a way as to conflict with the address range and protection mask
    specified.

Arguments:

    Vad - Supplies a pointer to the VAD containing the address range.

    Base - Supplies the base of the range the protection starts at.

    Size - Supplies the size of the range.

    ProtectionMask - Supplies the protection mask being set.

Return Value:

    Status value.

Environment:

    Kernel mode.

--*/

{
    PVOID End;
    PLIST_ENTRY Next;
    PMMSECURE_ENTRY Entry;
    NTSTATUS Status = STATUS_SUCCESS;

    End = (PVOID)((PCHAR)Base + Size - 1);

    if (ProtectionMask < MM_SECURE_DELETE_CHECK) {
        if ((Vad->u.VadFlags.NoChange == 1) &&
            (Vad->u2.VadFlags2.SecNoChange == 1) &&
            (Vad->u.VadFlags.Protection != ProtectionMask)) {

            //
            // An attempt is made at changing the protection
            // of a SEC_NO_CHANGE section - return an error.
            //

            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto done;
        }
    }
    else {

        //
        // Deletion - set to no-access for check.  SEC_NOCHANGE allows
        // deletion, but does not allow page protection changes.
        //

        ProtectionMask = 0;
    }

    if (Vad->u2.VadFlags2.OneSecured) {

        if (((ULONG_PTR)Base <= ((PMMVAD_LONG)Vad)->u3.Secured.EndVpn) &&
             ((ULONG_PTR)End >= ((PMMVAD_LONG)Vad)->u3.Secured.StartVpn)) {

            //
            // This region conflicts, check the protections.
            //

            if (MI_IS_GUARD (ProtectionMask)) {
                Status = STATUS_INVALID_PAGE_PROTECTION;
                goto done;
            }

            if (Vad->u2.VadFlags2.ReadOnly) {
                if (MmReadWrite[ProtectionMask] < 10) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
            }
            else {
                if (MmReadWrite[ProtectionMask] < 11) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
            }
        }

    }
    else if (Vad->u2.VadFlags2.MultipleSecured) {

        Next = ((PMMVAD_LONG)Vad)->u3.List.Flink;
        do {
            Entry = CONTAINING_RECORD( Next,
                                       MMSECURE_ENTRY,
                                       List);

            if (((ULONG_PTR)Base <= Entry->EndVpn) &&
                ((ULONG_PTR)End >= Entry->StartVpn)) {

                //
                // This region conflicts, check the protections.
                //

                if (MI_IS_GUARD (ProtectionMask)) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
    
                if (Entry->u2.VadFlags2.ReadOnly) {
                    if (MmReadWrite[ProtectionMask] < 10) {
                        Status = STATUS_INVALID_PAGE_PROTECTION;
                        goto done;
                    }
                }
                else {
                    if (MmReadWrite[ProtectionMask] < 11) {
                        Status = STATUS_INVALID_PAGE_PROTECTION;
                        goto done;
                    }
                }
            }
            Next = Entry->List.Flink;
        } while (Entry->List.Flink != &((PMMVAD_LONG)Vad)->u3.List);
    }

done:
    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\pfndec.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   pfndec.c

Abstract:

    This module contains the routines to decrement the share count and
    the reference counts within the Page Frame Database.

--*/

#include "mi.h"

ULONG MmFrontOfList;


VOID
FASTCALL
MiDecrementShareCount (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine decrements the share count within the PFN element
    for the specified physical page.  If the share count becomes
    zero the corresponding PTE is converted to the transition state
    and the reference count is decremented and the ValidPte count
    of the PTEframe is decremented.

Arguments:

    Pfn1 - Supplies the PFN database entry to decrement.

    PageFrameIndex - Supplies the physical page number of which to decrement
                     the share count.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    ULONG FreeBit;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PEPROCESS Process;

    ASSERT (PageFrameIndex > 0);
    ASSERT (MI_IS_PFN (PageFrameIndex));
    ASSERT (Pfn1 == MI_PFN_ELEMENT (PageFrameIndex));

    if (Pfn1->u3.e1.PageLocation != ActiveAndValid &&
        Pfn1->u3.e1.PageLocation != StandbyPageList) {
            KeBugCheckEx (PFN_LIST_CORRUPT,
                      0x99,
                      PageFrameIndex,
                      Pfn1->u3.e1.PageLocation,
                      0);
    }

    Pfn1->u2.ShareCount -= 1;

    ASSERT (Pfn1->u2.ShareCount < 0xF000000);

    if (Pfn1->u2.ShareCount == 0) {

        if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
            PERFINFO_PFN_INFORMATION PerfInfoPfn;

            PerfInfoPfn.PageFrameIndex = PageFrameIndex;
            PerfInfoLogBytes(PERFINFO_LOG_TYPE_ZEROSHARECOUNT, 
                             &PerfInfoPfn, 
                             sizeof(PerfInfoPfn));
        }

        //
        // The share count is now zero, decrement the reference count
        // for the PFN element and turn the referenced PTE into
        // the transition state if it refers to a prototype PTE.
        // PTEs which are not prototype PTEs do not need to be placed
        // into transition as they are placed in transition when
        // they are removed from the working set (working set free routine).
        //

        //
        // If the PTE referenced by this PFN element is actually
        // a prototype PTE, it must be mapped into hyperspace and
        // then operated on.
        //

        if (Pfn1->u3.e1.PrototypePte == 1) {

            if (MiIsProtoAddressValid (Pfn1->PteAddress)) {
                Process = NULL;
                PointerPte = Pfn1->PteAddress;
            }
            else {

                //
                // The address is not valid in this process, map it into
                // hyperspace so it can be operated upon.
                //

                Process = PsGetCurrentProcess ();
                PointerPte = (PMMPTE) MiMapPageInHyperSpaceAtDpc(Process, Pfn1->u4.PteFrame);
                PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                        MiGetByteOffset(Pfn1->PteAddress));
            }

            TempPte = *PointerPte;

            MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                          Pfn1->OriginalPte.u.Soft.Protection);
            MI_WRITE_INVALID_PTE (PointerPte, TempPte);

            if (Process != NULL) {
                MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
            }

            //
            // There is no need to flush the translation buffer at this
            // time as we only invalidated a prototype PTE.
            //
        }

        //
        // Change the page location to inactive (from active and valid).
        //

        Pfn1->u3.e1.PageLocation = TransitionPage;

        //
        // Decrement the reference count as the share count is now zero.
        //

        MM_PFN_LOCK_ASSERT();

        ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

        if (Pfn1->u3.e2.ReferenceCount == 1) {

            if (MI_IS_PFN_DELETED (Pfn1)) {

                Pfn1->u3.e2.ReferenceCount = 0;

                //
                // There is no referenced PTE for this page, delete the page
                // file space (if any), and place the page on the free list.
                //

                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 0);

                FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                    MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                }

                //
                // Temporarily mark the frame as active and valid so that
                // MiIdentifyPfn knows it is safe to walk back through the
                // containing frames for a more accurate identification.
                // Note the page will be immediately re-marked as it is
                // inserted into the freelist.
                //

                Pfn1->u3.e1.PageLocation = ActiveAndValid;

                MiInsertPageInFreeList (PageFrameIndex);
            }
            else {
                MiDecrementReferenceCount (Pfn1, PageFrameIndex);
            }
        }
        else {
            InterlockedDecrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);
        }
    }

    return;
}

VOID
FASTCALL
MiDecrementReferenceCount (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine decrements the reference count for the specified page.
    If the reference count becomes zero, the page is placed on the
    appropriate list (free, modified, standby or bad).  If the page
    is placed on the free or standby list, the number of available
    pages is incremented and if it transitions from zero to one, the
    available page event is set.


Arguments:

    Pfn1 - Supplies the PFN database entry to decrement.

    PageFrameIndex - Supplies the physical page number of which to
                     decrement the reference count.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    ULONG FreeBit;

    MM_PFN_LOCK_ASSERT();

    ASSERT (MI_IS_PFN (PageFrameIndex));
    ASSERT (Pfn1 == MI_PFN_ELEMENT (PageFrameIndex));
    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

    InterlockedDecrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);

    if (Pfn1->u3.e2.ReferenceCount != 0) {

        //
        // The reference count is not zero, return.
        //

        return;
    }

    //
    // The reference count is now zero, put the page on some list.
    //

    if (Pfn1->u2.ShareCount != 0) {

        KeBugCheckEx (PFN_LIST_CORRUPT,
                      7,
                      PageFrameIndex,
                      Pfn1->u2.ShareCount,
                      0);
        return;
    }

    ASSERT (Pfn1->u3.e1.PageLocation != ActiveAndValid);

    if (MI_IS_PFN_DELETED (Pfn1)) {

        //
        // There is no referenced PTE for this page, delete the page
        // file space (if any), and place the page on the free list.
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

            if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
            }
        }

        MiInsertPageInFreeList (PageFrameIndex);

        return;
    }

    //
    // Place the page on the modified or standby list depending
    // on the state of the modify bit in the PFN element.
    //

    if (Pfn1->u3.e1.Modified == 1) {
        MiInsertPageInList (&MmModifiedPageListHead, PageFrameIndex);
    }
    else {

        if (Pfn1->u3.e1.RemovalRequested == 1) {

            //
            // The page may still be marked as on the modified list if the
            // current thread is the modified writer completing the write.
            // Mark it as standby so restoration of the transition PTE
            // doesn't flag this as illegal.
            //

            Pfn1->u3.e1.PageLocation = StandbyPageList;

            MiRestoreTransitionPte (Pfn1);
            MiInsertPageInList (&MmBadPageListHead, PageFrameIndex);
            return;
        }

        if (!MmFrontOfList) {
            MiInsertPageInList (&MmStandbyPageListHead, PageFrameIndex);
        }
        else {
            MiInsertStandbyListAtFront (PageFrameIndex);
        }
    }

    return;
}

VOID
MiBadRefCount (
    __in PMMPFN Pfn1
    )
{
    KeBugCheckEx (PFN_LIST_CORRUPT,
                  0x9A,
                  Pfn1 - MmPfnDatabase,
                  Pfn1->u3.e1.PageLocation,
                  Pfn1->u3.e2.ReferenceCount);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\querysec.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   querysec.c

Abstract:

    This module contains the routines which implement the
    NtQuerySection service.

--*/


#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtQuerySection)
#endif

NTSTATUS
NtQuerySection(
    __in HANDLE SectionHandle,
    __in SECTION_INFORMATION_CLASS SectionInformationClass,
    __out_bcount(SectionInformationLength) PVOID SectionInformation,
    __in SIZE_T SectionInformationLength,
    __out_opt PSIZE_T ReturnLength
    )

/*++

Routine Description:

   This function provides the capability to determine the base address,
   size, granted access, and allocation of an opened section object.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    SectionInformationClass - The section information class about
                              which to retrieve information.

    SectionInformation - A pointer to a buffer that receives the
                         specified information.  The format and content of the
                         buffer depend on the specified section class.

       SectionInformation Format by Information Class:

       SectionBasicInformation - Data type is PSECTION_BASIC_INFORMATION.

           SECTION_BASIC_INFORMATION Structure

           PVOID BaseAddress - The base virtual address of the
                               section if the section is based.

           LARGE_INTEGER MaximumSize - The maximum size of the section in
                                       bytes.

           ULONG AllocationAttributes - The allocation attributes flags.

               AllocationAttributes Flags

               SEC_BASED - The section is a based section.

               SEC_FILE - The section is backed by a data file.

               SEC_RESERVE - All pages of the section were initially
                             set to the reserved state.

               SEC_COMMIT - All pages of the section were initially
                            to the committed state.

               SEC_IMAGE - The section was mapped as an executable image file.

        SECTION_IMAGE_INFORMATION

    SectionInformationLength - Specifies the length in bytes of the
                               section information buffer.

    ReturnLength - An optional pointer which, if specified, receives the
                   number of bytes placed in the section information buffer.


Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    PSECTION Section;
    KPROCESSOR_MODE PreviousMode;

    PAGED_CODE();

    //
    // Get previous processor mode and probe output argument if necessary.
    //

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {

        //
        // Check arguments.
        //

        try {

            ProbeForWrite(SectionInformation,
                          SectionInformationLength,
                          sizeof(ULONG));

            if (ARGUMENT_PRESENT (ReturnLength)) {
                ProbeForWriteUlong_ptr (ReturnLength);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }

    //
    // Check argument validity.
    //

    if ((SectionInformationClass != SectionBasicInformation) &&
        (SectionInformationClass != SectionImageInformation)) {
        return STATUS_INVALID_INFO_CLASS;
    }

    if (SectionInformationClass == SectionBasicInformation) {
        if (SectionInformationLength < (ULONG)sizeof(SECTION_BASIC_INFORMATION)) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }
    }
    else {
        if (SectionInformationLength < (ULONG)sizeof(SECTION_IMAGE_INFORMATION)) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }
    }

    //
    // Reference section object by handle for READ access, get the information
    // from the section object, dereference the section
    // object, fill in information structure, optionally return the length of
    // the information structure, and return service status.
    //

    Status = ObReferenceObjectByHandle (SectionHandle,
                                        SECTION_QUERY,
                                        MmSectionObjectType,
                                        PreviousMode,
                                        (PVOID *)&Section,
                                        NULL);

    if (NT_SUCCESS(Status)) {

        try {

            if (SectionInformationClass == SectionBasicInformation) {
                ((PSECTION_BASIC_INFORMATION)SectionInformation)->BaseAddress =
                                           (PVOID)Section->Address.StartingVpn;

                ((PSECTION_BASIC_INFORMATION)SectionInformation)->MaximumSize =
                                                 Section->SizeOfSection;

                ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes =
                                                        0;

                if (Section->u.Flags.Image) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes =
                                                        SEC_IMAGE;
                }
                if (Section->u.Flags.Based) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_BASED;
                }
                if (Section->u.Flags.File) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_FILE;
                }
                if (Section->u.Flags.NoCache) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_NOCACHE;
                }
                if (Section->u.Flags.Reserve) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_RESERVE;
                }
                if (Section->u.Flags.Commit) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_COMMIT;
                }
                if (Section->Segment->ControlArea->u.Flags.GlobalMemory) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_GLOBAL;
                }

                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(SECTION_BASIC_INFORMATION);
                }
            }
            else {

                if (Section->u.Flags.Image == 0) {
                    Status = STATUS_SECTION_NOT_IMAGE;
                }
                else {
                    *((PSECTION_IMAGE_INFORMATION)SectionInformation) =
                        *Section->Segment->u2.ImageInformation;
    
                    if (ARGUMENT_PRESENT(ReturnLength)) {
                        *ReturnLength = sizeof(SECTION_IMAGE_INFORMATION);
                    }
                }
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode ();
        }

        ObDereferenceObject ((PVOID)Section);
    }
    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\pfnlist.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   pfnlist.c

Abstract:

    This module contains the routines to manipulate pages within the
    Page Frame Database.

Revision History:

--*/
#include "mi.h"

//
// The following line will generate an error if the number of colored
// free page lists is not 2, ie ZeroedPageList and FreePageList.  If
// this number is changed, the size of the FreeCount array in the kernel
// node structure (KNODE) must be updated.
//

C_ASSERT(StandbyPageList == 2);

KEVENT MmAvailablePagesEventHigh;

PFN_NUMBER MmTransitionPrivatePages;
PFN_NUMBER MmTransitionSharedPages;

#define MI_TALLY_TRANSITION_PAGE_ADDITION(Pfn) \
    if (Pfn->u3.e1.PrototypePte) { \
        MmTransitionSharedPages += 1; \
    } \
    else { \
        MmTransitionPrivatePages += 1; \
    }

#define MI_TALLY_TRANSITION_PAGE_REMOVAL(Pfn) \
    if (Pfn->u3.e1.PrototypePte) { \
        MmTransitionSharedPages -= 1; \
    } \
    else { \
        MmTransitionPrivatePages -= 1; \
    }

//
// This counter is used to determine if standby pages are being cannibalized
// for use as free pages and therefore more aging should be attempted.
//

ULONG MmStandbyRePurposed;

MM_LDW_WORK_CONTEXT MiLastChanceLdwContext;
    
ULONG MiAvailablePagesEventLowSets;
ULONG MiAvailablePagesEventHighSets;

extern ULONG MmSystemShutdown;

extern NTSTATUS MiLastModifiedWriteError;
extern NTSTATUS MiLastMappedWriteError;

PFN_NUMBER
FASTCALL
MiRemovePageByColor (
    IN PFN_NUMBER Page,
    IN ULONG PageColor
    );

VOID
FASTCALL
MiLogPfnInformation (
    IN PMMPFN Pfn1,
    IN USHORT Reason
    );

extern LOGICAL MiZeroingDisabled;

#if DBG
#if defined(_X86_) || defined(_AMD64_)
ULONG MiSaveStacks = 1;
#endif
#endif

#define MI_SNAP_PFN(_Pfn, dest, callerid)

VOID
MiInitializePfnTracing (
    VOID
    )
{
}


VOID
FASTCALL
MiInsertPageInFreeList (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the end of the free list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    PFN lock held.

--*/

{
    PFN_NUMBER last;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
    PMMPFNLIST ListHead;
    PMMCOLOR_TABLES ColorHead;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 8);

    MI_SNAP_PFN(Pfn1, FreePageList, 0x1);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_INSERTINFREELIST);
    }

#if DBG
#if defined(_X86_)
    if ((KeFeatureBits & KF_LARGE_PAGE) == 0) {
        Pfn1->u4.MustBeCached = 0;
    }
#endif
#endif

    ASSERT (Pfn1->u4.MustBeCached == 0);

    //
    // The page is being reused, so reset its priority.
    //

    MI_RESET_PFN_PRIORITY (Pfn1);

    ASSERT (Pfn1->u3.e1.Rom != 1);

    if (Pfn1->u3.e1.RemovalRequested == 1) {
        MiInsertPageInList (&MmBadPageListHead, PageFrameIndex);
        return;
    }

    ListHead = &MmFreePageListHead;
    ListName = FreePageList;

#if DBG
#if defined(_X86_) || defined(_AMD64_)
    if ((MiSaveStacks != 0) && (MmFirstReservedMappingPte != NULL)) {

        ULONG_PTR StackPointer;
        ULONG_PTR StackBytes;
        PULONG_PTR DataPage;
        PEPROCESS Process;

        MiGetStackPointer (&StackPointer);

        Process = PsGetCurrentProcess ();

        DataPage = MiMapPageInHyperSpaceAtDpc (Process, PageFrameIndex);

        //
        // Copy the callstack into the middle of the page (since special pool
        // is using the beginning of the freed page).
        //

        DataPage[PAGE_SIZE / (2 * sizeof(ULONG_PTR))] = StackPointer;
    
        //
        // For now, don't get fancy with copying more than what's in the current
        // stack page.  To do so would require checking the thread stack limits,
        // DPC stack limits, etc.
        //
    
        StackBytes = PAGE_SIZE - BYTE_OFFSET(StackPointer);
        DataPage[PAGE_SIZE / (2 * sizeof (ULONG_PTR)) + 1] = StackBytes;
    
        if (StackBytes != 0) {
    
            if (StackBytes > MI_STACK_BYTES) {
                StackBytes = MI_STACK_BYTES;
            }
    
            RtlCopyMemory ((PVOID)&DataPage[PAGE_SIZE / (2 * sizeof (ULONG_PTR)) + 2], (PVOID)StackPointer, StackBytes);
        }

        MiUnmapPageInHyperSpaceFromDpc (Process, DataPage);
    }
#endif
#endif

    ASSERT (Pfn1->u4.VerifierAllocation == 0);

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    ListHead->Total += 1;  // One more page on the list.

    //
    // Inserting the page at the front would make better use of the hardware
    // caches, but it makes debugging much harder because the pages get
    // reused so quickly.  For now, continue to insert at the back of the list.
    //

    last = ListHead->Blink;

    if (last != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT (last);
        Pfn2->u1.Flink = PageFrameIndex;
    }
    else {

        //
        // List is empty, add the page to the ListHead.
        //

        ListHead->Flink = PageFrameIndex;
    }

    ListHead->Blink = PageFrameIndex;
    Pfn1->u1.Flink = MM_EMPTY_LIST;
    Pfn1->u2.Blink = last;

    Pfn1->u3.e1.PageLocation = ListName;
    Pfn1->u4.InPageError = 0;
    Pfn1->u4.AweAllocation = 0;

    //
    // Update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    MmAvailablePages += 1;

    //
    // A page has just become available, check to see if the
    // page wait events should be signaled.
    //

    if (MmAvailablePages <= MM_HIGH_LIMIT) {
        if (MmAvailablePages == MM_HIGH_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
            MiAvailablePagesEventHighSets += 1;
        }
        else if (MmAvailablePages == MM_LOW_LIMIT) {
            KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
            MiAvailablePagesEventLowSets += 1;
        }
    }

    //
    // Signal applications if the freed page crosses a threshold.
    //

    if (MmAvailablePages == MmLowMemoryThreshold) {
        KeClearEvent (MiLowMemoryEvent);
    }
    else if (MmAvailablePages == MmHighMemoryThreshold) {
        KeSetEvent (MiHighMemoryEvent, 0, FALSE);
    }

#if defined(MI_MULTINODE)

    //
    // Increment the free page count for this node.
    //

    if (KeNumberNodes > 1) {
        KeNodeBlock[Pfn1->u3.e1.PageColor]->FreeCount[ListName]++;
    }

#endif

    //
    // We are adding a page to the free page list.
    // Add the page to the end of the correct colored page list.
    //

    Color = MI_GET_COLOR_FROM_LIST_ENTRY(PageFrameIndex, Pfn1);

    ColorHead = &MmFreePagesByColor[ListName][Color];

    if (ColorHead->Flink == MM_EMPTY_LIST) {

        //
        // This list is empty, add this as the first and last
        // entry.
        //

        Pfn1->u4.PteFrame = MM_EMPTY_LIST;
        ColorHead->Flink = PageFrameIndex;
    }
    else {
        Pfn2 = (PMMPFN)ColorHead->Blink;
        Pfn1->u4.PteFrame = MI_PFN_ELEMENT_TO_INDEX (Pfn2);
        Pfn2->OriginalPte.u.Long = PageFrameIndex;
    }
    ColorHead->Blink = Pfn1;
    ColorHead->Count += 1;
    Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

    if ((ListHead->Total >= MmMinimumFreePagesToZero) &&
        (MmZeroingPageThreadActive == FALSE)) {

        //
        // There are enough pages on the free list, start
        // the zeroing page thread.
        //

        MmZeroingPageThreadActive = TRUE;
        KeSetEvent (&MmZeroingPageEvent, 0, FALSE);
    }

    return;
}

VOID
FASTCALL
MiInsertPageInList (
    IN PMMPFNLIST ListHead,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the end of the specified list (standby,
    bad, zeroed, modified).

Arguments:

    ListHead - Supplies the listhead of the list in which to insert the
               specified physical page.

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER last;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
#if MI_BARRIER_SUPPORTED
    ULONG BarrierStamp;
#endif

    ASSERT (ListHead != &MmFreePageListHead);

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    ListName = ListHead->ListName;

#if DBG
    if (ListName != BadPageList) {

#if defined(_X86_)
        if ((KeFeatureBits & KF_LARGE_PAGE) == 0) {
            Pfn1->u4.MustBeCached = 0;
        }
#endif

        ASSERT (Pfn1->u4.MustBeCached == 0);
    }
#endif

    MI_SNAP_PFN(Pfn1, ListName, 0x2);

#if DBG
    if (MmDebug & MM_DBG_PAGE_REF_COUNT) {

        if ((ListName == StandbyPageList) || (ListName == ModifiedPageList)) {

            PMMPTE PointerPte;
            PEPROCESS Process;

            if ((Pfn1->u3.e1.PrototypePte == 1)  &&
                (MmIsAddressValid (Pfn1->PteAddress))) {

                Process = NULL;
                PointerPte = Pfn1->PteAddress;
            }
            else {

                //
                // The page containing the prototype PTE is not valid,
                // map the page into hyperspace and reference it that way.
                //

                Process = PsGetCurrentProcess ();
                PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
                PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                        MiGetByteOffset(Pfn1->PteAddress));
            }

            ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) ||
                    (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) == PageFrameIndex));
            ASSERT (PointerPte->u.Soft.Transition == 1);
            ASSERT (PointerPte->u.Soft.Prototype == 0);
            if (Process != NULL) {
                MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
            }
        }
    }

    if ((ListName == StandbyPageList) || (ListName == ModifiedPageList)) {
        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
           (Pfn1->OriginalPte.u.Soft.Transition == 1)) {
            KeBugCheckEx (MEMORY_MANAGEMENT, 0x8888, 0,0,0);
        }
    }
#endif

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u3.e1.Rom != 1);

    if (ListHead == &MmStandbyPageListHead) {
        ListHead = &MmStandbyPageListByPriority [Pfn1->u4.Priority];
        ASSERT (ListHead->ListName == ListName);
    }

    ListHead->Total += 1;  // One more page on the list.

    if (ListHead == &MmModifiedPageListHead) {

        //
        // On MIPS R4000 modified pages destined for the paging file are
        // kept on separate lists which group pages of the same color
        // together.
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            //
            // This page is destined for the paging file (not
            // a mapped file).  Change the list head to the
            // appropriate colored list head.
            //

#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
            ListHead = &MmModifiedPageListByColor [Pfn1->u3.e1.PageColor];
#else
            ListHead = &MmModifiedPageListByColor [0];
#endif
            ASSERT (ListHead->ListName == ListName);
            ListHead->Total += 1;
            MmTotalPagesForPagingFile += 1;
        }
        else {

            //
            // This page is destined for a mapped file (not
            // the paging file).  If there are no other pages currently
            // destined for the mapped file, start our timer so that we can
            // ensure that these pages make it to disk even if we don't pile
            // up enough of them to trigger the modified page writer or need
            // the memory.  If we don't do this here, then for this scenario,
            // only an orderly system shutdown will write them out (days,
            // weeks, months or years later) and any power out in between
            // means we'll have lost the data.
            //

            if (ListHead->Total - MmTotalPagesForPagingFile == 1) {

                //
                // Start the DPC timer because we're the first on the list.
                //

                if (MiTimerPending == FALSE) {
                    MiTimerPending = TRUE;

                    KeSetTimerEx (&MiModifiedPageWriterTimer,
                                  MiModifiedPageLife,
                                  0,
                                  &MiModifiedPageWriterTimerDpc);
                }
            }
        }
    }
    else if ((Pfn1->u3.e1.RemovalRequested == 1) &&
             (ListName <= StandbyPageList)) {

        ListHead->Total -= 1;  // Undo previous increment

        if (ListName == StandbyPageList) {
            Pfn1->u3.e1.PageLocation = StandbyPageList;
            MiRestoreTransitionPte (Pfn1);
        }

        ListHead = &MmBadPageListHead;
        ASSERT (ListHead->ListName == BadPageList);
        ListHead->Total += 1;  // One more page on the list.
        ListName = BadPageList;
    }

    //
    // Pages destined for the zeroed list go to the front to take advantage
    // of cache locality.  All other lists get the page at the rear for LRU
    // longest life.
    //

    if (ListName == ZeroedPageList) {

        last = ListHead->Flink;

        ListHead->Flink = PageFrameIndex;

        Pfn1->u1.Flink = last;
        Pfn1->u2.Blink = MM_EMPTY_LIST;

        if (last != MM_EMPTY_LIST) {
            Pfn2 = MI_PFN_ELEMENT (last);
            Pfn2->u2.Blink = PageFrameIndex;
        }
        else {
            ListHead->Blink = PageFrameIndex;
        }
    }
    else {

        last = ListHead->Blink;

        if (last != MM_EMPTY_LIST) {
            Pfn2 = MI_PFN_ELEMENT (last);
            Pfn2->u1.Flink = PageFrameIndex;
        }
        else {

            //
            // List is empty, add the page to the ListHead.
            //

            ListHead->Flink = PageFrameIndex;
        }

        ListHead->Blink = PageFrameIndex;
        Pfn1->u1.Flink = MM_EMPTY_LIST;
        Pfn1->u2.Blink = last;
    }

    Pfn1->u3.e1.PageLocation = ListName;

    //
    // If the page was placed on the standby or zeroed list,
    // update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    if (ListName <= StandbyPageList) {

        MmAvailablePages += 1;

        //
        // A page has just become available, check to see if the
        // page wait events should be signaled.
        //

        if (MmAvailablePages <= MM_HIGH_LIMIT) {
            if (MmAvailablePages == MM_HIGH_LIMIT) {
                KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
                MiAvailablePagesEventHighSets += 1;
            }
            else if (MmAvailablePages == MM_LOW_LIMIT) {
                KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
                MiAvailablePagesEventLowSets += 1;
            }
        }

        //
        // Signal applications if the freed page crosses a threshold.
        //

        if (MmAvailablePages == MmLowMemoryThreshold) {
            KeClearEvent (MiLowMemoryEvent);
        }
        else if (MmAvailablePages == MmHighMemoryThreshold) {
            KeSetEvent (MiHighMemoryEvent, 0, FALSE);
        }

        if (ListName <= FreePageList) {

            PMMCOLOR_TABLES ColorHead;

            ASSERT (ListName == ZeroedPageList);
            ASSERT (Pfn1->u4.InPageError == 0);

#if defined(MI_MULTINODE)

            //
            // Increment the zero page count for this node.
            //

            if (KeNumberNodes > 1) {
                KeNodeBlock[Pfn1->u3.e1.PageColor]->FreeCount[ZeroedPageList]++;
            }
#endif

            //
            // We are adding a page to the zeroed page list.
            // Add the page to the front of the correct colored page list.
            //

            Color = MI_GET_COLOR_FROM_LIST_ENTRY (PageFrameIndex, Pfn1);

            ColorHead = &MmFreePagesByColor[ZeroedPageList][Color];

            last = ColorHead->Flink;

            Pfn1->OriginalPte.u.Long = last;
            Pfn1->u4.PteFrame = MM_EMPTY_LIST;

            ColorHead->Flink = PageFrameIndex;

            if (last != MM_EMPTY_LIST) {
                Pfn2 = MI_PFN_ELEMENT (last);
                Pfn2->u4.PteFrame = PageFrameIndex;
            }
            else {
                ColorHead->Blink = (PVOID) Pfn1;
            }

            ColorHead->Count += 1;

#if MI_BARRIER_SUPPORTED
            MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
            Pfn1->u4.PteFrame = BarrierStamp;
#endif
        }
        else {

            //
            // Transition page list so tally it appropriately.
            //

            MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);
        }

        return;
    }

    //
    // Check to see if there are too many modified pages.
    //

    if (ListName == ModifiedPageList) {

        //
        // Transition page list so tally it appropriately.
        //

        MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {
            ASSERT (Pfn1->OriginalPte.u.Soft.PageFileHigh == 0);
        }

        PsGetCurrentProcess()->ModifiedPageCount += 1;

        if (MmAvailablePages < MM_PLENTY_FREE_LIMIT) {

            //
            // If necessary, start the modified page writer.
            //

            if (MmModifiedPageListHead.Total >= MmModifiedPageMaximum) {
                KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
            }
            else if ((MmAvailablePages < MM_TIGHT_LIMIT) &&
                     (MmModifiedPageListHead.Total >= MmModifiedWriteClusterSize)) {

                KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
            }
        }
    }
    else if (ListName == ModifiedNoWritePageList) {
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
        MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);
    }

    return;
}


VOID
FASTCALL
MiInsertZeroListAtBack (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the end of the zeroed list.
    This is only needed at system initialization to keep the higher
    physically numbered pages at the front of the zeroed list (normally
    we only put zeroed pages in the front of the list for better cache
    locality).

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER last;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
    PMMCOLOR_TABLES ColorHead;
    PMMPFNLIST ListHead;
#if MI_BARRIER_SUPPORTED
    ULONG BarrierStamp;
#endif

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_PFN (Pfn1, ZeroedPageList, 0x2);

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u4.MustBeCached == 0);
    ASSERT (Pfn1->u3.e1.Rom == 0);

    if (Pfn1->u3.e1.RemovalRequested == 0) {
        ListHead = &MmZeroedPageListHead;
        ListName = ZeroedPageList;
        MmZeroedPageListHead.Total += 1;    // One more page on the list.
    }
    else {
        ListHead = &MmBadPageListHead;
        ListName = BadPageList;
        ListHead->Total += 1;  // One more page on the list.
    }

    last = ListHead->Blink;

    if (last != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT (last);
        Pfn2->u1.Flink = PageFrameIndex;
    }
    else {

        //
        // List is empty, add the page to the ListHead.
        //

        ListHead->Flink = PageFrameIndex;
    }

    ListHead->Blink = PageFrameIndex;
    Pfn1->u1.Flink = MM_EMPTY_LIST;
    Pfn1->u2.Blink = last;

    Pfn1->u3.e1.PageLocation = ListName;

    if (ListHead == &MmBadPageListHead) {
        return;
    }

    //
    // Update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    MmAvailablePages += 1;

    //
    // A page has just become available, check to see if the
    // page wait events should be signaled.
    //

    if (MmAvailablePages <= MM_HIGH_LIMIT) {
        if (MmAvailablePages == MM_HIGH_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
            MiAvailablePagesEventHighSets += 1;
        }
        else if (MmAvailablePages == MM_LOW_LIMIT) {
            KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
            MiAvailablePagesEventLowSets += 1;
        }
    }

    //
    // Signal applications if the freed page crosses a threshold.
    //

    if (MmAvailablePages == MmLowMemoryThreshold) {
        KeClearEvent (MiLowMemoryEvent);
    }
    else if (MmAvailablePages == MmHighMemoryThreshold) {
        KeSetEvent (MiHighMemoryEvent, 0, FALSE);
    }

    ASSERT (ListName == ZeroedPageList);
    ASSERT (Pfn1->u4.InPageError == 0);

#if defined(MI_MULTINODE)

    //
    // Increment the zero page count for this node.
    //

    if (KeNumberNodes > 1) {
        KeNodeBlock[Pfn1->u3.e1.PageColor]->FreeCount[ZeroedPageList]++;
    }
#endif

    //
    // We are adding a page to the zeroed page list.
    // Add the page to the back of the correct colored page list.
    //

    Color = MI_GET_COLOR_FROM_LIST_ENTRY (PageFrameIndex, Pfn1);

    ColorHead = &MmFreePagesByColor[ZeroedPageList][Color];

    Pfn2 = ColorHead->Blink;

    if (Pfn2 == (PVOID) MM_EMPTY_LIST) {
        ColorHead->Flink = PageFrameIndex;
        Pfn1->u4.PteFrame = MM_EMPTY_LIST;
    }
    else {
        Pfn2->OriginalPte.u.Long = PageFrameIndex;
        Pfn1->u4.PteFrame = Pfn2 - MmPfnDatabase;
    }

    Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

    ColorHead->Blink = (PVOID) Pfn1;

    ColorHead->Count += 1;

#if MI_BARRIER_SUPPORTED
    MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
    Pfn1->u4.PteFrame = BarrierStamp;
#endif

    return;
}


VOID
FASTCALL
MiInsertStandbyListAtFront (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the front of the standby list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    PFN lock held.

--*/

{
    PFN_NUMBER first;
    PMMPFNLIST ListHead;
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) && (PageFrameIndex <= MmHighestPhysicalPage) &&
        (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_PFN(Pfn1, StandbyPageList, 0x3);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 9);

    ASSERT (Pfn1->u4.MustBeCached == 0);

#if DBG
    if (MmDebug & MM_DBG_PAGE_REF_COUNT) {

        PMMPTE PointerPte;
        PEPROCESS Process;

        if ((Pfn1->u3.e1.PrototypePte == 1)  &&
                (MmIsAddressValid (Pfn1->PteAddress))) {
            PointerPte = Pfn1->PteAddress;
            Process = NULL;
        }
        else {

            //
            // The page containing the prototype PTE is not valid,
            // map the page into hyperspace and reference it that way.
            //

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) ||
                (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) == PageFrameIndex));
        ASSERT (PointerPte->u.Soft.Transition == 1);
        ASSERT (PointerPte->u.Soft.Prototype == 0);
        if (Process != NULL) {
            MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
        }
    }

    if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
       (Pfn1->OriginalPte.u.Soft.Transition == 1)) {
        KeBugCheckEx (MEMORY_MANAGEMENT, 0x8889, 0,0,0);
    }
#endif

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u3.e1.PrototypePte == 1);
    ASSERT (Pfn1->u3.e1.Rom != 1);

    MmTransitionSharedPages += 1;

    ListHead = &MmStandbyPageListByPriority [Pfn1->u4.Priority];

    ListHead->Total += 1;  // One more page on the list.

    first = ListHead->Flink;
    if (first == MM_EMPTY_LIST) {

        //
        // List is empty add the page to the ListHead.
        //

        ListHead->Blink = PageFrameIndex;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT (first);
        Pfn2->u2.Blink = PageFrameIndex;
    }

    ListHead->Flink = PageFrameIndex;
    Pfn1->u2.Blink = MM_EMPTY_LIST;
    Pfn1->u1.Flink = first;
    Pfn1->u3.e1.PageLocation = StandbyPageList;

    //
    // If the page was placed on the free, standby or zeroed list,
    // update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    MmAvailablePages += 1;

    //
    // A page has just become available, check to see if the
    // page wait events should be signaled.
    //

    if (MmAvailablePages <= MM_HIGH_LIMIT) {
        if (MmAvailablePages == MM_HIGH_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
            MiAvailablePagesEventHighSets += 1;
        }
        else if (MmAvailablePages == MM_LOW_LIMIT) {
            KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
            MiAvailablePagesEventLowSets += 1;
        }
    }

    //
    // Signal applications if the freed page crosses a threshold.
    //

    if (MmAvailablePages == MmLowMemoryThreshold) {
        KeClearEvent (MiLowMemoryEvent);
    }
    else if (MmAvailablePages == MmHighMemoryThreshold) {
        KeSetEvent (MiHighMemoryEvent, 0, FALSE);
    }

    return;
}

PFN_NUMBER
FASTCALL
MiRemovePageFromList (
    IN PMMPFNLIST ListHead
    )

/*++

Routine Description:

    This procedure removes a page from the head of the specified list (free,
    standby or zeroed).

    This routine clears the flags word in the PFN database, hence the
    PFN information for this page must be initialized.

Arguments:

    ListHead - Supplies the listhead of the list in which to remove the
               specified physical page.

Return Value:

    The physical page number removed from the specified list.

Environment:

    PFN lock held.

--*/

{
    PMMCOLOR_TABLES ColorHead;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    MM_PFN_LOCK_ASSERT();

    //
    // For standby removals, the caller is responsible for specifying
    // which prioritized standby list to remove from.
    //

    ASSERT (ListHead != &MmStandbyPageListHead);

    if (ListHead->Total == 0) {
        KeBugCheckEx (PFN_LIST_CORRUPT, 1, (ULONG_PTR)ListHead, MmAvailablePages, 0);
    }

    ListName = ListHead->ListName;
    ASSERT (ListName != ModifiedPageList);

    //
    // Decrement the count of pages on the list and remove the first
    // page from the list.
    //

    ListHead->Total -= 1;
    PageFrameIndex = ListHead->Flink;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_REMOVEPAGEFROMLIST);
    }

    ListHead->Flink = Pfn1->u1.Flink;

    //
    // Zero the flink and blink in the PFN database element.
    //

    Pfn1->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Pfn1->u2.Blink = 0;

    //
    // If the last page was removed (the ListHead->Flink is now
    // MM_EMPTY_LIST) make the Listhead->Blink MM_EMPTY_LIST as well.
    //

    if (ListHead->Flink != MM_EMPTY_LIST) {

        //
        // Make the PFN element blink point to MM_EMPTY_LIST signifying this
        // is the first page in the list.
        //

        Pfn2 = MI_PFN_ELEMENT (ListHead->Flink);
        Pfn2->u2.Blink = MM_EMPTY_LIST;
    }
    else {
        ListHead->Blink = MM_EMPTY_LIST;
    }

    //
    // We now have one less page available.
    //

    ASSERT (ListName <= StandbyPageList);

    //
    // Signal if allocating this page caused a threshold cross.
    //

    if (MmAvailablePages == MmHighMemoryThreshold) {
        KeClearEvent (MiHighMemoryEvent);
    }
    else if (MmAvailablePages == MmLowMemoryThreshold) {
        KeSetEvent (MiLowMemoryEvent, 0, FALSE);
    }

    MmAvailablePages -= 1;

    if (ListName == StandbyPageList) {

        //
        // This page is currently in transition, restore the PTE to
        // its original contents so this page can be reused.
        //

        MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn1);
        MiRestoreTransitionPte (Pfn1);
    }

    if (MmAvailablePages < MmMinimumFreePages) {

        //
        // Obtain free pages.
        //

        MiObtainFreePages ();
    }

    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    //
    // Zero the PFN flags longword.
    //

    Color = Pfn1->u3.e1.PageColor;
    CacheAttribute = Pfn1->u3.e1.CacheAttribute;
    ASSERT (Pfn1->u3.e1.RemovalRequested == 0);
    ASSERT (Pfn1->u3.e1.Rom == 0);
    Pfn1->u3.e2.ShortFlags = 0;
    Pfn1->u3.e1.PageColor = (USHORT) Color;
    Pfn1->u3.e1.CacheAttribute = CacheAttribute;

    if (ListName <= FreePageList) {

        //
        // Update the color lists.
        //

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(PageFrameIndex, Pfn1);
        ColorHead = &MmFreePagesByColor[ListName][Color];
        ASSERT (ColorHead->Flink == PageFrameIndex);
        ColorHead->Flink = (PFN_NUMBER) Pfn1->OriginalPte.u.Long;
        if (ColorHead->Flink != MM_EMPTY_LIST) {
            MI_PFN_ELEMENT (ColorHead->Flink)->u4.PteFrame = MM_EMPTY_LIST;
        }
        else {
            ColorHead->Blink = (PVOID) MM_EMPTY_LIST;
        }
        ASSERT (ColorHead->Count >= 1);
        ColorHead->Count -= 1;
    }

    return PageFrameIndex;
}

VOID
FASTCALL
MiUnlinkPageFromList (
    IN PMMPFN Pfn
    )

/*++

Routine Description:

    This procedure removes a page from the middle of a list.  This is
    designed for the faulting of transition pages from the standby and
    modified list and making them active and valid again.

Arguments:

    Pfn - Supplies a pointer to the PFN database element for the physical
          page to remove from the list.

Return Value:

    none.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PMMPFNLIST ListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn2;

    MM_PFN_LOCK_ASSERT();

    //
    // Page not on standby or modified list, check to see if the
    // page is currently being written by the modified page
    // writer, if so, just return this page.  The reference
    // count for the page will be incremented, so when the modified
    // page write completes, the page will not be put back on
    // the list, rather, it will remain active and valid.
    //

    if (Pfn->u3.e2.ReferenceCount > 0) {

        //
        // The page was not on any "transition lists", check to see
        // if this has I/O in progress.
        //

        if (Pfn->u2.ShareCount == 0) {
            return;
        }
        KdPrint(("MM:attempt to remove page from wrong page list\n"));
        KeBugCheckEx (PFN_LIST_CORRUPT,
                      2,
                      MI_PFN_ELEMENT_TO_INDEX (Pfn),
                      MmHighestPhysicalPage,
                      Pfn->u3.e2.ReferenceCount);
    }

    ListHead = MmPageLocationList[Pfn->u3.e1.PageLocation];

    //
    // Must not remove pages from free or zeroed without updating
    // the colored lists.
    //

    ASSERT (ListHead->ListName >= StandbyPageList);

    if (ListHead == &MmStandbyPageListHead) {

        ASSERT (Pfn->u3.e1.Rom == 0);

        ListHead = &MmStandbyPageListByPriority [Pfn->u4.Priority];

        //
        // Signal if allocating this page caused a threshold cross.
        //

        if (MmAvailablePages == MmHighMemoryThreshold) {
            KeClearEvent (MiHighMemoryEvent);
        }
        else if (MmAvailablePages == MmLowMemoryThreshold) {
            KeSetEvent (MiLowMemoryEvent, 0, FALSE);
        }

        //
        // We now have one less page available.
        //

        MmAvailablePages -= 1;

        MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn);

        if (MmAvailablePages < MmMinimumFreePages) {

            //
            // Obtain free pages.
            //

            MiObtainFreePages ();
        }
    }
    else if (ListHead == &MmModifiedPageListHead) {

        if (Pfn->OriginalPte.u.Soft.Prototype == 0) {

            //
            // This page is destined for the paging file (not
            // a mapped file).  Change the list head to the
            // appropriate colored list head.
            //
            // On MIPS R4000 modified pages destined for the paging file are
            // kept on separate lists which group pages of the same color
            // together.
            //

            ListHead->Total -= 1;
            MmTotalPagesForPagingFile -= 1;
#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
            ListHead = &MmModifiedPageListByColor [Pfn->u3.e1.PageColor];
#else
            ListHead = &MmModifiedPageListByColor [0];
#endif
        }

        MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn);
    }
    else if (ListHead == &MmModifiedNoWritePageListHead) {
        MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn);
    }

    ASSERT (Pfn->u3.e1.WriteInProgress == 0);
    ASSERT (Pfn->u3.e1.ReadInProgress == 0);
    ASSERT (ListHead->Total != 0);

    Next = Pfn->u1.Flink;
    Previous = Pfn->u2.Blink;

    if (Next != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }
    else {
        ListHead->Blink = Previous;
    }

    if (Previous != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }
    else {
        ListHead->Flink = Next;
    }

    Pfn->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Pfn->u2.Blink = 0;

    ListHead->Total -= 1;

    return;
}

VOID
MiUnlinkFreeOrZeroedPage (
    IN PMMPFN Pfn
    )

/*++

Routine Description:

    This procedure removes a page from the middle of a list.  This is
    designed for the removing of free or zeroed pages from the middle of
    their lists.

Arguments:

    Pfn - Supplies a PFN element to remove from the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER Page;
    PMMPFNLIST ListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn2;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    MMLISTS ListName;

    Page = MI_PFN_ELEMENT_TO_INDEX (Pfn);

    MM_PFN_LOCK_ASSERT();

    ListHead = MmPageLocationList[Pfn->u3.e1.PageLocation];
    ListName = ListHead->ListName;
    ASSERT (ListHead->Total != 0);
    ListHead->Total -= 1;

    ASSERT (ListName <= FreePageList);
    ASSERT (Pfn->u3.e1.WriteInProgress == 0);
    ASSERT (Pfn->u3.e1.ReadInProgress == 0);

    Next = Pfn->u1.Flink;
    Previous = Pfn->u2.Blink;

    if (Next != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }
    else {
        ListHead->Blink = Previous;
    }

    if (Previous == MM_EMPTY_LIST) {
        ListHead->Flink = Next;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }

    //
    // Remove the page from its colored list.
    //

    Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, Pfn);

    ColorHead = &MmFreePagesByColor[ListName][Color];

    Next = ColorHead->Flink;

    if (Next == Page) {
        ColorHead->Flink = (PFN_NUMBER) Pfn->OriginalPte.u.Long;
        if (ColorHead->Flink != MM_EMPTY_LIST) {
            MI_PFN_ELEMENT(ColorHead->Flink)->u4.PteFrame = MM_EMPTY_LIST;
        }
        else {
            ColorHead->Blink = (PVOID) MM_EMPTY_LIST;
        }
    }
    else {

        ASSERT (Pfn->u4.PteFrame != MM_EMPTY_LIST);

        Pfn2 = MI_PFN_ELEMENT (Pfn->u4.PteFrame);
        Pfn2->OriginalPte.u.Long = Pfn->OriginalPte.u.Long;

        if (Pfn->OriginalPte.u.Long != MM_EMPTY_LIST) {
            Pfn2 = MI_PFN_ELEMENT (Pfn->OriginalPte.u.Long);
            Pfn2->u4.PteFrame = Pfn->u4.PteFrame;
        }
        else {
            ColorHead->Blink = Pfn2;
        }
    }

    Pfn->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Pfn->u2.Blink = 0;

    ASSERT (ColorHead->Count >= 1);
    ColorHead->Count -= 1;

    //
    // Decrement availability count.
    //

#if defined(MI_MULTINODE)

    if (KeNumberNodes > 1) {
        MI_NODE_FROM_COLOR(Color)->FreeCount[ListName]--;
    }

#endif

    //
    // Signal if allocating this page caused a threshold cross.
    //

    if (MmAvailablePages == MmHighMemoryThreshold) {
        KeClearEvent (MiHighMemoryEvent);
    }
    else if (MmAvailablePages == MmLowMemoryThreshold) {
        KeSetEvent (MiLowMemoryEvent, 0, FALSE);
    }

    MmAvailablePages -= 1;

    if (MmAvailablePages < MmMinimumFreePages) {

        //
        // Obtain free pages.
        //

        MiObtainFreePages ();
    }

    return;
}

LOGICAL
MiDereferenceLastChanceLdw (
    IN PMM_LDW_WORK_CONTEXT LdwContext
    )
{
    KIRQL OldIrql;

    if (LdwContext != &MiLastChanceLdwContext) {
        return FALSE;
    }

    LOCK_PFN (OldIrql);

    ASSERT (MiLastChanceLdwContext.FileObject != NULL);
    MiLastChanceLdwContext.FileObject = NULL;

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

BOOLEAN Mi4dBreak = TRUE;
ULONG Mi4dFiles;
ULONG Mi4dPages;

VOID
MiNoPagesLastChance (
    IN ULONG Limit                
    )
{
    ULONG i;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PSUBSECTION Subsection;
    PFN_NUMBER ModifiedPage;
    PFN_NUMBER PagesOnList;
    PFN_NUMBER FreeSpace;
    PFN_NUMBER GrowthLeft;
    ULONG BitField;
    NTSTATUS Status;
    ULONG BugcheckCode;
    PCONTROL_AREA ControlArea;
    PFILE_OBJECT FilePointer;
    PMM_LDW_WORK_CONTEXT LdwContext;
    LOGICAL MarchOn;

    //
    // This bugcheck can occur for the following reasons:
    //
    // A driver has blocked, deadlocking the modified or mapped
    // page writers.  Examples of this include mutex deadlocks or
    // accesses to paged out memory in filesystem drivers, filter
    // drivers, etc.  This indicates a driver bug.
    //
    // The storage driver(s) are not processing requests.  Examples
    // of this are stranded queues, non-responding drives, etc.  This
    // indicates a driver bug.
    //
    // Not enough pool is available for the storage stack to write out
    // modified pages.  This indicates a driver bug.
    //
    // A high priority realtime thread has starved the balance set
    // manager from trimming pages and/or starved the modified writer
    // from writing them out.  This indicates a bug in the component
    // that created this thread.
    //

    BitField = 0;
    Status = STATUS_SUCCESS;
    BugcheckCode = NO_PAGES_AVAILABLE;
    PagesOnList = MmTotalPagesForPagingFile;

    if (!NT_SUCCESS (MiLastMappedWriteError)) {
        Status = MiLastMappedWriteError;
        BitField |= 0x1;
    }

    if (!NT_SUCCESS (MiLastModifiedWriteError)) {
        Status = MiLastModifiedWriteError;
        BitField |= 0x2;
    }

    GrowthLeft = 0;
    FreeSpace = 0;

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        GrowthLeft += (MmPagingFile[i]->MaximumSize - MmPagingFile[i]->Size);
        FreeSpace += MmPagingFile[i]->FreeSpace;
    }

    if (FreeSpace < (4 * 1024 * 1024) / PAGE_SIZE) {
        BitField |= 0x4;
    }

    if (GrowthLeft < (4 * 1024 * 1024) / PAGE_SIZE) {
        BitField |= 0x8;
    }

    if (MmSystemShutdown != 0) {

        //
        // Because applications are not terminated and drivers are
        // not unloaded, they can continue to access pages even after
        // the modified writer has terminated.  This can cause the
        // system to run out of pages since the pagefile(s) cannot be
        // used.
        //

        BugcheckCode = DISORDERLY_SHUTDOWN;
    }
    else if (MmModifiedNoWritePageListHead.Total >= (MmModifiedPageListHead.Total >> 2)) {
        BugcheckCode = DIRTY_NOWRITE_PAGES_CONGESTION;
        PagesOnList = MmModifiedNoWritePageListHead.Total;
    }
    else if (MmTotalPagesForPagingFile >= (MmModifiedPageListHead.Total >> 2)) {
        BugcheckCode = NO_PAGES_AVAILABLE;
    }
    else {
        BugcheckCode = DIRTY_MAPPED_PAGES_CONGESTION;
    }

    if ((KdPitchDebugger == FALSE) && (KdDebuggerNotPresent == FALSE)) {

        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "Without a debugger attached, the following bugcheck would have occurred.\n");
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "%4lx ", BugcheckCode);
    
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "%p %p %p %p\n",
                  MmModifiedPageListHead.Total,
                  PagesOnList,
                  BitField,
                  Status);
    
        //
        // Pop into the debugger (even on free builds) to determine
        // the cause of the starvation and march on.
        //
    
        if (Mi4dBreak == TRUE) {
            DbgBreakPoint ();
        }

        MarchOn = TRUE;
    }
    else {
        MarchOn = FALSE;
    }

    //
    // Toss modified mapped pages until the limit is reached allowing this
    // thread to make forward progress.  Each toss represents lost data so
    // this is very bad.  But if the filesystem is deadlocked then there is
    // nothing else that can be done other than to crash.  This tossing
    // provides the marginal benefit that at least a delayed write popup
    // event is queued (with the name of the file) and the system is kept
    // alive - the administrator will then *HAVE* to fix these files.  The
    // alternative would be to continue to bugcheck the system which not
    // only impacts availability, but also loses *all* the delayed mapped
    // writes without any event queueing to inform the administrator about
    // which files have been corrupted.
    //

    FilePointer = NULL;
    LdwContext = &MiLastChanceLdwContext;

    LOCK_PFN (OldIrql);

    //
    // If enough free pages have been created then return to give our
    // caller another chance.  Note another thread may have already done 
    // this for us.
    //

    if (MmAvailablePages >= Limit) {
        UNLOCK_PFN (OldIrql);
        return;
    }

    if (LdwContext->FileObject != NULL) {

        //
        // Some other thread is tossing pages right now, just return for
        // another delay.
        //

        UNLOCK_PFN (OldIrql);
        return;
    }

    ModifiedPage = MmModifiedPageListHead.Flink;

    while (ModifiedPage != MM_EMPTY_LIST) {

        //
        // There are modified mapped pages.
        //

        Pfn1 = MI_PFN_ELEMENT (ModifiedPage);

        ModifiedPage = Pfn1->u1.Flink;

        if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {

            //
            // This page is destined for a file.  Chuck it and any other
            // pages destined for the same file.
            //

            Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);

            ControlArea = Subsection->ControlArea;
            ASSERT (ControlArea->FilePointer != NULL);

            if ((!ControlArea->u.Flags.Image) &&
                (!ControlArea->u.Flags.NoModifiedWriting) &&
                ((FilePointer == NULL) || (FilePointer == ControlArea->FilePointer))) {

                ASSERT (ControlArea->NumberOfPfnReferences >= 1);

                MiUnlinkPageFromList (Pfn1);

                //
                // Use the reference count macros so we don't prematurely
                // free the page because the physical page may have
                // references already.
                //

                MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

                //
                // Clear the dirty bit in the PFN entry so the page will go
                // to the standby instead of the modified list.  If the page
                // is not modified again before it is reused, the data will
                // be lost.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x1A);

                //
                // Put the page on the standby list if it is
                // the last reference to the page.
                //

                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);

                Mi4dPages += 1;

                if (FilePointer == NULL) {
                    FilePointer = ControlArea->FilePointer;
                    ASSERT (FilePointer != NULL);
                    ObReferenceObject (FilePointer);
                }

                //
                // Search for any other modified pages in this control area
                // because the entire file needs to be fixed anyway.
                //
            }
        }
    }

    UNLOCK_PFN (OldIrql);

    //
    // If we were able to free up some pages then return to our caller for
    // another attempt/wait if necessary.
    //
    // Otherwise if no debugger is attached, then there's no point in
    // continuing so bugcheck.
    //

    if (FilePointer != NULL) {

        ASSERT (LdwContext->FileObject == NULL);
        LdwContext->FileObject = FilePointer;
        ExInitializeWorkItem (&LdwContext->WorkItem,
                              MiLdwPopupWorker,
                              (PVOID)LdwContext);

        ExQueueWorkItem (&LdwContext->WorkItem, DelayedWorkQueue);
        Mi4dFiles += 1;

        return;
    }

    if (MarchOn == FALSE) {
        KeBugCheckEx (BugcheckCode,
                      MmModifiedPageListHead.Total,
                      PagesOnList,
                      BitField,
                      Status);
    }

    return;
}


ULONG
FASTCALL
MiEnsureAvailablePageOrWait (
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This procedure ensures that a physical page is available on
    the zeroed, free or standby list such that the next call to remove a
    page absolutely will not block.  This is necessary as blocking would
    require a wait which could cause a deadlock condition.

    If a page is available the function returns immediately with a value
    of FALSE indicating no wait operation was performed.  If no physical
    page is available, the thread enters a wait state and the function
    returns the value TRUE when the wait operation completes.

Arguments:

    Process - Supplies a pointer to the current process if, and only if,
              the working set mutex is currently held and should
              be released if a wait operation is issued.  Supplies
              the value NULL otherwise.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    FALSE - if a page was immediately available.

    TRUE - if a wait operation occurred before a page became available.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PVOID Event;
    NTSTATUS Status;
    ULONG Limit;
    LOGICAL WsHeldSafe;
    LOGICAL WsHeldShared;
    PETHREAD Thread;
    PMMSUPPORT Ws;
    PULONG EventSetPointer;
    ULONG EventSetCounter;
    
    MM_PFN_LOCK_ASSERT();

    if (MmAvailablePages >= MM_HIGH_LIMIT) {

        //
        // Pages are available.
        //

        return FALSE;
    }

    //
    // If this thread has explicitly disabled APCs (FsRtlEnterFileSystem
    // does this), then it may be holding resources or mutexes that may in
    // turn be blocking memory making threads from making progress.  We'd
    // like to detect this but cannot (without changing the
    // FsRtlEnterFileSystem macro) since other components (win32k for
    // example) also enter critical regions and then access paged pool.
    //
    // At least give system threads a free pass as they may be worker
    // threads processing potentially blocking items drivers have queued.
    //

    Thread = PsGetCurrentThread ();

    if (Thread->MemoryMaker == 1) {
        if (MmAvailablePages >= MM_LOW_LIMIT) {
            return FALSE;
        }
        Limit = MM_LOW_LIMIT;
        Event = (PVOID) &MmAvailablePagesEvent;
        EventSetPointer = &MiAvailablePagesEventLowSets;
    }
    else {
        Limit = MM_HIGH_LIMIT;
        Event = (PVOID) &MmAvailablePagesEventHigh;
        EventSetPointer = &MiAvailablePagesEventHighSets;
    }

    EventSetCounter = *EventSetPointer;
    ASSERT (MmAvailablePages < Limit);

    //
    // Initializing WsHeldSafe is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Ws = NULL;
    WsHeldSafe = FALSE;
    WsHeldShared = FALSE;

    do {

        KeClearEvent ((PKEVENT)Event);

        UNLOCK_PFN (OldIrql);

        if (Process == HYDRA_PROCESS) {
            Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
            UNLOCK_WORKING_SET (Thread, Ws);
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Handle both cases here and below.
            //

            UNLOCK_WS_REGARDLESS (Thread, Process, WsHeldSafe, WsHeldShared);
        }
        else {
            if ((Thread->OwnsSystemWorkingSetExclusive) ||
                (Thread->OwnsSystemWorkingSetShared)) {

                Ws = &MmSystemCacheWs;
                UNLOCK_WORKING_SET (Thread, Ws);
            }
        }

        //
        // Wait 70 seconds for pages to become available.  Note this number
        // was picked because it is larger than both SCSI and redirector
        // timeout values.
        //
        // Unfortunately we are using a notification event and may be waiting
        // in some cases with APCs enabled.  Thus inside KeWait, the APC is
        // delivered and then the event gets signaled.  The APC is handled,
        // but the available pages are taken and the event above cleared
        // by other thread(s).  Then this thread looks at the event and
        // sees it isn't signaled (and thus doesn't realize it ever happened)
        // and so goes back into a wait state.  In a pathological case (we
        // have seen this happen), this scenario repeats until the thread's
        // timeout expires and gets returned as such, even though the event
        // has been signaled many times.
        //

        Status = KeWaitForSingleObject (Event,
                                        WrFreePage,
                                        KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER) &MmSeventySeconds);

        if (Status == STATUS_TIMEOUT) {

            if (EventSetCounter == *EventSetPointer) {
                MiNoPagesLastChance (Limit);
            }
        }

        EventSetCounter = *EventSetPointer;

        if (Ws != NULL) {
            LOCK_WORKING_SET (Thread, Ws);
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Reacquire it in the same manner our caller did.
            //

            LOCK_WS_REGARDLESS (Thread, Process, WsHeldSafe, WsHeldShared);
        }

        LOCK_PFN (OldIrql);

    } while (MmAvailablePages < Limit);

    return TRUE;
}


PFN_NUMBER
FASTCALL
MiRemoveZeroPage (
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a zero page from either the zeroed, free
    or standby lists (in that order).  If no pages exist on the zeroed
    or free list a transition page is removed from the standby list
    and the PTE (may be a prototype PTE) which refers to this page is
    changed from transition back to its original contents.

    If the page is not obtained from the zeroed list, it is zeroed.

Arguments:

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

            The above was true when we were concerned about caches
            which are virtually indexed (ie MIPS).  Today we
            are more concerned that we get a good usage spread across
            the L2 caches of most machines.  These caches are physically
            indexed.  By gathering pages that would have the same
            index to the same color, then maximizing the color spread,
            we maximize the effective use of the caches.

            This has been extended for NUMA machines.  The high part
            of the color gives the node color (basically node number).
            If we cannot allocate a page of the requested color, we
            try to allocate a page on the same node before taking a
            page from a different node.

Return Value:

    The physical page number removed from the specified list.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER Page;
    PMMPFN Pfn1;
    PMMCOLOR_TABLES FreePagesByColor;
    PMMPFNLIST ListHead;
#if MI_BARRIER_SUPPORTED
    ULONG BarrierStamp;
#endif
#if defined(MI_MULTINODE)
    PKNODE Node;
    ULONG NodeColor;
    ULONG OriginalColor;
#endif

    MM_PFN_LOCK_ASSERT();
    ASSERT(MmAvailablePages != 0);

    FreePagesByColor = MmFreePagesByColor[ZeroedPageList];

#if defined(MI_MULTINODE)

    //
    // Initializing Node is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    Node = NULL;

    NodeColor = Color & ~MmSecondaryColorMask;
    OriginalColor = Color;

    if (KeNumberNodes > 1) {
        Node = MI_NODE_FROM_COLOR(Color);
    }

    do {

#endif

        //
        // Attempt to remove a page from the zeroed page list. If a page
        // is available, then remove it and return its page frame index.
        // Otherwise, attempt to remove a page from the free page list or
        // the standby list.
        //
        // N.B. It is not necessary to change page colors even if the old
        //      color is not equal to the new color. The zero page thread
        //      ensures that all zeroed pages are removed from all caches.
        //

        ASSERT (Color < MmSecondaryColors);
        Page = FreePagesByColor[Color].Flink;

        if (Page != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the zeroed by color list.
            //

#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT ((Pfn1->u3.e1.PageLocation == ZeroedPageList) ||
                    ((Pfn1->u3.e1.PageLocation == FreePageList) &&
                     (FreePagesByColor == MmFreePagesByColor[FreePageList])));

            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));

#if defined(MI_MULTINODE)

            if (FreePagesByColor != MmFreePagesByColor[ZeroedPageList]) {
                goto ZeroPage;
            }

#endif

            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            return Page;

        }

#if defined(MI_MULTINODE)

        //
        // If this is a multinode machine and there are zero
        // pages on this node, select another color on this 
        // node in preference to random selection.
        //

        if (KeNumberNodes > 1) {
            if (Node->FreeCount[ZeroedPageList] != 0) {
                Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
                ASSERT(Color != OriginalColor);
                continue;
            }

            //
            // No previously zeroed page with the specified secondary
            // color exists.  Since this is a multinode machine, zero
            // an available local free page now instead of allocating a
            // zeroed page from another node below.
            //

            if (Node->FreeCount[FreePageList] != 0) {
                if (FreePagesByColor != MmFreePagesByColor[FreePageList]) {
                    FreePagesByColor = MmFreePagesByColor[FreePageList];
                    Color = OriginalColor;
                }
                else {
                    Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
                    ASSERT(Color != OriginalColor);
                }
                continue;
            }
        }

        break;
    } while (TRUE);

#endif

    //
    // No previously zeroed page with the specified secondary color exists.
    // Try a zeroed page of any color.
    //

    Page = MmZeroedPageListHead.Flink;
    if (Page != MM_EMPTY_LIST) {
#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        ASSERT (Pfn1->u3.e1.PageLocation == ZeroedPageList);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        return Page;
    }

    //
    // No zeroed page of the primary color exists, try a free page of the
    // secondary color.  Note in the multinode case this has already been done
    // above.
    //

#if defined(MI_MULTINODE)
    if (KeNumberNodes <= 1) {
#endif
        FreePagesByColor = MmFreePagesByColor[FreePageList];
    
        Page = FreePagesByColor[Color].Flink;
        if (Page != MM_EMPTY_LIST) {
    
            //
            // Remove the first entry on the free list by color.
            //
    
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
    
            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            goto ZeroPage;
        }
#if defined(MI_MULTINODE)
    }
#endif

    Page = MmFreePageListHead.Flink;
    if (Page != MM_EMPTY_LIST) {

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));
#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        goto ZeroPage;
    }

    ASSERT (MmZeroedPageListHead.Total == 0);
    ASSERT (MmFreePageListHead.Total == 0);

    //
    // Remove a page from the standby list and restore the original
    // contents of the PTE to free the last reference to the physical page.
    //

    for (ListHead = &MmStandbyPageListByPriority[0];
         ListHead < &MmStandbyPageListByPriority[MI_PFN_PRIORITIES];
         ListHead += 1) {

        if (ListHead->Total != 0) {
            Page = MiRemovePageFromList (ListHead);
            break;
        }
    }

    ASSERT (ListHead < &MmStandbyPageListByPriority[MI_PFN_PRIORITIES]);
    ASSERT ((MI_PFN_ELEMENT(Page))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
    MmStandbyRePurposed += 1;

    //
    // Zero the page removed from the free or standby list.
    //

ZeroPage:

    Pfn1 = MI_PFN_ELEMENT(Page);

    MiZeroPhysicalPage (Page);

#if MI_BARRIER_SUPPORTED

    //
    // Note the stamping must occur after the page is zeroed.
    //

    MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
    Pfn1->u4.PteFrame = BarrierStamp;

#endif

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u2.ShareCount == 0);
    ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

    return Page;
}

PFN_NUMBER
FASTCALL
MiRemoveAnyPage (
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a page from either the free, zeroed,
    or standby lists (in that order).  If no pages exist on the zeroed
    or free list a transition page is removed from the standby list
    and the PTE (may be a prototype PTE) which refers to this page is
    changed from transition back to its original contents.

    Note pages MUST exist to satisfy this request.  The caller ensures this
    by first calling MiEnsureAvailablePageOrWait.

Arguments:

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

            The above was true when we were concerned about caches
            which are virtually indexed.   (eg MIPS).   Today we
            are more concerned that we get a good usage spread across
            the L2 caches of most machines.  These caches are physically
            indexed.   By gathering pages that would have the same
            index to the same color, then maximizing the color spread,
            we maximize the effective use of the caches.

            This has been extended for NUMA machines.   The high part
            of the color gives the node color (basically node number).
            If we cannot allocate a page of the requested color, we
            try to allocate a page on the same node before taking a
            page from a different node.

Return Value:

    The physical page number removed from the specified list.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER Page;
    PMMPFNLIST ListHead;
#if DBG
    PMMPFN Pfn1;
#endif
#if defined(MI_MULTINODE)
    PKNODE Node;
    ULONG NodeColor;
    ULONG OriginalColor;
    PFN_NUMBER LocalNodePagesAvailable;
#endif

    MM_PFN_LOCK_ASSERT();
    ASSERT(MmAvailablePages != 0);

#if defined(MI_MULTINODE)

    //
    // Bias color to memory node.  The assumption is that if memory
    // of the correct color is not available on this node, it is
    // better to choose memory of a different color if you can stay
    // on this node.
    //

    LocalNodePagesAvailable = 0;
    NodeColor = Color & ~MmSecondaryColorMask;
    OriginalColor = Color;

    if (KeNumberNodes > 1) {
        Node = MI_NODE_FROM_COLOR(Color);
        LocalNodePagesAvailable = (Node->FreeCount[ZeroedPageList] | Node->FreeCount[FreePageList]);
    }

    do {

#endif

        //
        // Check the free page list, and if a page is available
        // remove it and return its value.
        //

        ASSERT (Color < MmSecondaryColors);
        if (MmFreePagesByColor[FreePageList][Color].Flink != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the free by color list.
            //

            Page = MmFreePagesByColor[FreePageList][Color].Flink;
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                
            return Page;
        }

        //
        // Try the zero page list by primary color.
        //

        if (MmFreePagesByColor[ZeroedPageList][Color].Flink
                                                        != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the zeroed by color list.
            //

            Page = MmFreePagesByColor[ZeroedPageList][Color].Flink;
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.PageLocation == ZeroedPageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                
            return Page;
        }

        //
        // If this is a multinode machine and there are free
        // pages on this node, select another color on this 
        // node in preference to random selection.
        //

#if defined(MI_MULTINODE)

        if (LocalNodePagesAvailable != 0) {
            Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
            ASSERT(Color != OriginalColor);
            continue;
        }

        break;
    } while (TRUE);

#endif

    //
    // Check the free page list, and if a page is available
    // remove it and return its value.
    //

    if (MmFreePageListHead.Flink != MM_EMPTY_LIST) {
        Page = MmFreePageListHead.Flink;
        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        return Page;
    }
    ASSERT (MmFreePageListHead.Total == 0);

    //
    // Check the zeroed page list, and if a page is available
    // remove it and return its value.
    //

    if (MmZeroedPageListHead.Flink != MM_EMPTY_LIST) {
        Page = MmZeroedPageListHead.Flink;
        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        return Page;
    }
    ASSERT (MmZeroedPageListHead.Total == 0);

    //
    // No pages exist on the free or zeroed list, use the standby list.
    //

    SATISFY_OVERZEALOUS_COMPILER (Page = MM_EMPTY_LIST);

    for (ListHead = &MmStandbyPageListByPriority[0];
         ListHead < &MmStandbyPageListByPriority[MI_PFN_PRIORITIES];
         ListHead += 1) {

        if (ListHead->Total != 0) {
            Page = MiRemovePageFromList (ListHead);
            break;
        }
    }

    ASSERT (ListHead < &MmStandbyPageListByPriority[MI_PFN_PRIORITIES]);
    ASSERT ((MI_PFN_ELEMENT(Page))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
    MmStandbyRePurposed += 1;

    MI_CHECK_PAGE_ALIGNMENT(Page, Color & MM_COLOR_MASK);
#if DBG
    Pfn1 = MI_PFN_ELEMENT (Page);
#endif
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u2.ShareCount == 0);

    return Page;
}


PFN_NUMBER
FASTCALL
MiRemovePageByColor (
    IN PFN_NUMBER Page,
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a page from the front of the free or
    zeroed page list.

Arguments:

    Page - Supplies the physical page number to unlink from the list.

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

Return Value:

    The page frame number that was unlinked (always equal to the one
    passed in, but returned so the caller's fastcall sequences save
    extra register pushes and pops.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PMMPFNLIST ListHead;
    PMMPFNLIST PrimaryListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG NodeColor;
    MMLISTS ListName;
    PMMCOLOR_TABLES ColorHead;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (Page);
    NodeColor = Pfn1->u3.e1.PageColor;
    CacheAttribute = Pfn1->u3.e1.CacheAttribute;

#if defined(MI_MULTINODE)

    ASSERT (NodeColor == (Color >> MmSecondaryColorNodeShift));

#endif

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_REMOVEPAGEBYCOLOR);
    }

    ListHead = MmPageLocationList[Pfn1->u3.e1.PageLocation];
    ListName = ListHead->ListName;

    ListHead->Total -= 1;

    PrimaryListHead = ListHead;

    Next = Pfn1->u1.Flink;
    Previous = Pfn1->u2.Blink;

    if (Next == MM_EMPTY_LIST) {
        PrimaryListHead->Blink = Previous;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }

    if (Previous == MM_EMPTY_LIST) {
        PrimaryListHead->Flink = Next;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }

    ASSERT (Pfn1->u3.e1.RemovalRequested == 0);

    //
    // Zero the flags longword, but keep the color and attribute information.
    //

    ASSERT (Pfn1->u3.e1.Rom == 0);
    Pfn1->u3.e2.ShortFlags = 0;
    Pfn1->u3.e1.PageColor = (USHORT) NodeColor;
    Pfn1->u3.e1.CacheAttribute = CacheAttribute;

    Pfn1->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Pfn1->u2.Blink = 0;

    //
    // Update the color lists.
    //

    ASSERT (Color < MmSecondaryColors);

    ColorHead = &MmFreePagesByColor[ListName][Color];
    ASSERT (ColorHead->Count >= 1);
    ColorHead->Flink = (PFN_NUMBER) Pfn1->OriginalPte.u.Long;
    if (ColorHead->Flink != MM_EMPTY_LIST) {
        MI_PFN_ELEMENT (ColorHead->Flink)->u4.PteFrame = MM_EMPTY_LIST;
    }
    else {
        ColorHead->Blink = (PVOID) MM_EMPTY_LIST;
    }

    ColorHead->Count -= 1;

    //
    // Note that we now have one less page available.
    //

#if defined(MI_MULTINODE)
    if (KeNumberNodes > 1) {
        KeNodeBlock[NodeColor]->FreeCount[ListName]--;
    }
#endif

    //
    // Signal if allocating this page caused a threshold cross.
    //

    if (MmAvailablePages == MmHighMemoryThreshold) {
        KeClearEvent (MiHighMemoryEvent);
    }
    else if (MmAvailablePages == MmLowMemoryThreshold) {
        KeSetEvent (MiLowMemoryEvent, 0, FALSE);
    }

    MmAvailablePages -= 1;

    if (MmAvailablePages < MmMinimumFreePages) {

        //
        // Obtain free pages.
        //

        MiObtainFreePages ();
    }

    return Page;
}


VOID
FASTCALL
MiInsertFrontModifiedNoWrite (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the FRONT of the modified no write list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock.

--*/

{
    PFN_NUMBER first;
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) && (PageFrameIndex <= MmHighestPhysicalPage) &&
        (PageFrameIndex >= MmLowestPhysicalPage));

    //
    // Check to ensure the reference count for the page is zero.
    //

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_PFN(Pfn1, StandbyPageList, 0x4);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 0xA);

    ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u4.MustBeCached == 0);

    MmModifiedNoWritePageListHead.Total += 1;  // One more page on the list.

    MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);

    first = MmModifiedNoWritePageListHead.Flink;
    if (first == MM_EMPTY_LIST) {

        //
        // List is empty add the page to the ListHead.
        //

        MmModifiedNoWritePageListHead.Blink = PageFrameIndex;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT (first);
        Pfn2->u2.Blink = PageFrameIndex;
    }

    MmModifiedNoWritePageListHead.Flink = PageFrameIndex;
    Pfn1->u1.Flink = first;
    Pfn1->u2.Blink = MM_EMPTY_LIST;
    Pfn1->u3.e1.PageLocation = ModifiedNoWritePageList;
    return;
}

PFN_NUMBER
MiAllocatePfn (
    IN PMMPTE PointerPte,
    IN ULONG Protection
    )

/*++

Routine Description:

    This procedure allocates and initializes a page of memory.

Arguments:

    PointerPte - Supplies the PTE to initialize.

Return Value:

    The page frame index allocated.

Environment:

    Kernel mode.

--*/
{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    MMPTE DemandZeroPte;

    DemandZeroPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

    LOCK_PFN (OldIrql);

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    MI_WRITE_INVALID_PTE (PointerPte, DemandZeroPte);

    PointerPte->u.Soft.Protection |= Protection;

    MiInitializePfn (PageFrameIndex, PointerPte, 1);

    UNLOCK_PFN (OldIrql);

    return PageFrameIndex;
}

VOID
FASTCALL
MiLogPfnInformation (
    IN PMMPFN Pfn1,
    IN USHORT Reason
    )
{
    MMPFN_IDENTITY PfnIdentity;

    RtlZeroMemory (&PfnIdentity, sizeof(PfnIdentity));

    MiIdentifyPfn (Pfn1, &PfnIdentity);

    PerfInfoLogBytes (Reason, 
                      &PfnIdentity, 
                      sizeof(PfnIdentity));

    return;
}

VOID
MiPurgeTransitionList (
    VOID
    )
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PMMPFNLIST ListHead;

    //
    // Run the transition list and free all the entries so transition
    // faults are not satisfied for any of the non modified pages that were
    // freed.
    //

    for (ListHead = &MmStandbyPageListByPriority[0];
         ListHead < &MmStandbyPageListByPriority[MI_PFN_PRIORITIES];
         ListHead += 1) {

        if (ListHead->Total == 0) {
            continue;
        }

        LOCK_PFN (OldIrql);
    
        while (ListHead->Total != 0) {
    
            PageFrameIndex = MiRemovePageFromList (ListHead);
    
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    
            InterlockedIncrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);
            Pfn1->OriginalPte.u.Long = 0;
    
            MI_SET_PFN_DELETED (Pfn1);
    
            MiDecrementReferenceCount (Pfn1, PageFrameIndex);
        }
    
        UNLOCK_PFN (OldIrql);
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\procsup.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   procsup.c

Abstract:

    This module contains routines which support the process structure.

--*/


#include "mi.h"

#if (_MI_PAGING_LEVELS >= 3)

#include "wow64t.h"

#define MI_LARGE_STACK_SIZE     KERNEL_LARGE_STACK_SIZE

#if defined(_AMD64_)

#define MM_PROCESS_CREATE_CHARGE 8

#endif

#else

//
// Registry settable but must always be a page multiple and less than
// or equal to KERNEL_LARGE_STACK_SIZE.
//

ULONG MmLargeStackSize = KERNEL_LARGE_STACK_SIZE;

#define MI_LARGE_STACK_SIZE     MmLargeStackSize

#if !defined (_X86PAE_)
#define MM_PROCESS_CREATE_CHARGE 6
#else
#define MM_PROCESS_CREATE_CHARGE 9
#endif

#endif

#if defined MAXIMUM_EXPANSION_SIZE

//
// Ke support is not enabled for all platforms, so key Mm support off the Ke
// define (MAXIMUM_EXPANSION_SIZE) for the platforms that are supported.
//

#define _MI_CHAINED_STACKS     1
#endif

#if defined _MI_CHAINED_STACKS
#define MI_STACK_IS_TRIMMABLE(_THREAD) (KeIsKernelStackTrimable (_THREAD))
#else
#define MI_STACK_IS_TRIMMABLE(_THREAD) (_THREAD->LargeStack)
#endif

#define DONTASSERT(x)

extern ULONG MmAllocationPreference;

extern MM_SYSTEMSIZE MmSystemSize;

extern PVOID BBTBuffer;

SIZE_T MmProcessCommit;

LONG MmKernelStackPages;
PFN_NUMBER MmKernelStackResident;
LONG MmLargeStacks;
LONG MmSmallStacks;

MMPTE KernelDemandZeroPte = {MM_KERNEL_DEMAND_ZERO_PTE};

CCHAR MmRotatingUniprocessorNumber;

PFN_NUMBER MmLeakedLockedPages;

extern LOGICAL MiSafeBooted;

//
// Enforced minimal commit for user mode stacks
//

ULONG MmMinimumStackCommitInBytes;

PFN_NUMBER
MiMakeOutswappedPageResident (
    IN PMMPTE ActualPteAddress,
    IN PMMPTE PointerTempPte,
    IN ULONG Global,
    IN PFN_NUMBER ContainingPage,
    IN KIRQL OldIrql
    );

NTSTATUS
MiCreatePebOrTeb (
    IN PEPROCESS TargetProcess,
    IN ULONG Size,
    OUT PVOID *Base
    );

VOID
MiDeleteAddressesInWorkingSet (
    IN PEPROCESS Process
    );

typedef struct _MMPTE_DELETE_LIST {
    ULONG Count;
    PMMPTE PointerPte[MM_MAXIMUM_FLUSH_COUNT];
    MMPTE PteContents[MM_MAXIMUM_FLUSH_COUNT];
} MMPTE_DELETE_LIST, *PMMPTE_DELETE_LIST;

VOID
MiDeletePteList (
    IN PMMPTE_DELETE_LIST PteDeleteList,
    IN PEPROCESS CurrentProcess
    );

VOID
VadTreeWalk (
    VOID
    );

PMMVAD
MiAllocateVad (
    IN ULONG_PTR StartingVirtualAddress,
    IN ULONG_PTR EndingVirtualAddress,
    IN LOGICAL Deletable
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmCreateTeb)
#pragma alloc_text(PAGE,MmCreatePeb)
#pragma alloc_text(PAGE,MiAllocateVad)
#pragma alloc_text(PAGE,MmSetMemoryPriorityProcess)
#pragma alloc_text(PAGE,MmInitializeHandBuiltProcess)
#pragma alloc_text(PAGE,MmInitializeHandBuiltProcess2)
#pragma alloc_text(PAGE,MmGetDirectoryFrameFromProcess)
#endif

#if !defined(_WIN64)
LIST_ENTRY MmProcessList;

VOID
MiUpdateSystemPdes (
    IN PEPROCESS Process
    );
#endif

#if defined(_WIN64)

BOOLEAN
MmCreateProcessAddressSpace (
    IN ULONG MinimumWorkingSetSize,
    IN PEPROCESS NewProcess,
    OUT PULONG_PTR DirectoryTableBase
    )

/*++

Routine Description:

    This routine creates an address space which maps the system
    portion and contains a hyper space entry.

Arguments:

    MinimumWorkingSetSize - Supplies the minimum working set size for
                            this address space.  This value is only used
                            to ensure that ample physical pages exist
                            to create this process.

    NewProcess - Supplies a pointer to the process object being created.

    DirectoryTableBase - Returns the value of the newly created
                         address space's Page Directory (PD) page and
                         hyper space page.

Return Value:

    Returns TRUE if an address space was successfully created, FALSE
    if ample physical pages do not exist.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PFN_NUMBER PageDirectoryIndex;
    PFN_NUMBER HyperSpaceIndex;
    PFN_NUMBER PageContainingWorkingSet;
    PFN_NUMBER VadBitMapPage;
    MMPTE TempPte;
#if defined (_AMD64_)
    MMPTE TempPte2;
#endif
    PEPROCESS CurrentProcess;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    ULONG Color;
    PMMPTE PointerPte;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PFN_NUMBER HyperDirectoryIndex;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PFN_NUMBER PageDirectoryParentIndex;
#endif
    PMMPTE MappingPte;
    PMMPTE PointerFillPte;
    PMMPTE CurrentAddressSpacePde;

    CurrentProcess = PsGetCurrentProcess ();

    //
    // Charge commitment for the page directory pages, working set page table
    // page, and working set list.  If Vad bitmap lookups are enabled, then
    // charge for a page or two for that as well.
    //

    if (MiChargeCommitment (MM_PROCESS_COMMIT_CHARGE, NULL) == FALSE) {
        return FALSE;
    }

    NewProcess->NextPageColor = (USHORT) (RtlRandom (&MmProcessColorSeed));
    KeInitializeSpinLock (&NewProcess->HyperSpaceLock);

    //
    // Get the PFN lock to get physical pages.
    //

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if (MI_NONPAGEABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)MinimumWorkingSetSize){

        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (MM_PROCESS_COMMIT_CHARGE);

        //
        // Indicate no directory base was allocated.
        //

        return FALSE;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_PROCESS_CREATE, MM_PROCESS_COMMIT_CHARGE);

    MI_DECREMENT_RESIDENT_AVAILABLE (MinimumWorkingSetSize,
                                     MM_RESAVAIL_ALLOCATE_CREATE_PROCESS);

    ASSERT (NewProcess->AddressSpaceInitialized == 0);
    PS_SET_BITS (&NewProcess->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
    ASSERT (NewProcess->AddressSpaceInitialized == 1);

    NewProcess->Vm.MinimumWorkingSetSize = MinimumWorkingSetSize;

    //
    // Allocate a page directory (parent for 64-bit systems) page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color =  MI_PAGE_COLOR_PTE_PROCESS (PDE_BASE,
                                        &CurrentProcess->NextPageColor);

    PageDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    INITIALIZE_DIRECTORY_TABLE_BASE(&DirectoryTableBase[0], PageDirectoryIndex);

    TempPte = ValidPdePde;

    PointerPpe = KSEG_ADDRESS (PageDirectoryIndex);

    //
    // Map the top level page directory parent page recursively onto itself.
    //

    TempPte.u.Hard.PageFrameNumber = PageDirectoryIndex;

    //
    // Set the PTE address in the PFN for the top level page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryIndex);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageDirectoryIndex,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

#if (_MI_PAGING_LEVELS >= 4)

    PageDirectoryParentIndex = PageDirectoryIndex;

    PointerPxe = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                             PageDirectoryIndex);

    Pfn1->PteAddress = MiGetPteAddress(PXE_BASE);

    PointerPxe[MiGetPxeOffset(PXE_BASE)] = TempPte;

    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPxe);

    //
    // Now that the top level extended page parent page is initialized,
    // allocate a page parent page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color =  MI_PAGE_COLOR_PTE_PROCESS (PDE_BASE,
                                        &CurrentProcess->NextPageColor);

    PageDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryIndex);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageDirectoryIndex,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

    //
    //
    // Map this directory parent page into the top level
    // extended page directory parent page.
    //

    TempPte.u.Hard.PageFrameNumber = PageDirectoryIndex;

    PointerPxe = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                             PageDirectoryParentIndex);

    PointerPxe[MiGetPxeOffset(HYPER_SPACE)] = TempPte;

    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPxe);

#else
    Pfn1->PteAddress = MiGetPteAddress((PVOID)PDE_TBASE);
    PointerPpe[MiGetPpeOffset(PDE_TBASE)] = TempPte;
#endif

    //
    // Allocate the page directory for hyper space and map this directory
    // page into the page directory parent page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color = MI_PAGE_COLOR_PTE_PROCESS (MiGetPpeAddress(HYPER_SPACE),
                                       &CurrentProcess->NextPageColor);

    HyperDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    Pfn1 = MI_PFN_ELEMENT (HyperDirectoryIndex);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (HyperDirectoryIndex,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

    TempPte.u.Hard.PageFrameNumber = HyperDirectoryIndex;

#if (_MI_PAGING_LEVELS >= 4)
    PointerPpe = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                             PageDirectoryIndex);
#endif

    PointerPpe[MiGetPpeOffset(HYPER_SPACE)] = TempPte;

#if (_MI_PAGING_LEVELS >= 4)
    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPpe);
#endif

    //
    // Allocate the hyper space page table page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color = MI_PAGE_COLOR_PTE_PROCESS (MiGetPdeAddress(HYPER_SPACE),
                                       &CurrentProcess->NextPageColor);

    HyperSpaceIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    Pfn1 = MI_PFN_ELEMENT (HyperSpaceIndex);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (HyperSpaceIndex,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

#if (_AMD64_)
    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex;
    PointerPde = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                             HyperDirectoryIndex);

    PointerPde[MiGetPdeOffset(HYPER_SPACE)] = TempPte;
    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPde);
#endif

    INITIALIZE_DIRECTORY_TABLE_BASE(&DirectoryTableBase[1], HyperSpaceIndex);

    //
    // Remove page(s) for the VAD bitmap.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color = MI_PAGE_COLOR_VA_PROCESS (MmWorkingSetList,
                                      &CurrentProcess->NextPageColor);

    VadBitMapPage = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (VadBitMapPage,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

    //
    // Remove page for the working set list.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    Color = MI_PAGE_COLOR_VA_PROCESS (MmWorkingSetList,
                                      &CurrentProcess->NextPageColor);

    PageContainingWorkingSet = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    Pfn1 = MI_PFN_ELEMENT (PageContainingWorkingSet);

    if (Pfn1->u3.e1.CacheAttribute != MiCached) {
        MI_FLUSH_TB_FOR_INDIVIDUAL_ATTRIBUTE_CHANGE (PageContainingWorkingSet,
                                                     MiCached);
        Pfn1->u3.e1.CacheAttribute = MiCached;
    }

    UNLOCK_PFN (OldIrql);

    NewProcess->WorkingSetPage = PageContainingWorkingSet;

    //
    // Initialize the page reserved for hyper space.
    //

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = VadBitMapPage;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    //
    // Set the PTE address in the PFN for the hyper space page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (HyperDirectoryIndex);

    Pfn1->PteAddress = MiGetPpeAddress (HYPER_SPACE);

#if defined (_AMD64_)

    //
    // Copy the system mappings including the shared user page & session space.
    //

    CurrentAddressSpacePde = MiGetPxeAddress (KI_USER_SHARED_DATA);

    MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MappingPte != NULL) {

        MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                  PageDirectoryParentIndex,
                                  MM_READWRITE,
                                  MappingPte);

        MI_SET_PTE_DIRTY (TempPte2);

        MI_WRITE_VALID_PTE (MappingPte, TempPte2);

        PointerPte = MiGetVirtualAddressMappedByPte (MappingPte);
    }
    else {
        PointerPte = MiMapPageInHyperSpace (CurrentProcess,
                                            PageDirectoryParentIndex,
                                            &OldIrql);
    }

    PointerFillPte = &PointerPte[MiGetPxeOffset(KI_USER_SHARED_DATA)];

    RtlCopyMemory (PointerFillPte,
                   CurrentAddressSpacePde,
                   ((1 + (MiGetPxeAddress(MM_SYSTEM_SPACE_END) -
                      CurrentAddressSpacePde)) * sizeof(MMPTE)));
    
    if (MappingPte != NULL) {
        MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
    }
    else {
        MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);
    }

    //
    // Initialize the VAD bitmap and working set mappings.
    //

    MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MappingPte != NULL) {

        MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                  HyperSpaceIndex,
                                  MM_READWRITE,
                                  MappingPte);

        MI_SET_PTE_DIRTY (TempPte2);

        MI_WRITE_VALID_PTE (MappingPte, TempPte2);

        PointerPte = MiGetVirtualAddressMappedByPte (MappingPte);
    }
    else {
        PointerPte = MiMapPageInHyperSpace (CurrentProcess,
                                            HyperSpaceIndex,
                                            &OldIrql);
   
    }

    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;

    if (MappingPte != NULL) {
        MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
    }
    else {
        MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);
    }
#else

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = VadBitMapPage;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    PointerPte = KSEG_ADDRESS (HyperSpaceIndex);
    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;
#endif

    InterlockedExchangeAddSizeT (&MmProcessCommit, MM_PROCESS_COMMIT_CHARGE);

    //
    // Up the session space reference count.
    //

    MiSessionAddProcess (NewProcess);

    return TRUE;
}
#endif


NTSTATUS
MmInitializeProcessAddressSpace (
    IN PEPROCESS ProcessToInitialize,
    IN PEPROCESS ProcessToClone OPTIONAL,
    IN PVOID SectionToMap OPTIONAL,
    IN OUT PULONG CreateFlags,
    OUT POBJECT_NAME_INFORMATION *AuditName OPTIONAL
    )

/*++

Routine Description:

    This routine initializes the working set and mutexes within a
    newly created address space to support paging.

    No page faults may occur in a new process until this routine has
    completed.

Arguments:

    ProcessToInitialize - Supplies a pointer to the process to initialize.

    ProcessToClone - Optionally supplies a pointer to the process whose
                     address space should be copied into the
                     ProcessToInitialize address space.

    SectionToMap - Optionally supplies a section to map into the newly
                   initialized address space.

    Only one of ProcessToClone and SectionToMap may be specified.

    CreateFlags - Flags specifying varying parameters pertinent to process
        creation. Currently, only the PROCESS_CREATE_FLAGS_ALL_LARGE_PAGE_FLAGS
        are relevant to memory management. On output, these indicate to the
        caller whether or not the process image was successfully mapped with
        large pages.
        
    AuditName - Supplies an opaque object name information pointer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  APCs disabled.

--*/

{
    KIRQL OldIrql;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PVOID BaseAddress;
    SIZE_T ViewSize;
    NTSTATUS Status;
    NTSTATUS SystemDllStatus;
    PFILE_OBJECT FilePointer;
    PFN_NUMBER PageContainingWorkingSet;
    LARGE_INTEGER SectionOffset;
    PSECTION_IMAGE_INFORMATION ImageInfo;
    PMMVAD VadShare;
    PMMVAD VadReserve;
    PETHREAD CurrentThread;
    PLOCK_HEADER LockedPagesHeader;
    PFN_NUMBER PdePhysicalPage;
    PFN_NUMBER VadBitMapPage;
    ULONG i;
    ULONG NumberOfPages;
    MMPTE DemandZeroPte;
    ULONG AllocationType;
    ULONG WowLACompatRes = 0;
    
#if defined (_X86PAE_)
    PFN_NUMBER PdePhysicalPage2;
#endif

#if (_MI_PAGING_LEVELS >= 3)
    ULONG MaximumUserPageTablePages = MM_USER_PAGE_TABLE_PAGES;
    ULONG MaximumUserPageDirectoryPages = MM_USER_PAGE_DIRECTORY_PAGES;
    PFN_NUMBER PpePhysicalPage;
#if DBG
    ULONG j;
    PUCHAR p;
#endif
#endif

#if (_MI_PAGING_LEVELS >= 4)
    PFN_NUMBER PxePhysicalPage;
#endif

#if defined(_WIN64)
    MMPTE TempPte2;
    PMMPTE MappingPte;
    PMMWSL WorkingSetList;
    PVOID HighestUserAddress;
    PWOW64_PROCESS Wow64Process;
#endif

    CurrentThread = PsGetCurrentThread ();
    DemandZeroPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

#if !defined(_WIN64)

    //
    // Check whether our new process needs an update that occurred while we
    // were filling its system PDEs.  If so, we must recopy here as the updater
    // isn't able to tell where we were in the middle of our first copy.
    //

    ASSERT (ProcessToInitialize->Pcb.DirectoryTableBase[0] != 0);

    LOCK_EXPANSION (OldIrql);

    if (ProcessToInitialize->PdeUpdateNeeded) {

        //
        // Another thread updated the system PDE range while this process
        // was being created.  Update the PDEs now (prior to attaching so
        // if an interrupt occurs that accesses the mapping it will be correct).
        //

        PS_CLEAR_BITS (&ProcessToInitialize->Flags,
                       PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);

        MiUpdateSystemPdes (ProcessToInitialize);
    }

    UNLOCK_EXPANSION (OldIrql);

#endif

    VadReserve = NULL;

    //
    // Initialize Working Set Mutex in process header.
    //

    KeAttachProcess (&ProcessToInitialize->Pcb);

    ASSERT (ProcessToInitialize->AddressSpaceInitialized <= 1);
    PS_CLEAR_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
    ASSERT (ProcessToInitialize->AddressSpaceInitialized == 0);

    PS_SET_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE2);
    ASSERT (ProcessToInitialize->AddressSpaceInitialized == 2);


    KeInitializeGuardedMutex (&ProcessToInitialize->AddressCreationLock);
    ExInitializePushLock (&ProcessToInitialize->Vm.WorkingSetMutex);

    //
    // NOTE:  The process block has been zeroed when allocated, so
    // there is no need to zero fields and set pointers to NULL.
    //

    ASSERT (ProcessToInitialize->VadRoot.NumberGenericTableElements == 0);

    ProcessToInitialize->VadRoot.BalancedRoot.u1.Parent = &ProcessToInitialize->VadRoot.BalancedRoot;

    KeQuerySystemTime (&ProcessToInitialize->Vm.LastTrimTime);
    ProcessToInitialize->Vm.VmWorkingSetList = MmWorkingSetList;

    //
    // Obtain a page to map the working set and initialize the
    // working set.  Get the PFN lock to allocate physical pages.
    //

    LOCK_PFN (OldIrql);

    //
    // Initialize the PFN database for the Page Directory and the
    // PDE which maps hyper space.
    //

#if (_MI_PAGING_LEVELS >= 3)

#if (_MI_PAGING_LEVELS >= 4)
    PointerPte = MiGetPteAddress (PXE_BASE);
    PxePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    MiInitializePfn (PxePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPxeAddress (HYPER_SPACE);
#else
    PointerPte = MiGetPteAddress ((PVOID)PDE_TBASE);
#endif

    PpePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    MiInitializePfn (PpePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPpeAddress (HYPER_SPACE);

#elif defined (_X86PAE_)
    PointerPte = MiGetPdeAddress (PDE_BASE);
#else
    PointerPte = MiGetPteAddress (PDE_BASE);
#endif

    PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    MiInitializePfn (PdePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPdeAddress (HYPER_SPACE);
    MiInitializePfn (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte), PointerPte, 1);

#if defined (_X86PAE_)

    for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
        PointerPte = MiGetPteAddress (PDE_BASE + (i << PAGE_SHIFT));
        PdePhysicalPage2 = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        MiInitializePfn (PdePhysicalPage2, PointerPte, 1);
    }

#endif

    //
    // The VAD bitmap spans one page when booted 2GB and the working set
    // page follows it.  If booted 3GB, the VAD bitmap spans 1.5 pages and
    // the working set list uses the last half of the second page.
    //

    NumberOfPages = 2;

    PointerPte = MiGetPteAddress (VAD_BITMAP_SPACE);

    MI_MAKE_VALID_PTE (TempPte,
                       0,
                       MM_READWRITE,
                       PointerPte);

    MI_SET_PTE_DIRTY (TempPte);

    for (i = 0; i < NumberOfPages; i += 1) {

        ASSERT (PointerPte->u.Long != 0);
        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        MI_WRITE_INVALID_PTE (PointerPte, DemandZeroPte);

        MiInitializePfn (VadBitMapPage, PointerPte, 1);

        TempPte.u.Hard.PageFrameNumber = VadBitMapPage;

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);

    PageContainingWorkingSet = ProcessToInitialize->WorkingSetPage;

    ASSERT (ProcessToInitialize->LockedPagesList == NULL);

    if (MmTrackLockedPages == TRUE) {
        LockedPagesHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                   sizeof(LOCK_HEADER),
                                                   'xTmM');

        if (LockedPagesHeader != NULL) {

            LockedPagesHeader->Count = 0;
            LockedPagesHeader->Valid = TRUE;
            InitializeListHead (&LockedPagesHeader->ListHead);
            KeInitializeSpinLock (&LockedPagesHeader->Lock);
            
            //
            // Note an explicit memory barrier is not needed here because
            // we must detach from this process before the field can be
            // accessed.  And any other processor would need to context
            // swap to this process before the field could be accessed, so
            // the implicit memory barriers in context swap are sufficient.
            //

            ProcessToInitialize->LockedPagesList = (PVOID) LockedPagesHeader;
        }
    }

    MiInitializeWorkingSetList (ProcessToInitialize);

    ASSERT (ProcessToInitialize->PhysicalVadRoot == NULL);

    BaseAddress = NULL;

    //
    // Page faults may be taken now.
    //
    // If the system has been biased to an alternate base address to allow
    // 3gb of user address space and the process is not being cloned, then
    // create a VAD for the shared memory page.
    //
    // Always create a VAD for the shared memory page for 64-bit systems as
    // clearly it always falls into the user address space there.
    //
    // Only x86 booted without /3GB doesn't need the VAD (because the shared
    // memory page lies above the highest VAD the user can allocate so the user
    // can never delete it).
    //

    if ((MM_HIGHEST_VAD_ADDRESS > (PVOID) MM_SHARED_USER_DATA_VA) &&
        (ProcessToClone == NULL)) {

        //
        // Allocate a VAD to map the shared memory page. If a VAD cannot be
        // allocated, then detach from the target process and return a failure
        // status.  This VAD is marked as not deletable.
        //

        VadShare = MiAllocateVad (MM_SHARED_USER_DATA_VA,
                                  MM_SHARED_USER_DATA_VA+ (X64K)-1,
                                  FALSE);

        if (VadShare == NULL) {
            KeDetachProcess ();
            return STATUS_NO_MEMORY;
        }

        //
        // If a section is being mapped and the executable is not large
        // address space aware, then create a VAD that reserves the address
        // space between 2gb and the highest user address.
        //

        if (SectionToMap != NULL) {

            if (!((PSECTION)SectionToMap)->u.Flags.Image) {
                KeDetachProcess ();
                if (VadShare != NULL) {
                    ExFreePool (VadShare);
                }
                return STATUS_SECTION_NOT_IMAGE;
            }

            ImageInfo = ((PSECTION)SectionToMap)->Segment->u2.ImageInformation;

#if defined(_X86_)

            if ((ImageInfo->ImageCharacteristics & IMAGE_FILE_LARGE_ADDRESS_AWARE) == 0) {
                BaseAddress = (PVOID) _2gb;
            }

#else

            if (ProcessToInitialize->Flags & PS_PROCESS_FLAGS_OVERRIDE_ADDRESS_SPACE) {
                NOTHING;
            }
            else {
                if ((ImageInfo->ImageCharacteristics & IMAGE_FILE_LARGE_ADDRESS_AWARE) == 0) {
                    BaseAddress = (PVOID) (ULONG_PTR) _2gb;
                }
                else if (ImageInfo->Machine == IMAGE_FILE_MACHINE_I386) {

                    //
                    // Provide 4 gigabytes of address space for wow64 apps that
                    // have the large address aware bit set.
                    //

                    BaseAddress = (PVOID) (ULONG_PTR) _4gb;
                    WowLACompatRes = X64K;

                    PS_SET_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_WOW64_4GB_VA_SPACE);
                }

                //
                // Create a guard 64K area for Wow64 compatibility
                //

                if (ImageInfo->Machine == IMAGE_FILE_MACHINE_I386) {
                    BaseAddress = (PVOID) ((ULONG_PTR)BaseAddress - X64K);
                }
            }

#endif

            if (BaseAddress != NULL) {
            
                //
                // Allocate a VAD to map the address space between 2gb and
                // the highest user address. If a VAD cannot be allocated,
                // then deallocate the shared address space VAD, detach from
                // the target process, and return a failure status.
                // This VAD is marked as not deletable.
                //

                VadReserve = MiAllocateVad ((ULONG_PTR) BaseAddress - WowLACompatRes,
                                            (ULONG_PTR) MM_HIGHEST_VAD_ADDRESS,
                                            FALSE);

                if (VadReserve == NULL) {
                    KeDetachProcess ();
                    if (VadShare != NULL) {
                        ExFreePool (VadShare);
                    }
                    return STATUS_NO_MEMORY;
                }

                //
                // Insert the VAD.
                //
                // N.B. No failure can occur since there is no commit charge.
                //

                Status = MiInsertVadCharges (VadReserve, ProcessToInitialize);

                ASSERT (NT_SUCCESS (Status));

                LOCK_WS (CurrentThread, ProcessToInitialize);
               
                MiInsertVad (VadReserve, ProcessToInitialize);

                UNLOCK_WS (CurrentThread, ProcessToInitialize);

#if !defined(_X86_)

                if (ImageInfo->Machine == IMAGE_FILE_MACHINE_I386) {

                    //
                    // Initialize the Wow64 process structure.
                    //

                    Wow64Process = (PWOW64_PROCESS) ExAllocatePoolWithTag (
                                                        NonPagedPool,
                                                        sizeof(WOW64_PROCESS),
                                                        'WowM');

                    if (Wow64Process == NULL) {
                        KeDetachProcess ();
                        ExFreePool (VadShare);
                        return STATUS_NO_MEMORY;
                    }

                    RtlZeroMemory (Wow64Process, sizeof(WOW64_PROCESS));

                    ProcessToInitialize->Wow64Process = Wow64Process;

                    MmWorkingSetList->HighestUserAddress = BaseAddress;

                    //
                    // Since the user can only access a limited amount of
                    // virtual address space, reduce the number of commit
                    // bits needed to span his page table pages.
                    //

                    MaximumUserPageTablePages = (ULONG) (MI_ROUND_TO_SIZE ((SIZE_T)BaseAddress, MM_VA_MAPPED_BY_PDE) / MM_VA_MAPPED_BY_PDE);
                    MaximumUserPageDirectoryPages = (ULONG) (MI_ROUND_TO_SIZE ((SIZE_T)BaseAddress, MM_VA_MAPPED_BY_PPE) / MM_VA_MAPPED_BY_PPE);
                }
#endif

            }
        }

        //
        // Insert the VAD.
        //
        // N.B. No failure can occur since there is no commit charge.
        //

        if (VadShare != NULL) {

            Status = MiInsertVadCharges (VadShare, ProcessToInitialize);

            ASSERT (NT_SUCCESS (Status));

            LOCK_WS (CurrentThread, ProcessToInitialize);

            MiInsertVad (VadShare, ProcessToInitialize);

            UNLOCK_WS (CurrentThread, ProcessToInitialize);
        }
    }

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Allocate the commitment tracking bitmaps for page directory and page
    // table pages.  This must be done before any non-MM_MAX_COMMIT VAD
    // creations occur.
    //

    ASSERT (MmWorkingSetList->CommittedPageTables == NULL);
    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectories == 0);

    ASSERT ((ULONG_PTR)MM_SYSTEM_RANGE_START % (PTE_PER_PAGE * PAGE_SIZE) == 0);

    MmWorkingSetList->MaximumUserPageTablePages = MaximumUserPageTablePages;

    MmWorkingSetList->CommittedPageTables = (PULONG)
        ExAllocatePoolWithTag (MmPagedPoolEnd != NULL ? PagedPool : NonPagedPool,
                               (MaximumUserPageTablePages + 7) / 8,
                               'dPmM');

    if (MmWorkingSetList->CommittedPageTables == NULL) {
        KeDetachProcess ();
        return STATUS_NO_MEMORY;
    }

#if (_MI_PAGING_LEVELS >= 4)

#if DBG
    p = (PUCHAR) MmWorkingSetList->CommittedPageDirectoryParents;

    for (j = 0; j < ((MM_USER_PAGE_DIRECTORY_PARENT_PAGES + 7) / 8); j += 1) {
        ASSERT (*p == 0);
        p += 1;
    }
#endif

    ASSERT (MmWorkingSetList->CommittedPageDirectories == NULL);
    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectoryParents == 0);

    MmWorkingSetList->MaximumUserPageDirectoryPages = MaximumUserPageDirectoryPages;

    MmWorkingSetList->CommittedPageDirectories = (PULONG)
        ExAllocatePoolWithTag (MmPagedPoolEnd != NULL ? PagedPool : NonPagedPool,
                               (MaximumUserPageDirectoryPages + 7) / 8,
                               'dPmM');

    if (MmWorkingSetList->CommittedPageDirectories == NULL) {
        ExFreePool (MmWorkingSetList->CommittedPageTables);
        MmWorkingSetList->CommittedPageTables = NULL;
        KeDetachProcess ();
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (MmWorkingSetList->CommittedPageDirectories,
                   (MaximumUserPageDirectoryPages + 7) / 8);
#endif

    RtlZeroMemory (MmWorkingSetList->CommittedPageTables,
                   (MaximumUserPageTablePages + 7) / 8);

#if DBG
    p = (PUCHAR) MmWorkingSetList->CommittedPageDirectories;

    for (j = 0; j < ((MaximumUserPageDirectoryPages + 7) / 8); j += 1) {
        ASSERT (*p == 0);
        p += 1;
    }
#endif

#endif

    //
    // If the registry indicates all applications should get virtual address
    // ranges from the highest address downwards then enforce it now.  This
    // makes it easy to test 3GB-aware apps on 32-bit machines as well as
    // 64-bit apps on NT64.
    //
    //
    // Note this is only done if the image has the large-address-aware bit set
    // because otherwise the compatibility VAD occupies the range from 2gb->3gb
    // and setting top-down by default can cause allocations like the stack
    // trace database to displace kernel32 causing the process launch to fail.
    //

    if ((MmAllocationPreference != 0) &&
        ((VadReserve == NULL) || (BaseAddress > (PVOID)(ULONG_PTR)_2gb))) {

        PS_SET_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_VM_TOP_DOWN);
    }

#if defined(_WIN64)

    if (ProcessToClone == NULL) {

        //
        // Reserve the address space just below KUSER_SHARED_DATA as the
        // compatibility area.  This range (and pieces of it) can be
        // unreserved by user mode code such as WOW64 or csrss.  Hence
        // commit must be charged for the page directory and table pages.
        //

        ASSERT(MiCheckForConflictingVad(ProcessToInitialize, WOW64_COMPATIBILITY_AREA_ADDRESS, MM_SHARED_USER_DATA_VA - 1) == NULL);

        VadShare = MiAllocateVad (WOW64_COMPATIBILITY_AREA_ADDRESS,
                                  MM_SHARED_USER_DATA_VA - 1,
                                  TRUE);

    	if (VadShare == NULL) {
           KeDetachProcess ();
           return STATUS_NO_MEMORY;
    	}

        //
        // Zero the commit charge so inserting the VAD will result in the
        // proper charges being applied.  This way when it is split later,
        // the correct commitment will be returned.
        //
        // N.B.  The system process is not allocated with commit because
        //       paged pool and quotas don't exist at the point in Phase0
        //       where this is called.
        //

        if (MmPagedPoolEnd != NULL) {
            VadShare->u.VadFlags.CommitCharge = 0;
        }

    	//
        // Insert the VAD.  Since this VAD has a commit charge, the working set
        // mutex must be held (as calls inside MiInsertVad to support routines
        // to charge commit require it), failures can occur and must be handled.
    	//

        Status = MiInsertVadCharges (VadShare, ProcessToInitialize);

        if (!NT_SUCCESS (Status)) {

            //
            // Note that any inserted VAD (ie: the VadReserve and Wow64
            // allocations) are automatically released on process destruction
            // so there is no need to tear them down here.
            //

            ExFreePool (VadShare);
            KeDetachProcess ();
            return Status;
        }

        LOCK_WS (CurrentThread, ProcessToInitialize);

        MiInsertVad (VadShare, ProcessToInitialize);

        UNLOCK_WS (CurrentThread, ProcessToInitialize);
    }

#endif

    if (SectionToMap != NULL) {

        //
        // Map the specified section into the address space of the
        // process but only if it is an image section.
        //

        if (!((PSECTION)SectionToMap)->u.Flags.Image) {
            Status = STATUS_SECTION_NOT_IMAGE;
        }
        else {
            UNICODE_STRING UnicodeString;
            ULONG n;
            PWSTR Src;
            PCHAR Dst;
            PSECTION_IMAGE_INFORMATION ImageInformation;

            FilePointer = ((PSECTION)SectionToMap)->Segment->ControlArea->FilePointer;
            ImageInformation = ((PSECTION)SectionToMap)->Segment->u2.ImageInformation;
            UnicodeString = FilePointer->FileName;
            Src = (PWSTR)((PCHAR)UnicodeString.Buffer + UnicodeString.Length);
            n = 0;
            if (UnicodeString.Buffer != NULL) {
                while (Src > UnicodeString.Buffer) {
                    if (*--Src == OBJ_NAME_PATH_SEPARATOR) {
                        Src += 1;
                        break;
                    }
                    else {
                        n += 1;
                    }
                }
            }
            Dst = (PCHAR)ProcessToInitialize->ImageFileName;
            if (n >= sizeof (ProcessToInitialize->ImageFileName)) {
                n = sizeof (ProcessToInitialize->ImageFileName) - 1;
            }

            while (n--) {
                *Dst++ = (UCHAR)*Src++;
            }
            *Dst = '\0';

            if (AuditName != NULL) {
                Status = SeInitializeProcessAuditName (FilePointer, FALSE, AuditName);

                if (!NT_SUCCESS(Status)) {
                    KeDetachProcess ();
                    return Status;
                }
            }

            ProcessToInitialize->SubSystemMajorVersion =
                (UCHAR)ImageInformation->SubSystemMajorVersion;
            ProcessToInitialize->SubSystemMinorVersion =
                (UCHAR)ImageInformation->SubSystemMinorVersion;

            BaseAddress = NULL;
            ViewSize = 0;
            ZERO_LARGE (SectionOffset);

            AllocationType = 0;

            if ((*CreateFlags & PROCESS_CREATE_FLAGS_LARGE_PAGES) != 0) {
                AllocationType = MEM_LARGE_PAGES;
            }

            Status = MmMapViewOfSection ((PSECTION)SectionToMap,
                                         ProcessToInitialize,
                                         &BaseAddress,
                                         0,
                                         0,
                                         &SectionOffset,
                                         &ViewSize,
                                         ViewShare,
                                         AllocationType,
                                         PAGE_READWRITE);

            //
            // The status of mapping the section is what must be returned
            // unless the system DLL load fails.  This is because if the
            // exe section is relocated (ie: STATUS_IMAGE_NOT_AT_BASE), this
            // must be returned (not STATUS_SUCCESS from the system DLL
            // mapping).
            //

            ProcessToInitialize->SectionBaseAddress = BaseAddress;

            if (NT_SUCCESS (Status)) {

                if ((*CreateFlags & PROCESS_CREATE_FLAGS_LARGE_PAGES) != 0) {
                    PMMPTE PointerPde;
                    
                    //
                    // Determine whether or not the image was succesfully mapped with
                    // large pages. Note that no locks are required as no other thread
                    // can access this process, and no page in the process can be trimmed 
                    // (the process is not yet on Mm's list of working sets).
                    //
                    
                    PointerPde = MiGetPdeAddress (BaseAddress);

                    if (
#if (_MI_PAGING_LEVELS>=4)
                        (MiGetPxeAddress (BaseAddress)->u.Hard.Valid == 0) ||
#endif
#if (_MI_PAGING_LEVELS>=3)
                        (MiGetPpeAddress (BaseAddress)->u.Hard.Valid == 0) ||
#endif
                        (PointerPde->u.Hard.Valid == 0) ||
                        (MI_PDE_MAPS_LARGE_PAGE (PointerPde) == 0)) {
                        *CreateFlags &= ~PROCESS_CREATE_FLAGS_LARGE_PAGES;
                    }
                }

                //
                // Map the system DLL now since it is required by all processes.
                //

                SystemDllStatus = PsMapSystemDll (ProcessToInitialize, 
                                                  NULL,
                                                  FALSE);

                if (!NT_SUCCESS (SystemDllStatus)) {
                    Status = SystemDllStatus;
                }
            }
        }

        MiAllowWorkingSetExpansion (&ProcessToInitialize->Vm);

        KeDetachProcess ();
        return Status;
    }
 
    if (ProcessToClone != NULL) {

        strcpy ((PCHAR)ProcessToInitialize->ImageFileName,
                (PCHAR)ProcessToClone->ImageFileName);

        //
        // Clone the address space of the specified process.
        //
        // As the page directory and page tables are private to each
        // process, the physical pages which map the directory page
        // and the page table usage must be mapped into system space
        // so they can be updated while in the context of the process
        // we are cloning.
        //

#if defined(_WIN64)

        if (ProcessToClone->Wow64Process != NULL) {

            //
            // Initialize the Wow64 process structure.
            //

            Wow64Process = (PWOW64_PROCESS) ExAllocatePoolWithTag (
                                                NonPagedPool,
                                                sizeof(WOW64_PROCESS),
                                                'WowM');

            if (Wow64Process == NULL) {
                KeDetachProcess ();
                return STATUS_NO_MEMORY;
            }

            RtlZeroMemory (Wow64Process, sizeof(WOW64_PROCESS));

            ProcessToInitialize->Wow64Process = Wow64Process;

            Wow64Process->Wow64 = ProcessToClone->Wow64Process->Wow64;

            MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

            if (MappingPte != NULL) {

                MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                          ProcessToClone->WorkingSetPage,
                                          MM_READWRITE,
                                          MappingPte);

                MI_SET_PTE_DIRTY (TempPte2);

                MI_WRITE_VALID_PTE (MappingPte, TempPte2);

                WorkingSetList = MiGetVirtualAddressMappedByPte (MappingPte);
            }
            else {
                WorkingSetList = MiMapPageInHyperSpace (ProcessToInitialize,
                                                        ProcessToClone->WorkingSetPage,
                                                        &OldIrql);
            }

            HighestUserAddress = WorkingSetList->HighestUserAddress;

            if (MappingPte != NULL) {
                MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
            }
            else {
                MiUnmapPageInHyperSpace (ProcessToInitialize,
                                         WorkingSetList,
                                         OldIrql);
            }

            MmWorkingSetList->HighestUserAddress = HighestUserAddress;
        }

#endif

        KeDetachProcess ();

        Status = MiCloneProcessAddressSpace (ProcessToClone,
                                             ProcessToInitialize);

        MiAllowWorkingSetExpansion (&ProcessToInitialize->Vm);
    } else {

        //
        // System Process.
        //
    
        KeDetachProcess ();
        Status = STATUS_SUCCESS;
    }

    //
    // If this a cloned process or the system process, then large pages
    // were not used.
    //
    
    if (NT_SUCCESS (Status)) {
        *CreateFlags &= ~(PROCESS_CREATE_FLAGS_LARGE_PAGES);
    }
    
    return Status;
}

#if !defined (_WIN64)
VOID
MiInsertHandBuiltProcessIntoList (
    IN PEPROCESS ProcessToInitialize
    )

/*++

Routine Description:

    Nonpaged helper routine.

--*/

{
    KIRQL OldIrql;

    ASSERT (ProcessToInitialize->MmProcessLinks.Flink == NULL);
    ASSERT (ProcessToInitialize->MmProcessLinks.Blink == NULL);

    LOCK_EXPANSION (OldIrql);

    InsertTailList (&MmProcessList, &ProcessToInitialize->MmProcessLinks);

    UNLOCK_EXPANSION (OldIrql);
}
#endif


NTSTATUS
MmInitializeHandBuiltProcess (
    IN PEPROCESS ProcessToInitialize,
    OUT PULONG_PTR DirectoryTableBase
    )

/*++

Routine Description:

    This routine initializes the working set mutex and
    address creation mutex for this "hand built" process.
    Normally the call to MmInitializeAddressSpace initializes the
    working set mutex.  However, in this case, we have already initialized
    the address space and we are now creating a second process using
    the address space of the idle thread.

Arguments:

    ProcessToInitialize - Supplies a pointer to the process to initialize.

    DirectoryTableBase - Receives the pair of directory table base pointers.

Return Value:

    None.

Environment:

    Kernel mode.  APCs disabled, idle process context.

--*/

{
#if !defined(NT_UP)

    //
    // On MP machines the idle & system process do not share a top level
    // page directory because hyperspace mappings are protected by a
    // per-process spinlock.  Having two processes share a single hyperspace
    // (by virtue of sharing a top level page directory) would make the
    // spinlock synchronization meaningless.
    //
    // Note that it is completely illegal for the idle process to ever enter
    // a wait state, but the code below should never encounter waits for
    // mutexes, etc.
    //

    return MmCreateProcessAddressSpace (0,
                                        ProcessToInitialize,
                                        DirectoryTableBase);
#else

    PEPROCESS CurrentProcess;

    CurrentProcess = PsGetCurrentProcess();

    DirectoryTableBase[0] = CurrentProcess->Pcb.DirectoryTableBase[0];
    DirectoryTableBase[1] = CurrentProcess->Pcb.DirectoryTableBase[1];

    KeInitializeGuardedMutex (&ProcessToInitialize->AddressCreationLock);
    ExInitializePushLock (&ProcessToInitialize->Vm.WorkingSetMutex);

    KeInitializeSpinLock (&ProcessToInitialize->HyperSpaceLock);

    ASSERT (ProcessToInitialize->VadRoot.NumberGenericTableElements == 0);

    ProcessToInitialize->VadRoot.BalancedRoot.u1.Parent = &ProcessToInitialize->VadRoot.BalancedRoot;

    ProcessToInitialize->Vm.WorkingSetSize = CurrentProcess->Vm.WorkingSetSize;
    ProcessToInitialize->Vm.VmWorkingSetList = MmWorkingSetList;

    KeQuerySystemTime (&ProcessToInitialize->Vm.LastTrimTime);

#if defined (_X86PAE_)
    ProcessToInitialize->PaeTop = &MiSystemPaeVa;
#endif

#if !defined (_WIN64)
    MiInsertHandBuiltProcessIntoList (ProcessToInitialize);
#endif

    MiAllowWorkingSetExpansion (&ProcessToInitialize->Vm);

    return STATUS_SUCCESS;
#endif
}

NTSTATUS
MmInitializeHandBuiltProcess2 (
    IN PEPROCESS ProcessToInitialize
    )

/*++

Routine Description:

    This routine initializes the shared user VAD.  This only needs to be done
    for NT64 (and x86 when booted /3GB) because on all other systems, the
    shared user address is located above the highest user address.

    For NT64 and x86 /3GB, this VAD must be allocated so that other random
    VAD allocations do not overlap this area which would cause the mapping
    to receive the wrong data.

Arguments:

    ProcessToInitialize - Supplies the process that needs initialization.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    NTSTATUS Status;
    PLOCK_HEADER LockedPagesHeader;
#if !defined(NT_UP)
    ULONG DummyFlags;
#if defined(_X86_)
    MMPTE TempPte2;
    PMMPTE MappingPte;
#endif // _X86_
#endif // NT_UP

#if !defined(NT_UP)

    //
    // On MP machines the idle & system process do not share a top level
    // page directory because hyperspace mappings are protected by a
    // per-process spinlock.  Having two processes share a single hyperspace
    // (by virtue of sharing a top level page directory) would make the
    // spinlock synchronization meaningless.
    //

    DummyFlags = 0;
    Status = MmInitializeProcessAddressSpace (ProcessToInitialize,
                                              NULL,
                                              NULL,
                                              &DummyFlags,
                                              NULL);

#if defined(_X86_)

    if ((MmVirtualBias != 0) &&
        (PsInitialSystemProcess == NULL) &&
        (NT_SUCCESS (Status))) {

        KIRQL OldIrql;
        PMMPTE PointerPte;
        PFN_NUMBER PageFrameIndex;
        PMMPTE PointerFillPte;
        PEPROCESS CurrentProcess;
        PMMPTE CurrentAddressSpacePde;

        //
        // When booted /3GB, the initial system mappings at 8xxxxxxx must be
        // copied because things like the loader block contain pointers to
        // this area and are referenced by the system during early startup
        // despite the fact that the rest of the system is biased correctly.
        //

        CurrentProcess = PsGetCurrentProcess ();

#if defined (_X86PAE_)

        //
        // Select the top level page directory that maps 2GB->3GB.
        //

        PointerPte = (PMMPTE) ProcessToInitialize->PaeTop;

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + 2);

#else

        PageFrameIndex = MI_GET_DIRECTORY_FRAME_FROM_PROCESS (ProcessToInitialize);

#endif

        MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

        if (MappingPte != NULL) {

            MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                      PageFrameIndex,
                                      MM_READWRITE,
                                      MappingPte);

            MI_SET_PTE_DIRTY (TempPte2);

            MI_WRITE_VALID_PTE (MappingPte, TempPte2);

            PointerPte = MiGetVirtualAddressMappedByPte (MappingPte);
            OldIrql = MM_NOIRQL;
        }
        else {
            PointerPte = MiMapPageInHyperSpace (CurrentProcess,
                                                PageFrameIndex,
                                                &OldIrql);
        }

        PointerFillPte = &PointerPte[MiGetPdeOffset(CODE_START)];

        CurrentAddressSpacePde = MiGetPdeAddress ((PVOID) CODE_START);

        RtlCopyMemory (PointerFillPte,
                       CurrentAddressSpacePde,
                       (((1 + CODE_END) - CODE_START) / MM_VA_MAPPED_BY_PDE) * sizeof(MMPTE));

        if (MappingPte != NULL) {
            MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
        }
        else {
            MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);
        }
    }

#endif

#else

    PMMVAD VadShare;
    PETHREAD CurrentThread;

    Status = STATUS_SUCCESS;
    CurrentThread = PsGetCurrentThread ();

    //
    // Allocate a non-user-deletable VAD to map the shared memory page.
    //

    if (MM_HIGHEST_VAD_ADDRESS > (PVOID) MM_SHARED_USER_DATA_VA) {

        KeAttachProcess (&ProcessToInitialize->Pcb);

        VadShare = MiAllocateVad (MM_SHARED_USER_DATA_VA,
                                  MM_SHARED_USER_DATA_VA+ (X64K)-1,
                                  FALSE);

        //
        // Insert the VAD.
        //
        // N.B. No failure can occur since there is no commit charge.
        //

        if (VadShare != NULL) {

            Status = MiInsertVadCharges (VadShare, ProcessToInitialize);

            ASSERT (NT_SUCCESS (Status));

            LOCK_WS (CurrentThread, ProcessToInitialize);

            MiInsertVad (VadShare, ProcessToInitialize);

            UNLOCK_WS (CurrentThread, ProcessToInitialize);
        }
        else {
            Status = STATUS_NO_MEMORY;
        }

        KeDetachProcess ();
    }

#endif

    if ((MmTrackLockedPages == TRUE) && (NT_SUCCESS (Status))) {

        LockedPagesHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                   sizeof(LOCK_HEADER),
                                                   'xTmM');

        if (LockedPagesHeader != NULL) {

            LockedPagesHeader->Count = 0;
            LockedPagesHeader->Valid = TRUE;
            InitializeListHead (&LockedPagesHeader->ListHead);
            KeInitializeSpinLock (&LockedPagesHeader->Lock);
            
            //
            // Note an explicit memory barrier is not needed here because
            // we must detach from this process before the field can be
            // accessed.  And any other processor would need to context
            // swap to this process before the field could be accessed, so
            // the implicit memory barriers in context swap are sufficient.
            //

            ProcessToInitialize->LockedPagesList = (PVOID) LockedPagesHeader;
        }
        else {
            Status = STATUS_NO_MEMORY;
        }
    }

    return Status;
}


VOID
MmDeleteProcessAddressSpace (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine deletes a process's Page Directory and working set page.

Arguments:

    Process - Supplies a pointer to the deleted process.

Return Value:

    None.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PEPROCESS CurrentProcess;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER VadBitMapPage;
    PFN_NUMBER PageFrameIndex2;
#if (_MI_PAGING_LEVELS >= 4)
    PFN_NUMBER PageFrameIndex3;
    PMMPTE ExtendedPageDirectoryParent;
    PMMPTE PointerPxe;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PageDirectoryParent;
    PMMPTE PointerPpe;
#endif
#if defined (_X86PAE_)
    ULONG i;
    PPAE_ENTRY PaeVa;

    PaeVa = (PPAE_ENTRY) Process->PaeTop;
#endif

    CurrentProcess = PsGetCurrentProcess ();

#if !defined(_WIN64)

    LOCK_EXPANSION (OldIrql);

    RemoveEntryList (&Process->MmProcessLinks);

    UNLOCK_EXPANSION (OldIrql);

#endif

    //
    // Return commitment.
    //

    MiReturnCommitment (MM_PROCESS_COMMIT_CHARGE);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROCESS_DELETE, MM_PROCESS_COMMIT_CHARGE);
    ASSERT (Process->CommitCharge == 0);

    //
    // Remove the working set list page from the deleted process.
    //

    Pfn1 = MI_PFN_ELEMENT (Process->WorkingSetPage);
    Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

    InterlockedExchangeAddSizeT (&MmProcessCommit, 0 - MM_PROCESS_COMMIT_CHARGE);

    LOCK_PFN (OldIrql);

    if (Process->AddressSpaceInitialized == 2) {

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);
        MiDecrementShareCount (Pfn1, Process->WorkingSetPage);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

        //
        // Map the hyper space page table page from the deleted process
        // so the vad bit map page can be captured.
        //

        PageFrameIndex = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (Process);

        PointerPte = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                 PageFrameIndex);

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + MiGetPteOffset(VAD_BITMAP_SPACE));

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPte);

        //
        // Remove the VAD bitmap page.
        //

        Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);
        MiDecrementShareCount (Pfn1, VadBitMapPage);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

        //
        // Remove the first hyper space page table page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);
        MiDecrementShareCount (Pfn1, PageFrameIndex);
        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

#if defined (_X86PAE_)

        //
        // Remove the page directory pages.
        //

        PointerPte = (PMMPTE) PaeVa;
        ASSERT (PaeVa != &MiSystemPaeVa);

        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            PageFrameIndex2 = Pfn1->u4.PteFrame;
            Pfn2 = MI_PFN_ELEMENT (PageFrameIndex2);
            MI_SET_PFN_DELETED (Pfn1);

            MiDecrementShareCount (Pfn1, PageFrameIndex);
            MiDecrementShareCount (Pfn2, PageFrameIndex2);

            ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));
            PointerPte += 1;
        }
#endif

        //
        // Remove the top level page directory page.
        //

        PageFrameIndex = MI_GET_DIRECTORY_FRAME_FROM_PROCESS(Process);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Get a pointer to the top-level page directory parent page via
        // its KSEG0 address.
        //

#if (_MI_PAGING_LEVELS >= 4)

        ExtendedPageDirectoryParent = MiMapPageInHyperSpaceAtDpc (
                                                             CurrentProcess,
                                                             PageFrameIndex);

        //
        // Remove the hyper space page directory parent page
        // from the deleted process.
        //

        PointerPxe = &ExtendedPageDirectoryParent[MiGetPxeOffset(HYPER_SPACE)];
        PageFrameIndex3 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPxe);
        ASSERT (MI_PFN_ELEMENT(PageFrameIndex3)->u4.PteFrame == PageFrameIndex);

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, ExtendedPageDirectoryParent);

        PageDirectoryParent = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                          PageFrameIndex3);

#else
        PageDirectoryParent = KSEG_ADDRESS (PageFrameIndex);
#endif

        //
        // Remove the hyper space page directory page from the deleted process.
        //

        PointerPpe = &PageDirectoryParent[MiGetPpeOffset(HYPER_SPACE)];
        PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPpe);

#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);
#endif

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex2);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

        MiDecrementShareCount (Pfn1, PageFrameIndex2);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

#if (_MI_PAGING_LEVELS >= 4)
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex3);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);
        MiDecrementShareCount (Pfn1, PageFrameIndex3);
        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));
#endif
#endif

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareCount (Pfn1, PageFrameIndex);

        MiDecrementShareCount (Pfn1, PageFrameIndex);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

    }
    else {

        //
        // Process initialization never completed, just return the pages
        // to the free list.
        //

        MiInsertPageInFreeList (Process->WorkingSetPage);

        PageFrameIndex = MI_GET_DIRECTORY_FRAME_FROM_PROCESS (Process);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Get a pointer to the top-level page directory parent page via
        // its KSEG0 address.
        //

        PageDirectoryParent = KSEG_ADDRESS (PageFrameIndex);

#if (_MI_PAGING_LEVELS >= 4)
        PageDirectoryParent = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                          PageFrameIndex);

        PageFrameIndex3 = MI_GET_PAGE_FRAME_FROM_PTE (&PageDirectoryParent[MiGetPxeOffset(HYPER_SPACE)]);

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);

        PageDirectoryParent = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                          PageFrameIndex3);
#endif

        PointerPpe = &PageDirectoryParent[MiGetPpeOffset(HYPER_SPACE)];
        PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPpe);

#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);
#endif

        MiInsertPageInFreeList (PageFrameIndex2);

#if (_MI_PAGING_LEVELS >= 4)
        MiInsertPageInFreeList (PageFrameIndex3);
#endif
#endif

        PageFrameIndex2 = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (Process);

        PointerPte = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                 PageFrameIndex2);

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + MiGetPteOffset(VAD_BITMAP_SPACE));

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPte);

        MiInsertPageInFreeList (VadBitMapPage);

        //
        // Free the hyper space page table page.
        //

        MiInsertPageInFreeList (PageFrameIndex2);

#if defined (_X86PAE_)
        PointerPte = (PMMPTE) PaeVa;
        ASSERT (PaeVa != &MiSystemPaeVa);

        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            MiInsertPageInFreeList (PageFrameIndex2);
            PointerPte += 1;
        }
#endif

        //
        // Free the topmost page directory page.
        //

        MiInsertPageInFreeList (PageFrameIndex);
    }

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (MM_PROCESS_CREATE_CHARGE,
                                     MM_RESAVAIL_FREE_DELETE_PROCESS);

#if defined (_X86PAE_)

    //
    // Free the page directory page pointers.
    //

    ASSERT (PaeVa != &MiSystemPaeVa);
    MiPaeFree (PaeVa);

#endif

    if (Process->Session != NULL) {

        //
        // The Terminal Server session space data page and mapping PTE can only
        // be freed when the last process in the session is deleted.  This is
        // because IA64 maps session space into region 1 and exited processes
        // maintain their session space mapping as attaches may occur even
        // after process exit that reference win32k, etc.  Since the region 1
        // mapping is being inserted into region registers during swap context,
        // these mappings cannot be torn down until the very last deletion
        // occurs.
        //

        MiReleaseProcessReferenceToSessionDataPage (Process->Session);
    }

    //
    // Check to see if the paging files should be contracted.
    //

    MiContractPagingFiles ();

    return;
}


VOID
MiDeletePteRange (
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL AddressSpaceDeletion
    )

/*++

Routine Description:

    This routine deletes a range of PTEs and when possible, the PDEs, PPEs and
    PXEs as well.  Commit is returned here for the hierarchies here.

Arguments:

    WsInfo - Supplies the working set structure whose PTEs are being deleted.

    PointerPte - Supplies the PTE to begin deleting at.

    LastPte - Supplies the PTE to stop deleting at (don't delete this one).
              -1 signifies keep going until a nonvalid PTE is found.

    AddressSpaceDeletion - Supplies TRUE if the address space is in the final
                           stages of deletion, FALSE otherwise.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PVOID TempVa;
    KIRQL OldIrql;
    MMPTE_FLUSH_LIST PteFlushList;
    PFN_NUMBER CommittedPages;
    PEPROCESS Process;
    BOOLEAN AllProcessors;
    PMMPTE PointerPde;
    LOGICAL Boundary;
    LOGICAL FinalPte;
    PMMPFN Pfn1;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    if (PointerPte >= LastPte) {
        return;
    }

    if (WsInfo->VmWorkingSetList == MmWorkingSetList) {
        AllProcessors = FALSE;
        Process = CONTAINING_RECORD (WsInfo, EPROCESS, Vm);
    }
    else {
        AllProcessors = TRUE;
        Process = NULL;
    }

    CommittedPages = 0;
    PteFlushList.Count = 0;
    SATISFY_OVERZEALOUS_COMPILER (PteFlushList.FlushVa[0] = NULL);

    PointerPde = MiGetPteAddress (PointerPte);

    LOCK_PFN (OldIrql);

#if (_MI_PAGING_LEVELS >= 3)
    PointerPpe = MiGetPdeAddress (PointerPte);

#if (_MI_PAGING_LEVELS >= 4)
    PointerPxe = MiGetPpeAddress (PointerPte);
    if ((PointerPxe->u.Hard.Valid == 1) &&
        (PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1) &&
        (PointerPte->u.Hard.Valid == 1))
#else
    if ((PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1) &&
        (PointerPte->u.Hard.Valid == 1))
#endif
#else
    if ((PointerPde->u.Hard.Valid == 1) &&
        (PointerPte->u.Hard.Valid == 1))
#endif
    {

        do {

            ASSERT (PointerPte->u.Hard.Valid == 1);

            TempVa = MiGetVirtualAddressMappedByPte (PointerPte);

            if (Process != NULL) {
                MiDeletePte (PointerPte,
                             TempVa,
                             AddressSpaceDeletion,
                             Process,
                             NULL,
                             &PteFlushList,
                             OldIrql);
                Process->NumberOfPrivatePages += 1;
            }
            else {
                MiDeleteValidSystemPte (PointerPte,
                                        TempVa,
                                        WsInfo,
                                        &PteFlushList);
            }

            CommittedPages += 1;
            PointerPte += 1;

            //
            // If all the entries have been removed from the previous page
            // table page, delete the page table page itself.  Likewise with
            // the page directory page.
            //

            if (MiIsPteOnPdeBoundary (PointerPte)) {
                Boundary = TRUE;
            }
            else {
                Boundary = FALSE;
            }

            if ((PointerPte >= LastPte) ||
#if (_MI_PAGING_LEVELS >= 3)
#if (_MI_PAGING_LEVELS >= 4)
                ((MiGetPpeAddress (PointerPte))->u.Hard.Valid == 0) ||
#endif
                ((MiGetPdeAddress (PointerPte))->u.Hard.Valid == 0) ||
#endif
                ((MiGetPteAddress (PointerPte))->u.Hard.Valid == 0) ||
                (PointerPte->u.Hard.Valid == 0)) {

                FinalPte = TRUE;
            }
            else {
                FinalPte = FALSE;
            }

            if ((Boundary == TRUE) || (FinalPte == TRUE)) {

                if (PteFlushList.Count == 0) {
                    NOTHING;
                }
                else if (PteFlushList.Count == 1) {
                    MI_FLUSH_SINGLE_TB (PteFlushList.FlushVa[0], AllProcessors);
                }
                else if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                    MI_FLUSH_MULTIPLE_TB (PteFlushList.Count,
                                          &PteFlushList.FlushVa[0],
                                          AllProcessors);
                }
                else {
                    if (AllProcessors) {
                        MI_FLUSH_ENTIRE_TB (0x1C);
                    }
                    else {
                        MI_FLUSH_PROCESS_TB (FALSE);
                    }
                }

                PointerPde = MiGetPteAddress (PointerPte - 1);

                ASSERT (PointerPde->u.Hard.Valid == 1);

                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPde));

                if ((Pfn1->u2.ShareCount == 1) &&
                    (Pfn1->u3.e2.ReferenceCount == 1) &&
                    (Pfn1->u1.WsIndex != 0)) {

                    if (Process != NULL) {
                        MiDeletePte (PointerPde,
                                     PointerPte - 1,
                                     AddressSpaceDeletion,
                                     Process,
                                     NULL,
                                     NULL,
                                     OldIrql);
                        Process->NumberOfPrivatePages += 1;
                    }
                    else {
                        MiDeleteValidSystemPte (PointerPde,
                                                PointerPte - 1,
                                                WsInfo,
                                                &PteFlushList);
                    }

                    CommittedPages += 1;

#if (_MI_PAGING_LEVELS >= 3)
                    if ((FinalPte == TRUE) || (MiIsPteOnPpeBoundary(PointerPte))) {

                        PointerPpe = MiGetPteAddress (PointerPde);

                        ASSERT (PointerPpe->u.Hard.Valid == 1);

                        Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPpe));

                        if (Pfn1->u2.ShareCount == 1 && Pfn1->u3.e2.ReferenceCount == 1)
                        {
                            if (Process != NULL) {
                                MiDeletePte (PointerPpe,
                                             PointerPde,
                                             AddressSpaceDeletion,
                                             Process,
                                             NULL,
                                             NULL,
                                             OldIrql);
                                Process->NumberOfPrivatePages += 1;
                            }
                            else {
                                MiDeleteValidSystemPte (PointerPpe,
                                                        PointerPde,
                                                        WsInfo,
                                                        &PteFlushList);
                            }

                            CommittedPages += 1;
#if (_MI_PAGING_LEVELS >= 4)
                            if ((FinalPte == TRUE) || (MiIsPteOnPxeBoundary(PointerPte))) {

                                PointerPxe = MiGetPdeAddress (PointerPde);

                                ASSERT (PointerPxe->u.Hard.Valid == 1);

                                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPxe));

                                if (Pfn1->u2.ShareCount == 1 && Pfn1->u3.e2.ReferenceCount == 1)
                                {
                                    if (Process != NULL) {
                                        MiDeletePte (PointerPxe,
                                                     PointerPpe,
                                                     AddressSpaceDeletion,
                                                     Process,
                                                     NULL,
                                                     NULL,
                                                     OldIrql);
                                        Process->NumberOfPrivatePages += 1;
                                    }
                                    else {
                                        MiDeleteValidSystemPte (PointerPxe,
                                                                PointerPpe,
                                                                WsInfo,
                                                                &PteFlushList);
                                    }
                                    CommittedPages += 1;
                                }
                            }
#endif
                        }
                    }
#endif
                }
                if (FinalPte == TRUE) {
                    break;
                }
            }
            ASSERT (PointerPte->u.Hard.Valid == 1);
        } while (TRUE);
    }

    if (PteFlushList.Count == 0) {
        NOTHING;
    }
    else if (PteFlushList.Count == 1) {
        MI_FLUSH_SINGLE_TB (PteFlushList.FlushVa[0], AllProcessors);
    }
    else if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (PteFlushList.Count,
                              &PteFlushList.FlushVa[0],
                              AllProcessors);
    }
    else {
        if (AllProcessors) {
            MI_FLUSH_ENTIRE_TB (0x1D);
        }
        else {
            MI_FLUSH_PROCESS_TB (FALSE);
        }
    }

    UNLOCK_PFN (OldIrql);

    if (CommittedPages != 0) {
        MiReturnCommitment (CommittedPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PTE_RANGE, CommittedPages);

        MI_INCREMENT_RESIDENT_AVAILABLE (CommittedPages, MM_RESAVAIL_FREE_CLEAN_PROCESS_WS);
    }

    return;
}


VOID
MiUnlinkWorkingSet (
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This routine removes the argument working set from the working set
    manager's linked list.

Arguments:

    WsInfo - Supplies the working set to remove.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    KIRQL OldIrql;
    KEVENT Event;
    PKTHREAD CurrentThread;

    //
    // If working set expansion for this process is allowed, disable
    // it and remove the process from expanded process list if it
    // is on it.
    //

    LOCK_EXPANSION (OldIrql);

    if (WsInfo->WorkingSetExpansionLinks.Flink == MM_WS_TRIMMING) {

        //
        // Initialize an event and put the event address
        // in the blink field.  When the trimming is complete,
        // this event will be set.
        //

        KeInitializeEvent (&Event, NotificationEvent, FALSE);

        WsInfo->WorkingSetExpansionLinks.Blink = (PLIST_ENTRY)&Event;

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);

        UNLOCK_EXPANSION (OldIrql);

        KeWaitForSingleObject (&Event,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        KeLeaveCriticalRegionThread (CurrentThread);

        ASSERT (WsInfo->WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED);
    }
    else if (WsInfo->WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED) {

        //
        // This process' working set is in an initialization state and has
        // never been inserted into any lists.
        //

        UNLOCK_EXPANSION (OldIrql);
    }
    else {

        RemoveEntryList (&WsInfo->WorkingSetExpansionLinks);

        //
        // Disable expansion.
        //

        WsInfo->WorkingSetExpansionLinks.Flink = MM_WS_NOT_LISTED;

        UNLOCK_EXPANSION (OldIrql);
    }

    return;
}


VOID
MmCleanProcessAddressSpace (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine cleans an address space by deleting all the user and
    pageable portions of the address space.  At the completion of this
    routine, no page faults may occur within the process.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PMMVAD Vad;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PETHREAD Thread;
    LONG AboveWsMin;
    ULONG NumberOfCommittedPageTables;
    PLOCK_HEADER LockedPagesHeader;
    PLIST_ENTRY NextEntry;
    PLOCK_TRACKER Tracker;
    PIO_ERROR_LOG_PACKET ErrLog;
#if defined (_WIN64)
    PWOW64_PROCESS TempWow64;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID CommittedPageTables;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PVOID CommittedPageDirectories;
#endif

    if ((Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) ||
        (Process->AddressSpaceInitialized == 0)) {

        //
        // This process's address space has already been deleted.  However,
        // this process can still have a session space.  Get rid of it now.
        //

        MiSessionRemoveProcess ();

        return;
    }

    if (Process->AddressSpaceInitialized == 1) {

        //
        // The process has been created but not fully initialized.
        // Return partial resources now.
        //

        MI_INCREMENT_RESIDENT_AVAILABLE (
            Process->Vm.MinimumWorkingSetSize - MM_PROCESS_CREATE_CHARGE,
            MM_RESAVAIL_FREE_CLEAN_PROCESS1);

        //
        // Clear the AddressSpaceInitialized flag so we don't over-return
        // resident available as this routine can be called more than once
        // for the same process.
        //

        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
        ASSERT (Process->AddressSpaceInitialized == 0);

        //
        // This process's address space has already been deleted.  However,
        // this process can still have a session space.  Get rid of it now.
        //

        MiSessionRemoveProcess ();

        return;
    }

    //
    // Remove this process from the trim list.
    //

    MiUnlinkWorkingSet (&Process->Vm);

    //
    // Remove this process from the session list.
    //

    MiSessionRemoveProcess ();

    PointerPte = MiGetPteAddress (&MmWsle[MM_MAXIMUM_WORKING_SET]) + 1;

    Thread = PsGetCurrentThread ();

    //
    // Delete all the user owned pageable virtual addresses in the process.
    //

    //
    // Both mutexes must be owned to synchronize with the bit setting and
    // clearing of VM_DELETED.   This is because various callers acquire
    // only one of them (either one) before checking.
    //

    LOCK_ADDRESS_SPACE (Process);

    LOCK_WS_UNSAFE (Thread, Process)

    PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_VM_DELETED);

    //
    // Delete all the valid user mode addresses from the working set list.
    // At this point page faults are allowed on user space addresses but are
    // generally rare (would have to be an attached thread doing the
    // references).  Faults are allowed on page tables for user space, which
    // requires that we keep the working set structure consistent until we
    // finally take it all down.
    //

    MiDeleteAddressesInWorkingSet (Process);

    //
    // Remove hash table pages, if any.  This is the first time we do this
    // during the deletion path, but we need to do it again before we finish
    // because we may fault in some page tables during the VAD clearing.  We
    // could have maintained the hash table validity during the WorkingSet
    // deletion above in order to avoid freeing the hash table twice, but since
    // we're just deleting it all anyway, it's faster to do it this way.  Note
    // that if we don't do this or maintain the validity, we can trap later
    // in MiGrowWsleHash.
    //

    LastPte = MiGetPteAddress (MmWorkingSetList->HighestPermittedHashAddress);

    MiDeletePteRange (&Process->Vm, PointerPte, LastPte, FALSE);

    //
    // Clear the hash fields as a fault may occur below on the page table
    // pages during VAD clearing and resolution of the fault may result in
    // adding a hash table.  Thus these fields must be consistent with the
    // clearing just done above.
    //

    MmWorkingSetList->HashTableSize = 0;
    MmWorkingSetList->HashTable = NULL;

    UNLOCK_WS_UNSAFE (Thread, Process)

    //
    // Delete the virtual address descriptors and dereference any
    // section objects.
    //

    while (Process->VadRoot.NumberGenericTableElements != 0) {

        Vad = (PMMVAD) Process->VadRoot.BalancedRoot.RightChild;

        MiRemoveVadCharges (Vad, Process);

        LOCK_WS_UNSAFE (Thread, Process)

        MiRemoveVad (Vad, Process);

        //
        // If the system is NT64 (or NT32 and has been biased to an
        // alternate base address to allow 3gb of user address space),
        // then check if the current VAD describes the shared memory page.
        //

        if (MM_HIGHEST_VAD_ADDRESS > (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // If the VAD describes the shared memory page, then free the
            // VAD and continue with the next entry.
            //

            if (Vad->StartingVpn == MI_VA_TO_VPN (MM_SHARED_USER_DATA_VA)) {
                ASSERT (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA);
                UNLOCK_WS_UNSAFE (Thread, Process)
                ExFreePool (Vad);
                continue;
            }
        }

        if (((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) ||
            (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory)) {

            //
            // This VAD represents a mapped view or a driver-mapped physical
            // view - delete the view and perform any section related cleanup
            // operations.
            //

            if (Vad->u.VadFlags.VadType == VadRotatePhysical) {
                MiPhysicalViewRemover (Process, Vad);
            }

            //
            // NOTE: MiRemoveMappedView releases the working set pushlock
            // before returning.
            //

            MiRemoveMappedView (Process, Vad);
        }
        else {

            if (Vad->u.VadFlags.VadType == VadLargePages) {

                UNLOCK_WS_UNSAFE (Thread, Process);

                MiAweViewRemover (Process, Vad);

                MiReleasePhysicalCharges (Vad->EndingVpn - Vad->StartingVpn + 1,
                                          Process);

                LOCK_WS_UNSAFE (Thread, Process);

                MiFreeLargePages (MI_VPN_TO_VA (Vad->StartingVpn),
                                  MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                  FALSE);

            }
            else if (Vad->u.VadFlags.VadType == VadAwe) {

                //
                // Free all the physical pages that this VAD might be mapping.
                // Since only the AWE lock synchronizes the remap API, carefully
                // remove this VAD from the list first.
                //

                UNLOCK_WS_UNSAFE (Thread, Process);

                MiAweViewRemover (Process, Vad);
                MiRemoveUserPhysicalPagesVad ((PMMVAD_SHORT)Vad);

                LOCK_WS_UNSAFE (Thread, Process);

                MiDeletePageTablesForPhysicalRange (
                        MI_VPN_TO_VA (Vad->StartingVpn),
                        MI_VPN_TO_VA_ENDING (Vad->EndingVpn));
            }
            else {

                if (Vad->u.VadFlags.VadType == VadWriteWatch) {
                    MiPhysicalViewRemover (Process, Vad);
                }

                MiDeleteVirtualAddresses (MI_VPN_TO_VA (Vad->StartingVpn),
                                          MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                          Vad);
            }

            UNLOCK_WS_UNSAFE (Thread, Process)
        }

        ExFreePool (Vad);
    }

    if (Process->AweInfo != NULL) {
        MiCleanPhysicalProcessPages (Process);
    }

    LOCK_WS_UNSAFE (Thread, Process);

    if (Process->CloneRoot != NULL) {
        ASSERT (((PMM_AVL_TABLE)Process->CloneRoot)->NumberGenericTableElements == 0);
        ExFreePool (Process->CloneRoot);
        Process->CloneRoot = NULL;
    }

    if (Process->PhysicalVadRoot != NULL) {
        ASSERT (Process->PhysicalVadRoot->NumberGenericTableElements == 0);
        ExFreePool (Process->PhysicalVadRoot);
        Process->PhysicalVadRoot = NULL;
    }

    //
    // Delete the shared data page, if any.  Note this deliberately
    // compares the highest user address instead of the highest VAD address.
    // This is because we must always delete the link to the physical page
    // even on platforms where the VAD was not allocated.  The only exception
    // to this is when we're booted on x86 with /USERVA=1nnn to increase the
    // kernel address space beyond 2GB.
    //

    if (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA) {

        //
        // This zeroes the PTE - so you would think after this, it can still
        // be faulted on and re-evaluated directly via MiCheckVirtualAddress
        // even though it has no VAD - however this cannot happen because
        // during this deletion, the containing page table page will be
        // deleted and this cannot be replaced without a VAD, thus short
        // circuiting the re-evaluation case above.
        //

        MiDeleteVirtualAddresses ((PVOID) MM_SHARED_USER_DATA_VA,
                                  (PVOID) MM_SHARED_USER_DATA_VA,
                                  NULL);
    }

    //
    // Adjust the count of pages above working set maximum.  This
    // must be done here because the working set list is not
    // updated during this deletion.
    //

    AboveWsMin = (LONG)(Process->Vm.WorkingSetSize - Process->Vm.MinimumWorkingSetSize);

    UNLOCK_WS_UNSAFE (Thread, Process);

    if (AboveWsMin > 0) {
        InterlockedExchangeAddSizeT (&MmPagesAboveWsMinimum, 0 - (PFN_NUMBER)AboveWsMin);
    }

    //
    // Delete the system portion of the address space.
    // Only now is it safe to specify TRUE to MiDelete because now that the
    // VADs have been deleted we can no longer fault on user space pages.
    //
    // Return commitment for page table pages.  Note that for NT64, we
    // allocate the arrays *AFTER* the initial MM_MAX_COMMIT VADs are created
    // because we detect 32 bit processes and allocate smaller arrays for
    // those since they have less accessible user virtual address space.
    // Thus for NT64, we must check that the allocation actually succeeded.
    //

    NumberOfCommittedPageTables = MmWorkingSetList->NumberOfCommittedPageTables;

#if (_MI_PAGING_LEVELS >= 3)
    NumberOfCommittedPageTables += MmWorkingSetList->NumberOfCommittedPageDirectories;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    NumberOfCommittedPageTables += MmWorkingSetList->NumberOfCommittedPageDirectoryParents;
#endif

    MiReturnCommitment (NumberOfCommittedPageTables);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROCESS_CLEAN_PAGETABLES,
                     NumberOfCommittedPageTables);

    Process->CommitCharge -= NumberOfCommittedPageTables;
    PsReturnProcessPageFileQuota (Process, NumberOfCommittedPageTables);


    MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - NumberOfCommittedPageTables);

#if (_MI_PAGING_LEVELS >= 3)
    CommittedPageTables = MmWorkingSetList->CommittedPageTables;
    if (CommittedPageTables != NULL) {
        MmWorkingSetList->CommittedPageTables = NULL;
        ExFreePool (CommittedPageTables);
    }
#endif

#if (_MI_PAGING_LEVELS >= 4)
    CommittedPageDirectories = MmWorkingSetList->CommittedPageDirectories;
    if (CommittedPageDirectories != NULL) {
        MmWorkingSetList->CommittedPageDirectories = NULL;
        ExFreePool (CommittedPageDirectories);
    }
#endif

    LOCK_WS_UNSAFE (Thread, Process);

    //
    // Make sure all the clone descriptors went away.
    //

    ASSERT (Process->CloneRoot == NULL);

    //
    // Make sure there are no dangling locked pages.
    //

    LockedPagesHeader = (PLOCK_HEADER) Process->LockedPagesList;

    if (Process->NumberOfLockedPages != 0) {

        if (LockedPagesHeader != NULL) {

            if ((LockedPagesHeader->Count != 0) &&
                (LockedPagesHeader->Valid == TRUE)) {

                ASSERT (IsListEmpty (&LockedPagesHeader->ListHead) == 0);
                NextEntry = LockedPagesHeader->ListHead.Flink;

                Tracker = CONTAINING_RECORD (NextEntry,
                                             LOCK_TRACKER,
                                             ListEntry);

                KeBugCheckEx (DRIVER_LEFT_LOCKED_PAGES_IN_PROCESS,
                              (ULONG_PTR)Tracker->CallingAddress,
                              (ULONG_PTR)Tracker->CallersCaller,
                              (ULONG_PTR)Tracker->Mdl,
                              Process->NumberOfLockedPages);
            }
        }
        else if (MiSafeBooted == FALSE) {

            if ((KdDebuggerEnabled) && (KdDebuggerNotPresent == FALSE)) {

                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                    "A driver has leaked %d bytes of physical memory.\n",
                        Process->NumberOfLockedPages << PAGE_SHIFT);

                //
                // Pop into the debugger (even on free builds) to determine
                // the cause of the leak and march on.
                //
        
                DbgBreakPoint ();
            }

            if (MmTrackLockedPages == FALSE) {

                MmTrackLockedPages = TRUE;
                MmLeakedLockedPages += Process->NumberOfLockedPages;

                ErrLog = IoAllocateGenericErrorLogEntry (ERROR_LOG_MAXIMUM_SIZE);

                if (ErrLog != NULL) {

                    //
                    // Fill it in and write it out.
                    //

                    ErrLog->FinalStatus = STATUS_DRIVERS_LEAKING_LOCKED_PAGES;
                    ErrLog->ErrorCode = STATUS_DRIVERS_LEAKING_LOCKED_PAGES;
                    ErrLog->UniqueErrorValue = (ULONG) MmLeakedLockedPages;

                    IoWriteErrorLogEntry (ErrLog);
                }
            }
        }
    }

    if (LockedPagesHeader != NULL) {

        //
        // No need to acquire the spinlock to traverse here as the pages are
        // (unfortunately) never going to be unlocked.  Since this routine is
        // pageable, this removes the need to add a nonpaged stub routine to
        // do the traverses and frees.
        //

        NextEntry = LockedPagesHeader->ListHead.Flink;

        while (NextEntry != &LockedPagesHeader->ListHead) {

            Tracker = CONTAINING_RECORD (NextEntry,
                                         LOCK_TRACKER,
                                         ListEntry);

            RemoveEntryList (NextEntry);

            NextEntry = Tracker->ListEntry.Flink;

            ExFreePool (Tracker);
        }

        ExFreePool (LockedPagesHeader);
        Process->LockedPagesList = NULL;
    }

#if DBG
    if ((Process->NumberOfPrivatePages != 0) && (MmDebug & MM_DBG_PRIVATE_PAGES)) {
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
            "MM: Process contains private pages %ld\n",
               Process->NumberOfPrivatePages);
        DbgBreakPoint ();
    }
#endif

#if defined(_WIN64)

    //
    // Delete the WowProcess structure.
    //

    if (Process->Wow64Process != NULL) {
        TempWow64 = Process->Wow64Process;
        Process->Wow64Process = NULL;
        ExFreePool (TempWow64);
    }
#endif

    //
    // Remove hash table pages, if any.  Yes, we've already done this once
    // during the deletion path, but we need to do it again because we may
    // have faulted in some page tables during the VAD clearing.
    //

    PointerPte = MiGetPteAddress (&MmWsle[MM_MAXIMUM_WORKING_SET]) + 1;

    ASSERT (PointerPte < LastPte);

    MiDeletePteRange (&Process->Vm, PointerPte, LastPte, FALSE);

    ASSERT (Process->Vm.MinimumWorkingSetSize >= MM_PROCESS_CREATE_CHARGE);
    ASSERT (Process->Vm.WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED);

    MmWorkingSetList->HashTableSize = 0;
    MmWorkingSetList->HashTable = NULL;

    //
    // Remove all the working set list pages except for the first one.
    //
    // The first page is not removed because an attached thread could
    // still reference the shared user data page and since no VAD is
    // required for this translation, the fault code will materialize
    // a page table page(s) and access the working set list to add entries
    // for it (and its page table pages) to the working set list.
    //

    MiRemoveWorkingSetPages (&Process->Vm);

    //
    // Update the count of available resident pages.
    //

    MI_INCREMENT_RESIDENT_AVAILABLE (
        Process->Vm.MinimumWorkingSetSize - MM_PROCESS_CREATE_CHARGE,
        MM_RESAVAIL_FREE_CLEAN_PROCESS2);

    UNLOCK_WS_AND_ADDRESS_SPACE (Thread, Process);

    if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
        PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES,
                                -(SSIZE_T)NumberOfCommittedPageTables);
    }

    return;
}

#define KERNEL_BSTORE_SIZE          0
#define KERNEL_LARGE_BSTORE_SIZE    0
#define KERNEL_LARGE_BSTORE_COMMIT  0
#define KERNEL_STACK_GUARD_PAGES    1


PVOID
MmCreateKernelStack (
    IN BOOLEAN LargeStack,
    IN UCHAR PreferredNode
    )

/*++

Routine Description:

    This routine allocates a kernel stack and a no-access page within
    the non-pageable portion of the system address space.

Arguments:

    LargeStack - Supplies the value TRUE if a large stack should be
                 created.  FALSE if a small stack is to be created.

    PreferredNode - Supplies the preferred node to use for the physical
                    page allocations.  MP/NUMA systems only.

Return Value:

    Returns a pointer to the base of the kernel stack.  Note, that the
    base address points to the guard page, so space must be allocated
    on the stack before accessing the stack.

    If a kernel stack cannot be created, the value NULL is returned.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PMMPTE BasePte;
    MMPTE TempPte;
    PFN_NUMBER NumberOfPages;
    ULONG NumberOfPtes;
    ULONG ChargedPtes;
    ULONG RequestedPtes;
    ULONG NumberOfBackingStorePtes;
    PFN_NUMBER PageFrameIndex;
    ULONG i;
    PVOID StackVa;
    KIRQL OldIrql;
    PSLIST_HEADER DeadStackList;
    MMPTE DemandZeroPte;

    if (!LargeStack) {

        //
        // Check to see if any unused stacks are available.
        //

#if defined(MI_MULTINODE)
        DeadStackList = &KeNodeBlock[PreferredNode]->DeadStackList;
#else
        UNREFERENCED_PARAMETER (PreferredNode);
        DeadStackList = &MmDeadStackSListHead;
#endif

        if (ExQueryDepthSList (DeadStackList) != 0) {

            Pfn1 = (PMMPFN) InterlockedPopEntrySList (DeadStackList);

            if (Pfn1 != NULL) {
                PointerPte = Pfn1->PteAddress;
                PointerPte += 1;
                StackVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);
                return StackVa;
            }
        }
        NumberOfPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE);
        NumberOfBackingStorePtes = BYTES_TO_PAGES (KERNEL_BSTORE_SIZE);
        NumberOfPages = NumberOfPtes + NumberOfBackingStorePtes;
    }
    else {
        NumberOfPtes = BYTES_TO_PAGES (MI_LARGE_STACK_SIZE);
        NumberOfBackingStorePtes = BYTES_TO_PAGES (KERNEL_LARGE_BSTORE_SIZE);
        NumberOfPages = BYTES_TO_PAGES (KERNEL_LARGE_STACK_COMMIT
                                        + KERNEL_LARGE_BSTORE_COMMIT);
    }

    ChargedPtes = NumberOfPtes + NumberOfBackingStorePtes;

    //
    // Charge commitment for the page file space for the kernel stack.
    //

    if (MiChargeCommitment (ChargedPtes, NULL) == FALSE) {

        //
        // Commitment exceeded, return NULL, indicating no kernel
        // stacks are available.
        //

        return NULL;
    }

    //
    // Obtain enough pages to contain the stack plus a guard page from
    // the system PTE pool.  The system PTE pool contains nonpaged PTEs
    // which are currently empty.
    //

    RequestedPtes = ChargedPtes + KERNEL_STACK_GUARD_PAGES;

    BasePte = MiReserveSystemPtes (RequestedPtes, SystemPteSpace);

    if (BasePte == NULL) {
        MiReturnCommitment (ChargedPtes);
        return NULL;
    }

    PointerPte = BasePte;

    StackVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte + NumberOfPtes + 1);

    if (LargeStack) {
        PointerPte += BYTES_TO_PAGES (MI_LARGE_STACK_SIZE - KERNEL_LARGE_STACK_COMMIT);
    }

    DemandZeroPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

    DemandZeroPte.u.Soft.Protection = MM_NOACCESS;

    MI_MAKE_VALID_KERNEL_PTE (TempPte,
                              0,
                              MM_READWRITE,
                              (PointerPte + 1));

    MI_SET_PTE_DIRTY (TempPte);

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if (MI_NONPAGEABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        MiReleaseSystemPtes (BasePte, RequestedPtes, SystemPteSpace);
        MiReturnCommitment (ChargedPtes);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_KERNEL_STACK_CREATE, ChargedPtes);

    MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages,
                                     MM_RESAVAIL_ALLOCATE_CREATE_STACK);

    for (i = 0; i < NumberOfPages; i += 1) {
        PointerPte += 1;
        ASSERT (PointerPte->u.Hard.Valid == 0);
        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }
        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_NODE (PreferredNode));

        MI_WRITE_INVALID_PTE (PointerPte, DemandZeroPte);

        MiInitializePfn (PageFrameIndex, PointerPte, 1);

        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        MI_WRITE_VALID_PTE (PointerPte, TempPte);
    }

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAddSizeT (&MmProcessCommit, ChargedPtes);
    InterlockedExchangeAddSizeT (&MmKernelStackResident, NumberOfPages);
    InterlockedExchangeAdd (&MmKernelStackPages, (LONG) RequestedPtes);

    if (LargeStack) {
        InterlockedIncrement (&MmLargeStacks);
    }
    else {
        InterlockedIncrement (&MmSmallStacks);
    }

    return StackVa;
}

VOID
MmDeleteKernelStack (
    IN PVOID PointerKernelStack,
    IN BOOLEAN LargeStack
    )

/*++

Routine Description:

    This routine deletes a kernel stack and the no-access page within
    the non-pageable portion of the system address space.

Arguments:

    PointerKernelStack - Supplies a pointer to the base of the kernel stack.

    LargeStack - Supplies the value TRUE if a large stack is being deleted.
                 FALSE if a small stack is to be deleted.

Return Value:

    None.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER NumberOfPages;
    ULONG NumberOfPtes;
    ULONG NumberOfStackPtes;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    ULONG i;
    KIRQL OldIrql;
    MMPTE PteContents;
    PSLIST_HEADER DeadStackList;

    PointerPte = MiGetPteAddress (PointerKernelStack);

    //
    // PointerPte points to the guard page, point to the previous
    // page before removing physical pages.
    //

    PointerPte -= 1;

    //
    // Check to see if the stack page should be placed on the dead
    // kernel stack page list.  The dead kernel stack list is a
    // singly linked list of kernel stacks from terminated threads.
    // The stacks are saved on a linked list up to a maximum number
    // to avoid the overhead of flushing the entire TB on all processors
    // everytime a thread terminates.  The TB on all processors must
    // be flushed as kernel stacks reside in the non paged system part
    // of the address space.
    //

    if (!LargeStack) {

#if defined(MI_MULTINODE)

        //
        // Scan the physical page frames and only place this stack on the
        // dead stack list if all the pages are on the same node.  Realize
        // if this push goes cross node it may make the interlocked instruction
        // slightly more expensive, but worth it all things considered.
        //

        ULONG NodeNumber;

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        NodeNumber = Pfn1->u3.e1.PageColor;

        DeadStackList = &KeNodeBlock[NodeNumber]->DeadStackList;

#else

        DeadStackList = &MmDeadStackSListHead;

#endif

        NumberOfPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE + KERNEL_BSTORE_SIZE);

        if (ExQueryDepthSList (DeadStackList) < MmMaximumDeadKernelStacks) {

#if defined(MI_MULTINODE)

            //
            // The node could use some more dead stacks - but first make sure
            // all the physical pages are from the same node in a multinode
            // system.
            //

            if (KeNumberNodes > 1) {

                ULONG CheckPtes;

                CheckPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE);

                PointerPte -= 1;
                for (i = 1; i < CheckPtes; i += 1) {

                    PteContents = *PointerPte;

                    if (PteContents.u.Hard.Valid == 0) {
                        break;
                    }

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                    if (NodeNumber != Pfn1->u3.e1.PageColor) {
                        PointerPte += i;
                        goto FreeStack;
                    }
                    PointerPte -= 1;
                }
                PointerPte += CheckPtes;
            }
#endif

            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

            InterlockedPushEntrySList (DeadStackList,
                                       (PSLIST_ENTRY)&Pfn1->u1.NextStackPfn);

            return;
        }
    }
    else {
        NumberOfPtes = BYTES_TO_PAGES (MI_LARGE_STACK_SIZE + KERNEL_LARGE_BSTORE_SIZE);
    }


#if defined(MI_MULTINODE)
FreeStack:
#endif

    //
    // We have exceeded the limit of dead kernel stacks or this is a large
    // stack, delete this kernel stack.
    //

    NumberOfPages = 0;

    NumberOfStackPtes = NumberOfPtes + KERNEL_STACK_GUARD_PAGES;

    LOCK_PFN (OldIrql);

    for (i = 0; i < NumberOfPtes; i += 1) {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            PageTableFrameIndex = Pfn1->u4.PteFrame;
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Mark the page as deleted so it will be freed when the
            // reference count goes to zero.
            //

            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCount (Pfn1, PageFrameIndex);
            NumberOfPages += 1;
        }
        PointerPte -= 1;
    }

    //
    // Now at the stack guard page, ensure it is still a guard page.
    //

    ASSERT (PointerPte->u.Hard.Valid == 0);

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAddSizeT (&MmProcessCommit, 0 - (ULONG_PTR)NumberOfPtes);
    InterlockedExchangeAddSizeT (&MmKernelStackResident, 0 - NumberOfPages);
    InterlockedExchangeAdd (&MmKernelStackPages, (LONG)(0 - NumberOfStackPtes));

    if (LargeStack) {
        InterlockedDecrement (&MmLargeStacks);
    }
    else {
        InterlockedDecrement (&MmSmallStacks);
    }

    //
    // Update the count of resident available pages.
    //

    MI_INCREMENT_RESIDENT_AVAILABLE (NumberOfPages, 
                                     MM_RESAVAIL_FREE_DELETE_STACK);

    //
    // Return PTEs and commitment.
    //

    MiReleaseSystemPtes (PointerPte, NumberOfStackPtes, SystemPteSpace);

    MiReturnCommitment (NumberOfPtes);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_KERNEL_STACK_DELETE, NumberOfPtes);

    return;
}

ULONG MiStackGrowthFailures[1];

NTSTATUS
MmGrowKernelStack (
    __in PVOID CurrentStack
    )

/*++

Routine Description:

    This function attempts to grows the current thread's kernel stack
    such that there is always KERNEL_LARGE_STACK_COMMIT bytes below
    the current stack pointer.

Arguments:

    CurrentStack - Supplies a pointer to the current stack pointer.

Return Value:

    STATUS_SUCCESS is returned if the stack was grown.

    STATUS_STACK_OVERFLOW is returned if there was not enough space reserved
    for the commitment.

    STATUS_NO_MEMORY is returned if there was not enough physical memory
    in the system.

--*/

{
    return MmGrowKernelStackEx (CurrentStack, KERNEL_LARGE_STACK_COMMIT);
}

NTSTATUS
MmGrowKernelStackEx (
    __in PVOID CurrentStack,
    __in SIZE_T CommitSize
    )

/*++

Routine Description:

    This function attempts to grows the current thread's kernel stack
    such that there is the specified number of bytes commited below
    the specified stack pointer.

Arguments:

    CurrentStack - Supplies a pointer to the current stack pointer.

    CommmitSize - Supplies the required commit size in bytes.

Return Value:

    STATUS_SUCCESS is returned if the stack was grown.

    STATUS_STACK_OVERFLOW is returned if there was not enough space reserved
    for the commitment.

    STATUS_NO_MEMORY is returned if there was not enough physical memory
    in the system.

--*/

{
    PMMPTE NewLimit;
    PMMPTE StackLimit;
    PMMPTE EndStack;
    PKTHREAD Thread;
    PFN_NUMBER NumberOfPages;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    MMPTE TempPte;

    Thread = KeGetCurrentThread ();
    ASSERT (((PCHAR)Thread->StackBase - (PCHAR)Thread->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    StackLimit = MiGetPteAddress (Thread->StackLimit);

    ASSERT (StackLimit->u.Hard.Valid == 1);

    NewLimit = MiGetPteAddress ((PVOID)((PUCHAR)CurrentStack - CommitSize));

    if (NewLimit == StackLimit) {
        return STATUS_SUCCESS;
    }

    //
    // If the new stack limit exceeds the reserved region for the kernel
    // stack, then return an error.
    //

    EndStack = MiGetPteAddress ((PVOID)((PUCHAR)Thread->StackBase -
                                                    MI_LARGE_STACK_SIZE));

    if (NewLimit < EndStack) {

        //
        // Don't go into guard page.
        //

        MiStackGrowthFailures[0] += 1;

#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
            "MmGrowKernelStack failed: Thread %p %p %p\n",
                        Thread, NewLimit, EndStack);
#endif

        return STATUS_STACK_OVERFLOW;

    }

    //
    // Lock the PFN database and attempt to expand the kernel stack.
    //

    StackLimit -= 1;

    NumberOfPages = (PFN_NUMBER) (StackLimit - NewLimit + 1);

    LOCK_PFN (OldIrql);

    if (MI_NONPAGEABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        return STATUS_NO_MEMORY;
    }

    //
    // Note MmResidentAvailablePages must be charged before calling
    // MiEnsureAvailablePageOrWait as it may release the PFN lock.
    //

    MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages,
                                     MM_RESAVAIL_ALLOCATE_GROW_STACK);

    while (StackLimit >= NewLimit) {

        ASSERT (StackLimit->u.Hard.Valid == 0);

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }
        PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (StackLimit));
        StackLimit->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

        StackLimit->u.Soft.Protection = MM_NOACCESS;

        MiInitializePfn (PageFrameIndex, StackLimit, 1);

        MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                  PageFrameIndex,
                                  MM_READWRITE,
                                  StackLimit);

        MI_SET_PTE_DIRTY (TempPte);
        *StackLimit = TempPte;
        StackLimit -= 1;
    }

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAddSizeT (&MmKernelStackResident, NumberOfPages);

#if DBG
    ASSERT (NewLimit->u.Hard.Valid == 1);
    if (NewLimit != EndStack) {
        ASSERT ((NewLimit - 1)->u.Hard.Valid == 0);
    }
#endif

    Thread->StackLimit = MiGetVirtualAddressMappedByPte (NewLimit);

    PERFINFO_GROW_STACK(Thread);

    return STATUS_SUCCESS;
}


VOID
MiOutPageSingleKernelStack (
    IN PKTHREAD Thread,
    IN PKERNEL_STACK_SEGMENT StackInfo,
    IN PMMPTE_FLUSH_LIST PteFlushList
    )

/*++

Routine Description:

    This routine makes the specified kernel stack non-resident and
    puts the pages on the transition list.  Note that pages below
    the CurrentStackPointer are not useful and these pages are freed here.

Arguments:

    Thread - Supplies a pointer to the thread whose stack should be removed.

    StackInfo - Supplies a pointer to the relevant stack information :

                    StackBase - Supplies the highest virtual address of the
                                stack.  This is where the stack begins.

                    KernelStack - Supplies the current stack pointer location.

                    StackLimit - Supplies the lowest committed virtual address
                                 in the stack.

                    ActualLimit - Supplies the lowest possible virtual address
                                  in the stack.

    PteFlushList - Supplies a flush list to use so the TB flush can be
                   deferred to the caller.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID DiscardExcess;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE EndOfStackPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PFN_NUMBER ResAvailToReturn;
    KIRQL OldIrql;
    MMPTE TempPte;
    PVOID BaseOfKernelStack;

    ULONG Count;
    PMMPTE LimitPte;
    PMMPTE LowestLivePte;

    ASSERT (KERNEL_LARGE_STACK_SIZE >= MI_LARGE_STACK_SIZE);

    ASSERT (BYTE_OFFSET (StackInfo->StackBase) == 0);
    ASSERT (BYTE_OFFSET (StackInfo->ActualLimit) == 0);

    ASSERT (((PCHAR)StackInfo->StackBase - (PCHAR)StackInfo->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    //
    // The first page of the stack is the page before the base
    // of the stack.
    //

    BaseOfKernelStack = ((PCHAR)StackInfo->StackBase - PAGE_SIZE);
    PointerPte = MiGetPteAddress (BaseOfKernelStack);
    LastPte = MiGetPteAddress ((PULONG_PTR)StackInfo->KernelStack - 1);

    LowestLivePte = NULL;

    if (MI_STACK_IS_TRIMMABLE (Thread)) {

        DiscardExcess = (PVOID) (ROUND_TO_PAGES (Thread->InitialStack) - KERNEL_LARGE_STACK_COMMIT);

        //
        // This is a large stack.  The stack pagein won't necessarily
        // bring back all the pages.  Make sure that we account now for the
        // ones that will disappear.
        //

        LowestLivePte = MiGetPteAddress (DiscardExcess);

        LimitPte = MiGetPteAddress ((PVOID) StackInfo->StackLimit);

        if (LowestLivePte < LimitPte) {
            LowestLivePte = LimitPte;
        }
    }

    EndOfStackPte = MiGetPteAddress ((PVOID) StackInfo->ActualLimit) - 1;

    ASSERT (LowestLivePte <= LastPte);

    //
    // Put a signature at the current stack location - sizeof(ULONG_PTR).
    //

    *((PULONG_PTR)StackInfo->KernelStack - 1) = (ULONG_PTR) Thread;

    Count = 0;
    ResAvailToReturn = 0;

    LOCK_PFN (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);


        TempPte = *PointerPte;
        MI_MAKE_VALID_PTE_TRANSITION (TempPte, 0);

        TempPte.u.Soft.Protection = MM_NOACCESS;

        Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn2->OriginalPte.u.Soft.Protection = MM_NOACCESS;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        if (PteFlushList->Count != MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList->FlushVa[PteFlushList->Count] = BaseOfKernelStack;
            PteFlushList->Count += 1;
        }

        MiDecrementShareCount (Pfn2, PageFrameIndex);
        PointerPte -= 1;
        Count += 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack - PAGE_SIZE);
    } while (PointerPte >= LastPte);

    //
    // Just toss the pages that won't ever come back in.
    //

    while (PointerPte != EndOfStackPte) {
        if (PointerPte->u.Hard.Valid == 0) {
            break;
        }

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageFrameIndex);

        TempPte = KernelDemandZeroPte;

        TempPte.u.Soft.Protection = MM_NOACCESS;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        if (PteFlushList->Count != MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList->FlushVa[PteFlushList->Count] = BaseOfKernelStack;
            PteFlushList->Count += 1;
        }

        Count += 1;

        //
        // Return resident available for pages beyond the guaranteed portion
        // as an explicit call to grow the kernel stack will be needed to get
        // these pages back.
        //

        if (PointerPte < LowestLivePte) {
            ASSERT (Thread->LargeStack);
            ResAvailToReturn += 1;
        }

        PointerPte -= 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack - PAGE_SIZE);
    }

    if (ResAvailToReturn != 0) {
        MI_INCREMENT_RESIDENT_AVAILABLE (ResAvailToReturn,
                                         MM_RESAVAIL_FREE_OUTPAGE_STACK);
        ResAvailToReturn = 0;
    }

    ASSERT (PteFlushList->Count != 0);
    ASSERT (Count != 0);

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAddSizeT (&MmKernelStackResident, 0 - Count);

    return;
}

VOID
MmOutPageKernelStack (
    IN PKTHREAD Thread
    )

/*++

Routine Description:

    This routine makes the specified kernel stack non-resident and
    puts the pages on the transition list.  Note that pages below
    the CurrentStackPointer are not useful and these pages are freed here.

Arguments:

    Thread - Supplies a pointer to the thread whose stack should be removed.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    MMPTE_FLUSH_LIST PteFlushList;

    ASSERT (KERNEL_LARGE_STACK_SIZE >= MI_LARGE_STACK_SIZE);

    ASSERT (((PCHAR)Thread->StackBase - (PCHAR)Thread->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    if (NtGlobalFlag & FLG_DISABLE_PAGE_KERNEL_STACKS) {
        return;
    }

    PteFlushList.Count = 0;
    SATISFY_OVERZEALOUS_COMPILER (PteFlushList.FlushVa[0] = NULL);

#if defined (_MI_CHAINED_STACKS)

    {

    PKERNEL_STACK_SEGMENT StackInfo;
    PKERNEL_STACK_SEGMENT NextStackInfo;

    //
    // Note ActualLimit is only set in Current, and InitialStack in Previous.
    //
    // StackLimit is the lowest committed address.
    //
    // KernelStack is the lowest referenced virtual address.
    //
    // StackBase is the highest virtual address.
    // ActualLimit is the lowest possible virtual address.
    //

    StackInfo = KeGetFirstKernelStackSegment (Thread);

    ASSERT (StackInfo->StackLimit == (ULONG_PTR) Thread->StackLimit);

    ASSERT (StackInfo->StackBase == (ULONG_PTR) Thread->StackBase);     // hi VA
    ASSERT (StackInfo->KernelStack == (ULONG_PTR) Thread->KernelStack);


    ASSERT (StackInfo->StackLimit < StackInfo->StackBase);
    ASSERT (StackInfo->StackLimit >= StackInfo->ActualLimit);

    ASSERT (StackInfo->ActualLimit < StackInfo->StackBase);
    ASSERT (StackInfo->ActualLimit >= StackInfo->StackBase - MI_LARGE_STACK_SIZE);

    ASSERT (StackInfo->KernelStack < StackInfo->StackBase);
    ASSERT (StackInfo->KernelStack >= StackInfo->StackLimit);

    do {

        NextStackInfo = KeGetNextKernelStackSegment (StackInfo);

        MiOutPageSingleKernelStack (Thread, StackInfo, &PteFlushList);

        StackInfo = NextStackInfo;

    } while (StackInfo != NULL);

    }

#else

    {

    KERNEL_STACK_SEGMENT LocalStackInfo;

    LocalStackInfo.StackBase = (ULONG_PTR) Thread->StackBase;
    LocalStackInfo.StackLimit = (ULONG_PTR) Thread->StackLimit;
    LocalStackInfo.KernelStack = (ULONG_PTR) Thread->KernelStack;

    if (Thread->LargeStack) {
        LocalStackInfo.ActualLimit = LocalStackInfo.StackBase - MI_LARGE_STACK_SIZE;
    }
    else {
        LocalStackInfo.ActualLimit = LocalStackInfo.StackBase - KERNEL_STACK_SIZE;
    }

    MiOutPageSingleKernelStack (Thread, &LocalStackInfo, &PteFlushList);

    }

#endif

    ASSERT (PteFlushList.Count != 0);

    if (PteFlushList.Count == 1) {
        MI_FLUSH_SINGLE_TB (PteFlushList.FlushVa[0], TRUE);
    }
    else if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (PteFlushList.Count,
                              &PteFlushList.FlushVa[0],
                              TRUE);
    }
    else {
        MI_FLUSH_ENTIRE_TB (0x24);
    }

    return;
}

VOID
MiInPageSingleKernelStack (
    IN PKTHREAD Thread,
    IN PKERNEL_STACK_SEGMENT StackInfo
    )

/*++

Routine Description:

    This routine makes the specified kernel stack resident.

Arguments:

    Thread - Supplies a pointer to the thread whose stack should be
             made resident.

    StackInfo - Supplies a pointer to the relevant stack information.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID DiscardExcess;
    PFN_NUMBER NumberOfPages;
    PVOID BaseOfKernelStack;
    PMMPTE PointerPte;
    PMMPTE EndOfStackPte;
    PMMPTE SignaturePte;
    ULONG DiskRead;
    PFN_NUMBER ContainingPage;
    KIRQL OldIrql;

    //
    // The first page of the stack is the page before the base of the stack.
    //

    if (MI_STACK_IS_TRIMMABLE (Thread)) {

        DiscardExcess = (PVOID) (ROUND_TO_PAGES (StackInfo->InitialStack) - KERNEL_LARGE_STACK_COMMIT);

        EndOfStackPte = MiGetPteAddress (DiscardExcess);

        PointerPte = MiGetPteAddress ((PVOID)StackInfo->StackLimit);

        //
        // Trim back the stack.  Make sure that the stack does not
        // grow, i.e.  StackLimit remains the limit.
        //

        if (EndOfStackPte < PointerPte) {
            EndOfStackPte = PointerPte;
        }

        DiscardExcess = MiGetVirtualAddressMappedByPte (EndOfStackPte);

        Thread->StackLimit = DiscardExcess;
        StackInfo->StackLimit = (ULONG_PTR) DiscardExcess;
    }
    else {
        EndOfStackPte = MiGetPteAddress ((PVOID) StackInfo->StackLimit);
    }

    BaseOfKernelStack = (PVOID) (StackInfo->StackBase - PAGE_SIZE);

    PointerPte = MiGetPteAddress (BaseOfKernelStack);

    DiskRead = 0;
    SignaturePte = MiGetPteAddress ((PULONG_PTR)StackInfo->KernelStack - 1);
    ASSERT (SignaturePte->u.Hard.Valid == 0);
    if ((SignaturePte->u.Long != MM_KERNEL_DEMAND_ZERO_PTE) &&
        (SignaturePte->u.Soft.Transition == 0)) {
            DiskRead = 1;
    }

    NumberOfPages = 0;

    LOCK_PFN (OldIrql);

    while (PointerPte >= EndOfStackPte) {

        if (!((PointerPte->u.Long == KernelDemandZeroPte.u.Long) ||
                (PointerPte->u.Soft.Protection == MM_NOACCESS))) {

            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x3451,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)Thread,
                          0);
        }
        ASSERT (PointerPte->u.Hard.Valid == 0);
        if (PointerPte->u.Soft.Protection == MM_NOACCESS) {
            PointerPte->u.Soft.Protection = PAGE_READWRITE;
        }

        ContainingPage = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress (PointerPte));
        MiMakeOutswappedPageResident (PointerPte,
                                      PointerPte,
                                      1,
                                      ContainingPage,
                                      OldIrql);

        PointerPte -= 1;
        NumberOfPages += 1;
    }

    //
    // Check the signature at the current stack location - sizeof (ULONG_PTR).
    //

    if (*((PULONG_PTR)StackInfo->KernelStack - 1) != (ULONG_PTR)Thread) {
        KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                      DiskRead,
                      *((PULONG_PTR)StackInfo->KernelStack - 1),
                      0,
                      (ULONG_PTR)StackInfo->KernelStack);
    }

    UNLOCK_PFN (OldIrql);

    InterlockedExchangeAddSizeT (&MmKernelStackResident, NumberOfPages);

    return;
}


VOID
MmInPageKernelStack (
    IN PKTHREAD Thread
    )

/*++

Routine Description:

    This routine makes the specified kernel stack resident.

Arguments:

    Supplies a pointer to the base of the kernel stack.

Return Value:

    Thread - Supplies a pointer to the thread whose stack should be
             made resident.

Environment:

    Kernel mode.

--*/

{
    PKERNEL_STACK_SEGMENT StackInfo;
    KERNEL_STACK_SEGMENT LocalStackInfo;

    ASSERT (((PCHAR)Thread->StackBase - (PCHAR)Thread->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    if (NtGlobalFlag & FLG_DISABLE_PAGE_KERNEL_STACKS) {
        return;
    }

    //
    // Construct a local StackInfo structure and make the pertinent
    // stack resident so that any others can be walked to.
    //
    // Note ActualLimit is only set in Current, and InitialStack in Previous.
    //
    // StackLimit is the lowest committed address.
    //
    // KernelStack is the lowest referenced virtual address.
    //
    // StackBase is the highest virtual address.
    //
    // ActualLimit is the lowest possible virtual address but this field
    // in the local StackInfo is not filled in here.
    //
    // DO NOT INITIALIZE or REFERENCE the StackInfo->ActualLimit field !!!
    //
    // The StackInfo->ActualLimit is not replicated in the KTHREAD anywhere.
    // And the StackInfo itself is not resident.  So this field cannot be
    // referenced until *AFTER* the stack has been made resident.
    //

    LocalStackInfo.StackBase = (ULONG_PTR) Thread->StackBase;
    LocalStackInfo.StackLimit = (ULONG_PTR) Thread->StackLimit;
    LocalStackInfo.KernelStack = (ULONG_PTR) Thread->KernelStack;
    LocalStackInfo.InitialStack = (ULONG_PTR) Thread->InitialStack;

    StackInfo = &LocalStackInfo;

    MiInPageSingleKernelStack (Thread, StackInfo);

#if defined (_MI_CHAINED_STACKS)

    //
    // Note constructing the local StackInfo (instead of calling
    // KeGetFirstKernelStackSegment) was necessary because the thread's
    // initial stack VA is not resident on entry to this function.
    //

    StackInfo = KeGetFirstKernelStackSegment (Thread);

    ASSERT (StackInfo == &((PKERNEL_STACK_CONTROL) (LocalStackInfo.InitialStack))->Current);
    ASSERT (StackInfo->StackBase == LocalStackInfo.StackBase);
    ASSERT (StackInfo->StackLimit == LocalStackInfo.StackLimit);
    ASSERT (StackInfo->KernelStack == LocalStackInfo.KernelStack);

    do {

        PKERNEL_STACK_SEGMENT PreviousStackInfo;

        //
        // KeGetNextKernelStackSegment (StackInfo) cannot be used because
        // it reaches into the next segment which is currently paged out.
        //

        PreviousStackInfo = StackInfo + 1;

        if (PreviousStackInfo->StackBase == 0) {
            break;
        }

        LocalStackInfo = *PreviousStackInfo;

        MiInPageSingleKernelStack (Thread, &LocalStackInfo);

        StackInfo = &((PKERNEL_STACK_CONTROL) LocalStackInfo.InitialStack)->Current;

    } while (TRUE);

#endif

    return;
}

#if (_MI_PAGING_LEVELS >= 3)

PFN_NUMBER
MiTrimPageParents (
    IN PEPROCESS OutProcess,
    IN PFN_NUMBER PdePage,
    IN PMMPTE OutTempPte
    )
{
    PFN_NUMBER TopPage;
    KIRQL OldIrql;
    MMPTE TempPte;
    MMPTE TempPte2;
    PMMPTE MappingPte;
    PMMPTE PageDirectoryMap;
    PEPROCESS CurrentProcess;
    PMMPFN Pfn1;
    PFN_NUMBER PpePage;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPFN Pfn2;
    PFN_NUMBER PxePage;
#endif

    //
    // OutProcess really is referenced on checked or non-AMD builds, but it's
    // not worth ifdefing this just for the compiler.
    //

    UNREFERENCED_PARAMETER (OutProcess);

    OldIrql = MM_NOIRQL;
    Pfn1 = MI_PFN_ELEMENT (PdePage);
    CurrentProcess = PsGetCurrentProcess ();

    //
    // Remove the page directory page.
    //

    PpePage = Pfn1->u4.PteFrame;
    ASSERT (PpePage != 0);

#if (_MI_PAGING_LEVELS < 4)
    ASSERT (PpePage == MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&(OutProcess->Pcb.DirectoryTableBase[0]))));
#endif

    PageDirectoryMap = NULL;

    MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MappingPte > (PMMPTE)1) {

        MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                  PpePage,
                                  MM_READWRITE,
                                  MappingPte);

        MI_SET_PTE_DIRTY (TempPte2);

        MI_WRITE_VALID_PTE (MappingPte, TempPte2);

        PageDirectoryMap = MiGetVirtualAddressMappedByPte (MappingPte);
    }
    else {
        if (MappingPte == NULL) {
            PageDirectoryMap = MiMapPageInHyperSpace (CurrentProcess, PpePage, &OldIrql);
        }
    }

    TempPte = PageDirectoryMap[MiGetPpeOffset(MmWorkingSetList)];

    ASSERT (TempPte.u.Hard.Valid == 1);
    ASSERT (TempPte.u.Hard.PageFrameNumber == PdePage);

    MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

    PageDirectoryMap[MiGetPpeOffset(MmWorkingSetList)] = TempPte;

    ASSERT (Pfn1->u3.e1.Modified == 1);

#if (_MI_PAGING_LEVELS < 4)

    //
    // Remove the top level page directory parent page.
    //

    TempPte = PageDirectoryMap[MiGetPpeOffset(PDE_TBASE)];

    MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                  MM_READWRITE);

    PageDirectoryMap[MiGetPpeOffset(PDE_TBASE)] = TempPte;

    TopPage = PpePage;

#endif

    if (MappingPte == (PMMPTE)1) {

        //
        // Nothing needs to be done if a KSEG3 mapping was used ...
        //

        NOTHING;
    }
    else if (MappingPte != NULL) {
        MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
    }
    else {
        MiUnmapPageInHyperSpace (CurrentProcess, PageDirectoryMap, OldIrql);
    }

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Remove the page directory parent page.  Then remove
    // the top level extended page directory parent page.
    //

    Pfn2 = MI_PFN_ELEMENT (PpePage);
    PxePage = Pfn2->u4.PteFrame;
    ASSERT (PxePage);
    ASSERT (PxePage == MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&(OutProcess->Pcb.DirectoryTableBase[0]))));

    MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MappingPte != NULL) {

        MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                  PxePage,
                                  MM_READWRITE,
                                  MappingPte);

        MI_SET_PTE_DIRTY (TempPte2);

        MI_WRITE_VALID_PTE (MappingPte, TempPte2);

        PageDirectoryMap = MiGetVirtualAddressMappedByPte (MappingPte);
    }
    else {
        PageDirectoryMap = MiMapPageInHyperSpace (CurrentProcess, PxePage, &OldIrql);
    }

    TempPte = PageDirectoryMap[MiGetPxeOffset(MmWorkingSetList)];

    ASSERT (TempPte.u.Hard.Valid == 1);
    ASSERT (TempPte.u.Hard.PageFrameNumber == PpePage);

    MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

    PageDirectoryMap[MiGetPxeOffset(MmWorkingSetList)] = TempPte;

    ASSERT (MI_PFN_ELEMENT(PpePage)->u3.e1.Modified == 1);

    TempPte = PageDirectoryMap[MiGetPxeOffset(PXE_BASE)];

    MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

    PageDirectoryMap[MiGetPxeOffset(PXE_BASE)] = TempPte;

    if (MappingPte != NULL) {
        MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
    }
    else {
        MiUnmapPageInHyperSpace (CurrentProcess, PageDirectoryMap, OldIrql);
    }

    TopPage = PxePage;

#endif

    LOCK_PFN (OldIrql);
    MiDecrementShareCount (Pfn1, PdePage);
#if (_MI_PAGING_LEVELS >= 4)
    MiDecrementShareCount (Pfn2, PpePage);
#endif
    UNLOCK_PFN (OldIrql);

    *OutTempPte = TempPte;

    return TopPage;
}

#endif


VOID
MmOutSwapProcess (
    IN PKPROCESS Process
    )

/*++

Routine Description:

    This routine out swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process to swap out of memory.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PEPROCESS OutProcess;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPFN Pfn3;
    PFN_NUMBER HyperSpacePageTable;
    PMMPTE HyperSpacePageTableMap;
    PFN_NUMBER PdePage;
    PFN_NUMBER ProcessPage;
    MMPTE TempPte;
    MMPTE TempPte2;
    MMPTE TempPte3;
    PMMPTE MappingPte;
    PMMPTE PageDirectoryMap;
    PFN_NUMBER VadBitMapPage;
    PEPROCESS CurrentProcess;
#if defined (_X86PAE_)
    PMMPFN Pfn5[PD_PER_SYSTEM];
    ULONG i;
    PFN_NUMBER PdePage2;
    PFN_NUMBER PageDirectories[PD_PER_SYSTEM];
    PPAE_ENTRY PaeVa;
#endif

    OutProcess = CONTAINING_RECORD (Process, EPROCESS, Pcb);

    PS_SET_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAP_ENABLED);

#if DBG
    if ((MmDebug & MM_DBG_SWAP_PROCESS) != 0) {
        return;
    }
#endif

    if (OutProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        MiSessionOutSwapProcess (OutProcess);
    }

    //
    // The process cannot be fully swapped out unless its working set has
    // been reduced to the bare minimum.  If the working set is larger than
    // this, then just return as the working set manager thread will trim it
    // down.  Eventually when it is trimmed to the bare minimum, the working
    // set manager thread will detach from the process and if it is the last
    // thread (ie: all the others are in usermode waits), then another process
    // outswap request will occur for this process at which time we will be
    // able to proceed below.
    //

    if (OutProcess->Vm.WorkingSetSize == MM_PROCESS_COMMIT_CHARGE) {

        LOCK_EXPANSION (OldIrql);

        ASSERT (OutProcess->Outswapped == 0);

        if (OutProcess->Vm.WorkingSetExpansionLinks.Flink == MM_WS_TRIMMING) {

            //
            // An outswap is not allowed at this point because the process
            // has been attached to and is being trimmed.
            //

            UNLOCK_EXPANSION (OldIrql);
            return;
        }

        //
        // Swap the process working set info and page parent/directory/table
        // pages from memory.
        //

        PS_SET_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAPPED);

        UNLOCK_EXPANSION (OldIrql);

        CurrentProcess = PsGetCurrentProcess ();

        //
        // Remove the working set list page from the process.
        //
        // Note the PFN lock does not need to be held to put the PTEs for
        // these pages into transition because no one can be referencing
        // these addresses in the context of the process being outswapped.
        //

        HyperSpacePageTable = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (OutProcess);
        HyperSpacePageTableMap = NULL;

        MappingPte = MiReserveSystemPtes (1, SystemPteSpace);
    
        if (MappingPte > (PMMPTE) 1) {
    
            MI_MAKE_VALID_KERNEL_PTE (TempPte3,
                                      HyperSpacePageTable,
                                      MM_READWRITE,
                                      MappingPte);
    
            MI_SET_PTE_DIRTY (TempPte3);
    
            MI_WRITE_VALID_PTE (MappingPte, TempPte3);
    
            HyperSpacePageTableMap = MiGetVirtualAddressMappedByPte (MappingPte);
        }
        else if (MappingPte == NULL) {
            HyperSpacePageTableMap = MiMapPageInHyperSpace (CurrentProcess,
                                                            HyperSpacePageTable,
                                                            &OldIrql);
        }

        //
        // Put the working set list table PTE into transition.
        //

        TempPte = HyperSpacePageTableMap[MiGetPteOffset(MmWorkingSetList)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        HyperSpacePageTableMap[MiGetPteOffset(MmWorkingSetList)] = TempPte;

        //
        // Put the VAD bitmap PTE into transition.
        //

        PointerPte = &HyperSpacePageTableMap[MiGetPteOffset (VAD_BITMAP_SPACE)];
        TempPte2 = *PointerPte;

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte2);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte2, MM_READWRITE);

        MI_WRITE_INVALID_PTE (PointerPte, TempPte2);

        if (MappingPte == (PMMPTE)1) {

            //
            // Nothing needs to be done if a KSEG3 mapping was used ...
            //

            NOTHING;
        }
        else if (MappingPte != NULL) {
            MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
        }
        else {
            MiUnmapPageInHyperSpace (CurrentProcess, HyperSpacePageTableMap, OldIrql);
        }

        //
        // Capture all the PFN information and then get the PFN lock and
        // actually move all the pages onto the modified list.
        //

        Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);
        Pfn2 = MI_PFN_ELEMENT (OutProcess->WorkingSetPage);

        Pfn3 = MI_PFN_ELEMENT (HyperSpacePageTable);
        ASSERT (Pfn3->u3.e1.Modified == 1);
        PdePage = Pfn3->u4.PteFrame;
        ASSERT (PdePage != 0);

        //
        // Remove the hyper space page table from the process.
        //

        PageDirectoryMap = NULL;

        MappingPte = MiReserveSystemPtes (1, SystemPteSpace);
    
        if (MappingPte > (PMMPTE) 1) {
    
            MI_MAKE_VALID_KERNEL_PTE (TempPte3,
                                      PdePage,
                                      MM_READWRITE,
                                      MappingPte);
    
            MI_SET_PTE_DIRTY (TempPte3);
    
            MI_WRITE_VALID_PTE (MappingPte, TempPte3);
    
            PageDirectoryMap = MiGetVirtualAddressMappedByPte (MappingPte);
        }
        else {
            if (MappingPte == NULL) {
                PageDirectoryMap = MiMapPageInHyperSpace (CurrentProcess, PdePage, &OldIrql);
            }
        }

        TempPte = PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)];

        ASSERT (TempPte.u.Hard.Valid == 1);
        ASSERT (TempPte.u.Hard.PageFrameNumber == HyperSpacePageTable);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)] = TempPte;

#if defined (_X86PAE_)

        //
        // Remove the additional page directory pages.
        //

        PaeVa = (PPAE_ENTRY)OutProcess->PaeTop;
        ASSERT (PaeVa != &MiSystemPaeVa);

        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {

            TempPte = PageDirectoryMap[i];
            PdePage2 = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte);

            MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

            PageDirectoryMap[i] = TempPte;
            ASSERT (MI_PFN_ELEMENT (PdePage2)->u3.e1.Modified == 1);

            PageDirectories[i] = PdePage2;
            PaeVa->PteEntry[i].u.Long = TempPte.u.Long;

            Pfn5[i] = MI_PFN_ELEMENT (PdePage2);
        }

#if DBG
        TempPte = PageDirectoryMap[i];
        PdePage2 = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
        ASSERT ((MI_PFN_ELEMENT (PdePage2))->u3.e1.Modified == 1);
#endif

#endif

#if (_MI_PAGING_LEVELS < 3)

        //
        // Remove the top level page directory page.
        //

        TempPte = PageDirectoryMap[MiGetPdeOffset(PDE_BASE)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPdeOffset(PDE_BASE)] = TempPte;

#endif

        if (MappingPte == (PMMPTE)1) {

            //
            // Nothing needs to be done if a KSEG3 mapping was used ...
            //

            NOTHING;
        }
        else if (MappingPte != NULL) {
            MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
        }
        else {
            MiUnmapPageInHyperSpace (CurrentProcess, PageDirectoryMap, OldIrql);
        }

        //
        // Now acquire the PFN lock and free all the pages whose PTEs we
        // put in transition above.
        //

        LOCK_PFN (OldIrql);

        ASSERT (Pfn1->u3.e1.Modified == 1);
        MiDecrementShareCount (Pfn1, VadBitMapPage);

        ASSERT (Pfn2->u3.e1.Modified == 1);
        MiDecrementShareCount (Pfn2, OutProcess->WorkingSetPage);

        ASSERT (Pfn3->u3.e1.Modified == 1);
        MiDecrementShareCount (Pfn3, HyperSpacePageTable);

#if defined (_X86PAE_)
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            MiDecrementShareCount (Pfn5[i], PageDirectories[i]);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        UNLOCK_PFN (OldIrql);
        PdePage = MiTrimPageParents (OutProcess, PdePage, &TempPte);
        LOCK_PFN (OldIrql);
#endif

        Pfn1 = MI_PFN_ELEMENT (PdePage);

        //
        // Decrement share count so the top level page directory page gets
        // removed.  This can cause the PteCount to equal the sharecount as the
        // page directory page no longer contains itself, yet can have
        // itself as a transition page.
        //

        Pfn1->u2.ShareCount -= 2;
        Pfn1->PteAddress = (PMMPTE) &OutProcess->PageDirectoryPte;

        OutProcess->PageDirectoryPte = TempPte.u.Flush;

#if defined (_X86PAE_)
        PaeVa->PteEntry[i].u.Long = TempPte.u.Long;
#endif

        if (MI_IS_PHYSICAL_ADDRESS(OutProcess)) {
            ProcessPage = MI_CONVERT_PHYSICAL_TO_PFN (OutProcess);
        }
        else {
            PointerPte = MiGetPteAddress (OutProcess);
            ProcessPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        }

        Pfn1->u4.PteFrame = ProcessPage;
        Pfn1 = MI_PFN_ELEMENT (ProcessPage);

        //
        // Increment the share count for the process page.
        //

        Pfn1->u2.ShareCount += 1;

        UNLOCK_PFN (OldIrql);

        LOCK_EXPANSION (OldIrql);

        if (OutProcess->Vm.WorkingSetExpansionLinks.Flink > MM_WS_TRIMMING) {

            //
            // The entry must be on the list.
            //

            RemoveEntryList (&OutProcess->Vm.WorkingSetExpansionLinks);
            OutProcess->Vm.WorkingSetExpansionLinks.Flink = MM_WS_SWAPPED_OUT;
        }

        UNLOCK_EXPANSION (OldIrql);

        OutProcess->WorkingSetPage = 0;
        OutProcess->Vm.WorkingSetSize = 0;
    }

    return;
}

VOID
MmInSwapProcess (
    IN PKPROCESS Process
    )

/*++

Routine Description:

    This routine in swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process that is to be swapped
              into memory.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PEPROCESS OutProcess;
    PEPROCESS CurrentProcess;
    PFN_NUMBER PdePage;
    PMMPTE PageDirectoryMap;
    MMPTE VadBitMapPteContents;
    PFN_NUMBER VadBitMapPage;
    ULONG WorkingSetListPteOffset;
    ULONG VadBitMapPteOffset;
    PMMPTE WorkingSetListPte;
    PMMPTE VadBitMapPte;
    MMPTE TempPte;
    PFN_NUMBER HyperSpacePageTable;
    PMMPTE HyperSpacePageTableMap;
    PFN_NUMBER WorkingSetPage;
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PFN_NUMBER ProcessPage;
#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER TopPage;
    PFN_NUMBER PageDirectoryPage;
    PMMPTE PageDirectoryParentMap;
#endif
#if defined (_X86PAE_)
    ULONG i;
    PPAE_ENTRY PaeVa;
    MMPTE PageDirectoryPtes[PD_PER_SYSTEM];
#endif

    CurrentProcess = PsGetCurrentProcess ();

    OutProcess = CONTAINING_RECORD (Process, EPROCESS, Pcb);

    if (OutProcess->Flags & PS_PROCESS_FLAGS_OUTSWAPPED) {

        //
        // The process is out of memory, rebuild the initialized page
        // structure.
        //

        if (MI_IS_PHYSICAL_ADDRESS(OutProcess)) {
            ProcessPage = MI_CONVERT_PHYSICAL_TO_PFN (OutProcess);
        }
        else {
            PointerPte = MiGetPteAddress (OutProcess);
            ProcessPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        }

        WorkingSetListPteOffset = MiGetPteOffset (MmWorkingSetList);
        VadBitMapPteOffset = MiGetPteOffset (VAD_BITMAP_SPACE);

        WorkingSetListPte = MiGetPteAddress (MmWorkingSetList);
        VadBitMapPte = MiGetPteAddress (VAD_BITMAP_SPACE);

        LOCK_PFN (OldIrql);

        PdePage = MiMakeOutswappedPageResident (
#if (_MI_PAGING_LEVELS >= 4)
                                        MiGetPteAddress (PXE_BASE),
#elif (_MI_PAGING_LEVELS >= 3)
                                        MiGetPteAddress ((PVOID)PDE_TBASE),
#else
                                        MiGetPteAddress (PDE_BASE),
#endif
                                        (PMMPTE)&OutProcess->PageDirectoryPte,
                                        0,
                                        ProcessPage,
                                        OldIrql);

        //
        // Adjust the counts for the process page.
        //

        Pfn1 = MI_PFN_ELEMENT (ProcessPage);
        Pfn1->u2.ShareCount -= 1;

        ASSERT ((LONG)Pfn1->u2.ShareCount >= 1);

#if (_MI_PAGING_LEVELS >= 3)
        TopPage = PdePage;
#endif

        //
        // Adjust the counts properly for the page directory page.
        //

        Pfn1 = MI_PFN_ELEMENT (PdePage);
        Pfn1->u2.ShareCount += 1;
        Pfn1->u1.Event = (PVOID)OutProcess;
        Pfn1->u4.PteFrame = PdePage;

#if (_MI_PAGING_LEVELS >= 4)
        Pfn1->PteAddress = MiGetPteAddress (PXE_BASE);
#elif (_MI_PAGING_LEVELS >= 3)
        Pfn1->PteAddress = MiGetPteAddress ((PVOID)PDE_TBASE);
#else
        Pfn1->PteAddress = MiGetPteAddress (PDE_BASE);
#endif

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Only the extended page directory parent page has really been
        // read in above.  Read in the page directory parent page now.
        //

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryParentMap[MiGetPxeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PageDirectoryPage = MiMakeOutswappedPageResident (
                                 MiGetPxeAddress (MmWorkingSetList),
                                 &TempPte,
                                 0,
                                 PdePage,
                                 OldIrql);

        ASSERT (PageDirectoryPage == TempPte.u.Hard.PageFrameNumber);
        ASSERT (Pfn1->u2.ShareCount >= 3);

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        PageDirectoryParentMap[MiGetPxeOffset(PXE_BASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
        PageDirectoryParentMap[MiGetPxeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PdePage = PageDirectoryPage;

#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Only the page directory parent page has really been read in above
        // (and the extended page directory parent for 4-level architectures).
        // Read in the page directory page now.
        //

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryParentMap[MiGetPpeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PageDirectoryPage = MiMakeOutswappedPageResident (
                                 MiGetPpeAddress (MmWorkingSetList),
                                 &TempPte,
                                 0,
                                 PdePage,
                                 OldIrql);

        ASSERT (PageDirectoryPage == TempPte.u.Hard.PageFrameNumber);

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

#if (_MI_PAGING_LEVELS==3)
        ASSERT (Pfn1->u2.ShareCount >= 3);
        PageDirectoryParentMap[MiGetPpeOffset(PDE_TBASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
#endif

        PageDirectoryParentMap[MiGetPpeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PdePage = PageDirectoryPage;

#endif

#if defined (_X86PAE_)

        //
        // Locate the additional page directory pages and make them resident.
        //

        PaeVa = (PPAE_ENTRY)OutProcess->PaeTop;
        ASSERT (PaeVa != &MiSystemPaeVa);

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageDirectoryPtes[i] = PageDirectoryMap[i];
        }
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            MiMakeOutswappedPageResident (
                                 MiGetPteAddress (PDE_BASE + (i << PAGE_SHIFT)),
                                 &PageDirectoryPtes[i],
                                 0,
                                 PdePage,
                                 OldIrql);
            PageDirectoryPtes[i].u.Long &= ~MmPaeMask;
            PaeVa->PteEntry[i].u.Long = (PageDirectoryPtes[i].u.Long & ~MM_PAE_PDPTE_MASK);
        }

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageDirectoryMap[i] = PageDirectoryPtes[i];
        }
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        TempPte.u.Flush = OutProcess->PageDirectoryPte;
        TempPte.u.Long &= ~MM_PAE_PDPTE_MASK;
        TempPte.u.Long &= ~MmPaeMask;
        PaeVa->PteEntry[i].u.Flush = TempPte.u.Flush;

#endif

        //
        // Locate the page table page for hyperspace and make it resident.
        //

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        HyperSpacePageTable = MiMakeOutswappedPageResident (
                                 MiGetPdeAddress (HYPER_SPACE),
                                 &TempPte,
                                 0,
                                 PdePage,
                                 OldIrql);

        ASSERT (Pfn1->u2.ShareCount >= 3);

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

#if (_MI_PAGING_LEVELS==2)
        PageDirectoryMap[MiGetPdeOffset(PDE_BASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
#endif

        PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        //
        // Map in the hyper space page table page and retrieve the
        // PTEs that map the working set list and VAD bitmap.  Note that
        // although both PTEs lie in the same page table page, they must
        // be retrieved separately because: the Vad PTE may indicate its page
        // is in a paging file and the WSL PTE may indicate its PTE is in
        // transition.  The VAD page inswap may take the WSL page from
        // the transition list - CHANGING the WSL PTE !  So the WSL PTE cannot
        // be captured until after the VAD inswap completes.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        VadBitMapPteContents = HyperSpacePageTableMap[VadBitMapPteOffset];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        Pfn1 = MI_PFN_ELEMENT (HyperSpacePageTable);
        Pfn1->u1.WsIndex = 1;

        //
        // Read in the VAD bitmap page.
        //

        VadBitMapPage = MiMakeOutswappedPageResident (VadBitMapPte,
                                                      &VadBitMapPteContents,
                                                      0,
                                                      HyperSpacePageTable,
                                                      OldIrql);

        //
        // Read in the working set list page.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        TempPte = HyperSpacePageTableMap[WorkingSetListPteOffset];
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        WorkingSetPage = MiMakeOutswappedPageResident (WorkingSetListPte,
                                                       &TempPte,
                                                       0,
                                                       HyperSpacePageTable,
                                                       OldIrql);

        //
        // Update the PTEs, this can be done together for PTEs that lie within
        // the same page table page.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        HyperSpacePageTableMap[WorkingSetListPteOffset] = TempPte;

        HyperSpacePageTableMap[VadBitMapPteOffset] = VadBitMapPteContents;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        Pfn1 = MI_PFN_ELEMENT (WorkingSetPage);
        Pfn1->u1.WsIndex = 3;

        Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);
        Pfn1->u1.WsIndex = 2;

        UNLOCK_PFN (OldIrql);

        //
        // Set up process structures.
        //

#if (_MI_PAGING_LEVELS >= 3)
        PdePage = TopPage;
#endif

        OutProcess->WorkingSetPage = WorkingSetPage;

        OutProcess->Vm.WorkingSetSize = MM_PROCESS_COMMIT_CHARGE;

#if !defined (_X86PAE_)

        INITIALIZE_DIRECTORY_TABLE_BASE (&Process->DirectoryTableBase[0],
                                         PdePage);
        INITIALIZE_DIRECTORY_TABLE_BASE (&Process->DirectoryTableBase[1],
                                         HyperSpacePageTable);
#else
        //
        // The DirectoryTableBase[0] never changes for PAE processes.
        //

        Process->DirectoryTableBase[1] = HyperSpacePageTable;
#endif

        LOCK_EXPANSION (OldIrql);

        //
        // Allow working set trimming on this process.
        //

        if (OutProcess->Vm.WorkingSetExpansionLinks.Flink == MM_WS_SWAPPED_OUT) {
            InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                            &OutProcess->Vm.WorkingSetExpansionLinks);
        }

        PS_CLEAR_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAPPED);

#if !defined(_WIN64)

        if (OutProcess->PdeUpdateNeeded) {

            //
            // Another thread updated the system PDE range while this process
            // was outswapped.  Update the PDEs now.
            //

            PS_CLEAR_BITS (&OutProcess->Flags,
                           PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);

            MiUpdateSystemPdes (OutProcess);
        }

#endif

        UNLOCK_EXPANSION (OldIrql);
    }

    if (OutProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        MiSessionInSwapProcess (OutProcess, FALSE);
    }

    PS_CLEAR_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAP_ENABLED);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        PERFINFO_SWAPPROCESS_INFORMATION PerfInfoSwapProcess;
        PerfInfoSwapProcess.ProcessId = HandleToUlong((OutProcess)->UniqueProcessId);
        PerfInfoSwapProcess.PageDirectoryBase = MmGetDirectoryFrameFromProcess(OutProcess);
        PerfInfoLogBytes (PERFINFO_LOG_TYPE_INSWAPPROCESS,
                          &PerfInfoSwapProcess,
                          sizeof(PerfInfoSwapProcess));
    }
    return;
}

NTSTATUS
MiCreatePebOrTeb (
    IN PEPROCESS TargetProcess,
    IN ULONG Size,
    OUT PVOID *Base
    )

/*++

Routine Description:

    This routine creates a TEB or PEB page within the target process.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    the structure.

    Size - Supplies the size of the structure to create a VAD for.

    Base - Supplies a pointer to place the PEB/TEB virtual address on success.
           This has no meaning if success is not returned.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, attached to the specified process.

--*/

{
    PMMVAD_LONG Vad;
    PETHREAD Thread;
    NTSTATUS Status;

    Thread = PsGetCurrentThread ();

    //
    // Allocate and initialize the Vad before acquiring the address space
    // and working set mutexes so as to minimize mutex hold duration.
    //

    Vad = (PMMVAD_LONG) ExAllocatePoolWithTag (NonPagedPool,
                                               sizeof(MMVAD_LONG),
                                               'ldaV');

    if (Vad == NULL) {
        return STATUS_NO_MEMORY;
    }

    Vad->u.LongFlags = 0;

    Vad->u.VadFlags.CommitCharge = BYTES_TO_PAGES (Size);
    Vad->u.VadFlags.MemCommit = 1;
    Vad->u.VadFlags.PrivateMemory = 1;
    Vad->u.VadFlags.Protection = MM_READWRITE;

    //
    // Mark VAD as not deletable, no protection change.
    //

    Vad->u.VadFlags.NoChange = 1;
    Vad->u2.LongFlags2 = 0;
    Vad->u2.VadFlags2.OneSecured = 1;
    Vad->u2.VadFlags2.LongVad = 1;
    Vad->u2.VadFlags2.ReadOnly = 0;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);


    //
    // Find a VA for the PEB on a page-size boundary.
    //

    if (Size == sizeof(PEB)
#if defined(_WIN64)
        || (Size == sizeof(PEB32))
#endif
    ) {
        PVOID HighestVadAddress;

        LARGE_INTEGER CurrentTime;

        HighestVadAddress = (PVOID) ((PCHAR)MM_HIGHEST_VAD_ADDRESS + 1);

        KeQueryTickCount (&CurrentTime);

        CurrentTime.LowPart &= ((X64K >> PAGE_SHIFT) - 1);
        if (CurrentTime.LowPart <= 1) {
            CurrentTime.LowPart = 2;
        }

        //
        // Select a varying PEB address without fragmenting the address space.
        //

        HighestVadAddress = (PVOID) ((PCHAR)HighestVadAddress - (CurrentTime.LowPart << PAGE_SHIFT));

        if (MiCheckForConflictingVadExistence (TargetProcess, HighestVadAddress, (PVOID) ((PCHAR) HighestVadAddress + ROUND_TO_PAGES (Size) - 1)) == FALSE) {

            //
            // Got an address ...
            //
    
            *Base = HighestVadAddress;
            goto AllocatedAddress;
        }
    }


    Status = MiFindEmptyAddressRangeDown (&TargetProcess->VadRoot,
                                          ROUND_TO_PAGES (Size),
                                          ((PCHAR)MM_HIGHEST_VAD_ADDRESS + 1),
                                          PAGE_SIZE,
                                          Base);

    if (!NT_SUCCESS(Status)) {

        //
        // No range was available, deallocate the Vad and return the status.
        //

        UNLOCK_ADDRESS_SPACE (TargetProcess);
        ExFreePool (Vad);
        return Status;
    }

AllocatedAddress:

    //
    // An unoccupied address range has been found, finish initializing the
    // virtual address descriptor to describe this range.
    //

    Vad->StartingVpn = MI_VA_TO_VPN (*Base);
    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)*Base + Size - 1);

    Vad->u3.Secured.StartVpn = (ULONG_PTR)*Base;
    Vad->u3.Secured.EndVpn = (ULONG_PTR)MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

    Status = MiInsertVadCharges ((PMMVAD) Vad, TargetProcess);

    if (!NT_SUCCESS (Status)) {

        //
        // A failure has occurred.  Deallocate the Vad and return the status.
        //

        UNLOCK_ADDRESS_SPACE (TargetProcess);
        ExFreePool (Vad);

        return Status;
    }

    LOCK_WS_UNSAFE (Thread, TargetProcess);

    MiInsertVad ((PMMVAD) Vad, TargetProcess);

    UNLOCK_WS_UNSAFE (Thread, TargetProcess);

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    return Status;
}

NTSTATUS
MmCreateTeb (
    IN PEPROCESS TargetProcess,
    IN PINITIAL_TEB InitialTeb,
    IN PCLIENT_ID ClientId,
    OUT PTEB *Base
    )

/*++

Routine Description:

    This routine creates a TEB page within the target process
    and copies the initial TEB values into it.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the TEB.

    InitialTeb - Supplies a pointer to the initial TEB to copy into the
                 newly created TEB.

    ClientId - Supplies a client ID.

    Base - Supplies a location to return the base of the newly created
           TEB on success.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    PTEB TebBase;
    NTSTATUS Status;
    ULONG TebSize;
#if defined(_WIN64)
    PWOW64_PROCESS Wow64Process;
    PTEB32 Teb32Base = NULL;
    PINITIAL_TEB InitialTeb32Ptr = NULL;
    INITIAL_TEB InitialTeb32;
#endif

    //
    // Get the size of the TEB
    //

    TebSize = sizeof (TEB);

#if defined(_WIN64)
    Wow64Process = TargetProcess->Wow64Process;
    if (Wow64Process != NULL) {
        TebSize = ROUND_TO_PAGES (sizeof (TEB)) + sizeof (TEB32);

        //
        // Capture the 32-bit InitialTeb if the target thread's process is a Wow64 process,
        // and the creating the thread is running inside a Wow64 process as well.
        //

        if (PsGetCurrentProcess()->Wow64Process != NULL) {
            
            try {
                
                InitialTeb32Ptr = Wow64GetInitialTeb32 ();

                if (InitialTeb32Ptr != NULL) {
                
                    ProbeForReadSmallStructure (InitialTeb32Ptr,
                                                sizeof (InitialTeb32),
                                                PROBE_ALIGNMENT (INITIAL_TEB));
                
                    RtlCopyMemory (&InitialTeb32,
                                   InitialTeb32Ptr,
                                   sizeof (InitialTeb32));

                    InitialTeb32Ptr = &InitialTeb32;
                }
            } except (EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode ();
            }
        }
    }
#endif

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    Status = MiCreatePebOrTeb (TargetProcess, TebSize, (PVOID) &TebBase);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess();
        return Status;
    }

    //
    // Initialize the TEB.  Note accesses to the TEB can raise exceptions
    // if no address space is available for the TEB or the user has exceeded
    // quota (non-paged, pagefile, commit) or the TEB is paged out and an
    // inpage error occurs when fetching it.
    //

    //
    // Note that since the TEB is populated with demand zero pages, only
    // nonzero fields need to be initialized here.
    //

    try {

#if !defined(_WIN64)
        TebBase->NtTib.ExceptionList = EXCEPTION_CHAIN_END;
#endif

        //
        // Although various fields must be zero for the process to launch
        // properly, don't assert them as an ordinary user could provoke these
        // by maliciously writing over random addresses in another thread,
        // hoping to nail a just-being-created TEB.
        //

        DONTASSERT (TebBase->NtTib.SubSystemTib == NULL);
        TebBase->NtTib.Version = OS2_VERSION;
        DONTASSERT (TebBase->NtTib.ArbitraryUserPointer == NULL);
        TebBase->NtTib.Self = (PNT_TIB)TebBase;
        DONTASSERT (TebBase->EnvironmentPointer == NULL);
        TebBase->ProcessEnvironmentBlock = TargetProcess->Peb;
        TebBase->ClientId = *ClientId;
        TebBase->RealClientId = *ClientId;
        DONTASSERT (TebBase->ActivationContextStackPointer == NULL);

        if ((InitialTeb->OldInitialTeb.OldStackBase == NULL) &&
            (InitialTeb->OldInitialTeb.OldStackLimit == NULL)) {

            TebBase->NtTib.StackBase = InitialTeb->StackBase;
            TebBase->NtTib.StackLimit = InitialTeb->StackLimit;
            TebBase->DeallocationStack = InitialTeb->StackAllocationBase;
        }
        else {
            TebBase->NtTib.StackBase = InitialTeb->OldInitialTeb.OldStackBase;
            TebBase->NtTib.StackLimit = InitialTeb->OldInitialTeb.OldStackLimit;
        }

        TebBase->StaticUnicodeString.Buffer = TebBase->StaticUnicodeBuffer;
        TebBase->StaticUnicodeString.MaximumLength = (USHORT) sizeof (TebBase->StaticUnicodeBuffer);
        DONTASSERT (TebBase->StaticUnicodeString.Length == 0);

        //
        // Used for BBT of ntdll and kernel32.dll.
        //

        TebBase->ReservedForPerf = BBTBuffer;

#if defined(_WIN64)
        if (Wow64Process != NULL) {

            Teb32Base = (PTEB32)((PCHAR)TebBase + ROUND_TO_PAGES (sizeof(TEB)));

            Teb32Base->NtTib.ExceptionList = PtrToUlong (EXCEPTION_CHAIN_END);
            Teb32Base->NtTib.Version = TebBase->NtTib.Version;
            Teb32Base->NtTib.Self = PtrToUlong (Teb32Base);
            Teb32Base->ProcessEnvironmentBlock = PtrToUlong (Wow64Process->Wow64);
            Teb32Base->ClientId.UniqueProcess = PtrToUlong (TebBase->ClientId.UniqueProcess);
            Teb32Base->ClientId.UniqueThread = PtrToUlong (TebBase->ClientId.UniqueThread);
            Teb32Base->RealClientId.UniqueProcess = PtrToUlong (TebBase->RealClientId.UniqueProcess);
            Teb32Base->RealClientId.UniqueThread = PtrToUlong (TebBase->RealClientId.UniqueThread);
            Teb32Base->StaticUnicodeString.Buffer = PtrToUlong (Teb32Base->StaticUnicodeBuffer);
            Teb32Base->StaticUnicodeString.MaximumLength = (USHORT)sizeof (Teb32Base->StaticUnicodeBuffer);
            ASSERT (Teb32Base->StaticUnicodeString.Length == 0);
            Teb32Base->GdiBatchCount = PtrToUlong (TebBase);
            Teb32Base->Vdm = PtrToUlong (TebBase->Vdm);
            ASSERT (Teb32Base->ActivationContextStackPointer == 0);

            if (InitialTeb32Ptr != NULL) {
                Teb32Base->NtTib.StackBase = PtrToUlong (InitialTeb32Ptr->StackBase);
                Teb32Base->NtTib.StackLimit = PtrToUlong (InitialTeb32Ptr->StackLimit);
                Teb32Base->DeallocationStack = PtrToUlong (InitialTeb32Ptr->StackAllocationBase);
            }
        }
        
        TebBase->NtTib.ExceptionList = (PVOID)Teb32Base;
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {

        //
        // An exception has occurred, inform our caller.
        //

        Status = GetExceptionCode ();
    }

    KeDetachProcess();
    *Base = TebBase;

    return Status;
}

//
// This code is built twice on the Win64 build - once for PE32+
// and once for PE32 images.
//

#define MI_INIT_PEB_FROM_IMAGE(Hdrs, ImgConfig) {                           \
    PebBase->ImageSubsystem = (Hdrs)->OptionalHeader.Subsystem;             \
    PebBase->ImageSubsystemMajorVersion =                                   \
        (Hdrs)->OptionalHeader.MajorSubsystemVersion;                       \
    PebBase->ImageSubsystemMinorVersion =                                   \
        (Hdrs)->OptionalHeader.MinorSubsystemVersion;                       \
                                                                            \
    /*                                                                   */ \
    /* See if this image wants GetVersion to lie about who the system is */ \
    /* If so, capture the lie into the PEB for the process.              */ \
    /*                                                                   */ \
                                                                            \
    if ((Hdrs)->OptionalHeader.Win32VersionValue != 0) {                    \
        PebBase->OSMajorVersion =                                           \
            (Hdrs)->OptionalHeader.Win32VersionValue & 0xFF;                \
        PebBase->OSMinorVersion =                                           \
            ((Hdrs)->OptionalHeader.Win32VersionValue >> 8) & 0xFF;         \
        PebBase->OSBuildNumber  =                                           \
            (USHORT)(((Hdrs)->OptionalHeader.Win32VersionValue >> 16) & 0x3FFF); \
        if ((ImgConfig) != NULL && (ImgConfig)->CSDVersion != 0) {          \
            PebBase->OSCSDVersion = (ImgConfig)->CSDVersion;                \
            }                                                               \
                                                                            \
        /* Win32 API GetVersion returns the following bogus bit definitions */ \
        /* in the high two bits:                                            */ \
        /*                                                                  */ \
        /*      00 - Windows NT                                             */ \
        /*      01 - reserved                                               */ \
        /*      10 - Win32s running on Windows 3.x                          */ \
        /*      11 - Windows 95                                             */ \
        /*                                                                  */ \
        /*                                                                  */ \
        /* Win32 API GetVersionEx returns a dwPlatformId with the following */ \
        /* values defined in winbase.h                                      */ \
        /*                                                                  */ \
        /*      00 - VER_PLATFORM_WIN32s                                    */ \
        /*      01 - VER_PLATFORM_WIN32_WINDOWS                             */ \
        /*      10 - VER_PLATFORM_WIN32_NT                                  */ \
        /*      11 - reserved                                               */ \
        /*                                                                  */ \
        /*                                                                  */ \
        /* So convert the former from the Win32VersionValue field into the  */ \
        /* OSPlatformId field.  This is done by XORing with 0x2.  The       */ \
        /* translation is symmetric so there is the same code to do the     */ \
        /* reverse in windows\base\client\module.c (GetVersion)             */ \
        /*                                                                  */ \
        PebBase->OSPlatformId   =                                           \
            ((Hdrs)->OptionalHeader.Win32VersionValue >> 30) ^ 0x2;         \
        }                                                                   \
    }


#if defined(_WIN64)
NTSTATUS
MiInitializeWowPeb (
    IN PIMAGE_NT_HEADERS NtHeaders,
    IN PPEB PebBase,
    IN PEPROCESS TargetProcess
    )

/*++

Routine Description:

    This routine creates a PEB32 page within the target process
    and copies the initial PEB32 values into it.

Arguments:

    NtHeaders - Supplies a pointer to the NT headers for the image.

    PebBase - Supplies a pointer to the initial PEB to derive the PEB32 values
              from.

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the PEB32.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    PMMVAD Vad;
    NTSTATUS Status;
    ULONG ReturnedSize;
    PPEB32 PebBase32;
    ULONG ProcessAffinityMask;
    PVOID ImageBase;
    BOOLEAN MappedAsImage;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PIMAGE_LOAD_CONFIG_DIRECTORY32 ImageConfigData32;

    ProcessAffinityMask = 0;
    ImageConfigData32 = NULL;
    MappedAsImage = FALSE;

    //
    // All references to the Peb and NtHeaders must be wrapped in try-except
    // in case the user has exceeded quota (non-paged, pagefile, commit)
    // or any inpage errors happen for the user addresses, etc.
    //

    try {

        ImageBase = PebBase->ImageBaseAddress;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    //
    // Inspect the address space to determine if the executable image is
    // mapped with a single copy on write subsection (ie: as data) or if
    // the alignment was such that it was mapped as a full image section.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    ASSERT ((TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) == 0);

    Vad = MiCheckForConflictingVad (TargetProcess, ImageBase, ImageBase);

    if (Vad == NULL) {

        //
        // No virtual address is reserved at the specified base address,
        // return an error.
        //

        UNLOCK_ADDRESS_SPACE (TargetProcess);
        return STATUS_ACCESS_VIOLATION;
    }

    if (Vad->u.VadFlags.PrivateMemory == 0) {

        ControlArea = Vad->ControlArea;

        if ((ControlArea->u.Flags.Image == 1) &&
            (ControlArea->Segment->SegmentFlags.ExtraSharedWowSubsections == 0)) {

            if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
                (ControlArea->u.Flags.Rom == 0)) {

                Subsection = (PSUBSECTION) (ControlArea + 1);
            }
            else {
                Subsection = (PSUBSECTION) ((PLARGE_CONTROL_AREA)ControlArea + 1);
            }

            //
            // Real images always have a subsection for the PE header and then
            // at least one other subsection for the other sections in the
            // image.
            //

            if (Subsection->NextSubsection != NULL) {
                MappedAsImage = TRUE;
            }
        }
    }

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    try {

        ImageConfigData32 = RtlImageDirectoryEntryToData (
                                PebBase->ImageBaseAddress,
                                MappedAsImage,
                                IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG,
                                &ReturnedSize);

        if (ImageConfigData32 != NULL) {
            ProbeForReadSmallStructure ((PVOID)ImageConfigData32,
                                        sizeof (*ImageConfigData32),
                                        sizeof (ULONG));
        }

        MI_INIT_PEB_FROM_IMAGE ((PIMAGE_NT_HEADERS32)NtHeaders,
                                ImageConfigData32);

        if ((ImageConfigData32 != NULL) &&
            (ImageConfigData32->ProcessAffinityMask != 0)) {

            ProcessAffinityMask = ImageConfigData32->ProcessAffinityMask;
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    //
    // Create a PEB32 for the process.
    //

    Status = MiCreatePebOrTeb (TargetProcess,
                               (ULONG)sizeof (PEB32),
                               (PVOID)&PebBase32);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // Mark the process as WOW64 by storing the 32-bit PEB pointer
    // in the Wow64 field.
    //

    TargetProcess->Wow64Process->Wow64 = PebBase32;

    //
    // Clone the PEB into the PEB32.
    //

    try {
        PebBase32->InheritedAddressSpace = PebBase->InheritedAddressSpace;
        PebBase32->ImageUsesLargePages = PebBase->ImageUsesLargePages;
        PebBase32->Mutant = PtrToUlong(PebBase->Mutant);
        PebBase32->ImageBaseAddress = PtrToUlong(PebBase->ImageBaseAddress);
        PebBase32->AnsiCodePageData = PtrToUlong(PebBase->AnsiCodePageData);
        PebBase32->OemCodePageData = PtrToUlong(PebBase->OemCodePageData);
        PebBase32->UnicodeCaseTableData = PtrToUlong(PebBase->UnicodeCaseTableData);
        PebBase32->NumberOfProcessors = PebBase->NumberOfProcessors;
        PebBase32->BeingDebugged = PebBase->BeingDebugged;
        PebBase32->NtGlobalFlag = PebBase->NtGlobalFlag;
        PebBase32->CriticalSectionTimeout = PebBase->CriticalSectionTimeout;

        if (PebBase->HeapSegmentReserve > 1024*1024*1024) { // 1GB
            PebBase32->HeapSegmentReserve = 1024*1024;      // 1MB
        }
        else {
            PebBase32->HeapSegmentReserve = (ULONG)PebBase->HeapSegmentReserve;
        }

        if (PebBase->HeapSegmentCommit > PebBase32->HeapSegmentReserve) {
            PebBase32->HeapSegmentCommit = 2*PAGE_SIZE;
        }
        else {
            PebBase32->HeapSegmentCommit = (ULONG)PebBase->HeapSegmentCommit;
        }

        PebBase32->HeapDeCommitTotalFreeThreshold = (ULONG)PebBase->HeapDeCommitTotalFreeThreshold;
        PebBase32->HeapDeCommitFreeBlockThreshold = (ULONG)PebBase->HeapDeCommitFreeBlockThreshold;
        PebBase32->NumberOfHeaps = PebBase->NumberOfHeaps;
        PebBase32->MaximumNumberOfHeaps = (PAGE_SIZE - sizeof(PEB32)) / sizeof(ULONG);
        PebBase32->ProcessHeaps = PtrToUlong(PebBase32+1);
        PebBase32->OSMajorVersion = PebBase->OSMajorVersion;
        PebBase32->OSMinorVersion = PebBase->OSMinorVersion;
        PebBase32->OSBuildNumber = PebBase->OSBuildNumber;
        PebBase32->OSPlatformId = PebBase->OSPlatformId;
        PebBase32->OSCSDVersion = PebBase->OSCSDVersion;
        PebBase32->ImageSubsystem = PebBase->ImageSubsystem;
        PebBase32->ImageSubsystemMajorVersion = PebBase->ImageSubsystemMajorVersion;
        PebBase32->ImageSubsystemMinorVersion = PebBase->ImageSubsystemMinorVersion;
        PebBase32->SessionId = MmGetSessionId (TargetProcess);
        DONTASSERT (PebBase32->pShimData == 0);
        DONTASSERT (PebBase32->AppCompatFlags.QuadPart == 0);

        //
        // Leave the AffinityMask in the 32bit PEB as zero and let the
        // 64bit NTDLL set the initial mask.  This is to allow the
        // round robin scheduling of non MP safe imaging in the
        // caller to work correctly.
        //
        // Later code will set the affinity mask in the PEB32 if the
        // image actually specifies one.
        //
        // Note that the AffinityMask in the PEB is simply a mechanism
        // to pass affinity information from the image to the loader.
        //
        // Pass the affinity mask up to the 32 bit NTDLL via
        // the PEB32.  The 32 bit NTDLL will determine that the
        // affinity is not zero and try to set the affinity
        // mask from user-mode.  This call will be intercepted
        // by the wow64 thunks which will convert it
        // into a 64bit affinity mask and call the kernel.
        //

        PebBase32->ImageProcessAffinityMask = ProcessAffinityMask;

        DONTASSERT (PebBase32->ActivationContextData == 0);
        DONTASSERT (PebBase32->SystemDefaultActivationContextData == 0);

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode ();
    }
    return Status;
}
#endif


NTSTATUS
MmCreatePeb (
    IN PEPROCESS TargetProcess,
    IN PINITIAL_PEB InitialPeb,
    OUT PPEB *Base
    )

/*++

Routine Description:

    This routine creates a PEB page within the target process
    and copies the initial PEB values into it.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the PEB.

    InitialPeb - Supplies a pointer to the initial PEB to copy into the
                 newly created PEB.

    Base - Supplies a location to return the base of the newly created
           PEB on success.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    PPEB PebBase;
    USHORT Magic;
    USHORT Characteristics;
    NTSTATUS Status;
    PVOID ViewBase;
    LARGE_INTEGER SectionOffset;
    PIMAGE_NT_HEADERS NtHeaders;
    SIZE_T ViewSize;
    ULONG ReturnedSize;
    PIMAGE_LOAD_CONFIG_DIRECTORY ImageConfigData;
    ULONG_PTR ProcessAffinityMask;

    ViewBase = NULL;
    SectionOffset.LowPart = 0;
    SectionOffset.HighPart = 0;
    ViewSize = 0;

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    //
    // Map the NLS tables into the application's address space.
    //

    Status = MmMapViewOfSection (InitNlsSectionPointer,
                                 TargetProcess,
                                 &ViewBase,
                                 0L,
                                 0L,
                                 &SectionOffset,
                                 &ViewSize,
                                 ViewShare,
                                 MEM_TOP_DOWN | SEC_NO_CHANGE,
                                 PAGE_READONLY);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess ();
        return Status;
    }

    Status = MiCreatePebOrTeb (TargetProcess, sizeof(PEB), (PVOID)&PebBase);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess ();
        return Status;
    }

    //
    // Initialize the Peb.  Every reference to the Peb
    // must be wrapped in try-except in case the inpage fails.  The inpage
    // can fail for any reason including network failures, disk errors,
    // low resources, etc.
    //

    try {
        PebBase->InheritedAddressSpace = InitialPeb->InheritedAddressSpace;
        PebBase->ImageUsesLargePages = InitialPeb->ImageUsesLargePages;
        PebBase->Mutant = InitialPeb->Mutant;
        PebBase->ImageBaseAddress = TargetProcess->SectionBaseAddress;

        PebBase->AnsiCodePageData = (PVOID)((PUCHAR)ViewBase+InitAnsiCodePageDataOffset);
        PebBase->OemCodePageData = (PVOID)((PUCHAR)ViewBase+InitOemCodePageDataOffset);
        PebBase->UnicodeCaseTableData = (PVOID)((PUCHAR)ViewBase+InitUnicodeCaseTableDataOffset);

        PebBase->NumberOfProcessors = KeNumberProcessors;
        PebBase->BeingDebugged = (BOOLEAN)(TargetProcess->DebugPort != NULL ? TRUE : FALSE);
        PebBase->NtGlobalFlag = NtGlobalFlag;
        PebBase->CriticalSectionTimeout = MmCriticalSectionTimeout;
        PebBase->HeapSegmentReserve = MmHeapSegmentReserve;
        PebBase->HeapSegmentCommit = MmHeapSegmentCommit;
        PebBase->HeapDeCommitTotalFreeThreshold = MmHeapDeCommitTotalFreeThreshold;
        PebBase->HeapDeCommitFreeBlockThreshold = MmHeapDeCommitFreeBlockThreshold;
        DONTASSERT (PebBase->NumberOfHeaps == 0);
        PebBase->MaximumNumberOfHeaps = (PAGE_SIZE - sizeof (PEB)) / sizeof( PVOID);
        PebBase->ProcessHeaps = (PVOID *)(PebBase+1);

        PebBase->OSMajorVersion = NtMajorVersion;
        PebBase->OSMinorVersion = NtMinorVersion;
        PebBase->OSBuildNumber = (USHORT)(NtBuildNumber & 0x3FFF);
        PebBase->OSPlatformId = 2;      // VER_PLATFORM_WIN32_NT from winbase.h
        PebBase->OSCSDVersion = (USHORT)CmNtCSDVersion;
        DONTASSERT (PebBase->pShimData == 0);
        DONTASSERT (PebBase->AppCompatFlags.QuadPart == 0);
        DONTASSERT (PebBase->ActivationContextData == NULL);
        DONTASSERT (PebBase->SystemDefaultActivationContextData == NULL);

        if (TargetProcess->Session != NULL) {
            PebBase->SessionId = MmGetSessionId (TargetProcess);
        }

        PebBase->MinimumStackCommit = (SIZE_T)MmMinimumStackCommitInBytes;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        KeDetachProcess();
        return GetExceptionCode ();
    }

    //
    // Every reference to NtHeaders (including the call to RtlImageNtHeader)
    // must be wrapped in try-except in case the inpage fails.  The inpage
    // can fail for any reason including network failures, disk errors,
    // low resources, etc.
    //

    try {
        NtHeaders = RtlImageNtHeader (PebBase->ImageBaseAddress);
        Magic = NtHeaders->OptionalHeader.Magic;
        Characteristics = NtHeaders->FileHeader.Characteristics;
    } except (EXCEPTION_EXECUTE_HANDLER) {
        KeDetachProcess();
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if (NtHeaders != NULL) {

        ProcessAffinityMask = 0;

#if defined(_WIN64)

        if (TargetProcess->Wow64Process) {

            Status = MiInitializeWowPeb (NtHeaders, PebBase, TargetProcess);

            if (!NT_SUCCESS(Status)) {
                KeDetachProcess ();
                return Status;
            }
        }
        else      // a PE32+ image
#endif
        {
            try {
                ImageConfigData = RtlImageDirectoryEntryToData (
                                        PebBase->ImageBaseAddress,
                                        TRUE,
                                        IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG,
                                        &ReturnedSize);

                if (ImageConfigData != NULL) {
                    ProbeForReadSmallStructure ((PVOID)ImageConfigData,
                                                sizeof (*ImageConfigData),
                                                PROBE_ALIGNMENT (IMAGE_LOAD_CONFIG_DIRECTORY));
                }

                MI_INIT_PEB_FROM_IMAGE(NtHeaders, ImageConfigData);

                if (ImageConfigData != NULL && ImageConfigData->ProcessAffinityMask != 0) {
                    ProcessAffinityMask = ImageConfigData->ProcessAffinityMask;
                }

            } except (EXCEPTION_EXECUTE_HANDLER) {
                KeDetachProcess();
                return STATUS_INVALID_IMAGE_PROTECT;
            }

        }

        //
        // Note NT4 examined the NtHeaders->FileHeader.Characteristics
        // for the IMAGE_FILE_AGGRESIVE_WS_TRIM bit, but this is not needed
        // or used for NT5 and above.
        //

        //
        // See if image wants to override the default processor affinity mask.
        //

        try {

            if (Characteristics & IMAGE_FILE_UP_SYSTEM_ONLY) {

                //
                // Image is NOT MP safe.  Assign it a processor on a rotating
                // basis to spread these processes around on MP systems.
                //

                do {
                    PebBase->ImageProcessAffinityMask = ((KAFFINITY)0x1 << MmRotatingUniprocessorNumber);
                    if (++MmRotatingUniprocessorNumber >= KeNumberProcessors) {
                        MmRotatingUniprocessorNumber = 0;
                    }
                } while ((PebBase->ImageProcessAffinityMask & KeActiveProcessors) == 0);
            }
            else {

                if (ProcessAffinityMask != 0) {

                    //
                    // Pass the affinity mask from the image header
                    // to LdrpInitializeProcess via the PEB.
                    //

                    PebBase->ImageProcessAffinityMask = ProcessAffinityMask;
                }
            }
        } except (EXCEPTION_EXECUTE_HANDLER) {
            KeDetachProcess();
            return STATUS_INVALID_IMAGE_PROTECT;
        }
    }

    KeDetachProcess();

    *Base = PebBase;

    return STATUS_SUCCESS;
}

VOID
MmDeleteTeb (
    IN PEPROCESS TargetProcess,
    IN PVOID TebBase
    )

/*++

Routine Description:

    This routine deletes a TEB page within the target process.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to delete
                    the TEB.

    TebBase - Supplies the base address of the TEB to delete.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID EndingAddress;
    PMMVAD_LONG Vad;
    PETHREAD Thread;
    NTSTATUS Status;
    PMMSECURE_ENTRY Secure;
    PMMVAD PreviousVad;
    PMMVAD NextVad;

    EndingAddress = ((PCHAR)TebBase +
                                ROUND_TO_PAGES (sizeof(TEB)) - 1);

#if defined(_WIN64)
    if (TargetProcess->Wow64Process) {
        EndingAddress = ((PCHAR)EndingAddress + ROUND_TO_PAGES (sizeof(TEB32)));
    }
#endif

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    Vad = (PMMVAD_LONG) MiLocateAddress (TebBase);

    ASSERT (Vad != NULL);

    ASSERT ((Vad->StartingVpn == MI_VA_TO_VPN (TebBase)) &&
            (Vad->EndingVpn == MI_VA_TO_VPN (EndingAddress)));

    //
    // If someone has secured the TEB (in addition to the standard securing
    // that was done by memory management on creation, then don't delete it
    // now - just leave it around until the entire process is deleted.
    //

    ASSERT (Vad->u.VadFlags.NoChange == 1);
    if (Vad->u2.VadFlags2.OneSecured) {
        Status = STATUS_SUCCESS;
    }
    else {
        ASSERT (Vad->u2.VadFlags2.MultipleSecured);
        ASSERT (IsListEmpty (&Vad->u3.List) == 0);

        //
        // If there's only one entry, then that's the one we defined when we
        // initially created the TEB.  So TEB deletion can take place right
        // now.  If there's more than one entry, let the TEB sit around until
        // the process goes away.
        //

        Secure = CONTAINING_RECORD (Vad->u3.List.Flink,
                                    MMSECURE_ENTRY,
                                    List);

        if (Secure->List.Flink == &Vad->u3.List) {
            Status = STATUS_SUCCESS;
        }
        else {
            Status = STATUS_NOT_FOUND;
        }
    }

    if (NT_SUCCESS (Status)) {

        PreviousVad = MiGetPreviousVad (Vad);
        NextVad = MiGetNextVad (Vad);

        Thread = PsGetCurrentThread ();

        MiRemoveVadCharges ((PMMVAD)Vad, TargetProcess);

        //
        // Return commitment for page table pages and clear VAD bitmaps
        // if possible.
        //

        MiReturnPageTablePageCommitment (TebBase,
                                         EndingAddress,
                                         TargetProcess,
                                         PreviousVad,
                                         NextVad);

        LOCK_WS_UNSAFE (Thread, TargetProcess);

        MiRemoveVad ((PMMVAD)Vad, TargetProcess);

        MiDeleteVirtualAddresses (TebBase, EndingAddress, NULL);

        UNLOCK_WS_AND_ADDRESS_SPACE (Thread, TargetProcess);

        ExFreePool (Vad);
    }
    else {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
    }

    KeDetachProcess();
}

VOID
MiAllowWorkingSetExpansion (
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This routine inserts the working set into the list scanned by the trimmer.

Arguments:

    WsInfo - Supplies the working set to insert.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;

    ASSERT (WsInfo->WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED);
    ASSERT (WsInfo->WorkingSetExpansionLinks.Blink == MM_WS_NOT_LISTED);

    LOCK_EXPANSION (OldIrql);

    InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                    &WsInfo->WorkingSetExpansionLinks);

    UNLOCK_EXPANSION (OldIrql);

    return;
}

#if DBG
ULONG MiDeleteLocked;
#endif


VOID
MiDeleteAddressesInWorkingSet (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine deletes all user mode addresses from the working set list.

Arguments:

    Process - Supplies a pointer to the current process.

Return Value:

    None.

Environment:

    Kernel mode, Working Set Lock held.

--*/

{
    PMMWSLE Wsle;
    WSLE_NUMBER index;
    WSLE_NUMBER Entry;
    PVOID Va;
    PMMPTE PointerPte;
    MMPTE_DELETE_LIST PteDeleteList;
#if DBG
    PMMPFN Pfn1;
    PVOID SwapVa;
    PMMWSLE LastWsle;
#endif

    //
    // Go through the working set and for any user-accessible page which is
    // in it, rip it out of the working set and free the page.
    //

    index = 2;
    Wsle = &MmWsle[index];
    PteDeleteList.Count = 0;

    MmWorkingSetList->HashTable = NULL;

    //
    // Go through the working set list and remove all pages for user
    // space addresses.
    //

    for ( ; index <= MmWorkingSetList->LastEntry; index += 1, Wsle += 1) {

        if (Wsle->u1.e1.Valid == 0) {
            continue;
        }

        Va = Wsle->u1.VirtualAddress;

#if (_MI_PAGING_LEVELS >= 4)
        ASSERT (MiGetPxeAddress(Va)->u.Hard.Valid == 1);
#endif
#if (_MI_PAGING_LEVELS >= 3)
        ASSERT (MiGetPpeAddress(Va)->u.Hard.Valid == 1);
#endif
        ASSERT (MiGetPdeAddress(Va)->u.Hard.Valid == 1);
        ASSERT (MiGetPteAddress(Va)->u.Hard.Valid == 1);

        if (Va >= (PVOID)MM_HIGHEST_USER_ADDRESS) {
            continue;
        }

        //
        // Ensure the WSLE and the PTE are both valid so that any
        // conflict between them can be resolved before both are deleted.
        //

        PointerPte = MiGetPteAddress (Va);

        if (PointerPte->u.Hard.Valid == 0) {
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x3452,
                          (ULONG_PTR) Va,
                          (ULONG_PTR) Wsle,
                          (ULONG_PTR) PointerPte->u.Long);
        }

        //
        // This is a user mode address, for each one we remove we must
        // maintain the NonDirectCount.  This is because we may fault
        // later for page tables and need to grow the hash table when
        // updating the working set.  NonDirectCount needs to be correct
        // at that point.
        //

        if (Wsle->u1.e1.Direct == 0) {
            Process->Vm.VmWorkingSetList->NonDirectCount -= 1;
        }

        //
        // This entry is in the working set list.
        //

        MiReleaseWsle (index, &Process->Vm);

        if (index < MmWorkingSetList->FirstDynamic) {

            //
            // This entry is locked.
            //

            MmWorkingSetList->FirstDynamic -= 1;

            if (index != MmWorkingSetList->FirstDynamic) {

                Entry = MmWorkingSetList->FirstDynamic;
#if DBG
                MiDeleteLocked += 1;
                SwapVa = MmWsle[MmWorkingSetList->FirstDynamic].u1.VirtualAddress;
                SwapVa = PAGE_ALIGN (SwapVa);

                Pfn1 = MI_PFN_ELEMENT (MiGetPteAddress (SwapVa)->u.Hard.PageFrameNumber);

                ASSERT (Entry == MiLocateWsle (SwapVa, MmWorkingSetList, Pfn1->u1.WsIndex, FALSE));
#endif
                MiSwapWslEntries (Entry, index, &Process->Vm, FALSE);

                //
                // We may have reused the WSLE we just deleted to hold a valid
                // entry as a result of the swap, so make sure we check it
                // again.
                //

                index -= 1;
                Wsle -= 1;
            }
        }

        PteDeleteList.PointerPte[PteDeleteList.Count] = PointerPte;
        PteDeleteList.PteContents[PteDeleteList.Count] = *PointerPte;
        PteDeleteList.Count += 1;

        if (PteDeleteList.Count == MM_MAXIMUM_FLUSH_COUNT) {
            MiDeletePteList (&PteDeleteList, Process);
            PteDeleteList.Count = 0;
        }
    }

    if (PteDeleteList.Count != 0) {
        MiDeletePteList (&PteDeleteList, Process);
    }

#if DBG
    Wsle = &MmWsle[2];
    LastWsle = &MmWsle[MmWorkingSetList->LastInitializedWsle];
    while (Wsle <= LastWsle) {
        if (Wsle->u1.e1.Valid == 1) {
#if (_MI_PAGING_LEVELS >= 4)
            ASSERT(MiGetPxeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
#if (_MI_PAGING_LEVELS >= 3)
            ASSERT(MiGetPpeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
            ASSERT(MiGetPdeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
            ASSERT(MiGetPteAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
        }
        Wsle += 1;
    }
#endif
}


VOID
MiDeletePteList (
    IN PMMPTE_DELETE_LIST PteDeleteList,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine deletes the specified virtual address.

Arguments:

    PteDeleteList - Supplies the list of PTEs to delete.

    CurrentProcess - Supplies the current process.

Return Value:

    None.

Environment:

    Kernel mode.  Working set mutex held.

    Note since this is only called during process teardown, the write watch
    bits are not updated.  If this ever called from other places, code
    will need to be added here to update those bits.

--*/

{
    ULONG i;
    ULONG j;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    MMPTE DemandZeroWritePte;
    MMPTE_FLUSH_LIST PteFlushList;

    j = 0;
    PteFlushList.Count = 0;
    DemandZeroWritePte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    LOCK_PFN (OldIrql);
    
    for (i = 0; i < PteDeleteList->Count; i += 1) {

        PointerPte = PteDeleteList->PointerPte[i];
    
        ASSERT (PointerPte->u.Hard.Valid == 1);
    
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    
        if (Pfn1->u3.e1.PrototypePte == 1) {
    
            CloneBlock = (PMMCLONE_BLOCK) Pfn1->PteAddress;
    
            PointerPde = MiGetPteAddress (PointerPte);
    
            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
    
            //
            // Capture the state of the modified bit for this PTE.
            //
    
            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);
    
            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //
    
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
    
            //
            // Decrement the share count for the physical page.
            //
    
            MiDecrementShareCount (Pfn1, PageFrameIndex);
    
            //
            // Set the pointer to PTE to be a demand zero PTE.  This allows
            // the page usage count to be kept properly and handles the case
            // when a page table page has only valid PTEs and needs to be
            // deleted later when the VADs are removed.
            //
    
            MI_WRITE_INVALID_PTE (PointerPte, DemandZeroWritePte);
    
            //
            // Check to see if this is a fork prototype PTE and if so
            // update the clone descriptor address.
            //
    
            ASSERT (MiGetVirtualAddressMappedByPte (PointerPte) <= MM_HIGHEST_USER_ADDRESS);
    
            //
            // Locate the clone descriptor within the clone tree.
            //
    
            CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);
            if (CloneDescriptor != NULL) {
    
                //
                // Decrement the reference count for the clone block,
                // note that this could release and reacquire
                // the mutexes hence cannot be done until after the
                // working set index has been removed.
                //
                // Flush the TB as this path could release the PFN lock.
                // Even though the process is terminating, a thread could
                // attach to it and try to reference the user address space.
                // By flushing the TB we ensure no access to free pages is
                // allowed. 
                //
                // The PteDeleteList could be overloaded to save stack, but
                // using a separate list lets us flush just the entries since
                // the last call and still retain a pristine PteDeleteList
                // for debugging purposes.
                //

                while (j <= i) {
                    ASSERT (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT);
                    PteFlushList.FlushVa[PteFlushList.Count] = MiGetVirtualAddressMappedByPte (PteDeleteList->PointerPte[j]);
                    j += 1;
                    PteFlushList.Count += 1;
                }

                MiDecrementCloneBlockReference (CloneDescriptor,
                                                CloneBlock,
                                                CurrentProcess,
                                                &PteFlushList,
                                                OldIrql);

                //
                // Note PteFlushList.Count will be set to 0 if
                // MiDecrementCloneBlockReference did a flush.
                //
            }
        }
        else {
    
            //
            // This PTE is NOT a prototype PTE, delete the physical page.
            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //
    
            PageTableFrameIndex = Pfn1->u4.PteFrame;
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
    
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
    
            MI_SET_PFN_DELETED (Pfn1);
    
            //
            // Decrement the share count for the physical page.  As the page
            // is private it will be put on the free list.
            //
    
            MiDecrementShareCount (Pfn1, PageFrameIndex);
    
            //
            // Decrement the count for the number of private pages.
            //
    
            CurrentProcess->NumberOfPrivatePages -= 1;
    
            //
            // Set the pointer to PTE to be a demand zero PTE.  This allows
            // the page usage count to be kept properly and handles the case
            // when a page table page has only valid PTEs and needs to be
            // deleted later when the VADs are removed.
            //
    
            MI_WRITE_INVALID_PTE (PointerPte, DemandZeroWritePte);
        }
    }

    //
    // Flush the TB - even though the process is terminating, a thread could
    // attach to it and try to reference the user address space.  By flushing
    // the TB we ensure no access to free pages is allowed.
    //

    MI_FLUSH_PROCESS_TB (FALSE);

    UNLOCK_PFN (OldIrql);
    
    return;
}

PFN_NUMBER
MiMakeOutswappedPageResident (
    IN PMMPTE ActualPteAddress,
    IN OUT PMMPTE PointerTempPte,
    IN ULONG Global,
    IN PFN_NUMBER ContainingPage,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine makes the specified PTE valid.

Arguments:

    ActualPteAddress - Supplies the actual address that the PTE will
                       reside at.  This is used for page coloring.

    PointerTempPte - Supplies the PTE to operate on, returns a valid PTE.

    Global - Supplies 1 if the resulting PTE is global.

    ContainingPage - Supplies the physical page number of the page which
                     contains the resulting PTE.  If this value is 0, no
                     operations on the containing page are performed.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    Returns the physical page number that was allocated for the PTE.

Environment:

    Kernel mode, PFN LOCK HELD - may be released and reacquired.

--*/

{
    MMPTE TempPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PMDL Mdl;
    LARGE_INTEGER StartingOffset;
    KEVENT Event;
    IO_STATUS_BLOCK IoStatus;
    PFN_NUMBER PageFileNumber;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    ULONG RefaultCount;
#if DBG
    PVOID HyperVa;
    PEPROCESS CurrentProcess;
#endif

    MM_PFN_LOCK_ASSERT();

restart:

    ASSERT (PointerTempPte->u.Hard.Valid == 0);

    if (PointerTempPte->u.Long == MM_KERNEL_DEMAND_ZERO_PTE) {

        //
        // Any page will do.
        //

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }

        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_FROM_PTE (ActualPteAddress));

        if (Global) {
            MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                      PageFrameIndex,
                                      MM_READWRITE,
                                      ActualPteAddress);
        }
        else {
            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               MM_READWRITE,
                               ActualPteAddress);
        }

        MI_SET_PTE_DIRTY (TempPte);

        MI_SET_GLOBAL_STATE (TempPte, Global);

        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);
        MiInitializePfnForOtherProcess (PageFrameIndex,
                                        ActualPteAddress,
                                        ContainingPage);

    }
    else if (PointerTempPte->u.Soft.Transition == 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerTempPte);
        PointerTempPte->u.Trans.Protection = MM_READWRITE;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        if ((MmAvailablePages == 0) ||
            ((Pfn1->u4.InPageError == 1) && (Pfn1->u3.e1.ReadInProgress == 1))) {

            //
            // This can only happen if the system is utilizing a hardware
            // compression cache.  This ensures that only a safe amount
            // of the compressed virtual cache is directly mapped so that
            // if the hardware gets into trouble, we can bail it out.
            //

            UNLOCK_PFN (OldIrql);

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmHalfSecond);
            LOCK_PFN (OldIrql);
            goto restart;
        }

        //
        // PTE refers to a transition PTE.
        //

        if (Pfn1->u3.e1.PageLocation != ActiveAndValid) {
            MiUnlinkPageFromList (Pfn1);

            //
            // Even though this routine is only used to bring in special
            // system pages that are separately charged, a modified write
            // may be in progress and if so, will have applied a systemwide
            // charge against the locked pages count.  This all works out nicely
            // (with no code needed here) as the write completion will see
            // the nonzero ShareCount and remove the charge.
            //

            InterlockedIncrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
        }

        //
        // Update the PFN database, the share count is now 1 and
        // the reference count is incremented as the share count
        // just went from zero to 1.
        //

        Pfn1->u2.ShareCount += 1;

        MI_SET_MODIFIED (Pfn1, 1, 0x12);

        if (Pfn1->u3.e1.WriteInProgress == 0) {

            //
            // Release the page file space for this page.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
        }

        if (Global) {
            MI_MAKE_TRANSITION_KERNELPTE_VALID (TempPte, PointerTempPte);
        }
        else {
            MI_MAKE_TRANSITION_PTE_VALID (TempPte, PointerTempPte);
        }

        MI_SET_PTE_DIRTY (TempPte);
        MI_SET_GLOBAL_STATE (TempPte, Global);
        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);
    }
    else {

        //
        // Page resides in a paging file.
        // Any page will do.
        //

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }

        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_FROM_PTE (ActualPteAddress));

        //
        // Initialize the PFN database element, but don't
        // set read in progress as collided page faults cannot
        // occur here.
        //

        MiInitializePfnForOtherProcess (PageFrameIndex,
                                        ActualPteAddress,
                                        ContainingPage);

        UNLOCK_PFN (OldIrql);

        PointerTempPte->u.Soft.Protection = MM_READWRITE;

        KeInitializeEvent (&Event, NotificationEvent, FALSE);

        //
        // Calculate the VPN for the in-page operation.
        //

        TempPte = *PointerTempPte;
        PageFileNumber = GET_PAGING_FILE_NUMBER (TempPte);

        StartingOffset.QuadPart = (LONGLONG)(GET_PAGING_FILE_OFFSET (TempPte)) <<
                                    PAGE_SHIFT;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Build MDL for request.
        //

        Mdl = (PMDL)&MdlHack[0];
        MmInitializeMdl (Mdl,
                         MiGetVirtualAddressMappedByPte (ActualPteAddress),
                         PAGE_SIZE);
        Mdl->MdlFlags |= MDL_PAGES_LOCKED;

        Page = (PPFN_NUMBER)(Mdl + 1);
        *Page = PageFrameIndex;

#if DBG
        CurrentProcess = PsGetCurrentProcess ();

        HyperVa = MiMapPageInHyperSpace (CurrentProcess, PageFrameIndex, &OldIrql);
        RtlFillMemoryUlong (HyperVa,
                            PAGE_SIZE,
                            0x34785690);
        MiUnmapPageInHyperSpace (CurrentProcess, HyperVa, OldIrql);
#endif

        //
        // Issue the read request.
        //

        RefaultCount = 0;

Refault:
        Status = IoPageRead (MmPagingFile[PageFileNumber]->File,
                             Mdl,
                             &StartingOffset,
                             &Event,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject (&Event,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)NULL);
            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (NT_SUCCESS(Status)) {
            if (IoStatus.Information != PAGE_SIZE) {
                KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                              2,
                              IoStatus.Status,
                              PageFileNumber,
                              StartingOffset.LowPart);
            }
        }

        if ((!NT_SUCCESS(Status)) || (!NT_SUCCESS(IoStatus.Status))) {

            if ((MmIsRetryIoStatus (Status)) ||
                (MmIsRetryIoStatus (IoStatus.Status))) {
                    
                RefaultCount -= 1;

                if (RefaultCount & MiFaultRetryMask) {

                    //
                    // Insufficient resources, delay and reissue
                    // the in page operation.
                    //

                    KeDelayExecutionThread (KernelMode,
                                            FALSE,
                                            (PLARGE_INTEGER)&MmHalfSecond);

                    KeClearEvent (&Event);
                    goto Refault;
                }
            }
            KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                          Status,
                          IoStatus.Status,
                          PageFileNumber,
                          StartingOffset.LowPart);
        }

        PteContents = TempPte;

        if (Global) {
            MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                      PageFrameIndex,
                                      MM_READWRITE,
                                      ActualPteAddress);
        }
        else {
            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               MM_READWRITE,
                               ActualPteAddress);
        }

        MI_SET_PTE_DIRTY (TempPte);

        MI_SET_GLOBAL_STATE (TempPte, Global);

        LOCK_PFN (OldIrql);

        //
        // Release the page file space.
        //

        MiReleasePageFileSpace (PteContents);
        Pfn1->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        MI_SET_MODIFIED (Pfn1, 1, 0x13);

        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);
    }
    return PageFrameIndex;
}


UCHAR
MiSetMemoryPriorityProcess (
    IN PEPROCESS Process,
    IN UCHAR MemoryPriority
    )

/*++

Routine Description:

    Nonpaged wrapper to set the memory priority of a process.

Arguments:

    Process - Supplies the process to update.

    MemoryPriority - Supplies the new memory priority of the process.

Return Value:

    Old priority.

--*/

{
    KIRQL OldIrql;
    UCHAR OldPriority;

    LOCK_EXPANSION (OldIrql);

    OldPriority = (UCHAR) Process->Vm.Flags.MemoryPriority;
    Process->Vm.Flags.MemoryPriority = MemoryPriority;

    UNLOCK_EXPANSION (OldIrql);

    return OldPriority;
}

VOID
MmSetMemoryPriorityProcess (
    IN PEPROCESS Process,
    IN UCHAR MemoryPriority
    )

/*++

Routine Description:

    Sets the memory priority of a process.

Arguments:

    Process - Supplies the process to update

    MemoryPriority - Supplies the new memory priority of the process

Return Value:

    None.

--*/

{
    if (MmSystemSize == MmSmallSystem && MmNumberOfPhysicalPages < ((15*1024*1024)/PAGE_SIZE)) {

        //
        // If this is a small system, make every process BACKGROUND.
        //

        MemoryPriority = MEMORY_PRIORITY_BACKGROUND;
    }

    MiSetMemoryPriorityProcess (Process, MemoryPriority);

    return;
}


PMMVAD
MiAllocateVad (
    IN ULONG_PTR StartingVirtualAddress,
    IN ULONG_PTR EndingVirtualAddress,
    IN LOGICAL Deletable
    )

/*++

Routine Description:

    Reserve the specified range of address space.

Arguments:

    StartingVirtualAddress - Supplies the starting virtual address.

    EndingVirtualAddress - Supplies the ending virtual address.

    Deletable - Supplies TRUE if the VAD is to be marked as deletable, FALSE
                if deletions of this VAD should be disallowed.

Return Value:

    A VAD pointer on success, NULL on failure.

--*/

{
    PMMVAD_LONG Vad;

    ASSERT (StartingVirtualAddress <= EndingVirtualAddress);

    if (Deletable == TRUE) {
        Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_SHORT), 'SdaV');
    }
    else {
        Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');
    }

    if (Vad == NULL) {
       return NULL;
    }

    //
    // Set the starting and ending virtual page numbers of the VAD.
    //

    Vad->StartingVpn = MI_VA_TO_VPN (StartingVirtualAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingVirtualAddress);

    //
    // Mark VAD as no commitment, private, and readonly.
    //

    Vad->u.LongFlags = 0;
    Vad->u.VadFlags.CommitCharge = MM_MAX_COMMIT;
    Vad->u.VadFlags.Protection = MM_READONLY;
    Vad->u.VadFlags.PrivateMemory = 1;

    if (Deletable == TRUE) {
        ASSERT (Vad->u.VadFlags.NoChange == 0);
    }
    else {
        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.LongFlags2 = 0;
        Vad->u2.VadFlags2.OneSecured = 1;
        Vad->u2.VadFlags2.LongVad = 1;
        Vad->u2.VadFlags2.ReadOnly = 1;
        Vad->u3.Secured.StartVpn = StartingVirtualAddress;
        Vad->u3.Secured.EndVpn = EndingVirtualAddress;
    }

    return (PMMVAD) Vad;
}


PFN_NUMBER
MmGetDirectoryFrameFromProcess(
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine retrieves the PFN of the process's top pagetable page.  It can
    be used to map physical pages back to a process.

Arguments:

    Process - Supplies the process to query.

Return Value:

    Page frame number of the top level page table page.

Environment:

    Kernel mode.  No locks held.

--*/

{
    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    return MI_GET_DIRECTORY_FRAME_FROM_PROCESS(Process);
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\readwrt.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   readwrt.c

Abstract:

    This module contains the routines which implement the capability
    to read and write the virtual memory of a target process.

--*/

#include "mi.h"

//
// The maximum amount to try to Probe and Lock is 14 pages, this
// way it always fits in a 16 page allocation.
//

#define MAX_LOCK_SIZE ((ULONG)(14 * PAGE_SIZE))

//
// The maximum to move in a single block is 64k bytes.
//

#define MAX_MOVE_SIZE (LONG)0x10000

//
// The minimum to move is a single block is 128 bytes.
//

#define MINIMUM_ALLOCATION (LONG)128

//
// Define the pool move threshold value.
//

#define POOL_MOVE_THRESHOLD 511

//
// Define forward referenced procedure prototypes.
//

ULONG
MiGetExceptionInfo (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PLOGICAL ExceptionAddressConfirmed,
    IN PULONG_PTR BadVa
    );

NTSTATUS
MiDoMappedCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     );

NTSTATUS
MiDoPoolCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiGetExceptionInfo)
#pragma alloc_text(PAGE,NtReadVirtualMemory)
#pragma alloc_text(PAGE,NtWriteVirtualMemory)
#pragma alloc_text(PAGE,MiDoMappedCopy)
#pragma alloc_text(PAGE,MiDoPoolCopy)
#pragma alloc_text(PAGE,MmCopyVirtualMemory)
#endif

#define COPY_STACK_SIZE 64

NTSTATUS
NtReadVirtualMemory(
    __in HANDLE ProcessHandle,
    __in_opt PVOID BaseAddress,
    __out_bcount(BufferSize) PVOID Buffer,
    __in SIZE_T BufferSize,
    __out_opt PSIZE_T NumberOfBytesRead
    )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address in the specified process
                   to be read.

     Buffer - Supplies the address of a buffer which receives the
              contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    SIZE_T BytesCopied;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    PETHREAD CurrentThread;

    PAGED_CODE();

    //
    // Get the previous mode and probe output argument if necessary.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);
    if (PreviousMode != KernelMode) {

        if (((PCHAR)BaseAddress + BufferSize < (PCHAR)BaseAddress) ||
            ((PCHAR)Buffer + BufferSize < (PCHAR)Buffer) ||
            ((PVOID)((PCHAR)BaseAddress + BufferSize) > MM_HIGHEST_USER_ADDRESS) ||
            ((PVOID)((PCHAR)Buffer + BufferSize) > MM_HIGHEST_USER_ADDRESS)) {

            return STATUS_ACCESS_VIOLATION;
        }

        if (ARGUMENT_PRESENT(NumberOfBytesRead)) {
            try {
                ProbeForWriteUlong_ptr (NumberOfBytesRead);

            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }
    }

    //
    // If the buffer size is not zero, then attempt to read data from the
    // specified process address space into the current process address
    // space.
    //

    BytesCopied = 0;
    Status = STATUS_SUCCESS;
    if (BufferSize != 0) {

        //
        // Reference the target process.
        //

        Status = ObReferenceObjectByHandle(ProcessHandle,
                                           PROCESS_VM_READ,
                                           PsProcessType,
                                           PreviousMode,
                                           (PVOID *)&Process,
                                           NULL);

        //
        // If the process was successfully referenced, then attempt to
        // read the specified memory either by direct mapping or copying
        // through nonpaged pool.
        //

        if (Status == STATUS_SUCCESS) {

            Status = MmCopyVirtualMemory (Process,
                                          BaseAddress,
                                          PsGetCurrentProcessByThread(CurrentThread),
                                          Buffer,
                                          BufferSize,
                                          PreviousMode,
                                          &BytesCopied);

            //
            // Dereference the target process.
            //

            ObDereferenceObject(Process);
        }
    }

    //
    // If requested, return the number of bytes read.
    //

    if (ARGUMENT_PRESENT(NumberOfBytesRead)) {
        try {
            *NumberOfBytesRead = BytesCopied;

        } except(EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }
    }

    return Status;
}

NTSTATUS
NtWriteVirtualMemory(
    __in HANDLE ProcessHandle,
    __in_opt PVOID BaseAddress,
    __in_bcount(BufferSize) CONST VOID *Buffer,
    __in SIZE_T BufferSize,
    __out_opt PSIZE_T NumberOfBytesWritten
    )

/*++

Routine Description:

    This function copies the specified address range from the current
    process into the specified address range of the specified process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address to be written to in the
                   specified process.

     Buffer - Supplies the address of a buffer which contains the
              contents to be written into the specified process
              address space.

     BufferSize - Supplies the requested number of bytes to write
                  into the specified process.

     NumberOfBytesWritten - Receives the actual number of bytes
                            transferred into the specified address space.

Return Value:

    NTSTATUS.

--*/

{
    SIZE_T BytesCopied;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    PETHREAD CurrentThread;

    PAGED_CODE();

    //
    // Get the previous mode and probe output argument if necessary.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);
    if (PreviousMode != KernelMode) {

        if (((PCHAR)BaseAddress + BufferSize < (PCHAR)BaseAddress) ||
            ((PCHAR)Buffer + BufferSize < (PCHAR)Buffer) ||
            ((PVOID)((PCHAR)BaseAddress + BufferSize) > MM_HIGHEST_USER_ADDRESS) ||
            ((PVOID)((PCHAR)Buffer + BufferSize) > MM_HIGHEST_USER_ADDRESS)) {

            return STATUS_ACCESS_VIOLATION;
        }

        if (ARGUMENT_PRESENT(NumberOfBytesWritten)) {
            try {
                ProbeForWriteUlong_ptr(NumberOfBytesWritten);

            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }
    }

    //
    // If the buffer size is not zero, then attempt to write data from the
    // current process address space into the target process address space.
    //

    BytesCopied = 0;
    Status = STATUS_SUCCESS;
    if (BufferSize != 0) {

        //
        // Reference the target process.
        //

        Status = ObReferenceObjectByHandle(ProcessHandle,
                                           PROCESS_VM_WRITE,
                                           PsProcessType,
                                           PreviousMode,
                                           (PVOID *)&Process,
                                           NULL);

        //
        // If the process was successfully referenced, then attempt to
        // write the specified memory either by direct mapping or copying
        // through nonpaged pool.
        //

        if (Status == STATUS_SUCCESS) {

            Status = MmCopyVirtualMemory (PsGetCurrentProcessByThread(CurrentThread),
                                          Buffer,
                                          Process,
                                          BaseAddress,
                                          BufferSize,
                                          PreviousMode,
                                          &BytesCopied);

            //
            // Dereference the target process.
            //

            ObDereferenceObject(Process);
        }
    }

    //
    // If requested, return the number of bytes read.
    //

    if (ARGUMENT_PRESENT(NumberOfBytesWritten)) {
        try {
            *NumberOfBytesWritten = BytesCopied;

        } except(EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }
    }

    return Status;
}


NTSTATUS
MmCopyVirtualMemory(
    IN PEPROCESS FromProcess,
    IN CONST VOID *FromAddress,
    IN PEPROCESS ToProcess,
    OUT PVOID ToAddress,
    IN SIZE_T BufferSize,
    IN KPROCESSOR_MODE PreviousMode,
    OUT PSIZE_T NumberOfBytesCopied
    )
{
    NTSTATUS Status;
    PEPROCESS ProcessToLock;

    if (BufferSize == 0) {
        ASSERT (FALSE);         // No one should call with a zero size.
        return STATUS_SUCCESS;
    }

    ProcessToLock = FromProcess;
    if (FromProcess == PsGetCurrentProcess()) {
        ProcessToLock = ToProcess;
    }

    //
    // Make sure the process still has an address space.
    //

    if (ExAcquireRundownProtection (&ProcessToLock->RundownProtect) == FALSE) {
        return STATUS_PROCESS_IS_TERMINATING;
    }

    //
    // If the buffer size is greater than the pool move threshold,
    // then attempt to write the memory via direct mapping.
    //

    if (BufferSize > POOL_MOVE_THRESHOLD) {
        Status = MiDoMappedCopy(FromProcess,
                                FromAddress,
                                ToProcess,
                                ToAddress,
                                BufferSize,
                                PreviousMode,
                                NumberOfBytesCopied);

        //
        // If the completion status is not a working quota problem,
        // then finish the service. Otherwise, attempt to write the
        // memory through nonpaged pool.
        //

        if (Status != STATUS_WORKING_SET_QUOTA) {
            goto CompleteService;
        }

        *NumberOfBytesCopied = 0;
    }

    //
    // There was not enough working set quota to write the memory via
    // direct mapping or the size of the write was below the pool move
    // threshold. Attempt to write the specified memory through nonpaged
    // pool.
    //

    Status = MiDoPoolCopy(FromProcess,
                          FromAddress,
                          ToProcess,
                          ToAddress,
                          BufferSize,
                          PreviousMode,
                          NumberOfBytesCopied);

    //
    // Dereference the target process.
    //

CompleteService:

    //
    // Indicate that the vm operation is complete.
    //

    ExReleaseRundownProtection (&ProcessToLock->RundownProtect);

    return Status;
}


ULONG
MiGetExceptionInfo (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN OUT PLOGICAL ExceptionAddressConfirmed,
    IN OUT PULONG_PTR BadVa
    )

/*++

Routine Description:

    This routine examines a exception record and extracts the virtual
    address of an access violation, guard page violation, or in-page error.

Arguments:

    ExceptionPointers - Supplies a pointer to the exception record.

    ExceptionAddressConfirmed - Receives TRUE if the exception address was
                                reliably detected, FALSE if not.

    BadVa - Receives the virtual address which caused the access violation.

Return Value:

    EXECUTE_EXCEPTION_HANDLER

--*/

{
    PEXCEPTION_RECORD ExceptionRecord;

    PAGED_CODE();

    //
    // If the exception code is an access violation, guard page violation,
    // or an in-page read error, then return the faulting address. Otherwise.
    // return a special address value.
    //

    *ExceptionAddressConfirmed = FALSE;

    ExceptionRecord = ExceptionPointers->ExceptionRecord;

    if ((ExceptionRecord->ExceptionCode == STATUS_ACCESS_VIOLATION) ||
        (ExceptionRecord->ExceptionCode == STATUS_GUARD_PAGE_VIOLATION) ||
        (ExceptionRecord->ExceptionCode == STATUS_IN_PAGE_ERROR)) {

        //
        // The virtual address which caused the exception is the 2nd
        // parameter in the exception information array.
        //
        // The number of parameters will be zero if an exception handler
        // above us (like the one in MmProbeAndLockPages) caught the
        // original exception and subsequently just raised status.
        // This means the number of bytes copied is zero.
        //

        if (ExceptionRecord->NumberParameters > 1) {
            *ExceptionAddressConfirmed = TRUE;
            *BadVa = ExceptionRecord->ExceptionInformation[1];
        }
    }

    return EXCEPTION_EXECUTE_HANDLER;
}

NTSTATUS
MiDoMappedCopy (
    IN PEPROCESS FromProcess,
    IN CONST VOID *FromAddress,
    IN PEPROCESS ToProcess,
    OUT PVOID ToAddress,
    IN SIZE_T BufferSize,
    IN KPROCESSOR_MODE PreviousMode,
    OUT PSIZE_T NumberOfBytesRead
    )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     FromProcess - Supplies an open handle to a process object.

     FromAddress - Supplies the base address in the specified process
                   to be read.

     ToProcess - Supplies an open handle to a process object.

     ToAddress - Supplies the address of a buffer which receives the
                 contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     PreviousMode - Supplies the previous processor mode.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    SIZE_T AmountToMove;
    ULONG_PTR BadVa;
    LOGICAL Moving;
    LOGICAL Probing;
    LOGICAL LockedMdlPages;
    CONST VOID *InVa;
    SIZE_T LeftToMove;
    PSIZE_T MappedAddress;
    SIZE_T MaximumMoved;
    PMDL Mdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + (MAX_LOCK_SIZE >> PAGE_SHIFT) + 1];
    PVOID OutVa;
    LOGICAL MappingFailed;
    LOGICAL ExceptionAddressConfirmed;

    PAGED_CODE();

    MappingFailed = FALSE;

    InVa = FromAddress;
    OutVa = ToAddress;

    MaximumMoved = MAX_LOCK_SIZE;
    if (BufferSize <= MAX_LOCK_SIZE) {
        MaximumMoved = BufferSize;
    }

    Mdl = (PMDL)&MdlHack[0];

    //
    // Map the data into the system part of the address space, then copy it.
    //

    LeftToMove = BufferSize;
    AmountToMove = MaximumMoved;

    Probing = FALSE;

    //
    // Initializing BadVa & ExceptionAddressConfirmed is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BadVa = 0;
    ExceptionAddressConfirmed = FALSE;

    while (LeftToMove > 0) {

        if (LeftToMove < AmountToMove) {

            //
            // Set to move the remaining bytes.
            //

            AmountToMove = LeftToMove;
        }

        KeStackAttachProcess (&FromProcess->Pcb, &ApcState);

        MappedAddress = NULL;
        LockedMdlPages = FALSE;
        Moving = FALSE;
        ASSERT (Probing == FALSE);

        //
        // We may be touching a user's memory which could be invalid,
        // declare an exception handler.
        //

        try {

            //
            // Probe to make sure that the specified buffer is accessible in
            // the target process.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForRead (FromAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            //
            // Initialize MDL for request.
            //

            MmInitializeMdl (Mdl, (PVOID)InVa, AmountToMove);

            MmProbeAndLockPages (Mdl, PreviousMode, IoReadAccess);

            LockedMdlPages = TRUE;

            MappedAddress = MmMapLockedPagesSpecifyCache (Mdl,
                                                          KernelMode,
                                                          MmCached,
                                                          NULL,
                                                          FALSE,
                                                          HighPagePriority);

            if (MappedAddress == NULL) {
                MappingFailed = TRUE;
                ExRaiseStatus(STATUS_INSUFFICIENT_RESOURCES);
            }

            //
            // Deattach from the FromProcess and attach to the ToProcess.
            //

            KeUnstackDetachProcess (&ApcState);
            KeStackAttachProcess (&ToProcess->Pcb, &ApcState);

            //
            // Now operating in the context of the ToProcess.
            //
            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForWrite (ToAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            Moving = TRUE;
            RtlCopyMemory (OutVa, MappedAddress, AmountToMove);

        } except (MiGetExceptionInfo (GetExceptionInformation(),
                                      &ExceptionAddressConfirmed,
                                      &BadVa)) {


            //
            // If an exception occurs during the move operation or probe,
            // return the exception code as the status value.
            //

            KeUnstackDetachProcess (&ApcState);

            if (MappedAddress != NULL) {
                MmUnmapLockedPages (MappedAddress, Mdl);
            }
            if (LockedMdlPages == TRUE) {
                MmUnlockPages (Mdl);
            }

            if (GetExceptionCode() == STATUS_WORKING_SET_QUOTA) {
                return STATUS_WORKING_SET_QUOTA;
            }

            if ((Probing == TRUE) || (MappingFailed == TRUE)) {
                return GetExceptionCode();

            }

            //
            // If the failure occurred during the move operation, determine
            // which move failed, and calculate the number of bytes
            // actually moved.
            //

            *NumberOfBytesRead = BufferSize - LeftToMove;

            if (Moving == TRUE) {
                if (ExceptionAddressConfirmed == TRUE) {
                    *NumberOfBytesRead = (SIZE_T)((ULONG_PTR)BadVa - (ULONG_PTR)FromAddress);
                }
            }

            return STATUS_PARTIAL_COPY;
        }

        KeUnstackDetachProcess (&ApcState);

        MmUnmapLockedPages (MappedAddress, Mdl);
        MmUnlockPages (Mdl);

        LeftToMove -= AmountToMove;
        InVa = (PVOID)((ULONG_PTR)InVa + AmountToMove);
        OutVa = (PVOID)((ULONG_PTR)OutVa + AmountToMove);
    }

    //
    // Set number of bytes moved.
    //

    *NumberOfBytesRead = BufferSize;
    return STATUS_SUCCESS;
}

NTSTATUS
MiDoPoolCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address in the specified process
                   to be read.

     Buffer - Supplies the address of a buffer which receives the
              contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     PreviousMode - Supplies the previous processor mode.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    SIZE_T AmountToMove;
    LOGICAL ExceptionAddressConfirmed;
    ULONG_PTR BadVa;
    PEPROCESS CurrentProcess;
    LOGICAL Moving;
    LOGICAL Probing;
    CONST VOID *InVa;
    SIZE_T LeftToMove;
    SIZE_T MaximumMoved;
    PVOID OutVa;
    PVOID PoolArea;
    LONGLONG StackArray[COPY_STACK_SIZE];
    ULONG FreePool;

    PAGED_CODE();

    ASSERT (BufferSize != 0);

    //
    // Get the address of the current process object and initialize copy
    // parameters.
    //

    CurrentProcess = PsGetCurrentProcess();

    InVa = FromAddress;
    OutVa = ToAddress;

    //
    // Allocate non-paged memory to copy in and out of.
    //

    MaximumMoved = MAX_MOVE_SIZE;
    if (BufferSize <= MAX_MOVE_SIZE) {
        MaximumMoved = BufferSize;
    }

    FreePool = FALSE;
    if (BufferSize <= sizeof(StackArray)) {
        PoolArea = (PVOID)&StackArray[0];
    } else {
        do {
            PoolArea = ExAllocatePoolWithTag (NonPagedPool, MaximumMoved, 'wRmM');
            if (PoolArea != NULL) {
                FreePool = TRUE;
                break;
            }

            MaximumMoved = MaximumMoved >> 1;
            if (MaximumMoved <= sizeof(StackArray)) {
                PoolArea = (PVOID)&StackArray[0];
                break;
            }
        } while (TRUE);
    }

    //
    // Initializing BadVa & ExceptionAddressConfirmed is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BadVa = 0;
    ExceptionAddressConfirmed = FALSE;

    //
    // Copy the data into pool, then copy back into the ToProcess.
    //

    LeftToMove = BufferSize;
    AmountToMove = MaximumMoved;
    Probing = FALSE;

    while (LeftToMove > 0) {

        if (LeftToMove < AmountToMove) {

            //
            // Set to move the remaining bytes.
            //

            AmountToMove = LeftToMove;
        }

        KeStackAttachProcess (&FromProcess->Pcb, &ApcState);

        Moving = FALSE;
        ASSERT (Probing == FALSE);

        //
        // We may be touching a user's memory which could be invalid,
        // declare an exception handler.
        //

        try {

            //
            // Probe to make sure that the specified buffer is accessible in
            // the target process.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForRead (FromAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            RtlCopyMemory (PoolArea, InVa, AmountToMove);

            KeUnstackDetachProcess (&ApcState);

            KeStackAttachProcess (&ToProcess->Pcb, &ApcState);

            //
            // Now operating in the context of the ToProcess.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForWrite (ToAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            Moving = TRUE;

            RtlCopyMemory (OutVa, PoolArea, AmountToMove);

        } except (MiGetExceptionInfo (GetExceptionInformation(),
                                      &ExceptionAddressConfirmed,
                                      &BadVa)) {

            //
            // If an exception occurs during the move operation or probe,
            // return the exception code as the status value.
            //

            KeUnstackDetachProcess (&ApcState);

            if (FreePool) {
                ExFreePool (PoolArea);
            }
            if (Probing == TRUE) {
                return GetExceptionCode();

            }

            //
            // If the failure occurred during the move operation, determine
            // which move failed, and calculate the number of bytes
            // actually moved.
            //

            *NumberOfBytesRead = BufferSize - LeftToMove;

            if (Moving == TRUE) {

                //
                // The failure occurred writing the data.
                //

                if (ExceptionAddressConfirmed == TRUE) {
                    *NumberOfBytesRead = (SIZE_T)((ULONG_PTR)(BadVa - (ULONG_PTR)FromAddress));
                }

            }

            return STATUS_PARTIAL_COPY;
        }

        KeUnstackDetachProcess (&ApcState);

        LeftToMove -= AmountToMove;
        InVa = (PVOID)((ULONG_PTR)InVa + AmountToMove);
        OutVa = (PVOID)((ULONG_PTR)OutVa + AmountToMove);
    }

    if (FreePool) {
        ExFreePool (PoolArea);
    }

    //
    // Set number of bytes moved.
    //

    *NumberOfBytesRead = BufferSize;
    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\queryvm.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   queryvm.c

Abstract:

    This module contains the routines which implement the
    NtQueryVirtualMemory service.

--*/

#include "mi.h"

extern POBJECT_TYPE IoFileObjectType;

NTSTATUS
MiGetWorkingSetInfo (
    IN PMEMORY_WORKING_SET_INFORMATION WorkingSetInfo,
    IN SIZE_T Length,
    IN PEPROCESS Process
    );

NTSTATUS
MiGetWorkingSetInfoList (
    __in PMEMORY_WORKING_SET_EX_INFORMATION WorkingSetInfo,
    __in SIZE_T Length,
    __in PEPROCESS Process
    );

NTSTATUS
NtQueryVirtualMemory(
    __in HANDLE ProcessHandle,
    __in PVOID BaseAddress,
    __in MEMORY_INFORMATION_CLASS MemoryInformationClass,
    __out_bcount(MemoryInformationLength) PVOID MemoryInformation,
    __in SIZE_T MemoryInformationLength,
    __out_opt PSIZE_T ReturnLength
    )

/*++

Routine Description:

    This function provides the capability to determine the state,
    protection, and type of a region of pages within the virtual address
    space of the subject process.

    The state of the first page within the region is determined and then
    subsequent entries in the process address map are scanned from the
    base address upward until either the entire range of pages has been
    scanned or until a page with a nonmatching set of attributes is
    encountered. The region attributes, the length of the region of pages
    with matching attributes, and an appropriate status value are
    returned.

    If the entire region of pages does not have a matching set of
    attributes, then the returned length parameter value can be used to
    calculate the address and length of the region of pages that was not
    scanned.

Arguments:


    ProcessHandle - An open handle to a process object.

    BaseAddress - The base address of the region of pages to be
                  queried. This value is rounded down to the next host-page-
                  address boundary.

    MemoryInformationClass - The memory information class about which
                             to retrieve information.

    MemoryInformation - A pointer to a buffer that receives the specified
                        information.  The format and content of the buffer
                        depend on the specified information class.


        MemoryBasicInformation - Data type is PMEMORY_BASIC_INFORMATION.

            MEMORY_BASIC_INFORMATION Structure


            ULONG RegionSize - The size of the region in bytes beginning at
                               the base address in which all pages have
                               identical attributes.

            ULONG State - The state of the pages within the region.

                State Values

                MEM_COMMIT - The state of the pages within the region
                             is committed.

                MEM_FREE - The state of the pages within the region
                           is free.

                MEM_RESERVE - The state of the pages within the
                              region is reserved.

            ULONG Protect - The protection of the pages within the region.

                Protect Values

                PAGE_NOACCESS - No access to the region of pages is allowed.
                                An attempt to read, write, or execute within
                                the region results in an access violation.

                PAGE_EXECUTE - Execute access to the region of pages
                               is allowed. An attempt to read or write within
                               the region results in an access violation.

                PAGE_READONLY - Read-only and execute access to the region
                                of pages is allowed. An attempt to write within
                                the region results in an access violation.

                PAGE_READWRITE - Read, write, and execute access to the region
                                 of pages is allowed. If write access to the
                                 underlying section is allowed, then a single
                                 copy of the pages are shared. Otherwise,
                                 the pages are shared read-only/copy-on-write.

                PAGE_GUARD - Read, write, and execute access to the
                             region of pages is allowed; however, access to
                             the region causes a "guard region entered"
                             condition to be raised in the subject process.

                PAGE_NOCACHE - Disable the placement of committed
                               pages into the data cache.

                PAGE_WRITECOMBINE - Disable the placement of committed
                                    pages into the data cache, combine the
                                    writes as well.

            ULONG Type - The type of pages within the region.

                Type Values

                MEM_PRIVATE - The pages within the region are private.

                MEM_MAPPED - The pages within the region are mapped
                             into the view of a section.

                MEM_IMAGE - The pages within the region are mapped
                            into the view of an image section.

    MemoryInformationLength - Specifies the length in bytes of
                              the memory information buffer.

    ReturnLength - An optional pointer which, if specified, receives the
                   number of bytes placed in the process information buffer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    ULONG LocalReturnLength;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS TargetProcess;
    PETHREAD Thread;
    NTSTATUS Status;
    PMMVAD Vad;
    PVOID Va;
    PVOID NextVaToQuery;
    LOGICAL Found;
    SIZE_T TheRegionSize;
    ULONG NewProtect;
    ULONG NewState;
    PVOID FilePointer;
    ULONG_PTR BaseVpn;
    MEMORY_BASIC_INFORMATION Info;
    PMEMORY_BASIC_INFORMATION BasicInfo;
    LOGICAL Attached;
    LOGICAL Leaped;
    ULONG MemoryInformationLengthUlong;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;
    PVOID HighestVadAddress;
    PVOID HighestUserAddress;

    Found = FALSE;
    Leaped = TRUE;
    FilePointer = NULL;

    //
    // Make sure the user's buffer is large enough for the requested operation.
    //
    // Check argument validity.
    //

    switch (MemoryInformationClass) {
        case MemoryBasicInformation:
            if (MemoryInformationLength < sizeof(MEMORY_BASIC_INFORMATION)) {
                return STATUS_INFO_LENGTH_MISMATCH;
            }
            break;

        case MemoryWorkingSetInformation:
            if (MemoryInformationLength < sizeof(ULONG_PTR)) {
                return STATUS_INFO_LENGTH_MISMATCH;
            }
            break;

        case MemoryWorkingSetExInformation:
            if (MemoryInformationLength < sizeof (MEMORY_WORKING_SET_EX_INFORMATION)) {
                return STATUS_INFO_LENGTH_MISMATCH;
            }
            break;

        case MemoryMappedFilenameInformation:
            break;

        default:
            return STATUS_INVALID_INFO_CLASS;
    }

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        //
        // Check arguments.
        //

        try {

            ProbeForWrite(MemoryInformation,
                          MemoryInformationLength,
                          sizeof(ULONG_PTR));

            if (ARGUMENT_PRESENT(ReturnLength)) {
                ProbeForWriteUlong_ptr(ReturnLength);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER;
    }

    HighestUserAddress = MM_HIGHEST_USER_ADDRESS;
    HighestVadAddress  = (PCHAR) MM_HIGHEST_VAD_ADDRESS;

#if defined(_WIN64)

    if (ProcessHandle == NtCurrentProcess()) {
        TargetProcess = PsGetCurrentProcessByThread(CurrentThread);
    }
    else {
        Status = ObReferenceObjectByHandle (ProcessHandle,
                                            PROCESS_QUERY_INFORMATION,
                                            PsProcessType,
                                            PreviousMode,
                                            (PVOID *)&TargetProcess,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    //
    // If this is a wow64 process, then return the appropriate highest
    // user address depending on whether the process has been started with
    // a 2GB or a 4GB address space.
    //

    if (TargetProcess->Wow64Process != NULL) {

        if (TargetProcess->Flags & PS_PROCESS_FLAGS_WOW64_4GB_VA_SPACE) {
            HighestUserAddress = (PVOID) ((ULONG_PTR)_4gb - X64K - 1);
        }
        else {
            HighestUserAddress = (PVOID) ((ULONG_PTR)_2gb - X64K - 1);
        }

        HighestVadAddress  = (PCHAR)HighestUserAddress - X64K;

        if (BaseAddress > HighestUserAddress) {

            if (ProcessHandle != NtCurrentProcess()) {
                ObDereferenceObject (TargetProcess);
            }
            return STATUS_INVALID_PARAMETER;
        }
    }

#endif

    if ((BaseAddress > HighestVadAddress) ||
        (PAGE_ALIGN(BaseAddress) == (PVOID)MM_SHARED_USER_DATA_VA)) {

        //
        // Indicate a reserved area from this point on.
        //

        Status = STATUS_INVALID_ADDRESS;

        if (MemoryInformationClass == MemoryBasicInformation) {

            try {
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationBase =
                                      (PCHAR) HighestVadAddress + 1;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationProtect =
                                                                      PAGE_READONLY;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->BaseAddress =
                                                       PAGE_ALIGN(BaseAddress);
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->RegionSize =
                                    ((PCHAR)HighestUserAddress + 1) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->State = MEM_RESERVE;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Protect = PAGE_NOACCESS;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Type = MEM_PRIVATE;

                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
                }

                if (PAGE_ALIGN(BaseAddress) == (PVOID)MM_SHARED_USER_DATA_VA) {

                    //
                    // This is the page that is double mapped between
                    // user mode and kernel mode.
                    //

                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationBase =
                                (PVOID)MM_SHARED_USER_DATA_VA;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Protect =
                                                                 PAGE_READONLY;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->RegionSize =
                                                                 PAGE_SIZE;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->State =
                                                                 MEM_COMMIT;
                }

            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Just return success.
                //

                NOTHING;
            }

            Status = STATUS_SUCCESS;
        }
            
#if defined(_WIN64)
        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (TargetProcess);
        }
#endif
            
        return Status;
    }

#if !defined(_WIN64)

    if (ProcessHandle == NtCurrentProcess()) {
        TargetProcess = PsGetCurrentProcessByThread(CurrentThread);
    }
    else {
        Status = ObReferenceObjectByHandle (ProcessHandle,
                                            PROCESS_QUERY_INFORMATION,
                                            PsProcessType,
                                            PreviousMode,
                                            (PVOID *)&TargetProcess,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

#endif

    if (MemoryInformationClass == MemoryWorkingSetExInformation) {

        Status = MiGetWorkingSetInfoList (
                            (PMEMORY_WORKING_SET_EX_INFORMATION) MemoryInformation,
                            MemoryInformationLength,
                            TargetProcess);

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (TargetProcess);
        }

        //
        // If MiGetWorkingSetInfoList failed then inform the caller.
        //

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        try {

            if (ARGUMENT_PRESENT (ReturnLength)) {
                *ReturnLength = MemoryInformationLength;
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }

        return STATUS_SUCCESS;
    }

    if (MemoryInformationClass == MemoryWorkingSetInformation) {

        Status = MiGetWorkingSetInfo (
                            (PMEMORY_WORKING_SET_INFORMATION) MemoryInformation,
                            MemoryInformationLength,
                            TargetProcess);

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (TargetProcess);
        }

        //
        // If MiGetWorkingSetInfo failed then inform the caller.
        //

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        try {

            if (ARGUMENT_PRESENT(ReturnLength)) {
                *ReturnLength = ((((PMEMORY_WORKING_SET_INFORMATION)
                                    MemoryInformation)->NumberOfEntries - 1) *
                                        sizeof(ULONG_PTR)) +
                                        sizeof(MEMORY_WORKING_SET_INFORMATION);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
        }

        return STATUS_SUCCESS;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    //
    // Get working set mutex and block APCs.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject (TargetProcess);
        }
        return STATUS_PROCESS_IS_TERMINATING;
    }

    //
    // Locate the VAD that contains the base address or the VAD
    // which follows the base address.
    //

    if (TargetProcess->VadRoot.NumberGenericTableElements != 0) {

        Vad = (PMMVAD) TargetProcess->VadRoot.BalancedRoot.RightChild;
        BaseVpn = MI_VA_TO_VPN (BaseAddress);

        while (TRUE) {

            if (Vad == NULL) {
                break;
            }

            if ((BaseVpn >= Vad->StartingVpn) &&
                (BaseVpn <= Vad->EndingVpn)) {
                Found = TRUE;
                break;
            }

            if (BaseVpn < Vad->StartingVpn) {
                if (Vad->LeftChild == NULL) {
                    break;
                }
                Vad = Vad->LeftChild;
            }
            else {
                ASSERT (BaseVpn > Vad->EndingVpn);

                if (Vad->RightChild == NULL) {
                    break;
                }

                Vad = Vad->RightChild;
            }
        }
    }
    else {
        Vad = NULL;
        BaseVpn = 0;
    }

    if (!Found) {

        //
        // There is no virtual address allocated at the base
        // address.  Return the size of the hole starting at
        // the base address.
        //

        if (Vad == NULL) {
            TheRegionSize = (((PCHAR)HighestVadAddress + 1) - 
                                         (PCHAR)PAGE_ALIGN(BaseAddress));
        }
        else {
            if (Vad->StartingVpn < BaseVpn) {

                //
                // We are looking at the Vad which occupies the range
                // just before the desired range.  Get the next Vad.
                //

                Vad = MiGetNextVad (Vad);
                if (Vad == NULL) {
                    TheRegionSize = (((PCHAR)HighestVadAddress + 1) - 
                                                (PCHAR)PAGE_ALIGN(BaseAddress));
                }
                else {
                    TheRegionSize = (PCHAR)MI_VPN_TO_VA (Vad->StartingVpn) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
                }
            }
            else {
                TheRegionSize = (PCHAR)MI_VPN_TO_VA (Vad->StartingVpn) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
            }
        }

        UNLOCK_ADDRESS_SPACE (TargetProcess);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject (TargetProcess);
        }

        //
        // Establish an exception handler and write the information and
        // returned length.
        //

        if (MemoryInformationClass == MemoryBasicInformation) {
            BasicInfo = (PMEMORY_BASIC_INFORMATION) MemoryInformation;
            Found = FALSE;
            try {

                BasicInfo->AllocationBase = NULL;
                BasicInfo->AllocationProtect = 0;
                BasicInfo->BaseAddress = PAGE_ALIGN(BaseAddress);
                BasicInfo->RegionSize = TheRegionSize;
                BasicInfo->State = MEM_FREE;
                BasicInfo->Protect = PAGE_NOACCESS;
                BasicInfo->Type = 0;

                Found = TRUE;
                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
                }

            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Just return success if the BasicInfo was successfully
                // filled in.
                //
                
                if (Found == FALSE) {
                    return GetExceptionCode ();
                }
            }

            return STATUS_SUCCESS;
        }
        return STATUS_INVALID_ADDRESS;
    }

    //
    // Found a VAD.
    //
   
    Va = PAGE_ALIGN(BaseAddress);
    Info.BaseAddress = Va;
    Info.AllocationBase = MI_VPN_TO_VA (Vad->StartingVpn);
    Info.AllocationProtect = MI_CONVERT_FROM_PTE_PROTECTION (
                                             Vad->u.VadFlags.Protection);
    
    //
    // There is a page mapped at the base address.
    //
    
    if ((Vad->u.VadFlags.PrivateMemory) || 
        (Vad->u.VadFlags.VadType == VadRotatePhysical)) {

        Info.Type = MEM_PRIVATE;
    }
    else {
        if (Vad->u.VadFlags.VadType == VadImageMap) {
            Info.Type = MEM_IMAGE;
        }
        else {
            Info.Type = MEM_MAPPED;
        }

        if (MemoryInformationClass == MemoryMappedFilenameInformation) {

            if (Vad->ControlArea != NULL) {
                FilePointer = Vad->ControlArea->FilePointer;
            }
            if (FilePointer == NULL) {
                FilePointer = (PVOID)1;
            }
            else {
                ObReferenceObject (FilePointer);
            }
        } 
    }

    Thread = PsGetCurrentThread ();

    LOCK_WS_SHARED (Thread, TargetProcess);

    Info.State = MiQueryAddressState (Va,
                                      Vad,
                                      TargetProcess,
                                      &Info.Protect,
                                      &NextVaToQuery);

    Va = NextVaToQuery;

    while (MI_VA_TO_VPN (Va) <= Vad->EndingVpn) {

        NewState = MiQueryAddressState (Va,
                                        Vad,
                                        TargetProcess,
                                        &NewProtect,
                                        &NextVaToQuery);

        if ((NewState != Info.State) || (NewProtect != Info.Protect)) {

            //
            // The state for this address does not match, calculate
            // size and return.
            //

            Leaped = FALSE;
            break;
        }
        Va = NextVaToQuery;
    }

    UNLOCK_WS_SHARED (Thread, TargetProcess);    

    //
    // We may have aggressively leaped past the end of the VAD.  Shorten the
    // Va here if we did.
    //

    if (Leaped == TRUE) {
        Va = MI_VPN_TO_VA (Vad->EndingVpn + 1);
    }

    Info.RegionSize = ((PCHAR)Va - (PCHAR)Info.BaseAddress);

    //
    // A range has been found, release the mutexes, detach from the
    // target process and return the information.
    //

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        ObDereferenceObject (TargetProcess);
    }

    if (MemoryInformationClass == MemoryBasicInformation) {
        Found = FALSE;
        try {

            *(PMEMORY_BASIC_INFORMATION)MemoryInformation = Info;

            Found = TRUE;
            if (ARGUMENT_PRESENT(ReturnLength)) {
                *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // Just return success if the BasicInfo was successfully
            // filled in.
            //
                
            if (Found == FALSE) {
                return GetExceptionCode ();
            }
        }
        return STATUS_SUCCESS;
    }

    //
    // Try to return the name of the file that is mapped.
    //

    if (FilePointer == NULL) {
        return STATUS_INVALID_ADDRESS;
    }

    if (FilePointer == (PVOID)1) {
        return STATUS_FILE_INVALID;
    }

    MemoryInformationLengthUlong = (ULONG)MemoryInformationLength;

    if ((SIZE_T)MemoryInformationLengthUlong < MemoryInformationLength) {
        return STATUS_INVALID_PARAMETER_5;
    }
    
    //
    // We have a referenced pointer to the file.  Call ObQueryNameString
    // and get the file name.
    //

    Status = ObQueryNameString (FilePointer,
                                (POBJECT_NAME_INFORMATION) MemoryInformation,
                                 MemoryInformationLengthUlong,
                                 &LocalReturnLength);

    ObDereferenceObject (FilePointer);

    if (ARGUMENT_PRESENT (ReturnLength)) {
        try {
            *ReturnLength = LocalReturnLength;
        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode ();
        }
    }

    return Status;
}


ULONG
MiQueryAddressState (
    IN PVOID Va,
    IN PMMVAD Vad,
    IN PEPROCESS TargetProcess,
    OUT PULONG ReturnedProtect,
    OUT PVOID *NextVaToQuery
    )

/*++

Routine Description:


Arguments:

Return Value:

    Returns the state (MEM_COMMIT, MEM_RESERVE, MEM_PRIVATE).

Environment:

    Kernel mode.  Address creation mutex held (exclusive) and working
    set pushlock held (shared).

    This routine may release and reacquire the working set pushlock, callers
    must be prepared to handle this.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PETHREAD Thread;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    MMPTE PteContents;
    MMPTE CapturedProtoPte;
    PMMPTE ProtoPte;
    LOGICAL PteIsZero;
    ULONG State;
    ULONG Protect;
    ULONG Waited;
    LOGICAL PteDetected;
    PVOID NextVa;
    PFN_NUMBER PageFrameIndex;

    State = MEM_RESERVE;
    Protect = 0;

    PointerPxe = MiGetPxeAddress (Va);
    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);

    ASSERT ((Vad->StartingVpn <= MI_VA_TO_VPN (Va)) &&
            (Vad->EndingVpn >= MI_VA_TO_VPN (Va)));

    PteIsZero = TRUE;
    PteDetected = FALSE;

    *NextVaToQuery = (PVOID)((PCHAR)Va + PAGE_SIZE);

    do {

        if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                         TargetProcess,
                                         MM_NOIRQL,
                                         &Waited)) {

#if (_MI_PAGING_LEVELS >= 4)
            NextVa = MiGetVirtualAddressMappedByPte (PointerPxe + 1);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
#endif
            break;
        }
    
#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                         TargetProcess,
                                         MM_NOIRQL,
                                         &Waited)) {
#if (_MI_PAGING_LEVELS >= 3)
            NextVa = MiGetVirtualAddressMappedByPte (PointerPpe + 1);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
#endif
            break;
        }
    
#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

        if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                         TargetProcess,
                                         MM_NOIRQL,
                                         &Waited)) {
            NextVa = MiGetVirtualAddressMappedByPte (PointerPde + 1);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
            break;
        }

        if (Waited == 0) {
            PteDetected = TRUE;
        }

    } while (Waited != 0);

    if (PteDetected == TRUE) {

        //
        // A PTE exists at this address, see if it is zero.
        //

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {

            *ReturnedProtect = MI_CONVERT_FROM_PTE_PROTECTION (Vad->u.VadFlags.Protection);
            NextVa = MiGetVirtualAddressMappedByPte (PointerPde + 1);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
            return MEM_COMMIT;
        }

        PteContents = *PointerPte;

        if (PteContents.u.Long != 0) {

            PteIsZero = FALSE;

            //
            // There is a non-zero PTE at this address, use
            // it to build the information block.
            //

            if (MiIsPteDecommittedPage (&PteContents)) {
                ASSERT (Protect == 0);
                ASSERT (State == MEM_RESERVE);
            }
            else {
                State = MEM_COMMIT;
                if (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory) {

                    //
                    // Physical mapping, there is no corresponding
                    // PFN element to get the page protection from.
                    //

                    Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                             Vad->u.VadFlags.Protection);
                }
                else if ((Vad->u.VadFlags.VadType == VadRotatePhysical) &&
                         (PteContents.u.Hard.Valid)) {

                    ProtoPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (Va));

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                    if ((!MI_IS_PFN (PageFrameIndex)) ||
                        (Pfn1->PteAddress != ProtoPte)) {

                        //
                        // This address in the view is currently pointing at
                        // the frame buffer (video RAM or AGP), protection is
                        // in the prototype (which may itself be paged out - so
                        // release the process working set pushlock before
                        // accessing it).
                        //

                        Thread = PsGetCurrentThread ();
        
                        UNLOCK_WS_SHARED (Thread, TargetProcess);

                        CapturedProtoPte = *ProtoPte;

                        ASSERT (CapturedProtoPte.u.Hard.Valid == 0);
                        ASSERT (CapturedProtoPte.u.Soft.Prototype == 0);

                        //
                        // PTE is either demand zero, pagefile or transition,
                        // in all cases protection is in the prototype PTE.
                        //

                        Protect = MI_CONVERT_FROM_PTE_PROTECTION (CapturedProtoPte.u.Soft.Protection);

                        LOCK_WS_SHARED (Thread, TargetProcess);
                    }
                    else {
                        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
                        Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                    Pfn1->OriginalPte.u.Soft.Protection);
                    }
                }
                else if (Vad->u.VadFlags.VadType == VadAwe) {

                    //
                    // This is an AWE frame - the original PTE field in the PFN
                    // actually contains the AweReferenceCount.  These pages are
                    // either noaccess, readonly or readwrite.
                    //

                    if (PteContents.u.Hard.Owner == MI_PTE_OWNER_KERNEL) {
                        Protect = PAGE_NOACCESS;
                    }
                    else if (PteContents.u.Hard.Write == 1) {
                        Protect = PAGE_READWRITE;
                    }
                    else {
                        Protect = PAGE_READONLY;
                    }
                    State = MEM_COMMIT;
                }
                else {

                    Protect = MiGetPageProtection (PointerPte);

                    //
                    // The PointerPte contents may have changed if
                    // MiGetPageProtection had to release the working set
                    // pushlock, thus we snapped it prior and can continue
                    // to use the old copy regardless.  Note also that the
                    // page table page containing PointerPte may now have
                    // been paged out as well.
                    //

                    if ((PteContents.u.Soft.Valid == 0) &&
                        (PteContents.u.Soft.Prototype == 1) &&
                        (Vad->u.VadFlags.PrivateMemory == 0) &&
                        (Vad->ControlArea != NULL)) {

                        //
                        // Make sure the prototype PTE is committed.  Note
                        // that the prototype PTE itself may be paged out,
                        // thus the process working set pushlock must be
                        // released before accessing it.
                        //

                        ProtoPte = MiGetProtoPteAddress (Vad,
                                                         MI_VA_TO_VPN (Va));

                        if (ProtoPte != NULL) {
                            Thread = PsGetCurrentThread ();
                            UNLOCK_WS_SHARED (Thread, TargetProcess);
                            CapturedProtoPte = *ProtoPte;
                            LOCK_WS_SHARED (Thread, TargetProcess);
                        }
                        else {
                            CapturedProtoPte.u.Long = 0;
                        }

                        if (CapturedProtoPte.u.Long == 0) {
                            State = MEM_RESERVE;
                            Protect = 0;
                        }
                    }
                }
            }
        }
    }

    if (PteIsZero) {

        //
        // There is no PDE at this address, the template from
        // the VAD supplies the information unless the VAD is
        // for an image file.  For image files the individual
        // protection is on the prototype PTE.
        //

        //
        // Get the default protection information.
        //

        State = MEM_RESERVE;
        Protect = 0;

        if (Vad->u.VadFlags.VadType == VadAwe) {
            NOTHING;
        }
        else if (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory) {

            //
            // Must be banked memory, just return reserved.
            //

            NOTHING;
        }
        else if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                 (Vad->ControlArea != NULL)) {

            //
            // This VAD refers to a section.  Even though the PTE is
            // zero, the actual page may be committed in the section.
            //

            *NextVaToQuery = (PVOID)((PCHAR)Va + PAGE_SIZE);

            ProtoPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (Va));

            if (ProtoPte != NULL) {

                Thread = PsGetCurrentThread ();

                UNLOCK_WS_SHARED (Thread, TargetProcess);

                CapturedProtoPte = *ProtoPte;

                if (CapturedProtoPte.u.Long != 0) {
                    State = MEM_COMMIT;
    
                    if (Vad->u.VadFlags.VadType != VadImageMap) {
                        Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                                  Vad->u.VadFlags.Protection);
                    }
                    else {
    
                        //
                        // This is an image file, the protection is in the
                        // prototype PTE.
                        //
    
                        if (CapturedProtoPte.u.Hard.Valid == 0) {
                            Protect = MI_CONVERT_FROM_PTE_PROTECTION (CapturedProtoPte.u.Soft.Protection);
                        }
                        else {

                            //
                            // The prototype PTE is valid but not in this
                            // process (at least not before we released the
                            // working set pushlock above).  So the PFN's
                            // OriginalPte field contains the correct
                            // protection value, but it may get trimmed at any
                            // time.  Acquire the PFN lock so we can examine
                            // it safely.
                            //

                            PointerPde = MiGetPteAddress (ProtoPte);

                            LOCK_PFN (OldIrql);

                            if (PointerPde->u.Hard.Valid == 0) {
                                MiMakeSystemAddressValidPfn (ProtoPte, OldIrql);
                            }

                            CapturedProtoPte = *ProtoPte;

                            ASSERT (CapturedProtoPte.u.Long != 0);

                            if (CapturedProtoPte.u.Hard.Valid) {
                                Pfn1 = MI_PFN_ELEMENT (CapturedProtoPte.u.Hard.PageFrameNumber);

                                Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                          Pfn1->OriginalPte.u.Soft.Protection);
                            }
                            else {
                                Protect = MI_CONVERT_FROM_PTE_PROTECTION (CapturedProtoPte.u.Soft.Protection);
                            }
                            UNLOCK_PFN (OldIrql);
                        }
                    }
                }

                LOCK_WS_SHARED (Thread, TargetProcess);
            }
        }
        else {

            //
            // Get the protection from the corresponding VAD.
            //

            if (Vad->u.VadFlags.MemCommit) {
                State = MEM_COMMIT;
                Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                            Vad->u.VadFlags.Protection);
            }
        }
    }

    *ReturnedProtect = Protect;
    return State;
}


NTSTATUS
MiGetWorkingSetInfo (
    IN PMEMORY_WORKING_SET_INFORMATION WorkingSetInfo,
    IN SIZE_T Length,
    IN PEPROCESS Process
    )

{
    PMDL Mdl;
    PMEMORY_WORKING_SET_INFORMATION Info;
    PMEMORY_WORKING_SET_BLOCK Entry;
#if DBG
    PMEMORY_WORKING_SET_BLOCK LastEntry;
#endif
    PMMWSLE Wsle;
    PMMWSLE LastWsle;
    WSLE_NUMBER WsSize;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    NTSTATUS status;
    LOGICAL Attached;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;

    //
    // Allocate an MDL to map the request.
    //

    Mdl = ExAllocatePoolWithTag (NonPagedPool,
                                 sizeof(MDL) + sizeof(PFN_NUMBER) +
                                     BYTES_TO_PAGES (Length) * sizeof(PFN_NUMBER),
                                 '  mM');

    if (Mdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initialize the MDL for the request.
    //

    MmInitializeMdl(Mdl, WorkingSetInfo, Length);

    CurrentThread = PsGetCurrentThread ();

    try {
        MmProbeAndLockPages (Mdl,
                             KeGetPreviousModeByThread (&CurrentThread->Tcb),
                             IoWriteAccess);

    } except (EXCEPTION_EXECUTE_HANDLER) {

        ExFreePool (Mdl);
        return GetExceptionCode();
    }

    Info = MmGetSystemAddressForMdlSafe (Mdl, NormalPagePriority);

    if (Info == NULL) {
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (PsGetCurrentProcessByThread (CurrentThread) != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    status = STATUS_SUCCESS;

    LOCK_WS_SHARED (CurrentThread, Process);

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
    }
    else {
        WsSize = Process->Vm.WorkingSetSize;
        ASSERT (WsSize != 0);
        Info->NumberOfEntries = WsSize;
        if (sizeof(MEMORY_WORKING_SET_INFORMATION) + (WsSize-1) * sizeof(ULONG_PTR) > Length) {
            status = STATUS_INFO_LENGTH_MISMATCH;
        }
    }

    if (!NT_SUCCESS(status)) {

        UNLOCK_WS_SHARED (CurrentThread, Process);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return status;
    }

    Wsle = MmWsle;
    LastWsle = &MmWsle[MmWorkingSetList->LastEntry];
    Entry = &Info->WorkingSetInfo[0];

#if DBG
    LastEntry = (PMEMORY_WORKING_SET_BLOCK)(
                            (PCHAR)Info + (Length & (~(sizeof(ULONG_PTR) - 1))));
#endif

    do {
        if (Wsle->u1.e1.Valid == 1) {
            Entry->VirtualPage = Wsle->u1.e1.VirtualPageNumber;
            PointerPte = MiGetPteAddress (Wsle->u1.VirtualAddress);
            ASSERT (PointerPte->u.Hard.Valid == 1);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

#if defined(MI_MULTINODE)
            Entry->Node = Pfn1->u3.e1.PageColor;
#else
            Entry->Node = 0;
#endif
            Entry->Shared = Pfn1->u3.e1.PrototypePte;
            if (Pfn1->u3.e1.PrototypePte == 0) {
                Entry->ShareCount = 0;
                Entry->Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn1->OriginalPte);
            }
            else {
                if (Pfn1->u2.ShareCount <= 7) {
                    Entry->ShareCount = Pfn1->u2.ShareCount;
                }
                else {
                    Entry->ShareCount = 7;
                }
                if (Wsle->u1.e1.Protection == MM_ZERO_ACCESS) {
                    Entry->Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn1->OriginalPte);
                }
                else {
                    Entry->Protection = Wsle->u1.e1.Protection;
                }
            }
            Entry += 1;
        }
        Wsle += 1;
#if DBG
        ASSERT ((Entry < LastEntry) || (Wsle > LastWsle));
#endif
    } while (Wsle <= LastWsle);

    UNLOCK_WS_SHARED (CurrentThread, Process);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    MmUnlockPages (Mdl);
    ExFreePool (Mdl);
    return STATUS_SUCCESS;
}

NTSTATUS
MiGetWorkingSetInfoList (
    IN PMEMORY_WORKING_SET_EX_INFORMATION WorkingSetInfo,
    IN SIZE_T Length,
    IN PEPROCESS Process
    )
{
    PMDL Mdl;
    PVOID VirtualAddress;
    TABLE_SEARCH_RESULT SearchResult;
    PMI_PHYSICAL_VIEW PhysicalView;
    PMEMORY_WORKING_SET_EX_INFORMATION Info;
    MEMORY_WORKING_SET_EX_INFORMATION Entry;
    PMMVAD Vad;
    PMMWSLE Wsle;
    SIZE_T WsSize;
    WSLE_NUMBER WorkingSetIndex;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    NTSTATUS status;
    LOGICAL Attached;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;

    WsSize = Length / sizeof (MEMORY_WORKING_SET_EX_INFORMATION);

    if (WsSize == 0) {
        return STATUS_INFO_LENGTH_MISMATCH;
    }

    //
    // Allocate an MDL to map the request.
    //

    Mdl = ExAllocatePoolWithTag (NonPagedPool,
                                 sizeof (MDL) + sizeof (PFN_NUMBER) +
                                     BYTES_TO_PAGES (Length) * sizeof (PFN_NUMBER),
                                 '  mM');

    if (Mdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initialize the MDL for the request.
    //

    MmInitializeMdl (Mdl, WorkingSetInfo, Length);

    CurrentThread = PsGetCurrentThread ();

    try {
        MmProbeAndLockPages (Mdl,
                             KeGetPreviousModeByThread (&CurrentThread->Tcb),
                             IoWriteAccess);

    } except (EXCEPTION_EXECUTE_HANDLER) {

        ExFreePool (Mdl);
        return GetExceptionCode();
    }

    Info = MmGetSystemAddressForMdlSafe (Mdl, NormalPagePriority);

    if (Info == NULL) {
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (PsGetCurrentProcessByThread (CurrentThread) != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    status = STATUS_SUCCESS;

    LOCK_WS_SHARED (CurrentThread, Process);

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {

        UNLOCK_WS_SHARED (CurrentThread, Process);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return STATUS_PROCESS_IS_TERMINATING;
    }

    for ( ; WsSize != 0; Info->u1.Long = Entry.u1.Long, Info += 1, WsSize -= 1) {
        Entry.u1.Long = 0;

        VirtualAddress = Info->VirtualAddress;

        if (VirtualAddress > MM_HIGHEST_USER_ADDRESS) {
            continue;
        }

        if (Process->PhysicalVadRoot != NULL) {

            //
            // This process has a \Device\PhysicalMemory VAD so it must be
            // checked to see if the current address resides in it since
            // the PFN cannot be safely examined if it is one of these.
            //

            SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                               MI_VA_TO_VPN (VirtualAddress),
                                               (PMMADDRESS_NODE *) &PhysicalView);
            if ((SearchResult == TableFoundNode) &&
                (PhysicalView->VadType == VadDevicePhysicalMemory)) {

                ASSERT ((ULONG)PhysicalView->Vad->u.VadFlags.VadType == (ULONG)PhysicalView->VadType);

                //
                // The VA lies within a device physical memory VAD.
                // Extract the protection from the VAD now.
                //

                Entry.u1.VirtualAttributes.Valid = 1;
                Entry.u1.VirtualAttributes.Locked = 1;

                Entry.u1.VirtualAttributes.Win32Protection =
                            MI_CONVERT_FROM_PTE_PROTECTION (
                                     PhysicalView->Vad->u.VadFlags.Protection);

                continue;
            }
        }

        PointerPxe = MiGetPxeAddress (VirtualAddress);
        PointerPpe = MiGetPpeAddress (VirtualAddress);
        PointerPde = MiGetPdeAddress (VirtualAddress);
        PointerPte = MiGetPteAddress (VirtualAddress);

        if (
#if (_MI_PAGING_LEVELS>=4)
           (PointerPxe->u.Hard.Valid == 0) ||
#endif
#if (_MI_PAGING_LEVELS>=3)
           (PointerPpe->u.Hard.Valid == 0) ||
#endif
           (PointerPde->u.Hard.Valid == 0)) {

            continue;
        }

        PteContents = *PointerPde;

        if (MI_PDE_MAPS_LARGE_PAGE (&PteContents)) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents) + MiGetPteOffset (VirtualAddress);
            Entry.u1.VirtualAttributes.Valid = 1;
            Entry.u1.VirtualAttributes.LargePage = 1;
            SATISFY_OVERZEALOUS_COMPILER (PteContents.u.Long = 0);
        }
        else {

            //
            // Snap the contents since AWE PTEs may be changing underneath us.
            //

            PteContents = *PointerPte;

            if (PteContents.u.Hard.Valid) {
                Entry.u1.VirtualAttributes.Valid = 1;
            }
            else {
                continue;
            }

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        }

        //
        // We have a valid address and the corresponding PFN.  Extract the
        // interesting information now.
        //

        ASSERT (MI_IS_PFN (PageFrameIndex));

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if (Pfn1->u3.e1.PrototypePte) {
            Entry.u1.VirtualAttributes.Shared = 1;
        }

        Entry.u1.VirtualAttributes.Node = Pfn1->u3.e1.PageColor;

        Entry.u1.VirtualAttributes.Priority = MI_GET_PFN_PRIORITY (Pfn1);

        //
        // AWE, large page and \Device\PhysicalMemory entries have no WSLE
        // and so must be handled carefully.
        //

        if (Entry.u1.VirtualAttributes.LargePage == 1) {

            Vad = MiLocateAddress (VirtualAddress);

            ASSERT (Vad != NULL);

            Entry.u1.VirtualAttributes.Win32Protection = MI_CONVERT_FROM_PTE_PROTECTION (Vad->u.VadFlags.Protection);

            Entry.u1.VirtualAttributes.Locked = 1;

            ASSERT (Entry.u1.VirtualAttributes.ShareCount == 0);

            if (Pfn1->u3.e1.PrototypePte) {

                ULONG_PTR ShareCount;

                ShareCount = Pfn1->u2.ShareCount;

                if (Vad->u.VadFlags.VadType == VadLargePageSection) {

                    //
                    // Prior to Longhorn, the pagefile-backed large section
                    // code would increment the share count internally and
                    // this not something we want to return back to applications
                    // because it would confuse them so explicitly set it
                    // here for these types of sections.  In Longhorn the share
                    // count is always 1 for these so no setting at this
                    // location in the code is needed.
                    //

                    ShareCount -= 1;
                }

                if (ShareCount > 7) {
                    ShareCount = 7;
                }

                Entry.u1.VirtualAttributes.ShareCount = ShareCount;
            }

            continue;
        }

        if (Pfn1->u4.AweAllocation == 1) {

            //
            // AWE page.
            //

            if (PteContents.u.Hard.Owner == MI_PTE_OWNER_USER) {
                if (PteContents.u.Hard.Write) {
                    Entry.u1.VirtualAttributes.Win32Protection = PAGE_READWRITE;
                }
                else {
                    Entry.u1.VirtualAttributes.Win32Protection = PAGE_READONLY;
                }
            }
            else {
                Entry.u1.VirtualAttributes.Win32Protection = PAGE_NOACCESS;
            }

            Entry.u1.VirtualAttributes.Locked = 1;
            ASSERT (Entry.u1.VirtualAttributes.ShareCount == 0);
            continue;
        }

        ASSERT (Pfn1->u1.WsIndex != 0);

        WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                        MmWorkingSetList,
                                        Pfn1->u1.WsIndex,
                                        FALSE);

        Wsle = MmWsle + WorkingSetIndex;

        ASSERT (Wsle->u1.e1.Valid == 1);

        if (WorkingSetIndex < MmWorkingSetList->FirstDynamic) {
            Entry.u1.VirtualAttributes.Locked = 1;
        }

        if (Pfn1->u3.e1.PrototypePte == 0) {
            Entry.u1.VirtualAttributes.ShareCount = 0;
            Entry.u1.VirtualAttributes.Win32Protection = MI_CONVERT_FROM_PTE_PROTECTION (MI_GET_PROTECTION_FROM_SOFT_PTE (&Pfn1->OriginalPte));
        }
        else {
            if (Pfn1->u2.ShareCount <= 7) {
                Entry.u1.VirtualAttributes.ShareCount = Pfn1->u2.ShareCount;
            }
            else {
                Entry.u1.VirtualAttributes.ShareCount = 7;
            }
            if (Wsle->u1.e1.Protection == MM_ZERO_ACCESS) {
                Entry.u1.VirtualAttributes.Win32Protection = MI_CONVERT_FROM_PTE_PROTECTION (MI_GET_PROTECTION_FROM_SOFT_PTE (&Pfn1->OriginalPte));
            }
            else {
                Entry.u1.VirtualAttributes.Win32Protection = MI_CONVERT_FROM_PTE_PROTECTION (Wsle->u1.e1.Protection);
            }
        }
    }

    UNLOCK_WS_SHARED (CurrentThread, Process);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    MmUnlockPages (Mdl);
    ExFreePool (Mdl);
    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\sectsup.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   sectsup.c

Abstract:

    This module contains the routines which implement the
    section object.

--*/


#include "mi.h"

VOID
FASTCALL
MiRemoveBasedSection (
    IN PSECTION Section
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiSectionInitialization)
#pragma alloc_text(PAGE,MiRemoveBasedSection)
#pragma alloc_text(PAGE,MmGetFileNameForSection)
#pragma alloc_text(PAGE,MmGetFileNameForAddress)
#pragma alloc_text(PAGE,MiSectionDelete)
#pragma alloc_text(PAGE,MiInsertBasedSection)
#pragma alloc_text(PAGE,MiGetEventCounter)
#pragma alloc_text(PAGE,MiFreeEventCounter)
#pragma alloc_text(PAGE,MmGetFileObjectForSection)
#endif

#define MI_LOG_DEREF_INFO(a,b,c)

ULONG   MmUnusedSegmentForceFree;

ULONG   MiSubsectionsProcessed;
ULONG   MiSubsectionActions;

SIZE_T MmSharedCommit = 0;
extern const ULONG MMCONTROL;

extern MMPAGE_FILE_EXPANSION MiPageFileContract;

//
// Define segment dereference thread wait object types.
//

typedef enum _SEGMENT_DEREFERENCE_OBJECT {
    SegmentDereference,
    UsedSegmentCleanup,
    SegMaximumObject
    } BALANCE_OBJECT;

extern POBJECT_TYPE IoFileObjectType;

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("INITCONST")
#endif
const GENERIC_MAPPING MiSectionMapping = {
    STANDARD_RIGHTS_READ |
        SECTION_QUERY | SECTION_MAP_READ,
    STANDARD_RIGHTS_WRITE |
        SECTION_MAP_WRITE,
    STANDARD_RIGHTS_EXECUTE |
        SECTION_MAP_EXECUTE,
    SECTION_ALL_ACCESS
};
#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg()
#endif

VOID
MiRemoveUnusedSegments (
    VOID
    );


VOID
FASTCALL
MiInsertBasedSection (
    IN PSECTION Section
    )

/*++

Routine Description:

    This function inserts a virtual address descriptor into the tree and
    reorders the splay tree as appropriate.

Arguments:

    Section - Supplies a pointer to a based section.

Return Value:

    None.

Environment:

    Must be holding the section based mutex.

--*/

{
    ASSERT (Section->Address.EndingVpn >= Section->Address.StartingVpn);

    MiInsertNode (&Section->Address, &MmSectionBasedRoot);

    return;
}


VOID
FASTCALL
MiRemoveBasedSection (
    IN PSECTION Section
    )

/*++

Routine Description:

    This function removes a based section from the tree.

Arguments:

    Section - pointer to the based section object to remove.

Return Value:

    None.

Environment:

    Must be holding the section based mutex.

--*/

{
    MiRemoveNode (&Section->Address, &MmSectionBasedRoot);

    return;
}


VOID
MiSegmentDelete (
    PSEGMENT Segment
    )

/*++

Routine Description:

    This routine is called whenever the last reference to a segment object
    has been removed.  This routine releases the pool allocated for the
    prototype PTEs and performs consistency checks on those PTEs.

    For segments which map files, the file object is dereferenced.

    Note, that for a segment which maps a file, no PTEs may be valid
    or transition.
    
    A segment which is backed by a paging file may have transition pages,
    but no valid pages (there can be no PTEs which refer to the segment)
    unless it is a large page segment.

    If it is a large page segment that backs the paging file, then all
    the prototype PTEs are expected to be valid, but their sharecounts
    must be exactly 1.

Arguments:

    Segment - a pointer to the segment structure.

Return Value:

    None.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PteForProto;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PEVENT_COUNTER Event;
    MMPTE PteContents;
    PSUBSECTION Subsection;
    PSUBSECTION NextSubsection;
    PMSUBSECTION MappedSubsection;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    SIZE_T NumberOfCommittedPages;
    SEGMENT_FLAGS SegmentFlags;

    //
    // Capture the segment flags locally so we can check them while the PFN
    // lock is held below (the segment is pageable).
    //

    SegmentFlags = Segment->SegmentFlags;

    ControlArea = Segment->ControlArea;

    ASSERT (ControlArea->u.Flags.BeingDeleted == 1);

    ASSERT (ControlArea->WritableUserReferences == 0);

    LOCK_PFN (OldIrql);
    if (ControlArea->DereferenceList.Flink != NULL) {

        //
        // Remove this from the list of unused segments.  The dereference
        // segment thread cannot be processing any subsections from this
        // control area right now because it bumps the NumberOfMappedViews
        // for the control area prior to releasing the PFN lock and it checks
        // for BeingDeleted.
        //

        ExAcquireSpinLockAtDpcLevel (&MmDereferenceSegmentHeader.Lock);
        RemoveEntryList (&ControlArea->DereferenceList);

        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

        ExReleaseSpinLockFromDpcLevel (&MmDereferenceSegmentHeader.Lock);
    }
    UNLOCK_PFN (OldIrql);

    if ((ControlArea->u.Flags.Image) || (ControlArea->u.Flags.File)) {

        //
        // Unload kernel debugger symbols if any were loaded.
        //

        if (ControlArea->u.Flags.DebugSymbolsLoaded != 0) {

            //
            //  TEMP TEMP TEMP rip out when debugger converted
            //

            ANSI_STRING AnsiName;
            NTSTATUS Status;

            Status = RtlUnicodeStringToAnsiString( &AnsiName,
                                                   (PUNICODE_STRING)&Segment->ControlArea->FilePointer->FileName,
                                                   TRUE );

            if (NT_SUCCESS( Status)) {
                DbgUnLoadImageSymbols( &AnsiName,
                                       Segment->BasedAddress,
                                       (ULONG_PTR)PsGetCurrentProcess());
                RtlFreeAnsiString( &AnsiName );
            }
            LOCK_PFN (OldIrql);
            ControlArea->u.Flags.DebugSymbolsLoaded = 0;
        }
        else {
            LOCK_PFN (OldIrql);
        }

        //
        // Signal any threads waiting on the deletion event.
        //

        Event = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;

        UNLOCK_PFN (OldIrql);

        if (Event != NULL) {
            KeSetEvent (&Event->Event, 0, FALSE);
        }

        //
        // Clear the segment context and dereference the file object
        // for this Segment.
        //
        // If the segment was deleted due to a name collision at insertion
        // we don't want to dereference the file pointer.
        //

        if (ControlArea->u.Flags.BeingCreated == FALSE) {

#if DBG
            if (ControlArea->u.Flags.Image == 1) {
                ASSERT (ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject != (PVOID)ControlArea);
            }
            else {
                ASSERT (ControlArea->FilePointer->SectionObjectPointer->DataSectionObject != (PVOID)ControlArea);
            }
#endif

            ObDereferenceObject (ControlArea->FilePointer);
        }

        //
        // If there have been committed pages in this segment, adjust
        // the total commit count.
        //

        if (ControlArea->u.Flags.Image == 0) {

            //
            // This is a mapped data file.  None of the prototype
            // PTEs may be referencing a physical page (valid or transition).
            //

            if (ControlArea->u.Flags.Rom == 0) {
                Subsection = (PSUBSECTION)(ControlArea + 1);
            }
            else {
                Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
            }

#if DBG
            if (Subsection->SubsectionBase != NULL) {
                PointerPte = Subsection->SubsectionBase;
                LastPte = PointerPte + Segment->NonExtendedPtes;

                while (PointerPte < LastPte) {

                    //
                    // Prototype PTEs for segments backed by paging file are
                    // either in demand zero, page file format, or transition.
                    //

                    ASSERT (PointerPte->u.Hard.Valid == 0);
                    ASSERT ((PointerPte->u.Soft.Prototype == 1) ||
                            (PointerPte->u.Long == 0));
                    PointerPte += 1;
                }
            }
#endif

            //
            // Deallocate the control area and subsections.
            //

            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

            if (ControlArea->FilePointer != NULL) {

                MappedSubsection = (PMSUBSECTION) Subsection;

                LOCK_PFN (OldIrql);

                while (MappedSubsection != NULL) {

                    if (MappedSubsection->DereferenceList.Flink != NULL) {

                        //
                        // Remove this from the list of unused subsections.
                        //

                        RemoveEntryList (&MappedSubsection->DereferenceList);

                        MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);
                    }
                    MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;
                }
                UNLOCK_PFN (OldIrql);

                if (Subsection->SubsectionBase != NULL) {
                    ExFreePool (Subsection->SubsectionBase);
                }
            }

            Subsection = Subsection->NextSubsection;

            while (Subsection != NULL) {
                if (Subsection->SubsectionBase != NULL) {
                    ExFreePool (Subsection->SubsectionBase);
                }
                NextSubsection = Subsection->NextSubsection;
                ExFreePool (Subsection);
                Subsection = NextSubsection;
            }

            NumberOfCommittedPages = Segment->NumberOfCommittedPages;

            if (NumberOfCommittedPages != 0) {
                MiReturnCommitment (NumberOfCommittedPages);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE1,
                                 NumberOfCommittedPages);

                InterlockedExchangeAddSizeT (&MmSharedCommit, 0-NumberOfCommittedPages);
            }

            ExFreePool (ControlArea);
            ExFreePool (Segment);

            //
            // The file mapped Segment object is now deleted.
            //

            return;
        }
    }

    //
    // This is a page file backed or image segment.  The segment is being
    // deleted, remove all references to the paging file and physical memory.
    //
    // The PFN lock is required for deallocating pages from a paging
    // file and for deleting transition PTEs.
    //

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PointerPte = Subsection->SubsectionBase;
    LastPte = PointerPte + Segment->NonExtendedPtes;
    PteForProto = MiGetPteAddress (PointerPte);

    //
    // Access the first prototype PTE to try and make it resident before
    // acquiring the PFN lock.  This is purely an optimization to reduce
    // PFN lock hold duration.
    //

    *(volatile MMPTE *) PointerPte;

    LOCK_PFN (OldIrql);

    if (PteForProto->u.Hard.Valid == 0) {
        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
    }

    while (PointerPte < LastPte) {

        if ((MiIsPteOnPdeBoundary (PointerPte)) &&
            (PointerPte != Subsection->SubsectionBase)) {

            //
            // Briefly release and reacquire the PFN lock so that
            // processing large segments here doesn't stall other contending
            // threads or DPCs for long periods of time.
            //

            UNLOCK_PFN (OldIrql);

            PteForProto = MiGetPteAddress (PointerPte);

            LOCK_PFN (OldIrql);

            //
            // We are on a page boundary, make sure this PTE is resident.
            //

            if (PteForProto->u.Hard.Valid == 0) {
                MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
            }
        }

        PteContents = *PointerPte;

        //
        // Prototype PTEs for segments backed by paging file
        // are either in demand zero, page file format, or transition.
        //

        if (PteContents.u.Hard.Valid == 1) {

            ASSERT (SegmentFlags.LargePages == 1);

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->u3.e2.ReferenceCount >= 1);
            ASSERT (Pfn1->u4.AweAllocation == 1);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);

            ASSERT (Pfn1->PteAddress->u.Hard.Valid == 1);
            ASSERT (Pfn1->PteAddress == PointerPte);

            MI_SET_PFN_DELETED (Pfn1);

            PageTableFrameIndex = Pfn1->u4.PteFrame;

            ASSERT (PageTableFrameIndex == PteForProto->u.Hard.PageFrameNumber);
            ASSERT (MI_IS_PFN_DELETED (Pfn1));
            ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 0);

            Pfn1->u3.e1.StartOfAllocation = 0;
            Pfn1->u3.e1.EndOfAllocation = 0;

            //
            // Clear the prototype bit since we don't want MiDecrementShareCount
            // to bother updating the prototype PTE (especially since we've
            // just biased the PteAddress in the PFN entry).
            //

            Pfn1->u3.e1.PrototypePte = 0;

            //
            // Delete these one by one in case there is pending I/O.
            //

            MiDecrementShareCountInline (Pfn1, PageFrameIndex);
            MI_INCREMENT_RESIDENT_AVAILABLE (1,
                                        MM_RESAVAIL_FREE_LARGE_PAGES_PF_BACKED);

            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
        }
        else if (PteContents.u.Soft.Prototype == 0) {

            ASSERT (SegmentFlags.LargePages == 0);
            if (PteContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, put the page on the free list.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // Check the reference count for the page, if the reference
                // count is zero and the page is not on the freelist,
                // move the page to the free list, if the reference
                // count is not zero, ignore this page.
                // When the reference count goes to zero, it will be placed
                // on the free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                }
            }
            else {

                //
                // This is not a prototype PTE, if any paging file
                // space has been allocated, release it.
                //

                if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {
                    MiReleasePageFileSpace (PteContents);
                }
            }
        }
        else {
            ASSERT (SegmentFlags.LargePages == 0);
        }
#if DBG
        MI_WRITE_ZERO_PTE (PointerPte);
#endif
        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);

    //
    // If there have been committed pages in this segment, adjust
    // the total commit count.
    //

    NumberOfCommittedPages = Segment->NumberOfCommittedPages;

    if (NumberOfCommittedPages != 0) {
        MiReturnCommitment (NumberOfCommittedPages);

        if (ControlArea->u.Flags.Image) {
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE2,
                             NumberOfCommittedPages);
        }
        else {
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE3,
                             NumberOfCommittedPages);
        }

        InterlockedExchangeAddSizeT (&MmSharedCommit, 0-NumberOfCommittedPages);
    }

    ExFreePool (ControlArea);
    ExFreePool (Segment);

    return;
}

ULONG
MmDoesFileHaveUserWritableReferences (
    IN PSECTION_OBJECT_POINTERS SectionPointer
    )

/*++

Routine Description:

    This routine is called by the transaction filesystem to determine if
    the given transaction is referencing a file which has user writable sections
    or user writable views into it.  If so, the transaction must be aborted
    as it cannot be guaranteed atomicity.

    The transaction filesystem is responsible for checking and intercepting
    file object creates that specify write access prior to using this
    interface.  Specifically, prior to starting a transaction, the transaction
    filesystem must ensure that there are no writable file objects that
    currently exist for the given file in the transaction.  While the
    transaction is ongoing, requests to create file objects with write access
    for the transaction files must be refused.

    This Mm routine exists to catch the case where the user has closed the
    file handles and the section handles, but still has open writable views.

    For this reason, no locks are needed to read the value below.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

Return Value:

    Number of user writable references.

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{
    KIRQL OldIrql;
    ULONG WritableUserReferences;
    PCONTROL_AREA ControlArea;

    WritableUserReferences = 0;

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);

    if (ControlArea != NULL) {
        WritableUserReferences = ControlArea->WritableUserReferences;
    }

    UNLOCK_PFN (OldIrql);

    return WritableUserReferences;
}

VOID
MiDereferenceControlAreaBySection (
    IN PCONTROL_AREA ControlArea,
    IN ULONG UserRef
    )

/*++

Routine Description:

    This is a nonpaged helper routine to dereference the specified control area.

Arguments:

    ControlArea - Supplies a pointer to the control area.

    UserRef - Supplies the number of user dereferences to apply.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ControlArea->NumberOfSectionReferences -= 1;
    ControlArea->NumberOfUserReferences -= UserRef;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, OldIrql);
}

VOID
MiSectionDelete (
    IN PVOID Object
    )

/*++

Routine Description:


    This routine is called by the object management procedures whenever
    the last reference to a section object has been removed.  This routine
    dereferences the associated segment object and checks to see if
    the segment object should be deleted by queueing the segment to the
    segment deletion thread.

Arguments:

    Object - a pointer to the body of the section object.

Return Value:

    None.

--*/

{
    PSECTION Section;
    PCONTROL_AREA ControlArea;
    ULONG UserRef;

    Section = (PSECTION)Object;

    if (Section->Segment == NULL) {

        //
        // The section was never initialized, no need to remove
        // any structures.
        //

        return;
    }

    UserRef = Section->u.Flags.UserReference;
    ControlArea = Section->Segment->ControlArea;

    if (Section->Address.StartingVpn != 0) {

        //
        // This section is based, remove the base address from the
        // tree.
        //
        // Get the allocation base mutex.
        //

        KeAcquireGuardedMutex (&MmSectionBasedMutex);

        MiRemoveBasedSection (Section);

        KeReleaseGuardedMutex (&MmSectionBasedMutex);
    }

    //
    // Adjust the count of writable user sections for transaction support.
    //

    if ((Section->u.Flags.UserWritable == 1) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL)) {

        ASSERT (Section->InitialPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE));

        InterlockedDecrement ((PLONG)&ControlArea->WritableUserReferences);
    }

    //
    // Decrement the number of section references to the segment for this
    // section.  This requires APCs to be blocked and the PFN lock to
    // synchronize upon.
    //

    MiDereferenceControlAreaBySection (ControlArea, UserRef);

    return;
}


VOID
MiDereferenceSegmentThread (
    IN PVOID StartContext
    )

/*++

Routine Description:

    This routine is the thread for dereferencing segments which have
    no references from any sections or mapped views AND there are
    no prototype PTEs within the segment which are in the transition
    state (i.e., no PFN database references to the segment).

    It also does double duty and is used for expansion of paging files.

Arguments:

    StartContext - Not used.

Return Value:

    None.

--*/

{
    PCONTROL_AREA ControlArea;
    PETHREAD CurrentThread;
    PMMPAGE_FILE_EXPANSION PageExpand;
    PLIST_ENTRY NextEntry;
    KIRQL OldIrql;
    static KWAIT_BLOCK WaitBlockArray[SegMaximumObject];
    PVOID WaitObjects[SegMaximumObject];
    NTSTATUS Status;

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Make this a real time thread.
    //

    CurrentThread = PsGetCurrentThread();
    KeSetPriorityThread (&CurrentThread->Tcb, LOW_REALTIME_PRIORITY + 2);

    CurrentThread->MemoryMaker = 1;

    WaitObjects[SegmentDereference] = (PVOID)&MmDereferenceSegmentHeader.Semaphore;
    WaitObjects[UsedSegmentCleanup] = (PVOID)&MmUnusedSegmentCleanup;

    for (;;) {

        Status = KeWaitForMultipleObjects(SegMaximumObject,
                                          &WaitObjects[0],
                                          WaitAny,
                                          WrVirtualMemory,
                                          UserMode,
                                          FALSE,
                                          NULL,
                                          &WaitBlockArray[0]);

        //
        // Switch on the wait status.
        //

        switch (Status) {

        case SegmentDereference:

            //
            // An entry is available to dereference, acquire the spinlock
            // and remove the entry.
            //

            ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

            if (IsListEmpty (&MmDereferenceSegmentHeader.ListHead)) {

                //
                // There is nothing in the list, rewait.
                //

                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);
                break;
            }

            NextEntry = RemoveHeadList (&MmDereferenceSegmentHeader.ListHead);

            ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

            ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

            ControlArea = CONTAINING_RECORD (NextEntry,
                                             CONTROL_AREA,
                                             DereferenceList);

            if (ControlArea->Segment != NULL) {

                //
                // This is a control area, delete it after indicating
                // this entry is not on any list.
                //

                ControlArea->DereferenceList.Flink = NULL;

                ASSERT (ControlArea->u.Flags.FilePointerNull == 1);
                MiSegmentDelete (ControlArea->Segment);
            }
            else {

                //
                // This is a request to expand or reduce the paging files.
                //

                PageExpand = (PMMPAGE_FILE_EXPANSION)ControlArea;

                if (PageExpand->RequestedExpansionSize == MI_CONTRACT_PAGEFILES) {

                    //
                    // Attempt to reduce the size of the paging files.
                    //

                    ASSERT (PageExpand == &MiPageFileContract);

                    MiAttemptPageFileReduction ();
                }
                else {

                    //
                    // Attempt to expand the size of the paging files.
                    //

                    MiExtendPagingFiles (PageExpand);

                    KeSetEvent (&PageExpand->Event, 0, FALSE);
                }
            }
            break;

        case UsedSegmentCleanup:

            MiRemoveUnusedSegments ();

            KeClearEvent (&MmUnusedSegmentCleanup);

            break;

        default:

            KdPrint(("MMSegmentderef: Illegal wait status, %lx =\n", Status));
            break;
        } // end switch

    } //end for

    return;
}


ULONG
MiSectionInitialization (
    )

/*++

Routine Description:

    This function creates the section object type descriptor at system
    initialization and stores the address of the object type descriptor
    in global storage.

Arguments:

    None.

Return Value:

    TRUE - Initialization was successful.

    FALSE - Initialization Failed.



--*/

{
    OBJECT_TYPE_INITIALIZER ObjectTypeInitializer;
    UNICODE_STRING TypeName;
    HANDLE ThreadHandle;
    OBJECT_ATTRIBUTES ObjectAttributes;
    UNICODE_STRING SectionName;
    PSECTION Section;
    HANDLE Handle;
    PSEGMENT Segment;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;

    ASSERT (MmSectionBasedRoot.NumberGenericTableElements == 0);
    MmSectionBasedRoot.BalancedRoot.u1.Parent = &MmSectionBasedRoot.BalancedRoot;

    //
    // Initialize the common fields of the Object Type Initializer record
    //

    RtlZeroMemory (&ObjectTypeInitializer, sizeof(ObjectTypeInitializer));
    ObjectTypeInitializer.Length = sizeof (ObjectTypeInitializer);
    ObjectTypeInitializer.InvalidAttributes = OBJ_OPENLINK;
    ObjectTypeInitializer.GenericMapping = MiSectionMapping;
    ObjectTypeInitializer.PoolType = PagedPool;
    ObjectTypeInitializer.DefaultPagedPoolCharge = sizeof(SECTION);

    //
    // Initialize string descriptor.
    //

#define TYPE_SECTION L"Section"

    TypeName.Buffer = (const PUSHORT) TYPE_SECTION;
    TypeName.Length = sizeof (TYPE_SECTION) - sizeof (WCHAR);
    TypeName.MaximumLength = sizeof TYPE_SECTION;

    //
    // Create the section object type descriptor
    //

    ObjectTypeInitializer.ValidAccessMask = SECTION_ALL_ACCESS;
    ObjectTypeInitializer.DeleteProcedure = MiSectionDelete;
    ObjectTypeInitializer.GenericMapping = MiSectionMapping;
    ObjectTypeInitializer.UseDefaultObject = TRUE;

    if (!NT_SUCCESS(ObCreateObjectType (&TypeName,
                                        &ObjectTypeInitializer,
                                        (PSECURITY_DESCRIPTOR) NULL,
                                        &MmSectionObjectType))) {
        return FALSE;
    }

    //
    // Create the Segment dereferencing thread.
    //

    InitializeObjectAttributes (&ObjectAttributes,
                                NULL,
                                0,
                                NULL,
                                NULL);

    if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                          THREAD_ALL_ACCESS,
                                          &ObjectAttributes,
                                          0,
                                          NULL,
                                          MiDereferenceSegmentThread,
                                          NULL))) {
        return FALSE;
    }

    ZwClose (ThreadHandle);

    //
    // Create the permanent section which maps physical memory.
    //

    Segment = (PSEGMENT)ExAllocatePoolWithTag (PagedPool,
                                               sizeof(SEGMENT),
                                               'gSmM');
    if (Segment == NULL) {
        return FALSE;
    }

    ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                         (ULONG)sizeof(CONTROL_AREA),
                                         MMCONTROL);
    if (ControlArea == NULL) {
        ExFreePool (Segment);
        return FALSE;
    }

    RtlZeroMemory (Segment, sizeof(SEGMENT));
    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA));

    ControlArea->Segment = Segment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->u.Flags.PhysicalMemory = 1;

    Segment->ControlArea = ControlArea;
    Segment->SegmentPteTemplate.u.Long = 0;

    //
    // Now that the segment object is created, create a section object
    // which refers to the segment object.
    //

#define DEVICE_PHYSICAL_MEMORY L"\\Device\\PhysicalMemory"

    SectionName.Buffer = (const PUSHORT)DEVICE_PHYSICAL_MEMORY;
    SectionName.Length = sizeof (DEVICE_PHYSICAL_MEMORY) - sizeof (WCHAR);
    SectionName.MaximumLength = sizeof (DEVICE_PHYSICAL_MEMORY);

    InitializeObjectAttributes (&ObjectAttributes,
                                &SectionName,
                                OBJ_PERMANENT | OBJ_KERNEL_EXCLUSIVE,
                                NULL,
                                NULL);

    Status = ObCreateObject (KernelMode,
                             MmSectionObjectType,
                             &ObjectAttributes,
                             KernelMode,
                             NULL,
                             sizeof(SECTION),
                             sizeof(SECTION),
                             0,
                             (PVOID *)&Section);

    if (!NT_SUCCESS(Status)) {
        ExFreePool (ControlArea);
        ExFreePool (Segment);
        return FALSE;
    }

    Section->Segment = Segment;
    Section->SizeOfSection.QuadPart = ((LONGLONG)1 << PHYSICAL_ADDRESS_BITS) - 1;
    Section->u.LongFlags = 0;
    Section->InitialPageProtection = PAGE_EXECUTE_READWRITE;

    Status = ObInsertObject ((PVOID)Section,
                             NULL,
                             SECTION_MAP_READ,
                             0,
                             NULL,
                             &Handle);

    if (!NT_SUCCESS (Status)) {
        return FALSE;
    }

    if (!NT_SUCCESS (NtClose (Handle))) {
        return FALSE;
    }

    return TRUE;
}

BOOLEAN
MmForceSectionClosed (
    __in PSECTION_OBJECT_POINTERS SectionObjectPointer,
    __in BOOLEAN DelayClose
    )

/*++

Routine Description:

    This function examines the Section object pointers.  If they are NULL,
    no further action is taken and the value TRUE is returned.

    If the Section object pointer is not NULL, the section reference count
    and the map view count are checked. If both counts are zero, the
    segment associated with the file is deleted and the file closed.
    If one of the counts is non-zero, no action is taken and the
    value FALSE is returned.

Arguments:

    SectionObjectPointer - Supplies a pointer to a section object.

    DelayClose - Supplies the value TRUE if the close operation should
                 occur as soon as possible in the event this section
                 cannot be closed now due to outstanding references.

Return Value:

    TRUE - The segment was deleted and the file closed or no segment was
           located.

    FALSE - The segment was not deleted and no action was performed OR
            an I/O error occurred trying to write the pages.

--*/

{
    PCONTROL_AREA ControlArea;
    KIRQL OldIrql;
    LOGICAL state;

    //
    // Check the status of the control area, if the control area is in use
    // or the control area is being deleted, this operation cannot continue.
    //

    state = MiCheckControlAreaStatus (CheckBothSection,
                                      SectionObjectPointer,
                                      DelayClose,
                                      &ControlArea,
                                      &OldIrql);

    if (ControlArea == NULL) {
        return (BOOLEAN) state;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    //
    // Repeat until there are no more control areas - multiple control areas
    // for the same image section occur to support user global DLLs - these DLLs
    // require data that is shared within a session but not across sessions.
    // Note this can only happen for Hydra.
    //

    do {

        //
        // Set the being deleted flag and up the number of mapped views
        // for the segment.  Upping the number of mapped views prevents
        // the segment from being deleted and passed to the deletion thread
        // while we are forcing a delete.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ASSERT (ControlArea->NumberOfMappedViews == 0);
        ControlArea->NumberOfMappedViews = 1;

        //
        // This is a page file backed or image Segment.  The Segment is being
        // deleted, remove all references to the paging file and physical memory.
        //

        UNLOCK_PFN (OldIrql);

        //
        // Delete the section by flushing all modified pages back to the section
        // if it is a file and freeing up the pages such that the
        // PfnReferenceCount goes to zero.
        //

        MiCleanSection (ControlArea, TRUE);

        //
        // Get the next Hydra control area.
        //

        state = MiCheckControlAreaStatus (CheckBothSection,
                                          SectionObjectPointer,
                                          DelayClose,
                                          &ControlArea,
                                          &OldIrql);

    } while (ControlArea);

    return (BOOLEAN) state;
}


VOID
MiCleanSection (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL DirtyDataPagesOk
    )

/*++

Routine Description:

    This function examines each prototype PTE in the section and
    takes the appropriate action to "delete" the prototype PTE.

    If the PTE is dirty and is backed by a file (not a paging file),
    the corresponding page is written to the file.

    At the completion of this service, the section which was
    operated upon is no longer usable.

    NOTE - ALL I/O ERRORS ARE IGNORED.  IF ANY WRITES FAIL, THE
           DIRTY PAGES ARE MARKED CLEAN AND THE SECTION IS DELETED.

Arguments:

    ControlArea - Supplies a pointer to the control area for the section.

    DirtyDataPagesOk - Supplies TRUE if dirty data pages are ok.  If FALSE
                       is specified then no dirty data pages are expected (as
                       this is a dereference operation) so any encountered
                       must be due to pool corruption so bugcheck.

                       Note that dirty image pages are always discarded.
                       This should only happen for images that were either
                       read in from floppies or images with shared global
                       subsections.

Return Value:

    None.

--*/

{
    LOGICAL DroppedPfnLock;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPte;
    PMMPTE LastWritten;
    PMMPTE FirstWritten;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPFN Pfn3;
    PMMPTE WrittenPte;
    MMPTE WrittenContents;
    KIRQL OldIrql;
    PMDL Mdl;
    PSUBSECTION Subsection;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    LARGE_INTEGER StartingOffset;
    LARGE_INTEGER TempOffset;
    NTSTATUS Status;
    IO_STATUS_BLOCK IoStatus;
    ULONG WriteNow;
    ULONG ImageSection;
    ULONG DelayCount;
    ULONG First;
    KEVENT IoEvent;
    PFN_NUMBER PageTableFrameIndex;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_WRITE_CLUSTER];
    ULONG ReflushCount;
    ULONG MaxClusterSize;

    WriteNow = FALSE;
    ImageSection = FALSE;
    DelayCount = 0;
    MaxClusterSize = MmModifiedWriteClusterSize;
    FirstWritten = NULL;

    ASSERT (ControlArea->FilePointer);

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (ControlArea->u.Flags.Image) {
        ImageSection = TRUE;
        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + ControlArea->Segment->NonExtendedPtes;
    }
    else {

        //
        // Initializing these are not needed for correctness as they are
        // overwritten below, but without it the compiler cannot compile
        // this code W4 to check for use of uninitialized variables.
        //

        PointerPte = NULL;
        LastPte = NULL;
    }

    Mdl = (PMDL) MdlHack;

    KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

    LastWritten = NULL;
    ASSERT (MmModifiedWriteClusterSize == MM_MAXIMUM_WRITE_CLUSTER);
    LastPage = NULL;

    //
    // Initializing StartingOffset is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    StartingOffset.QuadPart = 0;

    //
    // The PFN lock is required for deallocating pages from a paging
    // file and for deleting transition PTEs.
    //

    LOCK_PFN (OldIrql);

    //
    // Stop the modified page writer from writing pages to this
    // file, and if any paging I/O is in progress, wait for it
    // to complete.
    //

    ControlArea->u.Flags.NoModifiedWriting = 1;

    while (ControlArea->ModifiedWriteCount != 0) {

        //
        // There is modified page writing in progess.  Set the
        // flag in the control area indicating the modified page
        // writer should signal when a write to this control area
        // is complete.  Release the PFN LOCK and wait in an
        // atomic operation.  Once the wait is satisfied, recheck
        // to make sure it was this file's I/O that was written.
        //

        ControlArea->u.Flags.SetMappedFileIoComplete = 1;

        //
        // Keep APCs blocked so no special APCs can be delivered in KeWait
        // which would cause the dispatcher lock to be released opening a
        // window where this thread could miss a pulse.
        //

        UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

        KeWaitForSingleObject (&MmMappedFileIoComplete,
                               WrPageOut,
                               KernelMode,
                               FALSE,
                               NULL);
        KeLowerIrql (OldIrql);

        LOCK_PFN (OldIrql);
    }

    if (ImageSection == FALSE) {
        while (Subsection->SubsectionBase == NULL) {
            Subsection = Subsection->NextSubsection;
            if (Subsection == NULL) {
                goto alldone;
            }
        }

        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + Subsection->PtesInSubsection;
    }

    for (;;) {

restartchunk:

        First = TRUE;

        while (PointerPte < LastPte) {

            if ((MiIsPteOnPdeBoundary(PointerPte)) || (First)) {

                First = FALSE;

                if ((ImageSection) ||
                    (MiCheckProtoPtePageState(PointerPte, MM_NOIRQL, &DroppedPfnLock))) {
                    MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                }
                else {

                    //
                    // Paged pool page is not resident, hence no transition or
                    // valid prototype PTEs can be present in it.  Skip it.
                    //

                    PointerPte = (PMMPTE)((((ULONG_PTR)PointerPte | PAGE_SIZE - 1)) + 1);
                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                    goto WriteItOut;
                }
            }

            PteContents = *PointerPte;

            //
            // Prototype PTEs for Segments backed by paging file
            // are either in demand zero, page file format, or transition.
            //

            if (PteContents.u.Hard.Valid == 1) {
                KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                              0x0,
                              (ULONG_PTR)ControlArea,
                              (ULONG_PTR)PointerPte,
                              (ULONG_PTR)PteContents.u.Long);
            }

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // This is a normal prototype PTE in mapped file format.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }
            else if (PteContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, there are 3 possible cases:
                //  1. The page is part of an image which is sharable and
                //     refers to the paging file - dereference page file
                //     space and free the physical page.
                //  2. The page refers to the segment but is not modified -
                //     free the physical page.
                //  3. The page refers to the segment and is modified -
                //     write the page to the file and free the physical page.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                if (Pfn1->u3.e2.ReferenceCount != 0) {
                    if (DelayCount < 20) {

                        //
                        // There must be an I/O in progress on this
                        // page.  Wait for the I/O operation to complete.
                        //

                        UNLOCK_PFN (OldIrql);

                        //
                        // Drain the deferred lists as these pages may be
                        // sitting in there right now.
                        //

                        MiDeferredUnlockPages (0);

                        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

                        DelayCount += 1;

                        //
                        // Redo the loop, if the delay count is greater than
                        // 20, assume that this thread is deadlocked and
                        // don't purge this page.  The file system can deal
                        // with the write operation in progress.
                        //

                        PointerPde = MiGetPteAddress (PointerPte);
                        LOCK_PFN (OldIrql);
                        if (PointerPde->u.Hard.Valid == 0) {
                            MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                        }
                        continue;
                    }
#if DBG
                    //
                    // The I/O still has not completed, just ignore
                    // the fact that the I/O is in progress and
                    // delete the page.
                    //

                    KdPrint(("MM:CLEAN - page number %lx has i/o outstanding\n",
                          PteContents.u.Trans.PageFrameNumber));
#endif
                }

                if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

                    //
                    // Paging file reference (case 1).
                    //

                    MI_SET_PFN_DELETED (Pfn1);

                    if (!ImageSection) {

                        //
                        // This is not an image section, it must be a
                        // page file backed section, therefore decrement
                        // the PFN reference count for the control area.
                        //

                        ControlArea->NumberOfPfnReferences -= 1;
                        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
                    }
#if DBG
                    else {
                        //
                        // This should only happen for images with shared
                        // global subsections.
                        //
                    }
#endif

                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    //
                    // Check the reference count for the page, if the                               // reference count is zero and the page is not on the
                    // freelist, move the page to the free list, if the
                    // reference count is not zero, ignore this page.  When
                    // the reference count goes to zero, it will be placed
                    // on the free list.
                    //

                    if ((Pfn1->u3.e2.ReferenceCount == 0) &&
                         (Pfn1->u3.e1.PageLocation != FreePageList)) {

                        MiUnlinkPageFromList (Pfn1);
                        MiReleasePageFileSpace (Pfn1->OriginalPte);
                        MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));

                    }

                    MI_WRITE_ZERO_PTE (PointerPte);

                    //
                    // If a cluster of pages to write has been completed,
                    // set the WriteNow flag.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }

                }
                else {

                    if ((Pfn1->u3.e1.Modified == 0) || (ImageSection)) {

                        //
                        // Non modified or image file page (case 2).
                        //

                        MI_SET_PFN_DELETED (Pfn1);
                        ControlArea->NumberOfPfnReferences -= 1;
                        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // Check the reference count for the page, if the
                        // reference count is zero and the page is not on
                        // the freelist, move the page to the free list,
                        // if the reference count is not zero, ignore this
                        // page. When the reference count goes to zero, it
                        // will be placed on the free list.
                        //

                        if ((Pfn1->u3.e2.ReferenceCount == 0) &&
                             (Pfn1->u3.e1.PageLocation != FreePageList)) {

                            MiUnlinkPageFromList (Pfn1);
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                        }

                        MI_WRITE_ZERO_PTE (PointerPte);

                        //
                        // If a cluster of pages to write has been
                        // completed, set the WriteNow flag.
                        //

                        if (LastWritten != NULL) {
                            WriteNow = TRUE;
                        }

                    }
                    else {

                        //
                        // Modified page backed by the file (case 3).
                        // Check to see if this is the first page of a
                        // cluster.
                        //

                        if (LastWritten == NULL) {
                            LastPage = (PPFN_NUMBER)(Mdl + 1);
                            ASSERT (MiGetSubsectionAddress(&Pfn1->OriginalPte) ==
                                                                Subsection);

                            //
                            // Calculate the offset to read into the file.
                            //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
                            //

                            ASSERT (Subsection->ControlArea->u.Flags.Image == 0);
                            StartingOffset.QuadPart = MiStartingOffset(
                                                         Subsection,
                                                         Pfn1->PteAddress);

                            MI_INITIALIZE_ZERO_MDL (Mdl);
                            Mdl->MdlFlags |= MDL_PAGES_LOCKED;

                            Mdl->StartVa = NULL;
                            Mdl->Size = (CSHORT)(sizeof(MDL) +
                                       (sizeof(PFN_NUMBER) * MaxClusterSize));
                            FirstWritten = PointerPte;
                        }

                        LastWritten = PointerPte;
                        Mdl->ByteCount += PAGE_SIZE;

                        //
                        // If the cluster is now full,
                        // set the write now flag.
                        //

                        if (Mdl->ByteCount == (PAGE_SIZE * MaxClusterSize)) {
                            WriteNow = TRUE;
                        }

                        MiUnlinkPageFromList (Pfn1);

                        MI_SET_MODIFIED (Pfn1, 0, 0x27);

                        //
                        // Up the reference count for the physical page as
                        // there is I/O in progress.
                        //

                        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

                        //
                        // Clear the modified bit for the page and set the
                        // write in progress bit.
                        //

                        *LastPage = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

                        LastPage += 1;
                    }
                }
            }
            else {

                if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {
                    MiReleasePageFileSpace (PteContents);
                }

                MI_WRITE_ZERO_PTE (PointerPte);

                //
                // If a cluster of pages to write has been completed,
                // set the WriteNow flag.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }

            //
            // Write the current cluster if it is complete,
            // full, or the loop is now complete.
            //

            PointerPte += 1;
WriteItOut:
            DelayCount = 0;

            if ((WriteNow) ||
                ((PointerPte == LastPte) && (LastWritten != NULL))) {

                //
                // Issue the write request.
                //

                UNLOCK_PFN (OldIrql);

                if (DirtyDataPagesOk == FALSE) {
                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x1,
                                  (ULONG_PTR)ControlArea,
                                  (ULONG_PTR)Mdl,
                                  ControlArea->u.LongFlags);
                }

                WriteNow = FALSE;

                //
                // Make sure the write does not go past the
                // end of file. (segment size).
                //

                ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

                TempOffset = MiEndingOffset(Subsection);

                if (((UINT64)StartingOffset.QuadPart + Mdl->ByteCount) >
                             (UINT64)TempOffset.QuadPart) {

                    ASSERT ((ULONG)(TempOffset.QuadPart -
                                        StartingOffset.QuadPart) >
                             (Mdl->ByteCount - PAGE_SIZE));

                    Mdl->ByteCount = (ULONG)(TempOffset.QuadPart -
                                            StartingOffset.QuadPart);
                }

                ReflushCount = 0;

                while (TRUE) {

                    KeClearEvent (&IoEvent);

                    Status = IoSynchronousPageWrite (ControlArea->FilePointer,
                                                     Mdl,
                                                     &StartingOffset,
                                                     &IoEvent,
                                                     &IoStatus);

                    if (NT_SUCCESS(Status)) {

                        KeWaitForSingleObject (&IoEvent,
                                               WrPageOut,
                                               KernelMode,
                                               FALSE,
                                               NULL);
                    }
                    else {
                        IoStatus.Status = Status;
                    }

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (MmIsRetryIoStatus(IoStatus.Status)) {

                        ReflushCount -= 1;
                        if (ReflushCount & MiIoRetryMask) {
                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
                            continue;
                        }
                    }
                    break;
                }

                Page = (PPFN_NUMBER)(Mdl + 1);

                LOCK_PFN (OldIrql);

                if (MiIsPteOnPdeBoundary(PointerPte) == 0) {

                    //
                    // The next PTE is not in a different page, make
                    // sure this page did not leave memory when the
                    // I/O was in progress.
                    //

                    if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                    }
                }

                if (!NT_SUCCESS(IoStatus.Status)) {

                    if ((MmIsRetryIoStatus(IoStatus.Status)) &&
                        (MaxClusterSize != 1) &&
                        (Mdl->ByteCount > PAGE_SIZE)) {

                        //
                        // Retried I/O of a cluster have failed, reissue
                        // the cluster one page at a time as the
                        // storage stack should always be able to
                        // make forward progress this way.
                        //

                        ASSERT (FirstWritten != NULL);
                        ASSERT (LastWritten != NULL);
                        ASSERT (FirstWritten != LastWritten);

                        IoStatus.Information = 0;

                        while (Page < LastPage) {

                            Pfn2 = MI_PFN_ELEMENT (*Page);

                            //
                            // Mark the page dirty again so it can be rewritten.
                            //

                            MI_SET_MODIFIED (Pfn2, 1, 0xE);

                            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn2);
                            Page += 1;
                        }

                        PointerPte = FirstWritten;
                        LastWritten = NULL;

                        MaxClusterSize = 1;
                        goto restartchunk;
                    }
                }

                //
                // I/O complete unlock pages.
                //
                // NOTE that the error status is ignored.
                //

                while (Page < LastPage) {

                    Pfn2 = MI_PFN_ELEMENT (*Page);

                    //
                    // Make sure the page is still transition.
                    //

                    WrittenPte = Pfn2->PteAddress;

                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn2);

                    if (!MI_IS_PFN_DELETED (Pfn2)) {

                        //
                        // Make sure the prototype PTE is
                        // still in the working set.
                        //

                        if (MiGetPteAddress (WrittenPte)->u.Hard.Valid == 0) {
                            MiMakeSystemAddressValidPfn (WrittenPte, OldIrql);
                        }

                        if (Pfn2->PteAddress != WrittenPte) {

                            //
                            // The PFN lock was released to make the
                            // page table page valid, and while it
                            // was released, the physical page
                            // was reused.  Go onto the next one.
                            //

                            Page += 1;
                            continue;
                        }

                        WrittenContents = *WrittenPte;

                        if ((WrittenContents.u.Soft.Prototype == 0) &&
                             (WrittenContents.u.Soft.Transition == 1)) {

                            MI_SET_PFN_DELETED (Pfn2);
                            ControlArea->NumberOfPfnReferences -= 1;
                            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                            PageTableFrameIndex = Pfn2->u4.PteFrame;
                            Pfn3 = MI_PFN_ELEMENT (PageTableFrameIndex);
                            MiDecrementShareCountInline (Pfn3, PageTableFrameIndex);

                            //
                            // Check the reference count for the page,
                            // if the reference count is zero and the
                            // page is not on the freelist, move the page
                            // to the free list, if the reference
                            // count is not zero, ignore this page.
                            // When the reference count goes to zero,
                            // it will be placed on the free list.
                            //

                            if ((Pfn2->u3.e2.ReferenceCount == 0) &&
                                (Pfn2->u3.e1.PageLocation != FreePageList)) {

                                MiUnlinkPageFromList (Pfn2);
                                MiReleasePageFileSpace (Pfn2->OriginalPte);
                                MiInsertPageInFreeList (*Page);
                            }
                        }
                        WrittenPte->u.Long = 0;
                    }
                    Page += 1;
                }

                //
                // Indicate that there is no current cluster being built.
                //

                LastWritten = NULL;
            }

        } // end while

        //
        // Get the next subsection if any.
        //

        if (Subsection->NextSubsection == NULL) {
            break;
        }

        Subsection = Subsection->NextSubsection;

        if (ImageSection == FALSE) {
            while (Subsection->SubsectionBase == NULL) {
                Subsection = Subsection->NextSubsection;
                if (Subsection == NULL) {
                    goto alldone;
                }
            }
        }

        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + Subsection->PtesInSubsection;

    } // end for

alldone:

    ControlArea->NumberOfMappedViews = 0;

    ASSERT (ControlArea->NumberOfPfnReferences == 0);

    if (ControlArea->u.Flags.FilePointerNull == 0) {
        ControlArea->u.Flags.FilePointerNull = 1;

        if (ControlArea->u.Flags.Image) {

            MiRemoveImageSectionObject (ControlArea->FilePointer, ControlArea);
        }
        else {

            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
            ControlArea->FilePointer->SectionObjectPointer->DataSectionObject = NULL;

        }
    }
    UNLOCK_PFN (OldIrql);

    //
    // Delete the segment structure.
    //

    MiSegmentDelete (ControlArea->Segment);

    return;
}

NTSTATUS
MmGetFileNameForSection (
    IN PSECTION SectionObject,
    OUT POBJECT_NAME_INFORMATION *FileNameInfo
    )

/*++

Routine Description:

    This function returns the file name for the corresponding section.

Arguments:

    SectionObject - Supplies the section to get the name of.

    FileNameInfo - Returns the Unicode name of the corresponding section.  The
                   caller must free this pool block when success is returned.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{
    ULONG NumberOfBytes;
    ULONG AdditionalLengthNeeded;
    NTSTATUS Status;
    PFILE_OBJECT FileObject;

    NumberOfBytes = 1024;

    *FileNameInfo = NULL;

    if (SectionObject->u.Flags.Image == 0) {
        return STATUS_SECTION_NOT_IMAGE;
    }

    *FileNameInfo = ExAllocatePoolWithTag (PagedPool, NumberOfBytes, '  mM');

    if (*FileNameInfo == NULL) {
        return STATUS_NO_MEMORY;
    }

    FileObject = SectionObject->Segment->ControlArea->FilePointer;

    Status = ObQueryNameString (FileObject,
                                *FileNameInfo,
                                NumberOfBytes,
                                &AdditionalLengthNeeded);

    if (!NT_SUCCESS (Status)) {

        if (Status == STATUS_INFO_LENGTH_MISMATCH) {

            //
            // Our buffer was not large enough, retry just once with a larger
            // one (as specified by ObQuery).  Don't try more than once to
            // prevent broken parse procedures which give back wrong
            // AdditionalLengthNeeded values from causing problems.
            //

            ExFreePool (*FileNameInfo);

            NumberOfBytes += AdditionalLengthNeeded;

            *FileNameInfo = ExAllocatePoolWithTag (PagedPool,
                                                   NumberOfBytes,
                                                   '  mM');

            if (*FileNameInfo == NULL) {
                return STATUS_NO_MEMORY;
            }

            Status = ObQueryNameString (FileObject,
                                        *FileNameInfo,
                                        NumberOfBytes,
                                        &AdditionalLengthNeeded);

            if (NT_SUCCESS (Status)) {
                return STATUS_SUCCESS;
            }
        }

        ExFreePool (*FileNameInfo);
        *FileNameInfo = NULL;
        return Status;
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MmGetFileNameForAddress (
    IN PVOID ProcessVa,
    OUT PUNICODE_STRING FileName
    )

/*++

Routine Description:

    This function returns the file name for the corresponding process address if it corresponds to an image section.

Arguments:

    ProcessVa - Process virtual address

    FileName - Returns the name of the corresponding section.

Return Value:

    NTSTATUS - Status of operation

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/
{
    PMMVAD Vad;
    PFILE_OBJECT FileObject;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    ULONG RetLen;
    ULONG BufLen;
    PEPROCESS Process;
    POBJECT_NAME_INFORMATION FileNameInfo;

    PAGED_CODE ();

    Process = PsGetCurrentProcess();

    LOCK_ADDRESS_SPACE (Process);

    Vad = MiLocateAddress (ProcessVa);

    if (Vad == NULL) {

        //
        // No virtual address is allocated at the specified base address,
        // return an error.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    //
    // Reject private memory.
    //

    if (Vad->u.VadFlags.PrivateMemory == 1) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    ControlArea = Vad->ControlArea;

    if (ControlArea == NULL) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    //
    // Reject non-image sections.
    //

    if (ControlArea->u.Flags.Image == 0) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    FileObject = ControlArea->FilePointer;

    ASSERT (FileObject != NULL);

    ObReferenceObject (FileObject);

    UNLOCK_ADDRESS_SPACE (Process);

    //
    // Pick an initial size big enough for most reasonable files.
    //

    BufLen = sizeof (*FileNameInfo) + 1024;

    do {

        FileNameInfo = ExAllocatePoolWithTag (PagedPool, BufLen, '  mM');

        if (FileNameInfo == NULL) {
            Status = STATUS_NO_MEMORY;
            break;
        }

        RetLen = 0;

        Status = ObQueryNameString (FileObject, FileNameInfo, BufLen, &RetLen);

        if (NT_SUCCESS (Status)) {
            FileName->Length = FileName->MaximumLength = FileNameInfo->Name.Length;
            FileName->Buffer = (PWCHAR) FileNameInfo;
            RtlMoveMemory (FileName->Buffer, FileNameInfo->Name.Buffer, FileName->Length);
        }
        else {
            ExFreePool (FileNameInfo);
            if (RetLen > BufLen) {
                BufLen = RetLen;
                continue;
            }
        }
        break;

    } while (TRUE);

    ObDereferenceObject (FileObject);
    return Status;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}

PFILE_OBJECT
MmGetFileObjectForSection (
    IN PVOID Section
    )

/*++

Routine Description:

    This routine returns a pointer to the file object backing a section object.

Arguments:

    Section - Supplies the section to query.

Return Value:

    A pointer to the file object backing the argument section.

Environment:

    Kernel mode, PASSIVE_LEVEL.

    The caller must ensure that the section is valid for the
    duration of the call.

--*/

{
    PFILE_OBJECT FileObject;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (Section != NULL);

    FileObject = ((PSECTION)Section)->Segment->ControlArea->FilePointer;

    return FileObject;
}

VOID
MiCheckControlArea (
    IN PCONTROL_AREA ControlArea,
    IN KIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine checks the reference counts for the specified
    control area, and if the counts are all zero, it marks the
    control area for deletion and queues it to the deletion thread.

    *********************** NOTE ********************************
    This routine returns with the PFN LOCK RELEASED!!!!!

Arguments:

    ControlArea - Supplies a pointer to the control area to check.

    PreviousIrql - Supplies the previous IRQL.

Return Value:

    NONE.

Environment:

    Kernel mode, PFN lock held, PFN lock released upon return!!!

--*/

{
    PEVENT_COUNTER PurgeEvent;

#define DELETE_ON_CLOSE 0x1
#define DEREF_SEGMENT   0x2

    ULONG Action;

    Action = 0;
    PurgeEvent = NULL;

    MM_PFN_LOCK_ASSERT();
    if ((ControlArea->NumberOfMappedViews == 0) &&
         (ControlArea->NumberOfSectionReferences == 0)) {

        ASSERT (ControlArea->NumberOfUserReferences == 0);

        if (ControlArea->FilePointer != NULL) {

            if (ControlArea->NumberOfPfnReferences == 0) {

                //
                // There are no views and no physical pages referenced
                // by the Segment, dereference the Segment object.
                //

                ControlArea->u.Flags.BeingDeleted = 1;
                Action |= DEREF_SEGMENT;

                ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
                ControlArea->u.Flags.FilePointerNull = 1;

                if (ControlArea->u.Flags.Image) {

                    MiRemoveImageSectionObject (ControlArea->FilePointer, ControlArea);
                }
                else {

                    ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
                    ControlArea->FilePointer->SectionObjectPointer->DataSectionObject = NULL;

                }
            }
            else {

                //
                // Insert this segment into the unused segment list (unless
                // it is already on the list).
                //

                if (ControlArea->DereferenceList.Flink == NULL) {
                    MI_INSERT_UNUSED_SEGMENT (ControlArea);
                }

                //
                // Indicate if this section should be deleted now that
                // the reference counts are zero.
                //

                if (ControlArea->u.Flags.DeleteOnClose) {
                    Action |= DELETE_ON_CLOSE;
                }

                //
                // The number of mapped views are zero, the number of
                // section references are zero, but there are some
                // pages of the file still resident.  If this is
                // an image with Global Memory, "purge" the subsections
                // which contain the global memory and reset them to
                // point back to the file.
                //

                if (ControlArea->u.Flags.GlobalMemory == 1) {

                    ASSERT (ControlArea->u.Flags.Image == 1);

                    ControlArea->u.Flags.BeingPurged = 1;
                    ControlArea->NumberOfMappedViews = 1;

                    MiPurgeImageSection (ControlArea, PreviousIrql);

                    ControlArea->u.Flags.BeingPurged = 0;
                    ControlArea->NumberOfMappedViews -= 1;
                    if ((ControlArea->NumberOfMappedViews == 0) &&
                        (ControlArea->NumberOfSectionReferences == 0) &&
                        (ControlArea->NumberOfPfnReferences == 0)) {

                        ControlArea->u.Flags.BeingDeleted = 1;
                        Action |= DEREF_SEGMENT;
                        ControlArea->u.Flags.FilePointerNull = 1;

                        MiRemoveImageSectionObject (ControlArea->FilePointer,
                                                    ControlArea);
                    }
                    else {
                        PurgeEvent = ControlArea->WaitingForDeletion;
                        ControlArea->WaitingForDeletion = NULL;
                    }
                }

                //
                // If delete on close is set and the segment was
                // not deleted, up the count of mapped views so the
                // control area will not be deleted when the PFN lock
                // is released.
                //

                if (Action == DELETE_ON_CLOSE) {
                    ControlArea->NumberOfMappedViews = 1;
                    ControlArea->u.Flags.BeingDeleted = 1;
                }
            }
        }
        else {

            //
            // This Segment is backed by a paging file, dereference the
            // Segment object when the number of views goes from 1 to 0
            // without regard to the number of PFN references.
            //

            ControlArea->u.Flags.BeingDeleted = 1;
            Action |= DEREF_SEGMENT;
        }
    }
    else if (ControlArea->WaitingForDeletion != NULL) {
        PurgeEvent = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;
    }

    UNLOCK_PFN (PreviousIrql);

    if (Action != 0) {

        ASSERT (PurgeEvent == NULL);
        ASSERT (ControlArea->WritableUserReferences == 0);

        if (Action & DEREF_SEGMENT) {

            //
            // Delete the segment.
            //

            MiSegmentDelete (ControlArea->Segment);
        }
        else {

            //
            // The segment should be forced closed now.
            //

            MiCleanSection (ControlArea, TRUE);
        }
    }
    else {

        //
        // If any threads are waiting for the segment, indicate the
        // the purge operation has completed.
        //

        if (PurgeEvent != NULL) {
            KeSetEvent (&PurgeEvent->Event, 0, FALSE);
        }

        if (MI_UNUSED_SEGMENTS_SURPLUS()) {
            KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
        }
    }

    return;
}


VOID
MiCheckForControlAreaDeletion (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine checks the reference counts for the specified
    control area, and if the counts are all zero, it marks the
    control area for deletion and queues it to the deletion thread.

Arguments:

    ControlArea - Supplies a pointer to the control area to check.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    KIRQL OldIrql;

    MM_PFN_LOCK_ASSERT();
    if ((ControlArea->NumberOfPfnReferences == 0) &&
        (ControlArea->NumberOfMappedViews == 0) &&
        (ControlArea->NumberOfSectionReferences == 0 )) {

        //
        // This segment is no longer mapped in any address space
        // nor are there any prototype PTEs within the segment
        // which are valid or in a transition state.  Queue
        // the segment to the segment-dereferencer thread
        // which will dereference the segment object, potentially
        // causing the segment to be deleted.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
        ControlArea->u.Flags.FilePointerNull = 1;

        if (ControlArea->u.Flags.Image) {

            MiRemoveImageSectionObject (ControlArea->FilePointer,
                                        ControlArea);
        }
        else {
            ControlArea->FilePointer->SectionObjectPointer->DataSectionObject =
                                                            NULL;
        }

        ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

        if (ControlArea->DereferenceList.Flink != NULL) {

            //
            // Remove the entry from the unused segment list and put it
            // on the dereference list.
            //

            RemoveEntryList (&ControlArea->DereferenceList);

            MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);
        }

        //
        // Image sections still have useful header information in their segment
        // even if no pages are valid or transition so put these at the tail.
        // Data sections have nothing of use if all the data pages are gone so
        // we used to put those at the front.  Now both types go to the rear
        // so that commit extensions go to the front for earlier processing.
        //

        InsertTailList (&MmDereferenceSegmentHeader.ListHead,
                        &ControlArea->DereferenceList);

        ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

        KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                            0L,
                            1L,
                            FALSE);
    }
    return;
}


LOGICAL
MiCheckControlAreaStatus (
    IN SECTION_CHECK_TYPE SectionCheckType,
    IN PSECTION_OBJECT_POINTERS SectionObjectPointers,
    IN ULONG DelayClose,
    OUT PCONTROL_AREA *ControlAreaOut,
    OUT PKIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine checks the status of the control area for the specified
    SectionObjectPointers.  If the control area is in use, that is, the
    number of section references and the number of mapped views are not
    both zero, no action is taken and the function returns FALSE.

    If there is no control area associated with the specified
    SectionObjectPointers or the control area is in the process of being
    created or deleted, no action is taken and the value TRUE is returned.

    If, there are no section objects and the control area is not being
    created or deleted, the address of the control area is returned
    in the ControlArea argument, the address of a pool block to free
    is returned in the SegmentEventOut argument and the PFN_LOCK is
    still held at the return.

Arguments:

    *SegmentEventOut - Returns a pointer to NonPaged Pool which much be
                       freed by the caller when the PFN_LOCK is released.
                       This value is NULL if no pool is allocated and the
                       PFN_LOCK is not held.

    SectionCheckType - Supplies the type of section to check on, one of
                      CheckImageSection, CheckDataSection, CheckBothSection.

    SectionObjectPointers - Supplies the section object pointers through
                            which the control area can be located.

    DelayClose - Supplies a boolean which if TRUE and the control area
                 is being used, the delay on close field should be set
                 in the control area.

    *ControlAreaOut - Returns the address of the control area.

    PreviousIrql - Returns, in the case the PFN_LOCK is held, the previous
                   IRQL so the lock can be released properly.

Return Value:

    FALSE if the control area is in use, TRUE if the control area is gone or
    in the process or being created or deleted.

Environment:

    Kernel mode, PFN lock NOT held.

--*/


{
    PKTHREAD CurrentThread;
    PEVENT_COUNTER IoEvent;
    PEVENT_COUNTER SegmentEvent;
    LOGICAL DeallocateSegmentEvent;
    PCONTROL_AREA ControlArea;
    ULONG SectRef;
    KIRQL OldIrql;

    //
    // Allocate an event to wait on in case the segment is in the
    // process of being deleted.  This event cannot be allocated
    // with the PFN database locked as pool expansion would deadlock.
    //

    *ControlAreaOut = NULL;

    do {

        SegmentEvent = MiGetEventCounter ();

        if (SegmentEvent != NULL) {
            break;
        }

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&MmShortTime);

    } while (TRUE);

    //
    // Acquire the PFN lock and examine the section object pointer
    // value within the file object.
    //
    // File control blocks live in non-paged pool.
    //

    LOCK_PFN (OldIrql);

    if (SectionCheckType != CheckImageSection) {
        ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->DataSectionObject));
    }
    else {
        ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->ImageSectionObject));
    }

    if (ControlArea == NULL) {

        if (SectionCheckType != CheckBothSection) {

            //
            // This file no longer has an associated segment.
            //

            UNLOCK_PFN (OldIrql);
            MiFreeEventCounter (SegmentEvent);
            return TRUE;
        }
        else {
            ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->ImageSectionObject));
            if (ControlArea == NULL) {

                //
                // This file no longer has an associated segment.
                //

                UNLOCK_PFN (OldIrql);
                MiFreeEventCounter (SegmentEvent);
                return TRUE;
            }
        }
    }

    //
    //  Depending on the type of section, check for the pertinent
    //  reference count being non-zero.
    //

    if (SectionCheckType != CheckUserDataSection) {
        SectRef = ControlArea->NumberOfSectionReferences;
    }
    else {
        SectRef = ControlArea->NumberOfUserReferences;
    }

    if ((SectRef != 0) ||
        (ControlArea->NumberOfMappedViews != 0) ||
        (ControlArea->u.Flags.BeingCreated)) {


        //
        // The segment is currently in use or being created.
        //

        if (DelayClose) {

            //
            // The section should be deleted when the reference
            // counts are zero, set the delete on close flag.
            //

            ControlArea->u.Flags.DeleteOnClose = 1;
        }

        UNLOCK_PFN (OldIrql);
        MiFreeEventCounter (SegmentEvent);
        return FALSE;
    }

    //
    // The segment has no references, delete it.  If the segment
    // is already being deleted, set the event field in the control
    // area and wait on the event.
    //

    if (ControlArea->u.Flags.BeingDeleted) {

        //
        // The segment object is in the process of being deleted.
        // Check to see if another thread is waiting for the deletion,
        // otherwise create and event object to wait upon.
        //

        if (ControlArea->WaitingForDeletion == NULL) {

            //
            // Create an event and put its address in the control area.
            //

            DeallocateSegmentEvent = FALSE;
            ControlArea->WaitingForDeletion = SegmentEvent;
            IoEvent = SegmentEvent;
        }
        else {
            DeallocateSegmentEvent = TRUE;
            IoEvent = ControlArea->WaitingForDeletion;

            //
            // No interlock is needed for the RefCount increment as
            // no thread can be decrementing it since it is still
            // pointed to by the control area.
            //

            IoEvent->RefCount += 1;
        }

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_PFN_AND_THEN_WAIT(OldIrql);

        KeWaitForSingleObject(&IoEvent->Event,
                              WrPageOut,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);

        //
        // Before this event can be set, the control area
        // WaitingForDeletion field must be cleared (and may be
        // reinitialized to something else), but cannot be reset
        // to our local event.  This allows us to dereference the
        // event count lock free.
        //

        KeLeaveCriticalRegionThread (CurrentThread);

        MiFreeEventCounter (IoEvent);
        if (DeallocateSegmentEvent == TRUE) {
            MiFreeEventCounter (SegmentEvent);
        }
        return TRUE;
    }

    //
    // Return with the PFN database locked.
    //

    ASSERT (SegmentEvent->RefCount == 1);
    ASSERT (SegmentEvent->ListEntry.Next == NULL);

    //
    // NO interlock is needed for the RefCount clearing as the event counter
    // was never pointed to by a control area.
    //

#if DBG
    SegmentEvent->RefCount = 0;
#endif

    InterlockedPushEntrySList (&MmEventCountSListHead,
                               (PSLIST_ENTRY)&SegmentEvent->ListEntry);

    *ControlAreaOut = ControlArea;
    *PreviousIrql = OldIrql;
    return FALSE;
}


PEVENT_COUNTER
MiGetEventCounter (
    VOID
    )

/*++

Routine Description:

    This function maintains a list of "events" to allow waiting
    on segment operations (deletion, creation, purging).

Arguments:

    None.

Return Value:

    Event to be used for waiting (stored into the control area) or NULL if
    no event could be allocated.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSLIST_ENTRY SingleListEntry;
    PEVENT_COUNTER Support;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    if (ExQueryDepthSList (&MmEventCountSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmEventCountSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         EVENT_COUNTER,
                                         ListEntry);

            ASSERT (Support->RefCount == 0);
            KeClearEvent (&Support->Event);
            Support->RefCount = 1;
#if DBG
            Support->ListEntry.Next = NULL;
#endif
            return Support;
        }
    }

    Support = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(EVENT_COUNTER),
                                     'xEmM');
    if (Support == NULL) {
        return NULL;
    }

    KeInitializeEvent (&Support->Event, NotificationEvent, FALSE);

    Support->RefCount = 1;
#if DBG
    Support->ListEntry.Next = NULL;
#endif

    return Support;
}


VOID
MiFreeEventCounter (
    IN PEVENT_COUNTER Support
    )

/*++

Routine Description:

    This routine frees an event counter back to the free list.

Arguments:

    Support - Supplies a pointer to the event counter.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSLIST_ENTRY SingleListEntry;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    ASSERT (Support->RefCount != 0);
    ASSERT (Support->ListEntry.Next == NULL);

    //
    // An interlock is needed for the RefCount decrement as the event counter
    // is no longer pointed to by a control area and thus, any number of
    // threads can be running this code without any other serialization.
    //

    if (InterlockedDecrement ((PLONG)&Support->RefCount) == 0) {

        if (ExQueryDepthSList (&MmEventCountSListHead) < 4) {
            InterlockedPushEntrySList (&MmEventCountSListHead,
                                       &Support->ListEntry);
            return;
        }
        ExFreePool (Support);
    }

    //
    // If excess event blocks are stashed then free them now.
    //

    while (ExQueryDepthSList (&MmEventCountSListHead) > 4) {

        SingleListEntry = InterlockedPopEntrySList (&MmEventCountSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         EVENT_COUNTER,
                                         ListEntry);

            ExFreePool (Support);
        }
    }

    return;
}


BOOLEAN
MmCanFileBeTruncated (
    __in PSECTION_OBJECT_POINTERS SectionPointer,
    __in_opt PLARGE_INTEGER NewFileSize
    )

/*++

Routine Description:

    This routine does the following:

        1.  Checks to see if a image section is in use for the file,
            if so it returns FALSE.

        2.  Checks to see if a user section exists for the file, if
            it does, it checks to make sure the new file size is greater
            than the size of the file, if not it returns FALSE.

        3.  If no image section exists, and no user created data section
            exists or the file's size is greater, then TRUE is returned.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

    NewFileSize - Supplies a pointer to the size the file is getting set to.

Return Value:

    TRUE if the file can be truncated, FALSE if it cannot be.

Environment:

    Kernel mode.

--*/

{
    LARGE_INTEGER LocalOffset;
    KIRQL OldIrql;

    //
    //  Capture caller's file size, since we may modify it.
    //

    if (ARGUMENT_PRESENT(NewFileSize)) {

        LocalOffset = *NewFileSize;
        NewFileSize = &LocalOffset;
    }

    if (MiCanFileBeTruncatedInternal( SectionPointer, NewFileSize, FALSE, &OldIrql )) {

        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    return FALSE;
}

ULONG
MiCanFileBeTruncatedInternal (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN PLARGE_INTEGER NewFileSize OPTIONAL,
    IN LOGICAL BlockNewViews,
    OUT PKIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine does the following:

        1.  Checks to see if a image section is in use for the file,
            if so it returns FALSE.

        2.  Checks to see if a user section exists for the file, if
            it does, it checks to make sure the new file size is greater
            than the size of the file, if not it returns FALSE.

        3.  If no image section exists, and no user created data section
            exists or the files size is greater, then TRUE is returned.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

    NewFileSize - Supplies a pointer to the size the file is getting set to.

    BlockNewViews - Supplies TRUE if the caller will block new views while
                    the operation (usually a purge) proceeds.  This allows
                    this routine to return TRUE even if the user has section
                    references, provided the user currently has no mapped views.

    PreviousIrql - If returning TRUE, returns Irql to use when unlocking
                   Pfn database.

Return Value:

    TRUE if the file can be truncated (PFN locked).
    FALSE if it cannot be truncated (PFN not locked).

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    LARGE_INTEGER SegmentSize;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMAPPED_FILE_SEGMENT Segment;

    if (!MmFlushImageSection (SectionPointer, MmFlushForWrite)) {
        return FALSE;
    }

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);

    if (ControlArea != NULL) {

        if ((ControlArea->u.Flags.BeingCreated) ||
            (ControlArea->u.Flags.BeingDeleted) ||
            (ControlArea->u.Flags.Rom)) {
            goto UnlockAndReturn;
        }

        //
        // If there are user references and the size is less than the
        // size of the user view, don't allow the truncation.
        //

        if ((ControlArea->NumberOfUserReferences != 0) &&
            ((BlockNewViews == FALSE) || (ControlArea->NumberOfMappedViews != 0))) {

            //
            // You cannot truncate the entire section if there is a user
            // reference.
            //

            if (!ARGUMENT_PRESENT(NewFileSize)) {
                goto UnlockAndReturn;
            }

            //
            // Locate last subsection and get total size.
            //

            ASSERT (ControlArea->u.Flags.Image == 0);
            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

            Subsection = (PSUBSECTION)(ControlArea + 1);

            if (ControlArea->FilePointer != NULL) {
                Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

                if (MiIsAddressValid (Segment, TRUE)) {
                    if (Segment->LastSubsectionHint != NULL) {
                        Subsection = (PSUBSECTION) Segment->LastSubsectionHint;
                    }
                }
            }

            while (Subsection->NextSubsection != NULL) {
                Subsection = Subsection->NextSubsection;
            }

            ASSERT (Subsection->ControlArea == ControlArea);

            SegmentSize = MiEndingOffset(Subsection);

            if ((UINT64)NewFileSize->QuadPart < (UINT64)SegmentSize.QuadPart) {
                goto UnlockAndReturn;
            }

            //
            // If there are mapped views, we will skip the last page
            // of the section if the size passed in falls in that page.
            // The caller (like Cc) may want to clear this fractional page.
            //

            SegmentSize.QuadPart += PAGE_SIZE - 1;
            SegmentSize.LowPart &= ~(PAGE_SIZE - 1);
            if ((UINT64)NewFileSize->QuadPart < (UINT64)SegmentSize.QuadPart) {
                *NewFileSize = SegmentSize;
            }
        }
    }

    *PreviousIrql = OldIrql;
    return TRUE;

UnlockAndReturn:
    UNLOCK_PFN (OldIrql);
    return FALSE;
}

PFILE_OBJECT *
MmPerfUnusedSegmentsEnumerate (
    VOID
    )

/*++

Routine Description:

    This routine walks the MmUnusedSegmentList and returns 
    a pointer to a pool allocation containing the 
    referenced file object pointers.

Arguments:

    None.

Return Value:
    
    Returns a pointer to a NULL terminated pool allocation containing the
    file object pointers from the unused segment list, NULL if the memory
    could not be allocated.

    It is also the responsibility of the caller to dereference each
    file object in the list and then free the returned pool.

Environment:

    PASSIVE_LEVEL, arbitrary thread context.

--*/
{
    KIRQL OldIrql;
    ULONG SegmentCount;
    PFILE_OBJECT *FileObjects;
    PFILE_OBJECT *File;
    PLIST_ENTRY NextEntry;
    PCONTROL_AREA ControlArea;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

ReAllocate:

    SegmentCount = MmUnusedSegmentCount + 10;

    FileObjects = (PFILE_OBJECT *) ExAllocatePoolWithTag (
                                            NonPagedPool,
                                            SegmentCount * sizeof(PFILE_OBJECT),
                                            '01pM');

    if (FileObjects == NULL) {
        return NULL;
    }

    File = FileObjects;

    LOCK_PFN (OldIrql);

    //
    // Leave space for NULL terminator.
    //

    if (SegmentCount - 1 < MmUnusedSegmentCount) {
        UNLOCK_PFN (OldIrql);
        ExFreePool (FileObjects);
        goto ReAllocate;
    }

    NextEntry = MmUnusedSegmentList.Flink; 

    while (NextEntry != &MmUnusedSegmentList) {

        ControlArea = CONTAINING_RECORD (NextEntry,
                                         CONTROL_AREA,
                                         DereferenceList);

        *File = ControlArea->FilePointer;
        ObReferenceObject(*File);
        File += 1;

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_PFN (OldIrql);

    *File = NULL;

    return FileObjects;
}

#if DBG
PMSUBSECTION MiActiveSubsection;
LOGICAL MiRemoveSubsectionsFirst;

#define MI_DEREF_ACTION_SIZE 64

ULONG MiDerefActions[MI_DEREF_ACTION_SIZE];

#define MI_INSTRUMENT_DEREF_ACTION(i)       \
        ASSERT (i < MI_DEREF_ACTION_SIZE);   \
        MiDerefActions[i] += 1;

#else
#define MI_INSTRUMENT_DEREF_ACTION(i)
#endif


VOID
MiRemoveUnusedSegments (
    VOID
    )

/*++

Routine Description:

    This routine removes unused segments (no section references,
    no mapped views only PFN references that are in transition state).

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    LOGICAL DroppedPfnLock;
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    ULONG ConsecutiveFileLockFailures;
    ULONG ConsecutivePagingIOs;
    PSUBSECTION Subsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    PMSUBSECTION MappedSubsection;
    ULONG NumberOfPtes;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE ProtoPtes;
    PMMPTE ProtoPtes2;
    PMMPTE LastProtoPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    IO_STATUS_BLOCK IoStatus;
    LOGICAL DirtyPagesOk;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    ULONG ForceFree;
    ULONG LoopCount;
    PMMPAGE_FILE_EXPANSION PageExpand;

    LoopCount = 0;
    ConsecutivePagingIOs = 0;
    ConsecutiveFileLockFailures = 0;

    //
    // If overall system pool usage is acceptable, then don't discard
    // any cache.
    //

    MI_LOG_DEREF_INFO (0, 0, NULL);

    while ((MI_UNUSED_SEGMENTS_SURPLUS()) || (MmUnusedSegmentForceFree != 0)) {

        LoopCount += 1;
        MI_INSTRUMENT_DEREF_ACTION(1);

        if ((LoopCount & (64 - 1)) == 0) {

            MI_INSTRUMENT_DEREF_ACTION(2);

            //
            // Periodically delay so the mapped and modified writers get
            // a shot at writing out the pages this (higher priority) thread
            // is releasing.
            //

            ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

            while (!IsListEmpty (&MmDereferenceSegmentHeader.ListHead)) {

                MiSubsectionActions |= 0x8000000;

                //
                // The list is not empty, see if the first request is for
                // a commit extension and if so, process it now.
                //

                NextEntry = RemoveHeadList (&MmDereferenceSegmentHeader.ListHead);

                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

                ControlArea = CONTAINING_RECORD (NextEntry,
                                                 CONTROL_AREA,
                                                 DereferenceList);

                if (ControlArea->Segment != NULL) {

                    MI_INSTRUMENT_DEREF_ACTION(3);

                    //
                    // This is a control area deletion that needs to be
                    // processed (so the file object can be dereferenced, etc).
                    // Note the section object pointers have already been
                    // zeroed.
                    //

                    ControlArea->DereferenceList.Flink = NULL;

                    ASSERT (ControlArea->u.Flags.FilePointerNull == 1);
                    MiSegmentDelete (ControlArea->Segment);
                }
                else {

                    PageExpand = (PMMPAGE_FILE_EXPANSION) ControlArea;

                    if (PageExpand->RequestedExpansionSize == MI_CONTRACT_PAGEFILES) {
                        //
                        // Attempt to reduce the size of the paging files.
                        //
                        ASSERT (PageExpand == &MiPageFileContract);
                        MiAttemptPageFileReduction ();
                    }
                    else {
                        //
                        // This is a request to expand the paging files.
                        //
                        MiSubsectionActions |= 0x10000000;
                        MI_LOG_DEREF_INFO (1, 0, NULL);

                        MiExtendPagingFiles (PageExpand);
                        KeSetEvent (&PageExpand->Event, 0, FALSE);
                    }
                }

                ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);
            }

            ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

            //
            // If we are looping without freeing enough pool then
            // signal the cache manager to start unmapping
            // system cache views in an attempt to get back the paged
            // pool containing its prototype PTEs.
            //

            if (LoopCount >= 128) {
                MI_INSTRUMENT_DEREF_ACTION(55);
                if (CcUnmapInactiveViews (50) == TRUE) {
                    MI_INSTRUMENT_DEREF_ACTION(56);
                }
            }

            MI_LOG_DEREF_INFO (2, 0, NULL);

            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        }

        //
        // Eliminate some of the unused segments which are only
        // kept in memory because they contain transition pages.
        //

        Status = STATUS_SUCCESS;

        LOCK_PFN (OldIrql);

        if ((IsListEmpty(&MmUnusedSegmentList)) &&
            (IsListEmpty(&MmUnusedSubsectionList))) {

            //
            // There is nothing in the list, rewait.
            //

            MI_LOG_DEREF_INFO (3, 0, NULL);

            MI_INSTRUMENT_DEREF_ACTION(6);
            ForceFree = MmUnusedSegmentForceFree;
            MmUnusedSegmentForceFree = 0;
            ASSERT (MmUnusedSegmentCount == 0);
            UNLOCK_PFN (OldIrql);

            //
            // We weren't able to get as many segments or subsections as we
            // wanted.  So signal the cache manager to start unmapping
            // system cache views in an attempt to get back the paged
            // pool containing its prototype PTEs.  If Cc was able to free
            // any at all, then restart our loop.
            //

            if (CcUnmapInactiveViews (50) == TRUE) {
                LOCK_PFN (OldIrql);
                if (ForceFree > MmUnusedSegmentForceFree) {
                    MmUnusedSegmentForceFree = ForceFree;
                }
                MI_INSTRUMENT_DEREF_ACTION(7);
                UNLOCK_PFN (OldIrql);
                continue;
            }

            break;
        }

        MI_INSTRUMENT_DEREF_ACTION(8);

        if (MmUnusedSegmentForceFree != 0) {
            MmUnusedSegmentForceFree -= 1;
            MI_INSTRUMENT_DEREF_ACTION(9);
        }

#if DBG
        if (MiRemoveSubsectionsFirst == TRUE) {
            if (!IsListEmpty(&MmUnusedSubsectionList)) {
                goto ProcessSubsectionsFirst;
            }
        }
#endif

        if (IsListEmpty(&MmUnusedSegmentList)) {

#if DBG
ProcessSubsectionsFirst:
#endif

            MI_INSTRUMENT_DEREF_ACTION(10);

            //
            // The unused segment list was empty, go for the unused subsection
            // list instead.
            //

            ASSERT (!IsListEmpty(&MmUnusedSubsectionList));

            MiSubsectionsProcessed += 1;
            NextEntry = RemoveHeadList(&MmUnusedSubsectionList);

            MappedSubsection = CONTAINING_RECORD (NextEntry,
                                                  MSUBSECTION,
                                                  DereferenceList);

            ControlArea = MappedSubsection->ControlArea;

            ASSERT (ControlArea->u.Flags.Image == 0);
            ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);
            ASSERT (ControlArea->FilePointer != NULL);
            ASSERT (MappedSubsection->NumberOfMappedViews == 0);
            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            MI_LOG_DEREF_INFO (4, 0, ControlArea);

            //
            // Set the flink to NULL indicating this subsection
            // is not on any lists.
            //

            MappedSubsection->DereferenceList.Flink = NULL;

            if (ControlArea->u.Flags.BeingDeleted == 1) {
                MI_LOG_DEREF_INFO (5, 0, ControlArea);
                MI_INSTRUMENT_DEREF_ACTION(11);
                MiSubsectionActions |= 0x1;
                UNLOCK_PFN (OldIrql);
                ConsecutivePagingIOs = 0;
                continue;
            }

            if (ControlArea->u.Flags.NoModifiedWriting == 1) {
                MI_LOG_DEREF_INFO (6, 0, ControlArea);
                MiSubsectionActions |= 0x2;
                MI_INSTRUMENT_DEREF_ACTION(12);
                InsertTailList (&MmUnusedSubsectionList,
                                &MappedSubsection->DereferenceList);
                MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
                UNLOCK_PFN (OldIrql);
                ConsecutivePagingIOs = 0;
                continue;
            }

            //
            // Up the number of mapped views to prevent other threads
            // from freeing this.  Clear the accessed bit so we'll know
            // if another thread opens the subsection while we're flushing
            // and closes it before we finish the flush - the other thread
            // may have modified some pages which can then cause our
            // MiCleanSection call (which expects no modified pages in this
            // case) to deadlock with the filesystem.
            //

            MappedSubsection->NumberOfMappedViews = 1;
            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 0;

#if DBG
            MiActiveSubsection = MappedSubsection;
#endif

            //
            // Increment the number of mapped views on the control area to
            // prevent threads that are purging the section from deleting it
            // from underneath us while we process one of its subsections.
            //

            ControlArea->NumberOfMappedViews += 1;

            UNLOCK_PFN (OldIrql);

            ASSERT (MappedSubsection->SubsectionBase != NULL);

            PointerPte = &MappedSubsection->SubsectionBase[0];
            LastPte = &MappedSubsection->SubsectionBase
                            [MappedSubsection->PtesInSubsection - 1];

            //
            // Preacquire the file to prevent deadlocks with other flushers
            // Also mark ourself as a top level IRP so the filesystem knows
            // we are holding no other resources and that it can unroll if
            // it needs to in order to avoid deadlock.  Don't hold this
            // protection any longer than we need to.
            //

            MI_LOG_DEREF_INFO (0x10, 0, ControlArea);

            Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            MI_LOG_DEREF_INFO (0x11, Status, ControlArea);

            if (NT_SUCCESS(Status)) {
                PIRP tempIrp = (PIRP)FSRTL_FSP_TOP_LEVEL_IRP;

                MI_INSTRUMENT_DEREF_ACTION (13);
                IoSetTopLevelIrp (tempIrp);

                Status = MiFlushSectionInternal (PointerPte,
                                                 LastPte,
                                                 (PSUBSECTION) MappedSubsection,
                                                 (PSUBSECTION) MappedSubsection,
                                                 MM_FLUSH_FAIL_COLLISIONS | MM_FLUSH_IN_PARALLEL | MM_FLUSH_SEG_DEREF,
                                                 &IoStatus);

                MI_LOG_DEREF_INFO (0x12, Status, ControlArea);

                IoSetTopLevelIrp (NULL);

                //
                //  Now release the file.
                //

                FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);
            }
            else {
                MI_INSTRUMENT_DEREF_ACTION (14);
            }

            LOCK_PFN (OldIrql);

#if DBG
            MiActiveSubsection = NULL;
#endif

            //
            // Before checking for any failure codes, see if any other
            // threads accessed the subsection while the flush was ongoing.
            //
            // Note that beyond the case of another thread currently using
            // the subsection, the more subtle one is where another
            // thread accessed the subsection and modified some pages.
            // The flush needs to redone (so the clean is guaranteed to work)
            // before another clean can be issued.
            //
            // If any of these cases have occurred, grant this subsection
            // a reprieve.
            //

            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
            if ((MappedSubsection->NumberOfMappedViews != 1) ||
                (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1) ||
                (ControlArea->u.Flags.BeingDeleted == 1)) {

                MI_INSTRUMENT_DEREF_ACTION(15);
Requeue:
                MI_INSTRUMENT_DEREF_ACTION(16);
                ASSERT ((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1);
                MappedSubsection->NumberOfMappedViews -= 1;

                MiSubsectionActions |= 0x4;

                //
                // If the other thread(s) are done with this subsection,
                // it MUST be requeued here - otherwise if there are any
                // pages in the subsection, when they are reclaimed,
                // MiCheckForControlAreaDeletion checks for and expects
                // the control area to be queued on the unused segment list.
                //
                // Note this must be done very carefully because if the other
                // threads are not done with the subsection, it had better
                // not get put on the unused subsection list.
                //

                if ((MappedSubsection->NumberOfMappedViews == 0) &&
                    (ControlArea->u.Flags.BeingDeleted == 0)) {

                    MI_INSTRUMENT_DEREF_ACTION(17);
                    MiSubsectionActions |= 0x8;
                    ASSERT (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1);
                    ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

                    InsertTailList (&MmUnusedSubsectionList,
                                    &MappedSubsection->DereferenceList);

                    MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
                }

                ControlArea->NumberOfMappedViews -= 1;
                UNLOCK_PFN (OldIrql);

                MI_LOG_DEREF_INFO (0x13, 0, ControlArea);

                continue;
            }

            MI_INSTRUMENT_DEREF_ACTION(18);
            ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

            if (!NT_SUCCESS(Status)) {

                MiSubsectionActions |= 0x10;

                //
                // If the filesystem told us it had to unroll to avoid
                // deadlock OR we hit a mapped writer collision OR
                // the error occurred on a local file:
                //
                // Then requeue this at the end so we can try again later.
                //
                // Any other errors for networked files are assumed to be
                // permanent (ie: the link may have gone down for an indefinite
                // period), so these sections are cleaned regardless.
                //

                ASSERT ((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1);
                MappedSubsection->NumberOfMappedViews -= 1;

                InsertTailList (&MmUnusedSubsectionList,
                                &MappedSubsection->DereferenceList);

                MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);

                ControlArea->NumberOfMappedViews -= 1;

                UNLOCK_PFN (OldIrql);

                if (Status == STATUS_FILE_LOCK_CONFLICT) {
                    MI_INSTRUMENT_DEREF_ACTION(19);
                    ConsecutiveFileLockFailures += 1;
                }
                else {
                    MI_INSTRUMENT_DEREF_ACTION(20);
                    ConsecutiveFileLockFailures = 0;
                }

                //
                // 10 consecutive file locking failures means we need to
                // yield the processor to allow the filesystem to unjam.
                // Nothing magic about 10, just a number so it
                // gives the worker threads a chance to run.
                //

                MI_LOG_DEREF_INFO (0x14, ConsecutiveFileLockFailures, ControlArea);

                if (ConsecutiveFileLockFailures >= 10) {
                    MI_INSTRUMENT_DEREF_ACTION(21);
                    KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                    ConsecutiveFileLockFailures = 0;
                }
                continue;
            }

            //
            // The final check that must be made is whether any faults are
            // currently in progress which are backed by this subsection.
            // Note this is the perverse case where one thread in a process
            // has unmapped the relevant VAD even while other threads in the
            // same process are faulting on the addresses in that VAD (if the
            // VAD had not been unmapped then the subsection view count would
            // have been nonzero and caught above).  Clearly this is a bad
            // process, but nonetheless it must be detected and handled here
            // because upon conclusion of the inpage, the thread will compare
            // (unsynchronized) against the prototype PTEs which may in
            // various stages of deletion below and would cause corruption.
            //

            MI_INSTRUMENT_DEREF_ACTION(22);
            MiSubsectionActions |= 0x20;

            ASSERT (MappedSubsection->NumberOfMappedViews == 1);
            ProtoPtes = MappedSubsection->SubsectionBase;
            NumberOfPtes = MappedSubsection->PtesInSubsection;

            //
            // Note checking the prototype PTEs must be done carefully as
            // they are pageable and the PFN lock is (and must be) held.
            //

            ProtoPtes2 = ProtoPtes;
            LastProtoPte = ProtoPtes + NumberOfPtes;

            MI_LOG_DEREF_INFO (0x15, 0, ControlArea);

            while (ProtoPtes2 < LastProtoPte) {

                if ((ProtoPtes2 == ProtoPtes) ||
                    (MiIsPteOnPdeBoundary (ProtoPtes2))) {

                    if (MiCheckProtoPtePageState (ProtoPtes2, OldIrql, &DroppedPfnLock) == FALSE) {

                        //
                        // Skip this chunk as it is paged out and thus, cannot
                        // have any valid or transition PTEs within it.
                        //

                        ProtoPtes2 = (PMMPTE)(((ULONG_PTR)ProtoPtes2 | (PAGE_SIZE - 1)) + 1);
                        MI_INSTRUMENT_DEREF_ACTION(23);

                        MI_LOG_DEREF_INFO (0x16, 0, ControlArea);

                        continue;
                    }
                    else {

                        //
                        // The prototype PTE page is resident right now - but
                        // if the PFN lock was dropped & reacquired to make it
                        // so, then anything could have changed - so everything
                        // must be rechecked.
                        //

                        if (DroppedPfnLock == TRUE) {
                            if ((MappedSubsection->NumberOfMappedViews != 1) ||
                                (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1) ||
                                (ControlArea->u.Flags.BeingDeleted == 1)) {

                                MI_INSTRUMENT_DEREF_ACTION(57);
                                MiSubsectionActions |= 0x40;
                                MI_LOG_DEREF_INFO (0x17, 0, ControlArea);
                                goto Requeue;
                            }
                        }
                    }
                    MI_INSTRUMENT_DEREF_ACTION(24);
                }

                MI_INSTRUMENT_DEREF_ACTION(25);
                PteContents = *ProtoPtes2;
                if (PteContents.u.Hard.Valid == 1) {
                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x3,
                                  (ULONG_PTR)MappedSubsection,
                                  (ULONG_PTR)ProtoPtes2,
                                  (ULONG_PTR)PteContents.u.Long);
                }

                if (PteContents.u.Soft.Prototype == 1) {
                    MI_INSTRUMENT_DEREF_ACTION(26);
                    MiSubsectionActions |= 0x200;
                    NOTHING;        // This is the expected case.
                }
                else if (PteContents.u.Soft.Transition == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);

                    if (Pfn1->u3.e1.Modified == 1) {

                        //
                        // An I/O transfer finished after the last view was
                        // unmapped.  MmUnlockPages can set the modified bit
                        // in this situation so it must be handled properly
                        // here - ie: mark the subsection as needing to be
                        // reprocessed and march on.
                        //

                        MI_INSTRUMENT_DEREF_ACTION(27);
                        MiSubsectionActions |= 0x8000000;
                        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
                        MI_LOG_DEREF_INFO (0x18, 0, ControlArea);
                        goto Requeue;
                    }

                    if (Pfn1->u3.e2.ReferenceCount != 0) {

                        //
                        // A fault is being satisfied for deleted address space,
                        // so don't eliminate this subsection right now.
                        //

                        MI_INSTRUMENT_DEREF_ACTION(28);
                        MiSubsectionActions |= 0x400;
                        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
                        MI_LOG_DEREF_INFO (0x19, 0, ControlArea);
                        goto Requeue;
                    }
                    MiSubsectionActions |= 0x800;
                }
                else {
                    if (PteContents.u.Long != 0) {
                        KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                      0x4,
                                      (ULONG_PTR)MappedSubsection,
                                      (ULONG_PTR)ProtoPtes2,
                                      (ULONG_PTR)PteContents.u.Long);
                    }

                    MI_INSTRUMENT_DEREF_ACTION(29);
                    MiSubsectionActions |= 0x1000;
                }

                ProtoPtes2 += 1;
            }

            MiSubsectionActions |= 0x2000;
            MI_INSTRUMENT_DEREF_ACTION(30);

            //
            // There can be no modified pages in this subsection at this point.
            // Sever the subsection's tie to the prototype PTEs while still
            // holding the lock and then decrement the counts on any resident
            // prototype pages.
            //

            ASSERT (MappedSubsection->NumberOfMappedViews == 1);
            MappedSubsection->NumberOfMappedViews = 0;

            MappedSubsection->SubsectionBase = NULL;

            MiSubsectionActions |= 0x8000;
            ProtoPtes2 = ProtoPtes;

            MI_LOG_DEREF_INFO (0x1A, 0, ControlArea);

            while (ProtoPtes2 < LastProtoPte) {

                if ((ProtoPtes2 == ProtoPtes) ||
                    (MiIsPteOnPdeBoundary (ProtoPtes2))) {

                    if (MiCheckProtoPtePageState (ProtoPtes2, OldIrql, &DroppedPfnLock) == FALSE) {

                        //
                        // Skip this chunk as it is paged out and thus, cannot
                        // have any valid or transition PTEs within it.
                        //

                        ProtoPtes2 = (PMMPTE)(((ULONG_PTR)ProtoPtes2 | (PAGE_SIZE - 1)) + 1);
                        MI_INSTRUMENT_DEREF_ACTION(31);
                        continue;
                    }
                    else {

                        //
                        // The prototype PTE page is resident right now - but
                        // if the PFN lock was dropped & reacquired to make it
                        // so, then anything could have changed - but notice
                        // that the SubsectionBase was zeroed above before
                        // entering this loop, so even if the PFN lock was
                        // dropped & reacquired, nothing needs to be rechecked.
                        //
                    }
                    MI_INSTRUMENT_DEREF_ACTION(32);
                }

                MI_INSTRUMENT_DEREF_ACTION(33);
                PteContents = *ProtoPtes2;

                ASSERT (PteContents.u.Hard.Valid == 0);

                if (PteContents.u.Soft.Prototype == 1) {
                    MiSubsectionActions |= 0x10000;
                    MI_INSTRUMENT_DEREF_ACTION(34);
                    NOTHING;        // This is the expected case.
                }
                else if (PteContents.u.Soft.Transition == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);
                    ASSERT (Pfn1->u3.e1.Modified == 0);

                    //
                    // If the page is on the standby list, move it to the
                    // freelist.  If it's not on the standby list (ie: I/O
                    // is still in progress), when the Iast I/O completes, the
                    // page will be placed on the freelist as the PFN entry
                    // is always marked as deleted now.
                    //

                    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

                    MI_SET_PFN_DELETED (Pfn1);

                    ControlArea->NumberOfPfnReferences -= 1;
                    ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    ASSERT (Pfn1->u3.e1.PageLocation != FreePageList);

                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (PageFrameIndex);
                    MiSubsectionActions |= 0x20000;
                    MI_INSTRUMENT_DEREF_ACTION(35);
                }
                else {
                    MiSubsectionActions |= 0x80000;
                    ASSERT (PteContents.u.Long == 0);
                    MI_INSTRUMENT_DEREF_ACTION(36);
                }

                ProtoPtes2 += 1;
            }

            //
            // If all the cached pages for this control area have been removed
            // then delete it.  This will actually insert the control
            // area into the dereference segment header list.
            //

            ControlArea->NumberOfMappedViews -= 1;

#if DBG
            if ((ControlArea->NumberOfPfnReferences == 0) &&
                (ControlArea->NumberOfMappedViews == 0) &&
                (ControlArea->NumberOfSectionReferences == 0 )) {
                MiSubsectionActions |= 0x100000;
            }
#endif

            MI_INSTRUMENT_DEREF_ACTION(37);
            MiCheckForControlAreaDeletion (ControlArea);

            UNLOCK_PFN (OldIrql);

            ExFreePool (ProtoPtes);

            ConsecutiveFileLockFailures = 0;

            MI_LOG_DEREF_INFO (0x1B, 0, ControlArea);

            continue;
        }

        ASSERT (!IsListEmpty(&MmUnusedSegmentList));

        NextEntry = RemoveHeadList(&MmUnusedSegmentList);

        ControlArea = CONTAINING_RECORD (NextEntry,
                                         CONTROL_AREA,
                                         DereferenceList);

        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

#if DBG
        if (ControlArea->u.Flags.BeingDeleted == 0) {
          if (ControlArea->u.Flags.Image) {
            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject)) != NULL);
          }
          else {
            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
          }
        }
#endif

        //
        // Set the flink to NULL indicating this control area
        // is not on any lists.
        //

        MI_INSTRUMENT_DEREF_ACTION(38);
        ControlArea->DereferenceList.Flink = NULL;

        MI_LOG_DEREF_INFO (0x30, 0, ControlArea);

        if ((ControlArea->NumberOfMappedViews == 0) &&
            (ControlArea->NumberOfSectionReferences == 0) &&
            (ControlArea->u.Flags.BeingDeleted == 0)) {

            //
            // If there is paging I/O in progress on this
            // segment, just put this at the tail of the list, as
            // the call to MiCleanSegment would block waiting
            // for the I/O to complete.  As this could tie up
            // the thread, don't do it.  Check if these are the only
            // types of segments on the dereference list so we don't
            // spin forever and wedge the system.
            //

            if (ControlArea->ModifiedWriteCount > 0) {
                MI_INSERT_UNUSED_SEGMENT (ControlArea);

                UNLOCK_PFN (OldIrql);

                ConsecutivePagingIOs += 1;

                MI_LOG_DEREF_INFO (0x31, ConsecutivePagingIOs, ControlArea);

                if (ConsecutivePagingIOs > 10) {
                    KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                    MI_INSTRUMENT_DEREF_ACTION(39);
                    ConsecutivePagingIOs = 0;
                }
                MI_INSTRUMENT_DEREF_ACTION(40);
                continue;
            }
            ConsecutivePagingIOs = 0;

            //
            // Up the number of mapped views to prevent other threads
            // from freeing this.  Clear the accessed bit so we'll know
            // if another thread opens the control area while we're flushing
            // and closes it before we finish the flush - the other thread
            // may have modified some pages which can then cause our
            // MiCleanSection call (which expects no modified pages in this
            // case) to deadlock with the filesystem.
            //

            ControlArea->NumberOfMappedViews = 1;
            ControlArea->u.Flags.Accessed = 0;

            MI_INSTRUMENT_DEREF_ACTION(41);
            if (ControlArea->u.Flags.Image == 0) {

                ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);
                if (ControlArea->u.Flags.Rom == 0) {
                    Subsection = (PSUBSECTION)(ControlArea + 1);
                }
                else {
                    Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
                }

                MiSubsectionActions |= 0x200000;

                MI_INSTRUMENT_DEREF_ACTION(42);
                while (Subsection->SubsectionBase == NULL) {

                    Subsection = Subsection->NextSubsection;

                    if (Subsection == NULL) {

                        MiSubsectionActions |= 0x400000;

                        //
                        // All the subsections for this segment have already
                        // been trimmed so nothing left to flush.  Just get rid
                        // of the segment carcass provided no other thread
                        // accessed it while we weren't holding the PFN lock.
                        //

                        MI_INSTRUMENT_DEREF_ACTION(43);
                        UNLOCK_PFN (OldIrql);
                        MI_LOG_DEREF_INFO (0x32, 0, ControlArea);
                        goto skip_flush;
                    }
                    else {
                        MI_INSTRUMENT_DEREF_ACTION(44);
                        MiSubsectionActions |= 0x800000;
                    }
                }

                PointerPte = &Subsection->SubsectionBase[0];
                LastSubsection = Subsection;
                LastSubsectionWithProtos = Subsection;

                MI_INSTRUMENT_DEREF_ACTION(45);
                while (LastSubsection->NextSubsection != NULL) {
                    if (LastSubsection->SubsectionBase != NULL) {
                        LastSubsectionWithProtos = LastSubsection;
                        MiSubsectionActions |= 0x1000000;
                    }
                    else {
                        MiSubsectionActions |= 0x2000000;
                    }
                    LastSubsection = LastSubsection->NextSubsection;
                }

                if (LastSubsection->SubsectionBase == NULL) {
                    MiSubsectionActions |= 0x4000000;
                    LastSubsection = LastSubsectionWithProtos;
                }

                UNLOCK_PFN (OldIrql);

                LastPte = &LastSubsection->SubsectionBase
                                [LastSubsection->PtesInSubsection - 1];

                //
                // Preacquire the file to prevent deadlocks with other flushers
                // Also mark ourself as a top level IRP so the filesystem knows
                // we are holding no other resources and that it can unroll if
                // it needs to in order to avoid deadlock.  Don't hold this
                // protection any longer than we need to.
                //

                MI_LOG_DEREF_INFO (0x33, 0, ControlArea);

                Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

                MI_LOG_DEREF_INFO (0x34, Status, ControlArea);

                if (NT_SUCCESS(Status)) {
                    PIRP tempIrp = (PIRP)FSRTL_FSP_TOP_LEVEL_IRP;

                    MI_INSTRUMENT_DEREF_ACTION(46);
                    IoSetTopLevelIrp (tempIrp);

                    Status = MiFlushSectionInternal (PointerPte,
                                                     LastPte,
                                                     Subsection,
                                                     LastSubsection,
                                                     MM_FLUSH_FAIL_COLLISIONS | MM_FLUSH_IN_PARALLEL | MM_FLUSH_SEG_DEREF,
                                                     &IoStatus);
                    
                    IoSetTopLevelIrp(NULL);

                    //
                    //  Now release the file.
                    //

                    FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);
                }
                else {
                    MI_INSTRUMENT_DEREF_ACTION(47);
                }

                MI_LOG_DEREF_INFO (0x35, Status, ControlArea);

skip_flush:
                LOCK_PFN (OldIrql);
            }

            //
            // Before checking for any failure codes, see if any other
            // threads accessed the control area while the flush was ongoing.
            //
            // Note that beyond the case of another thread currently using
            // the control area, the more subtle one is where another
            // thread accessed the control area and modified some pages.
            // The flush needs to redone (so the clean is guaranteed to work)
            // before another clean can be issued.
            //
            // If any of these cases have occurred, grant this control area
            // a reprieve.
            //

            if (!((ControlArea->NumberOfMappedViews == 1) &&
                (ControlArea->u.Flags.Accessed == 0) &&
                (ControlArea->NumberOfSectionReferences == 0) &&
                (ControlArea->u.Flags.BeingDeleted == 0))) {

                ControlArea->NumberOfMappedViews -= 1;
                MI_INSTRUMENT_DEREF_ACTION(48);

                //
                // If the other thread(s) are done with this control area,
                // it MUST be requeued here - otherwise if there are any
                // pages in the control area, when they are reclaimed,
                // MiCheckForControlAreaDeletion checks for and expects
                // the control area to be queued on the unused segment list.
                //
                // Note this must be done very carefully because if the other
                // threads are not done with the control area, it had better
                // not get put on the unused segment list.
                //

                //
                // Need to do the equivalent of a MiCheckControlArea here.
                // or reprocess.  Only iff mappedview & sectref = 0.
                //

                if ((ControlArea->NumberOfMappedViews == 0) &&
                    (ControlArea->NumberOfSectionReferences == 0) &&
                    (ControlArea->u.Flags.BeingDeleted == 0)) {

                    ASSERT (ControlArea->u.Flags.Accessed == 1);
                    ASSERT(ControlArea->DereferenceList.Flink == NULL);

                    MI_INSERT_UNUSED_SEGMENT (ControlArea);
                }

                UNLOCK_PFN (OldIrql);
                MI_LOG_DEREF_INFO (0x36, 0, ControlArea);

                continue;
            }

            MI_INSTRUMENT_DEREF_ACTION(49);

            if (!NT_SUCCESS(Status)) {

                //
                // If the filesystem told us it had to unroll to avoid
                // deadlock OR we hit a mapped writer collision OR
                // the error occurred on a local file:
                //
                // Then requeue this at the end so we can try again later.
                //
                // Any other errors for networked files are assumed to be
                // permanent (ie: the link may have gone down for an indefinite
                // period), so these sections are cleaned regardless.
                //

                MI_INSTRUMENT_DEREF_ACTION(50);

                if ((Status == STATUS_FILE_LOCK_CONFLICT) ||
                    (Status == STATUS_ENCOUNTERED_WRITE_IN_PROGRESS) ||
                    (ControlArea->u.Flags.Networked == 0)) {

                    ASSERT(ControlArea->DereferenceList.Flink == NULL);

                    ControlArea->NumberOfMappedViews -= 1;

                    MI_INSERT_UNUSED_SEGMENT (ControlArea);

                    UNLOCK_PFN (OldIrql);

                    if (Status == STATUS_FILE_LOCK_CONFLICT) {
                        ConsecutiveFileLockFailures += 1;
                    }
                    else {
                        ConsecutiveFileLockFailures = 0;
                    }

                    //
                    // 10 consecutive file locking failures means we need to
                    // yield the processor to allow the filesystem to unjam.
                    // Nothing magic about 10, just a number so it
                    // gives the worker threads a chance to run.
                    //

                    MI_INSTRUMENT_DEREF_ACTION(51);

                    MI_LOG_DEREF_INFO (0x37, Status, ControlArea);

                    if (ConsecutiveFileLockFailures >= 10) {
                        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                        ConsecutiveFileLockFailures = 0;
                    }
                    continue;
                }
                DirtyPagesOk = TRUE;
            }
            else {
                MI_INSTRUMENT_DEREF_ACTION(52);
                ConsecutiveFileLockFailures = 0;
                DirtyPagesOk = FALSE;
            }

            ControlArea->u.Flags.BeingDeleted = 1;

            //
            // Don't let any pages be written by the modified
            // page writer from this point on.
            //

            ControlArea->u.Flags.NoModifiedWriting = 1;
            ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
            UNLOCK_PFN (OldIrql);

            MI_LOG_DEREF_INFO (0x38, 0, ControlArea);

            MI_INSTRUMENT_DEREF_ACTION(53);
            MiCleanSection (ControlArea, DirtyPagesOk);
        }
        else {

            //
            // The segment was not eligible for deletion.  Just leave
            // it off the unused segment list and continue the loop.
            //

            MI_LOG_DEREF_INFO (0x39, 0, ControlArea);

            MI_INSTRUMENT_DEREF_ACTION(54);
            UNLOCK_PFN (OldIrql);
            ConsecutivePagingIOs = 0;
        }
    }
    MI_LOG_DEREF_INFO (0xFF, 0, ControlArea);
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\sessload.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   sessload.c

Abstract:

    This module contains the routines which implement the loading of
    session space drivers.

--*/

#include "mi.h"

//
// This tracks allocated group virtual addresses.  The term SESSIONWIDE is used
// to denote data that is the same across all sessions (as opposed to
// per-session data which can vary from session to session).
//
// Since each driver loaded into a session space is linked and fixed up
// against the system image, it must remain at the same virtual address
// across the system regardless of the session.
//
// Access to these structures are generally guarded by the MmSystemLoadLock.
//

RTL_BITMAP MiSessionWideVaBitMap;

ULONG MiSessionUserCollisions;

NTSTATUS
MiSessionRemoveImage (
    IN PVOID BaseAddress
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MiSessionWideInitializeAddresses)

#pragma alloc_text(PAGE, MiSessionWideReserveImageAddress)
#pragma alloc_text(PAGE, MiRemoveImageSessionWide)
#pragma alloc_text(PAGE, MiShareSessionImage)

#pragma alloc_text(PAGE, MiSessionLookupImage)
#pragma alloc_text(PAGE, MiSessionUnloadAllImages)
#endif


LOGICAL
MiMarkImageInSystem (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine marks the given image as mapped into system space.

Arguments:

    ControlArea - Supplies the relevant control area.

Return Value:

    TRUE on success, FALSE on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    LOGICAL Status;
    KIRQL OldIrql;

    ASSERT (ControlArea->u.Flags.ImageMappedInSystemSpace == 0);

    Status = TRUE;

    //
    // Lock synchronization is not needed for our callers as they always hold
    // the system load mutant - but it is needed to modify this field in the
    // control area as other threads may be modifying other bits in the flags.
    //

    LOCK_PFN (OldIrql);

    //
    // Before handling the relocations for this image, ensure it
    // is not mapped in user space anywhere.  Note we have 1 user
    // reference at this point, so any beyond that are someone
    // elses and force us to pagefile-back this image.
    //

    if (ControlArea->NumberOfUserReferences <= 1) {

        ControlArea->u.Flags.ImageMappedInSystemSpace = 1;
        
        //
        // This flag is set so when the image is removed from the loaded
        // module list, the control area is destroyed.  This is required
        // because images mapped in session space inherit their PTE protections
        // from the shared prototype PTEs.
        //
        // Consider the following scenario :
        //
        // If image A is loaded at its based (preferred) address, and then
        // unloaded.  Image B is not rebased properly and then loads at
        // image A's preferred address.  Image A then reloads.
        //
        // Now image A cannot use the original prototype PTEs which enforce
        // readonly code, etc, because fixups will need to be done on it.
        //
        // Setting DeleteOnClose solves this problem by simply destroying
        // the entire control area on last unload.
        //
    
        ControlArea->u.Flags.DeleteOnClose = 1;
    }
    else {
        Status = FALSE;
    }

    UNLOCK_PFN (OldIrql);

    return Status;
}

NTSTATUS
MiShareSessionImage (
    IN PVOID MappedBase,
    IN PSECTION Section
    )

/*++

Routine Description:

    This routine maps the given image into the current session space.
    This allows the image to be executed backed by the image file in the
    filesystem and allow code and read-only data to be shared.

Arguments:

    MappedBase - Supplies the base address the image is to be mapped at.

    Section - Supplies a pointer to a section.

Return Value:

    Returns STATUS_SUCCESS on success, various NTSTATUS codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PFN_NUMBER NumberOfPtes;
    PMMPTE StartPte;
#if DBG
    PMMPTE EndPte;
#endif
    SIZE_T AllocationSize;
    NTSTATUS Status;
    SIZE_T CommittedPages;
    LOGICAL Relocated;
    PIMAGE_ENTRY_IN_SESSION DriverImage;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    if (MappedBase != Section->Segment->BasedAddress) {
        Relocated = TRUE;
    }
    else {
        Relocated = FALSE;
    }

    ASSERT (BYTE_OFFSET (MappedBase) == 0);

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    ControlArea = Section->Segment->ControlArea;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    Status = MiCheckPurgeAndUpMapCount (ControlArea, FALSE);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    NumberOfPtes = Section->Segment->TotalNumberOfPtes;
    AllocationSize = NumberOfPtes << PAGE_SHIFT;

    //
    // Calculate the PTE ranges and amount.
    //

    StartPte = MiGetPteAddress (MappedBase);

    //
    // The image commitment will be the same as the number of PTEs if the
    // image was not linked with native page alignment for the subsections.
    //
    // If it is linked native, then the commit will just be the number of
    // writable pages.  Note for this case, if we need to relocate it, then
    // we need to charge for the full number of PTEs and bump the commit
    // charge in the segment so that the return on unload is correct also.
    //

    ASSERT (Section->Segment->u1.ImageCommitment != 0);
    ASSERT (Section->Segment->u1.ImageCommitment <= NumberOfPtes);

    if (Relocated == TRUE) {
        CommittedPages = NumberOfPtes;
    }
    else {
        CommittedPages = Section->Segment->u1.ImageCommitment;
    }

    if (MiChargeCommitment (CommittedPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);

        //
        // Don't bother releasing the page tables or their commit here, another
        // load will happen shortly or the whole session will go away.  On
        // session exit everything will be released automatically.
        //

        MiDereferenceControlArea (ControlArea);
        return STATUS_NO_MEMORY;
    }

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                 CommittedPages);

    //
    // Make sure we have page tables for the PTE
    // entries we must fill in the session space structure.
    //

    Status = MiSessionCommitPageTables (MappedBase,
                                        (PVOID)((PCHAR)MappedBase + AllocationSize));

    if (!NT_SUCCESS(Status)) {

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CommittedPages);

        MiDereferenceControlArea (ControlArea);
        MiReturnCommitment (CommittedPages);

        return STATUS_NO_MEMORY;
    }

#if DBG
    EndPte = StartPte + NumberOfPtes;
    while (StartPte < EndPte) {
        ASSERT (StartPte->u.Long == 0);
        StartPte += 1;
    }

    StartPte = MiGetPteAddress (MappedBase);
#endif

    //
    // If the image was linked with subsection alignment of >= PAGE_SIZE,
    // then all of the prototype PTEs were initialized to proper protections by
    // the initial section creation.  The protections in these PTEs are used
    // to fill the actual PTEs as each address is faulted on.
    //
    // If the image has less than PAGE_SIZE section alignment, then 
    // section creation uses a single subsection to map the entire file and
    // sets all the prototype PTEs to copy on write.  For this case, the
    // appropriate permissions are set by the MiWriteProtectSystemImage below.
    //

    //
    // Initialize the PTEs to point at the prototype PTEs.
    //

    Status = MiAddMappedPtes (StartPte, NumberOfPtes, ControlArea);

    if (!NT_SUCCESS (Status)) {

        //
        // Regardless of whether the PTEs were mapped, leave the control area
        // marked as mapped in system space so user applications cannot map the
        // file as an image as clearly the intent is to run it as a driver.
        //

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CommittedPages);

        MiDereferenceControlArea (ControlArea);
        MiReturnCommitment (CommittedPages);
    	return Status;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_SHARED_IMAGE, CommittedPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_SYSMAPPED_PAGES_COMMITTED, (ULONG)CommittedPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_SYSMAPPED_PAGES_ALLOC, (ULONG)NumberOfPtes);

    //
    // No session space image faults may be taken until these fields of the
    // image entry are initialized.
    //

    DriverImage = MiSessionLookupImage (MappedBase);
    ASSERT (DriverImage);

    DriverImage->LastAddress = (PVOID)((PCHAR)MappedBase + AllocationSize - 1);
    DriverImage->PrototypePtes = Subsection->SubsectionBase;

    //
    // The loaded module list section reference protects the image from
    // being purged while it's in use.
    //

    MiDereferenceControlArea (ControlArea);

    return STATUS_SUCCESS;
}


NTSTATUS
MiSessionInsertImage (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This routine allocates an image entry for the specified address in the
    current session space.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    STATUS_SUCCESS or various NTSTATUS error codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.
    
    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;
    PIMAGE_ENTRY_IN_SESSION NewImage;
    PETHREAD Thread;
    PMMSUPPORT Ws;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    Thread = PsGetCurrentThread ();

    //
    // Create and initialize a new image entry prior to acquiring the session
    // space ws mutex.  This is to reduce the amount of time the mutex is held.
    // If an existing entry is found this allocation is just discarded.
    //

    NewImage = ExAllocatePoolWithTag (NonPagedPool,
                                      sizeof(IMAGE_ENTRY_IN_SESSION),
                                      'iHmM');

    if (NewImage == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (NewImage, sizeof(IMAGE_ENTRY_IN_SESSION));

    NewImage->Address = BaseAddress;
    NewImage->ImageCountInThisSession = 1;

    //
    // Check to see if the address is already loaded.
    //

    Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;

    LOCK_WORKING_SET (Thread, Ws);

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {
        Image = CONTAINING_RECORD (NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddress) {
            Image->ImageCountInThisSession += 1;
            UNLOCK_WORKING_SET (Thread, Ws);
            ExFreePool (NewImage);
            return STATUS_ALREADY_COMMITTED;
        }
        NextEntry = NextEntry->Flink;
    }

    //
    // Insert the image entry into the session space structure.
    //

    InsertTailList (&MmSessionSpace->ImageList, &NewImage->Link);

    UNLOCK_WORKING_SET (Thread, Ws);

    return STATUS_SUCCESS;
}


NTSTATUS
MiSessionRemoveImage (
    PVOID BaseAddr
    )

/*++

Routine Description:

    This routine removes the given image entry from the current session space.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND if the image is not
    in the current session space.

Environment:

    Kernel mode, APC_LEVEL and below.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    PETHREAD Thread;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;
    PMMSUPPORT Ws;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    Thread = PsGetCurrentThread ();

    Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;

    LOCK_WORKING_SET (Thread, Ws);

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddr) {

            RemoveEntryList (NextEntry);

            UNLOCK_WORKING_SET (Thread, Ws);

            ASSERT (MmSessionSpace->ImageLoadingCount >= 0);

            if (Image->ImageLoading == TRUE) {
                ASSERT (MmSessionSpace->ImageLoadingCount > 0);
                InterlockedDecrement (&MmSessionSpace->ImageLoadingCount);
            }

            ExFreePool (Image);
            return STATUS_SUCCESS;
        }

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_WORKING_SET (Thread, Ws);

    return STATUS_NOT_FOUND;
}


PIMAGE_ENTRY_IN_SESSION
MiSessionLookupImage (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This routine looks up the image entry within the current session by the 
    specified base address.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    The image entry within this session on success or NULL on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddress) {
            return Image;
        }

        NextEntry = NextEntry->Flink;
    }

    return NULL;
}


VOID
MiSessionUnloadAllImages (
    VOID
    )

/*++

Routine Description:

    This routine dereferences each image that has been loaded in the
    current session space.

    As each image is dereferenced, checks are made:

    If this session's reference count to the image reaches zero, the VA
    range in this session is deleted.  If the reference count to the image
    in the SESSIONWIDE list drops to zero, then the SESSIONWIDE's VA
    reservation is removed and the address space is made available to any
    new image.

    If this is the last systemwide reference to the driver then the driver
    is deleted from memory.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  This is called in one of two contexts:
        1. the last thread in the last process of the current session space.
        2. or by any thread in the SMSS process.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    NTSTATUS Status;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Module;
    PKLDR_DATA_TABLE_ENTRY ImageHandle;

    ASSERT (MmSessionSpace->ReferenceCount == 0);

    //
    // The session's working set lock does not need to be acquired here since
    // no thread can be faulting on these addresses.
    //

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Module = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        //
        // Lookup the image entry in the system PsLoadedModuleList,
        // unload the image and delete it.
        //

        ImageHandle = MiLookupDataTableEntry (Module->Address, FALSE);

        ASSERT (ImageHandle);

        Status = MmUnloadSystemImage (ImageHandle);

        //
        // Restart the search at the beginning since the entry has been deleted.
        //

        ASSERT (MmSessionSpace->ReferenceCount == 0);

        NextEntry = MmSessionSpace->ImageList.Flink;
    }
}


VOID
MiSessionWideInitializeAddresses (
    VOID
    )

/*++

Routine Description:

    This routine is called at system initialization to set up the group
    address list.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID Bitmap;
    SIZE_T NumberOfPages;

    NumberOfPages = (MiSessionImageEnd - MiSessionImageStart) >> PAGE_SHIFT;

    Bitmap = ExAllocatePoolWithTag (PagedPool,
                                    ((NumberOfPages + 31) / 32) * 4,
                                    '  mM');

    if (Bitmap == NULL) {
        KeBugCheckEx (INSTALL_MORE_MEMORY,
                      MmNumberOfPhysicalPages,
                      MmLowestPhysicalPage,
                      MmHighestPhysicalPage,
                      0x301);
    }

    RtlInitializeBitMap (&MiSessionWideVaBitMap,
                         Bitmap,
                         (ULONG) NumberOfPages);

    RtlClearAllBits (&MiSessionWideVaBitMap);

    return;
}

NTSTATUS
MiSessionWideReserveImageAddress (
    IN PSECTION Section,
    OUT PVOID *AssignedAddress,
    OUT PSECTION *NewSectionPointer
    )

/*++

Routine Description:

    This routine allocates a range of virtual address space within
    session space.  This address range is unique system-wide and in this
    manner, code and pristine data of session drivers can be shared across
    multiple sessions.

    This routine does not actually commit pages, but reserves the virtual
    address region for the named image.  An entry is created here and attached
    to the current session space to track the loaded image.  Thus if all
    the references to a given range go away, the range can then be reused.

Arguments:

    Section - Supplies the section (and thus, the preferred address that the
              driver has been linked (rebased) at.  If this address is
              available, the driver will require no relocation.  The section
              is also used to derive the number of bytes to reserve.

    AssignedAddress - Supplies a pointer to a variable that receives the
                      allocated address if the routine succeeds.

Return Value:

    Returns STATUS_SUCCESS on success, various NTSTATUS codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    ULONG StartPosition;
    ULONG NumberOfPtes;
    NTSTATUS Status;
    PWCHAR pName;
    PVOID NewAddress;
    ULONG_PTR SessionSpaceEnd;
    PVOID PreferredAddress;
    PCONTROL_AREA ControlArea;
    PIMAGE_ENTRY_IN_SESSION Image;
    PSESSION_GLOBAL_SUBSECTION_INFO GlobalSubs;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION);
    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);
    ASSERT (Section->u.Flags.Image == 1);

    *NewSectionPointer = NULL;

    ControlArea = Section->Segment->ControlArea;

    if (ControlArea->u.Flags.ImageMappedInSystemSpace == 1) {

        //
        // We are going to add a new entry to the loaded module list.  We
        // have a section in hand.  The case which must be handled carefully is:
        //
        // When a session image unloads, its entry is removed from the
        // loaded module list, but the section object itself may continue
        // to live on due to other references on the object.  For session
        // images, the relocations and import snaps, verifier updates, etc,
        // are done directly to the prototype PTEs (the modified ones become
        // backed by the pagefile) at whatever address the image is loaded at.
        //
        // Thus, if the image's load address the second time around is different
        // from the first time around, this presents problems.  Likewise, any
        // image it imports from is an issue, since if their load addresses
        // change, the IAT snaps need updating.  This only gets more complicated
        // with recursive imports, imports where only some of the images have
        // lingering object reference counts, etc.
        //
        // There is a lingering object reference to this image since it
        // was last unloaded.  It's ok just to fail this since it
        // can't be a user-generated reference and any kernel/driver
        // SEC_IMAGE reference would be extremely unusual (and short lived).
        //

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_IMAGE_ZOMBIE);

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        return STATUS_CONFLICTING_ADDRESSES;
    }

    pName = NULL;

    StartPosition = NO_BITS_FOUND;
    PreferredAddress = Section->Segment->BasedAddress;
    NumberOfPtes = Section->Segment->TotalNumberOfPtes;

    SessionSpaceEnd = MiSessionImageEnd;

    //
    // Try to put the module into its requested address so it can be shared.
    //
    // If the requested address is not properly aligned or not in the session
    // space region, pick an address for it.  This image will not be shared.
    //

    if ((BYTE_OFFSET (PreferredAddress) == 0) &&
        (PreferredAddress >= (PVOID) MiSessionImageStart) &&
	    (PreferredAddress < (PVOID) MiSessionImageEnd)) {

        StartPosition = (ULONG) ((ULONG_PTR) PreferredAddress - MiSessionImageStart) >> PAGE_SHIFT;

        if (RtlAreBitsClear (&MiSessionWideVaBitMap,
                             StartPosition,
                             NumberOfPtes) == TRUE) {

            RtlSetBits (&MiSessionWideVaBitMap,
                        StartPosition,
                        NumberOfPtes);
        }
        else {
            PreferredAddress = NULL;
        }
    }
    else {
        PreferredAddress = NULL;
    }

    if (PreferredAddress == NULL) {

        StartPosition = RtlFindClearBitsAndSet (&MiSessionWideVaBitMap,
                                                NumberOfPtes,
                                                0);

        if (StartPosition == NO_BITS_FOUND) {
            MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_IMAGE_VA_SPACE);
            return STATUS_NO_MEMORY;
        }

    }

    NewAddress = (PVOID) (MiSessionImageStart + ((ULONG_PTR) StartPosition << PAGE_SHIFT));

    //
    // Create an entry for this image in the current session space.
    //

    Status = MiSessionInsertImage (NewAddress);

    if (!NT_SUCCESS (Status)) {

Failure1:
        ASSERT (RtlAreBitsSet (&MiSessionWideVaBitMap,
                               StartPosition,
                               NumberOfPtes) == TRUE);

        RtlClearBits (&MiSessionWideVaBitMap,
                      StartPosition,
                      NumberOfPtes);

        return Status;
    }

    *AssignedAddress = NewAddress;

    GlobalSubs = NULL;

    //
    // This is the first load of this image in any session, so mark this
    // image so that copy-on-write flows through into read-write until
    // relocations (if any) and import image resolution is finished updating
    // all parts of this image.  This way all future instantiations of this
    // image won't need to reprocess them (and can share the pages).
    // This is deliberately done this way so that any concurrent usermode
    // access to this image does not receive direct read-write privilege.
    //
    // Note images with less than native page subsection alignment are
    // currently marked copy on write for the entire image.  Native page
    // aligned images have individual subsections with associated
    // permissions.  Both image types get temporarily mapped read-write in
    // their first session mapping.
    //
    // After everything (relocations & image imports) is done, then
    // the real permissions (based on the PE header) can be applied and
    // the real PTEs automatically inherit the proper permissions
    // from the prototype PTEs.
    //
    // Since the fixups are only done once, they can then be
    // shared by any subsequent driver instantiations.  Note that
    // any fixed-up pages are never written to the image, but are
    // instead converted to pagefile backing by the modified writer.
    //

    if (MiMarkImageInSystem (ControlArea) == FALSE) {

        ULONG Count;
        SIZE_T ViewSize;
        PVOID SrcVa;
        PVOID DestVa;
        PVOID SourceVa;
        PVOID DestinationVa;
        MMPTE PteContents;
        PMMPTE ProtoPte;
        PMMPTE PointerPte;
        PMMPTE LastPte;
        PFN_NUMBER ResidentPages;
        HANDLE NewSectionHandle;
        LARGE_INTEGER MaximumSectionSize;
        OBJECT_ATTRIBUTES ObjectAttributes;
        PSUBSECTION Subsection;
        PSUBSECTION SubsectionBase;
        PMMPTE PrototypePteBase;

        if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
            (ControlArea->u.Flags.Rom == 0)) {
            Subsection = (PSUBSECTION)(ControlArea + 1);
        }
        else {
            Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }

        SubsectionBase = Subsection;
        PrototypePteBase = Subsection->SubsectionBase;

        //
        // Count the number of global subsections.
        //

        Count = 0;

        do {

            if (Subsection->u.SubsectionFlags.GlobalMemory == 1) {
                Count += 1;
            }

            Subsection = Subsection->NextSubsection;

        } while (Subsection != NULL);

        //
        // Allocate pool to store the global subsection information as this
        // multi subsection image is going to be converted to a single
        // subsection pagefile section.
        //

        if (Count != 0) {

            GlobalSubs = ExAllocatePoolWithTag (PagedPool,
                                                (Count + 1) * sizeof (SESSION_GLOBAL_SUBSECTION_INFO),
                                                'sGmM');

            if (GlobalSubs == NULL) {
                MiSessionRemoveImage (NewAddress);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto Failure1;
            }

            GlobalSubs[Count].PteCount = 0;     // NULL-terminate the list.
            Count -= 1;

            Subsection = SubsectionBase;

            do {

                if (Subsection->u.SubsectionFlags.GlobalMemory == 1) {

                    GlobalSubs[Count].PteIndex = Subsection->SubsectionBase - PrototypePteBase;
                    GlobalSubs[Count].PteCount = Subsection->PtesInSubsection;
                    GlobalSubs[Count].Protection = Subsection->u.SubsectionFlags.Protection;

                    if (Count == 0) {
                        break;
                    }
                    Count -= 1;
                }

                Subsection = Subsection->NextSubsection;

            } while (Subsection != NULL);
            ASSERT (Count == 0);
        }

        MaximumSectionSize.QuadPart = ((ULONG64) NumberOfPtes) << PAGE_SHIFT;
        ViewSize = 0;

        InitializeObjectAttributes (&ObjectAttributes,
                                    NULL,
                                    (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                    NULL,
                                    NULL);

        //
        // Create a pagefile-backed section to copy the image into.
        //

        Status = ZwCreateSection (&NewSectionHandle,
                                  SECTION_ALL_ACCESS,
                                  &ObjectAttributes,
                                  &MaximumSectionSize,
                                  PAGE_EXECUTE_READWRITE,
                                  SEC_COMMIT,
                                  NULL);

        if (!NT_SUCCESS (Status)) {
            if (GlobalSubs != NULL) {
                ExFreePool (GlobalSubs);
            }
            MiSessionRemoveImage (NewAddress);
            goto Failure1;
        }

        //
        // Now reference the section handle.  If this fails something is
        // very wrong because it is a kernel handle.
        //
        // N.B.  ObRef sets SectionPointer to NULL on failure.
        //

        Status = ObReferenceObjectByHandle (NewSectionHandle,
                                            SECTION_MAP_EXECUTE,
                                            MmSectionObjectType,
                                            KernelMode,
                                            (PVOID *) NewSectionPointer,
                                            (POBJECT_HANDLE_INFORMATION) NULL);

        ZwClose (NewSectionHandle);

        if (!NT_SUCCESS (Status)) {
            if (GlobalSubs != NULL) {
                ExFreePool (GlobalSubs);
            }
            MiSessionRemoveImage (NewAddress);
            goto Failure1;
        }

        //
        // Map the destination.  Deliberately put the destination in system
        // space and the source in session space to increase the chances
        // that enough virtual address space can be found.
        //

        Status = MmMapViewInSystemSpace (*NewSectionPointer,
                                         &DestinationVa,
                                         &ViewSize);

        if (!NT_SUCCESS (Status)) {
            if (GlobalSubs != NULL) {
                ExFreePool (GlobalSubs);
            }
            ObDereferenceObject (*NewSectionPointer);
            MiSessionRemoveImage (NewAddress);
            goto Failure1;
        }

        //
        // Map the source.  Note this choice of session view space is crucial
        // for the mapping because MiResolveDemandZeroFault explictly checks
        // for fault addresses in this range to decide whether to provide a
        // zero (vs just a free) page to the caller.
        //

        Status = MmMapViewInSessionSpace (Section, &SourceVa, &ViewSize);

        if (!NT_SUCCESS (Status)) {
            if (GlobalSubs != NULL) {
                ExFreePool (GlobalSubs);
            }
            MmUnmapViewInSystemSpace (DestinationVa);
            ObDereferenceObject (*NewSectionPointer);
            MiSessionRemoveImage (NewAddress);
            goto Failure1;
        }

        //
        // Copy the pristine executable.
        //

        ProtoPte = Section->Segment->PrototypePte;
        LastPte = ProtoPte + NumberOfPtes;
        SrcVa = SourceVa;
        DestVa = DestinationVa;

        while (ProtoPte < LastPte) {

            PteContents = *ProtoPte;

            if ((PteContents.u.Hard.Valid == 1) ||
                (PteContents.u.Soft.Protection != MM_NOACCESS)) {

                RtlCopyMemory (DestVa, SrcVa, PAGE_SIZE);
            }
            else {

                //
                // The source PTE is no access, just leave the destination
                // PTE as demand zero.
                //
            }

            ProtoPte += 1;
            SrcVa = ((PCHAR)SrcVa + PAGE_SIZE);
            DestVa = ((PCHAR)DestVa + PAGE_SIZE);
        }

        Status = MmUnmapViewInSystemSpace (DestinationVa);

        if (!NT_SUCCESS (Status)) {
            ASSERT (FALSE);
        }

        //
        // Delete the source image pages as the BSS ones have been expanded
        // into private demand zero as part of the copy above.  If we don't
        // delete them all, the private demand zero would go on the modified
        // list with a PTE address pointing at the session view space which
        // will have been reused.
        //

        PointerPte = MiGetPteAddress (SourceVa);

        MiDeleteSystemPageableVm (PointerPte,
                                 NumberOfPtes,
                                 MI_DELETE_FLUSH_TB,
                                 &ResidentPages);

        MI_INCREMENT_RESIDENT_AVAILABLE (ResidentPages,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE);

        Status = MmUnmapViewInSessionSpace (SourceVa);

        if (!NT_SUCCESS (Status)) {
            ASSERT (FALSE);
        }

        //
        // Our caller will be using the new pagefile-backed section we
        // just created.  Copy over useful fields now and then dereference
        // the entry section.
        //

        ((PSECTION)*NewSectionPointer)->Segment->u1.ImageCommitment =
                                        Section->Segment->u1.ImageCommitment;

        ((PSECTION)*NewSectionPointer)->Segment->BasedAddress =
                                        Section->Segment->BasedAddress;

        ObDereferenceObject (Section);
    }
    else {
        *NewSectionPointer = NULL;
    }

    Image = MiSessionLookupImage (NewAddress);

    if (Image != NULL) {

        ASSERT (Image->GlobalSubs == NULL);
        Image->GlobalSubs = GlobalSubs;

        ASSERT (Image->ImageLoading == FALSE);
        Image->ImageLoading = TRUE;

        ASSERT (MmSessionSpace->ImageLoadingCount >= 0);
        InterlockedIncrement (&MmSessionSpace->ImageLoadingCount);
    }
    else {
        ASSERT (FALSE);
    }

    return STATUS_SUCCESS;
}

VOID
MiRemoveImageSessionWide (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry OPTIONAL,
    IN PVOID BaseAddress,
    IN ULONG_PTR NumberOfBytes
    )

/*++

Routine Description:

    Delete the image space region from the current session space.
    This dereferences the globally allocated SessionWide region.
    
    The SessionWide region will be deleted if the reference count goes to zero.
    
Arguments:

    DataTableEntry - Supplies the (optional) loader entry.

    BaseAddress - Supplies the address the driver is loaded at.

    NumberOfBytes - Supplies the number of bytes used by the driver.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    ULONG StartPosition;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    //
    // There is no data table entry if we are encountering an error during
    // the driver's first load (one hasn't been created yet).  But we still
    // need to clear out the in-use bits.
    //

    if ((DataTableEntry == NULL) || (DataTableEntry->LoadCount == 1)) {

        StartPosition = (ULONG)(((ULONG_PTR) BaseAddress - MiSessionImageStart) >> PAGE_SHIFT);

        ASSERT (RtlAreBitsSet (&MiSessionWideVaBitMap,
                               StartPosition,
                               (ULONG) (NumberOfBytes >> PAGE_SHIFT)) == TRUE);

        RtlClearBits (&MiSessionWideVaBitMap,
                      StartPosition,
                      (ULONG) (NumberOfBytes >> PAGE_SHIFT));
    }

    //
    // Remove the image reference from the current session space.
    //

    MiSessionRemoveImage (BaseAddress);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\shutdown.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

    shutdown.c

Abstract:

    This module contains the shutdown code for the memory management system.

--*/

#include "mi.h"

extern ULONG MmSystemShutdown;

typedef struct _MM_ZERO_PAGEFILE_CONTEXT {
    WORK_QUEUE_ITEM WorkItem;
    PMMPAGING_FILE PagingFile;
    PFN_NUMBER ZeroedPageFrame;
    PKEVENT AllDone;
} MM_ZERO_PAGEFILE_CONTEXT, *PMM_ZERO_PAGEFILE_CONTEXT;

VOID
MiReleaseAllMemory (
    VOID
    );

BOOLEAN
MiShutdownSystem (
    VOID
    );

VOID
MiZeroPageFile (
    IN PVOID Context
    );

LOGICAL
MiZeroAllPageFiles (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGELK,MiZeroPageFile)
#pragma alloc_text(PAGELK,MiZeroAllPageFiles)
#pragma alloc_text(PAGELK,MiShutdownSystem)
#pragma alloc_text(PAGELK,MiReleaseAllMemory)
#pragma alloc_text(PAGELK,MmShutdownSystem)
#endif

ULONG MmZeroPageFile;

extern ULONG MmUnusedSegmentForceFree;
extern LIST_ENTRY MiVerifierDriverAddedThunkListHead;
extern LIST_ENTRY MmLoadedUserImageList;
extern LOGICAL MiZeroingDisabled;
extern ULONG MmNumberOfMappedMdls;
extern ULONG MmNumberOfMappedMdlsInUse;

VOID
MiZeroPageFile (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine zeroes all inactive pagefile blocks in the specified paging
    file.

Arguments:

    Context - Supplies the information on which pagefile to zero and a zeroed
              page to use for the I/O.

Return Value:

    Returns TRUE on success, FALSE on failure.

Environment:

    Kernel mode, the caller must lock down PAGELK.

--*/

{
    PFN_NUMBER MaxPagesToWrite;
    PMMPFN Pfn1;
    PPFN_NUMBER Page;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_WRITE_CLUSTER];
    PMDL Mdl;
    NTSTATUS Status;
    KEVENT IoEvent;
    IO_STATUS_BLOCK IoStatus;
    KIRQL OldIrql;
    LARGE_INTEGER StartingOffset;
    ULONG count;
    ULONG i;
    PFN_NUMBER first;
    ULONG write;
    PKEVENT AllDone;
    SIZE_T NumberOfBytes;
    PMMPAGING_FILE PagingFile;
    PFN_NUMBER ZeroedPageFrame;
    PMM_ZERO_PAGEFILE_CONTEXT ZeroContext;

    ZeroContext = (PMM_ZERO_PAGEFILE_CONTEXT) Context;

    PagingFile = ZeroContext->PagingFile;
    ZeroedPageFrame = ZeroContext->ZeroedPageFrame;
    AllDone = ZeroContext->AllDone;

    ExFreePool (Context);

    NumberOfBytes = MmModifiedWriteClusterSize << PAGE_SHIFT;
    MaxPagesToWrite = NumberOfBytes >> PAGE_SHIFT;

    Mdl = (PMDL) MdlHack;
    Page = (PPFN_NUMBER)(Mdl + 1);

    KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

    MmInitializeMdl (Mdl, NULL, PAGE_SIZE);

    Mdl->MdlFlags |= MDL_PAGES_LOCKED;

    Mdl->StartVa = NULL;

    i = 0;
    Page = (PPFN_NUMBER)(Mdl + 1);

    for (i = 0; i < MaxPagesToWrite; i += 1) {
        *Page = ZeroedPageFrame;
        Page += 1;
    }

    count = 0;
    write = FALSE;

    SATISFY_OVERZEALOUS_COMPILER (first = 0);

    LOCK_PFN (OldIrql);

    for (i = 1; i < PagingFile->Size; i += 1) {

        if (RtlCheckBit (PagingFile->Bitmap, (ULONG) i) == 0) {

            //
            // Claim the pagefile location as the modified writer
            // may already be scanning.
            //

            RtlSetBit (PagingFile->Bitmap, (ULONG) i);

            if (count == 0) {
                first = i;
            }

            count += 1;

            if ((count == MaxPagesToWrite) || (i == PagingFile->Size - 1)) {
                write = TRUE;
            }
        }
        else {
            if (count != 0) {

                //
                // Issue a write.
                //

                write = TRUE;
            }
        }

        if (write) {

            UNLOCK_PFN (OldIrql);

            StartingOffset.QuadPart = (LONGLONG)first << PAGE_SHIFT;
            Mdl->ByteCount = count << PAGE_SHIFT;
            KeClearEvent (&IoEvent);

            Status = IoSynchronousPageWrite (PagingFile->File,
                                             Mdl,
                                             &StartingOffset,
                                             &IoEvent,
                                             &IoStatus);

            //
            // Ignore all I/O failures - there is nothing that can
            // be done at this point.
            //

            if (!NT_SUCCESS (Status)) {
                KeSetEvent (&IoEvent, 0, FALSE);
            }

            Status = KeWaitForSingleObject (&IoEvent,
                                            WrPageOut,
                                            KernelMode,
                                            FALSE,
                                            (PLARGE_INTEGER)&MmTwentySeconds);

            if (Status == STATUS_TIMEOUT) {

                //
                // The write did not complete in 20 seconds, assume
                // that the file systems are hung and return an error.
                //
                // Note the zero page (and any MDL system virtual address a
                // driver may have created) is leaked because we don't know
                // what the filesystem or storage stack might (still) be
                // doing to them.
                //

                Pfn1 = MI_PFN_ELEMENT (ZeroedPageFrame);

                LOCK_PFN (OldIrql);

                //
                // Increment the reference count on the zeroed page to ensure
                // it is never freed.
                //

                InterlockedIncrementPfn ((PSHORT)&Pfn1->u3.e2.ReferenceCount);

                RtlClearBits (PagingFile->Bitmap, (ULONG) first, count);

                break;
            }

            if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
            }

            write = FALSE;
            LOCK_PFN (OldIrql);
            RtlClearBits (PagingFile->Bitmap, (ULONG) first, count);
            count = 0;
        }
    }

    UNLOCK_PFN (OldIrql);

    KeSetEvent (AllDone, 0, FALSE);
    return;
}
LOGICAL
MiZeroAllPageFiles (
    VOID
    )

/*++

Routine Description:

    This routine zeroes all inactive pagefile blocks in all pagefiles.

Arguments:

    None.

Return Value:

    Returns TRUE on success, FALSE on failure.

Environment:

    Kernel mode, the caller must lock down PAGELK.

--*/

{
    PMMPFN Pfn1;
    PFN_NUMBER MaxPagesToWrite;
    KIRQL OldIrql;
    ULONG i;
    PFN_NUMBER j;
    PFN_NUMBER PageFrameIndex;
    PMM_ZERO_PAGEFILE_CONTEXT ZeroContext;
    ULONG NumberOfPagingFiles;
    KEVENT WaitEvents[MAX_PAGE_FILES];
    PKEVENT WaitObjects[MAX_PAGE_FILES];
    KWAIT_BLOCK WaitBlockArray[MAX_PAGE_FILES];

    MaxPagesToWrite = MmModifiedWriteClusterSize;

    //
    // Get a zeroed page to use as the source for the writes.
    //

    LOCK_PFN (OldIrql);

    MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_FOR_PAGEFILE_ZEROING);

    if (MmAvailablePages < MM_LOW_LIMIT) {
        UNLOCK_PFN (OldIrql);
        MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_FOR_PAGEFILE_ZEROING);
        return TRUE;
    }

    PageFrameIndex = MiRemoveZeroPage (0);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    ASSERT (Pfn1->u2.ShareCount == 0);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    Pfn1->u3.e2.ReferenceCount = (USHORT) MaxPagesToWrite;
    Pfn1->PteAddress = (PMMPTE) (ULONG_PTR)(X64K | 0x1);
    Pfn1->OriginalPte.u.Long = 0;
    MI_SET_PFN_DELETED (Pfn1);

    UNLOCK_PFN (OldIrql);

    //
    // Capture the number of paging files in case a new one gets added.
    //

    NumberOfPagingFiles = MmNumberOfPagingFiles;

    for (i = NumberOfPagingFiles; i != 0; i -= 1) {

        KeInitializeEvent (&WaitEvents[i - 1], NotificationEvent, FALSE);
        WaitObjects[i - 1] = &WaitEvents[i - 1];

        ZeroContext = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof (MM_ZERO_PAGEFILE_CONTEXT),
                                             'wZmM');

        if (ZeroContext == NULL) {
            KeSetEvent (WaitObjects[i - 1], 0, FALSE);
            continue;
        }

        ZeroContext->PagingFile = MmPagingFile[i - 1];
        ZeroContext->ZeroedPageFrame = PageFrameIndex;
        ZeroContext->AllDone = WaitObjects[i - 1];

        if (i != 1) {

            ExInitializeWorkItem (&ZeroContext->WorkItem,
                                  MiZeroPageFile,
                                  (PVOID) ZeroContext);

            ExQueueWorkItem (&ZeroContext->WorkItem, CriticalWorkQueue);
        }
        else {

            //
            // Zero the first pagefile ourself, then wait for
            // any others to finish.
            //

            KeSetEvent (WaitObjects[i - 1], 0, FALSE);
            MiZeroPageFile (ZeroContext);
        }
    }

    if (NumberOfPagingFiles > 1) {

        KeWaitForMultipleObjects (NumberOfPagingFiles,
                                  &WaitObjects[0],
                                  WaitAll,
                                  Executive,
                                  KernelMode,
                                  FALSE,
                                  NULL,
                                  &WaitBlockArray[0]);
    }

    LOCK_PFN (OldIrql);

    ASSERT (Pfn1->u3.e2.ReferenceCount >= MaxPagesToWrite);

    if (Pfn1->u3.e2.ReferenceCount == MaxPagesToWrite) {
        MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_FOR_PAGEFILE_ZEROING);
    }

    for (j = 0; j < MaxPagesToWrite; j += 1) {
        MiDecrementReferenceCountInline (Pfn1, PageFrameIndex);
    }

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

BOOLEAN
MiShutdownSystem (
    VOID
    )

/*++

Routine Description:

    This function performs the shutdown of memory management.  This
    is accomplished by writing out all modified pages which are
    destined for files other than the paging file.

    All processes have already been killed, the registry shutdown and
    shutdown IRPs already sent.  On return from this phase all mapped
    file data must be flushed and the unused segment list emptied.
    This releases all the Mm references to file objects, allowing many
    drivers (especially the network) to unload.

Arguments:

    None.

Return Value:

    TRUE if the pages were successfully written, FALSE otherwise.

--*/

{
    SIZE_T ImportListSize;
    PLOAD_IMPORTS ImportList;
    PLOAD_IMPORTS ImportListNonPaged;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PFN_NUMBER ModifiedPage;
    PMMPFN Pfn1;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PPFN_NUMBER Page;
    PFILE_OBJECT FilePointer;
    ULONG ConsecutiveFileLockFailures;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_WRITE_CLUSTER];
    PMDL Mdl;
    NTSTATUS Status;
    KEVENT IoEvent;
    IO_STATUS_BLOCK IoStatus;
    KIRQL OldIrql;
    LARGE_INTEGER StartingOffset;
    ULONG count;
    ULONG i;

    //
    // Don't do this more than once.
    //

    if (MmSystemShutdown == 0) {

        Mdl = (PMDL) MdlHack;
        Page = (PPFN_NUMBER)(Mdl + 1);

        KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

        MmInitializeMdl (Mdl, NULL, PAGE_SIZE);

        Mdl->MdlFlags |= MDL_PAGES_LOCKED;

        MmLockPageableSectionByHandle (ExPageLockHandle);

        LOCK_PFN (OldIrql);

        ModifiedPage = MmModifiedPageListHead.Flink;

        while (ModifiedPage != MM_EMPTY_LIST) {

            //
            // There are modified pages.
            //

            Pfn1 = MI_PFN_ELEMENT (ModifiedPage);

            if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {

                //
                // This page is destined for a file.
                //

                Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
                ControlArea = Subsection->ControlArea;
                if ((!ControlArea->u.Flags.Image) &&
                   (!ControlArea->u.Flags.NoModifiedWriting)) {

                    MiUnlinkPageFromList (Pfn1);

                    //
                    // Issue the write.
                    //

                    MI_SET_MODIFIED (Pfn1, 0, 0x28);

                    //
                    // Up the reference count for the physical page as there
                    // is I/O in progress.
                    //

                    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1);

                    *Page = ModifiedPage;
                    ControlArea->NumberOfMappedViews += 1;
                    ControlArea->NumberOfPfnReferences += 1;

                    UNLOCK_PFN (OldIrql);

                    StartingOffset.QuadPart = MiStartingOffset (Subsection,
                                                                Pfn1->PteAddress);
                    Mdl->StartVa = NULL;

                    ConsecutiveFileLockFailures = 0;
                    FilePointer = ControlArea->FilePointer;

retry:
                    KeClearEvent (&IoEvent);

                    Status = FsRtlAcquireFileForCcFlushEx (FilePointer);

                    if (NT_SUCCESS(Status)) {
                        Status = IoSynchronousPageWrite (FilePointer,
                                                         Mdl,
                                                         &StartingOffset,
                                                         &IoEvent,
                                                         &IoStatus);

                        //
                        // Release the file we acquired.
                        //

                        FsRtlReleaseFileForCcFlush (FilePointer);
                    }

                    if (!NT_SUCCESS(Status)) {

                        //
                        // Only try the request more than once if the
                        // filesystem said it had a deadlock.
                        //

                        if (Status == STATUS_FILE_LOCK_CONFLICT) {
                            ConsecutiveFileLockFailures += 1;
                            if (ConsecutiveFileLockFailures < 5) {
                                KeDelayExecutionThread (KernelMode,
                                                        FALSE,
                                                        (PLARGE_INTEGER)&MmShortTime);
                                goto retry;
                            }
                            goto wait_complete;
                        }

                        //
                        // Ignore all I/O failures - there is nothing that
                        // can be done at this point.
                        //

                        KeSetEvent (&IoEvent, 0, FALSE);
                    }

                    Status = KeWaitForSingleObject (&IoEvent,
                                                    WrPageOut,
                                                    KernelMode,
                                                    FALSE,
                                                    (PLARGE_INTEGER)&MmTwentySeconds);

wait_complete:

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (Status == STATUS_TIMEOUT) {

                        //
                        // The write did not complete in 20 seconds, assume
                        // that the file systems are hung and return an
                        // error.
                        //

                        LOCK_PFN (OldIrql);

                        MI_SET_MODIFIED (Pfn1, 1, 0xF);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);
                        ControlArea->NumberOfMappedViews -= 1;
                        ControlArea->NumberOfPfnReferences -= 1;

                        //
                        // This routine returns with the PFN lock released!
                        //

                        MiCheckControlArea (ControlArea, OldIrql);

                        MmUnlockPageableImageSection (ExPageLockHandle);

                        return FALSE;
                    }

                    LOCK_PFN (OldIrql);
                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1);
                    ControlArea->NumberOfMappedViews -= 1;
                    ControlArea->NumberOfPfnReferences -= 1;

                    //
                    // This routine returns with the PFN lock released!
                    //

                    MiCheckControlArea (ControlArea, OldIrql);
                    LOCK_PFN (OldIrql);

                    //
                    // Restart scan at the front of the list.
                    //

                    ModifiedPage = MmModifiedPageListHead.Flink;
                    continue;
                }
            }
            ModifiedPage = Pfn1->u1.Flink;
        }

        UNLOCK_PFN (OldIrql);

        //
        // Indicate to the modified page writer that the system has
        // shutdown.
        //

        MmSystemShutdown = 1;

        //
        // Check to see if the paging file should be overwritten.
        // Only free blocks are written.
        //

        if (MmZeroPageFile) {
            MiZeroAllPageFiles ();
        }

        MmUnlockPageableImageSection (ExPageLockHandle);
    }

    if (PoCleanShutdownEnabled ()) {

        //
        // Empty the unused segment list.
        //

        LOCK_PFN (OldIrql);
        MmUnusedSegmentForceFree = (ULONG)-1;
        KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);

        //
        // Give it 5 seconds to empty otherwise assume the filesystems are
        // hung and march on.
        //

        for (count = 0; count < 500; count += 1) {

            if (IsListEmpty(&MmUnusedSegmentList)) {
                break;
            }

            UNLOCK_PFN (OldIrql);

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmShortTime);
            LOCK_PFN (OldIrql);

#if DBG
            if (count == 400) {

                //
                // Everything should have been flushed by now.  Give the
                // filesystem team a chance to debug this on checked builds.
                //

                ASSERT (FALSE);
            }
#endif

            //
            // Resignal if needed in case more closed file objects triggered
            // additional entries.
            //

            if (MmUnusedSegmentForceFree == 0) {
                MmUnusedSegmentForceFree = (ULONG)-1;
                KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
            }
        }

        UNLOCK_PFN (OldIrql);

        //
        // Get rid of any paged pool references as they will be illegal
        // by the time MmShutdownSystem is called again since the filesystems
        // will have shutdown.
        //

        KeWaitForSingleObject (&MmSystemLoadLock,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        NextEntry = PsLoadedModuleList.Flink;
        while (NextEntry != &PsLoadedModuleList) {

            DataTableEntry = CONTAINING_RECORD (NextEntry,
                                                KLDR_DATA_TABLE_ENTRY,
                                                InLoadOrderLinks);

            ImportList = (PLOAD_IMPORTS)DataTableEntry->LoadedImports;

            if ((ImportList != (PVOID)LOADED_AT_BOOT) &&
                (ImportList != (PVOID)NO_IMPORTS_USED) &&
                (!SINGLE_ENTRY(ImportList))) {

                ImportListSize = ImportList->Count * sizeof(PVOID) + sizeof(SIZE_T);
                ImportListNonPaged = (PLOAD_IMPORTS) ExAllocatePoolWithTag (NonPagedPool,
                                                                    ImportListSize,
                                                                    'TDmM');

                if (ImportListNonPaged != NULL) {
                    RtlCopyMemory (ImportListNonPaged, ImportList, ImportListSize);
                    ExFreePool (ImportList);
                    DataTableEntry->LoadedImports = ImportListNonPaged;
                }
                else {

                    //
                    // Don't bother with the clean shutdown at this point.
                    //

                    PopShutdownCleanly = FALSE;
                    break;
                }
            }

            //
            // Free the full DLL name as it is pageable.
            //

            if (DataTableEntry->FullDllName.Buffer != NULL) {
                ExFreePool (DataTableEntry->FullDllName.Buffer);
                DataTableEntry->FullDllName.Buffer = NULL;
            }

            NextEntry = NextEntry->Flink;
        }

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

        //
        // Close all the pagefile handles, note we still have an object
        // reference to each keeping the underlying object resident.
        // At the end of Phase1 shutdown we'll release those references
        // to trigger the storage stack unload.  The handle close must be
        // done here however as it will reference pageable structures.
        //

        for (i = 0; i < MmNumberOfPagingFiles; i += 1) {

            //
            // Free each pagefile name now as it resides in paged pool and
            // may need to be inpaged to be freed.  Since the paging files
            // are going to be shutdown shortly, now is the time to access
            // pageable stuff and get rid of it.  Zeroing the buffer pointer
            // is sufficient as the only accesses to this are from the
            // try-except-wrapped GetSystemInformation APIs and all the
            // user processes are gone already.
            //
        
            ASSERT (MmPagingFile[i]->PageFileName.Buffer != NULL);
            ExFreePool (MmPagingFile[i]->PageFileName.Buffer);
            MmPagingFile[i]->PageFileName.Buffer = NULL;

            ZwClose (MmPagingFile[i]->FileHandle);
        }
    }

    return TRUE;
}

BOOLEAN
MmShutdownSystem (
    IN ULONG Phase
    )

/*++

Routine Description:

    This function performs the shutdown of memory management.  This
    is accomplished by writing out all modified pages which are
    destined for files other than the paging file.

Arguments:

    Phase - Supplies 0 on the initiation of shutdown.  All processes have
            already been killed, the registry shutdown and shutdown IRPs already
            sent.  On return from this phase all mapped file data must be
            flushed and the unused segment list emptied.  This releases all
            the Mm references to file objects, allowing many drivers (especially
            the network) to unload.

            Supplies 1 on the initiation of shutdown.  The filesystem stack
            has received its shutdown IRPs (the stack must free its paged pool
            allocations here and lock down any pageable code it intends to call)
            as no more references to pageable code or data are allowed on return.
            ie: Any IoPageRead at this point is illegal.
            Close the pagefile handles here so the filesystem stack will be
            dereferenced causing those drivers to unload as well.

            Supplies 2 on final shutdown of the system.  Any resources not
            freed by this point are treated as leaks and cause a bugcheck.

Return Value:

    TRUE if the pages were successfully written, FALSE otherwise.

--*/

{
    ULONG i;

    if (Phase == 0) {
        return MiShutdownSystem ();
    }

    if (Phase == 1) {

        //
        // The filesystem has shutdown.  References to pageable code or data
        // is no longer allowed at this point.
        //
        // Close the pagefile handles here so the filesystem stack will be
        // dereferenced causing those drivers to unload as well.
        //

        if (MmSystemShutdown < 2) {

            MmSystemShutdown = 2;

            if (PoCleanShutdownEnabled() & PO_CLEAN_SHUTDOWN_PAGING) {

                //
                // Make any IoPageRead at this point illegal.  Detect this by
                // purging all system pageable memory.
                //

                MmTrimAllSystemPageableMemory (TRUE);

                //
                // There should be no dirty pages destined for the filesystem.
                // Give the filesystem team a shot to debug this on checked
                // builds.
                //

                ASSERT (MmModifiedPageListHead.Total == MmTotalPagesForPagingFile);
                //
                // Dereference all the pagefile objects to trigger a cascading
                // unload of the storage stack as this should be the last
                // reference to their driver objects.
                //

                for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
                    ObDereferenceObject (MmPagingFile[i]->File);
                }
            }
        }
        return TRUE;
    }

    ASSERT (Phase == 2);

    //
    // Check for resource leaks and bugcheck if any are found.
    //

    if (MmSystemShutdown < 3) {
        MmSystemShutdown = 3;
        if (PoCleanShutdownEnabled ()) {
            MiReleaseAllMemory ();
        }
    }

    return TRUE;
}


VOID
MiReleaseAllMemory (
    VOID
    )

/*++

Routine Description:

    This function performs the final release of memory management allocations.

Arguments:

    None.

Return Value:

    None.

Environment:

    No references to paged pool or pageable code/data are allowed.

--*/

{
    ULONG i;
    ULONG j;
    PEVENT_COUNTER EventSupport;
    PUNLOADED_DRIVERS Entry;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLOAD_IMPORTS ImportList;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    PMMINPAGE_SUPPORT Support;
    PSLIST_ENTRY SingleListEntry;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    ASSERT (MmUnusedSegmentList.Flink == &MmUnusedSegmentList);

    //
    // Don't clear free pages so problems can be debugged.
    //

    MiZeroingDisabled = TRUE;

    //
    // Free the unloaded driver list.
    //

    if (MmUnloadedDrivers != NULL) {
        Entry = &MmUnloadedDrivers[0];
        for (i = 0; i < MI_UNLOADED_DRIVERS; i += 1) {
            if (Entry->Name.Buffer != NULL) {
                RtlFreeUnicodeString (&Entry->Name);
            }
            Entry += 1;
        }
        ExFreePool (MmUnloadedDrivers);
    }

    NextEntry = MmLoadedUserImageList.Flink;
    while (NextEntry != &MmLoadedUserImageList) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        NextEntry = NextEntry->Flink;

        ExFreePool ((PVOID)DataTableEntry);
    }

    //
    // Release the loaded module list entries.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        ImportList = (PLOAD_IMPORTS)DataTableEntry->LoadedImports;

        if ((ImportList != (PVOID)LOADED_AT_BOOT) &&
            (ImportList != (PVOID)NO_IMPORTS_USED) &&
            (!SINGLE_ENTRY(ImportList))) {

                ExFreePool (ImportList);
        }

        if (DataTableEntry->FullDllName.Buffer != NULL) {
            ASSERT (DataTableEntry->FullDllName.Buffer == DataTableEntry->BaseDllName.Buffer);
        }

        NextEntry = NextEntry->Flink;

        ExFreePool ((PVOID)DataTableEntry);
    }

    //
    // Free the physical memory descriptor block.
    //

    ExFreePool (MmPhysicalMemoryBlock);

    ExFreePool (MiPfnBitMap.Buffer);

    //
    // Free the system views structure.
    //

    if (MmSession.SystemSpaceViewTable != NULL) {
        ExFreePool (MmSession.SystemSpaceViewTable);
    }

    if (MmSession.SystemSpaceBitMap != NULL) {
        ExFreePool (MmSession.SystemSpaceBitMap);
    }

    //
    // Free the pagefile structures - note the PageFileName buffer was freed
    // earlier as it resided in paged pool and may have needed an inpage
    // to be freed.
    //

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        ASSERT (MmPagingFile[i]->PageFileName.Buffer == NULL);
        for (j = 0; j < MM_PAGING_FILE_MDLS; j += 1) {
            ExFreePool (MmPagingFile[i]->Entry[j]);
        }
        ExFreePool (MmPagingFile[i]->Bitmap);
        ExFreePool (MmPagingFile[i]);
    }

    ASSERT (MmNumberOfMappedMdlsInUse == 0);

    i = 0;
    while (IsListEmpty (&MmMappedFileHeader.ListHead) != 0) {

        ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                    &MmMappedFileHeader.ListHead);

        ExFreePool (ModWriterEntry);
        i += 1;
    }
    ASSERT (i == MmNumberOfMappedMdls);

    //
    // Free the paged pool bitmaps.
    //

    ExFreePool (MmPagedPoolInfo.PagedPoolAllocationMap);
    ExFreePool (MmPagedPoolInfo.EndOfPagedPoolBitmap);

    if (VerifierLargePagedPoolMap != NULL) {
        ExFreePool (VerifierLargePagedPoolMap);
    }

    //
    // Free the inpage structures.
    //

    while (ExQueryDepthSList (&MmInPageSupportSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmInPageSupportSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         MMINPAGE_SUPPORT,
                                         ListEntry);

            ASSERT (Support->u1.e1.PrefetchMdlHighBits == 0);
            ExFreePool (Support);
        }
    }

    while (ExQueryDepthSList (&MmEventCountSListHead) != 0) {

        EventSupport = (PEVENT_COUNTER) InterlockedPopEntrySList (&MmEventCountSListHead);

        if (EventSupport != NULL) {
            ExFreePool (EventSupport);
        }
    }

    //
    // Free the verifier list last because it must be consulted to debug
    // any bugchecks.
    //

    NextEntry = MiVerifierDriverAddedThunkListHead.Flink;
    if (NextEntry != NULL) {
        while (NextEntry != &MiVerifierDriverAddedThunkListHead) {

            ThunkTableBase = CONTAINING_RECORD (NextEntry,
                                                DRIVER_SPECIFIED_VERIFIER_THUNKS,
                                                ListEntry );

            NextEntry = NextEntry->Flink;
            ExFreePool (ThunkTableBase);
        }
    }

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        Verifier = CONTAINING_RECORD(NextEntry,
                                     MI_VERIFIER_DRIVER_ENTRY,
                                     Links);

        NextEntry = NextEntry->Flink;
        ExFreePool (Verifier);
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\session.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   session.c

Abstract:

    This module contains the routines which implement the creation and
    deletion of session spaces along with associated support routines.

--*/

#include "mi.h"

LONG MmSessionDataPages;

KGUARDED_MUTEX  MiSessionIdMutex;

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("PAGEDATA")
#endif

PRTL_BITMAP MiSessionIdBitmap = {0};

#define MI_SESSION_ID_INCREMENT (POOL_SMALLEST_BLOCK * 8)

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

ULONG MiSessionDataPages;
ULONG MiSessionTagPages;
ULONG MiSessionTagSizePages;
ULONG MiSessionBigPoolPages;

ULONG MiSessionCreateCharge;

//
// Note that actually the SUM of the two maximums below is currently
// MM_ALLOCATION_GRANULARITY / PAGE_SIZE.
//

#define MI_SESSION_DATA_PAGES_MAXIMUM (MM_ALLOCATION_GRANULARITY / PAGE_SIZE)
#define MI_SESSION_TAG_PAGES_MAXIMUM  (MM_ALLOCATION_GRANULARITY / PAGE_SIZE)

VOID
MiSessionAddProcess (
    PEPROCESS NewProcess
    );

VOID
MiSessionRemoveProcess (
    VOID
    );

VOID
MiInitializeSessionIds (
    VOID
    );

NTSTATUS
MiSessionCreateInternal (
    OUT PULONG SessionId
    );

NTSTATUS
MiSessionCommitPageTables (
    IN PVOID StartVa,
    IN PVOID EndVa
    );

VOID
MiDereferenceSession (
    VOID
    );

VOID
MiSessionDeletePde (
    IN PMMPTE Pde,
    IN PMMPTE SelfMapPde
    );

VOID
MiDereferenceSessionFinal (
    VOID
    );

#if DBG
VOID
MiCheckSessionVirtualSpace (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    );
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSessionIds)

#pragma alloc_text(PAGE, MmSessionSetUnloadAddress)
#pragma alloc_text(PAGE, MmSessionCreate)
#pragma alloc_text(PAGE, MmSessionDelete)
#pragma alloc_text(PAGE, MmGetSessionLocaleId)
#pragma alloc_text(PAGE, MmSetSessionLocaleId)
#pragma alloc_text(PAGE, MmQuitNextSession)
#pragma alloc_text(PAGE, MiDereferenceSession)
#pragma alloc_text(PAGE, MiSessionPoolLookaside)
#pragma alloc_text(PAGE, MiSessionPoolSmallLists)
#pragma alloc_text(PAGE, MiSessionPoolTrackTable)
#pragma alloc_text(PAGE, MiSessionPoolTrackTableSize)
#pragma alloc_text(PAGE, MiSessionPoolBigPageTable)
#pragma alloc_text(PAGE, MiSessionPoolBigPageTableSize)
#if DBG
#pragma alloc_text(PAGE, MiCheckSessionVirtualSpace)
#endif

#pragma alloc_text(PAGELK, MiSessionCreateInternal)
#pragma alloc_text(PAGELK, MiDereferenceSessionFinal)

#endif

VOID
MiSessionLeader (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    Mark the argument process as having the ability to create or delete session
    spaces.  This is only granted to the session manager process.

Arguments:

    Process - Supplies a pointer to the privileged process.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;

    LOCK_EXPANSION (OldIrql);

    Process->Vm.Flags.SessionLeader = 1;

    UNLOCK_EXPANSION (OldIrql);
}


VOID
MmSessionSetUnloadAddress (
    IN PDRIVER_OBJECT DriverObject
    )

/*++

Routine Description:

    Copy the win32k.sys driver object to the session structure for use during
    unload.

Arguments:

    DriverObject - Supplies a pointer to the win32k driver object.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ASSERT (PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION);
    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

    MmSessionSpace->Win32KDriverUnload = DriverObject->DriverUnload;
}

VOID
MiSessionAddProcess (
    PEPROCESS NewProcess
    )

/*++

Routine Description:

    Add the new process to the current session space.

Arguments:

    NewProcess - Supplies a pointer to the process being created.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    KIRQL OldIrql;
    PMM_SESSION_SPACE SessionGlobal;

    //
    // If the calling process has no session, then the new process won't get
    // one either.
    //

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    InterlockedIncrement ((PLONG)&SessionGlobal->ReferenceCount);

    InterlockedIncrement (&SessionGlobal->ResidentProcessCount);

    InterlockedIncrement (&SessionGlobal->ProcessReferenceToSession);

    //
    // Once the Session pointer in the EPROCESS is set it can never
    // be cleared because it is accessed lock-free.
    //

    ASSERT (NewProcess->Session == NULL);
    NewProcess->Session = (PVOID) SessionGlobal;

    //
    // Link the process entry into the session space and WSL structures.
    //

    LOCK_EXPANSION (OldIrql);

    InsertTailList (&SessionGlobal->ProcessList, &NewProcess->SessionProcessLinks);
    UNLOCK_EXPANSION (OldIrql);

    PS_SET_BITS (&NewProcess->Flags, PS_PROCESS_FLAGS_IN_SESSION);
}


VOID
MiSessionRemoveProcess (
    VOID
    )

/*++

Routine Description:

    This routine removes the current process from the current session space.
    This may trigger a substantial round of dereferencing and resource freeing
    if it is also the last process in the session, (holding the last image
    in the group, etc).

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL and below, but queueing of APCs to this thread has
    been permanently disabled.  This is the last thread in the process
    being deleted.  The caller has ensured that this process is not
    on the expansion list and therefore there can be no races in regards to
    trimming.

--*/

{
    KIRQL OldIrql;
    PEPROCESS CurrentProcess;
#if DBG
    ULONG Found;
    PEPROCESS Process;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE SessionGlobal;
#endif

    CurrentProcess = PsGetCurrentProcess();

    if (((CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) ||
        (CurrentProcess->Vm.Flags.SessionLeader == 1)) {
        return;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    //
    // Remove this process from the list of processes in the current session.
    //

    LOCK_EXPANSION (OldIrql);

#if DBG

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    Found = 0;
    NextEntry = SessionGlobal->ProcessList.Flink;

    while (NextEntry != &SessionGlobal->ProcessList) {
        Process = CONTAINING_RECORD (NextEntry, EPROCESS, SessionProcessLinks);

        if (Process == CurrentProcess) {
            Found = 1;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (Found == 1);

#endif

    RemoveEntryList (&CurrentProcess->SessionProcessLinks);

    UNLOCK_EXPANSION (OldIrql);

    //
    // Decrement this process' reference count to the session.  If this
    // is the last reference, then the entire session will be destroyed
    // upon return.  This includes unloading drivers, unmapping pools,
    // freeing page tables, etc.
    //

    MiDereferenceSession ();
}

LCID
MmGetSessionLocaleId (
    VOID
    )

/*++

Routine Description:

    This routine gets the locale ID for the current session.

Arguments:

    None.

Return Value:

    The locale ID for the current session.

Environment:

    PASSIVE_LEVEL, the caller must supply any desired synchronization.

--*/

{
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    PAGED_CODE ();

    Process = PsGetCurrentProcess ();

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        return PsDefaultThreadLocaleId;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        return PsDefaultThreadLocaleId;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    return SessionGlobal->LocaleId;
}

VOID
MmSetSessionLocaleId (
    IN LCID LocaleId
    )

/*++

Routine Description:

    This routine sets the locale ID for the current session.

Arguments:

    LocaleId - Supplies the desired locale ID.

Return Value:

    None.

Environment:

    PASSIVE_LEVEL, the caller must supply any desired synchronization.

--*/

{
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    PAGED_CODE ();

    Process = PsGetCurrentProcess ();

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        PsDefaultThreadLocaleId = LocaleId;
        return;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        PsDefaultThreadLocaleId = LocaleId;
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    SessionGlobal->LocaleId = LocaleId;
}

VOID
MiInitializeSessionIds (
    VOID
    )

/*++

Routine Description:

    This routine creates and initializes session ID allocation/deallocation.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG TotalPages;

    //
    // If this ever grows beyond the maximum size, the session virtual
    // address space layout will need to be enlarged.
    //

    TotalPages = MI_SESSION_DATA_PAGES_MAXIMUM;

    MiSessionDataPages = ROUND_TO_PAGES (sizeof (MM_SESSION_SPACE));
    MiSessionDataPages >>= PAGE_SHIFT;

    ASSERT (MiSessionDataPages <= MI_SESSION_DATA_PAGES_MAXIMUM - 3);

    TotalPages -= MiSessionDataPages;

    MiSessionTagSizePages = 2;
    MiSessionBigPoolPages = 1;

    MiSessionTagPages = MiSessionTagSizePages + MiSessionBigPoolPages;

    ASSERT (MiSessionTagPages <= TotalPages);
    ASSERT (MiSessionTagPages < MI_SESSION_TAG_PAGES_MAXIMUM);

#if defined(_AMD64_)
    MiSessionCreateCharge = 3 + MiSessionDataPages + MiSessionTagPages;
#elif defined(_X86_)
    MiSessionCreateCharge = 1 + MiSessionDataPages + MiSessionTagPages;
#else
#error "no target architecture"
#endif

    KeInitializeGuardedMutex (&MiSessionIdMutex);

    MiCreateBitMap (&MiSessionIdBitmap, MI_SESSION_ID_INCREMENT, PagedPool);

    if (MiSessionIdBitmap != NULL) {
        RtlClearAllBits (MiSessionIdBitmap);
    }
    else {
        KeBugCheckEx (INSTALL_MORE_MEMORY,
                      MmNumberOfPhysicalPages,
                      MmLowestPhysicalPage,
                      MmHighestPhysicalPage,
                      0x200);
    }
}


ULONG
MiSessionPoolSmallLists (
    VOID
    )
{
    return SESSION_POOL_SMALL_LISTS;
}

PGENERAL_LOOKASIDE
MiSessionPoolLookaside (
    VOID
    )
{
    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    return (PVOID) &MmSessionSpace->Lookaside;
}

PVOID
MiSessionPoolTrackTable (
    VOID
    )
{
    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    return (PPOOL_TRACKER_TABLE) ((ULONG_PTR)MmSessionSpace + (MiSessionDataPages << PAGE_SHIFT));
}

SIZE_T
MiSessionPoolTrackTableSize (
    VOID
    )
{
    SIZE_T i;
    SIZE_T NumberOfBytes;
    SIZE_T NumberOfEntries;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    NumberOfBytes = (MiSessionTagSizePages << PAGE_SHIFT);
    NumberOfEntries = NumberOfBytes / sizeof (POOL_TRACKER_TABLE);

    //
    // Convert to the closest power of 2 <= NumberOfEntries.
    //

    for (i = 0; i < 32; i += 1) {
        if (((SIZE_T)1 << i) > NumberOfEntries) {
            return (((SIZE_T)1) << (i - 1));
        }
    }

    ASSERT (FALSE);

    return 0;
}

PVOID
MiSessionPoolBigPageTable (
    VOID
    )
{
    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    return (PPOOL_TRACKER_BIG_PAGES) ((ULONG_PTR)MmSessionSpace + ((MiSessionDataPages + MiSessionTagSizePages) << PAGE_SHIFT));
}

SIZE_T
MiSessionPoolBigPageTableSize (
    VOID
    )
{
    SIZE_T i;
    SIZE_T NumberOfBytes;
    SIZE_T NumberOfEntries;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    NumberOfBytes = (MiSessionBigPoolPages << PAGE_SHIFT);
    NumberOfEntries = NumberOfBytes / sizeof (POOL_TRACKER_BIG_PAGES);

    //
    // Convert to the closest power of 2 <= NumberOfEntries.
    //

    for (i = 0; i < 32; i += 1) {
        if (((SIZE_T)1 << i) > NumberOfEntries) {
            return (((SIZE_T)1) << (i - 1));
        }
    }

    ASSERT (FALSE);

    return 0;
}

NTSTATUS
MiSessionCreateInternal (
    OUT PULONG SessionId
    )

/*++

Routine Description:

    This routine creates the data structure that describes and maintains
    the session space.  It resides at the beginning of the session space.
    Carefully construct the first page mapping to bootstrap the fault
    handler which relies on the session space data structure being
    present and valid.

    In NT32, this initial mapping for the portion of session space
    mapped by the first PDE will automatically be inherited by all child
    processes when the system copies the system portion of the page
    directory for new address spaces.  Additional entries are faulted
    in by the session space fault handler, which references this structure.

    For NT64, everything is automatically inherited.

    This routine commits virtual memory within the current session space with
    backing pages.  The virtual addresses within session space are
    allocated with a separate facility in the image management facility.
    This is because images must be at a unique system wide virtual address.

Arguments:

    SessionId - Supplies a pointer to place the new session ID into.

Return Value:

    STATUS_SUCCESS if all went well, various failure status codes
    if the session was not created.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    KIRQL  OldIrql;
    PRTL_BITMAP NewBitmap;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE GlobalMappingPte;
    NTSTATUS Status;
    PMM_SESSION_SPACE SessionSpace;
    PMM_SESSION_SPACE SessionGlobal;
    PFN_NUMBER ResidentPages;
    LOGICAL GotCommit;
    LOGICAL GotPages;
    LOGICAL PoolInitialized;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    MMPTE TempPte;
    PFN_NUMBER DataPage[MI_SESSION_DATA_PAGES_MAXIMUM] = {0};
    PFN_NUMBER PageTablePage;
    PFN_NUMBER TagPage[MI_SESSION_TAG_PAGES_MAXIMUM];
    ULONG i;
    ULONG PageColor;
    ULONG ProcessFlags;
    ULONG NewProcessFlags;
    PEPROCESS Process;
#if (_MI_PAGING_LEVELS < 3)
    SIZE_T PageTableBytes;
    PMMPTE PageTables;
#else
    PMMPTE PointerPpe;
    PFN_NUMBER PageDirectoryPage;
    PFN_NUMBER PageDirectoryParentPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    GotCommit = FALSE;
    GotPages = FALSE;
    GlobalMappingPte = NULL;
    PoolInitialized = FALSE;

    //
    // Initializing these are not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PageTablePage = 0;
    PointerPte = NULL;

#if (_MI_PAGING_LEVELS >= 3)
    PageDirectoryPage = 0;
    PageDirectoryParentPage = 0;
#endif

    Process = PsGetCurrentProcess ();

    ASSERT (MmIsAddressValid(MmSessionSpace) == FALSE);

    //
    // Check for concurrent session creation attempts.
    //

    ProcessFlags = Process->Flags;

    while (TRUE) {

        if (ProcessFlags & PS_PROCESS_FLAGS_CREATING_SESSION) {
            return STATUS_ALREADY_COMMITTED;
        }

        NewProcessFlags = (ProcessFlags | PS_PROCESS_FLAGS_CREATING_SESSION);

        NewProcessFlags = InterlockedCompareExchange ((PLONG)&Process->Flags,
                                                      (LONG)NewProcessFlags,
                                                      (LONG)ProcessFlags);
                                                             
        if (NewProcessFlags == ProcessFlags) {
            break;
        }

        //
        // The structure changed beneath us.  Use the return value from the
        // exchange and try it all again.
        //

        ProcessFlags = NewProcessFlags;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);

#if (_MI_PAGING_LEVELS < 3)

    PageTableBytes = MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES * sizeof (MMPTE);

    PageTables = (PMMPTE) ExAllocatePoolWithTag (NonPagedPool,
                                                 PageTableBytes,
                                                 'tHmM');

    if (PageTables == NULL) {

        ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (PageTables, PageTableBytes);

#endif

    //
    // Select a free session ID.
    //



    KeAcquireGuardedMutex (&MiSessionIdMutex);

    *SessionId = RtlFindClearBitsAndSet (MiSessionIdBitmap, 1, 0);

    if (*SessionId == NO_BITS_FOUND) {

        MiCreateBitMap (&NewBitmap,
                        MiSessionIdBitmap->SizeOfBitMap + MI_SESSION_ID_INCREMENT,
                        PagedPool);

        if (NewBitmap == NULL) {

            KeReleaseGuardedMutex (&MiSessionIdMutex);

            ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
            PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

#if (_MI_PAGING_LEVELS < 3)
            ExFreePool (PageTables);
#endif

            MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_IDS);
            return STATUS_NO_MEMORY;
        }

        RtlClearAllBits (NewBitmap);

        //
        // Copy the bits from the existing map.
        //

        RtlCopyMemory (NewBitmap->Buffer,
                       MiSessionIdBitmap->Buffer,
                       ((MiSessionIdBitmap->SizeOfBitMap + 31) / 32) * sizeof (ULONG));

        MiRemoveBitMap (&MiSessionIdBitmap);

        MiSessionIdBitmap = NewBitmap;

        *SessionId = RtlFindClearBitsAndSet (MiSessionIdBitmap, 1, 0);

        ASSERT (*SessionId != NO_BITS_FOUND);
    }

    KeReleaseGuardedMutex (&MiSessionIdMutex);

    //
    // Lock down this routine in preparation for the PFN lock acquisition.
    // Note this is done prior to the commitment charges just to simplify
    // error handling.
    //

    MmLockPageableSectionByHandle (ExPageLockHandle);

    //
    // Charge commitment.
    //


    ResidentPages = MiSessionCreateCharge;

    if (MiChargeCommitment (ResidentPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        goto Failure;
    }

    GotCommit = TRUE;

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_CREATE, ResidentPages);

    //
    // Reserve global system PTEs to map the data pages with.
    //

    GlobalMappingPte = MiReserveSystemPtes (MiSessionDataPages, SystemPteSpace);

    if (GlobalMappingPte == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_SYSPTES);
        goto Failure;
    }

    //
    // Ensure the resident physical pages are available.
    //

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)(ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM) > MI_NONPAGEABLE_MEMORY_AVAILABLE()) {

        UNLOCK_PFN (OldIrql);

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        goto Failure;
    }

    GotPages = TRUE;

    MI_DECREMENT_RESIDENT_AVAILABLE (
        ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM,
        MM_RESAVAIL_ALLOCATE_CREATE_SESSION);

    //
    // Allocate both session space data pages first as on some architectures
    // a region ID will be used immediately for the TB references as the
    // PTE mappings are initialized.
    //

    TempPte.u.Long = ValidKernelPte.u.Long;

    for (i = 0; i < MiSessionDataPages; i += 1) {

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }

        PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

        DataPage[i] = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

        TempPte.u.Hard.PageFrameNumber = DataPage[i];

        //
        // Map the data pages immediately in global space.  Some architectures
        // use a region ID which is used immediately for the TB references after
        // the PTE mappings are initialized.
        //
        //
        // The global bit can be left on for the global mappings (unlike the
        // session space mapping which must have the global bit off since
        // we need to make sure the TB entry is flushed when we switch to
        // a process in a different session space).
        //

        MI_WRITE_VALID_PTE (GlobalMappingPte + i, TempPte);
    }

    SessionGlobal = (PMM_SESSION_SPACE) MiGetVirtualAddressMappedByPte (GlobalMappingPte);


#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initialize the page directory parent page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageDirectoryParentPage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryParentPage;

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryParentPage;

    PointerPxe = MiGetPxeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPxe->u.Long == 0);

    MI_WRITE_VALID_PTE (PointerPxe, TempPte);

    //
    // Do not reference the top level parent page as it belongs to the
    // current process (SMSS).
    //

    MiInitializePfnForOtherProcess (PageDirectoryParentPage, PointerPxe, 0);

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryParentPage);
    Pfn1->u4.PteFrame = 0;

    ASSERT (MI_PFN_ELEMENT(PageDirectoryParentPage)->u1.WsIndex == 0);

    //
    // Initialize the page directory page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageDirectoryPage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryPage;

    PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPpe->u.Long == 0);

    MI_WRITE_VALID_PTE (PointerPpe, TempPte);

#if defined (_WIN64)

    //
    // IA64 can reference the top level parent page here because a unique
    // one is allocated per process.  AMD64 is also ok because there is
    // another hierarchy level above.
    //

    MiInitializePfnForOtherProcess (PageDirectoryPage, PointerPpe, PageDirectoryParentPage);

#else

    //
    // Do not reference the top level parent page as it belongs to the
    // current process (SMSS).
    //

    MiInitializePfnForOtherProcess (PageDirectoryPage, PointerPpe, 0);
    Pfn1 = MI_PFN_ELEMENT (PageDirectoryPage);
    Pfn1->u4.PteFrame = 0;

#endif

    ASSERT (MI_PFN_ELEMENT(PageDirectoryPage)->u1.WsIndex == 0);

#endif

    //
    // Initialize the page table page.
    //

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageTablePage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageTablePage;

    PointerPde = MiGetPdeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPde->u.Long == 0);

    MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
    MiInitializePfnForOtherProcess (PageTablePage, PointerPde, PageDirectoryPage);
#else
    //
    // This page frame references itself instead of the current (SMSS.EXE)
    // page directory as its PteFrame.  This allows the current process to
    // appear more normal (at least on 32-bit NT).  It just means we have
    // to treat this page specially during teardown.
    //

    MiInitializePfnForOtherProcess (PageTablePage, PointerPde, PageTablePage);
#endif

    //
    // This page is never paged, ensure that its WsIndex stays clear so the
    // release of the page is handled correctly.
    //

    ASSERT (MI_PFN_ELEMENT(PageTablePage)->u1.WsIndex == 0);





    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPteLocal.u.Long;

    PointerPte = MiGetPteAddress (MmSessionSpace);

    for (i = 0; i < MiSessionDataPages; i += 1) {

        TempPte.u.Hard.PageFrameNumber = DataPage[i];

        MiInitializePfnAndMakePteValid (DataPage[i], PointerPte + i, TempPte);

        ASSERT (MI_PFN_ELEMENT(DataPage[i])->u1.WsIndex == 0);
    }

    //
    // Allocate pages for session pool tag information and
    // large session pool allocation tracking.
    //

    for (i = 0; i < MiSessionTagPages; i += 1) {

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }

        PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

        TagPage[i] = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

        TempPte.u.Hard.PageFrameNumber = TagPage[i];

        MiInitializePfnAndMakePteValid (TagPage[i], PointerPte + MiSessionDataPages + i, TempPte);
    }

    //
    // Now that the data page is mapped, it can be freed as full hierarchy
    // teardown in the event of any future failures encountered by this routine.
    //

    UNLOCK_PFN (OldIrql);

    //
    // Initialize the new session space data structure.
    //

    SessionSpace = MmSessionSpace;

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGETABLE_ALLOC, 1);
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_ALLOC, 1);

    SessionSpace->GlobalVirtualAddress = SessionGlobal;
    SessionSpace->ReferenceCount = 1;
    SessionSpace->ResidentProcessCount = 1;
    SessionSpace->u.LongFlags = 0;
    SessionSpace->SessionId = *SessionId;
    SessionSpace->LocaleId = PsDefaultSystemLocaleId;
    SessionSpace->SessionPageDirectoryIndex = PageTablePage;

    SessionSpace->Color = PageColor;

    //
    // Track the page table page and the data page.
    //

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_CREATE, (ULONG)ResidentPages);
    SessionSpace->NonPageablePages = ResidentPages;
    SessionSpace->CommittedPages = ResidentPages;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initialize the session data page directory entry so trimmers can attach.
    //

#if defined(_AMD64_)
    PointerPpe = MiGetPxeAddress ((PVOID)MmSessionSpace);
#else
    PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);
#endif
    SessionSpace->PageDirectory = *PointerPpe;

#else

    SessionSpace->PageTables = PageTables;

    //
    // Load the session data page table entry so that other processes
    // can fault in the mapping.
    //

    SessionSpace->PageTables[PointerPde - MiGetPdeAddress (MmSessionBase)] = *PointerPde;

#endif

    //
    // This list entry is only referenced while within the
    // session space and has session space (not global) addresses.
    //

    InitializeListHead (&SessionSpace->ImageList);

    //
    // Initialize the session space pool.
    //

    Status = MiInitializeSessionPool ();

    if (!NT_SUCCESS(Status)) {
        goto Failure;
    }

    PoolInitialized = TRUE;

    //
    // Initialize the view mapping support - note this must happen after
    // initializing session pool.
    //

    if (MiInitializeSystemSpaceMap (&SessionGlobal->Session) == FALSE) {
        goto Failure;
    }

    MmUnlockPageableImageSection (ExPageLockHandle);

    //
    // Use the global virtual address rather than the session space virtual
    // address to set up fields that need to be globally accessible.
    //

    ASSERT (SessionGlobal->WsListEntry.Flink == NULL);
    ASSERT (SessionGlobal->WsListEntry.Blink == NULL);

    InitializeListHead (&SessionGlobal->ProcessList);

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

    ASSERT (Process->Session == NULL);

    ASSERT (SessionGlobal->ProcessReferenceToSession == 0);
    SessionGlobal->ProcessReferenceToSession = 1;

    InterlockedIncrement (&MmSessionDataPages);

    return STATUS_SUCCESS;

Failure:

#if (_MI_PAGING_LEVELS < 3)
    ExFreePool (PageTables);
#endif

    if (GotCommit == TRUE) {
        MiReturnCommitment (ResidentPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_CREATE_FAILURE,
                         ResidentPages);
    }

    if (GotPages == TRUE) {

#if (_MI_PAGING_LEVELS >= 4)
        PointerPxe = MiGetPxeAddress ((PVOID)MmSessionSpace);
        ASSERT (PointerPxe->u.Hard.Valid != 0);
#endif

#if (_MI_PAGING_LEVELS >= 3)
        PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);
        ASSERT (PointerPpe->u.Hard.Valid != 0);
#endif

        PointerPde = MiGetPdeAddress (MmSessionSpace);
        ASSERT (PointerPde->u.Hard.Valid != 0);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE_FAIL1, 1);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_FAIL1, 1);


        //
        // Do not call MiFreeSessionSpaceMap () as the maps cannot have been
        // initialized if we are in this path.
        //

        //
        // Free the initial page table page that was allocated for the
        // paged pool range (if it has been allocated at this point).
        //

        MiFreeSessionPoolBitMaps ();

        //
        // Capture all needed session information now as after sharing
        // is disabled below, no references to session space can be made.
        //

        ASSERT (MiSessionDataPages + MiSessionTagPages != 0);
        MiZeroMemoryPte (PointerPte, MiSessionDataPages + MiSessionTagPages);

        MI_WRITE_ZERO_PTE (PointerPde);
#if defined(_AMD64_)
        MI_WRITE_ZERO_PTE (PointerPpe);
        MI_WRITE_ZERO_PTE (PointerPxe);
#endif

        MI_FLUSH_SESSION_TB ();

        LOCK_PFN (OldIrql);

        //
        // Free the session tag structure pages.
        //

        for (i = 0; i < MiSessionTagPages; i += 1) {
            Pfn1 = MI_PFN_ELEMENT (TagPage[i]);
            Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

            MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCount (Pfn1, TagPage[i]);
        }

        //
        // Free the session data structure pages.
        //

        for (i = 0; i < MiSessionDataPages; i += 1) {
            Pfn1 = MI_PFN_ELEMENT (DataPage[i]);
            Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

            MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCount (Pfn1, DataPage[i]);
        }

        //
        // Free the page table page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageTablePage);

#if (_MI_PAGING_LEVELS >= 3)

        if (PoolInitialized == TRUE) {
            ASSERT (Pfn1->u2.ShareCount == 2);
            Pfn1->u2.ShareCount -= 1;
        }
        ASSERT (Pfn1->u2.ShareCount == 1);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

#else

        ASSERT (PageTablePage == Pfn1->u4.PteFrame);

        if (PoolInitialized == TRUE) {
            ASSERT (Pfn1->u2.ShareCount == 3);
            Pfn1->u2.ShareCount -= 2;
        }
        else {
            ASSERT (Pfn1->u2.ShareCount == 2);
            Pfn1->u2.ShareCount -= 1;
        }

#endif

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageTablePage);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Free the page directory page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageDirectoryPage);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        ASSERT (Pfn1->u4.PteFrame == PageDirectoryParentPage);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageDirectoryPage);

        //
        // Free the page directory parent page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageDirectoryParentPage);

        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageDirectoryParentPage);

#endif


        UNLOCK_PFN (OldIrql);

        MI_INCREMENT_RESIDENT_AVAILABLE (
            ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM,
            MM_RESAVAIL_FREE_CREATE_SESSION);
    }

    if (GlobalMappingPte != NULL) {
        MiReleaseSystemPtes (GlobalMappingPte,
                             MiSessionDataPages,
                             SystemPteSpace);
    }

    MmUnlockPageableImageSection (ExPageLockHandle);

    KeAcquireGuardedMutex (&MiSessionIdMutex);

    ASSERT (RtlCheckBit (MiSessionIdBitmap, *SessionId));
    RtlClearBit (MiSessionIdBitmap, *SessionId);

    KeReleaseGuardedMutex (&MiSessionIdMutex);

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

    return STATUS_NO_MEMORY;
}

LONG MiSessionLeaderExists;


NTSTATUS
MmSessionCreate (
    OUT PULONG SessionId
    )

/*++

Routine Description:

    Called from NtSetSystemInformation() to create a session space
    in the calling process with the specified SessionId.  An error is returned
    if the calling process already has a session space.

Arguments:

    SessionId - Supplies a pointer to place the resulting session id in.

Return Value:

    Various NTSTATUS error codes.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    ULONG i;
    ULONG SessionLeaderExists;
    PMM_SESSION_SPACE SessionGlobal;
    PETHREAD CurrentThread;
    NTSTATUS Status;
    PEPROCESS CurrentProcess;
#if DBG && (_MI_PAGING_LEVELS < 3)
    PMMPTE StartPde;
    PMMPTE EndPde;
#endif

    CurrentThread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    //
    // A simple check to see if the calling process already has a session space.
    // No need to go through all this if it does.  Creation races are caught
    // below and recovered from regardless.
    //

    if (CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        return STATUS_ALREADY_COMMITTED;
    }

    if (CurrentProcess->Vm.Flags.SessionLeader == 0) {

        //
        // Only the session manager can create a session.  Make the current
        // process the session leader if this is the first session creation
        // ever.
        //
        // Make sure the add is only done once as this is called multiple times.
        //
    
        SessionLeaderExists = InterlockedCompareExchange (&MiSessionLeaderExists, 1, 0);
    
        if (SessionLeaderExists != 0) {
            return STATUS_INVALID_SYSTEM_SERVICE;
        }

        MiSessionLeader (CurrentProcess);
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == FALSE);

#if defined (_AMD64_)
    ASSERT ((MiGetPxeAddress(MmSessionBase))->u.Long == 0);
#endif

#if (_MI_PAGING_LEVELS < 3)

#if DBG
    StartPde = MiGetPdeAddress (MmSessionBase);
    EndPde = MiGetPdeAddress (MiSessionSpaceEnd);

    while (StartPde < EndPde) {
        ASSERT (StartPde->u.Long == 0);
        StartPde += 1;
    }
#endif

#endif

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    Status = MiSessionCreateInternal (SessionId);

    if (!NT_SUCCESS(Status)) {
        KeLeaveCriticalRegionThread (&CurrentThread->Tcb);
        return Status;
    }

    //
    // Add the session space to the working set list.
    //
    // NO SESSION POOL CAN BE ALLOCATED UNTIL THIS COMPLETES.
    //

    Status = MiSessionInitializeWorkingSetList ();

    if (!NT_SUCCESS(Status)) {
        MiDereferenceSession ();
        KeLeaveCriticalRegionThread (&CurrentThread->Tcb);
        return Status;
    }

    //
    // Initialize the session paged pool lookaside lists.  Note this cannot
    // be done until both the session pool and working set lists are
    // fully initialized because on IA64, the lookaside list initialization
    // does a pool allocation which will trigger a demand zero fault in 
    // the session pool.
    //
    // THIS IS THE FIRST POOL ALLOCATION IN THIS SESSION SPACE.
    //

    SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

    for (i = 0; i < SESSION_POOL_SMALL_LISTS; i += 1) {

        ExInitializePagedLookasideList ((PPAGED_LOOKASIDE_LIST)&SessionGlobal->Lookaside[i],
                                        NULL,
                                        NULL,
                                        PagedPoolSession,
                                        (i + 1) * sizeof (POOL_BLOCK),
                                        'looP',
                                        256);
    }

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

#if defined (_WIN64)
    MiInitializeSpecialPool (PagedPoolSession);
#endif

    MmSessionSpace->u.Flags.Initialized = 1;

    PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_IN_SESSION);

    ASSERT (MiSessionLeaderExists == 1);

    return Status;
}


NTSTATUS
MmSessionDelete (
    ULONG SessionId
    )

/*++

Routine Description:

    Called from NtSetSystemInformation() to detach from an existing
    session space in the calling process.  An error is returned
    if the calling process has no session space.

Arguments:

    SessionId - Supplies the session id to delete.

Return Value:

    STATUS_SUCCESS on success, STATUS_UNABLE_TO_FREE_VM on failure.

    This process will not be able to access session space anymore upon
    a successful return.  If this is the last process in the session then
    the entire session is torn down.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    CurrentThread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    //
    // See if the calling process has a session space - this must be
    // checked since we can be called via a system service.
    //

    if ((CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
            "MmSessionDelete: Process %p not in a session\n",
            CurrentProcess);
        DbgBreakPoint ();
#endif
        return STATUS_UNABLE_TO_FREE_VM;
    }

    if (CurrentProcess->Vm.Flags.SessionLeader == 0) {

        //
        // Only the session manager can delete a session.  This is because
        // it affects the virtual mappings for all threads within the process
        // when this address space is deleted.  This is different from normal
        // VAD clearing because win32k and other drivers rely on this space.
        //

        return STATUS_UNABLE_TO_FREE_VM;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    if (MmSessionSpace->SessionId != SessionId) {
#if DBG
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
            "MmSessionDelete: Wrong SessionId! Own %d, Ask %d\n",
            MmSessionSpace->SessionId,
            SessionId);
        DbgBreakPoint();
#endif
        return STATUS_UNABLE_TO_FREE_VM;
    }

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    MiDereferenceSession ();

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    return STATUS_SUCCESS;
}


VOID
MiAttachSession (
    IN PMM_SESSION_SPACE SessionGlobal
    )

/*++

Routine Description:

    Attaches to the specified session space.

Arguments:

    SessionGlobal - Supplies a pointer to the session to attach to.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  Current process must not have a session
    space - ie: the caller should be the system process or smss.exe.

--*/

{
    PMMPTE PointerPde;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0);

#if defined (_AMD64_)

    PointerPde = MiGetPxeAddress (MmSessionBase);
    ASSERT (PointerPde->u.Long == 0);
    MI_WRITE_VALID_PTE (PointerPde, SessionGlobal->PageDirectory);

#else

    PointerPde = MiGetPdeAddress (MmSessionBase);

    ASSERT (RtlCompareMemoryUlong (PointerPde, 
                                   MiSessionSpacePageTables * sizeof (MMPTE),
                                   0) == MiSessionSpacePageTables * sizeof (MMPTE));

    RtlCopyMemory (PointerPde,
                   &SessionGlobal->PageTables[0],
                   MiSessionSpacePageTables * sizeof (MMPTE));

#endif
}


VOID
MiDetachSession (
    VOID
    )

/*++

Routine Description:

    Detaches from the specified session space.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  Current process must not have a session
    space to return to - ie: this should be the system process.

--*/

{
    PMMPTE PointerPde;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0);
    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

#if defined (_AMD64_)

    PointerPde = MiGetPxeAddress (MmSessionBase);

    MI_WRITE_ZERO_PTE (PointerPde);

#else

    PointerPde = MiGetPdeAddress (MmSessionBase);

    RtlZeroMemory (PointerPde, MiSessionSpacePageTables * sizeof (MMPTE));

#endif

    MI_FLUSH_SESSION_TB ();
}

#if DBG

VOID
MiCheckSessionVirtualSpace (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Used to verify that no drivers fail to clean up their session allocations.

Arguments:

    VirtualAddress - Supplies the starting virtual address to check.

    NumberOfBytes - Supplies the number of bytes to check.

Return Value:

    TRUE if all the PTEs have been freed, FALSE if not.

Environment:

    Kernel mode.  APCs disabled.

--*/

{
    PMMPTE StartPde;
    PMMPTE EndPde;
    PMMPTE StartPte;
    PMMPTE EndPte;
    ULONG Index;

    //
    // Check the specified region.  Everything should have been cleaned up
    // already.
    //

#if defined (_AMD64_)
    ASSERT64 (MiGetPxeAddress (VirtualAddress)->u.Hard.Valid == 1);
#endif

    ASSERT64 (MiGetPpeAddress (VirtualAddress)->u.Hard.Valid == 1);

    StartPde = MiGetPdeAddress (VirtualAddress);
    EndPde = MiGetPdeAddress ((PVOID)((PCHAR)VirtualAddress + NumberOfBytes - 1));

    StartPte = MiGetPteAddress (VirtualAddress);
    EndPte = MiGetPteAddress ((PVOID)((PCHAR)VirtualAddress + NumberOfBytes - 1));

    Index = (ULONG)(StartPde - MiGetPdeAddress ((PVOID)MmSessionBase));

#if (_MI_PAGING_LEVELS >= 3)
    while (StartPde <= EndPde && StartPde->u.Long == 0)
#else
    while (StartPde <= EndPde && MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
    {
        StartPde += 1;
        Index += 1;
        StartPte = MiGetVirtualAddressMappedByPte (StartPde);
    }

    while (StartPte <= EndPte) {

        if (MiIsPteOnPdeBoundary(StartPte)) {

            StartPde = MiGetPteAddress (StartPte);
            Index = (ULONG)(StartPde - MiGetPdeAddress ((PVOID)MmSessionBase));

#if (_MI_PAGING_LEVELS >= 3)
            while (StartPde <= EndPde && StartPde->u.Long == 0)
#else
            while (StartPde <= EndPde && MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
            {
                Index += 1;
                StartPde += 1;
                StartPte = MiGetVirtualAddressMappedByPte (StartPde);
            }
            if (StartPde > EndPde) {
                break;
            }
        }

        if (StartPte->u.Long != 0 && StartPte->u.Long != MM_KERNEL_NOACCESS_PTE) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                "MiCheckSessionVirtualSpace: StartPte 0x%p is still valid! 0x%p, VA 0x%p\n",
                StartPte,
                StartPte->u.Long,
                MiGetVirtualAddressMappedByPte(StartPte));

            DbgBreakPoint ();
        }
        StartPte += 1;
    }
}
#endif


VOID
MiReleaseProcessReferenceToSessionDataPage (
    PMM_SESSION_SPACE SessionGlobal
    )

/*++

Routine Description:

    Decrement this process' session reference.  The session itself may have
    already been deleted.  If this is the last reference to the session,
    then the session data page and its mapping PTE (if any) will be destroyed
    upon return.

Arguments:

    SessionGlobal - Supplies the global session space pointer being
                    dereferenced.  The caller has already verified that this
                    process is a member of the target session.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
    ULONG i;
    ULONG SessionId;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex[MI_SESSION_DATA_PAGES_MAXIMUM];
    PMMPFN Pfn1;
    KIRQL OldIrql;

    if (InterlockedDecrement (&SessionGlobal->ProcessReferenceToSession) != 0) {
        return;
    }

    SessionId = SessionGlobal->SessionId;

    //
    // Free the datapages & self-map PTE now since this is the last
    // process reference and KeDetach has returned.
    //

#if (_MI_PAGING_LEVELS < 3)
    ExFreePool (SessionGlobal->PageTables);
#endif

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(SessionGlobal));

    PointerPte = MiGetPteAddress (SessionGlobal);

    for (i = 0; i < MiSessionDataPages; i += 1) {
        PageFrameIndex[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + i);
    }

    MiReleaseSystemPtes (PointerPte, MiSessionDataPages, SystemPteSpace);

    for (i = 0; i < MiSessionDataPages; i += 1) {
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex[i]);
        MI_SET_PFN_DELETED (Pfn1);
    }

    LOCK_PFN (OldIrql);

    //
    // Get rid of the session data pages.
    //

    for (i = 0; i < MiSessionDataPages; i += 1) {

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex[i]);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);

        MiDecrementShareCount (Pfn1, PageFrameIndex[i]);
    }

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (MiSessionDataPages,
                                     MM_RESAVAIL_FREE_DEREFERENCE_SESSION);

    InterlockedDecrement (&MmSessionDataPages);

    //
    // Return commitment for the datapages.
    //

    MiReturnCommitment (MiSessionDataPages);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_DATAPAGE,
                     MiSessionDataPages);

    //
    // Release the session ID so it can be recycled.
    //

    KeAcquireGuardedMutex (&MiSessionIdMutex);

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));
    RtlClearBit (MiSessionIdBitmap, SessionId);

    KeReleaseGuardedMutex (&MiSessionIdMutex);
}


VOID
MiDereferenceSession (
    VOID
    )

/*++

Routine Description:

    Decrement this process' reference count to the session, unmapping access
    to the session for the current process.  If this is the last process
    reference to this session, then the entire session will be destroyed upon
    return.  This includes unloading drivers, unmapping pools, freeing
    page tables, etc.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
    PMMPTE StartPde;

    ULONG SessionId;
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) ||
            ((MmSessionSpace->u.Flags.Initialized == 0) && (PsGetCurrentProcess()->Vm.Flags.SessionLeader == 1) && (MmSessionSpace->ReferenceCount == 1)));

    SessionId = MmSessionSpace->SessionId;

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));

    InterlockedDecrement (&MmSessionSpace->ResidentProcessCount);

    if (InterlockedDecrement ((PLONG)&MmSessionSpace->ReferenceCount) != 0) {

        Process = PsGetCurrentProcess ();

        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_IN_SESSION);

        //
        // Don't delete any non-smss session space mappings here.  Let them
        // live on through process death.  This handles the case where
        // MmDispatchWin32Callout picks csrss - csrss has exited as it's not
        // the last process (smss is).  smss is simultaneously detaching from
        // the session and since it is the last process, it's waiting on
        // the AttachCount below.  The dispatch callout ends up in csrss but
        // has no way to synchronize against csrss exiting through this path
        // as the object reference count doesn't stop it.  So leave the
        // session space mappings alive so the callout can execute through
        // the remains of csrss.
        //
        // Note that when smss detaches, the address space must get cleared
        // here so that subsequent session creations by smss will succeed.
        //

        if (Process->Vm.Flags.SessionLeader == 1) {

            SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

#if defined (_AMD64_)

            StartPde = MiGetPxeAddress (MmSessionBase);
            StartPde->u.Long = 0;

#else

            StartPde = MiGetPdeAddress (MmSessionBase);
            RtlZeroMemory (StartPde, MiSessionSpacePageTables * sizeof(MMPTE));

#endif

            MI_FLUSH_SESSION_TB ();
    
            //
            // This process' reference to the session must be NULL as the
            // KeDetach has completed so no swap context referencing the
            // earlier session page can occur from here on.  This is also
            // needed because during clean shutdowns, the final dereference
            // of this process (smss) object will trigger an
            // MmDeleteProcessAddressSpace - this routine will dereference
            // the (no-longer existing) session space if this
            // bit is not cleared properly.
            //

            ASSERT (Process->Session == NULL);

            //
            // Another process may have won the race and exited the session
            // as this process is executing here.  Hence the reference count
            // is carefully checked here to ensure no leaks occur.
            //

            MiReleaseProcessReferenceToSessionDataPage (SessionGlobal);
        }
        return;
    }

    //
    // This is the final process in the session so the entire session must
    // be dereferenced now.
    //

    MiDereferenceSessionFinal ();
}


VOID
MiDereferenceSessionFinal (
    VOID
    )

/*++

Routine Description:

    Decrement this process' reference count to the session, unmapping access
    to the session for the current process.  If this is the last process
    reference to this session, then the entire session will be destroyed upon
    return.  This includes unloading drivers, unmapping pools, freeing
    page tables, etc.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    ULONG Index;
    ULONG_PTR CountReleased;
    ULONG_PTR CountReleased2;
    ULONG SessionId;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrame;
    KEVENT Event;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PointerPte;
    PMMPTE EndPte;
    PMMPTE GlobalPteEntrySave;
    PMMPTE StartPde;
    PMM_SESSION_SPACE SessionGlobal;
    ULONG AttachCount;
    PEPROCESS Process;
    PKTHREAD CurrentThread;
    PMMPFN DataFramePfn[MI_SESSION_DATA_PAGES_MAXIMUM];
    PMMPTE PointerPde;
#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER PageDirectoryFrame;
    PFN_NUMBER PageParentFrame;
#endif

    Process = PsGetCurrentProcess();

    ASSERT ((Process->Flags & PS_PROCESS_FLAGS_IN_SESSION) ||
            ((MmSessionSpace->u.Flags.Initialized == 0) && (Process->Vm.Flags.SessionLeader == 1) && (MmSessionSpace->ReferenceCount == 0)));

    SessionId = MmSessionSpace->SessionId;

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));

    //
    // This is the final dereference.  We could be any process
    // including SMSS when a session space load fails.  Note also that
    // processes can terminate in any order as well.
    //

    SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

    MmLockPageableSectionByHandle (ExPageLockHandle);

    LOCK_EXPANSION (OldIrql);

    //
    // Wait for any cross-session process attaches to detach.  Refuse
    // subsequent attempts to cross-session attach so the address invalidation
    // code doesn't surprise an ongoing or subsequent attachee.
    //

    ASSERT (MmSessionSpace->u.Flags.DeletePending == 0);

    MmSessionSpace->u.Flags.DeletePending = 1;

    AttachCount = MmSessionSpace->AttachCount;

    if (AttachCount) {

        KeInitializeEvent (&SessionGlobal->AttachEvent,
                           NotificationEvent,
                           FALSE);

        UNLOCK_EXPANSION (OldIrql);

        KeWaitForSingleObject (&SessionGlobal->AttachEvent,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        LOCK_EXPANSION (OldIrql);

        ASSERT (MmSessionSpace->u.Flags.DeletePending == 1);
        ASSERT (MmSessionSpace->AttachCount == 0);
    }

    if (MmSessionSpace->Vm.WorkingSetExpansionLinks.Flink == MM_WS_TRIMMING) {

        //
        // Initialize an event and put the event address
        // in the VmSupport.  When the trimming is complete,
        // this event will be set.
        //

        KeInitializeEvent (&Event, NotificationEvent, FALSE);

        MmSessionSpace->Vm.WorkingSetExpansionLinks.Blink = (PLIST_ENTRY)&Event;

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);

        UNLOCK_EXPANSION (OldIrql);

        KeWaitForSingleObject (&Event,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        KeLeaveCriticalRegionThread (CurrentThread);

        LOCK_EXPANSION (OldIrql);
        ASSERT (Process->Vm.WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED);
    }
    else if (MmSessionSpace->Vm.WorkingSetExpansionLinks.Flink == MM_WS_NOT_LISTED) {
        //
        // This process' working set is in an initialization state and has
        // never been inserted into any lists.
        //

        NOTHING;
    }
    else {

        //
        // Remove this session from the working set list.
        //

        RemoveEntryList (&SessionGlobal->Vm.WorkingSetExpansionLinks);

        SessionGlobal->Vm.WorkingSetExpansionLinks.Flink = MM_WS_NOT_LISTED;
    }

    if (SessionGlobal->WsListEntry.Flink != NULL) {
        RemoveEntryList (&SessionGlobal->WsListEntry);
    }

    UNLOCK_EXPANSION (OldIrql);

#if DBG
    if (Process->Vm.Flags.SessionLeader == 0) {
        ASSERT (MmSessionSpace->ResidentProcessCount == 0);
        ASSERT (MmSessionSpace->ReferenceCount == 0);
    }
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(0);

    //
    // If an unload function has been registered for WIN32K.SYS,
    // call it now before we force an unload on any modules.  WIN32K.SYS
    // is responsible for calling any other loaded modules that have
    // unload routines to be run.  Another option is to have the other
    // session drivers register a DLL initialize/uninitialize pair on load.
    //

    if (MmSessionSpace->Win32KDriverUnload) {

        //
        // Note win32k does not reference the argument driver object so just
        // pass in NULL.
        //

        MmSessionSpace->Win32KDriverUnload (NULL);
    }

    //
    // Delete the session paged pool lookaside lists.
    //

    if (MmSessionSpace->u.Flags.Initialized) {

        for (i = 0; i < SESSION_POOL_SMALL_LISTS; i += 1) {
            ExDrainPoolLookasideList ((PPAGED_LOOKASIDE_LIST)&SessionGlobal->Lookaside[i]);
            ExDeletePagedLookasideList ((PPAGED_LOOKASIDE_LIST)&SessionGlobal->Lookaside[i]);
        }
    }

    //
    // Complete all deferred pool block deallocations.
    //

    ExDeferredFreePool (&MmSessionSpace->PagedPool);

    //
    // Now that all modules have had their unload routine(s)
    // called, check for pool leaks before unloading the images.
    //

    MiCheckSessionPoolAllocations ();

    ASSERT (MmSessionSpace->ReferenceCount == 0);

#if defined (_WIN64)
    if (MmSessionSpecialPoolStart != 0) {
        MiDeleteSessionSpecialPool ();
    }
#endif

    PointerPte = MiGetPteAddress (MmSessionSpace);

    LOCK_PFN (OldIrql);

    //
    // Free the physical frames for pool tracking & tagging now.
    //

    for (i = 0; i < MiSessionTagPages; i += 1) {
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + MiSessionDataPages + i);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageFrameIndex);
    }

    UNLOCK_PFN (OldIrql);

    //
    // Track the resident available and commit for return later in this routine.
    //

    CountReleased = MiSessionTagPages;

    MmSessionSpace->NonPageablePages -= MiSessionTagPages;
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 0 - (SIZE_T)MiSessionTagPages);

    MM_SNAP_SESS_MEMORY_COUNTERS(1);

    //
    // Destroy the view mapping structures.
    //

    MiFreeSessionSpaceMap ();

    MM_SNAP_SESS_MEMORY_COUNTERS(2);

    //
    // Walk down the list of modules we have loaded dereferencing them.
    //
    // This allows us to force an unload of any kernel images loaded by
    // the session so we do not have any virtual space and paging
    // file leaks.
    //

    MiSessionUnloadAllImages ();

    MM_SNAP_SESS_MEMORY_COUNTERS(3);

    //
    // Destroy the session space bitmap structure
    //

    MiFreeSessionPoolBitMaps ();

    MM_SNAP_SESS_MEMORY_COUNTERS(4);

    //
    // Reference the session space structure using its global
    // kernel PTE based address.  This is to avoid deleting it out
    // from underneath ourselves.
    //

    GlobalPteEntrySave = MiGetPteAddress (SessionGlobal);

    //
    // Sweep the individual regions in their proper order.
    //

#if DBG

    //
    // Check the executable image region. All images
    // should have been unloaded by the image handler.
    //

    MiCheckSessionVirtualSpace ((PVOID) MiSessionImageStart, MmSessionImageSize);
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(5);

#if DBG

    //
    // Check the view region. All views should have been cleaned up already.
    //

    MiCheckSessionVirtualSpace ((PVOID) MiSessionViewStart, MmSessionViewSize);
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(6);

#if DBG
    //
    // Check everything possible before the remaining virtual address space
    // is torn down.  In this way if anything is amiss, the data can be
    // more easily examined.
    //

    Pfn1 = MI_PFN_ELEMENT (MmSessionSpace->SessionPageDirectoryIndex);

    //
    // This should be greater than 1 because working set page tables are
    // using this as their parent as well.
    //

    ASSERT (Pfn1->u2.ShareCount > 1);
#endif

    if (MmSessionSpace->u.Flags.Initialized == 1) {

        PointerPte = MiGetPteAddress ((PVOID)MiSessionSpaceWs);
#if (_MI_PAGING_LEVELS >= 3)
        PointerPde = MiGetPdeAddress ((PVOID)MiSessionSpaceWs);
#endif
        EndPte = MiGetPteAddress (MmSessionSpace->Vm.VmWorkingSetList->HighestPermittedHashAddress);

        CountReleased2 = 0;

        while (PointerPte < EndPte) {

            if (PointerPte->u.Long) {

                ASSERT (PointerPte->u.Hard.Valid == 1);

                //
                // Track the resident available and commit for return later
                // in this routine.
                //

                CountReleased2 += 1;
            }

            PointerPte += 1;

#if (_MI_PAGING_LEVELS >= 3)

            //
            // The PXE and PPE must be valid because all of session space is
            // contained within a single PPE.  However, each PDE must be
            // checked for validity.
            //

            if (MiIsPteOnPdeBoundary (PointerPte)) {
                ASSERT (PointerPde == MiGetPteAddress (PointerPte - 1));
                PointerPde += 1;
                while (PointerPde->u.Hard.Valid == 0) {
                    PointerPde += 1;
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    if (PointerPte >= EndPte) {
                        break;
                    }
                }
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            }
#endif

        }

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_PAGE_FREE, (ULONG) CountReleased2);

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CountReleased2);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_WS_PAGE_FREE, (ULONG) CountReleased2);
        MmSessionSpace->NonPageablePages -= CountReleased2;

        CountReleased += CountReleased2;
    }

    //
    // Account for the session data structure data pages in our tracking
    // structures.  The actual data pages and their commitment can only be
    // returned after the last process has been reaped (not just exited).
    //

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE, MiSessionDataPages);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 0 - (SIZE_T)MiSessionDataPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_DESTROY, MiSessionDataPages);
    MmSessionSpace->NonPageablePages -= MiSessionDataPages;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // For NT64, the page directory page and its parent are explicitly
    // accounted for here because only page table pages are checked in
    // the loop after this.
    //

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE, 2);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, -2);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_DESTROY, 2);
    MmSessionSpace->NonPageablePages -= 2;

    CountReleased += 2;

#endif

    //
    // Account for any needed session space page tables.  Note the common
    // structure (not the local PDEs) must be examined as any page tables
    // that were dynamically materialized in the context of a different
    // process may not be in the current process' page directory (ie: the
    // current process has never accessed the materialized VAs) !
    //

#if (_MI_PAGING_LEVELS >= 3)
    StartPde = MiGetPdeAddress ((PVOID)MmSessionBase);
#else
    StartPde = &MmSessionSpace->PageTables[0];
#endif

    CountReleased2 = 0;
    for (Index = 0; Index < MiSessionSpacePageTables; Index += 1) {

        if (StartPde->u.Long != 0) {
            CountReleased2 += 1;
        }

        StartPde += 1;
    }

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_PAGETABLE_FREE, (ULONG) CountReleased2);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                 0 - CountReleased2);
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_PTDESTROY, (ULONG) CountReleased2);
    MmSessionSpace->NonPageablePages -= CountReleased2;
    CountReleased += CountReleased2;

    ASSERT (MmSessionSpace->NonPageablePages == 0);

    //
    // Note that whenever win32k or drivers loaded by it leak pool, the
    // ASSERT below will be triggered.
    //

    ASSERT (MmSessionSpace->CommittedPages == 0);

    MiReturnCommitment (CountReleased);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_DEREFERENCE, CountReleased);

    //
    // Sweep the working set entries.
    // No more accesses to the working set or its lock are allowed.
    //

    if (MmSessionSpace->u.Flags.Initialized == 1) {

        PointerPte = MiGetPteAddress ((PVOID)MiSessionSpaceWs);
        EndPte = MiGetPteAddress (MmSessionSpace->Vm.VmWorkingSetList->HighestPermittedHashAddress);

#if (_MI_PAGING_LEVELS >= 3)
        PointerPde = MiGetPdeAddress ((PVOID)MiSessionSpaceWs);
#endif

        while (PointerPte < EndPte) {

            if (PointerPte->u.Long) {

                ASSERT (PointerPte->u.Hard.Valid == 1);

                //
                // Delete the page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

                //
                // Each page should still be locked in the session working set.
                //

                LOCK_PFN (OldIrql);

                ASSERT (Pfn1->u3.e2.ReferenceCount == 1);

                MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

                MI_SET_PFN_DELETED (Pfn1);
                MiDecrementShareCount (Pfn1, PageFrameIndex);

                //
                // Don't return the resident available pages charge here
                // as it's going to be returned in one chunk below as part of
                // CountReleased.
                //

                UNLOCK_PFN (OldIrql);

                MI_WRITE_ZERO_PTE (PointerPte);
            }

            PointerPte += 1;

#if (_MI_PAGING_LEVELS >= 3)

            //
            // The PXE and PPE must be valid because all of session space is
            // contained within a single PPE.  However, each PDE must be
            // checked for validity.
            //

            if (MiIsPteOnPdeBoundary (PointerPte)) {
                ASSERT (PointerPde == MiGetPteAddress (PointerPte - 1));
                PointerPde += 1;
                while (PointerPde->u.Hard.Valid == 0) {
                    PointerPde += 1;
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    if (PointerPte >= EndPte) {
                        break;
                    }
                }
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            }
#endif

        }
    }

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(SessionGlobal));

    for (i = 0; i < MiSessionDataPages; i += 1) {
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (GlobalPteEntrySave + i);
        DataFramePfn[i] = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (DataFramePfn[i]->u4.PteFrame == MmSessionSpace->SessionPageDirectoryIndex);

        //
        // Make sure the data pages are still locked.
        //

        ASSERT (DataFramePfn[i]->u1.WsIndex == 0);
        ASSERT (DataFramePfn[i]->u3.e2.ReferenceCount == 1);
    }

    //
    // Save the local page table index so it can be decremented safely below.
    //

    PageTableFrame = DataFramePfn[0]->u4.PteFrame;

#if (_MI_PAGING_LEVELS >= 3)

    PageDirectoryFrame = MI_PFN_ELEMENT(PageTableFrame)->u4.PteFrame;
    PageParentFrame = MI_PFN_ELEMENT(PageDirectoryFrame)->u4.PteFrame;

    for (i = 1; i < MiSessionDataPages; i += 1) {
        ASSERT (PageDirectoryFrame == MI_PFN_ELEMENT(DataFramePfn[i]->u4.PteFrame)->u4.PteFrame);
    }

#endif

    LOCK_PFN (OldIrql);

    //
    // Release the data page claims to their page table page.
    //

    for (i = 0; i < MiSessionDataPages; i += 1) {
        ASSERT (DataFramePfn[i]->u4.PteFrame == PageTableFrame);
        MiDecrementShareCount (MI_PFN_ELEMENT (PageTableFrame),
                               PageTableFrame);
    }

    //
    // Delete the VA space - no more accesses to MmSessionSpace at this point.
    //
    // Cut off the pagetable reference as the local page table is going to be
    // freed now.  Any needed references must go through the global PTE
    // space or superpages but never through the local session VA.  The
    // actual session data pages are not freed until the very last process
    // of the session receives its very last object dereference.
    //
    // Delete page table pages first.
    //

#if (_MI_PAGING_LEVELS >= 3)
    PointerPde = MiGetPdeAddress ((PVOID)MmSessionBase);
#else
    PointerPde = &SessionGlobal->PageTables[0];
#endif

    for (Index = 0; Index < MiSessionSpacePageTables; Index += 1, PointerPde += 1) {

        if (PointerPde->u.Long == 0) {
            continue;
        }
    
        ASSERT (PointerPde->u.Hard.Valid == 1);
    
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    
        PageTableFrame = Pfn1->u4.PteFrame;

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageFrameIndex);

        //
        // Do the page table page *AFTER* the data page because for 32-bit NT,
        // all the page session space page tables use the session data page's
        // page table as the containing frame (to avoid referencing a top level
        // user page directory page in smss).  However, the MiIdentifyPfn
        // logging code gets confused if we delete a page that has an
        // already-freed containing frame, hence the ordering.
        //

        Pfn2 = MI_PFN_ELEMENT (PageTableFrame);
        MiDecrementShareCount (Pfn2, PageTableFrame);

        PointerPde->u.Long = 0;
    }

#if defined (_AMD64_)
    StartPde = MiGetPxeAddress (MmSessionBase);
    StartPde->u.Long = 0;
#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Delete the session page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryFrame);
    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCount (Pfn1, PageDirectoryFrame);

    //
    // Delete the session page directory parent page.
    //

    Pfn1 = MI_PFN_ELEMENT (PageParentFrame);
    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCount (Pfn1, PageParentFrame);

    //
    // The page directory still has a share count with the page directory
    // parent so decrement that now so the parent can be freed.
    //

    MiDecrementShareCount (Pfn1, PageParentFrame);

#else

    StartPde = MiGetPdeAddress ((PVOID)MmSessionBase);
    MiZeroMemoryPte (StartPde, MiSessionSpacePageTables);

#endif

    UNLOCK_PFN (OldIrql);

    //
    // Return resident available for the pool tagging & tracking tables as
    // well as page table pages and working set structure pages.
    //

    MI_INCREMENT_RESIDENT_AVAILABLE (CountReleased,
                                     MM_RESAVAIL_FREE_DEREFERENCE_SESSION_PAGES);

    MI_INCREMENT_RESIDENT_AVAILABLE (MI_SESSION_SPACE_WORKING_SET_MINIMUM,
                                     MM_RESAVAIL_FREE_DEREFERENCE_SESSION_WS);

    //
    // Flush the session space TB entries.
    //

    MI_FLUSH_SESSION_TB ();

    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_IN_SESSION);

    //
    // The session space has been deleted and all TB flushing is complete.
    //

    MmUnlockPageableImageSection (ExPageLockHandle);

    //
    // The session leader must release the reference here since the
    // session leader never exits and thus doesn't trigger this via
    // process delete.
    //

    if (Process->Vm.Flags.SessionLeader == 1) {
        ASSERT (Process->Session == NULL);
        MiReleaseProcessReferenceToSessionDataPage (SessionGlobal);
    }

    return;
}

NTSTATUS
MiSessionCommitPageTables (
    IN PVOID StartVa,
    IN PVOID EndVa
    )

/*++

Routine Description:

    Fill in page tables covering the specified virtual address range.

Arguments:

    StartVa - Supplies a starting virtual address.

    EndVa - Supplies an ending virtual address.

Return Value:

    STATUS_SUCCESS on success, STATUS_NO_MEMORY on failure.

Environment:

    Kernel mode.  Session space working set mutex NOT held.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    ULONG Waited;
    KIRQL OldIrql;
    ULONG Color;
    ULONG Index;
    PMMPTE StartPde;
    PMMPTE EndPde;
    MMPTE TempPte;
    PMMPFN Pfn1;
    WSLE_NUMBER SwapEntry;
    WSLE_NUMBER WorkingSetIndex;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER ActualPages;
    PFN_NUMBER PageTablePage;
    PVOID SessionPte;
    PMMWSL WorkingSetList;
    PETHREAD CurrentThread;
    NTSTATUS Status;
    PMMSUPPORT Ws;
    
    ASSERT (StartVa >= (PVOID)MmSessionBase);
    ASSERT (EndVa < (PVOID)MiSessionSpaceEnd);
    ASSERT (PAGE_ALIGN (EndVa) == EndVa);

    //
    // Allocate the page table pages, loading them
    // into the current process's page directory.
    //

    StartPde = MiGetPdeAddress (StartVa);
    EndPde = MiGetPdeAddress ((PVOID)((ULONG_PTR)EndVa - 1));
    Index = MiGetPdeSessionIndex (StartVa);
    SizeInPages = 0;

    while (StartPde <= EndPde) {
#if (_MI_PAGING_LEVELS >= 3)
        if (StartPde->u.Long == 0)
#else
        if (MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
        {
            SizeInPages += 1;
        }
        StartPde += 1;
        Index += 1;
    }

    if (SizeInPages == 0) {
        return STATUS_SUCCESS;
    }

    if (MiChargeCommitment (SizeInPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        return STATUS_NO_MEMORY;
    }

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if ((SPFN_NUMBER)SizeInPages > MI_NONPAGEABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (SizeInPages);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        return STATUS_NO_MEMORY;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (SizeInPages, MM_RESAVAIL_ALLOCATE_SESSION_PAGE_TABLES);

    UNLOCK_PFN (OldIrql);



    CurrentThread = PsGetCurrentThread ();
    ActualPages = 0;
    TempPte = ValidKernelPdeLocal;
    Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
    WorkingSetList = MmSessionSpace->Vm.VmWorkingSetList;
    StartPde = MiGetPdeAddress (StartVa);
    Index = MiGetPdeSessionIndex (StartVa);
    Status = STATUS_SUCCESS;



    LOCK_WORKING_SET (CurrentThread, Ws);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_PAGETABLE_PAGES, SizeInPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_PAGETABLE_ALLOC, (ULONG)SizeInPages);

    while (StartPde <= EndPde) {

#if (_MI_PAGING_LEVELS >= 3)
        if (StartPde->u.Long == 0)
#else
        if (MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
        {

            ASSERT (StartPde->u.Hard.Valid == 0);

            LOCK_PFN (OldIrql);

            Waited = MiEnsureAvailablePageOrWait (HYDRA_PROCESS, OldIrql);

            if (Waited != 0) {

                //
                // The session space working set mutex & PFN lock were
                // released and reacquired so recheck the master page
                // directory as another thread may have filled this entry
                // in the interim.
                //

                UNLOCK_PFN (OldIrql);
                continue;
            }

            Color = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);

            PageTablePage = MiRemoveZeroPage (Color);

            TempPte.u.Hard.PageFrameNumber = PageTablePage;
            MI_WRITE_VALID_PTE (StartPde, TempPte);

#if (_MI_PAGING_LEVELS < 3)
            ASSERT (MmSessionSpace->PageTables[Index].u.Long == 0);
            MmSessionSpace->PageTables[Index] = TempPte;
#endif
            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_COMMIT_IMAGE_PT, 1);

            MiInitializePfnForOtherProcess (PageTablePage,
                                            StartPde,
                                            MmSessionSpace->SessionPageDirectoryIndex);
            UNLOCK_PFN (OldIrql);

            Pfn1 = MI_PFN_ELEMENT (PageTablePage);

            ASSERT (Pfn1->u1.Event == NULL);

            SessionPte = MiGetVirtualAddressMappedByPte (StartPde);

            WorkingSetIndex = MiAddValidPageToWorkingSet (SessionPte,
                                                          StartPde,
                                                          Pfn1,
                                                          0);

            if (WorkingSetIndex == 0) {

                //
                // A working set entry could not be allocated.  Just delete
                // the page table we just allocated as no one else could
                // be using it (as we have held the session's working set mutex
                // since initializing the PTE) and return a failure.
                //

                ASSERT (Pfn1->u3.e1.PrototypePte == 0);

                LOCK_PFN (OldIrql);
                MI_SET_PFN_DELETED (Pfn1);
                UNLOCK_PFN (OldIrql);

                MiTrimPte (SessionPte,
                           StartPde,
                           Pfn1,
                           HYDRA_PROCESS,
                           ZeroPte);

#if (_MI_PAGING_LEVELS < 3)

                ASSERT (MmSessionSpace->PageTables[Index].u.Long != 0);
                MmSessionSpace->PageTables[Index].u.Long = 0;
#endif

                Status = STATUS_NO_MEMORY;
                break;
            }

            ActualPages += 1;

            ASSERT (WorkingSetIndex == MiLocateWsle (SessionPte,
                                                     WorkingSetList,
                                                     Pfn1->u1.WsIndex,
                                                     FALSE));

            if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {

                SwapEntry = WorkingSetList->FirstDynamic;

                if (WorkingSetIndex != WorkingSetList->FirstDynamic) {

                    //
                    // Swap this entry with the one at first dynamic.
                    //

                    MiSwapWslEntries (WorkingSetIndex,
                                      SwapEntry,
                                      &MmSessionSpace->Vm,
                                      FALSE);
                }

                WorkingSetList->FirstDynamic += 1;
            }
            else {
                SwapEntry = WorkingSetIndex;
            }

            //
            // Indicate that the page is locked.
            //

            MmSessionSpace->Wsle[SwapEntry].u1.e1.LockedInWs = 1;
        }

        StartPde += 1;
        Index += 1;
    }

    ASSERT (ActualPages <= SizeInPages);

    UNLOCK_WORKING_SET (CurrentThread, Ws);

    if (ActualPages != 0) {

        InterlockedExchangeAddSizeT (&MmSessionSpace->NonPageablePages,
                                     ActualPages);

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     ActualPages);
    }

    //
    // Return any excess commitment and resident available charges.
    //

    if (ActualPages < SizeInPages) {
        MiReturnCommitment (SizeInPages - ActualPages);
        MI_INCREMENT_RESIDENT_AVAILABLE (SizeInPages - ActualPages,
                                   MM_RESAVAIL_FREE_SESSION_PAGE_TABLES_EXCESS);
    }

    return Status;
}

#if DBG
typedef struct _MISWAP {
    ULONG Flag;
    ULONG ResidentProcessCount;
    PEPROCESS Process;
    PMM_SESSION_SPACE Session;
} MISWAP, *PMISWAP;

#define MI_SESSION_SWAP_SIZE 0x100

ULONG MiSessionInfo[4];
MISWAP MiSessionSwap[MI_SESSION_SWAP_SIZE];
LONG  MiSwapIndex;
#endif


VOID
MiSessionOutSwapProcess (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine notifies the containing session that the specified process is
    being outswapped.  When all the processes within a session have been
    outswapped, the containing session undergoes a heavy trim.

Arguments:

    Process - Supplies a pointer to the process that is swapped out of memory.

Return Value:

    None.

Environment:

    Kernel mode.  This routine must not enter a wait state for memory resources
    or the system will deadlock.

--*/

{
    PMM_SESSION_SPACE SessionGlobal;
#if DBG
    LONG SwapIndex;
#endif

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_IN_SESSION);

    //
    // smss doesn't count when we swap it before it has detached from the
    // session it is currently creating.
    //

    if (Process->Vm.Flags.SessionLeader == 1) {
        return;
    }

    //
    // If the process has already exited then it is no longer really a
    // member of the session so don't count it in the outswap counts.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    ASSERT (SessionGlobal != NULL);
    ASSERT (SessionGlobal->ResidentProcessCount > 0);
    ASSERT (SessionGlobal->ResidentProcessCount <= SessionGlobal->ReferenceCount);

#if DBG
    SwapIndex = InterlockedIncrement (&MiSwapIndex);
    SwapIndex &= (MI_SESSION_SWAP_SIZE - 1);

    MiSessionSwap[SwapIndex].Flag = 1;
    MiSessionSwap[SwapIndex].Process = Process;
    MiSessionSwap[SwapIndex].Session = SessionGlobal;
    MiSessionSwap[SwapIndex].ResidentProcessCount = SessionGlobal->ResidentProcessCount;
#endif

    if (InterlockedDecrement (&SessionGlobal->ResidentProcessCount) == 0) {

#if DBG
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
                "Mm: Last process (%d total) just swapped out for session %d, %d pages\n",
                SessionGlobal->ReferenceCount,
                SessionGlobal->SessionId,
                SessionGlobal->Vm.WorkingSetSize);
        }
        MiSessionInfo[0] += 1;
#endif
        KeQuerySystemTime (&SessionGlobal->LastProcessSwappedOutTime);
    }
#if DBG
    else {
        MiSessionInfo[1] += 1;
    }
#endif

    return;
}


VOID
MiSessionInSwapProcess (
    IN PEPROCESS Process,
    IN LOGICAL Forced
    )

/*++

Routine Description:

    This routine in swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process that is to be swapped
              into memory.

    Forced - Supplies TRUE if the process was brought in via KeForceAttach,
             FALSE otherwise.

Return Value:

    None.

Environment:

    Kernel mode.  This routine must not enter a wait state for memory resources
    or the system will deadlock.

--*/

{
    PMM_SESSION_SPACE SessionGlobal;
#if DBG
    LONG SwapIndex;
#else
    UNREFERENCED_PARAMETER (Forced);
#endif

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_IN_SESSION);

    //
    // smss doesn't count when we swap it before it has detached from the
    // session it is currently creating.
    //

    if (Process->Vm.Flags.SessionLeader == 1) {
        return;
    }

    //
    // If the process has already exited then it is no longer really a
    // member of the session so don't count it in the inswap counts.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;
    ASSERT (SessionGlobal != NULL);

    ASSERT (SessionGlobal->ResidentProcessCount >= 0);
    ASSERT (SessionGlobal->ResidentProcessCount <= SessionGlobal->ReferenceCount);

#if DBG
    SwapIndex = InterlockedIncrement (&MiSwapIndex);
    SwapIndex &= (MI_SESSION_SWAP_SIZE - 1);

    if (Forced == FALSE) {
        MiSessionSwap[SwapIndex].Flag = 2;
    }
    else {
        MiSessionSwap[SwapIndex].Flag = 3;
    }
    MiSessionSwap[SwapIndex].Process = Process;
    MiSessionSwap[SwapIndex].Session = SessionGlobal;
    MiSessionSwap[SwapIndex].ResidentProcessCount = SessionGlobal->ResidentProcessCount;
#endif

    if (InterlockedIncrement (&SessionGlobal->ResidentProcessCount) == 1) {
#if DBG
        MiSessionInfo[2] += 1;
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
                "Mm: First process (%d total) just swapped back in for session %d, %d pages\n",
                SessionGlobal->ReferenceCount,
                SessionGlobal->SessionId,
                SessionGlobal->Vm.WorkingSetSize);
        }
#endif
        SessionGlobal->Vm.Flags.TrimHard = 0;
    }
#if DBG
    else {
        MiSessionInfo[3] += 1;
    }
#endif

    return;
}


ULONG
MmGetSessionId (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine returns the session ID of the specified process.

Arguments:

    Process - Supplies a pointer to the process whose session ID is desired.

Return Value:

    The session ID.  Note these are recycled when sessions exit, hence the
    caller must use proper object referencing on the specified process.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    PMM_SESSION_SPACE SessionGlobal;

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        return 0;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        return 0;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    return SessionGlobal->SessionId;
}

ULONG
MmGetSessionIdEx (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine returns the session ID of the specified process or -1 if 
    if the process does not belong to any session.

Arguments:

    Process - Supplies a pointer to the process whose session ID is desired.

Return Value:

    The session ID.  Note these are recycled when sessions exit, hence the
    caller must use proper object referencing on the specified process.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    PMM_SESSION_SPACE SessionGlobal;

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        return (ULONG)-1;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        return (ULONG)-1;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    return SessionGlobal->SessionId;
}

PVOID
MmGetNextSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function allows code to enumerate all the sessions in the system.
    The first session (if OpaqueSession is NULL) or subsequent session
    (if session is not NULL) is returned on each call.

    If OpaqueSession is not NULL then this session must have previously
    been obtained by a call to MmGetNextSession.

    Enumeration may be terminated early by calling MmQuitNextSession on
    the last non-NULL session returned by MmGetNextSession.

    Sessions may be referenced in this manner and used later safely.

    For example, to enumerate all sessions in a loop use this code fragment:

    for (OpaqueSession = MmGetNextSession (NULL);
         OpaqueSession != NULL;
         OpaqueSession = MmGetNextSession (OpaqueSession)) {

         ...
         ...

         //
         // Checking for a specific session (if needed) is handled like this:
         //

         if (MmGetSessionId (OpaqueSession) == DesiredId) {

             //
             // Attach to session now to perform operations...
             //

             KAPC_STATE ApcState;

             if (NT_SUCCESS (MmAttachSession (OpaqueSession, &ApcState))) {

                //
                // Session hasn't exited yet, so call interesting work
                // functions that need session context ...
                //

                ...

                //
                // Detach from session.
                //

                MmDetachSession (OpaqueSession, &ApcState);
             }

             //
             // If the interesting work functions failed and error recovery
             // (ie: walk back through all the sessions already operated on
             // and try to undo the actions), then do this.  Note you must add
             // similar checks to the above if the operations were only done
             // to specifically requested session IDs.
             //

             if (ErrorRecoveryNeeded) {

                 for (OpaqueSession = MmGetPreviousSession (OpaqueSession);
                      OpaqueSession != NULL;
                      OpaqueSession = MmGetPreviousSession (OpaqueSession)) {

                      //
                      // MmAttachSession/DetachSession as needed to obtain
                      // context, etc.
                      //
                 }

                 break;
             }

             //
             // Bail if only this session was of interest.
             //

             MmQuitNextSession (OpaqueSession);
             break;
         }

         //
         // Early terminating conditions are handled like this:
         //

         if (NeedToBreakOutEarly) {
             MmQuitNextSession (OpaqueSession);
             break;
         }
    }
    

Arguments:

    OpaqueSession - Supplies the session to get the next session from
                    or NULL for the first session.

Return Value:

    Next session or NULL if no more sessions exist.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PMM_SESSION_SPACE EntrySession;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueNextSession;
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueNextSession = NULL;

    EntryProcess = (PEPROCESS) OpaqueSession;

    if (EntryProcess == NULL) {
        EntrySession = NULL;
    }
    else {
        ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

        //
        // The Session field of the EPROCESS is never cleared once set so this
        // field can be used lock free.
        //

        EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

        ASSERT (EntrySession != NULL);
    }

    LOCK_EXPANSION (OldIrql);

    if (EntrySession == NULL) {
        NextEntry = MiSessionWsList.Flink;
    }
    else {
        NextEntry = EntrySession->WsListEntry.Flink;
    }

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if ((Session->u.Flags.DeletePending == 0) &&
            (NextProcessEntry != &Session->ProcessList)) {

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            if (Process->Vm.Flags.SessionLeader == 1) {

                //
                // If session manager is still the first process (ie: smss
                // hasn't detached yet), then don't bother delivering to this
                // session this early in its lifetime.  And since smss is
                // serialized, it can't be creating another session yet so
                // just bail now as we must be at the end of the list.
                //

                break;
            }

            //
            // If the process has finished rudimentary initialization, then
            // select it as an attach can be performed safely.  If this first
            // process has not finished initializing there can be no others
            // in this session, so just march on to the next session.
            //
            // Note the VmWorkingSetList is used instead of the
            // AddressSpaceInitialized field because the VmWorkingSetList is
            // never cleared so we can never see an exiting process (whose
            // AddressSpaceInitialized field gets zeroed) and incorrectly
            // decide the list must be empty.
            //

            if (Process->Vm.VmWorkingSetList != NULL) {

                //
                // Reference any process in the session so that the session
                // cannot be completely deleted once the expansion lock is
                // released (note this does NOT prevent the session from being
                // cleaned).
                //

                ObReferenceObject (Process);
                OpaqueNextSession = (PVOID) Process;
                break;
            }
        }
        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION (OldIrql);

    //
    // Regardless of whether a next session is returned, if a starting one
    // was passed in, it must be dereferenced now.
    //

    if (EntryProcess != NULL) {
        ObDereferenceObject (EntryProcess);
    }

    return OpaqueNextSession;
}

PVOID
MmGetPreviousSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function allows code to reverse-enumerate all the sessions in
    the system.  This is typically used for error recovery - ie: to walk
    backwards undoing work done by MmGetNextSession semantics.

    The first session (if OpaqueSession is NULL) or subsequent session
    (if session is not NULL) is returned on each call.

    If OpaqueSession is not NULL then this session must have previously
    been obtained by a call to MmGetNextSession.

Arguments:

    OpaqueSession - Supplies the session to get the next session from
                    or NULL for the first session.

Return Value:

    Next session or NULL if no more sessions exist.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PMM_SESSION_SPACE EntrySession;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueNextSession;
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueNextSession = NULL;

    EntryProcess = (PEPROCESS) OpaqueSession;

    if (EntryProcess == NULL) {
        EntrySession = NULL;
    }
    else {
        ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

        //
        // The Session field of the EPROCESS is never cleared once set so this
        // field can be used lock free.
        //

        EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

        ASSERT (EntrySession != NULL);
    }

    LOCK_EXPANSION (OldIrql);

    if (EntrySession == NULL) {
        NextEntry = MiSessionWsList.Blink;
    }
    else {
        NextEntry = EntrySession->WsListEntry.Blink;
    }

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if ((Session->u.Flags.DeletePending == 0) &&
            (NextProcessEntry != &Session->ProcessList)) {

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            ASSERT (Process->Vm.Flags.SessionLeader == 0);

            //
            // Reference any process in the session so that the session
            // cannot be completely deleted once the expansion lock is
            // released (note this does NOT prevent the session from being
            // cleaned).
            //

            ObReferenceObject (Process);
            OpaqueNextSession = (PVOID) Process;
            break;
        }
        NextEntry = NextEntry->Blink;
    }

    UNLOCK_EXPANSION (OldIrql);

    //
    // Regardless of whether a next session is returned, if a starting one
    // was passed in, it must be dereferenced now.
    //

    if (EntryProcess != NULL) {
        ObDereferenceObject (EntryProcess);
    }

    return OpaqueNextSession;
}

NTSTATUS
MmQuitNextSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function is used to prematurely terminate a session enumeration
    that began using MmGetNextSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

Return Value:

    NTSTATUS.

--*/

{
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    ASSERT (EntryProcess->Session != NULL);

    ObDereferenceObject (EntryProcess);

    return STATUS_SUCCESS;
}

NTSTATUS
MmAttachSession (
    IN PVOID OpaqueSession,
    OUT PRKAPC_STATE ApcState
    )

/*++

Routine Description:

    This function attaches the calling thread to a referenced session
    previously obtained via MmGetNextSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

    ApcState - Supplies APC state storage for the subsequent detach.

Return Value:

    NTSTATUS.  If successful then we are attached on return.  The caller is
               responsible for calling MmDetachSession when done.

--*/

{
    KIRQL OldIrql;
    PEPROCESS EntryProcess;
    PMM_SESSION_SPACE EntrySession;
    PEPROCESS CurrentProcess;
    PMM_SESSION_SPACE CurrentSession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

    ASSERT (EntrySession != NULL);

    CurrentProcess = PsGetCurrentProcess ();

    CurrentSession = (PMM_SESSION_SPACE) CurrentProcess->Session;

    LOCK_EXPANSION (OldIrql);

    if (EntrySession->u.Flags.DeletePending == 1) {
        UNLOCK_EXPANSION (OldIrql);
        return STATUS_PROCESS_IS_TERMINATING;
    }

    EntrySession->AttachCount += 1;

    UNLOCK_EXPANSION (OldIrql);

    if ((CurrentProcess->Vm.Flags.SessionLeader == 0) &&
        (CurrentSession != NULL)) {

        //
        // smss may transiently have a session space but that's of
        // no interest to our caller.
        //

        if (CurrentSession == EntrySession) {

            ASSERT (CurrentSession->SessionId == EntrySession->SessionId);

            //
            // The current and target sessions match so an attach is not needed.
            // Call KeStackAttach anyway (this has the overhead of an extra
            // dispatcher lock acquire and release) so that callers can always
            // use MmDetachSession to detach.  This is a very infrequent path so
            // the extra lock acquire and release is not significant.
            //
            // Note that by resetting EntryProcess below, an attach will not
            // actually occur.
            //

            EntryProcess = CurrentProcess;
        }
        else {
            ASSERT (CurrentSession->SessionId != EntrySession->SessionId);
        }
    }

    KeStackAttachProcess (&EntryProcess->Pcb, ApcState);

    return STATUS_SUCCESS;
}

NTSTATUS
MmDetachSession (
    IN PVOID OpaqueSession,
    IN PRKAPC_STATE ApcState
    )
/*++

Routine Description:

    This function detaches the calling thread from the referenced session
    previously attached to via MmAttachSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

    ApcState - Supplies APC state storage information for the detach.

Return Value:

    NTSTATUS.  If successful then we are detached on return.  The caller is
               responsible for eventually calling MmQuitNextSession on return.

--*/

{
    KIRQL OldIrql;
    PEPROCESS EntryProcess;
    PMM_SESSION_SPACE EntrySession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

    ASSERT (EntrySession != NULL);

    LOCK_EXPANSION (OldIrql);

    ASSERT (EntrySession->AttachCount >= 1);

    EntrySession->AttachCount -= 1;

    if ((EntrySession->u.Flags.DeletePending == 0) ||
        (EntrySession->AttachCount != 0)) {

        EntrySession = NULL;
    }

    UNLOCK_EXPANSION (OldIrql);

    KeUnstackDetachProcess (ApcState);

    if (EntrySession != NULL) {
        KeSetEvent (&EntrySession->AttachEvent, 0, FALSE);
    }

    return STATUS_SUCCESS;
}

PVOID
MmGetSessionById (
    IN ULONG SessionId
    )

/*++

Routine Description:

    This function allows callers to obtain a reference to a specific session.
    The caller can then MmAttachSession, MmDetachSession & MmQuitNextSession
    to complete the proper sequence so reference counting and address context
    operate properly.

Arguments:

    SessionId - Supplies the session ID of the desired session.

Return Value:

    An opaque session token or NULL if the session cannot be found.

Environment:

    Kernel mode, the caller must guarantee the session cannot exit or the ID
    becomes meaningless as it can be reused.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueSession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueSession = NULL;

    LOCK_EXPANSION (OldIrql);

    NextEntry = MiSessionWsList.Flink;

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if (Session->SessionId == SessionId) {

            if ((Session->u.Flags.DeletePending != 0) ||
                (NextProcessEntry == &Session->ProcessList)) {

                //
                // Session is empty or exiting so return failure to the caller.
                //

                break;
            }

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            if (Process->Vm.Flags.SessionLeader == 1) {

                //
                // Session manager is still the first process (ie: smss
                // hasn't detached yet), don't bother delivering to this
                // session this early in its lifetime.  And since smss is
                // serialized, it can't be creating another session yet so
                // just bail now as we must be at the end of the list.
                //

                break;
            }

            //
            // Reference any process in the session so that the session
            // cannot be completely deleted once the expansion lock is
            // released (note this does NOT prevent the session from being
            // cleaned).
            //

            ObReferenceObject (Process);
            OpaqueSession = (PVOID) Process;
            break;
        }
        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION (OldIrql);

    return OpaqueSession;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\sysptes.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   sysptes.c

Abstract:

    This module contains the routines which reserve and release
    system wide PTEs reserved within the non paged portion of the
    system space.  These PTEs are used for mapping I/O devices
    and mapping kernel stacks for threads.

--*/

#include "mi.h"

#ifdef MI_TB_FLUSH_TYPE_MAX
ULONG MiFlushType[MI_TB_FLUSH_TYPE_MAX];
#endif

VOID
MiFeedSysPtePool (
    IN ULONG Index
    );

ULONG
MiGetSystemPteListCount (
    IN ULONG ListSize
    );

VOID
MiPteSListExpansionWorker (
    IN PVOID Context
    );

LOGICAL
MiRecoverExtraPtes (
    ULONG NumberOfPtes
    );

#if !defined (_WIN64)
extern ULONG MiSpecialPoolExtraCount;
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSystemPtes)
#pragma alloc_text(PAGE,MiPteSListExpansionWorker)
#pragma alloc_text(MISYSPTE,MiReserveAlignedSystemPtes)
#pragma alloc_text(MISYSPTE,MiReserveSystemPtes)
#pragma alloc_text(MISYSPTE,MiFeedSysPtePool)
#pragma alloc_text(MISYSPTE,MiReleaseSystemPtes)
#pragma alloc_text(MISYSPTE,MiGetSystemPteListCount)
#endif

PVOID MiLowestSystemPteVirtualAddress;

ULONG MmTotalSystemPtes;
ULONG MmTotalFreeSystemPtes[MaximumPtePoolTypes];
PMMPTE MmSystemPtesStart[MaximumPtePoolTypes];
PMMPTE MmSystemPtesEnd[MaximumPtePoolTypes];
ULONG MmPteFailures[MaximumPtePoolTypes];
MMPTE MiAddPtesList;

PMMPTE MiPteStart;
PRTL_BITMAP MiPteStartBitmap;
PRTL_BITMAP MiPteEndBitmap;
extern KSPIN_LOCK MiPteTrackerLock;

ULONG MiSystemPteAllocationFailed;

extern PMMPTE MmSharedUserDataPte;

//
// Keep a list of the PTE ranges.
//

ULONG MiPteRangeIndex;
MI_PTE_RANGES MiPteRanges[MI_NUMBER_OF_PTE_RANGES];

#if defined (_AMD64_)

//
// AMD64 has a 4k page size.
// Small stacks consume 7 pages (including the guard page).
// Large stacks consume 19 pages (including the guard page).
//
// PTEs are binned at sizes 1, 2, 4, 7, 8, 16 and 19.
//

#define MM_SYS_PTE_TABLES_MAX 7

#define MM_PTE_TABLE_LIMIT 19

ULONG MmSysPteIndex[MM_SYS_PTE_TABLES_MAX] = {1,2,4,7,8,16,MM_PTE_TABLE_LIMIT};

UCHAR MmSysPteTables[MM_PTE_TABLE_LIMIT+1] = {0,0,1,2,2,3,3,3,4,5,5,5,5,5,5,5,5,6,6,6};

ULONG MmSysPteMinimumFree [MM_SYS_PTE_TABLES_MAX] = {100,50,30,100,20,20,20};

#else

//
// x86 has a 4k page size.
// Small stacks consume 4 pages (including the guard page).
// Large stacks consume 16 pages (including the guard page).
//
// PTEs are binned at sizes 1, 2, 4, 8, and 16.
//

#define MM_SYS_PTE_TABLES_MAX 5

#define MM_PTE_TABLE_LIMIT 16

ULONG MmSysPteIndex[MM_SYS_PTE_TABLES_MAX] = {1,2,4,8,MM_PTE_TABLE_LIMIT};

UCHAR MmSysPteTables[MM_PTE_TABLE_LIMIT+1] = {0,0,1,2,2,3,3,3,3,4,4,4,4,4,4,4,4};

ULONG MmSysPteMinimumFree [MM_SYS_PTE_TABLES_MAX] = {100,50,30,20,20};

#endif

KSPIN_LOCK MiSystemPteSListHeadLock;
SLIST_HEADER MiSystemPteSListHead;

#define MM_MIN_SYSPTE_FREE 500
#define MM_MAX_SYSPTE_FREE 3000

ULONG MmSysPteListBySizeCount [MM_SYS_PTE_TABLES_MAX];

//
// Initial sizes for PTE lists.
//

#define MM_PTE_LIST_1  400
#define MM_PTE_LIST_2  100
#define MM_PTE_LIST_4   60
#define MM_PTE_LIST_6  100
#define MM_PTE_LIST_8   50
#define MM_PTE_LIST_9   50
#define MM_PTE_LIST_16  40
#define MM_PTE_LIST_18  40
#define MM_PTE_LIST_19  40

PVOID MiSystemPteNBHead[MM_SYS_PTE_TABLES_MAX];
LONG MiSystemPteFreeCount[MM_SYS_PTE_TABLES_MAX];

ULONG MiSysPteTimeStamp[MaximumPtePoolTypes];

#if defined(_WIN64)
#define MI_MAXIMUM_SLIST_PTE_PAGES 16
#else
#define MI_MAXIMUM_SLIST_PTE_PAGES 8
#endif

typedef struct _MM_PTE_SLIST_EXPANSION_WORK_CONTEXT {
    WORK_QUEUE_ITEM WorkItem;
    LONG Active;
    ULONG SListPages;
} MM_PTE_SLIST_EXPANSION_WORK_CONTEXT, *PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT;

MM_PTE_SLIST_EXPANSION_WORK_CONTEXT MiPteSListExpand;

VOID
MiDumpSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

ULONG
MiCountFreeSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

PVOID
MiGetHighestPteConsumer (
    OUT PULONG_PTR NumberOfPtes
    );

VOID
MiCheckPteReserve (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    );

VOID
MiCheckPteRelease (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    );

extern ULONG MiCacheOverride[4];

#if DBG
extern PFN_NUMBER MiCurrentAdvancedPages;
extern PFN_NUMBER MiAdvancesGiven;
extern PFN_NUMBER MiAdvancesFreed;
#endif

PVOID
MiMapLockedPagesInUserSpace (
     IN PMDL MemoryDescriptorList,
     IN PVOID StartingVa,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID BaseVa
     );

VOID
MiUnmapLockedPagesInUserSpace (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     );

VOID
MiInsertPteTracker (
    IN PMDL MemoryDescriptorList,
    IN ULONG Flags,
    IN LOGICAL IoMapping,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute,
    IN PVOID MyCaller,
    IN PVOID MyCallersCaller
    );

VOID
MiRemovePteTracker (
    IN PMDL MemoryDescriptorList OPTIONAL,
    IN PVOID VirtualAddress,
    IN PFN_NUMBER NumberOfPtes
    );

LOGICAL
MiGetSystemPteAvailability (
    IN ULONG NumberOfPtes,
    IN MM_PAGE_PRIORITY Priority
    );

//
// Define inline functions to pack and unpack pointers in the platform
// specific non-blocking queue pointer structure.
//

typedef struct _PTE_SLIST {
    union {
        struct {
            SINGLE_LIST_ENTRY ListEntry;
        } Slist;
        NBQUEUE_BLOCK QueueBlock;
    } u1;
} PTE_SLIST, *PPTE_SLIST;

#if defined (_AMD64_)

typedef union _PTE_QUEUE_POINTER {
    struct {
        LONG64 PointerPte : 48;
        ULONG64 TimeStamp : 16;
    };

    LONG64 Data;
} PTE_QUEUE_POINTER, *PPTE_QUEUE_POINTER;

#elif defined(_X86_)

typedef union _PTE_QUEUE_POINTER {
    struct {
        LONG PointerPte;
        LONG TimeStamp;
    };

    LONG64 Data;
} PTE_QUEUE_POINTER, *PPTE_QUEUE_POINTER;

#else

#error "no target architecture"

#endif



#if defined(_AMD64_)

__inline
VOID
PackPTEValue (
    IN PPTE_QUEUE_POINTER Entry,
    IN PMMPTE PointerPte,
    IN ULONG TimeStamp
    )
{
    Entry->PointerPte = (LONG64)PointerPte;
    Entry->TimeStamp = (LONG64)TimeStamp;
    return;
}

__inline
PMMPTE
UnpackPTEPointer (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (PMMPTE)(Entry->PointerPte);
}

#define SYSPTES_FLUSH_COUNTER_MASK 0xFFFF

#elif defined(_X86_)

__inline
VOID
PackPTEValue (
    IN PPTE_QUEUE_POINTER Entry,
    IN PMMPTE PointerPte,
    IN ULONG TimeStamp
    )
{
    Entry->PointerPte = (LONG)PointerPte;
    Entry->TimeStamp = (LONG)TimeStamp;
    return;
}

__inline
PMMPTE
UnpackPTEPointer (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (PMMPTE)(Entry->PointerPte);
}

#define SYSPTES_FLUSH_COUNTER_MASK 0xFFFFFFFF

#else

#error "no target architecture"

#endif

__inline
ULONG
UnpackPTETimeStamp (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (ULONG)(Entry->TimeStamp);
}


PMMPTE
MiReserveSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function locates the specified number of unused PTEs
    within the non paged portion of system space.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to locate.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    Returns the address of the first PTE located.
    NULL if no system PTEs can be located.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    ULONG i;
    PMMPTE PointerPte;
    ULONG Index;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;

    if (SystemPtePoolType == SystemPteSpace) {

        if (NumberOfPtes <= MM_PTE_TABLE_LIMIT) {
            Index = MmSysPteTables [NumberOfPtes];
            ASSERT (NumberOfPtes <= MmSysPteIndex[Index]);

            if (ExRemoveHeadNBQueue (MiSystemPteNBHead[Index], (PULONG64)&Value) == TRUE) {
                InterlockedDecrement ((PLONG)&MmSysPteListBySizeCount[Index]);

                PointerPte = UnpackPTEPointer (&Value);

                TimeStamp = UnpackPTETimeStamp (&Value);

                ASSERT (PointerPte >= MmSystemPtesStart[SystemPtePoolType]);
                ASSERT (PointerPte <= MmSystemPtesEnd[SystemPtePoolType]);

#if DBG
                PointerPte->u.List.NextEntry = 0xABCDE;
                NumberOfPtes = MmSysPteIndex[Index];
                ASSERT (NumberOfPtes != 0);
                PointerPte += NumberOfPtes;

                do {
                    PointerPte -= 1;
                    ASSERT (PointerPte->u.Hard.Valid == 0);
                    NumberOfPtes -= 1;
                } while (NumberOfPtes != 0);
#endif

                i = MmSysPteMinimumFree[Index];

                if (MmSysPteListBySizeCount[Index] < i) {
                    MiFeedSysPtePool (Index);
                }

                //
                // The last thing is to check whether the TB needs flushing.
                //

                if (MiCompareTbFlushTimeStamp (TimeStamp, SYSPTES_FLUSH_COUNTER_MASK)) {
                    MI_FLUSH_ENTIRE_TB (0xC);
                }

                if (MmTrackPtes & 0x2) {
                    MiCheckPteReserve (PointerPte, MmSysPteIndex[Index]);
                }

                return PointerPte;
            }

            //
            // Fall through and go the long way to satisfy the PTE request.
            //

            NumberOfPtes = MmSysPteIndex [Index];
        }
    }

    PointerPte = MiReserveAlignedSystemPtes (NumberOfPtes,
                                             SystemPtePoolType,
                                             0);

    if (PointerPte == NULL) {
        MiSystemPteAllocationFailed += 1;
    }

#if DBG
    if (PointerPte != NULL) {

        ASSERT (NumberOfPtes != 0);
        PointerPte += NumberOfPtes;

        do {
            PointerPte -= 1;
            ASSERT (PointerPte->u.Hard.Valid == 0);
            NumberOfPtes -= 1;
        } while (NumberOfPtes != 0);
    }
#endif

    return PointerPte;
}

VOID
MiFeedSysPtePool (
    IN ULONG Index
    )

/*++

Routine Description:

    This routine adds PTEs to the nonblocking queue lists.

Arguments:

    Index - Supplies the index for the nonblocking queue list to fill.

Return Value:

    None.

Environment:

    Kernel mode, internal to SysPtes.

--*/

{
    ULONG i;
    ULONG NumberOfPtes;
    PMMPTE PointerPte;

    if (MmTotalFreeSystemPtes[SystemPteSpace] < MM_MIN_SYSPTE_FREE) {
#if defined (_X86_)
        MiRecoverExtraPtes (PTE_PER_PAGE);
#endif
        return;
    }

    //
    // Check the shared user data PTE to ensure that we have reached Mm
    // phase 1 initialization - this means that the Ex worker package is
    // ready to go (it is initialized in phase 1 just before Mm), so that
    // MiReleaseSystemPtes can queue SLIST expansion if needed.
    //

    if (MmSharedUserDataPte == NULL) {
        return;
    }

    NumberOfPtes = MmSysPteIndex[Index];

    for (i = 0; i < 10 ; i += 1) {

        PointerPte = MiReserveAlignedSystemPtes (NumberOfPtes,
                                                 SystemPteSpace,
                                                 0);
        if (PointerPte == NULL) {
            return;
        }

        MiReleaseSystemPtes (PointerPte, NumberOfPtes, SystemPteSpace);
    }

    return;
}


PMMPTE
MiReserveAlignedSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType,
    IN ULONG Alignment
    )

/*++

Routine Description:

    This function locates the specified number of unused PTEs to locate
    within the non paged portion of system space.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to locate.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

    Alignment - Supplies the virtual address alignment for the address
                the returned PTE maps. For example, if the value is 64K,
                the returned PTE will map an address on a 64K boundary.
                An alignment of zero means to align on a page boundary.

Return Value:

    Returns the address of the first PTE located.
    NULL if no system PTEs can be located.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerFollowingPte;
    PMMPTE Previous;
    ULONG_PTR SizeInSet;
    KIRQL OldIrql;
    ULONG MaskSize;
    ULONG NumberOfRequiredPtes;
    ULONG OffsetSum;
    ULONG PtesToObtainAlignment;
    PMMPTE NextSetPointer;
    ULONG_PTR LeftInSet;
    ULONG_PTR PteOffset;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    PVOID BaseAddress;
    ULONG TimeStamp;
    ULONG j;

    MaskSize = (Alignment - 1) >> (PAGE_SHIFT - PTE_SHIFT);

    OffsetSum = (Alignment >> (PAGE_SHIFT - PTE_SHIFT));

#if defined (_X86_)
restart:
#endif

    //
    // The nonpaged PTE pool uses the invalid PTEs to define the pool
    // structure.   A global pointer points to the first free set
    // in the list, each free set contains the number free and a pointer
    // to the next free set.  The free sets are kept in an ordered list
    // such that the pointer to the next free set is always greater
    // than the address of the current free set.
    //
    // As to not limit the size of this pool, two PTEs are used
    // to define a free region.  If the region is a single PTE, the
    // prototype field within the PTE is set indicating the set
    // consists of a single PTE.
    //
    // The page frame number field is used to define the next set
    // and the number free.  The two flavors are:
    //
    //                           o          V
    //                           n          l
    //                           e          d
    //  +-----------------------+-+----------+
    //  |  next set             |0|0        0|
    //  +-----------------------+-+----------+
    //  |  number in this set   |0|0        0|
    //  +-----------------------+-+----------+
    //
    //
    //  +-----------------------+-+----------+
    //  |  next set             |1|0        0|
    //  +-----------------------+-+----------+
    //  ...
    //

    //
    // Acquire the system space lock to synchronize access.
    //

    MiLockSystemSpace (OldIrql);

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];

    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

        //
        // End of list and none found.
        //

        MiUnlockSystemSpace (OldIrql);
#if defined (_X86_)
        if ((SystemPtePoolType == SystemPteSpace) &&
            (MiRecoverExtraPtes (NumberOfPtes) == TRUE)) {

            goto restart;
        }
#endif
        MmPteFailures[SystemPtePoolType] += 1;
        return NULL;
    }

    Previous = PointerPte;

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    if (Alignment <= PAGE_SIZE) {

        //
        // Don't deal with alignment issues.
        //

        while (TRUE) {

            if (PointerPte->u.List.OneEntry) {

                if (NumberOfPtes == 1) {
                    goto ExactFit;
                }

                goto NextEntry;
            }

            PointerFollowingPte = PointerPte + 1;
            SizeInSet = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;

            if (NumberOfPtes < SizeInSet) {

                //
                // Get the PTEs from this set and reduce the size of the
                // set.  Note that the size of the current set cannot be 1.
                //

                if ((SizeInSet - NumberOfPtes) == 1) {

                    //
                    // Collapse to the single PTE format.
                    //

                    PointerPte->u.List.OneEntry = 1;
                }
                else {

                    //
                    // Get the required PTEs from the end of the set.
                    //

                    PointerFollowingPte->u.List.NextEntry = SizeInSet - NumberOfPtes;
                }

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                //
                // Release the lock and flush the TB.
                //

                MiUnlockSystemSpace (OldIrql);

                PointerPte += (SizeInSet - NumberOfPtes);
                break;
            }

            if (NumberOfPtes == SizeInSet) {

ExactFit:

                //
                // Satisfy the request with this complete set and change
                // the list to reflect the fact that this set is gone.
                //

                Previous->u.List.NextEntry = PointerPte->u.List.NextEntry;

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                //
                // Release the lock and flush the TB.
                //

                MiUnlockSystemSpace (OldIrql);
                break;
            }

NextEntry:

            //
            // Point to the next set and try again.
            //

            if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

                //
                // End of list and none found.
                //

                MiUnlockSystemSpace (OldIrql);
#if defined (_X86_)
                if ((SystemPtePoolType == SystemPteSpace) &&
                    (MiRecoverExtraPtes (NumberOfPtes) == TRUE)) {

                    goto restart;
                }
#endif
                MmPteFailures[SystemPtePoolType] += 1;
                return NULL;
            }
            Previous = PointerPte;
            PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
            ASSERT (PointerPte > Previous);
        }
    }
    else {

        //
        // Deal with the alignment issues.
        //

        while (TRUE) {

            if (PointerPte->u.List.OneEntry) {

                //
                // Initializing PointerFollowingPte is not needed for
                // correctness, but without it the compiler cannot compile
                // this code W4 to check for use of uninitialized variables.
                //

                PointerFollowingPte = NULL;
                SizeInSet = 1;
            }
            else {
                PointerFollowingPte = PointerPte + 1;
                SizeInSet = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;
            }

            PtesToObtainAlignment = (ULONG)
                (((OffsetSum - ((ULONG_PTR)PointerPte & MaskSize)) & MaskSize) >>
                    PTE_SHIFT);

            NumberOfRequiredPtes = NumberOfPtes + PtesToObtainAlignment;

            if (NumberOfRequiredPtes < SizeInSet) {

                //
                // Get the PTEs from this set and reduce the size of the
                // set.  Note that the size of the current set cannot be 1.
                //
                // This current block will be slit into 2 blocks if
                // the PointerPte does not match the alignment.
                //

                //
                // Check to see if the first PTE is on the proper
                // alignment, if so, eliminate this block.
                //

                LeftInSet = SizeInSet - NumberOfRequiredPtes;

                //
                // Set up the new set at the end of this block.
                //

                NextSetPointer = PointerPte + NumberOfRequiredPtes;
                NextSetPointer->u.List.NextEntry =
                                       PointerPte->u.List.NextEntry;

                PteOffset = (ULONG_PTR)(NextSetPointer - MmSystemPteBase);

                if (PtesToObtainAlignment == 0) {

                    Previous->u.List.NextEntry += NumberOfRequiredPtes;

                }
                else {

                    //
                    // Point to the new set at the end of the block
                    // we are giving away.
                    //

                    PointerPte->u.List.NextEntry = PteOffset;

                    //
                    // Update the size of the current set.
                    //

                    if (PtesToObtainAlignment == 1) {

                        //
                        // Collapse to the single PTE format.
                        //

                        PointerPte->u.List.OneEntry = 1;

                    }
                    else {

                        //
                        // Set the set size in the next PTE.
                        //

                        PointerFollowingPte->u.List.NextEntry =
                                                        PtesToObtainAlignment;
                    }
                }

                //
                // Set up the new set at the end of the block.
                //

                if (LeftInSet == 1) {
                    NextSetPointer->u.List.OneEntry = 1;
                }
                else {
                    NextSetPointer->u.List.OneEntry = 0;
                    NextSetPointer += 1;
                    NextSetPointer->u.List.NextEntry = LeftInSet;
                }
                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                //
                // Release the lock and flush the TB.
                //

                MiUnlockSystemSpace (OldIrql);

                PointerPte += PtesToObtainAlignment;
                break;
            }

            if (NumberOfRequiredPtes == SizeInSet) {

                //
                // Satisfy the request with this complete set and change
                // the list to reflect the fact that this set is gone.
                //

                if (PtesToObtainAlignment == 0) {

                    //
                    // This block exactly satisfies the request.
                    //

                    Previous->u.List.NextEntry =
                                            PointerPte->u.List.NextEntry;

                }
                else {

                    //
                    // A portion at the start of this block remains.
                    //

                    if (PtesToObtainAlignment == 1) {

                        //
                        // Collapse to the single PTE format.
                        //

                        PointerPte->u.List.OneEntry = 1;

                    }
                    else {
                      PointerFollowingPte->u.List.NextEntry =
                                                        PtesToObtainAlignment;

                    }
                }

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                //
                // Release the lock and flush the TB.
                //

                MiUnlockSystemSpace (OldIrql);

                PointerPte += PtesToObtainAlignment;
                break;
            }

            //
            // Point to the next set and try again.
            //

            if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

                //
                // End of list and none found.
                //

                MiUnlockSystemSpace (OldIrql);
#if defined (_X86_)
                if ((SystemPtePoolType == SystemPteSpace) &&
                    (MiRecoverExtraPtes (NumberOfPtes) == TRUE)) {

                    goto restart;
                }
#endif
                MmPteFailures[SystemPtePoolType] += 1;
                return NULL;
            }
            Previous = PointerPte;
            PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
            ASSERT (PointerPte > Previous);
        }
    }

    //
    // If needed, flush the TB.
    //
    // Capture the global timestamp into a local so the compare will use
    // the same value throughout.
    //

    TimeStamp = *(volatile PULONG) &MiSysPteTimeStamp[SystemPtePoolType];

    if (MiCompareTbFlushTimeStamp (TimeStamp, SYSPTES_FLUSH_COUNTER_MASK) == FALSE) {
        //
        // The TB has been flushed since the last release so nothing
        // needs to be done here.
        //

        NOTHING;
    }
    else if (NumberOfPtes >= MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_ENTIRE_TB (0xD);
    }
    else {

        BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);

        if (NumberOfPtes == 1) {
            MI_FLUSH_SINGLE_TB (BaseAddress, TRUE);
        }
        else {
    
            ASSERT (NumberOfPtes < MM_MAXIMUM_FLUSH_COUNT);
            Previous = PointerPte;
    
            for (j = 0; j < NumberOfPtes; j += 1) {
    
                VaFlushList[j] = BaseAddress;
    
                //
                // PTEs being freed better be invalid.
                //
    
                ASSERT (Previous->u.Hard.Valid == 0);
    
                Previous->u.Long = 0;
                BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
                Previous += 1;
            }
    
            MI_FLUSH_MULTIPLE_TB (NumberOfPtes, &VaFlushList[0], TRUE);
        }
    }

    if ((MmTrackPtes & 0x2) &&
        (SystemPtePoolType == SystemPteSpace)) {

        MiCheckPteReserve (PointerPte, NumberOfPtes);
    }

    return PointerPte;
}

VOID
MiIssueNoPtesBugcheck (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function bugchecks when no PTEs are left.

Arguments:

    SystemPtePoolType - Supplies the PTE type of the pool that is empty.

    NumberOfPtes - Supplies the number of PTEs requested that failed.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID HighConsumer;
    ULONG_PTR HighPteUse;

    if (SystemPtePoolType == SystemPteSpace) {

        HighConsumer = MiGetHighestPteConsumer (&HighPteUse);

        if (HighConsumer != NULL) {
            KeBugCheckEx (DRIVER_USED_EXCESSIVE_PTES,
                          (ULONG_PTR)HighConsumer,
                          HighPteUse,
                          MmTotalFreeSystemPtes[SystemPtePoolType],
                          MmNumberOfSystemPtes);
        }
    }

    KeBugCheckEx (NO_MORE_SYSTEM_PTES,
                  (ULONG_PTR)SystemPtePoolType,
                  NumberOfPtes,
                  MmTotalFreeSystemPtes[SystemPtePoolType],
                  MmNumberOfSystemPtes);
}

VOID
MiPteSListExpansionWorker (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine is the worker routine to add additional SLISTs for the
    system PTE nonblocking queues.

Arguments:

    Context - Supplies a pointer to the MM_PTE_SLIST_EXPANSION_WORK_CONTEXT.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    ULONG i;
    ULONG SListEntries;
    PPTE_SLIST SListChunks;
    PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT Expansion;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    Expansion = (PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT) Context;

    ASSERT (Expansion->Active == 1);

    if (Expansion->SListPages < MI_MAXIMUM_SLIST_PTE_PAGES) {

        //
        // Allocate another page of SLIST entries for the
        // nonblocking PTE queues.
        //

        SListChunks = (PPTE_SLIST) ExAllocatePoolWithTag (NonPagedPool,
                                                          PAGE_SIZE,
                                                          'PSmM');

        if (SListChunks != NULL) {

            //
            // Carve up the pages into SLIST entries (with no pool headers).
            //

            Expansion->SListPages += 1;

            SListEntries = PAGE_SIZE / sizeof (PTE_SLIST);

            for (i = 0; i < SListEntries; i += 1) {
                InterlockedPushEntrySList (&MiSystemPteSListHead,
                                           (PSLIST_ENTRY)SListChunks);
                SListChunks += 1;
            }
        }
    }

    ASSERT (Expansion->Active == 1);
    InterlockedExchange (&Expansion->Active, 0);
}

PVOID
MmMapLockedPagesSpecifyCache (
     __in PMDL MemoryDescriptorList,
     __in KPROCESSOR_MODE AccessMode,
     __in MEMORY_CACHING_TYPE CacheType,
     __in_opt PVOID RequestedAddress,
     __in ULONG BugCheckOnFailure,
     __in MM_PAGE_PRIORITY Priority
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space or the user portion of
    the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    AccessMode - Supplies an indicator of where to map the pages;
                 KernelMode indicates that the pages should be mapped in the
                 system part of the address space, UserMode indicates the
                 pages should be mapped in the user part of the address space.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

    RequestedAddress - Supplies the base user address of the view.

                       This is only treated as an address if the AccessMode
                       is UserMode.  If the initial value of this argument
                       is not NULL, then the view will be allocated starting
                       at the specified virtual address rounded down to the
                       next 64kb address boundary. If the initial value of
                       this argument is NULL, then the operating system
                       will determine where to allocate the view.

                       If the AccessMode is KernelMode, then this argument is
                       treated as a bit field of attributes.

    BugCheckOnFailure - Supplies whether to bugcheck if the mapping cannot be
                        obtained.  This flag is only checked if the MDL's
                        MDL_MAPPING_CAN_FAIL is zero, which implies that the
                        default MDL behavior is to bugcheck.  This flag then
                        provides an additional avenue to avoid the bugcheck.
                        Done this way in order to provide WDM compatibility.

    Priority - Supplies an indication as to how important it is that this
               request succeed under low available PTE conditions.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if the processor mode is USER_MODE
    and quota limits or VM limits are exceeded.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if access mode is KernelMode,
                  APC_LEVEL or below if access mode is UserMode.

--*/

{
    ULONG i;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;
    ULONG Index;
    KIRQL OldIrql;
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PMMPTE PointerPte;
    PVOID BaseVa;
    MMPTE TempPte;
    MMPTE DefaultPte;
    PVOID StartingVa;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn2;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    //
    // If this assert fires, the MiPlatformCacheAttributes array
    // initialization needs to be checked.
    //

    ASSERT (MmMaximumCacheType == 6);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    ASSERT (MemoryDescriptorList->ByteCount != 0);

    if (AccessMode == KernelMode) {

        Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);
        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               MemoryDescriptorList->ByteCount);

        LastPage = Page + NumberOfPages;

        //
        // Map the pages into the system part of the address space as
        // kernel read/write.
        //

        ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_MAPPED_TO_SYSTEM_VA |
                        MDL_SOURCE_IS_NONPAGED_POOL |
                        MDL_PARTIAL_HAS_BEEN_MAPPED)) == 0);

        ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_PAGES_LOCKED |
                        MDL_PARTIAL)) != 0);

        //
        // Make sure there are enough PTEs of the requested size.
        // Try to ensure available PTEs inline when we're rich.
        // Otherwise go the long way.
        //

        if ((Priority != HighPagePriority) &&
            ((LONG)(NumberOfPages) > (LONG)MmTotalFreeSystemPtes[SystemPteSpace] - 2048) &&
            (MiGetSystemPteAvailability ((ULONG)NumberOfPages, Priority) == FALSE) && 
            ((PsGetCurrentThread()->MemoryMaker == 0) || KeIsExecutingDpc ())) {
            return NULL;
        }

        IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

        CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

        //
        // If a noncachable mapping is requested, none of the pages in the
        // requested MDL can reside in a large page.  Otherwise we would be
        // creating an incoherent overlapping TB entry as the same physical
        // page would be mapped by 2 different TB entries with different
        // cache attributes.
        //

        if (CacheAttribute != MiCached) {

            LOCK_PFN2 (OldIrql);

            do {

#pragma prefast(suppress: 2000, "SAL 1.2 needed for accurate MDL struct annotation.")
                if (*Page == MM_EMPTY_LIST) {
                    break;
                }

                PageFrameIndex = *Page;

                if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {

                    UNLOCK_PFN2 (OldIrql);

                    MiNonCachedCollisions += 1;

                    if (((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) == 0) && (BugCheckOnFailure)) {

                        KeBugCheckEx (MEMORY_MANAGEMENT,
                                      0x1000,
                                      (ULONG_PTR)MemoryDescriptorList,
                                      (ULONG_PTR)PageFrameIndex,
                                      (ULONG_PTR)CacheAttribute);
                    }
                    return NULL;
                }

                Page += 1;
            } while (Page < LastPage);

            UNLOCK_PFN2 (OldIrql);

            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
        }
   
        PointerPte = NULL;

        if (NumberOfPages <= MM_PTE_TABLE_LIMIT) {

            Index = MmSysPteTables [NumberOfPages];
            ASSERT (NumberOfPages <= MmSysPteIndex[Index]);

            if (ExRemoveHeadNBQueue (MiSystemPteNBHead[Index], (PULONG64)&Value) == TRUE) {
                InterlockedDecrement ((PLONG)&MmSysPteListBySizeCount[Index]);

                PointerPte = UnpackPTEPointer (&Value);

                ASSERT (PointerPte >= MmSystemPtesStart[SystemPteSpace]);
                ASSERT (PointerPte <= MmSystemPtesEnd[SystemPteSpace]);

                TimeStamp = UnpackPTETimeStamp (&Value);

                ASSERT (PointerPte >= MmSystemPtesStart[SystemPteSpace]);
                ASSERT (PointerPte <= MmSystemPtesEnd[SystemPteSpace]);

#if DBG
                PointerPte->u.List.NextEntry = 0xABCDE;

                for (i = 0; i < MmSysPteIndex[Index]; i += 1) {
                    ASSERT (PointerPte->u.Hard.Valid == 0);
                    PointerPte += 1;
                }

                PointerPte -= i;
#endif

                i = MmSysPteMinimumFree[Index];

                if (MmSysPteListBySizeCount[Index] < i) {
                    MiFeedSysPtePool (Index);
                }

                //
                // The last thing is to check whether the TB needs flushing.
                //

                if (MiCompareTbFlushTimeStamp (TimeStamp, SYSPTES_FLUSH_COUNTER_MASK)) {
                    MI_FLUSH_ENTIRE_TB (0xE);
                }

                if (MmTrackPtes & 0x2) {
                    MiCheckPteReserve (PointerPte, MmSysPteIndex[Index]);
                }
            }
            else {

                //
                // Fall through and go the long way to satisfy the PTE request.
                //

                NumberOfPages = MmSysPteIndex [Index];
            }
        }

        if (PointerPte == NULL) {

            PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages,
                                              SystemPteSpace);

            if (PointerPte == NULL) {

                if (((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) == 0) &&
                    (BugCheckOnFailure)) {

                    MiIssueNoPtesBugcheck ((ULONG)NumberOfPages, SystemPteSpace);
                }

                //
                // Not enough system PTES are available.
                //

                return NULL;
            }
        }

        BaseVa = (PVOID)((PCHAR)MiGetVirtualAddressMappedByPte (PointerPte) +
                                MemoryDescriptorList->ByteOffset);

        TempPte = ValidKernelPte;

        MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE (TempPte);

        if (CacheAttribute != MiCached) {

            switch (CacheAttribute) {

                case MiNonCached:
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }

            MI_PREPARE_FOR_NONCACHED (CacheAttribute);
        }

        OldIrql = HIGH_LEVEL;
        DefaultPte = TempPte;

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }
            ASSERT (PointerPte->u.Hard.Valid == 0);
    
            if ((IoMapping == 0) || (MI_IS_PFN (*Page))) {

                Pfn2 = MI_PFN_ELEMENT (*Page);
                ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

                if (CacheAttribute == (MI_PFN_CACHE_ATTRIBUTE)Pfn2->u3.e1.CacheAttribute) {
                    TempPte.u.Hard.PageFrameNumber = *Page;
                    MI_WRITE_VALID_PTE (PointerPte, TempPte);
                }
                else {

                    TempPte = ValidKernelPte;

                    switch (Pfn2->u3.e1.CacheAttribute) {
    
                        case MiCached:
    
                            //
                            // The caller asked for a noncached or
                            // writecombined mapping, but the page is
                            // already mapped cached by someone else.
                            // Override the caller's request in order
                            // to keep the TB page attribute coherent.
                            //
    
                            MiCacheOverride[0] += 1;
                            break;
    
                        case MiNonCached:
    
                            //
                            // The caller asked for a cached or
                            // writecombined mapping, but the page is
                            // already mapped noncached by someone else.
                            // Override the caller's request in order to
                            // keep the TB page attribute coherent.
                            //

                            MiCacheOverride[1] += 1;
                            MI_DISABLE_CACHING (TempPte);
                            break;
    
                        case MiWriteCombined:
    
                            //
                            // The caller asked for a cached or noncached
                            // mapping, but the page is already mapped
                            // writecombined by someone else.  Override the
                            // caller's request in order to keep the TB page
                            // attribute coherent.
                            //

                            MiCacheOverride[2] += 1;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;
    
                        case MiNotMapped:
    
                            //
                            // This better be for a page allocated with
                            // MmAllocatePagesForMdl.  Otherwise it might be a
                            // page on the freelist which could subsequently be
                            // given out with a different attribute !
                            //
    
                            ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                                    (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
    
                            if (OldIrql == HIGH_LEVEL) {
                                LOCK_PFN2 (OldIrql);
                            }
    
                            switch (CacheAttribute) {
    
                                case MiCached:
                                    Pfn2->u3.e1.CacheAttribute = MiCached;
                                    break;
    
                                case MiNonCached:
                                    Pfn2->u3.e1.CacheAttribute = MiNonCached;
                                    MI_DISABLE_CACHING (TempPte);
                                    break;
    
                                case MiWriteCombined:
                                    Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                                    break;
    
                                default:
                                    ASSERT (FALSE);
                                    break;
                            }
                            break;
    
                        default:
                            ASSERT (FALSE);
                            break;
                    }

                    TempPte.u.Hard.PageFrameNumber = *Page;
                    MI_WRITE_VALID_PTE (PointerPte, TempPte);

                    //
                    // We had to override the requested cache type for the
                    // current page, so reset the PTE for the next page back
                    // to the original entry requested cache type.
                    //

                    TempPte = DefaultPte;
                }
            }
            else {
                TempPte.u.Hard.PageFrameNumber = *Page;
                MI_WRITE_VALID_PTE (PointerPte, TempPte);
            }

            Page += 1;
            PointerPte += 1;
        } while (Page < LastPage);
    
        if (OldIrql != HIGH_LEVEL) {
            UNLOCK_PFN2 (OldIrql);
        }

        ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
        MemoryDescriptorList->MappedSystemVa = BaseVa;

        MemoryDescriptorList->MdlFlags |= MDL_MAPPED_TO_SYSTEM_VA;

        if (MmTrackPtes & 0x1) {

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            MiInsertPteTracker (MemoryDescriptorList,
                                0,
                                IoMapping,
                                CacheAttribute,
                                CallingAddress,
                                CallersCaller);
        }

        if ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) != 0) {
            MemoryDescriptorList->MdlFlags |= MDL_PARTIAL_HAS_BEEN_MAPPED;
        }

        return BaseVa;
    }

    return MiMapLockedPagesInUserSpace (MemoryDescriptorList,
                                        StartingVa,
                                        CacheType,
                                        RequestedAddress);
}

VOID
MmUnmapLockedPages (
    __in PVOID BaseAddress,
    __in PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    an MmMapLockedPages call.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if base address is within
    system space; APC_LEVEL or below if base address is user space.

    Note that in some instances the PFN lock is held by the caller.

--*/

{
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    PVOID StartingVa;
    PPFN_NUMBER Page;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;
    ULONG Index;
    PFN_NUMBER i;
#if DBG
    PMMPFN Pfn3;
#endif

    ASSERT (MemoryDescriptorList->ByteCount != 0);

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {

        ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARENT_MAPPED_SYSTEM_VA) == 0);
        StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                        MemoryDescriptorList->ByteOffset);

        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               MemoryDescriptorList->ByteCount);

        ASSERT (NumberOfPages != 0);

        ASSERT (MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA);

        ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));

        PointerPte = MiGetPteAddress (BaseAddress);

        //
        // Check to make sure the PTE address is within bounds.
        //

        ASSERT (PointerPte >= MmSystemPtesStart[SystemPteSpace]);
        ASSERT (PointerPte <= MmSystemPtesEnd[SystemPteSpace]);

#if DBG
        i = NumberOfPages;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        while (i != 0) {
            ASSERT (PointerPte->u.Hard.Valid == 1);
            ASSERT (*Page == MI_GET_PAGE_FRAME_FROM_PTE (PointerPte));
            if (MI_IS_PFN (*Page)) {
                Pfn3 = MI_PFN_ELEMENT (*Page);
                ASSERT (Pfn3->u3.e2.ReferenceCount != 0);
            }

            Page += 1;
            PointerPte += 1;
            i -= 1;
        }
        PointerPte -= NumberOfPages;
#endif

        if (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) {
            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            Page += NumberOfPages;
            ASSERT (*Page <= MiCurrentAdvancedPages);
            NumberOfPages += *Page;
            PointerPte -= *Page;
            ASSERT (PointerPte >= MmSystemPtesStart[SystemPteSpace]);
            ASSERT (PointerPte <= MmSystemPtesEnd[SystemPteSpace]);
            BaseAddress = (PVOID)((PCHAR)BaseAddress - ((*Page) << PAGE_SHIFT));
#if DBG
            InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, 0 - *Page);
            MiAdvancesFreed += *Page;
#endif
        }

        if (MmTrackPtes != 0) {
            if (MmTrackPtes & 0x1) {
                MiRemovePteTracker (MemoryDescriptorList,
                                    BaseAddress,
                                    NumberOfPages);
            }
            if (MmTrackPtes & 0x2) {
                MiCheckPteRelease (PointerPte, (ULONG) NumberOfPages);
            }
        }

        MemoryDescriptorList->MdlFlags &= ~(MDL_MAPPED_TO_SYSTEM_VA |
                                            MDL_PARTIAL_HAS_BEEN_MAPPED |
                                            MDL_FREE_EXTRA_PTES);

        //
        // If it's a small request (most are), try to finish it inline.
        //

        if (NumberOfPages <= MM_PTE_TABLE_LIMIT) {
    
            Index = MmSysPteTables [NumberOfPages];
    
            ASSERT (NumberOfPages <= MmSysPteIndex [Index]);
    
            if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MIN_SYSPTE_FREE) {
    
                //
                // Add to the pool if the size is less than 15 + the minimum.
                //
    
                i = MmSysPteMinimumFree[Index];
                if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MAX_SYSPTE_FREE) {
    
                    //
                    // Lots of free PTEs, quadruple the limit.
                    //
    
                    i = i * 4;
                }
                i += 15;

                if (MmSysPteListBySizeCount[Index] <= i) {

                    //
                    // Zero PTEs, then encode the PTE pointer and the TB flush
                    // counter into Value.
                    //

                    MiZeroMemoryPte (PointerPte, NumberOfPages);

                    TimeStamp = KeReadTbFlushTimeStamp ();
            
                    PackPTEValue (&Value, PointerPte, TimeStamp);
            
                    if (ExInsertTailNBQueue (MiSystemPteNBHead[Index], Value.Data) == TRUE) {
                        InterlockedIncrement ((PLONG)&MmSysPteListBySizeCount[Index]);
                        return;
                    }
                }
            }
        }

        if (MmTrackPtes & 0x2) {

            //
            // This release has already been updated in the tracking bitmaps
            // so mark it so that MiReleaseSystemPtes doesn't attempt to do
            // it also.
            //

            PointerPte = (PMMPTE) ((ULONG_PTR)PointerPte | 0x1);
        }
        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
    }
    else {
        MiUnmapLockedPagesInUserSpace (BaseAddress, MemoryDescriptorList);
    }

    return;
}

VOID
MiReleaseSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function releases the specified number of PTEs
    within the non paged portion of system space.

    Note that the PTEs must be invalid and the page frame number
    must have been set to zero.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

    SystemPtePoolType - Supplies the PTE type of the pool to release PTEs to,
                        one of SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG_PTR Size;
    ULONG i;
    ULONG_PTR PteOffset;
    PMMPTE PointerPte;
    PMMPTE PointerFollowingPte;
    PMMPTE NextPte;
    KIRQL OldIrql;
    ULONG Index;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;
    ULONG ExtensionInProgress;


    if ((MmTrackPtes & 0x2) && (SystemPtePoolType == SystemPteSpace)) {

        //
        // If the low bit is set, this range was never reserved and therefore
        // should not be validated during the release.
        //

        if ((ULONG_PTR)StartingPte & 0x1) {
            StartingPte = (PMMPTE) ((ULONG_PTR)StartingPte & ~0x1);
        }
        else {
            MiCheckPteRelease (StartingPte, NumberOfPtes);
        }
    }

    //
    // Check to make sure the PTE address is within bounds.
    //

    ASSERT (NumberOfPtes != 0);
    ASSERT (StartingPte >= MmSystemPtesStart[SystemPtePoolType]);
    ASSERT (StartingPte <= MmSystemPtesEnd[SystemPtePoolType]);

    //
    // Zero PTEs.
    //

    MiZeroMemoryPte (StartingPte, NumberOfPtes);

    if ((SystemPtePoolType == SystemPteSpace) &&
        (NumberOfPtes <= MM_PTE_TABLE_LIMIT)) {

        //
        // Encode the PTE pointer and the TB flush counter into Value.
        //

        TimeStamp = KeReadTbFlushTimeStamp ();

        PackPTEValue (&Value, StartingPte, TimeStamp);

        Index = MmSysPteTables [NumberOfPtes];

        ASSERT (NumberOfPtes <= MmSysPteIndex [Index]);

        if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MIN_SYSPTE_FREE) {

            //
            // Add to the pool if the size is less than 15 + the minimum.
            //

            i = MmSysPteMinimumFree[Index];
            if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MAX_SYSPTE_FREE) {

                //
                // Lots of free PTEs, quadruple the limit.
                //

                i = i * 4;
            }
            i += 15;
            if (MmSysPteListBySizeCount[Index] <= i) {

                if (ExInsertTailNBQueue (MiSystemPteNBHead[Index], Value.Data) == TRUE) {
                    InterlockedIncrement ((PLONG)&MmSysPteListBySizeCount[Index]);
                    return;
                }

                //
                // No lookasides are left for inserting this PTE allocation
                // into the nonblocking queues.  Queue an extension to a
                // worker thread so it can be done in a deadlock-free
                // manner.
                //

                if (MiPteSListExpand.SListPages < MI_MAXIMUM_SLIST_PTE_PAGES) {

                    //
                    // If an extension is not in progress then queue one now.
                    //

                    ExtensionInProgress = InterlockedCompareExchange (&MiPteSListExpand.Active, 1, 0);

                    if (ExtensionInProgress == 0) {

                        ExInitializeWorkItem (&MiPteSListExpand.WorkItem,
                                              MiPteSListExpansionWorker,
                                              (PVOID)&MiPteSListExpand);

                        ExQueueWorkItem (&MiPteSListExpand.WorkItem, CriticalWorkQueue);
                    }

                }
            }
        }

        //
        // The insert failed - our lookaside list must be empty or we are
        // low on PTEs systemwide or we already had plenty on our list and
        // didn't try to insert.  Fall through to queue this in the long way.
        //

        NumberOfPtes = MmSysPteIndex [Index];
    }

    PteOffset = (ULONG_PTR)(StartingPte - MmSystemPteBase);

    //
    // Acquire system space spin lock to synchronize access.
    //

    MiLockSystemSpace (OldIrql);

    //
    // Since the PTEs have already been zeroed, snap the TB flush time stamp
    // now (lock-free).
    //
    // Note this can only be set after the system space spin lock is acquired.
    // This prevents a first-thread in which becomes the last thread out from
    // setting a too-old timestamp which would cause PTEs inserted by other
    // threads in the gap to not get TB-flushed on reuse.
    //
    // This lock synchronization is necessary because one timestamp is
    // being used for all the system PTEs (instead of per-chunk) because
    // the different chunks get coalesced below.
    //

    MiSysPteTimeStamp[SystemPtePoolType] = KeReadTbFlushTimeStamp ();

    MmTotalFreeSystemPtes[SystemPtePoolType] += NumberOfPtes;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];

    while (TRUE) {
        NextPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
        if (PteOffset < PointerPte->u.List.NextEntry) {

            //
            // Insert in the list at this point.  The
            // previous one should point to the new freed set and
            // the new freed set should point to the place
            // the previous set points to.
            //
            // Attempt to combine the clusters before we
            // insert.
            //
            // Locate the end of the current structure.
            //

            ASSERT (((StartingPte + NumberOfPtes) <= NextPte) ||
                    (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST));

            PointerFollowingPte = PointerPte + 1;
            if (PointerPte->u.List.OneEntry) {
                Size = 1;
            }
            else {
                Size = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;
            }
            if ((PointerPte + Size) == StartingPte) {

                //
                // We can combine the clusters.
                //

                NumberOfPtes += (ULONG)Size;
                PointerFollowingPte->u.List.NextEntry = NumberOfPtes;
                PointerPte->u.List.OneEntry = 0;

                //
                // Point the starting PTE to the beginning of
                // the new free set and try to combine with the
                // following free cluster.
                //

                StartingPte = PointerPte;

            }
            else {

                //
                // Can't combine with previous. Make this PTE the
                // start of a cluster.
                //

                //
                // Point this cluster to the next cluster.
                //

                StartingPte->u.List.NextEntry = PointerPte->u.List.NextEntry;

                //
                // Point the current cluster to this cluster.
                //

                PointerPte->u.List.NextEntry = PteOffset;

                //
                // Set the size of this cluster.
                //

                if (NumberOfPtes == 1) {
                    StartingPte->u.List.OneEntry = 1;

                }
                else {
                    StartingPte->u.List.OneEntry = 0;
                    PointerFollowingPte = StartingPte + 1;
                    PointerFollowingPte->u.List.NextEntry = NumberOfPtes;
                }
            }

            //
            // Attempt to combine the newly created cluster with
            // the following cluster.
            //

            if ((StartingPte + NumberOfPtes) == NextPte) {

                //
                // Combine with following cluster.
                //

                //
                // Set the next cluster to the value contained in the
                // cluster we are merging into this one.
                //

                StartingPte->u.List.NextEntry = NextPte->u.List.NextEntry;
                StartingPte->u.List.OneEntry = 0;
                PointerFollowingPte = StartingPte + 1;

                if (NextPte->u.List.OneEntry) {
                    Size = 1;
                }
                else {
                    NextPte += 1;
                    Size = (ULONG_PTR) NextPte->u.List.NextEntry;
                }
                PointerFollowingPte->u.List.NextEntry = NumberOfPtes + Size;
            }

#if DBG
            if (MmDebug & MM_DBG_SYS_PTES) {
                ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                         MiCountFreeSystemPtes (SystemPtePoolType));
            }
#endif
            MiUnlockSystemSpace (OldIrql);
            return;
        }

        //
        // Point to next freed cluster.
        //

        PointerPte = NextPte;
    }
}

VOID
MiReleaseSplitSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function releases the specified number of PTEs
    within the non paged portion of system space.

    Note that the PTEs must be invalid and the page frame number
    must have been set to zero.

    This portion is a split portion from a larger allocation so
    careful updating of the tracking bitmaps must be done here.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

    SystemPtePoolType - Supplies the PTE type of the pool to release PTEs to,
                        one of SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG StartBit;
    KIRQL OldIrql;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
                
    //
    // Check to make sure the PTE address is within bounds.
    //

    ASSERT (NumberOfPtes != 0);
    ASSERT (StartingPte >= MmSystemPtesStart[SystemPtePoolType]);
    ASSERT (StartingPte <= MmSystemPtesEnd[SystemPtePoolType]);

    if ((MmTrackPtes & 0x2) && (SystemPtePoolType == SystemPteSpace)) {

        ASSERT (MmTrackPtes & 0x2);

        VirtualAddress = MiGetVirtualAddressMappedByPte (StartingPte);

        StartBit = (ULONG) (StartingPte - MiPteStart);

        ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

        //
        // Verify start and size of allocation using the tracking bitmaps.
        //

        StartBitMapBuffer = MiPteStartBitmap->Buffer;
        EndBitMapBuffer = MiPteEndBitmap->Buffer;

        //
        // All the start bits better be set.
        //

        for (i = StartBit; i < StartBit + NumberOfPtes; i += 1) {
            ASSERT (MI_CHECK_BIT (StartBitMapBuffer, i) == 1);
        }

        if (StartBit != 0) {

            if (RtlCheckBit (MiPteStartBitmap, StartBit - 1)) {

                if (!RtlCheckBit (MiPteEndBitmap, StartBit - 1)) {

                    //
                    // In the middle of an allocation - update the previous
                    // so it ends here.
                    //

                    MI_SET_BIT (EndBitMapBuffer, StartBit - 1);
                }
                else {

                    //
                    // The range being freed is the start of an allocation.
                    //
                }
            }
        }

        //
        // Unconditionally set the end bit (and clear any others) in case the
        // split chunk crosses multiple allocations.
        //

        MI_SET_BIT (EndBitMapBuffer, StartBit + NumberOfPtes - 1);

        ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
    }

    MiReleaseSystemPtes (StartingPte, NumberOfPtes, SystemPteSpace);
}


VOID
MiInitializeSystemPtes (
    IN PMMPTE StartingPte,
    IN PFN_NUMBER NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This routine initializes the system PTE pool.

Arguments:

    StartingPte - Supplies the address of the first PTE to put in the pool.

    NumberOfPtes - Supplies the number of PTEs to put in the pool.

    SystemPtePoolType - Supplies the PTE type of the pool to initialize, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    none.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG TotalPtes;
    ULONG SListEntries;
    SIZE_T SListBytes;
    ULONG TotalChunks;
    PMMPTE PointerPte;
    PPTE_SLIST Chunk;
    PPTE_SLIST SListChunks;

    //
    // Set the base of the system PTE pool to this PTE.  This takes into
    // account that systems may have additional PTE pools below the PTE_BASE.
    //

    ASSERT64 (NumberOfPtes < _4gb);

    MmSystemPteBase = MI_PTE_BASE_FOR_LOWEST_KERNEL_ADDRESS;

    MmSystemPtesStart[SystemPtePoolType] = StartingPte;

    //
    // If there are no PTEs specified, then make a valid chain by indicating
    // that the list is empty.
    //

    if (NumberOfPtes == 0) {
        MmFirstFreeSystemPte[SystemPtePoolType].u.Long = 0;
        MmFirstFreeSystemPte[SystemPtePoolType].u.List.NextEntry =
                                                                MM_EMPTY_LIST;
        MmSystemPtesEnd[SystemPtePoolType] = StartingPte;
        return;
    }

    MmSystemPtesEnd[SystemPtePoolType] = StartingPte + NumberOfPtes - 1;

    //
    // Initialize the specified system PTE pool.
    //

    MiZeroMemoryPte (StartingPte, NumberOfPtes);

    //
    // The page frame field points to the next cluster.  As we only
    // have one cluster at initialization time, mark it as the last
    // cluster.
    //

    StartingPte->u.List.NextEntry = MM_EMPTY_LIST;

    MmFirstFreeSystemPte[SystemPtePoolType].u.Long = 0;
    MmFirstFreeSystemPte[SystemPtePoolType].u.List.NextEntry =
                                                StartingPte - MmSystemPteBase;

    //
    // If there is only one PTE in the pool, then mark it as a one entry
    // PTE. Otherwise, store the cluster size in the following PTE.
    //

    if (NumberOfPtes == 1) {
        StartingPte->u.List.OneEntry = TRUE;

    }
    else {
        StartingPte += 1;
        MI_WRITE_ZERO_PTE (StartingPte);
        StartingPte->u.List.NextEntry = NumberOfPtes;
    }

    //
    // Set the total number of free PTEs for the specified type.
    //

    MmTotalFreeSystemPtes[SystemPtePoolType] = (ULONG) NumberOfPtes;

    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                         MiCountFreeSystemPtes (SystemPtePoolType));

    if (SystemPtePoolType == SystemPteSpace) {

        ULONG Lists[MM_SYS_PTE_TABLES_MAX] = {
#if defined(_AMD64_)
                MM_PTE_LIST_1,
                MM_PTE_LIST_2,
                MM_PTE_LIST_4,
                MM_PTE_LIST_6,
                MM_PTE_LIST_8,
                MM_PTE_LIST_16,
                MM_PTE_LIST_19
#else
                MM_PTE_LIST_1,
                MM_PTE_LIST_2,
                MM_PTE_LIST_4,
                MM_PTE_LIST_8,
                MM_PTE_LIST_16
#endif
        };

        MiLowestSystemPteVirtualAddress = MiGetVirtualAddressMappedByPte (MmSystemPtesStart[SystemPteSpace]);

        MmTotalSystemPtes = (ULONG) NumberOfPtes;

        ASSERT (MiPteRangeIndex == 0);
        MiPteRanges[0].StartingVa = MiLowestSystemPteVirtualAddress;
        MiPteRanges[0].EndingVa = (PVOID) ((PCHAR)MiLowestSystemPteVirtualAddress + (NumberOfPtes << PAGE_SHIFT) - 1);
        MiPteRangeIndex = 1;

        TotalPtes = 0;
        TotalChunks = 0;

        MiAddPtesList.u.List.NextEntry = MM_EMPTY_LIST;

        KeInitializeSpinLock (&MiSystemPteSListHeadLock);
        InitializeSListHead (&MiSystemPteSListHead);

        for (i = 0; i < MM_SYS_PTE_TABLES_MAX ; i += 1) {
            TotalPtes += (Lists[i] * MmSysPteIndex[i]);
            TotalChunks += Lists[i];
        }

        SListBytes = TotalChunks * sizeof (PTE_SLIST);
        SListBytes = MI_ROUND_TO_SIZE (SListBytes, PAGE_SIZE);
        SListEntries = (ULONG)(SListBytes / sizeof (PTE_SLIST));

        SListChunks = (PPTE_SLIST) ExAllocatePoolWithTag (NonPagedPool,
                                                          SListBytes,
                                                          'PSmM');

        if (SListChunks == NULL) {
            MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
        }

        ASSERT (MiPteSListExpand.Active == FALSE);
        ASSERT (MiPteSListExpand.SListPages == 0);

        MiPteSListExpand.SListPages = (ULONG)(SListBytes / PAGE_SIZE);

        ASSERT (MiPteSListExpand.SListPages != 0);

        //
        // Carve up the pages into SLIST entries (with no pool headers).
        //

        Chunk = SListChunks;
        for (i = 0; i < SListEntries; i += 1) {
            InterlockedPushEntrySList (&MiSystemPteSListHead,
                                       (PSLIST_ENTRY)Chunk);
            Chunk += 1;
        }

        //
        // Now that the SLIST is populated, initialize the nonblocking heads.
        //

        for (i = 0; i < MM_SYS_PTE_TABLES_MAX ; i += 1) {
            MiSystemPteNBHead[i] = ExInitializeNBQueueHead (&MiSystemPteSListHead);

            if (MiSystemPteNBHead[i] == NULL) {
                MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
            }
        }

        if (MmTrackPtes & 0x2) {

            //
            // Allocate PTE mapping verification bitmaps.
            //

            ULONG BitmapSize;

#if defined(_WIN64)
            BitmapSize = (ULONG) MmNumberOfSystemPtes;
            MiPteStart = MmSystemPtesStart[SystemPteSpace];
#else
            MiPteStart = MiGetPteAddress (MmSystemRangeStart);
            BitmapSize = ((ULONG_PTR)PTE_TOP + 1) - (ULONG_PTR) MiPteStart;
            BitmapSize /= sizeof (MMPTE);
#endif

            MiCreateBitMap (&MiPteStartBitmap, BitmapSize, NonPagedPool);

            if (MiPteStartBitmap != NULL) {

                MiCreateBitMap (&MiPteEndBitmap, BitmapSize, NonPagedPool);

                if (MiPteEndBitmap == NULL) {
                    ExFreePool (MiPteStartBitmap);
                    MiPteStartBitmap = NULL;
                }
            }

            if ((MiPteStartBitmap != NULL) && (MiPteEndBitmap != NULL)) {
                RtlClearAllBits (MiPteStartBitmap);
                RtlClearAllBits (MiPteEndBitmap);
            }
            MmTrackPtes &= ~0x2;
        }

        //
        // Initialize the by size lists.
        //

        PointerPte = MiReserveSystemPtes (TotalPtes, SystemPteSpace);

        if (PointerPte == NULL) {
            MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
        }

        i = MM_SYS_PTE_TABLES_MAX;
        do {
            i -= 1;
            do {
                Lists[i] -= 1;
                MiReleaseSystemPtes (PointerPte,
                                     MmSysPteIndex[i],
                                     SystemPteSpace);
                PointerPte += MmSysPteIndex[i];
            } while (Lists[i] != 0);
        } while (i != 0);

        //
        // Turn this on after the multiple releases of the binned PTEs (that
        // came from a single reservation) above.
        //

        if (MiPteStartBitmap != NULL) {
            MmTrackPtes |= 0x2;
        }
    }

    return;
}

VOID
MiIncrementSystemPtes (
    IN ULONG  NumberOfPtes
    )

/*++

Routine Description:

    This routine increments the total number of PTEs.  This is done
    separately from actually adding the PTEs to the pool so that
    autoconfiguration can use the high number in advance of the PTEs
    actually getting added.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to increment the total by.

Return Value:

    None.

Environment:

    Kernel mode.  Synchronization provided by the caller.

--*/

{
    MmTotalSystemPtes += NumberOfPtes;
}
VOID
MiAddSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG  NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This routine adds newly created PTEs to the specified pool.

Arguments:

    StartingPte - Supplies the address of the first PTE to put in the pool.

    NumberOfPtes - Supplies the number of PTEs to put in the pool.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE EndingPte;

    ASSERT (SystemPtePoolType == SystemPteSpace);

    EndingPte = StartingPte + NumberOfPtes - 1;

    if (StartingPte < MmSystemPtesStart[SystemPtePoolType]) {
        MmSystemPtesStart[SystemPtePoolType] = StartingPte;
        MiLowestSystemPteVirtualAddress = MiGetVirtualAddressMappedByPte (StartingPte);
    }

    if (EndingPte > MmSystemPtesEnd[SystemPtePoolType]) {
        MmSystemPtesEnd[SystemPtePoolType] = EndingPte;
    }

    //
    // Set the low bit to signify this range was never reserved and therefore
    // should not be validated during the release.
    //

    if (MmTrackPtes & 0x2) {
        StartingPte = (PMMPTE) ((ULONG_PTR)StartingPte | 0x1);
    }

    MiReleaseSystemPtes (StartingPte, NumberOfPtes, SystemPtePoolType);
}


ULONG
MiGetSystemPteListCount (
    IN ULONG ListSize
    )

/*++

Routine Description:

    This routine returns the number of free entries of the list which
    covers the specified size.  The size must be less than or equal to the
    largest list index.

Arguments:

    ListSize - Supplies the number of PTEs needed.

Return Value:

    Number of free entries on the list which contains ListSize PTEs.

Environment:

    Kernel mode.

--*/

{
    ULONG Index;

    ASSERT (ListSize <= MM_PTE_TABLE_LIMIT);

    Index = MmSysPteTables [ListSize];

    return MmSysPteListBySizeCount[Index];
}


LOGICAL
MiGetSystemPteAvailability (
    IN ULONG NumberOfPtes,
    IN MM_PAGE_PRIORITY Priority
    )

/*++

Routine Description:

    This routine checks how many SystemPteSpace PTEs are available for the
    requested size.  If plenty are available then TRUE is returned.
    If we are reaching a low resource situation, then the request is evaluated
    based on the argument priority.

Arguments:

    NumberOfPtes - Supplies the number of PTEs needed.

    Priority - Supplies the priority of the request.

Return Value:

    TRUE if the caller should allocate the PTEs, FALSE if not.

Environment:

    Kernel mode.

--*/

{
    ULONG Index;
    ULONG FreePtes;
    ULONG FreeBinnedPtes;

    ASSERT (Priority != HighPagePriority);

    FreePtes = MmTotalFreeSystemPtes[SystemPteSpace];

    if (NumberOfPtes <= MM_PTE_TABLE_LIMIT) {
        Index = MmSysPteTables [NumberOfPtes];
        FreeBinnedPtes = MmSysPteListBySizeCount[Index];

        if (FreeBinnedPtes > MmSysPteMinimumFree[Index]) {
            return TRUE;
        }
        if (FreeBinnedPtes != 0) {
            if (Priority == NormalPagePriority) {
                if (FreeBinnedPtes > 1 || FreePtes > 512) {
                    return TRUE;
                }
#if defined (_X86_)
                if (MiRecoverExtraPtes (NumberOfPtes) == TRUE) {
                    return TRUE;
                }
#endif
                MmPteFailures[SystemPteSpace] += 1;
                return FALSE;
            }
            if (FreePtes > 2048) {
                return TRUE;
            }
#if defined (_X86_)
            if (MiRecoverExtraPtes (NumberOfPtes) == TRUE) {
                return TRUE;
            }
#endif
            MmPteFailures[SystemPteSpace] += 1;
            return FALSE;
        }
    }

    if (Priority == NormalPagePriority) {
        if ((LONG)NumberOfPtes < (LONG)FreePtes - 512) {
            return TRUE;
        }
#if defined (_X86_)
        if (MiRecoverExtraPtes (NumberOfPtes) == TRUE) {
            return TRUE;
        }
#endif
        MmPteFailures[SystemPteSpace] += 1;
        return FALSE;
    }

    if ((LONG)NumberOfPtes < (LONG)FreePtes - 2048) {
        return TRUE;
    }
#if defined (_X86_)
    if (MiRecoverExtraPtes (NumberOfPtes) == TRUE) {
        return TRUE;
    }
#endif
    MmPteFailures[SystemPteSpace] += 1;
    return FALSE;
}

VOID
MiCheckPteReserve (
    IN PMMPTE PointerPte,
    IN ULONG NumberOfPtes
    )

/*++

Routine Description:

    This function checks the reserve of the specified number of system
    space PTEs.

Arguments:

    StartingPte - Supplies the address of the first PTE to reserve.

    NumberOfPtes - Supplies the number of PTEs to reserve.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    ULONG StartBit;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
        
    ASSERT (MmTrackPtes & 0x2);

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    if (NumberOfPtes == 0) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x200,
                      (ULONG_PTR) VirtualAddress,
                      0,
                      0);
    }

    StartBit = (ULONG) (PointerPte - MiPteStart);

    i = StartBit;

    StartBitMapBuffer = MiPteStartBitmap->Buffer;

    EndBitMapBuffer = MiPteEndBitmap->Buffer;

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    for ( ; i < StartBit + NumberOfPtes; i += 1) {
        if (MI_CHECK_BIT (StartBitMapBuffer, i)) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x201,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          NumberOfPtes);
        }
    }

    RtlSetBits (MiPteStartBitmap, StartBit, NumberOfPtes);

    for (i = StartBit; i < StartBit + NumberOfPtes; i += 1) {
        if (MI_CHECK_BIT (EndBitMapBuffer, i)) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x202,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          NumberOfPtes);
        }
    }

    MI_SET_BIT (EndBitMapBuffer, i - 1);

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}

VOID
MiCheckPteRelease (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    )

/*++

Routine Description:

    This function checks the release of the specified number of system
    space PTEs.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG Index;
    ULONG StartBit;
    KIRQL OldIrql;
    ULONG CalculatedPtes;
    ULONG NumberOfPtesRoundedUp;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
    PVOID LowestVirtualAddress;
    PVOID HighestVirtualAddress;
            
    ASSERT (MmTrackPtes & 0x2);

    VirtualAddress = MiGetVirtualAddressMappedByPte (StartingPte);

    LowestVirtualAddress = MiGetVirtualAddressMappedByPte (MmSystemPtesStart[SystemPteSpace]);

    HighestVirtualAddress = MiGetVirtualAddressMappedByPte (MmSystemPtesEnd[SystemPteSpace]);

    if (NumberOfPtes == 0) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x300,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    if (StartingPte < MmSystemPtesStart[SystemPteSpace]) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x301,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    if (StartingPte > MmSystemPtesEnd[SystemPteSpace]) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x302,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    StartBit = (ULONG) (StartingPte - MiPteStart);

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    //
    // Verify start and size of allocation using the tracking bitmaps.
    //

    if (!RtlCheckBit (MiPteStartBitmap, StartBit)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x303,
                      (ULONG_PTR) VirtualAddress,
                      NumberOfPtes,
                      0);
    }

    if (StartBit != 0) {

        if (RtlCheckBit (MiPteStartBitmap, StartBit - 1)) {

            if (!RtlCheckBit (MiPteEndBitmap, StartBit - 1)) {

                //
                // In the middle of an allocation... bugcheck.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x304,
                              (ULONG_PTR) VirtualAddress,
                              NumberOfPtes,
                              0);
            }
        }
    }

    //
    // Find the last allocated PTE to calculate the correct size.
    //

    EndBitMapBuffer = MiPteEndBitmap->Buffer;

    i = StartBit;
    while (!MI_CHECK_BIT (EndBitMapBuffer, i)) {
        i += 1;
    }

    CalculatedPtes = i - StartBit + 1;
    NumberOfPtesRoundedUp = NumberOfPtes;

    if (CalculatedPtes <= MM_PTE_TABLE_LIMIT) {
        Index = MmSysPteTables [NumberOfPtes];
        NumberOfPtesRoundedUp = MmSysPteIndex [Index];
    }

    if (CalculatedPtes != NumberOfPtesRoundedUp) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x305,
                      (ULONG_PTR) VirtualAddress,
                      NumberOfPtes,
                      CalculatedPtes);
    }

    StartBitMapBuffer = MiPteStartBitmap->Buffer;

    for (i = StartBit; i < StartBit + CalculatedPtes; i += 1) {
        if (MI_CHECK_BIT (StartBitMapBuffer, i) == 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x306,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          CalculatedPtes);
        }
    }

    RtlClearBits (MiPteStartBitmap, StartBit, CalculatedPtes);

    MI_CLEAR_BIT (EndBitMapBuffer, i - 1);

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}



#if DBG

VOID
MiDumpSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )
{
    PMMPTE PointerPte;
    PMMPTE PointerNextPte;
    ULONG_PTR ClusterSize;
    PMMPTE EndOfCluster;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];
    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        return;
    }

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    for (;;) {
        if (PointerPte->u.List.OneEntry) {
            ClusterSize = 1;
        }
        else {
            PointerNextPte = PointerPte + 1;
            ClusterSize = (ULONG_PTR) PointerNextPte->u.List.NextEntry;
        }

        EndOfCluster = PointerPte + (ClusterSize - 1);

        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
            "System Pte at %p for %p entries (%p)\n",
                PointerPte, ClusterSize, EndOfCluster);

        if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
            break;
        }

        PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
    }
    return;
}

ULONG
MiCountFreeSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )
{
    PMMPTE PointerPte;
    PMMPTE PointerNextPte;
    ULONG_PTR ClusterSize;
    ULONG_PTR FreeCount;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];
    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        return 0;
    }

    FreeCount = 0;

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    for (;;) {
        if (PointerPte->u.List.OneEntry) {
            ClusterSize = 1;

        }
        else {
            PointerNextPte = PointerPte + 1;
            ClusterSize = (ULONG_PTR) PointerNextPte->u.List.NextEntry;
        }

        FreeCount += ClusterSize;
        if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
            break;
        }

        PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
    }

    return (ULONG)FreeCount;
}

#endif

VOID
MiAddExtraSystemPteRanges (
    IN PMMPTE PointerPte,
    IN ULONG NumberOfPtes
    )
{
    PVOID StartingVa;
    KIRQL OldIrql;
    ULONG_PTR PteOffset;
    PMMPTE NextPte;
    PMMPTE ThisPte;

    ASSERT (NumberOfPtes != 0);
    ASSERT (MmSystemPteBase != NULL);

    //
    // Since the next PTE is used to hold the count, don't bother with any
    // single PTE additions (there shouldn't be any of those anyway).
    //

    if (NumberOfPtes == 1) {
        return;
    }

    PteOffset = (ULONG_PTR)(PointerPte - MmSystemPteBase);

    (PointerPte + 1)->u.List.NextEntry = NumberOfPtes;

    ThisPte = &MiAddPtesList;

    StartingVa = MiGetVirtualAddressMappedByPte (PointerPte);

    //
    // Insert the entry keeping the list sorted from small allocations to large.
    // This provides a crude way to keep the larger allocations contiguous for
    // a longer period of time.
    //

    MiLockSystemSpace (OldIrql);

    if (MiPteRangeIndex == MI_NUMBER_OF_PTE_RANGES) {
        ASSERT (FALSE);
        MiUnlockSystemSpace (OldIrql);
        return;
    }

    MiPteRanges[MiPteRangeIndex].StartingVa = StartingVa;
    MiPteRanges[MiPteRangeIndex].EndingVa = (PVOID) ((PCHAR)StartingVa + (NumberOfPtes << PAGE_SHIFT) - 1);
    MiPteRangeIndex += 1;

    while (ThisPte->u.List.NextEntry != MM_EMPTY_PTE_LIST) {
        NextPte = MmSystemPteBase + ThisPte->u.List.NextEntry;
        if (NumberOfPtes < (NextPte + 1)->u.List.NextEntry) {
            break;
        }
        ThisPte = NextPte;
    }

    PointerPte->u.List.NextEntry = ThisPte->u.List.NextEntry;
    ThisPte->u.List.NextEntry = PteOffset;

    MiUnlockSystemSpace (OldIrql);

    return;
}


LOGICAL
MiRecoverExtraPtes (
    IN ULONG NumberOfPtes
    )

/*++

Routine Description:

    This routine is called to recover extra PTEs for the system PTE pool.
    These are not just added in earlier in Phase 0 because the system PTE
    allocator uses the low addresses first which would fragment these
    bigger ranges.

Arguments:

    None.

Return Value:

    TRUE if any PTEs were added, FALSE if not.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    PMMPTE NextPte;
    PMMPTE ThisPte;

    ThisPte = &MiAddPtesList;

    //
    // Quickly do an unsynchronized check so we avoid the spinlock if we
    // have no chance.
    //

    if (ThisPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        return FALSE;
    }

    //
    // Search for an entry large enough to satisfy the request, if one
    // cannot be found, then remove them all in hopes that adjacent allocations
    // can be coalesced by our caller to satisfy the request.
    //

    MiLockSystemSpace (OldIrql);

    if (ThisPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        MiUnlockSystemSpace (OldIrql);
        return FALSE;
    }

    do {
        NextPte = MmSystemPteBase + ThisPte->u.List.NextEntry;

        //
        // Just free the whole entry as splitting it into the exact amount
        // can trigger race conditions where another thread allocates a PTE
        // from the front before our caller retries (causing our caller to
        // fail the API).
        //

        if (NumberOfPtes <= (NextPte + 1)->u.List.NextEntry) {

            //
            // The request can only be satisfied by using the entire entry so
            // delink it now.
            //

            ThisPte->u.List.NextEntry = NextPte->u.List.NextEntry;

            MiUnlockSystemSpace (OldIrql);

            NumberOfPtes = (ULONG)((NextPte + 1)->u.List.NextEntry);

            MiAddSystemPtes (NextPte, NumberOfPtes, SystemPteSpace);

            return TRUE;
        }

        ThisPte = NextPte;

    } while (ThisPte->u.List.NextEntry != MM_EMPTY_PTE_LIST);

    MiUnlockSystemSpace (OldIrql);

#if defined(_X86_)
    return MiRecoverSpecialPtes (NumberOfPtes);
#else
    return FALSE;
#endif
}

ULONG
MmGetNumberOfFreeSystemPtes (
    VOID
    )
/*++

Routine Description:

    This routine is count the number of system PTEs left.

Arguments:

    None.

Return Value:

    TRUE if any PTEs were added, FALSE if not.

Environment:

    Kernel mode.

--*/

{
#if !defined (_X86_)
    return MmTotalFreeSystemPtes[0];
#else
    ULONG NumberOfPtes;
    KIRQL OldIrql;
    PMMPTE NextPte;
    PMMPTE ThisPte;

    NumberOfPtes = MmTotalFreeSystemPtes[0];

    ThisPte = &MiAddPtesList;

    //
    // Quickly do an unsynchronized check so we avoid the spinlock if we
    // have no chance.
    //

    if (ThisPte->u.List.NextEntry != MM_EMPTY_PTE_LIST) {

        MiLockSystemSpace (OldIrql);

        while (ThisPte->u.List.NextEntry != MM_EMPTY_PTE_LIST) {

            NextPte = MmSystemPteBase + ThisPte->u.List.NextEntry;

            NumberOfPtes += (ULONG) ((NextPte + 1)->u.List.NextEntry);

            ThisPte = NextPte;
        }

        MiUnlockSystemSpace (OldIrql);
    }

    return NumberOfPtes + MiSpecialPoolExtraCount;
#endif
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\sysload.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   sysload.c

Abstract:

    This module contains the code to load DLLs into the system portion of
    the address space and calls the DLL at its initialization entry point.

--*/

#include "mi.h"

KMUTANT MmSystemLoadLock;

LONG MmTotalSystemDriverPages;

ULONG MmDriverCommit;

LONG MiFirstDriverLoadEver = 0;

//
// This key is set to TRUE to make more memory below 16mb available for drivers.
// It can be cleared via the registry.
//

LOGICAL MmMakeLowMemory = TRUE;

//
// Enabled via the registry to identify drivers which unload without releasing
// resources or still have active timers, etc.
//

PUNLOADED_DRIVERS MmUnloadedDrivers;

ULONG MmLastUnloadedDriver;
ULONG MiTotalUnloads;
ULONG MiUnloadsSkipped;

//
// This can be set by the registry.
//

ULONG MmEnforceWriteProtection = 1;

//
// Referenced by ke\bugcheck.c.
//

PVOID ExPoolCodeStart;
PVOID ExPoolCodeEnd;
PVOID MmPoolCodeStart;
PVOID MmPoolCodeEnd;
PVOID MmPteCodeStart;
PVOID MmPteCodeEnd;

NTSTATUS
LookupEntryPoint (
    IN PVOID DllBase,
    IN PSZ NameOfEntryPoint,
    OUT PVOID *AddressOfEntryPoint
    );

PVOID
MiCacheImageSymbols (
    IN PVOID ImageBase
    );

NTSTATUS
MiResolveImageReferences (
    PVOID ImageBase,
    IN PUNICODE_STRING ImageFileDirectory,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    OUT PCHAR *MissingProcedureName,
    OUT PWSTR *MissingDriverName,
    OUT PLOAD_IMPORTS *LoadedImports
    );

NTSTATUS
MiSnapThunk (
    IN PVOID DllBase,
    IN PVOID ImageBase,
    IN PIMAGE_THUNK_DATA NameThunk,
    OUT PIMAGE_THUNK_DATA AddrThunk,
    IN PIMAGE_EXPORT_DIRECTORY ExportDirectory,
    IN ULONG ExportSize,
    IN LOGICAL SnapForwarder,
    OUT PCHAR *MissingProcedureName
    );

NTSTATUS
MiLoadImageSection (
    IN OUT PSECTION *InputSectionPointer,
    OUT PVOID *ImageBase,
    IN PUNICODE_STRING ImageFileName,
    IN ULONG LoadInSessionSpace,
    IN PKLDR_DATA_TABLE_ENTRY FoundDataTableEntry
    );

VOID
MiEnablePagingOfDriver (
    IN PVOID ImageHandle
    );

VOID
MiSetPagingOfDriver (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    );

PVOID
MiLookupImageSectionByName (
    IN PVOID Base,
    IN LOGICAL MappedAsImage,
    IN PCHAR SectionName,
    OUT PULONG SectionSize
    );

VOID
MiClearImports (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

NTSTATUS
MiBuildImportsForBootDrivers (
    VOID
    );

LONG
MiMapCacheExceptionFilter (
    OUT PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    );

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

NTSTATUS
MiDereferenceImports (
    IN PLOAD_IMPORTS ImportList
    );

LOGICAL
MiCallDllUnloadAndUnloadDll (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

PVOID
MiLocateExportName (
    IN PVOID DllBase,
    IN PCHAR FunctionName
    );

VOID
MiRememberUnloadedDriver (
    IN PUNICODE_STRING DriverName,
    IN PVOID Address,
    IN ULONG Length
    );

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    );

VOID
MiLocateKernelSections (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
MiCaptureImageExceptionValues (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
MiUpdateThunks (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PVOID OldAddress,
    IN PVOID NewAddress,
    IN ULONG NumberOfBytes
    );

PVOID
MiFindExportedRoutineByName (
    IN PVOID DllBase,
    IN PANSI_STRING AnsiImageRoutineName
    );

LOGICAL
MiUseLargeDriverPage (
    IN ULONG NumberOfPtes,
    IN OUT PVOID *ImageBaseAddress,
    IN PUNICODE_STRING PrefixedImageName,
    IN ULONG Pass
    );

VOID
MiRundownHotpatchList (
    PVOID PatchHead
    );

VOID
MiSessionProcessGlobalSubsections (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

MM_PROTECTION_MASK
MiComputeDriverProtection (
    IN LOGICAL SessionDriver,
    IN ULONG SectionProtection
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmCheckSystemImage)
#pragma alloc_text(PAGE,MmLoadSystemImage)
#pragma alloc_text(PAGE,MiResolveImageReferences)
#pragma alloc_text(PAGE,MiSnapThunk)
#pragma alloc_text(PAGE,MiEnablePagingOfDriver)
#pragma alloc_text(PAGE,MmPageEntireDriver)
#pragma alloc_text(PAGE,MiDereferenceImports)
#pragma alloc_text(PAGE,MiCallDllUnloadAndUnloadDll)
#pragma alloc_text(PAGE,MiLocateExportName)
#pragma alloc_text(PAGE,MiClearImports)
#pragma alloc_text(PAGE,MmGetSystemRoutineAddress)
#pragma alloc_text(PAGE,MiFindExportedRoutineByName)
#pragma alloc_text(PAGE,MmCallDllInitialize)
#pragma alloc_text(PAGE,MmResetDriverPaging)
#pragma alloc_text(PAGE,MmUnloadSystemImage)
#pragma alloc_text(PAGE,MiLoadImageSection)
#pragma alloc_text(PAGE,MiRememberUnloadedDriver)
#pragma alloc_text(PAGE,MiUseLargeDriverPage)
#pragma alloc_text(PAGE,MiMakeEntireImageCopyOnWrite)
#pragma alloc_text(PAGE,MiWriteProtectSystemImage)
#pragma alloc_text(PAGE,MiSessionProcessGlobalSubsections)
#pragma alloc_text(PAGE,MiCaptureImageExceptionValues)
#pragma alloc_text(PAGE,MiComputeDriverProtection)
#pragma alloc_text(INIT,MiBuildImportsForBootDrivers)
#pragma alloc_text(INIT,MiReloadBootLoadedDrivers)
#pragma alloc_text(INIT,MiUpdateThunks)
#pragma alloc_text(INIT,MiInitializeLoadedModuleList)
#pragma alloc_text(INIT,MiLocateKernelSections)

#if !defined(NT_UP)
#pragma alloc_text(PAGE,MmVerifyImageIsOkForMpUse)
#endif

#endif

VOID
MiProcessLoaderEntry (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry,
    IN LOGICAL Insert
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PsLoadedModuleList
    lock to insert a new entry.

Arguments:

    DataTableEntry - Supplies the loaded module list entry to insert/remove.

    Insert - Supplies TRUE if the entry should be inserted, FALSE if the entry
             should be removed.

Return Value:

    None.

Environment:

    Kernel mode.  Normal APCs disabled (critical region held).

--*/

{
    KIRQL OldIrql;

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);
    OldIrql = KeRaiseIrqlToSynchLevel ();
    ExAcquireSpinLockAtDpcLevel (&PsLoadedModuleSpinLock);

    if (Insert == TRUE) {
        InsertTailList (&PsLoadedModuleList, &DataTableEntry->InLoadOrderLinks);

#if defined (_WIN64)

        RtlInsertInvertedFunctionTable (&PsInvertedFunctionTable,
                                        DataTableEntry->DllBase,
                                        DataTableEntry->SizeOfImage);

#endif

    }
    else {

#if defined (_WIN64)

        RtlRemoveInvertedFunctionTable (&PsInvertedFunctionTable,
                                        DataTableEntry->DllBase);

#endif

        RemoveEntryList (&DataTableEntry->InLoadOrderLinks);
    }

    ExReleaseSpinLockFromDpcLevel (&PsLoadedModuleSpinLock);
    KeLowerIrql (OldIrql);
    ExReleaseResourceLite (&PsLoadedModuleResource);
}

typedef struct _MI_LARGE_PAGE_DRIVER_ENTRY {
    LIST_ENTRY Links;
    UNICODE_STRING BaseName;
} MI_LARGE_PAGE_DRIVER_ENTRY, *PMI_LARGE_PAGE_DRIVER_ENTRY;

LIST_ENTRY MiLargePageDriverList;

ULONG MiLargePageAllDrivers;

VOID
MiInitializeDriverLargePageList (
    VOID
    )

/*++

Routine Description:

    Parse the registry settings and set up the list of driver names that we'll
    try to load in large pages.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 Initialization.

    Nonpaged pool exists but not paged pool.

    The PsLoadedModuleList has not been set up yet AND the boot drivers
    have NOT been relocated to their final resting places.

--*/
{
    PWCHAR Start;
    PWCHAR End;
    PWCHAR Walk;
    ULONG NameLength;
    PMI_LARGE_PAGE_DRIVER_ENTRY Entry;

    InitializeListHead (&MiLargePageDriverList);

    if (MmLargePageDriverBufferLength == (ULONG)-1) {
        return;
    }

    Start = MmLargePageDriverBuffer;
    End = MmLargePageDriverBuffer + (MmLargePageDriverBufferLength - sizeof(WCHAR)) / sizeof(WCHAR);

    while (Start < End) {
        if (UNICODE_WHITESPACE(*Start)) {
            Start += 1;
            continue;
        }

        if (*Start == (WCHAR)'*') {
            MiLargePageAllDrivers = 1;
            break;
        }

        for (Walk = Start; Walk < End; Walk += 1) {
            if (UNICODE_WHITESPACE(*Walk)) {
                break;
            }
        }

        //
        // Got a string - add it to our list.
        //

        NameLength = (ULONG)(Walk - Start) * sizeof (WCHAR);


        Entry = ExAllocatePoolWithTag (NonPagedPool,
                                       sizeof (MI_LARGE_PAGE_DRIVER_ENTRY),
                                       'pLmM');

        if (Entry == NULL) {
            break;
        }

        Entry->BaseName.Buffer = Start;
        Entry->BaseName.Length = (USHORT) NameLength;
        Entry->BaseName.MaximumLength = (USHORT) NameLength;

        InsertTailList (&MiLargePageDriverList, &Entry->Links);

        Start = Walk + 1;
    }

    return;
}

LOGICAL
MiUseLargeDriverPage (
    IN ULONG NumberOfPtes,
    IN OUT PVOID *ImageBaseAddress,
    IN PUNICODE_STRING BaseImageName,
    IN ULONG Pass
    )

/*++

Routine Description:

    This routine checks whether the specified image should be loaded into
    a large page address space, and if so, tries to load it.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to map for the image.

    ImageBaseAddress - Supplies the current address the image header is at,
                       and returns the (new) address for the image header.

    BaseImageName - Supplies the base path name of the image to load.

    Pass - Supplies 0 when called from Phase for the boot drivers, 1 otherwise.

Return Value:

    TRUE if large pages were used, FALSE if not.

--*/

{
    PFN_NUMBER PagesRequired;
    PFN_NUMBER ResidentPages;
    PLIST_ENTRY NextEntry;
    PVOID SmallVa;
    PVOID LargeVa;
    PVOID LargeBaseVa;
    LOGICAL UseLargePages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MMPTE PteContents;
    PMMPTE SmallPte;
    PMMPTE LastSmallPte;
    PMI_LARGE_PAGE_DRIVER_ENTRY LargePageDriverEntry;
#ifdef _X86_
    ULONG ProcessorFeatures;
#endif

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
    ASSERT (*ImageBaseAddress >= MmSystemRangeStart);

#ifdef _X86_
    if ((KeFeatureBits & KF_LARGE_PAGE) == 0) {
        return FALSE;
    }

    //
    // Capture cr4 to see if large page support has been enabled in the chip
    // yet (late in Phase 1).  Large page PDEs cannot be used until then.
    //
    // mov     eax, cr4
    //

    _asm {
        _emit 00fh
        _emit 020h
        _emit 0e0h
        mov     ProcessorFeatures, eax
    }

    if ((ProcessorFeatures & CR4_PSE) == 0) {
        return FALSE;
    }
#endif

    //
    // Check the number of free system PTEs left to prevent a runaway registry
    // key from exhausting all the system PTEs.
    //

    if (MmTotalFreeSystemPtes[SystemPteSpace] < 16 * (MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT)) {

        return FALSE;
    }

    if (MiLargePageAllDrivers == 0) {

        UseLargePages = FALSE;

        //
        // Check to see if this name exists in the large page image list.
        //

        NextEntry = MiLargePageDriverList.Flink;

        while (NextEntry != &MiLargePageDriverList) {

            LargePageDriverEntry = CONTAINING_RECORD (NextEntry,
                                                      MI_LARGE_PAGE_DRIVER_ENTRY,
                                                      Links);

            if (RtlEqualUnicodeString (BaseImageName,
                                       &LargePageDriverEntry->BaseName,
                                       TRUE)) {

                UseLargePages = TRUE;
                break;
            }

            NextEntry = NextEntry->Flink;
        }

        if (UseLargePages == FALSE) {
            return FALSE;
        }
    }

    //
    // First try to get physically contiguous memory for this driver.
    // Note we must allocate the entire large page here even though we will
    // almost always only use a portion of it.  This is to ensure no other
    // frames in it can be mapped with a different cache attribute.  After
    // updating the cache attribute lists, we'll immediately free the excess.
    // Note that the driver's INIT section will be subsequently freed
    // and clearly the cache attribute lists must be correct to support that
    // as well.
    //
    // Don't take memory below 16MB as we want to leave that available for
    // ISA drivers supporting older hardware that may require it.
    //

    NumberOfPages = (PFN_NUMBER) MI_ROUND_TO_SIZE (
                        NumberOfPtes,
                        MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT);

    PageFrameIndex = MiFindContiguousPages ((16 * 1024 * 1024) >> PAGE_SHIFT,
                                            MmHighestPossiblePhysicalPage,
                                            MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT,
                                            NumberOfPages,
                                            MmCached);

    //
    // If a contiguous range is not available then large pages cannot be used
    // for this driver at this time.
    //

    if (PageFrameIndex == 0) {
        return FALSE;
    }

    //
    // Add the contiguous range to the must-be-cached list so that the excess
    // memory (and INIT section) can be safely freed to the page lists.
    //

    if (MiAddCachedRange (PageFrameIndex, PageFrameIndex + NumberOfPages - 1) == FALSE) {
        MiFreeContiguousPages (PageFrameIndex, NumberOfPages);
        return FALSE;
    }

    //
    // Try to get large virtual address space for this driver.
    //

    LargeVa = MiMapWithLargePages (PageFrameIndex,
                                   NumberOfPages,
                                   MM_EXECUTE_READWRITE,
                                   MmCached);

    if (LargeVa == NULL) {
        MiRemoveCachedRange (PageFrameIndex, PageFrameIndex + NumberOfPages - 1);
        MiFreeContiguousPages (PageFrameIndex, NumberOfPages);
        return FALSE;
    }

    LargeBaseVa = LargeVa;

    //
    // Copy the driver a page at a time as in rare cases, it may have holes.
    //

    SmallPte = MiGetPteAddress (*ImageBaseAddress);
    LastSmallPte = SmallPte + NumberOfPtes;

    SmallVa = MiGetVirtualAddressMappedByPte (SmallPte);

    while (SmallPte < LastSmallPte) {

        PteContents = *SmallPte;

        if (PteContents.u.Hard.Valid == 1) {
            RtlCopyMemory (LargeVa, SmallVa, PAGE_SIZE);
        }
        else {

            //
            // Retain this page in the large page mapping to simplify unload -
            // ie: it can always free a single contiguous range.
            //
        }

        SmallPte += 1;

        LargeVa = (PVOID) ((PCHAR)LargeVa + PAGE_SIZE);
        SmallVa = (PVOID) ((PCHAR)SmallVa + PAGE_SIZE);
    }

    //
    // Inform our caller of the new (large page) address so the loader data
    // table entry gets created with it & fixups done accordingly, etc.
    //

    *ImageBaseAddress = LargeBaseVa;

    if (Pass != 0) {

        //
        // The system is fully booted so get rid of the original mapping now.
        // Otherwise, we're in Phase 0, so the caller gets rid of the original
        // mapping.
        //

        SmallPte -= NumberOfPtes;

        //
        // No explicit TB flush is done here, instead the system PTE management
        // code will handle this in a lazy fashion.
        //

        PagesRequired = MiDeleteSystemPageableVm (SmallPte,
                                                 NumberOfPtes,
                                                 0,
                                                 &ResidentPages);

        //
        // Non boot-loaded drivers have system PTEs and commit charged.
        //

        MiReleaseSystemPtes (SmallPte, (ULONG)NumberOfPtes, SystemPteSpace);

        InterlockedExchangeAdd (&MmTotalSystemDriverPages,
                                0 - (ULONG)(PagesRequired - ResidentPages));

        MI_INCREMENT_RESIDENT_AVAILABLE (ResidentPages,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1);

        MiReturnCommitment (PagesRequired);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1, PagesRequired);
    }

    //
    // Note the unused trailing portion (and their resident available charge)
    // of the large page mapping are kept as they would have to be mapped
    // cached to avoid TB attribute conflicts, and therefore cannot be placed
    // on the general purpose freelists.
    //

    return TRUE;
}

        
VOID
MiCaptureImageExceptionValues (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function stores the exception table information from the image in the
    loader data table entry.

Arguments:

    DataTableEntry - Supplies the kernel's data table entry.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below, arbitrary process context.

--*/

{
    PVOID CurrentBase;
    PIMAGE_NT_HEADERS NtHeader;

    CurrentBase = (PVOID) DataTableEntry->DllBase;

    NtHeader = RtlImageNtHeader (CurrentBase);

#if defined(_X86_)
    if (NtHeader->OptionalHeader.DllCharacteristics & IMAGE_DLLCHARACTERISTICS_NO_SEH) {
        DataTableEntry->ExceptionTable = (PCHAR)LongToPtr(-1);
        DataTableEntry->ExceptionTableSize = (ULONG)-1;
    } else {
        PIMAGE_LOAD_CONFIG_DIRECTORY32 LoadConfig;
        ULONG LoadConfigSize;
        if (IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG < NtHeader->OptionalHeader.NumberOfRvaAndSizes) {
            LoadConfig = (PIMAGE_LOAD_CONFIG_DIRECTORY32)((PCHAR)CurrentBase +
                    NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG].VirtualAddress);
            LoadConfigSize = NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG].Size;
            if (LoadConfig && 
                LoadConfigSize &&
                LoadConfig->Size >= RTL_SIZEOF_THROUGH_FIELD(IMAGE_LOAD_CONFIG_DIRECTORY32, SEHandlerCount) &&
                LoadConfig->SEHandlerTable &&
                LoadConfig->SEHandlerCount
                )
            {
                DataTableEntry->ExceptionTable = (PVOID)LoadConfig->SEHandlerTable;
                DataTableEntry->ExceptionTableSize = LoadConfig->SEHandlerCount;
            } else {
                DataTableEntry->ExceptionTable = 0;
                DataTableEntry->ExceptionTableSize = 0;
            }
        }
    }
#else
    if (IMAGE_DIRECTORY_ENTRY_EXCEPTION < NtHeader->OptionalHeader.NumberOfRvaAndSizes) {
        DataTableEntry->ExceptionTable = (PCHAR)CurrentBase + 
                NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_EXCEPTION].VirtualAddress;
        DataTableEntry->ExceptionTableSize = 
                NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_EXCEPTION].Size;
    }
#endif
}


NTSTATUS
MmLoadSystemImage (
    IN PUNICODE_STRING ImageFileName,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    IN PUNICODE_STRING LoadedBaseName OPTIONAL,
    IN ULONG LoadFlags,
    OUT PVOID *ImageHandle,
    OUT PVOID *ImageBaseAddress
    )

/*++

Routine Description:

    This routine reads the image pages from the specified section into
    the system and returns the address of the DLL's header.

    At successful completion, the Section is referenced so it remains
    until the system image is unloaded.

Arguments:

    ImageFileName - Supplies the full path name (including the image name)
                    of the image to load.

    NamePrefix - If present, supplies the prefix to use with the image name on
                 load operations.  This is used to load the same image multiple
                 times, by using different prefixes.

    LoadedBaseName - If present, supplies the base name to use on the
                     loaded image instead of the base name found on the
                     image name.

    LoadFlags - Supplies a combination of bit flags as follows:

        MM_LOAD_IMAGE_IN_SESSION :
                       - Supplies whether to load this image in session space.
                         Each session gets a different copy of this driver with
                         pages shared as much as possible via copy on write.

        MM_LOAD_IMAGE_AND_LOCKDOWN :
                       - Supplies TRUE if the image pages should be made
                         non-pageable.

    ImageHandle - Returns an opaque pointer to the referenced section object
                  of the image that was loaded.

    ImageBaseAddress - Returns the image base within the system.

Return Value:

    Status of the load operation.

Environment:

    Kernel mode, APC_LEVEL or below, arbitrary process context.

--*/

{
    LONG OldValue;
    ULONG i;
    ULONG DebugInfoSize;
    PIMAGE_DATA_DIRECTORY DataDirectory;
    PIMAGE_DEBUG_DIRECTORY DebugDir;
    PNON_PAGED_DEBUG_INFO ssHeader;
    PMMPTE PointerPte;
    SIZE_T DataTableEntrySize;
    PWSTR BaseDllNameBuffer;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    KLDR_DATA_TABLE_ENTRY TempDataTableEntry;
    PKLDR_DATA_TABLE_ENTRY FoundDataTableEntry;
    NTSTATUS Status;
    PSECTION SectionPointer;
    PIMAGE_NT_HEADERS NtHeaders;
    UNICODE_STRING PrefixedImageName;
    UNICODE_STRING BaseName;
    UNICODE_STRING BaseDirectory;
    OBJECT_ATTRIBUTES ObjectAttributes;
    HANDLE FileHandle;
    HANDLE SectionHandle;
    IO_STATUS_BLOCK IoStatus;
    PCHAR NameBuffer;
    PLIST_ENTRY NextEntry;
    ULONG NumberOfPtes;
    PCHAR MissingProcedureName;
    PWSTR MissingDriverName;
    PWSTR PrintableMissingDriverName;
    PLOAD_IMPORTS LoadedImports;
    LOGICAL AlreadyOpen;
    LOGICAL IssueUnloadOnFailure;
    LOGICAL LoadLockOwned;
    ULONG SectionAccess;
    PKTHREAD CurrentThread;

    PAGED_CODE();

    if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

        ASSERT (NamePrefix == NULL);
        ASSERT (LoadedBaseName == NULL);

        if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
            return STATUS_NO_MEMORY;
        }
    }

    LoadLockOwned = FALSE;
    LoadedImports = (PLOAD_IMPORTS) NO_IMPORTS_USED;
    SectionPointer = NULL;
    FileHandle = (HANDLE)0;
    MissingProcedureName = NULL;
    MissingDriverName = NULL;
    IssueUnloadOnFailure = FALSE;
    FoundDataTableEntry = NULL;

    NameBuffer = ExAllocatePoolWithTag (NonPagedPool,
                                        MAXIMUM_FILENAME_LENGTH,
                                        'nLmM');

    if (NameBuffer == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    SATISFY_OVERZEALOUS_COMPILER (NumberOfPtes = (ULONG)-1);
    DataTableEntry = NULL;

    //
    // Get name roots.
    //

    if (ImageFileName->Buffer[0] == OBJ_NAME_PATH_SEPARATOR) {
        PWCHAR p;
        ULONG l;

        p = &ImageFileName->Buffer[ImageFileName->Length>>1];
        while (*(p-1) != OBJ_NAME_PATH_SEPARATOR) {
            p--;
        }
        l = (ULONG)(&ImageFileName->Buffer[ImageFileName->Length>>1] - p);
        l *= sizeof(WCHAR);
        BaseName.Length = (USHORT)l;
        BaseName.Buffer = p;
    }
    else {
        BaseName.Length = ImageFileName->Length;
        BaseName.Buffer = ImageFileName->Buffer;
    }

    BaseName.MaximumLength = BaseName.Length;
    BaseDirectory = *ImageFileName;
    BaseDirectory.Length = (USHORT)(BaseDirectory.Length - BaseName.Length);
    BaseDirectory.MaximumLength = BaseDirectory.Length;
    PrefixedImageName = *ImageFileName;

    //
    // If there's a name prefix, add it to the PrefixedImageName.
    //

    if (NamePrefix) {
        PrefixedImageName.MaximumLength = (USHORT)(BaseDirectory.Length + NamePrefix->Length + BaseName.Length);

        PrefixedImageName.Buffer = ExAllocatePoolWithTag (
                                    NonPagedPool,
                                    PrefixedImageName.MaximumLength,
                                    'dLmM');

        if (!PrefixedImageName.Buffer) {
            ExFreePool (NameBuffer);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        PrefixedImageName.Length = 0;
        RtlAppendUnicodeStringToString(&PrefixedImageName, &BaseDirectory);
        RtlAppendUnicodeStringToString(&PrefixedImageName, NamePrefix);
        RtlAppendUnicodeStringToString(&PrefixedImageName, &BaseName);

        //
        // Alter the basename to match.
        //

        BaseName.Buffer = PrefixedImageName.Buffer + BaseDirectory.Length / sizeof(WCHAR);
        BaseName.Length = (USHORT)(BaseName.Length + NamePrefix->Length);
        BaseName.MaximumLength = (USHORT)(BaseName.MaximumLength + NamePrefix->Length);
    }

    //
    // If there's a loaded base name, use it instead of the base name.
    //

    if (LoadedBaseName) {
        BaseName = *LoadedBaseName;
    }

#if DBG
    if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
        DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
            "MM:SYSLDR Loading %wZ (%wZ) %s\n",
            &PrefixedImageName,
            &BaseName,
            (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) ? "in session space" : " ");
    }
#endif

    AlreadyOpen = FALSE;

ReCheckLoaderList:

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    ASSERT (LoadLockOwned == FALSE);
    LoadLockOwned = TRUE;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    //
    // Check to see if this name already exists in the loader database.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        if (RtlEqualUnicodeString (&PrefixedImageName,
                                   &DataTableEntry->FullDllName,
                                   TRUE)) {
            break;
        }

        NextEntry = NextEntry->Flink;
    }

    if (NextEntry != &PsLoadedModuleList) {

        //
        // Found a match in the loaded module list.  See if it's acceptable.
        //
        // If this thread already loaded the image below and upon rechecking
        // finds some other thread did so also, then get rid of our object
        // now and use the other thread's inserted entry instead.
        //

        if (SectionPointer != NULL) {
            ObDereferenceObject (SectionPointer);
            SectionPointer = NULL;
        }

        if ((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) {

            if (MI_IS_SESSION_ADDRESS (DataTableEntry->DllBase) == TRUE) {

                //
                // The caller is trying to load a driver in systemwide space
                // that has already been loaded in session space.  This is
                // not allowed.
                //

                Status = STATUS_CONFLICTING_ADDRESSES;
            }
            else {
                *ImageHandle = DataTableEntry;
                *ImageBaseAddress = DataTableEntry->DllBase;
                Status = STATUS_IMAGE_ALREADY_LOADED;
            }
            goto return2;
        }

        if (MI_IS_SESSION_ADDRESS (DataTableEntry->DllBase) == FALSE) {

            //
            // The caller is trying to load a driver in session space
            // that has already been loaded in system space.  This is
            // not allowed.
            //

            Status = STATUS_CONFLICTING_ADDRESSES;
            goto return2;
        }

        AlreadyOpen = TRUE;

        //
        // This image has already been loaded systemwide.  If it's
        // already been loaded in this session space as well, just
        // bump the reference count using the already allocated
        // address.  Otherwise, insert it into this session space.
        //

        Status = MiSessionInsertImage (DataTableEntry->DllBase);

        if (!NT_SUCCESS (Status)) {

            if (Status == STATUS_ALREADY_COMMITTED) {

                //
                // This driver's already been loaded in this session.
                //

                ASSERT (DataTableEntry->LoadCount >= 1);

                *ImageHandle = DataTableEntry;
                *ImageBaseAddress = DataTableEntry->DllBase;

                Status = STATUS_SUCCESS;
            }

            //
            // The LoadCount should generally not be 0 here, but it is
            // possible in the case where an attempt has been made to
            // unload a DLL on last dereference, but the DLL refused to
            // unload.
            //

            goto return2;
        }

        //
        // This driver is already loaded in the system, but not in
        // this particular session - share it now.
        //

        FoundDataTableEntry = DataTableEntry;

        DataTableEntry->LoadCount += 1;

        ASSERT (DataTableEntry->SectionPointer != NULL);

        SectionPointer = DataTableEntry->SectionPointer;
    }
    else if (SectionPointer == NULL) {

        //
        // This image is not already loaded.
        //
        // A NULL SectionPointer indicates this thread didn't already load
        // this image below either, so go and get it.
        //
        // Release the load lock first as getting the image is not cheap.
        //

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        LoadLockOwned = FALSE;

        InterlockedOr (&MiFirstDriverLoadEver, 0x1);

        //
        // Check and see if a user wants to replace this binary
        // via a transfer through the kernel debugger.  If this
        // fails just continue on with the existing file.
        //

        if ((KdDebuggerEnabled) && (KdDebuggerNotPresent == FALSE)) {

            Status = KdPullRemoteFile (ImageFileName,
                                       FILE_ATTRIBUTE_NORMAL,
                                       FILE_OVERWRITE_IF,
                                       FILE_SYNCHRONOUS_IO_NONALERT);

            if (NT_SUCCESS (Status)) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
                    "MmLoadSystemImage: Pulled %wZ from kd\n",
                          ImageFileName);
            }
        }

        DataTableEntry = NULL;

        //
        // Attempt to open the driver image itself.  If this fails, then the
        // driver image cannot be located, so nothing else matters.
        //

        InitializeObjectAttributes (&ObjectAttributes,
                                    ImageFileName,
                                    (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                    NULL,
                                    NULL);

        Status = ZwOpenFile (&FileHandle,
                             FILE_EXECUTE,
                             &ObjectAttributes,
                             &IoStatus,
                             FILE_SHARE_READ | FILE_SHARE_DELETE,
                             0);

        if (!NT_SUCCESS (Status)) {

#if DBG
            if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                    "MmLoadSystemImage: cannot open %wZ\n",
                    ImageFileName);
            }
#endif
            //
            // File not found.
            //

            goto return2;
        }

        Status = MmCheckSystemImage (FileHandle, FALSE);

        if ((Status == STATUS_IMAGE_CHECKSUM_MISMATCH) ||
            (Status == STATUS_IMAGE_MP_UP_MISMATCH) ||
            (Status == STATUS_INVALID_IMAGE_PROTECT)) {

            goto return1;
        }

        //
        // Now attempt to create an image section for the file.  If this fails,
        // then the driver file is not an image.  Session space drivers are
        // shared text with copy on write data, so don't allow writes here.
        //

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {
            SectionAccess = SECTION_MAP_READ | SECTION_MAP_EXECUTE;
        }
        else {
            SectionAccess = SECTION_ALL_ACCESS;
        }

        InitializeObjectAttributes (&ObjectAttributes,
                                    NULL,
                                    (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                    NULL,
                                    NULL);

        Status = ZwCreateSection (&SectionHandle,
                                  SectionAccess,
                                  &ObjectAttributes,
                                  (PLARGE_INTEGER) NULL,
                                  PAGE_EXECUTE,
                                  SEC_IMAGE,
                                  FileHandle);

        if (!NT_SUCCESS(Status)) {
            goto return1;
        }

        //
        // Now reference the section handle.  If this fails something is
        // very wrong because it is a kernel handle.
        //
        // N.B.  ObRef sets SectionPointer to NULL on failure.
        //

        Status = ObReferenceObjectByHandle (SectionHandle,
                                            SECTION_MAP_EXECUTE,
                                            MmSectionObjectType,
                                            KernelMode,
                                            (PVOID *) &SectionPointer,
                                            (POBJECT_HANDLE_INFORMATION) NULL);

        ZwClose (SectionHandle);

        if (!NT_SUCCESS (Status)) {
            goto return1;
        }

        if ((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) &&
            (SectionPointer->Segment->ControlArea->u.Flags.FloppyMedia == 0)) {

            //
            // Check with all of the drivers along the path to win32k.sys to
            // ensure that they are willing to follow the rules required
            // of them and to give them a chance to lock down code and data
            // that needs to be locked.  If any of the drivers along the path
            // refuses to participate, fail the win32k.sys load.
            //
            // It is assumed that all session drivers live on the same physical
            // drive, so when the very first session driver is loaded, this
            // check can be made.
            //
            // Note that this is important because these drivers are always
            // paged directly in/out from the filesystem so the drive
            // containing the filesystem better not get removed !
            //

            //
            // This is skipped for the WinPE removable media boot case because
            // the user may be running WinPE in RAM and want to swap out the
            // boot media.  In this case, the control area is marked as
            // FloppyMedia (even if it was CD-based) as all the pages have
            // already been converted to pagefile-backed.
            //

            do {
                OldValue = MiFirstDriverLoadEver;

                if (OldValue & 0x2) {
                    break;
                }

                if (InterlockedCompareExchange (&MiFirstDriverLoadEver, OldValue | 0x2, OldValue) == OldValue) {

                    Status = PpPagePathAssign (SectionPointer->Segment->ControlArea->FilePointer);

                    if (!NT_SUCCESS (Status)) {

                        KdPrint (("PpPagePathAssign FAILED for %wZ: %x\n",
                                 ImageFileName, Status));

                        //
                        // Failing the insertion of win32k.sys' device
                        // in the pagefile path is commented out until
                        // the storage drivers have been modified to
                        // correctly handle this request.  If this is
                        // added later, add code here to release relevant
                        // resources for this error path.
                        //
                    }

                    break;
                }
            } while (TRUE);
        }

        //
        // Anything may have changed while the load lock was released.
        // So before using the section we just created, go back and see
        // if any other threads have slipped through and did it already.
        //

        goto ReCheckLoaderList;
    }
    else {
        DataTableEntry = NULL;
    }

    //
    // Load the driver from the filesystem and pick a virtual address for it.
    // All session images are paged directly to and from the filesystem so these
    // images remain busy.
    //

    Status = MiLoadImageSection (&SectionPointer,
                                 ImageBaseAddress,
                                 ImageFileName,
                                 LoadFlags & MM_LOAD_IMAGE_IN_SESSION,
                                 FoundDataTableEntry);

    ASSERT (Status != STATUS_ALREADY_COMMITTED);

    NumberOfPtes = SectionPointer->Segment->TotalNumberOfPtes;

    //
    // Normal drivers are dereferenced here and their images can then be
    // overwritten.  This is ok because we've already read the whole thing
    // into memory and from here until reboot (or unload), we back them
    // with the pagefile.
    //
    // Session space drivers are the exception - these images
    // are inpaged from the filesystem and we need to keep our reference to
    // the file so that it doesn't get overwritten.
    //

    if ((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) {

        if (NT_SUCCESS (Status)) {

            //
            // Move the driver into a large page if requested via the registry.
            //

            MiUseLargeDriverPage (SectionPointer->Segment->TotalNumberOfPtes,
                                  ImageBaseAddress,
                                  &BaseName,
                                  1);
        }

        ObDereferenceObject (SectionPointer);
        SectionPointer = NULL;
    }

    //
    // The module LoadCount will be 1 here if the module was just loaded.
    // The LoadCount will be >1 if it was attached to by a session (as opposed
    // to just loaded).
    //

    if (!NT_SUCCESS (Status)) {

        if (AlreadyOpen == TRUE) {

            //
            // We're failing and we were just attaching to an already loaded
            // driver.  We don't want to go through the forced unload path
            // because we've already deleted the address space so
            // decrement our reference and clear the DataTableEntry.
            //

            ASSERT (DataTableEntry != NULL);
            DataTableEntry->LoadCount -= 1;
            DataTableEntry = NULL;
        }
        goto return1;
    }

    //
    // Error recovery from this point out for sessions works as follows:
    //
    // For sessions, we may or may not have a DataTableEntry at this point.
    // If we do, it's because we're attaching to a driver that has already
    // been loaded - and the DataTableEntry->LoadCount has been bumped - so
    // the error recovery from here on out is to just call
    // MmUnloadSystemImage with the DataTableEntry.
    //
    // If this is the first load of a given driver into a session space, we
    // have no DataTableEntry at this point.  The view has already been mapped
    // and committed and the group/session addresses reserved for this DLL.
    // The error recovery path handles all this because
    // MmUnloadSystemImage zeroes the relevant fields in the
    // LDR_DATA_TABLE_ENTRY so that MmUnloadSystemImage works properly.
    //

    IssueUnloadOnFailure = TRUE;

    if (AlreadyOpen == FALSE) {

        if (((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) ||
            (*ImageBaseAddress != SectionPointer->Segment->BasedAddress)) {

            //
            // Apply the fixups to the section.  Note session images need only
            // be fixed up once per insertion in the loaded module list.
            //
    
            try {
                Status = LdrRelocateImage (*ImageBaseAddress,
                                           "SYSLDR",
                                           STATUS_SUCCESS,
                                           STATUS_CONFLICTING_ADDRESSES,
                                           STATUS_INVALID_IMAGE_FORMAT);
    
            } except (EXCEPTION_EXECUTE_HANDLER) {
                Status = GetExceptionCode ();
                KdPrint(("MM:sysload - LdrRelocateImage failed status %lx\n",
                          Status));
            }
    
            if (!NT_SUCCESS(Status)) {
    
                //
                // Unload the system image and dereference the section.
                //
    
                goto return1;
            }
        }

        DebugInfoSize = 0;
        DataDirectory = NULL;
        DebugDir = NULL;

        NtHeaders = RtlImageNtHeader (*ImageBaseAddress);

        //
        // Create a loader table entry for this driver before resolving the
        // references so that any circular references can resolve properly.
        //

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

            DebugInfoSize = sizeof (NON_PAGED_DEBUG_INFO);

            if (IMAGE_DIRECTORY_ENTRY_DEBUG <
                NtHeaders->OptionalHeader.NumberOfRvaAndSizes) {

                DataDirectory = &NtHeaders->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_DEBUG];

                if (DataDirectory->VirtualAddress &&
                    DataDirectory->Size &&
                    (DataDirectory->VirtualAddress + DataDirectory->Size) <
                        NtHeaders->OptionalHeader.SizeOfImage) {

                    DebugDir = (PIMAGE_DEBUG_DIRECTORY)
                               ((PUCHAR)(*ImageBaseAddress) +
                                   DataDirectory->VirtualAddress);

                    DebugInfoSize += DataDirectory->Size;

                    for (i = 0;
                         i < DataDirectory->Size/sizeof(IMAGE_DEBUG_DIRECTORY);
                         i += 1) {

                        if ((DebugDir+i)->AddressOfRawData &&
                            (DebugDir+i)->AddressOfRawData <
                                NtHeaders->OptionalHeader.SizeOfImage &&
                            ((DebugDir+i)->AddressOfRawData +
                                (DebugDir+i)->SizeOfData) <
                                NtHeaders->OptionalHeader.SizeOfImage) {

                            DebugInfoSize += (DebugDir+i)->SizeOfData;
                        }
                    }
                }

                DebugInfoSize = MI_ROUND_TO_SIZE(DebugInfoSize, sizeof(ULONG));
            }
        }

        DataTableEntrySize = sizeof (KLDR_DATA_TABLE_ENTRY) +
                             DebugInfoSize +
                             BaseName.Length + sizeof(UNICODE_NULL);

        DataTableEntry = ExAllocatePoolWithTag (NonPagedPool,
                                                DataTableEntrySize,
                                                'dLmM');

        if (DataTableEntry == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto return1;
        }

        //
        // Initialize the flags and load count.
        //

        DataTableEntry->Flags = LDRP_LOAD_IN_PROGRESS;
        DataTableEntry->LoadCount = 1;
        DataTableEntry->LoadedImports = (PVOID)LoadedImports;
        DataTableEntry->PatchInformation = NULL;

        if ((NtHeaders->OptionalHeader.MajorOperatingSystemVersion >= 5) &&
            (NtHeaders->OptionalHeader.MajorImageVersion >= 5)) {
            DataTableEntry->Flags |= LDRP_ENTRY_NATIVE;
        }

        ssHeader = (PNON_PAGED_DEBUG_INFO) ((ULONG_PTR)DataTableEntry +
                                            sizeof (KLDR_DATA_TABLE_ENTRY));

        BaseDllNameBuffer = (PWSTR) ((ULONG_PTR)ssHeader + DebugInfoSize);

        //
        // If loading a session space image, store away some debug data.
        //

        DataTableEntry->NonPagedDebugInfo = NULL;

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

            DataTableEntry->NonPagedDebugInfo = ssHeader;
            DataTableEntry->Flags |= LDRP_NON_PAGED_DEBUG_INFO;

            ssHeader->Signature = NON_PAGED_DEBUG_SIGNATURE;
            ssHeader->Flags = 1;
            ssHeader->Size = DebugInfoSize;
            ssHeader->Machine = NtHeaders->FileHeader.Machine;
            ssHeader->Characteristics = NtHeaders->FileHeader.Characteristics;
            ssHeader->TimeDateStamp = NtHeaders->FileHeader.TimeDateStamp;
            ssHeader->CheckSum = NtHeaders->OptionalHeader.CheckSum;
            ssHeader->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
            ssHeader->ImageBase = (ULONG_PTR) *ImageBaseAddress;

            if (DebugDir) {

                RtlCopyMemory (ssHeader + 1,
                               DebugDir,
                               DataDirectory->Size);

                DebugInfoSize = DataDirectory->Size;

                for (i = 0;
                     i < DataDirectory->Size/sizeof(IMAGE_DEBUG_DIRECTORY);
                     i += 1) {

                    if ((DebugDir + i)->AddressOfRawData &&
                        (DebugDir+i)->AddressOfRawData <
                            NtHeaders->OptionalHeader.SizeOfImage &&
                        ((DebugDir+i)->AddressOfRawData +
                            (DebugDir+i)->SizeOfData) <
                            NtHeaders->OptionalHeader.SizeOfImage) {

                        RtlCopyMemory ((PUCHAR)(ssHeader + 1) +
                                          DebugInfoSize,
                                      (PUCHAR)(*ImageBaseAddress) +
                                          (DebugDir + i)->AddressOfRawData,
                                      (DebugDir + i)->SizeOfData);

                        //
                        // Reset the offset in the debug directory to point to
                        //

                        (((PIMAGE_DEBUG_DIRECTORY)(ssHeader + 1)) + i)->
                            AddressOfRawData = DebugInfoSize;

                        DebugInfoSize += (DebugDir+i)->SizeOfData;
                    }
                    else {
                        (((PIMAGE_DEBUG_DIRECTORY)(ssHeader + 1)) + i)->
                            AddressOfRawData = 0;
                    }
                }
            }
        }

        //
        // Initialize the address of the DLL image file header and the entry
        // point address.
        //

        DataTableEntry->DllBase = *ImageBaseAddress;
        DataTableEntry->EntryPoint =
            ((PCHAR)*ImageBaseAddress + NtHeaders->OptionalHeader.AddressOfEntryPoint);
        DataTableEntry->SizeOfImage = NumberOfPtes << PAGE_SHIFT;
        DataTableEntry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
        DataTableEntry->SectionPointer = (PVOID) SectionPointer;

        //
        // Store the DLL name.
        //

        DataTableEntry->BaseDllName.Buffer = BaseDllNameBuffer;

        DataTableEntry->BaseDllName.Length = BaseName.Length;
        DataTableEntry->BaseDllName.MaximumLength = BaseName.Length;
        RtlCopyMemory (DataTableEntry->BaseDllName.Buffer,
                       BaseName.Buffer,
                       BaseName.Length);
        DataTableEntry->BaseDllName.Buffer[BaseName.Length/sizeof(WCHAR)] = UNICODE_NULL;

        DataTableEntry->FullDllName.Buffer = ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                         PrefixedImageName.Length + sizeof(UNICODE_NULL),
                                                         'TDmM');

        if (DataTableEntry->FullDllName.Buffer == NULL) {

            //
            // Pool could not be allocated, just set the length to 0.
            //

            DataTableEntry->FullDllName.Length = 0;
            DataTableEntry->FullDllName.MaximumLength = 0;
        }
        else {
            DataTableEntry->FullDllName.Length = PrefixedImageName.Length;
            DataTableEntry->FullDllName.MaximumLength = PrefixedImageName.Length;
            RtlCopyMemory (DataTableEntry->FullDllName.Buffer,
                           PrefixedImageName.Buffer,
                           PrefixedImageName.Length);
            DataTableEntry->FullDllName.Buffer[PrefixedImageName.Length/sizeof(WCHAR)] = UNICODE_NULL;
        }

        //
        // Capture the exception table data info
        //

        MiCaptureImageExceptionValues (DataTableEntry);

        //
        // Acquire the loaded module list resource and insert this entry
        // into the list.
        //

        MiProcessLoaderEntry (DataTableEntry, TRUE);

        MissingProcedureName = NameBuffer;
    
        try {
    
            //
            // Resolving the image references results in other DLLs being
            // loaded if they are referenced by the module that was just loaded.
            // An example is when an OEM printer or FAX driver links with
            // other general libraries.  This is not a problem for session space
            // because the general libraries do not have the global data issues
            // that win32k.sys and the video drivers do.  So we just call the
            // standard kernel reference resolver and any referenced libraries
            // get loaded into system global space.  Code in the routine
            // restricts which libraries can be referenced by a driver.
            //
    
            Status = MiResolveImageReferences (*ImageBaseAddress,
                                               &BaseDirectory,
                                               NamePrefix,
                                               &MissingProcedureName,
                                               &MissingDriverName,
                                               &LoadedImports);
    
        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode ();
            KdPrint(("MM:sysload - ResolveImageReferences failed status %x\n",
                        Status));
        }

        if (!NT_SUCCESS (Status)) {
#if DBG
            if (Status == STATUS_OBJECT_NAME_NOT_FOUND) {
                ASSERT (MissingProcedureName == NULL);
            }
    
            if ((Status == STATUS_DRIVER_ORDINAL_NOT_FOUND) ||
                (Status == STATUS_OBJECT_NAME_NOT_FOUND) ||
                (Status == STATUS_DRIVER_ENTRYPOINT_NOT_FOUND)) {
    
                if ((ULONG_PTR)MissingProcedureName & ~((ULONG_PTR) (X64K-1))) {
    
                    //
                    // If not an ordinal, print string.
                    //
    
                    DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                        "MissingProcedureName %s\n", MissingProcedureName);
                }
                else {
                    DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                        "MissingProcedureName 0x%p\n", MissingProcedureName);
                }
            }
    
            if (MissingDriverName != NULL) {
                PrintableMissingDriverName = (PWSTR)((ULONG_PTR)MissingDriverName & ~0x1);
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                    "MissingDriverName %ws\n", PrintableMissingDriverName);
            }
#endif
            MiProcessLoaderEntry (DataTableEntry, FALSE);

            if (DataTableEntry->FullDllName.Buffer != NULL) {
                ExFreePool (DataTableEntry->FullDllName.Buffer);
            }
            ExFreePool (DataTableEntry);

            DataTableEntry = NULL;

            goto return1;
        }

        //
        // Reinitialize the flags and update the loaded imports.
        //

        DataTableEntry->Flags |= (LDRP_SYSTEM_MAPPED | LDRP_ENTRY_PROCESSED | LDRP_MM_LOADED);
        DataTableEntry->Flags &= ~LDRP_LOAD_IN_PROGRESS;
        DataTableEntry->LoadedImports = LoadedImports;

        MiApplyDriverVerifier (DataTableEntry, NULL);

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

            //
            // Attempt to lookup the win32k system service table. If the
            // service table is found, then compact the table into an array
            // of relative 32-bit pointers.
            //

#if defined(_AMD64_)

            PULONG Limit;
            BOOLEAN Match;
            UNICODE_STRING Name;
            NTSTATUS Status1;
            NTSTATUS Status2;
            PVOID Table;

            RtlInitUnicodeString (&Name, L"win32k.sys");
            Match = RtlEqualUnicodeString (&DataTableEntry->BaseDllName,
                                           &Name,
                                           TRUE);

            if (Match == TRUE) {
                Status1 = LookupEntryPoint (DataTableEntry->DllBase,
                                            "W32pServiceTable",
                                            &Table);
    
                Status2 = LookupEntryPoint (DataTableEntry->DllBase,
                                            "W32pServiceLimit",
                                            &Limit);

                if (NT_SUCCESS(Status1) && NT_SUCCESS(Status2)) {
                    KeCompactServiceTable (Table, *Limit, TRUE);

                } else {
                    DbgBreakPoint ();
                }
            }

#endif

            //
            // The session image was mapped entirely read-write on initial
            // creation.  Now that the relocations (if any), image
            // resolution and import table updates are complete, correct
            // permissions can be applied.
            //
            // Make the entire image copy on write.  The subsequent call
            // to MiWriteProtectSystemImage will make various portions
            // readonly.  Then apply flip various subsections into global
            // shared mode if their attributes specify it.
            //

            PointerPte = MiGetPteAddress (DataTableEntry->DllBase);

            MiSetSystemCodeProtection (PointerPte,
                                       PointerPte + NumberOfPtes - 1,
                                       MM_EXECUTE_WRITECOPY);
        }

        MiWriteProtectSystemImage (DataTableEntry->DllBase);

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {
            MiSessionProcessGlobalSubsections (DataTableEntry);
        }

        if (PsImageNotifyEnabled) {
            IMAGE_INFO ImageInfo;

            ImageInfo.Properties = 0;
            ImageInfo.ImageAddressingMode = IMAGE_ADDRESSING_MODE_32BIT;
            ImageInfo.SystemModeImage = TRUE;
            ImageInfo.ImageSize = DataTableEntry->SizeOfImage;
            ImageInfo.ImageBase = *ImageBaseAddress;
            ImageInfo.ImageSelector = 0;
            ImageInfo.ImageSectionNumber = 0;

            PsCallImageNotifyRoutines(ImageFileName, (HANDLE)NULL, &ImageInfo);
        }

        if (MiCacheImageSymbols (*ImageBaseAddress)) {

            //
            //  TEMP TEMP TEMP rip out when debugger converted
            //

            ANSI_STRING AnsiName;
            UNICODE_STRING UnicodeName;

            //
            //  \SystemRoot is 11 characters in length
            //
            if (PrefixedImageName.Length > (11 * sizeof (WCHAR )) &&
                !_wcsnicmp (PrefixedImageName.Buffer, (const PUSHORT)L"\\SystemRoot", 11)) {
                UnicodeName = PrefixedImageName;
                UnicodeName.Buffer += 11;
                UnicodeName.Length -= (11 * sizeof (WCHAR));
                sprintf (NameBuffer, "%ws%wZ", &SharedUserData->NtSystemRoot[2], &UnicodeName);
            }
            else {
                sprintf (NameBuffer, "%wZ", &BaseName);
            }
            RtlInitString (&AnsiName, NameBuffer);
            DbgLoadImageSymbols (&AnsiName,
                                 *ImageBaseAddress,
                                 (ULONG_PTR) -1);

            DataTableEntry->Flags |= LDRP_DEBUG_SYMBOLS_LOADED;
        }
    }

    //
    // Flush the instruction cache on all systems in the configuration.
    //

    KeSweepIcache (TRUE);
    *ImageHandle = DataTableEntry;
    Status = STATUS_SUCCESS;

    //
    // Session images are always paged by default.
    // Non-session images get paged now.
    //

    if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {
        MI_LOG_SESSION_DATA_START (DataTableEntry);
    }
    else if ((LoadFlags & MM_LOAD_IMAGE_AND_LOCKDOWN) == 0) {

        ASSERT (SectionPointer == NULL);

        MiEnablePagingOfDriver (DataTableEntry);
    }

return1:

    if (!NT_SUCCESS(Status)) {

        if (IssueUnloadOnFailure == TRUE) {

            if (DataTableEntry == NULL) {

                RtlZeroMemory (&TempDataTableEntry, sizeof (KLDR_DATA_TABLE_ENTRY));

                DataTableEntry = &TempDataTableEntry;

                DataTableEntry->DllBase = *ImageBaseAddress;
                DataTableEntry->SizeOfImage = NumberOfPtes << PAGE_SHIFT;
                DataTableEntry->LoadCount = 1;
                DataTableEntry->LoadedImports = LoadedImports;

                if ((AlreadyOpen == FALSE) && (SectionPointer != NULL)) {
                    DataTableEntry->SectionPointer = (PVOID) SectionPointer;
                }
            }
#if DBG
            else {

                //
                // If DataTableEntry is NULL, then we are unloading before one
                // got created.  Once a LDR_DATA_TABLE_ENTRY is created, the
                // load cannot fail, so if it exists here, at least one other
                // session contains this image as well.
                //

                ASSERT (DataTableEntry->LoadCount > 1);
            }
#endif

            MmUnloadSystemImage ((PVOID)DataTableEntry);
        }

        if ((AlreadyOpen == FALSE) && (SectionPointer != NULL)) {

            //
            // This is needed for failed win32k.sys loads or any session's
            // load of the first instance of a driver.
            //

            ObDereferenceObject (SectionPointer);
        }
    }

    if (LoadLockOwned == TRUE) {
        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        LoadLockOwned = FALSE;
    }

    if (FileHandle) {
        ZwClose (FileHandle);
    }

    if (!NT_SUCCESS(Status)) {

        UNICODE_STRING ErrorStrings[4];
        ULONG UniqueErrorValue;
        ULONG StringSize;
        ULONG StringCount;
        ANSI_STRING AnsiString;
        UNICODE_STRING ProcedureName = {0};
        UNICODE_STRING DriverName;
        ULONG i;
        PWCHAR temp;
        PWCHAR ptr;
        ULONG PacketSize;
        SIZE_T length;
        PIO_ERROR_LOG_PACKET ErrLog;

        //
        // The driver could not be loaded - log an event with the details.
        //

        StringSize = 0;

        *(&ErrorStrings[0]) = *ImageFileName;
        StringSize += (ImageFileName->Length + sizeof(UNICODE_NULL));
        StringCount = 1;

        UniqueErrorValue = 0;

        PrintableMissingDriverName = (PWSTR)((ULONG_PTR)MissingDriverName & ~0x1);
        if ((Status == STATUS_DRIVER_ORDINAL_NOT_FOUND) ||
            (Status == STATUS_DRIVER_ENTRYPOINT_NOT_FOUND) ||
            (Status == STATUS_OBJECT_NAME_NOT_FOUND) ||
            (Status == STATUS_PROCEDURE_NOT_FOUND)) {

            ErrorStrings[1].Buffer = L"cannot find";
            length = wcslen(ErrorStrings[1].Buffer) * sizeof(WCHAR);
            ErrorStrings[1].Length = (USHORT) length;
            StringSize += (ULONG)(length + sizeof (UNICODE_NULL));
            StringCount += 1;

            RtlInitUnicodeString (&DriverName, PrintableMissingDriverName);

            StringSize += (DriverName.Length + sizeof(UNICODE_NULL));
            StringCount += 1;
            *(&ErrorStrings[2]) = *(&DriverName);

            if ((ULONG_PTR)MissingProcedureName & ~((ULONG_PTR) (X64K-1))) {

                //
                // If not an ordinal, pass as a Unicode string
                //

                RtlInitAnsiString (&AnsiString, MissingProcedureName);
                if (NT_SUCCESS (RtlAnsiStringToUnicodeString (&ProcedureName, &AnsiString, TRUE))) {
                    StringSize += (ProcedureName.Length + sizeof(UNICODE_NULL));
                    StringCount += 1;
                    *(&ErrorStrings[3]) = *(&ProcedureName);
                }
                else {
                    goto GenericError;
                }
            }
            else {

                //
                // Just pass ordinal values as is in the UniqueErrorValue.
                //

                UniqueErrorValue = PtrToUlong (MissingProcedureName);
            }
        }
        else {

GenericError:

            UniqueErrorValue = (ULONG) Status;

            if (MmIsRetryIoStatus(Status)) {

                //
                // Coalesce the various low memory values into just one.
                //

                Status = STATUS_INSUFFICIENT_RESOURCES;

            }
            else {

                //
                // Ideally, the real failing status should be returned. However,
                // we need to do a full release worth of testing (ie Longhorn)
                // before making that change.
                //

                Status = STATUS_DRIVER_UNABLE_TO_LOAD;
            }

            ErrorStrings[1].Buffer = L"failed to load";
            length = wcslen(ErrorStrings[1].Buffer) * sizeof(WCHAR);
            ErrorStrings[1].Length = (USHORT) length;
            StringSize += (ULONG)(length + sizeof (UNICODE_NULL));
            StringCount += 1;
        }

        PacketSize = sizeof (IO_ERROR_LOG_PACKET) + StringSize;

        //
        // Enforce I/O manager interface (ie: UCHAR) size restrictions.
        //

        if (PacketSize < MAXUCHAR) {

            ErrLog = IoAllocateGenericErrorLogEntry ((UCHAR)PacketSize);

            if (ErrLog != NULL) {

                //
                // Fill it in and write it out as a single string.
                //

                ErrLog->ErrorCode = STATUS_LOG_HARD_ERROR;
                ErrLog->FinalStatus = Status;
                ErrLog->UniqueErrorValue = UniqueErrorValue;

                ErrLog->StringOffset = (USHORT) sizeof (IO_ERROR_LOG_PACKET);

                temp = (PWCHAR) ((PUCHAR) ErrLog + ErrLog->StringOffset);

                for (i = 0; i < StringCount; i += 1) {

                    ptr = ErrorStrings[i].Buffer;

                    RtlCopyMemory (temp, ptr, ErrorStrings[i].Length);
                    temp += (ErrorStrings[i].Length / sizeof (WCHAR));

                    *temp = L' ';
                    temp += 1;
                }

                *(temp - 1) = UNICODE_NULL;
                ErrLog->NumberOfStrings = 1;

                IoWriteErrorLogEntry (ErrLog);
            }
        }

        //
        // The only way this pointer has the low bit set is if we are expected
        // to free the pool containing the name.  Typically the name points at
        // a loaded module list entry and so no one has to free it and in this
        // case the low bit will NOT be set.  If the module could not be found
        // and was therefore not loaded, then we left a piece of pool around
        // containing the name since there is no loaded module entry already -
        // this must be released now.
        //

        if ((ULONG_PTR)MissingDriverName & 0x1) {
            ExFreePool (PrintableMissingDriverName);
        }

        if (ProcedureName.Buffer != NULL) {
            RtlFreeUnicodeString (&ProcedureName);
        }
        ExFreePool (NameBuffer);
        return Status;
    }

return2:

    if (LoadLockOwned == TRUE) {
        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        LoadLockOwned = FALSE;
    }

    if (NamePrefix) {
        ExFreePool (PrefixedImageName.Buffer);
    }

    ExFreePool (NameBuffer);

    return Status;
}

VOID
MiReturnFailedSessionPages (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This routine is a nonpaged wrapper which undoes session image loads
    that failed midway through reading in the pages.

Arguments:

    PointerPte - Supplies the starting PTE for the range to unload.

    LastPte - Supplies the ending PTE for the range to unload.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {
        if (PointerPte->u.Hard.Valid == 1) {

            //
            // Delete the page.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

            //
            // Set the pointer to PTE as empty so the page
            // is deleted when the reference count goes to zero.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

            MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCount (Pfn1, PageFrameIndex);

            MI_WRITE_ZERO_PTE (PointerPte);
        }
        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);
}


NTSTATUS
MiLoadImageSection (
    IN OUT PSECTION *InputSectionPointer,
    OUT PVOID *ImageBaseAddress,
    IN PUNICODE_STRING ImageFileName,
    IN ULONG LoadInSessionSpace,
    IN PKLDR_DATA_TABLE_ENTRY FoundDataTableEntry
    )

/*++

Routine Description:

    This routine loads the specified image into the kernel part of the
    address space.

Arguments:

    InputSectionPointer - Supplies the section object for the image.  This may
                          be replaced by a pagefile-backed section (for
                          protection purposes) for session images if it is
                          determined that the image section is concurrently
                          being accessed by a user app.

    ImageBaseAddress - Returns the address that the image header is at.

    ImageFileName - Supplies the full path name (including the image name)
                    of the image to load.

    LoadInSessionSpace - Supplies nonzero to load this image in session space.
                         Each session gets a different copy of this driver with
                         pages shared as much as possible via copy on write.

                         Supplies zero if this image should be loaded in global
                         space.

    FoundDataTableEntry - Supplies the loader data table entry if the image
                          has already been loaded.  This can only happen for
                          session space.  It means this driver has already
                          been loaded into a different session, so this session
                          still needs to map it.

Return Value:

    Status of the operation.

--*/

{
    KAPC_STATE ApcState;
    PFN_NUMBER PagesRequired;
    PFN_NUMBER ActualPagesUsed;
    PSECTION SectionPointer;
    PSECTION NewSectionPointer;
    PVOID OpaqueSession;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PEPROCESS Process;
    PEPROCESS TargetProcess;
    ULONG NumberOfPtes;
    MMPTE PteContents;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PVOID UserVa;
    PVOID SystemVa;
    NTSTATUS Status;
    NTSTATUS ExceptionStatus;
    PVOID Base;
    ULONG_PTR ViewSize;
    LARGE_INTEGER SectionOffset;
    LOGICAL LoadSymbols;
    PVOID BaseAddress;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;

    PAGED_CODE();

#if !DBG
    UNREFERENCED_PARAMETER (ImageFileName);
#endif

    SectionPointer = *InputSectionPointer;

    NumberOfPtes = SectionPointer->Segment->TotalNumberOfPtes;

    if (LoadInSessionSpace != 0) {

        //
        // Allocate a unique systemwide session space virtual address for
        // the driver.
        //

        if (FoundDataTableEntry == NULL) {

            Status = MiSessionWideReserveImageAddress (SectionPointer,
                                                       &BaseAddress,
                                                       &NewSectionPointer);

            if (!NT_SUCCESS(Status)) {
                return Status;
            }

            if (NewSectionPointer != NULL) {
                SectionPointer = NewSectionPointer;
                *InputSectionPointer = NewSectionPointer;
            }
        }
        else {
            BaseAddress = FoundDataTableEntry->DllBase;
        }

#if DBG
        if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
                "MM: MiLoadImageSection: Image %wZ, BasedAddress 0x%p, Allocated Session BaseAddress 0x%p\n",
                ImageFileName,
                SectionPointer->Segment->BasedAddress,
                BaseAddress);
        }
#endif

        //
        // Session images are mapped backed directly by the file image.
        // All pristine pages of the image will be shared across all
        // sessions, with each page treated as copy-on-write on first write.
        //
        // NOTE: This makes the file image "busy", a different behavior
        // as normal kernel drivers are backed by the paging file only.
        //

        Status = MiShareSessionImage (BaseAddress, SectionPointer);

        if (!NT_SUCCESS (Status)) {
            MiRemoveImageSessionWide (FoundDataTableEntry,
                                      BaseAddress,
                                      NumberOfPtes << PAGE_SHIFT);
            return Status;
        }

        *ImageBaseAddress = BaseAddress;

        return Status;
    }

    ASSERT (FoundDataTableEntry == NULL);

    //
    // Calculate the number of pages required to load this image.
    //
    // Start out by charging for everything and subtract out any gap
    // pages after the image loads successfully.
    //

    PagesRequired = NumberOfPtes;
    ActualPagesUsed = 0;

    //
    // See if ample pages exist to load this image.
    //

    if (MiChargeResidentAvailable (PagesRequired, MM_RESAVAIL_ALLOCATE_LOAD_SYSTEM_IMAGE) == FALSE) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Reserve the necessary system address space.
    //

    FirstPte = MiReserveSystemPtes (NumberOfPtes, SystemPteSpace);

    if (FirstPte == NULL) {
        MI_INCREMENT_RESIDENT_AVAILABLE (PagesRequired,
                                         MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE1);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    PointerPte = FirstPte;
    SystemVa = MiGetVirtualAddressMappedByPte (PointerPte);

    if (MiChargeCommitment (PagesRequired, NULL) == FALSE) {
        MI_INCREMENT_RESIDENT_AVAILABLE (PagesRequired,
                                         MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE1);
        MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_DRIVER_PAGES, PagesRequired);

    InterlockedExchangeAdd ((PLONG)&MmDriverCommit, (LONG) PagesRequired);

    //
    // Map a view into the user portion of the address space.
    //

    Process = PsGetCurrentProcess ();

    //
    // Since callees are not always in the context of the system process,
    // attach here when necessary to guarantee the driver load occurs in a
    // known safe address space to prevent security holes.
    //

    OpaqueSession = NULL;

    KeStackAttachProcess (&PsInitialSystemProcess->Pcb, &ApcState);

    ZERO_LARGE (SectionOffset);
    Base = NULL;
    ViewSize = 0;

    if (NtGlobalFlag & FLG_ENABLE_KDEBUG_SYMBOL_LOAD) {
        LoadSymbols = TRUE;
        NtGlobalFlag &= ~FLG_ENABLE_KDEBUG_SYMBOL_LOAD;
    }
    else {
        LoadSymbols = FALSE;
    }

    TargetProcess = PsGetCurrentProcess ();

    Status = MmMapViewOfSection (SectionPointer,
                                 TargetProcess,
                                 &Base,
                                 0,
                                 0,
                                 &SectionOffset,
                                 &ViewSize,
                                 ViewUnmap,
                                 0,
                                 PAGE_EXECUTE);

    if (LoadSymbols) {
        NtGlobalFlag |= FLG_ENABLE_KDEBUG_SYMBOL_LOAD;
    }

    if (Status == STATUS_IMAGE_MACHINE_TYPE_MISMATCH) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
    }

    if (!NT_SUCCESS(Status)) {

        KeUnstackDetachProcess (&ApcState);

        MI_INCREMENT_RESIDENT_AVAILABLE (PagesRequired,
                                         MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE2);

        MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);
        MiReturnCommitment (PagesRequired);

        return Status;
    }

    //
    // Allocate a physical page(s) and copy the image data.
    // Note for session drivers, the physical pages have already
    // been allocated and just data copying is done here.
    //

    ControlArea = SectionPointer->Segment->ControlArea;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    ASSERT (Subsection->SubsectionBase != NULL);
    ProtoPte = Subsection->SubsectionBase;

    *ImageBaseAddress = SystemVa;

    UserVa = Base;
    TempPte = ValidKernelPte;
    TempPte.u.Long |= MM_PTE_EXECUTE;

    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
    LastPte = PointerPte + NumberOfPtes;

    ExceptionStatus = STATUS_SUCCESS;

    while (PointerPte < LastPte) {

        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.
            //

            Subsection = Subsection->NextSubsection;
            ProtoPte = Subsection->SubsectionBase;
            LastProto = &Subsection->SubsectionBase[
                                        Subsection->PtesInSubsection];
        }

        PteContents = *ProtoPte;
        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Protection != MM_NOACCESS)) {

            ActualPagesUsed += 1;

            PageFrameIndex = MiAllocatePfn (PointerPte, MM_EXECUTE);

            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            ASSERT (MI_PFN_ELEMENT (PageFrameIndex)->u1.WsIndex == 0);

            try {

                RtlCopyMemory (SystemVa, UserVa, PAGE_SIZE);

            } except (MiMapCacheExceptionFilter (&ExceptionStatus,
                                                 GetExceptionInformation())) {

                //
                // An exception occurred, unmap the view and
                // return the error to the caller.
                //

#if DBG
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                    "MiLoadImageSection: Exception 0x%x copying driver SystemVa 0x%p, UserVa 0x%p\n",ExceptionStatus,SystemVa,UserVa);
#endif

                MiReturnFailedSessionPages (FirstPte, PointerPte);

                MI_INCREMENT_RESIDENT_AVAILABLE (PagesRequired,
                                                 MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE3);

                MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);

                MiReturnCommitment (PagesRequired);

                Status = MiUnmapViewOfSection (TargetProcess, Base, 0);

                ASSERT (NT_SUCCESS (Status));

                //
                // Purge the section as we want these pages on the freelist
                // instead of at the tail of standby, as we're completely
                // done with the section.  This is because other valuable
                // standby pages end up getting reused (especially during
                // bootup) when the section pages are the ones that really
                // will never be referenced again.
                //
                // Note this isn't done for session images as they're
                // inpaged directly from the filesystem via the section.
                //

                MmPurgeSection (ControlArea->FilePointer->SectionObjectPointer,
                                NULL,
                                0,
                                FALSE);

                KeUnstackDetachProcess (&ApcState);

                return ExceptionStatus;
            }
        }
        else {

            //
            // The PTE is no access.
            //

            MI_WRITE_ZERO_PTE (PointerPte);
        }

        ProtoPte += 1;
        PointerPte += 1;
        SystemVa = ((PCHAR)SystemVa + PAGE_SIZE);
        UserVa = ((PCHAR)UserVa + PAGE_SIZE);
    }

    Status = MiUnmapViewOfSection (TargetProcess, Base, 0);
    ASSERT (NT_SUCCESS (Status));

    //
    // Purge the section as we want these pages on the freelist instead of
    // at the tail of standby, as we're completely done with the section.
    // This is because other valuable standby pages end up getting reused
    // (especially during bootup) when the section pages are the ones that
    // really will never be referenced again.
    //

    MmPurgeSection (ControlArea->FilePointer->SectionObjectPointer,
                    NULL,
                    0,
                    FALSE);

    KeUnstackDetachProcess (&ApcState);

    //
    // Return any excess resident available and commit.
    //

    if (PagesRequired != ActualPagesUsed) {
        ASSERT (PagesRequired > ActualPagesUsed);
        PagesRequired -= ActualPagesUsed;

        MI_INCREMENT_RESIDENT_AVAILABLE (PagesRequired,
                        MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE_EXCESS);

        MiReturnCommitment (PagesRequired);
    }

    return Status;
}

VOID
MmFreeDriverInitialization (
    IN PVOID ImageHandle
    )

/*++

Routine Description:

    This routine removes the pages that relocate and debug information from
    the address space of the driver.

    NOTE:  This routine looks at the last sections defined in the image
           header and if that section is marked as DISCARDABLE in the
           characteristics, it is removed from the image.  This means
           that all discardable sections at the end of the driver are
           deleted.

Arguments:

    SectionObject - Supplies the section object for the image.

Return Value:

    None.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER NumberOfPtes;
    PVOID Base;
    PVOID StartVa;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    PIMAGE_SECTION_HEADER FoundSection;
    PFN_NUMBER PagesDeleted;

    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)ImageHandle;
    Base = DataTableEntry->DllBase;

    ASSERT (MI_IS_SESSION_ADDRESS (Base) == FALSE);

    NumberOfPtes = DataTableEntry->SizeOfImage >> PAGE_SHIFT;
    LastPte = MiGetPteAddress (Base) + NumberOfPtes;

    NtHeaders = (PIMAGE_NT_HEADERS) RtlImageNtHeader (Base);

    if (NtHeaders == NULL) {
        return;
    }

    NtSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    NtSection += NtHeaders->FileHeader.NumberOfSections;

    FoundSection = NULL;
    for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {
        NtSection -= 1;
        if ((NtSection->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0) {
            FoundSection = NtSection;
        }
        else {

            //
            // There was a non discardable section between this
            // section and the last non discardable section, don't
            // discard this section and don't look any more.
            //

            break;
        }
    }

    if (FoundSection != NULL) {

        StartVa = (PVOID) (ROUND_TO_PAGES (
                            (PCHAR)Base + FoundSection->VirtualAddress));

        PointerPte = MiGetPteAddress (StartVa);

        NumberOfPtes = (PFN_NUMBER)(LastPte - PointerPte);

        if (NumberOfPtes != 0) {

            if (MI_IS_PHYSICAL_ADDRESS (StartVa)) {

                //
                // Don't free the INIT code for a driver mapped by large pages
                // because if it unloads later, we'd have to deal with
                // discontiguous ranges of pages to free.
                //

                return;
            }
            else {
                PagesDeleted = MiDeleteSystemPageableVm (PointerPte,
                                                        NumberOfPtes,
                                                        MI_DELETE_FLUSH_TB,
                                                        NULL);
            }

            MI_INCREMENT_RESIDENT_AVAILABLE (PagesDeleted,
                                            MM_RESAVAIL_FREE_DRIVER_INITIALIZATION);

            MiReturnCommitment (PagesDeleted);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_INIT_CODE, PagesDeleted);

            InterlockedExchangeAdd ((PLONG)&MmDriverCommit,
                                    (LONG) (0 - PagesDeleted));
        }
    }

    return;
}

LOGICAL
MiChargeResidentAvailable (
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Id
    )

/*++

Routine Description:

    This routine is a nonpaged wrapper to charge resident available pages.

Arguments:

    NumberOfPages - Supplies the number of pages to charge.

    Id - Supplies a tracking ID for debugging purposes.

Return Value:

    TRUE if the pages were charged, FALSE if not.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    if (MI_NONPAGEABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages, Id);

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MiFlushPteListFreePfns (
    IN PVOID *VaFlushList,
    IN ULONG FlushCount
    )

/*++

Routine Description:

    This routine flushes all the PTEs in the PTE flush list.
    If the list has overflowed, the entire TB is flushed.

    This routine also decrements the sharecounts on the relevant PFNs.

Arguments:

    VaFlushList - Supplies a pointer to the list to be flushed.

    Count - Supplies a count of entries to be flushed.

Return Value:

    None.

Environment:

    Kernel mode, working set mutex held, APC_LEVEL.

--*/

{
    ULONG i;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE PreviousPte;
    KIRQL OldIrql;

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    ASSERT (FlushCount != 0);

    //
    // Put the PTEs in transition and decrement the number of
    // valid PTEs within the containing page table page.  Note
    // that for a private page, the page table page is still
    // needed because the page is in transition.
    //

    LOCK_PFN (OldIrql);

    for (i = 0; i < FlushCount; i += 1) {

        PointerPte = MiGetPteAddress (VaFlushList[i]);

        TempPte = *PointerPte;
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
        Pfn = MI_PFN_ELEMENT (PageFrameIndex);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                      Pfn->OriginalPte.u.Soft.Protection);

        PreviousPte = *PointerPte;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn);

        MiDecrementShareCount (Pfn, PageFrameIndex);
    }

    //
    // Flush the relevant entries from the translation buffer.
    //

    if (FlushCount == 1) {
        MI_FLUSH_SINGLE_TB (VaFlushList[0], TRUE);
    }
    else if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (FlushCount, &VaFlushList[0], TRUE);
    }
    else {
        MI_FLUSH_ENTIRE_TB (0x19);
    }

    UNLOCK_PFN (OldIrql);

    return;
}

VOID
MiEnablePagingOfDriver (
    IN PVOID ImageHandle
    )

{
    ULONG Span;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER FoundSection;
    PIMAGE_OPTIONAL_HEADER OptionalHeader;

    //
    // Don't page kernel mode code if customer does not want it paged.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

    //
    // If the driver has pageable code, make it paged.
    //

    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY) ImageHandle;
    Base = DataTableEntry->DllBase;

    NtHeaders = (PIMAGE_NT_HEADERS) RtlImageNtHeader (Base);

    if (NtHeaders == NULL) {
        return;
    }

    OptionalHeader = (PIMAGE_OPTIONAL_HEADER)((PCHAR)NtHeaders +
#if defined (_WIN64)
                        FIELD_OFFSET (IMAGE_NT_HEADERS64, OptionalHeader)
#else
                        FIELD_OFFSET (IMAGE_NT_HEADERS32, OptionalHeader)
#endif
                        );

    FoundSection = IMAGE_FIRST_SECTION (NtHeaders);

    i = NtHeaders->FileHeader.NumberOfSections;

    PointerPte = NULL;

    //
    // Initializing LastPte is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    LastPte = NULL;

    while (i > 0) {
#if DBG
            if ((*(PULONG)FoundSection->Name == 'tini') ||
                (*(PULONG)FoundSection->Name == 'egap')) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                    "driver %wZ has lower case sections (init or pagexxx)\n",
                    &DataTableEntry->FullDllName);
            }
#endif

        //
        // Mark as pageable any section which starts with the
        // first 4 characters PAGE or .eda (for the .edata section).
        //

        if ((*(PULONG)FoundSection->Name == 'EGAP') ||
           (*(PULONG)FoundSection->Name == 'ade.')) {

            //
            // This section is pageable, save away the start and end.
            //

            if (PointerPte == NULL) {

                //
                // Previous section was NOT pageable, get the start address.
                //

                PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                   (PCHAR)Base + FoundSection->VirtualAddress)));
            }

            //
            // Generally, SizeOfRawData is larger than VirtualSize for each
            // section because it includes the padding to get to the subsection
            // alignment boundary.  However, if the image is linked with
            // subsection alignment == native page alignment, the linker will
            // have VirtualSize be much larger than SizeOfRawData because it
            // will account for all the bss.
            //
    
            Span = FoundSection->SizeOfRawData;
    
            if (Span < FoundSection->Misc.VirtualSize) {
                Span = FoundSection->Misc.VirtualSize;
            }

            LastPte = MiGetPteAddress ((PCHAR)Base +
                                       FoundSection->VirtualAddress +
                                       (OptionalHeader->SectionAlignment - 1) +
                                       Span - PAGE_SIZE);

        }
        else {

            //
            // This section is not pageable, if the previous section was
            // pageable, enable it.
            //

            if (PointerPte != NULL) {
                MiSetPagingOfDriver (PointerPte, LastPte);
                PointerPte = NULL;
            }
        }
        i -= 1;
        FoundSection += 1;
    }
    if (PointerPte != NULL) {
        MiSetPagingOfDriver (PointerPte, LastPte);
    }
}


VOID
MiSetPagingOfDriver (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This routine marks the specified range of PTEs as pageable.

Arguments:

    PointerPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

Return Value:

    None.

Environment:

    Kernel Mode, IRQL of APC_LEVEL or below.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    PVOID Base;
    PFN_NUMBER PageCount;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    PETHREAD Thread;
    ULONG FlushCount;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];

    PAGED_CODE ();

    Base = MiGetVirtualAddressMappedByPte (PointerPte);

    if (MI_IS_PHYSICAL_ADDRESS (Base)) {

        //
        // No need to lock physical addresses.
        //

        return;
    }

    ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (Base) == FALSE);

    PageCount = 0;
    FlushCount = 0;
    Thread = PsGetCurrentThread ();

    LOCK_WORKING_SET (Thread, &MmSystemCacheWs);

    while (PointerPte <= LastPte) {

        //
        // Check to make sure this PTE has not already been
        // made pageable (or deleted).  It is pageable if it
        // is not valid, or if the PFN database wsindex element
        // is non zero.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn->u2.ShareCount == 1);

            //
            // If the wsindex is nonzero, then this page is already pageable
            // and has a WSLE entry.  Ignore it here and let the trimmer
            // take it if memory comes under pressure.
            //

            if (Pfn->u1.WsIndex == 0) {

                //
                // Original PTE may need to be set for drivers loaded
                // via ntldr.
                //

                if (Pfn->OriginalPte.u.Long == 0) {
                    Pfn->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
                    Pfn->OriginalPte.u.Soft.Protection |= MM_EXECUTE;
                }

                VaFlushList[FlushCount] = Base;
                FlushCount += 1;

                if (FlushCount == MM_MAXIMUM_FLUSH_COUNT) {
                    MiFlushPteListFreePfns ((PVOID *)&VaFlushList, FlushCount);
                    FlushCount = 0;
                }

                PageCount += 1;
            }
        }
        Base = (PVOID)((PCHAR)Base + PAGE_SIZE);
        PointerPte += 1;
    }

    if (FlushCount != 0) {
        MiFlushPteListFreePfns ((PVOID *)&VaFlushList, FlushCount);
    }

    UNLOCK_WORKING_SET (Thread, &MmSystemCacheWs);

    if (PageCount != 0) {
        InterlockedExchangeAdd (&MmTotalSystemDriverPages, (LONG) PageCount);

        MI_INCREMENT_RESIDENT_AVAILABLE (PageCount,
                                         MM_RESAVAIL_FREE_SET_DRIVER_PAGING);
    }
}


PVOID
MmPageEntireDriver (
    __in PVOID AddressWithinSection
    )

/*++

Routine Description:

    This routine allows a driver to page out all of its code and
    data regardless of the attributes of the various image sections.

    Note, this routine can be called multiple times with no
    intervening calls to MmResetDriverPaging.

Arguments:

    AddressWithinSection - Supplies an address within the driver, e.g.
                           DriverEntry.

Return Value:

    Base address of driver.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PVOID BaseAddress;

    PAGED_CODE();

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, FALSE);

    if (DataTableEntry == NULL) {
        return NULL;
    }

    //
    // Don't page kernel mode code if disabled via registry.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return DataTableEntry->DllBase;
    }

    if (DataTableEntry->SectionPointer != NULL) {

        //
        // Driver is mapped as an image (ie: session space), this is always
        // pageable.
        //

        ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (AddressWithinSection) == TRUE);

        return DataTableEntry->DllBase;
    }

    //
    // Force any active DPCs to clear the system before we page the driver.
    //

    KeFlushQueuedDpcs ();

    BaseAddress = DataTableEntry->DllBase;
    FirstPte = MiGetPteAddress (BaseAddress);
    LastPte = (FirstPte - 1) + (DataTableEntry->SizeOfImage >> PAGE_SHIFT);

    ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (AddressWithinSection) == FALSE);

    MiSetPagingOfDriver (FirstPte, LastPte);

    return BaseAddress;
}


VOID
MmResetDriverPaging (
    __in PVOID AddressWithinSection
    )

/*++

Routine Description:

    This routines resets the driver paging to what the image specified.
    Hence image sections such as the IAT, .text, .data will be locked
    down in memory.

    Note, there is no requirement that MmPageEntireDriver was called.

Arguments:

    AddressWithinSection - Supplies an address within the driver, e.g.
                           DriverEntry.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    ULONG Span;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER FoundSection;

    PAGED_CODE();

    //
    // Don't page kernel mode code if disabled via registry.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

    if (MI_IS_PHYSICAL_ADDRESS (AddressWithinSection)) {
        return;
    }

    //
    // If the driver has pageable code, make it paged.
    //

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, FALSE);

    if (DataTableEntry->SectionPointer != NULL) {

        //
        // Driver is mapped by image hence already paged.
        //

        ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (AddressWithinSection) == TRUE);

        return;
    }

    Base = DataTableEntry->DllBase;

    NtHeaders = (PIMAGE_NT_HEADERS) RtlImageNtHeader (Base);

    if (NtHeaders == NULL) {
        return;
    }

    FoundSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    i = NtHeaders->FileHeader.NumberOfSections;
    PointerPte = NULL;

    while (i > 0) {
#if DBG
            if ((*(PULONG)FoundSection->Name == 'tini') ||
                (*(PULONG)FoundSection->Name == 'egap')) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_INFO_LEVEL, 
                    "driver %wZ has lower case sections (init or pagexxx)\n",
                    &DataTableEntry->FullDllName);
            }
#endif

        //
        // Don't lock down code for sections marked as discardable or
        // sections marked with the first 4 characters PAGE or .eda
        // (for the .edata section) or INIT.
        //

        if (((FoundSection->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0) ||
           (*(PULONG)FoundSection->Name == 'EGAP') ||
           (*(PULONG)FoundSection->Name == 'ade.') ||
           (*(PULONG)FoundSection->Name == 'TINI')) {

            NOTHING;

        }
        else {

            //
            // This section is non-pageable.
            //

            PointerPte = MiGetPteAddress (
                                   (PCHAR)Base + FoundSection->VirtualAddress);

            //
            // Generally, SizeOfRawData is larger than VirtualSize for each
            // section because it includes the padding to get to the subsection
            // alignment boundary.  However, if the image is linked with
            // subsection alignment == native page alignment, the linker will
            // have VirtualSize be much larger than SizeOfRawData because it
            // will account for all the bss.
            //
    
            Span = FoundSection->SizeOfRawData;
    
            if (Span < FoundSection->Misc.VirtualSize) {
                Span = FoundSection->Misc.VirtualSize;
            }

            LastPte = MiGetPteAddress ((PCHAR)Base +
                                       FoundSection->VirtualAddress +
                                       (Span - 1));

            ASSERT (PointerPte <= LastPte);

            MiLockCode (PointerPte, LastPte, MM_LOCK_BY_NONPAGE);
        }
        i -= 1;
        FoundSection += 1;
    }
    return;
}


VOID
MiClearImports (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )
/*++

Routine Description:

    Free up the import list and clear the pointer.  This stops the
    recursion performed in MiDereferenceImports().

Arguments:

    DataTableEntry - provided for the driver.

Return Value:

    Status of the import list construction operation.

--*/

{
    PAGED_CODE();

    if (DataTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {
        return;
    }

    if (DataTableEntry->LoadedImports == (PVOID)NO_IMPORTS_USED) {
        NOTHING;
    }
    else if (SINGLE_ENTRY(DataTableEntry->LoadedImports)) {
        NOTHING;
    }
    else {
        //
        // free the memory
        //
        ExFreePool ((PVOID)DataTableEntry->LoadedImports);
    }

    //
    // stop the recursion
    //
    DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
}

VOID
MiRememberUnloadedDriver (
    IN PUNICODE_STRING DriverName,
    IN PVOID Address,
    IN ULONG Length
    )

/*++

Routine Description:

    This routine saves information about unloaded drivers so that ones that
    forget to delete lookaside lists or queues can be caught.

Arguments:

    DriverName - Supplies a Unicode string containing the driver's name.

    Address - Supplies the address the driver was loaded at.

    Length - Supplies the number of bytes the driver load spanned.

Return Value:

    None.

--*/

{
    PUNLOADED_DRIVERS Entry;
    ULONG NumberOfBytes;

    if (DriverName->Length == 0) {

        //
        // This is an aborted load and the driver name hasn't been filled
        // in yet.  No need to save it.
        //

        return;
    }

    //
    // Serialization is provided by the caller, so just update the list now.
    // Note the allocations are nonpaged so they can be searched at bugcheck
    // time.
    //

    if (MmUnloadedDrivers == NULL) {
        NumberOfBytes = MI_UNLOADED_DRIVERS * sizeof (UNLOADED_DRIVERS);

        MmUnloadedDrivers = (PUNLOADED_DRIVERS)ExAllocatePoolWithTag (NonPagedPool,
                                                                      NumberOfBytes,
                                                                      'TDmM');
        if (MmUnloadedDrivers == NULL) {
            return;
        }
        RtlZeroMemory (MmUnloadedDrivers, NumberOfBytes);
        MmLastUnloadedDriver = 0;
    }
    else if (MmLastUnloadedDriver >= MI_UNLOADED_DRIVERS) {
        MmLastUnloadedDriver = 0;
    }

    Entry = &MmUnloadedDrivers[MmLastUnloadedDriver];

    //
    // Free the old entry as we recycle into the new.
    //

    RtlFreeUnicodeString (&Entry->Name);

    Entry->Name.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                DriverName->Length,
                                                'TDmM');

    if (Entry->Name.Buffer == NULL) {
        Entry->Name.MaximumLength = 0;
        Entry->Name.Length = 0;
        MiUnloadsSkipped += 1;
        return;
    }

    RtlCopyMemory(Entry->Name.Buffer, DriverName->Buffer, DriverName->Length);
    Entry->Name.Length = DriverName->Length;
    Entry->Name.MaximumLength = DriverName->MaximumLength;

    Entry->StartAddress = Address;
    Entry->EndAddress = (PVOID)((PCHAR)Address + Length);

    KeQuerySystemTime (&Entry->CurrentTime);

    MiTotalUnloads += 1;
    MmLastUnloadedDriver += 1;
}

PUNICODE_STRING
MmLocateUnloadedDriver (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine attempts to find the specified virtual address in the
    unloaded driver list.

Arguments:

    VirtualAddress - Supplies a virtual address that might be within a driver
                     that has already unloaded.

Return Value:

    A pointer to a Unicode string containing the unloaded driver's name.

Environment:

    Kernel mode, bugcheck time.

--*/

{
    PUNLOADED_DRIVERS Entry;
    ULONG i;
    ULONG Index;

    //
    // No serialization is needed because we've crashed.
    //

    if (MmUnloadedDrivers == NULL) {
        return NULL;
    }

    Index = MmLastUnloadedDriver - 1;

    for (i = 0; i < MI_UNLOADED_DRIVERS; i += 1) {
        if (Index >= MI_UNLOADED_DRIVERS) {
            Index = MI_UNLOADED_DRIVERS - 1;
        }
        Entry = &MmUnloadedDrivers[Index];
        if (Entry->Name.Buffer != NULL) {
            if ((VirtualAddress >= Entry->StartAddress) &&
                (VirtualAddress < Entry->EndAddress)) {
                    return &Entry->Name;
            }
        }
        Index -= 1;
    }

    return NULL;
}


NTSTATUS
MmUnloadSystemImage (
    IN PVOID ImageHandle
    )

/*++

Routine Description:

    This routine unloads a previously loaded system image and returns
    the allocated resources.

Arguments:

    ImageHandle - Supplies a pointer to the section object of the
                  image to unload.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, APC_LEVEL or below, arbitrary process context.

--*/

{
    PFN_NUMBER PageFrameIndex;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PFN_NUMBER PagesRequired;
    PFN_NUMBER ResidentPages;
    PMMPTE PointerPte;
    PFN_NUMBER NumberOfPtes;
    PFN_NUMBER RoundedNumberOfPtes;
    PVOID BasedAddress;
    SIZE_T NumberOfBytes;
    LOGICAL MustFree;
    SIZE_T CommittedPages;
    LOGICAL ViewDeleted;
    PIMAGE_ENTRY_IN_SESSION DriverImage;
    NTSTATUS Status;
    PSECTION SectionPointer;
    PKTHREAD CurrentThread;

    ViewDeleted = FALSE;
    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)ImageHandle;
    BasedAddress = DataTableEntry->DllBase;

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    if (DataTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {

        //
        // Any driver loaded at boot that did not have its import list
        // and LoadCount reconstructed cannot be unloaded because we don't
        // know how many other drivers may be linked to it.
        //

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        return STATUS_SUCCESS;
    }

    ASSERT (DataTableEntry->LoadCount != 0);

    if (MI_IS_SESSION_IMAGE_ADDRESS (BasedAddress)) {

        //
        // A printer driver may be referenced multiple times for the
        // same session space.  Only unload the last reference.
        //

        DriverImage = MiSessionLookupImage (BasedAddress);

        ASSERT (DriverImage);

        ASSERT (DriverImage->ImageCountInThisSession);

        DriverImage->ImageCountInThisSession -= 1;

        if (DriverImage->ImageCountInThisSession != 0) {

            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);

            return STATUS_SUCCESS;
        }

        //
        // The reference count for this image has dropped to zero in this
        // session, so we can delete this session's view of the image.
        //

        NumberOfBytes = DataTableEntry->SizeOfImage;

        //
        // Free the session space taken up by the image, unmapping it from
        // the current VA space - note this does not remove page table pages
        // from the session PageTables[].  Each data page is only freed
        // if there are no other references to it (ie: from any other
        // sessions).
        //

        PointerPte = MiGetPteAddress (BasedAddress);
        LastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)BasedAddress + NumberOfBytes));

        PagesRequired = MiDeleteSystemPageableVm (PointerPte,
                                                 (PFN_NUMBER)(LastPte - PointerPte),
                                                 MI_DELETE_FLUSH_TB,
                                                 &ResidentPages);

        //
        // Note resident available is returned here without waiting for load
        // count to reach zero because it is charged each time a session space
        // driver locks down its code or data regardless of whether it is really
        // the same copy-on-write backing page(s) that some other session has
        // already locked down.
        //

        MI_INCREMENT_RESIDENT_AVAILABLE (ResidentPages,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE);

        SectionPointer = (PSECTION) DataTableEntry->SectionPointer;

        ASSERT (SectionPointer != NULL);
        ASSERT (SectionPointer->Segment->u1.ImageCommitment != 0);

        if (BasedAddress != SectionPointer->Segment->BasedAddress) {
            CommittedPages = SectionPointer->Segment->TotalNumberOfPtes;
        }
        else {
            CommittedPages = SectionPointer->Segment->u1.ImageCommitment;
        }

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CommittedPages);

        MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGE_UNLOAD,
            (ULONG)CommittedPages);

        ViewDeleted = TRUE;

        //
        // Return the commitment we took out on the pagefile when the
        // image was allocated.
        //

        MiReturnCommitment (CommittedPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD, CommittedPages);

        //
        // Tell the session space image handler that we are releasing
        // our claim to the image.
        //

        ASSERT (DataTableEntry->LoadCount != 0);

        MiRemoveImageSessionWide (DataTableEntry,
                                  BasedAddress,
                                  DataTableEntry->SizeOfImage);

        ASSERT (MiSessionLookupImage (BasedAddress) == NULL);
    }

    ASSERT (DataTableEntry->LoadCount != 0);

    DataTableEntry->LoadCount -= 1;

    if (DataTableEntry->LoadCount != 0) {

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        return STATUS_SUCCESS;
    }

    if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
        VerifierDeadlockFreePool (DataTableEntry->DllBase, DataTableEntry->SizeOfImage);
    }

    if (DataTableEntry->Flags & LDRP_IMAGE_VERIFYING) {
        MiVerifyingDriverUnloading (DataTableEntry);
    }

    if (MiActiveVerifierThunks != 0) {
        MiVerifierCheckThunks (DataTableEntry);
    }

    //
    // Unload symbols from debugger.
    //

    if (DataTableEntry->Flags & LDRP_DEBUG_SYMBOLS_LOADED) {

        //
        //  TEMP TEMP TEMP rip out when debugger converted
        //

        ANSI_STRING AnsiName;

        Status = RtlUnicodeStringToAnsiString (&AnsiName,
                                               &DataTableEntry->BaseDllName,
                                               TRUE);

        if (NT_SUCCESS (Status)) {
            DbgUnLoadImageSymbols (&AnsiName, BasedAddress, (ULONG)-1);
            RtlFreeAnsiString (&AnsiName);
        }
    }

    //
    // No unload can happen till after Mm has finished Phase 1 initialization.
    // Therefore, large pages are already in effect (if this platform supports
    // it).
    //

    if (ViewDeleted == FALSE) {

        NumberOfPtes = DataTableEntry->SizeOfImage >> PAGE_SHIFT;

        if (MmSnapUnloads) {
            MiRememberUnloadedDriver (&DataTableEntry->BaseDllName,
                                      BasedAddress,
                                      (ULONG)(NumberOfPtes << PAGE_SHIFT));
        }

        if (DataTableEntry->Flags & LDRP_SYSTEM_MAPPED) {

            if (MI_PDE_MAPS_LARGE_PAGE (MiGetPdeAddress (BasedAddress))) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPdeAddress (BasedAddress)) + MiGetPteOffset (BasedAddress);

                RoundedNumberOfPtes = MI_ROUND_TO_SIZE (NumberOfPtes,
                                      MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT);
                MiUnmapLargePages (BasedAddress,
                                   RoundedNumberOfPtes << PAGE_SHIFT);

                //
                // MiFreeContiguousPages is going to return commitment
                // and resident available so don't do it here.
                //

                MiRemoveCachedRange (PageFrameIndex, PageFrameIndex + RoundedNumberOfPtes - 1);
                MiFreeContiguousPages (PageFrameIndex, RoundedNumberOfPtes);
                PagesRequired = NumberOfPtes;
            }
            else {
                PointerPte = MiGetPteAddress (BasedAddress);

                //
                // No explicit TB flush is done here, instead the system PTE
                // management code will handle this in a lazy fashion.
                //

                PagesRequired = MiDeleteSystemPageableVm (PointerPte,
                                                         NumberOfPtes,
                                                         0,
                                                         &ResidentPages);

                //
                // Note that drivers loaded at boot that have not been relocated
                // have no system PTEs or commit charged.
                //

                MiReleaseSystemPtes (PointerPte,
                                     (ULONG)NumberOfPtes,
                                     SystemPteSpace);

                MI_INCREMENT_RESIDENT_AVAILABLE (ResidentPages,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1);

                MiReturnCommitment (PagesRequired);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1, PagesRequired);

                InterlockedExchangeAdd (&MmTotalSystemDriverPages,
                    0 - (ULONG)(PagesRequired - ResidentPages));
            }

            if (DataTableEntry->SectionPointer != NULL) {
                InterlockedExchangeAdd ((PLONG)&MmDriverCommit,
                                        (LONG) (0 - PagesRequired));
            }
        }
        else {

            //
            // This must be a boot driver that was not relocated into
            // system PTEs.  If large or super pages are enabled, the
            // image pages must be freed without referencing the
            // non-existent page table pages.  If large/super pages are
            // not enabled, note that system PTEs were not used to map the
            // image and thus, cannot be freed.

            //
            // This is further complicated by the fact that the INIT and/or
            // discardable portions of these images may have already been freed.
            //

            MI_INCREMENT_RESIDENT_AVAILABLE (NumberOfPtes,
                                     MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1);

            MiReturnCommitment (NumberOfPtes);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1, NumberOfPtes);
        }
    }

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible an entry is not in the
    // list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    if (DataTableEntry->InLoadOrderLinks.Flink != NULL) {
        MiProcessLoaderEntry (DataTableEntry, FALSE);
        MustFree = TRUE;
    }
    else {
        MustFree = FALSE;
    }

    //
    // Handle unloading of any dependent DLLs that we loaded automatically
    // for this image.
    //

    MiDereferenceImports ((PLOAD_IMPORTS)DataTableEntry->LoadedImports);

    MiClearImports (DataTableEntry);

    //
    // Free this loader entry.
    //

    if (MustFree == TRUE) {

        if (DataTableEntry->FullDllName.Buffer != NULL) {
            ExFreePool (DataTableEntry->FullDllName.Buffer);
        }

        //
        // Dereference the section object (session images only).
        //

        if (DataTableEntry->SectionPointer != NULL) {
            ObDereferenceObject (DataTableEntry->SectionPointer);
        }

        if (DataTableEntry->PatchInformation) {
            MiRundownHotpatchList ((PVOID)DataTableEntry->PatchInformation);
        }

        ExFreePool (DataTableEntry);
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    return STATUS_SUCCESS;
}


NTSTATUS
MiBuildImportsForBootDrivers (
    VOID
    )

/*++

Routine Description:

    Construct an import list chain for boot-loaded drivers.
    If this cannot be done for an entry, its chain is set to LOADED_AT_BOOT.

    If a chain can be successfully built, then this driver's DLLs
    will be automatically unloaded if this driver goes away (provided
    no other driver is also using them).  Otherwise, on driver unload,
    its dependent DLLs would have to be explicitly unloaded.

    Note that the incoming LoadCount values are not correct and thus, they
    are reinitialized here.

Arguments:

    None.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry2;
    PLIST_ENTRY NextEntry2;
    ULONG i;
    ULONG j;
    ULONG ImageCount;
    PVOID *ImageReferences;
    PVOID LastImageReference;
    PULONG_PTR ImportThunk;
    ULONG_PTR BaseAddress;
    ULONG_PTR LastAddress;
    ULONG ImportSize;
    ULONG ImportListSize;
    PLOAD_IMPORTS ImportList;
    LOGICAL UndoEverything;
    PKLDR_DATA_TABLE_ENTRY KernelDataTableEntry;
    PKLDR_DATA_TABLE_ENTRY HalDataTableEntry;
    UNICODE_STRING KernelString;
    UNICODE_STRING HalString;

    PAGED_CODE();

    ImageCount = 0;

    KernelDataTableEntry = NULL;
    HalDataTableEntry = NULL;

#define KERNEL_NAME L"ntoskrnl.exe"

    KernelString.Buffer = (const PUSHORT) KERNEL_NAME;
    KernelString.Length = sizeof (KERNEL_NAME) - sizeof (WCHAR);
    KernelString.MaximumLength = sizeof KERNEL_NAME;

#define HAL_NAME L"hal.dll"

    HalString.Buffer = (const PUSHORT) HAL_NAME;
    HalString.Length = sizeof (HAL_NAME) - sizeof (WCHAR);
    HalString.MaximumLength = sizeof HAL_NAME;

    NextEntry = PsLoadedModuleList.Flink;

    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        if (RtlEqualUnicodeString (&KernelString,
                                   &DataTableEntry->BaseDllName,
                                   TRUE)) {

            KernelDataTableEntry = CONTAINING_RECORD(NextEntry,
                                                     KLDR_DATA_TABLE_ENTRY,
                                                     InLoadOrderLinks);
        }
        else if (RtlEqualUnicodeString (&HalString,
                                        &DataTableEntry->BaseDllName,
                                        TRUE)) {

            HalDataTableEntry = CONTAINING_RECORD(NextEntry,
                                                  KLDR_DATA_TABLE_ENTRY,
                                                  InLoadOrderLinks);
        }

        //
        // Initialize these properly so error recovery is simplified.
        //

        if (DataTableEntry->Flags & LDRP_DRIVER_DEPENDENT_DLL) {
            if ((DataTableEntry == HalDataTableEntry) || (DataTableEntry == KernelDataTableEntry)) {
                DataTableEntry->LoadCount = 1;
            }
            else {
                DataTableEntry->LoadCount = 0;
            }
        }
        else {
            DataTableEntry->LoadCount = 1;
        }

        DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;

        ImageCount += 1;
        NextEntry = NextEntry->Flink;
    }

    if (KernelDataTableEntry == NULL || HalDataTableEntry == NULL) {
        return STATUS_NOT_FOUND;
    }

    ImageReferences = (PVOID *) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                       ImageCount * sizeof (PVOID),
                                                       'TDmM');

    if (ImageReferences == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    UndoEverything = FALSE;

    NextEntry = PsLoadedModuleList.Flink;

    for ( ; NextEntry != &PsLoadedModuleList; NextEntry = NextEntry->Flink) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        ImportThunk = (PULONG_PTR)RtlImageDirectoryEntryToData (
                                           DataTableEntry->DllBase,
                                           TRUE,
                                           IMAGE_DIRECTORY_ENTRY_IAT,
                                           &ImportSize);

        if (ImportThunk == NULL) {
            DataTableEntry->LoadedImports = NO_IMPORTS_USED;
            continue;
        }

        RtlZeroMemory (ImageReferences, ImageCount * sizeof (PVOID));

        ImportSize /= sizeof(PULONG_PTR);

        BaseAddress = 0;

        //
        // Initializing these locals is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        j = 0;
        LastAddress = 0;

        for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {

            //
            // Check the hint first.
            //

            if (BaseAddress != 0) {
                if (*ImportThunk >= BaseAddress && *ImportThunk < LastAddress) {
                    ASSERT (ImageReferences[j]);
                    continue;
                }
            }

            j = 0;
            NextEntry2 = PsLoadedModuleList.Flink;

            while (NextEntry2 != &PsLoadedModuleList) {

                DataTableEntry2 = CONTAINING_RECORD(NextEntry2,
                                                    KLDR_DATA_TABLE_ENTRY,
                                                    InLoadOrderLinks);

                BaseAddress = (ULONG_PTR) DataTableEntry2->DllBase;
                LastAddress = BaseAddress + DataTableEntry2->SizeOfImage;

                if (*ImportThunk >= BaseAddress && *ImportThunk < LastAddress) {
                    ImageReferences[j] = DataTableEntry2;
                    break;
                }

                NextEntry2 = NextEntry2->Flink;
                j += 1;
            }

            if (*ImportThunk < BaseAddress || *ImportThunk >= LastAddress) {
                if (*ImportThunk) {
#if DBG
                    DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                        "MM: broken import linkage %p %p %p\n",
                        DataTableEntry,
                        ImportThunk,
                        *ImportThunk);
                    DbgBreakPoint ();
#endif
                    UndoEverything = TRUE;
                    goto finished;
                }

                BaseAddress = 0;
            }
        }

        ImportSize = 0;

        //
        // Initializing LastImageReference is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        LastImageReference = NULL;

        for (i = 0; i < ImageCount; i += 1) {

            if ((ImageReferences[i] != NULL) &&
                (ImageReferences[i] != KernelDataTableEntry) &&
                (ImageReferences[i] != HalDataTableEntry)) {

                    LastImageReference = ImageReferences[i];
                    ImportSize += 1;
            }
        }

        if (ImportSize == 0) {
            DataTableEntry->LoadedImports = NO_IMPORTS_USED;
        }
        else if (ImportSize == 1) {
            DataTableEntry->LoadedImports = POINTER_TO_SINGLE_ENTRY (LastImageReference);
            ((PKLDR_DATA_TABLE_ENTRY)LastImageReference)->LoadCount += 1;
        }
        else {
            ImportListSize = ImportSize * sizeof(PVOID) + sizeof(SIZE_T);

            ImportList = (PLOAD_IMPORTS) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                                ImportListSize,
                                                                'TDmM');

            if (ImportList == NULL) {
                UndoEverything = TRUE;
                break;
            }

            ImportList->Count = ImportSize;

            j = 0;
            for (i = 0; i < ImageCount; i += 1) {

                if ((ImageReferences[i] != NULL) &&
                    (ImageReferences[i] != KernelDataTableEntry) &&
                    (ImageReferences[i] != HalDataTableEntry)) {

                        ImportList->Entry[j] = ImageReferences[i];
                        ((PKLDR_DATA_TABLE_ENTRY)ImageReferences[i])->LoadCount += 1;
                        j += 1;
                }
            }

            ASSERT (j == ImportSize);

            DataTableEntry->LoadedImports = ImportList;
        }
    }

finished:

    ExFreePool ((PVOID)ImageReferences);

    //
    // The kernel and HAL are never unloaded.
    //

    if ((KernelDataTableEntry->LoadedImports != NO_IMPORTS_USED) &&
        (!POINTER_TO_SINGLE_ENTRY(KernelDataTableEntry->LoadedImports))) {
            ExFreePool ((PVOID)KernelDataTableEntry->LoadedImports);
    }

    if ((HalDataTableEntry->LoadedImports != NO_IMPORTS_USED) &&
        (!POINTER_TO_SINGLE_ENTRY(HalDataTableEntry->LoadedImports))) {
            ExFreePool ((PVOID)HalDataTableEntry->LoadedImports);
    }

    KernelDataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
    HalDataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;

    if (UndoEverything == TRUE) {

        //
        // An error occurred and this is an all or nothing operation so
        // roll everything back.
        //

        NextEntry = PsLoadedModuleList.Flink;
        while (NextEntry != &PsLoadedModuleList) {
            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            ImportList = DataTableEntry->LoadedImports;
            if (ImportList == LOADED_AT_BOOT || ImportList == NO_IMPORTS_USED ||
                SINGLE_ENTRY(ImportList)) {
                    NOTHING;
            }
            else {
                ExFreePool (ImportList);
            }

            DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
            DataTableEntry->LoadCount = 1;
            NextEntry = NextEntry->Flink;
        }

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    return STATUS_SUCCESS;
}


LOGICAL
MiCallDllUnloadAndUnloadDll(
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    All the references from other drivers to this DLL have been cleared.
    The only remaining issue is that this DLL must support being unloaded.
    This means having no outstanding DPCs, allocated pool, etc.

    If the DLL has an unload routine that returns SUCCESS, then we clean
    it up and free up its memory now.

    Note this routine is NEVER called for drivers - only for DLLs that were
    loaded due to import references from various drivers.

Arguments:

    DataTableEntry - provided for the DLL.

Return Value:

    TRUE if the DLL was successfully unloaded, FALSE if not.

--*/

{
    PMM_DLL_UNLOAD Func;
    NTSTATUS Status;
    LOGICAL Unloaded;

    PAGED_CODE();

    Unloaded = FALSE;

    Func = (PMM_DLL_UNLOAD) (ULONG_PTR) MiLocateExportName (DataTableEntry->DllBase, "DllUnload");

    if (Func) {

        //
        // The unload function was found in the DLL so unload it now.
        //

        Status = Func();

        if (NT_SUCCESS(Status)) {

            //
            // Set up the reference count so the import DLL looks like a regular
            // driver image is being unloaded.
            //

            ASSERT (DataTableEntry->LoadCount == 0);
            DataTableEntry->LoadCount = 1;

            MmUnloadSystemImage ((PVOID)DataTableEntry);
            Unloaded = TRUE;
        }
    }

    return Unloaded;
}


PVOID
MiLocateExportName (
    IN PVOID DllBase,
    IN PCHAR FunctionName
    )

/*++

Routine Description:

    This function is invoked to locate a function name in an export directory.

Arguments:

    DllBase - Supplies the image base.

    FunctionName - Supplies the the name to be located.

Return Value:

    The address of the located function or NULL.

--*/

{
    PVOID Func;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;
    PULONG Addr;
    ULONG ExportSize;
    LONG Low;
    LONG Middle;
    LONG High;
    LONG Result;
    USHORT OrdinalNumber;

    PAGED_CODE();

    Func = NULL;

    //
    // Locate the DLL's export directory.
    //

    ExportDirectory = (PIMAGE_EXPORT_DIRECTORY) RtlImageDirectoryEntryToData (
                                DllBase,
                                TRUE,
                                IMAGE_DIRECTORY_ENTRY_EXPORT,
                                &ExportSize);

    if (ExportDirectory) {

        NameTableBase =  (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNames);
        NameOrdinalTableBase = (PUSHORT)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

        //
        // Look in the export name table for the specified function name.
        //

        Low = 0;
        Middle = 0;
        High = ExportDirectory->NumberOfNames - 1;

        while (High >= Low) {

            //
            // Compute the next probe index and compare the export name entry
            // with the specified function name.
            //

            Middle = (Low + High) >> 1;
            Result = strcmp(FunctionName,
                            (PCHAR)((PCHAR)DllBase + NameTableBase[Middle]));

            if (Result < 0) {
                High = Middle - 1;
            }
            else if (Result > 0) {
                Low = Middle + 1;
            }
            else {
                break;
            }
        }

        //
        // If the high index is less than the low index, then a matching table
        // entry was not found.  Otherwise, get the ordinal number from the
        // ordinal table and location the function address.
        //

        if (High >= Low) {

            OrdinalNumber = NameOrdinalTableBase[Middle];
            Addr = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfFunctions);
            Func = (PVOID)((ULONG_PTR)DllBase + Addr[OrdinalNumber]);

            //
            // If the function address is w/in range of the export directory,
            // then the function is forwarded, which is not allowed, so ignore
            // it.
            //

            if ((ULONG_PTR)Func > (ULONG_PTR)ExportDirectory &&
                (ULONG_PTR)Func < ((ULONG_PTR)ExportDirectory + ExportSize)) {
                Func = NULL;
            }
        }
    }

    return Func;
}


NTSTATUS
MiDereferenceImports (
    IN PLOAD_IMPORTS ImportList
    )

/*++

Routine Description:

    Decrement the reference count on each DLL specified in the image import
    list.  If any DLL's reference count reaches zero, then free the DLL.

    No locks may be held on entry as MmUnloadSystemImage may be called.

    The parameter list is freed here as well.

Arguments:

    ImportList - Supplies the list of DLLs to dereference.

Return Value:

    Status of the dereference operation.

--*/

{
    ULONG i;
    LOGICAL Unloaded;
    PVOID SavedImports;
    LOAD_IMPORTS SingleTableEntry;
    PKLDR_DATA_TABLE_ENTRY ImportTableEntry;

    PAGED_CODE();

    if (ImportList == LOADED_AT_BOOT || ImportList == NO_IMPORTS_USED) {
        return STATUS_SUCCESS;
    }

    if (SINGLE_ENTRY(ImportList)) {
        SingleTableEntry.Count = 1;
        SingleTableEntry.Entry[0] = SINGLE_ENTRY_TO_POINTER(ImportList);
        ImportList = &SingleTableEntry;
    }

    for (i = 0; i < ImportList->Count && ImportList->Entry[i]; i += 1) {
        ImportTableEntry = ImportList->Entry[i];

        if (ImportTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {

            //
            // Skip this one - it was loaded by ntldr.
            //

            continue;
        }

#if DBG
        {
            ULONG ImageCount;
            PLIST_ENTRY NextEntry;
            PKLDR_DATA_TABLE_ENTRY DataTableEntry;

            //
            // Assert that the first 2 entries are never dereferenced as
            // unloading the kernel or HAL would be fatal.
            //

            NextEntry = PsLoadedModuleList.Flink;

            ImageCount = 0;
            while (NextEntry != &PsLoadedModuleList && ImageCount < 2) {
                DataTableEntry = CONTAINING_RECORD(NextEntry,
                                                   KLDR_DATA_TABLE_ENTRY,
                                                   InLoadOrderLinks);
                ASSERT (ImportTableEntry != DataTableEntry);
                ASSERT (DataTableEntry->LoadCount == 1);
                NextEntry = NextEntry->Flink;
                ImageCount += 1;
            }
        }
#endif

        ASSERT (ImportTableEntry->LoadCount >= 1);

        ImportTableEntry->LoadCount -= 1;

        if (ImportTableEntry->LoadCount == 0) {

            //
            // Unload this dependent DLL - we only do this to non-referenced
            // non-boot-loaded drivers.  Stop the import list recursion prior
            // to unloading - we know we're done at this point.
            //
            // Note we can continue on afterwards without restarting
            // regardless of which locks get released and reacquired
            // because this chain is private.
            //

            SavedImports = ImportTableEntry->LoadedImports;

            ImportTableEntry->LoadedImports = (PVOID)NO_IMPORTS_USED;

            Unloaded = MiCallDllUnloadAndUnloadDll ((PVOID)ImportTableEntry);

            if (Unloaded == TRUE) {

                //
                // This DLL was unloaded so recurse through its imports and
                // attempt to unload all of those too.
                //

                MiDereferenceImports ((PLOAD_IMPORTS)SavedImports);

                if ((SavedImports != (PVOID)LOADED_AT_BOOT) &&
                    (SavedImports != (PVOID)NO_IMPORTS_USED) &&
                    (!SINGLE_ENTRY(SavedImports))) {

                        ExFreePool (SavedImports);
                }
            }
            else {
                ImportTableEntry->LoadedImports = SavedImports;
            }
        }
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MiResolveImageReferences (
    PVOID ImageBase,
    IN PUNICODE_STRING ImageFileDirectory,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    OUT PCHAR *MissingProcedureName,
    OUT PWSTR *MissingDriverName,
    OUT PLOAD_IMPORTS *LoadedImports
    )

/*++

Routine Description:

    This routine resolves the references from the newly loaded driver
    to the kernel, HAL and other drivers.

Arguments:

    ImageBase - Supplies the address of which the image header resides.

    ImageFileDirectory - Supplies the directory to load referenced DLLs.

Return Value:

    Status of the image reference resolution.

--*/

{
    PCHAR MissingProcedureStorageArea;
    PVOID ImportBase;
    ULONG ImportSize;
    ULONG ImportListSize;
    ULONG Count;
    ULONG i;
    PIMAGE_IMPORT_DESCRIPTOR ImportDescriptor;
    PIMAGE_IMPORT_DESCRIPTOR Imp;
    NTSTATUS st;
    ULONG ExportSize;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;
    PIMAGE_THUNK_DATA NameThunk;
    PIMAGE_THUNK_DATA AddrThunk;
    PSZ ImportName;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY SingleEntry;
    ANSI_STRING AnsiString;
    UNICODE_STRING ImportName_U;
    UNICODE_STRING ImportDescriptorName_U;
    UNICODE_STRING DllToLoad;
    UNICODE_STRING DllToLoad2;
    PVOID Section;
    PVOID BaseAddress;
    LOGICAL PrefixedNameAllocated;
    LOGICAL ReferenceImport;
    ULONG LinkWin32k = 0;
    ULONG LinkNonWin32k = 0;
    PLOAD_IMPORTS ImportList;
    PLOAD_IMPORTS CompactedImportList;
    LOGICAL Loaded;
    UNICODE_STRING DriverDirectory;

    PAGED_CODE();

    *LoadedImports = NO_IMPORTS_USED;

    MissingProcedureStorageArea = *MissingProcedureName;

    ImportDescriptor = (PIMAGE_IMPORT_DESCRIPTOR) RtlImageDirectoryEntryToData (
                        ImageBase,
                        TRUE,
                        IMAGE_DIRECTORY_ENTRY_IMPORT,
                        &ImportSize);

    if (ImportDescriptor == NULL) {
        return STATUS_SUCCESS;
    }

    // Count the number of imports so we can allocate enough room to
    // store them all chained off this module's LDR_DATA_TABLE_ENTRY.
    //

    Count = 0;
    for (Imp = ImportDescriptor; Imp->Name && Imp->OriginalFirstThunk; Imp += 1) {
        Count += 1;
    }

    if (Count != 0) {
        ImportListSize = Count * sizeof(PVOID) + sizeof(SIZE_T);

        ImportList = (PLOAD_IMPORTS) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                             ImportListSize,
                                             'TDmM');

        //
        // Zero it so we can recover gracefully if we fail in the middle.
        // If the allocation failed, just don't build the import list.
        //

        if (ImportList != NULL) {
            RtlZeroMemory (ImportList, ImportListSize);
            ImportList->Count = Count;
        }
    }
    else {
        ImportList = NULL;
    }

    Count = 0;
    while (ImportDescriptor->Name && ImportDescriptor->OriginalFirstThunk) {

        ImportName = (PSZ)((PCHAR)ImageBase + ImportDescriptor->Name);

        //
        // A driver can link with win32k.sys if and only if it is a GDI
        // driver.
        // Also display drivers can only link to win32k.sys (and BBT ...).
        //
        // So if we get a driver that links to win32k.sys and has more
        // than one set of imports, we will fail to load it.
        //

        LinkWin32k = LinkWin32k |
             (!_strnicmp(ImportName, "win32k", sizeof("win32k") - 1));

        //
        // We don't want to count coverage, win32k and irt (BBT) since
        // display drivers CAN link against these.
        //

        LinkNonWin32k = LinkNonWin32k |
            ((_strnicmp(ImportName, "win32k", sizeof("win32k") - 1)) &&

#if defined(_AMD64_)

             (_strnicmp(ImportName, "ntoskrnl", sizeof("ntoskrnl") - 1)) &&

#endif

             (_strnicmp(ImportName, "dxapi", sizeof("dxapi") - 1)) &&
             (_strnicmp(ImportName, "coverage", sizeof("coverage") - 1)) &&
             (_strnicmp(ImportName, "irt", sizeof("irt") - 1)));


        if (LinkNonWin32k && LinkWin32k) {
            MiDereferenceImports (ImportList);
            if (ImportList) {
                ExFreePool (ImportList);
            }
            return STATUS_PROCEDURE_NOT_FOUND;
        }

        if ((!_strnicmp(ImportName, "ntdll",    sizeof("ntdll") - 1))    ||
            (!_strnicmp(ImportName, "winsrv",   sizeof("winsrv") - 1))   ||
            (!_strnicmp(ImportName, "advapi32", sizeof("advapi32") - 1)) ||
            (!_strnicmp(ImportName, "kernel32", sizeof("kernel32") - 1)) ||
            (!_strnicmp(ImportName, "user32",   sizeof("user32") - 1))   ||
            (!_strnicmp(ImportName, "gdi32",    sizeof("gdi32") - 1)) ) {

            MiDereferenceImports (ImportList);

            if (ImportList) {
                ExFreePool (ImportList);
            }
            return STATUS_PROCEDURE_NOT_FOUND;
        }

        if ((!_strnicmp(ImportName, "ntoskrnl", sizeof("ntoskrnl") - 1)) ||
            (!_strnicmp(ImportName, "win32k", sizeof("win32k") - 1))     ||
            (!_strnicmp(ImportName, "hal",   sizeof("hal") - 1))) {

            //
            // These imports don't get refcounted because we don't
            // ever want to unload them.
            //

            ReferenceImport = FALSE;
        }
        else {
            ReferenceImport = TRUE;
        }

        RtlInitAnsiString (&AnsiString, ImportName);
        st = RtlAnsiStringToUnicodeString (&ImportName_U, &AnsiString, TRUE);

        if (!NT_SUCCESS(st)) {
            MiDereferenceImports (ImportList);
            if (ImportList != NULL) {
                ExFreePool (ImportList);
            }
            return st;
        }

        if (NamePrefix &&
            (_strnicmp(ImportName, "ntoskrnl", sizeof("ntoskrnl") - 1) &&
             _strnicmp(ImportName, "hal", sizeof("hal") - 1))) {

            ImportDescriptorName_U.MaximumLength = (USHORT)(ImportName_U.Length + NamePrefix->Length);
            ImportDescriptorName_U.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                ImportDescriptorName_U.MaximumLength,
                                                'TDmM');
            if (!ImportDescriptorName_U.Buffer) {
                RtlFreeUnicodeString (&ImportName_U);
                MiDereferenceImports (ImportList);
                if (ImportList != NULL) {
                    ExFreePool (ImportList);
                }
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            ImportDescriptorName_U.Length = 0;
            RtlAppendUnicodeStringToString(&ImportDescriptorName_U, NamePrefix);
            RtlAppendUnicodeStringToString(&ImportDescriptorName_U, &ImportName_U);
            PrefixedNameAllocated = TRUE;
        }
        else {
            ImportDescriptorName_U = ImportName_U;
            PrefixedNameAllocated = FALSE;
        }

        Loaded = FALSE;

ReCheck:
        NextEntry = PsLoadedModuleList.Flink;
        ImportBase = NULL;

        //
        // Initializing DataTableEntry is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        DataTableEntry = NULL;

        while (NextEntry != &PsLoadedModuleList) {

            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            if (RtlEqualUnicodeString (&ImportDescriptorName_U,
                                       &DataTableEntry->BaseDllName,
                                       TRUE)) {

                ImportBase = DataTableEntry->DllBase;

                //
                // Only bump the LoadCount if this thread did not initiate
                // the load below.  If this thread initiated the load, then
                // the LoadCount has already been bumped as part of the
                // load - we only want to increment it here if we are
                // "attaching" to a previously loaded DLL.
                //

                if ((Loaded == FALSE) && (ReferenceImport == TRUE)) {

                    //
                    // Only increment the load count on the import if it is not
                    // circular (ie: the import is not from the original
                    // caller).
                    //

                    if ((DataTableEntry->Flags & LDRP_LOAD_IN_PROGRESS) == 0) {
                        DataTableEntry->LoadCount += 1;
                    }
                }

                break;
            }
            NextEntry = NextEntry->Flink;
        }

        if (ImportBase == NULL) {

            //
            // The DLL name was not located, attempt to load this dll.
            //

            DllToLoad.MaximumLength = (USHORT)(ImportName_U.Length +
                                        ImageFileDirectory->Length +
                                        sizeof(WCHAR));

            DllToLoad.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                               DllToLoad.MaximumLength,
                                               'TDmM');

            if (DllToLoad.Buffer) {
                DllToLoad.Length = ImageFileDirectory->Length;
                RtlCopyMemory (DllToLoad.Buffer,
                               ImageFileDirectory->Buffer,
                               ImageFileDirectory->Length);

                RtlAppendStringToString ((PSTRING)&DllToLoad,
                                         (PSTRING)&ImportName_U);

                //
                // Add NULL termination in case the load fails so the name
                // can be returned as the PWSTR MissingDriverName.
                //

                DllToLoad.Buffer[(DllToLoad.MaximumLength - 1) / sizeof (WCHAR)] =
                    UNICODE_NULL;

                st = MmLoadSystemImage (&DllToLoad,
                                        NamePrefix,
                                        NULL,
                                        FALSE,
                                        &Section,
                                        &BaseAddress);

                if (NT_SUCCESS(st)) {

                    //
                    // No need to keep the temporary name buffer around now
                    // that there is a loaded module list entry for this DLL.
                    //

                    ExFreePool (DllToLoad.Buffer);
                }
                else {

                    if ((st == STATUS_OBJECT_NAME_NOT_FOUND) &&
                        (NamePrefix == NULL) &&
                        (MI_IS_SESSION_ADDRESS (ImageBase))) {

#define DRIVERS_SUBDIR_NAME L"drivers\\"

                        DriverDirectory.Buffer = (const PUSHORT) DRIVERS_SUBDIR_NAME;
                        DriverDirectory.Length = sizeof (DRIVERS_SUBDIR_NAME) - sizeof (WCHAR);
                        DriverDirectory.MaximumLength = sizeof DRIVERS_SUBDIR_NAME;

                        //
                        // The DLL file was not located, attempt to load it
                        // from the drivers subdirectory.  This makes it
                        // possible for drivers like win32k.sys to link to
                        // drivers that reside in the drivers subdirectory
                        // (like dxapi.sys).
                        //

                        DllToLoad2.MaximumLength = (USHORT)(ImportName_U.Length +
                                                    DriverDirectory.Length +
                                                    ImageFileDirectory->Length +
                                                    sizeof(WCHAR));

                        DllToLoad2.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                           DllToLoad2.MaximumLength,
                                                           'TDmM');

                        if (DllToLoad2.Buffer) {
                            DllToLoad2.Length = ImageFileDirectory->Length;
                            RtlCopyMemory (DllToLoad2.Buffer,
                                           ImageFileDirectory->Buffer,
                                           ImageFileDirectory->Length);

                            RtlAppendStringToString ((PSTRING)&DllToLoad2,
                                                     (PSTRING)&DriverDirectory);

                            RtlAppendStringToString ((PSTRING)&DllToLoad2,
                                                     (PSTRING)&ImportName_U);

                            //
                            // Add NULL termination in case the load fails
                            // so the name can be returned as the PWSTR
                            // MissingDriverName.
                            //

                            DllToLoad2.Buffer[(DllToLoad2.MaximumLength - 1) / sizeof (WCHAR)] =
                                UNICODE_NULL;

                            st = MmLoadSystemImage (&DllToLoad2,
                                                    NULL,
                                                    NULL,
                                                    FALSE,
                                                    &Section,
                                                    &BaseAddress);

                            ExFreePool (DllToLoad.Buffer);

                            DllToLoad.Buffer = DllToLoad2.Buffer;
                            DllToLoad.Length = DllToLoad2.Length;
                            DllToLoad.MaximumLength = DllToLoad2.MaximumLength;

                            if (NT_SUCCESS(st)) {
                                ExFreePool (DllToLoad.Buffer);
                                goto LoadFinished;
                            }
                        }
                        else {
                            Section = NULL;
                            BaseAddress = NULL;
                            st = STATUS_INSUFFICIENT_RESOURCES;
                            goto LoadFinished;
                        }
                    }

                    //
                    // Return the temporary name buffer to our caller so
                    // the name of the DLL that failed to load can be displayed.
                    // Set the low bit of the pointer so our caller knows to
                    // free this buffer when he's done displaying it (as opposed
                    // to loaded module list entries which should not be freed).
                    //

                    *MissingDriverName = DllToLoad.Buffer;
                    *(PULONG)MissingDriverName |= 0x1;

                    //
                    // Set this to NULL so the hard error prints properly.
                    //

                    *MissingProcedureName = NULL;
                }
            }
            else {

                //
                // Initializing Section and BaseAddress is not needed for
                // correctness but without it the compiler cannot compile
                // this code W4 to check for use of uninitialized variables.
                //

                Section = NULL;
                BaseAddress = NULL;
                st = STATUS_INSUFFICIENT_RESOURCES;
            }

LoadFinished:

            //
            // Call any needed DLL initialization now.
            //

            if (NT_SUCCESS(st)) {
#if DBG
                PLIST_ENTRY Entry;
#endif
                PKLDR_DATA_TABLE_ENTRY TableEntry;

                Loaded = TRUE;

                TableEntry = (PKLDR_DATA_TABLE_ENTRY) Section;
                ASSERT (BaseAddress == TableEntry->DllBase);

#if DBG
                //
                // Lookup the dll's table entry in the loaded module list.
                // This is expected to always succeed.
                //

                Entry = PsLoadedModuleList.Blink;
                while (Entry != &PsLoadedModuleList) {
                    TableEntry = CONTAINING_RECORD (Entry,
                                                    KLDR_DATA_TABLE_ENTRY,
                                                    InLoadOrderLinks);

                    if (BaseAddress == TableEntry->DllBase) {
                        ASSERT (TableEntry == (PKLDR_DATA_TABLE_ENTRY) Section);
                        break;
                    }
                    ASSERT (TableEntry != (PKLDR_DATA_TABLE_ENTRY) Section);
                    Entry = Entry->Blink;
                }

                ASSERT (Entry != &PsLoadedModuleList);
#endif

                //
                // Call the dll's initialization routine if it has
                // one.  This routine will reapply verifier thunks to
                // any modules that link to this one if necessary.
                //

                st = MmCallDllInitialize (TableEntry, &PsLoadedModuleList);

                //
                // If the module could not be properly initialized,
                // unload it.
                //

                if (!NT_SUCCESS(st)) {
                    MmUnloadSystemImage ((PVOID)TableEntry);
                    Loaded = FALSE;
                }
            }

            if (!NT_SUCCESS(st)) {

                RtlFreeUnicodeString (&ImportName_U);
                if (PrefixedNameAllocated == TRUE) {
                    ExFreePool (ImportDescriptorName_U.Buffer);
                }
                MiDereferenceImports (ImportList);
                if (ImportList != NULL) {
                    ExFreePool (ImportList);
                }
                return st;
            }

            goto ReCheck;
        }

        if ((ReferenceImport == TRUE) && (ImportList)) {

            //
            // Only add the image providing satisfying our imports to the
            // import list if the reference is not circular (ie: the import
            // is not from the original caller).
            //

            if ((DataTableEntry->Flags & LDRP_LOAD_IN_PROGRESS) == 0) {
                ImportList->Entry[Count] = DataTableEntry;
                Count += 1;
            }
        }

        RtlFreeUnicodeString (&ImportName_U);
        if (PrefixedNameAllocated) {
            ExFreePool (ImportDescriptorName_U.Buffer);
        }

        *MissingDriverName = DataTableEntry->BaseDllName.Buffer;

        ExportDirectory = (PIMAGE_EXPORT_DIRECTORY) RtlImageDirectoryEntryToData (
                                    ImportBase,
                                    TRUE,
                                    IMAGE_DIRECTORY_ENTRY_EXPORT,
                                    &ExportSize);

        if (!ExportDirectory) {
            MiDereferenceImports (ImportList);
            if (ImportList) {
                ExFreePool (ImportList);
            }
            return STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;
        }

        //
        // Walk through the IAT and snap all the thunks.
        //

        if (ImportDescriptor->OriginalFirstThunk) {

            NameThunk = (PIMAGE_THUNK_DATA)((PCHAR)ImageBase + (ULONG)ImportDescriptor->OriginalFirstThunk);
            AddrThunk = (PIMAGE_THUNK_DATA)((PCHAR)ImageBase + (ULONG)ImportDescriptor->FirstThunk);

            while (NameThunk->u1.AddressOfData) {

                st = MiSnapThunk (ImportBase,
                                  ImageBase,
                                  NameThunk++,
                                  AddrThunk++,
                                  ExportDirectory,
                                  ExportSize,
                                  FALSE,
                                  MissingProcedureName);

                if (!NT_SUCCESS(st) ) {
                    MiDereferenceImports (ImportList);
                    if (ImportList) {
                        ExFreePool (ImportList);
                    }
                    return st;
                }
                *MissingProcedureName = MissingProcedureStorageArea;
            }
        }

        ImportDescriptor += 1;
    }

    //
    // All the imports are successfully loaded so establish and compact
    // the import unload list.
    //

    if (ImportList) {

        //
        // Blank entries occur for things like the kernel, HAL & win32k.sys
        // that we never want to unload.  Especially for things like
        // win32k.sys where the reference count can really hit 0.
        //

        //
        // Initializing SingleEntry is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        SingleEntry = NULL;

        Count = 0;
        for (i = 0; i < ImportList->Count; i += 1) {
            if (ImportList->Entry[i]) {
                SingleEntry = POINTER_TO_SINGLE_ENTRY(ImportList->Entry[i]);
                Count += 1;
            }
        }

        if (Count == 0) {

            ExFreePool(ImportList);
            ImportList = NO_IMPORTS_USED;
        }
        else if (Count == 1) {
            ExFreePool(ImportList);
            ImportList = (PLOAD_IMPORTS)SingleEntry;
        }
        else if (Count != ImportList->Count) {

            ImportListSize = Count * sizeof(PVOID) + sizeof(SIZE_T);

            CompactedImportList = (PLOAD_IMPORTS)
                                        ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                        ImportListSize,
                                        'TDmM');
            if (CompactedImportList) {
                CompactedImportList->Count = Count;

                Count = 0;
                for (i = 0; i < ImportList->Count; i += 1) {
                    if (ImportList->Entry[i]) {
                        CompactedImportList->Entry[Count] = ImportList->Entry[i];
                        Count += 1;
                    }
                }

                ExFreePool(ImportList);
                ImportList = CompactedImportList;
            }
        }

        *LoadedImports = ImportList;
    }
    return STATUS_SUCCESS;
}


NTSTATUS
MiSnapThunk(
    IN PVOID DllBase,
    IN PVOID ImageBase,
    IN PIMAGE_THUNK_DATA NameThunk,
    OUT PIMAGE_THUNK_DATA AddrThunk,
    IN PIMAGE_EXPORT_DIRECTORY ExportDirectory,
    IN ULONG ExportSize,
    IN LOGICAL SnapForwarder,
    OUT PCHAR *MissingProcedureName
    )

/*++

Routine Description:

    This function snaps a thunk using the specified Export Section data.
    If the section data does not support the thunk, then the thunk is
    partially snapped (Dll field is still non-null, but snap address is
    set).

Arguments:

    DllBase - Base of DLL being snapped to.

    ImageBase - Base of image that contains the thunks to snap.

    Thunk - On input, supplies the thunk to snap.  When successfully
            snapped, the function field is set to point to the address in
            the DLL, and the DLL field is set to NULL.

    ExportDirectory - Supplies the Export Section data from a DLL.

    SnapForwarder - Supplies TRUE if the snap is for a forwarder, and therefore
                    Address of Data is already setup.

Return Value:

    STATUS_SUCCESS or STATUS_DRIVER_ENTRYPOINT_NOT_FOUND or
        STATUS_DRIVER_ORDINAL_NOT_FOUND

--*/

{
    BOOLEAN Ordinal;
    USHORT OrdinalNumber;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PULONG Addr;
    USHORT HintIndex;
    LONG High;
    LONG Low;
    LONG Middle;
    LONG Result;
    NTSTATUS Status;
    PCHAR MissingProcedureName2;
    CHAR NameBuffer[ MAXIMUM_FILENAME_LENGTH ];

    PAGED_CODE();

    //
    // Determine if snap is by name, or by ordinal
    //

    Ordinal = (BOOLEAN)IMAGE_SNAP_BY_ORDINAL(NameThunk->u1.Ordinal);

    if (Ordinal && !SnapForwarder) {

        OrdinalNumber = (USHORT)(IMAGE_ORDINAL(NameThunk->u1.Ordinal) -
                         ExportDirectory->Base);

        *MissingProcedureName = (PCHAR)(ULONG_PTR)OrdinalNumber;

    }
    else {

        //
        // Change AddressOfData from an RVA to a VA.
        //

        if (!SnapForwarder) {
            NameThunk->u1.AddressOfData = (ULONG_PTR)ImageBase + NameThunk->u1.AddressOfData;
        }

        strncpy (*MissingProcedureName,
                 (const PCHAR)&((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name[0],
                 MAXIMUM_FILENAME_LENGTH - 1);

        //
        // Lookup Name in NameTable
        //

        NameTableBase = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNames);
        NameOrdinalTableBase = (PUSHORT)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

        //
        // Before dropping into binary search, see if
        // the hint index results in a successful
        // match. If the hint index is zero, then
        // drop into binary search.
        //

        HintIndex = ((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Hint;
        if ((ULONG)HintIndex < ExportDirectory->NumberOfNames &&
            !strcmp((PSZ)((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name,
             (PSZ)((PCHAR)DllBase + NameTableBase[HintIndex]))) {
            OrdinalNumber = NameOrdinalTableBase[HintIndex];

        }
        else {

            //
            // Lookup the import name in the name table using a binary search.
            //

            Low = 0;
            Middle = 0;
            High = ExportDirectory->NumberOfNames - 1;

            while (High >= Low) {

                //
                // Compute the next probe index and compare the import name
                // with the export name entry.
                //

                Middle = (Low + High) >> 1;
                Result = strcmp((const PCHAR)&((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name[0],
                                (PCHAR)((PCHAR)DllBase + NameTableBase[Middle]));

                if (Result < 0) {
                    High = Middle - 1;
                }
                else if (Result > 0) {
                    Low = Middle + 1;
                }
                else {
                    break;
                }
            }

            //
            // If the high index is less than the low index, then a matching
            // table entry was not found. Otherwise, get the ordinal number
            // from the ordinal table.
            //

            if (High < Low) {
                return STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;
            }
            else {
                OrdinalNumber = NameOrdinalTableBase[Middle];
            }
        }
    }

    //
    // If OrdinalNumber is not within the Export Address Table,
    // then DLL does not implement function. Snap to LDRP_BAD_DLL.
    //

    if ((ULONG)OrdinalNumber >= ExportDirectory->NumberOfFunctions) {
        Status = STATUS_DRIVER_ORDINAL_NOT_FOUND;

    }
    else {

        MissingProcedureName2 = NameBuffer;

        Addr = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfFunctions);
        *(PULONG_PTR)&AddrThunk->u1.Function = (ULONG_PTR)DllBase + Addr[OrdinalNumber];

        // AddrThunk s/b used from here on.

        Status = STATUS_SUCCESS;

        if (((ULONG_PTR)AddrThunk->u1.Function > (ULONG_PTR)ExportDirectory) &&
            ((ULONG_PTR)AddrThunk->u1.Function < ((ULONG_PTR)ExportDirectory + ExportSize)) ) {

            UNICODE_STRING UnicodeString;
            ANSI_STRING ForwardDllName;

            PLIST_ENTRY NextEntry;
            PKLDR_DATA_TABLE_ENTRY DataTableEntry;
            ULONG LocalExportSize;
            PIMAGE_EXPORT_DIRECTORY LocalExportDirectory;

            Status = STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;

            //
            // Include the dot in the length so we can do prefix later on.
            //

            ForwardDllName.Buffer = (PCHAR)AddrThunk->u1.Function;
            ForwardDllName.Length = (USHORT)(strchr(ForwardDllName.Buffer, '.') -
                                           ForwardDllName.Buffer + 1);
            ForwardDllName.MaximumLength = ForwardDllName.Length;

            if (NT_SUCCESS(RtlAnsiStringToUnicodeString(&UnicodeString,
                                                        &ForwardDllName,
                                                        TRUE))) {

                NextEntry = PsLoadedModuleList.Flink;

                while (NextEntry != &PsLoadedModuleList) {

                    DataTableEntry = CONTAINING_RECORD(NextEntry,
                                                       KLDR_DATA_TABLE_ENTRY,
                                                       InLoadOrderLinks);

                    //
                    // We have to do a case INSENSITIVE comparison for
                    // forwarder because the linker just took what is in the
                    // def file, as opposed to looking in the exporting
                    // image for the name.
                    // we also use the prefix function to ignore the .exe or
                    // .sys or .dll at the end.
                    //

                    if (RtlPrefixString((PSTRING)&UnicodeString,
                                        (PSTRING)&DataTableEntry->BaseDllName,
                                        TRUE)) {

                        LocalExportDirectory = (PIMAGE_EXPORT_DIRECTORY)
                            RtlImageDirectoryEntryToData (DataTableEntry->DllBase,
                                                         TRUE,
                                                         IMAGE_DIRECTORY_ENTRY_EXPORT,
                                                         &LocalExportSize);

                        if (LocalExportDirectory != NULL) {

                            IMAGE_THUNK_DATA thunkData;
                            PIMAGE_IMPORT_BY_NAME addressOfData;
                            SIZE_T length;

                            //
                            // One extra byte for NULL termination.
                            //

                            length = strlen(ForwardDllName.Buffer +
                                                ForwardDllName.Length) + 1;

                            addressOfData = (PIMAGE_IMPORT_BY_NAME)
                                ExAllocatePoolWithTag (PagedPool,
                                                      length +
                                                   sizeof(IMAGE_IMPORT_BY_NAME),
                                                   '  mM');

                            if (addressOfData) {

                                RtlCopyMemory(&(addressOfData->Name[0]),
                                              ForwardDllName.Buffer +
                                                  ForwardDllName.Length,
                                              length);

                                addressOfData->Hint = 0;

                                *(PULONG_PTR)&thunkData.u1.AddressOfData =
                                                    (ULONG_PTR)addressOfData;

                                Status = MiSnapThunk (DataTableEntry->DllBase,
                                                     ImageBase,
                                                     &thunkData,
                                                     &thunkData,
                                                     LocalExportDirectory,
                                                     LocalExportSize,
                                                     TRUE,
                                                     &MissingProcedureName2);

                                ExFreePool (addressOfData);

                                AddrThunk->u1 = thunkData.u1;
                            }
                        }

                        break;
                    }

                    NextEntry = NextEntry->Flink;
                }

                RtlFreeUnicodeString (&UnicodeString);
            }

        }

    }
    return Status;
}

NTSTATUS
MmCheckSystemImage (
    IN HANDLE ImageFileHandle,
    IN LOGICAL PurgeSection
    )

/*++

Routine Description:

    This function ensures the checksum for a system image is correct
    and matches the data in the image.

Arguments:

    ImageFileHandle - Supplies the file handle of the image.  This is a kernel
                      handle (ie: cannot be tampered with by the user).

    PurgeSection - Supplies TRUE if the data section mapping the image should
                   be purged prior to returning.  Note that the first page
                   could be used to speed up subsequent image section creation,
                   but generally the cost of useless data pages sitting in
                   transition is costly.  Better to put the pages immediately
                   on the free list to preserve the transition cache for more
                   useful pages.

Return Value:

    Status value.

--*/

{
    NTSTATUS Status;
    NTSTATUS Status2;
    HANDLE Section;
    PVOID ViewBase;
    SIZE_T ViewSize;
    IO_STATUS_BLOCK IoStatusBlock;
    PIMAGE_FILE_HEADER FileHeader;
    PIMAGE_NT_HEADERS NtHeaders;
    FILE_STANDARD_INFORMATION StandardInfo;
    PSECTION SectionPointer;
    OBJECT_ATTRIBUTES ObjectAttributes;
    KAPC_STATE ApcState;
    ULONG SectionType;
    SIZE_T NumberOfBytes;

    PAGED_CODE();

    SectionType = SEC_IMAGE;

retry:

    InitializeObjectAttributes (&ObjectAttributes,
                                NULL,
                                (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                NULL,
                                NULL);

    Status = ZwCreateSection (&Section,
                              SECTION_MAP_EXECUTE,
                              &ObjectAttributes,
                              NULL,
                              PAGE_EXECUTE,
                              SectionType,
                              ImageFileHandle);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    ViewBase = NULL;
    ViewSize = 0;

    //
    // Since callees are not always in the context of the system process,
    // attach here when necessary to guarantee the driver load occurs in a
    // known safe address space to prevent security holes.
    //

    KeStackAttachProcess (&PsInitialSystemProcess->Pcb, &ApcState);

    Status = ZwMapViewOfSection (Section,
                                 NtCurrentProcess (),
                                 (PVOID *)&ViewBase,
                                 0L,
                                 0L,
                                 NULL,
                                 &ViewSize,
                                 ViewShare,
                                 0L,
                                 PAGE_EXECUTE);

    if (!NT_SUCCESS(Status)) {
        KeUnstackDetachProcess (&ApcState);
        ZwClose (Section);
        return Status;
    }

    //
    // Now the image is mapped as a data file... Calculate its size and then
    // check its checksum.
    //

    Status = ZwQueryInformationFile (ImageFileHandle,
                                     &IoStatusBlock,
                                     &StandardInfo,
                                     sizeof(StandardInfo),
                                     FileStandardInformation);

    if (NT_SUCCESS(Status)) {

        if (SectionType == SEC_IMAGE) {
            NumberOfBytes = ViewSize;
        }
        else {
            NumberOfBytes = StandardInfo.EndOfFile.LowPart;
        }

        try {

            if (!LdrVerifyMappedImageMatchesChecksum (
                                        ViewBase,
                                        NumberOfBytes,
                                        StandardInfo.EndOfFile.LowPart)) {

                Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
                goto out;
            }

            NtHeaders = RtlImageNtHeader (ViewBase);

            if (NtHeaders == NULL) {
                Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
                goto out;
            }

            FileHeader = &NtHeaders->FileHeader;

            //
            // Detect configurations inadvertently trying to load 32-bit
            // drivers on NT64 or mismatched platform architectures, etc.
            //

            if ((FileHeader->Machine != IMAGE_FILE_MACHINE_NATIVE) ||
                (NtHeaders->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR_MAGIC)) {
                Status = STATUS_INVALID_IMAGE_PROTECT;
                goto out;
            }

#if !defined(NT_UP)
            if (!MmVerifyImageIsOkForMpUse (ViewBase)) {
                Status = STATUS_IMAGE_MP_UP_MISMATCH;
                goto out;
            }
#endif
        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
        }
    }

out:

    ZwUnmapViewOfSection (NtCurrentProcess (), ViewBase);

    KeUnstackDetachProcess (&ApcState);

    if ((Status == STATUS_IMAGE_CHECKSUM_MISMATCH) &&
        (SectionType == SEC_IMAGE)) {

        //
        // We speculatively tried to verify the checksum using the image
        // mapping but this did not match.  This can occur if the image has
        // a self-signed certificate appended to it (this is a rare occurrence)
        // as the certificate data will be present in the data file but is
        // not mapped when the file is used as an image.  So fall back to
        // mapping the file as data and recompute.
        //
        // Note there are other cases above that can get us to this check but
        // it's always ok to retry as data ...
        //

        ZwClose (Section);
        SectionType = SEC_COMMIT;
        goto retry;
    }

    if ((PurgeSection == TRUE) && (SectionType == SEC_COMMIT)) {

        Status2 = ObReferenceObjectByHandle (Section,
                                             SECTION_MAP_EXECUTE,
                                             MmSectionObjectType,
                                             KernelMode,
                                             (PVOID *) &SectionPointer,
                                             (POBJECT_HANDLE_INFORMATION) NULL);

        if (NT_SUCCESS (Status2)) {

            MmPurgeSection (SectionPointer->Segment->ControlArea->FilePointer->SectionObjectPointer,
                            NULL,
                            0,
                            FALSE);
            ObDereferenceObject (SectionPointer);
        }
    }

    ZwClose (Section);
    return Status;
}

#if !defined(NT_UP)
BOOLEAN
MmVerifyImageIsOkForMpUse (
    IN PVOID BaseAddress
    )
{
    PIMAGE_NT_HEADERS NtHeaders;

    PAGED_CODE();

    NtHeaders = RtlImageNtHeader (BaseAddress);

    if ((NtHeaders != NULL) &&
        (KeNumberProcessors > 1) &&
        (NtHeaders->FileHeader.Characteristics & IMAGE_FILE_UP_SYSTEM_ONLY)) {

        return FALSE;
    }

    return TRUE;
}
#endif


PFN_NUMBER
MiDeleteSystemPageableVm (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER NumberOfPtes,
    IN ULONG Flags,
    OUT PPFN_NUMBER ResidentPages OPTIONAL
    )

/*++

Routine Description:

    This function deletes pageable system address space (paged pool
    or driver pageable sections).

Arguments:

    PointerPte - Supplies the start of the PTE range to delete.

    NumberOfPtes - Supplies the number of PTEs in the range.

    Flags - Supplies flags indicating what the caller desires.

    ResidentPages - If not NULL, the number of resident pages freed is
                    returned here.

Return Value:

    Returns the number of pages actually freed.

--*/

{
    ULONG TimeStamp;
    PMMSUPPORT Ws;
    PVOID VirtualAddress;
    PFN_NUMBER PageFrameIndex;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER ValidPages;
    PFN_NUMBER PagesRequired;
    WSLE_NUMBER WsIndex;
    KIRQL OldIrql;
    ULONG FlushCount;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    MMWSLENTRY Locked;
    PFN_NUMBER PageTableFrameIndex;
    LOGICAL WsHeld;
    PETHREAD Thread;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    ValidPages = 0;
    PagesRequired = 0;
    FlushCount = 0;
    WsHeld = FALSE;

    Thread = PsGetCurrentThread ();

    if (MI_IS_SESSION_PTE (PointerPte)) {
        Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
    }
    else {
        Ws = &MmSystemCacheWs;
    }

#if defined(_X86PAE_)

    //
    // PAE PTEs are written in 2 separate writes so the working set pushlock
    // must always be held even to examine PTEs in prototype format - because
    // a trim could be happening in parallel (writing the low half and then
    // the upper half which would cause otherwise cause us to read the
    // PTE contents split in the middle).
    //

    WsHeld = TRUE;
    LOCK_WORKING_SET (Thread, Ws);

#endif

    while (NumberOfPtes != 0) {
        PteContents = *PointerPte;

        if (PteContents.u.Long != 0) {

            if (PteContents.u.Hard.Valid == 1) {

                //
                // Once the working set mutex is acquired, it is deliberately
                // held until all the pages have been freed.  This is because
                // when paged pool is running low on large servers, we need the
                // segment dereference thread to be able to free large amounts
                // quickly.  Typically this thread will free 64k chunks and we
                // don't want to have to contend for the mutex 16 times to do
                // this as there may be thousands of other threads also trying
                // for it.
                //

                if (WsHeld == FALSE) {
                    WsHeld = TRUE;
                    LOCK_WORKING_SET (Thread, Ws);
                }

                PteContents = *PointerPte;
                if (PteContents.u.Hard.Valid == 0) {
                    continue;
                }

                //
                // Delete the page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                //
                // Check to see if this is a pageable page in which
                // case it needs to be removed from the working set list.
                //

                WsIndex = Pfn1->u1.WsIndex;
                if (WsIndex == 0) {
                    ValidPages += 1;
                    if (Ws != &MmSystemCacheWs) {
                        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_DELVA, 1);
                        InterlockedDecrementSizeT (&MmSessionSpace->NonPageablePages);
                    }
                }
                else {
                    if (Ws == &MmSystemCacheWs) {
                        MiRemoveWsle (WsIndex, MmSystemCacheWorkingSetList);
                        MiReleaseWsle (WsIndex, &MmSystemCacheWs);
                    }
                    else {
                        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                        WsIndex = MiLocateWsle (VirtualAddress,
                                              MmSessionSpace->Vm.VmWorkingSetList,
                                              WsIndex,
                                              TRUE);

                        ASSERT (WsIndex != WSLE_NULL_INDEX);

                        //
                        // Check to see if this entry is locked in
                        // the working set or locked in memory.
                        //

                        Locked = MmSessionSpace->Wsle[WsIndex].u1.e1;

                        MiRemoveWsle (WsIndex, MmSessionSpace->Vm.VmWorkingSetList);

                        MiReleaseWsle (WsIndex, &MmSessionSpace->Vm);

                        if (Locked.LockedInWs == 1 || Locked.LockedInMemory == 1) {

                            //
                            // This entry is locked.
                            //

                            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_DELVA, 1);
                            InterlockedDecrementSizeT (&MmSessionSpace->NonPageablePages);
                            ValidPages += 1;

                            ASSERT (WsIndex < MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic);
                            MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic -= 1;

                            if (WsIndex != MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic) {
                                WSLE_NUMBER Entry;
                                PVOID SwapVa;

                                Entry = MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic;
                                ASSERT (MmSessionSpace->Wsle[Entry].u1.e1.Valid);
                                SwapVa = MmSessionSpace->Wsle[Entry].u1.VirtualAddress;
                                SwapVa = PAGE_ALIGN (SwapVa);

                                MiSwapWslEntries (Entry,
                                                  WsIndex,
                                                  &MmSessionSpace->Vm,
                                                  FALSE);
                            }
                        }
                        else {
                            ASSERT (WsIndex >= MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic);
                        }
                    }
                }

                //
                // Check if this is a prototype PTE.
                //

                if (Pfn1->u3.e1.PrototypePte == 1) {

                    PMMPTE PointerPde;

                    ASSERT (Ws != &MmSystemCacheWs);

                    //
                    // Capture the state of the modified bit for this PTE.
                    //

                    PointerPde = MiGetPteAddress (PointerPte);

                    if (PointerPde->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                        if (!NT_SUCCESS (MiCheckPdeForPagedPool (PointerPte))) {
#endif
                            KeBugCheckEx (MEMORY_MANAGEMENT,
                                          0x61940,
                                          (ULONG_PTR)PointerPte,
                                          (ULONG_PTR)PointerPde->u.Long,
                                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
                        }
#endif
                    }

                    PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

                    LOCK_PFN (OldIrql);

                    MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);
                }
                else {
                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

                    LOCK_PFN (OldIrql);

                    MI_SET_PFN_DELETED (Pfn1);
                }

                //
                // Decrement the share count for the containing page table page.
                //

                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // Decrement the share count for the physical page.
                //

                MiDecrementShareCount (Pfn1, PageFrameIndex);

                UNLOCK_PFN (OldIrql);

                MI_WRITE_ZERO_PTE (PointerPte);

                if (Flags & MI_DELETE_FLUSH_TB) {

                    //
                    // Flush the TB for this virtual address.
                    //

                    if (FlushCount != MM_MAXIMUM_FLUSH_COUNT) {

                        VaFlushList[FlushCount] =
                                    MiGetVirtualAddressMappedByPte (PointerPte);
                        FlushCount += 1;
                    }
                }
                else {

                    //
                    // Our caller will lazy flush this entry so stamp the time 
                    // for him.  Since zero is used by our caller as an
                    // indicator no flush is needed, ensure that if the
                    // inserted field (which may be truncated) is zero, that
                    // a flush of this address is done before we return.
                    //

                    TimeStamp = KeReadTbFlushTimeStamp ();

                    PointerPte->u.Soft.PageFileHigh = TimeStamp;

                    if (PointerPte->u.Soft.PageFileHigh == 0) {

                        if (FlushCount != MM_MAXIMUM_FLUSH_COUNT) {

                            VaFlushList[FlushCount] =
                                    MiGetVirtualAddressMappedByPte (PointerPte);
                            FlushCount += 1;
                        }
                    }
                }
            }
            else if (PteContents.u.Soft.Prototype) {

                ASSERT (Ws != &MmSystemCacheWs);

                MI_WRITE_ZERO_PTE (PointerPte);
            }
            else if (PteContents.u.Soft.Transition == 1) {

                LOCK_PFN (OldIrql);

                PteContents = *PointerPte;

                if (PteContents.u.Soft.Transition == 0) {
                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                //
                // Transition, release page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

                //
                // Set the pointer to PTE as empty so the page
                // is deleted when the reference count goes to zero.
                //

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // Check the reference count for the page, if the reference
                // count is zero, move the page to the free list, if the
                // reference count is not zero, ignore this page.  When the
                // reference count goes to zero, it will be placed on the
                // free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (PageFrameIndex);
                }

                MI_WRITE_ZERO_PTE (PointerPte);
                UNLOCK_PFN (OldIrql);
            }
            else {

                //
                // Demand zero, release page file space.
                //
                if (PteContents.u.Soft.PageFileHigh != 0) {
                    LOCK_PFN (OldIrql);
                    MiReleasePageFileSpace (PteContents);
                    UNLOCK_PFN (OldIrql);
                }

                MI_WRITE_ZERO_PTE (PointerPte);
            }

            PagesRequired += 1;
        }

        NumberOfPtes -= 1;
        PointerPte += 1;
    }

    if (WsHeld == TRUE) {
        UNLOCK_WORKING_SET (Thread, Ws);
    }

    //
    // Some callers flush the TB themselves, but for the rest we must flush
    // here.
    //

    if (FlushCount == 0) {
        NOTHING;
    }
    else if (FlushCount == 1) {
        MI_FLUSH_SINGLE_TB (VaFlushList[0], TRUE);
    }
    else if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (FlushCount, &VaFlushList[0], TRUE);
    }
    else {
        MI_FLUSH_ENTIRE_TB (0x16);
    }

    if (ARGUMENT_PRESENT (ResidentPages)) {
        *ResidentPages = ValidPages;
    }

    return PagesRequired;
}

VOID
MiMarkSectionWritable (
    IN PIMAGE_SECTION_HEADER SectionTableEntry
    )

/*++

Routine Description:

    This function is a nonpaged helper routine that updates the characteristics
    field of the argument section table entry and marks the page dirty so
    that subsequent session loads share the same copy.

Arguments:

     SectionTableEntry - Supplies the relevant section table entry.

Return Value:

     None.

--*/

{
    PEPROCESS Process;
    PMMPTE PointerPte;
    ULONG FreeBit;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
    PULONG Characteristics;

    //
    // Modify the PE header through hyperspace and mark the header page
    // dirty so subsequent sections pick up the same copy.
    //
    // Note this makes the entire .rdata (.sdata on IA64) writable
    // instead of just the import tables.
    //

    Process = PsGetCurrentProcess ();

    PointerPte = MiGetPteAddress (&SectionTableEntry->Characteristics);
    LOCK_PFN (OldIrql);

    MiMakeSystemAddressValidPfn (&SectionTableEntry->Characteristics, OldIrql);
    ASSERT (PointerPte->u.Hard.Valid == 1);

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    Characteristics = MiMapPageInHyperSpaceAtDpc (Process, PageFrameIndex);
    Characteristics = (PULONG)((PCHAR)Characteristics + MiGetByteOffset (&SectionTableEntry->Characteristics));

    *Characteristics |= IMAGE_SCN_MEM_WRITE;

    MiUnmapPageInHyperSpaceFromDpc (Process, Characteristics);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    MI_SET_MODIFIED (Pfn1, 1, 0x7);

    if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
        (Pfn1->u3.e1.WriteInProgress == 0)) {

        FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

        if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
            MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
        }

        Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
    }

    UNLOCK_PFN (OldIrql);

    return;
}

VOID
MiMakeEntireImageCopyOnWrite (
    IN PSUBSECTION Subsection
    )

/*++

Routine Description:

    This function sets the protection of all prototype PTEs to copy on write.

Arguments:

     Subsection - Supplies the base subsection for the entire image.

Return Value:

     None.

--*/

{
    PMMPTE PointerPte;
    PMMPTE ProtoPte;
    PMMPTE LastProtoPte;
    MMPTE PteContents;

    //
    // Note this is only called for image control areas that have at least
    // PAGE_SIZE subsection alignment, and so the first
    // subsection which maps the header can always be skipped.
    //

    do {
        Subsection = Subsection->NextSubsection;

        if (Subsection == NULL) {
            break;
        }

        //
        // Don't mark global subsections as copy on write even when the
        // image is relocated.  This is easily distinguishable because
        // it is the only subsection that is marked readwrite.
        //

        if (Subsection->u.SubsectionFlags.Protection == MM_READWRITE) {
            continue;
        }

        ProtoPte = Subsection->SubsectionBase;
        LastProtoPte = Subsection->SubsectionBase + Subsection->PtesInSubsection;

        PointerPte = ProtoPte;

        MmLockPagedPool (ProtoPte, Subsection->PtesInSubsection * sizeof (MMPTE));

        do {
            PteContents = *PointerPte;
            ASSERT (PteContents.u.Hard.Valid == 0);
            if (PteContents.u.Long != 0) {
                if ((PteContents.u.Soft.Prototype == 0) &&
                    (PteContents.u.Soft.Transition == 1)) {
                    if (MiSetProtectionOnTransitionPte (PointerPte, MM_EXECUTE_WRITECOPY)) {
                        continue;
                    }
                }
                else {
                    PointerPte->u.Soft.Protection = MM_EXECUTE_WRITECOPY;
                }
            }
            PointerPte += 1;
        } while (PointerPte < LastProtoPte);

        MmUnlockPagedPool (ProtoPte, Subsection->PtesInSubsection * sizeof (MMPTE));

        Subsection->u.SubsectionFlags.Protection = MM_EXECUTE_WRITECOPY;

    } while (TRUE);

    return;
}


VOID
MiSetSystemCodeProtection (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:

    This function sets the protection of system code as specified.

Arguments:

    FirstPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

    ProtectionMask - Supplies the desired protection mask.

Return Value:

    None.

Environment:

    Kernel Mode, APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    MMPTE PteContents;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerProtoPte;
    PMMPFN Pfn1;
    LOGICAL SessionAddress;
    PVOID VirtualAddress;
    ULONG FlushCount;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    PETHREAD CurrentThread;
    PMMSUPPORT Ws;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    FlushCount = 0;

#if defined(_X86_)
    ASSERT (MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(FirstPte)) == 0);
#endif

    CurrentThread = PsGetCurrentThread ();

    PointerPte = FirstPte;

    if (MI_IS_SESSION_ADDRESS (MiGetVirtualAddressMappedByPte(FirstPte))) {
        Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
        SessionAddress = TRUE;
    }
    else {
        Ws = &MmSystemCacheWs;
        SessionAddress = FALSE;
    }

    LOCK_WORKING_SET (CurrentThread, Ws);

    //
    // Set these PTEs to the specified protection.
    //
    // Note that the write bit may already be off (in the valid PTE) if the
    // page has already been inpaged from the paging file and has not since
    // been dirtied.
    //

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {

        PteContents = *PointerPte;

        if (PteContents.u.Long == 0) {
            PointerPte += 1;
            continue;
        }

        if (PteContents.u.Hard.Valid == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            if (Pfn1->u3.e1.PrototypePte == 1) {

                //
                // This must be a session address.  The prototype PTE contains
                // the protection that is pushed out to the real PTE after
                // it's been trimmed so update that too.
                //

                PointerProtoPte = Pfn1->PteAddress;

                PointerPde = MiGetPteAddress (PointerProtoPte);

                if (PointerPde->u.Hard.Valid == 0) {

                    if (SessionAddress == TRUE) {

                        //
                        // Unlock the session working set and lock the
                        // system working set as we need to make the backing
                        // prototype PTE valid.
                        //

                        UNLOCK_PFN (OldIrql);

                        UNLOCK_WORKING_SET (CurrentThread, Ws);

                        LOCK_WORKING_SET (CurrentThread, &MmSystemCacheWs);

                        LOCK_PFN (OldIrql);
                    }

                    MiMakeSystemAddressValidPfnSystemWs (PointerProtoPte,
                                                         OldIrql);

                    if (SessionAddress == TRUE) {

                        //
                        // Unlock the system working set and lock the
                        // session working set as we have made the backing
                        // prototype PTE valid and can now handle the
                        // original session PTE.
                        //

                        UNLOCK_PFN (OldIrql);

                        UNLOCK_WORKING_SET (CurrentThread, &MmSystemCacheWs);

                        LOCK_WORKING_SET (CurrentThread, Ws);

                        LOCK_PFN (OldIrql);
                    }

                    //
                    // The world may have changed while we waited.
                    //

                    continue;
                }
            }
            else {
                if ((ProtectionMask & MM_COPY_ON_WRITE_MASK) == MM_COPY_ON_WRITE_MASK) {
                    //
                    // This page is already private so ignore the copy on
                    // write attribute (or anything else).  This is very
                    // unusual.
                    //

                    PointerPte += 1;
                    continue;
                }
                PointerProtoPte = NULL;
            }

            Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

            MI_MAKE_VALID_PTE (TempPte,
                               PteContents.u.Hard.PageFrameNumber,
                               ProtectionMask,
                               PointerPte);

            //
            // Note the dirty and write bits get turned off here.
            // Any existing pagefile addresses for clean pages are preserved.
            //

            if (MI_IS_PTE_DIRTY (PteContents)) {
                MI_CAPTURE_DIRTY_BIT_TO_PFN (&PteContents, Pfn1);
            }

            //
            // If the new protection is also directly writable, then preserve
            // the dirty and write bits so subsequent accesses do not fault.
            //

            if ((ProtectionMask == MM_READWRITE) ||
                (ProtectionMask == MM_EXECUTE_READWRITE)) {

                if (PteContents.u.Hard.Dirty == 1) {
                    TempPte.u.Hard.Dirty = 1;
                }

#if !defined(NT_UP)
#if defined(_X86_) || defined(_AMD64_)
                TempPte.u.Hard.Writable = 1;
#endif
#endif
            }

            if (PteContents.u.Hard.Accessed == 1) {
                TempPte.u.Hard.Accessed = 1;
            }

            MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

            if (PointerProtoPte != NULL) {
                MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerProtoPte, TempPte);
            }

            if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
                VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                VaFlushList[FlushCount] = VirtualAddress;
                FlushCount += 1;
            }

        }
        else if (PteContents.u.Soft.Prototype == 1) {

            //
            // WITH REGARDS TO SESSION SPACE :
            //
            // Nothing needs to be done if the image was linked with
            // greater than or equal to PAGE_SIZE subsection alignment
            // because image section creation assigned proper protections
            // to each subsection.
            //
            // However, if the image had less than PAGE_SIZE subsection
            // alignment, then image creation uses a single copyonwrite
            // subsection to control the entire image, so individual
            // protections need to be applied now.  Note well - this must
            // only be done *ONCE* when the image is first loaded - subsequent
            // loads of this image in other sessions do not need to update
            // the common prototype PTEs.
            //

            PointerProtoPte = MiPteToProto (PointerPte);

            ASSERT (!MI_IS_PHYSICAL_ADDRESS (PointerProtoPte));
            PointerPde = MiGetPteAddress (PointerProtoPte);

            if (PointerPde->u.Hard.Valid == 0) {

                if (SessionAddress == TRUE) {

                    UNLOCK_PFN (OldIrql);

                    UNLOCK_WORKING_SET (CurrentThread, Ws);

                    LOCK_WORKING_SET (CurrentThread, &MmSystemCacheWs);

                    LOCK_PFN (OldIrql);
                }

                MiMakeSystemAddressValidPfnSystemWs (PointerProtoPte, OldIrql);

                if (SessionAddress == TRUE) {

                    UNLOCK_PFN (OldIrql);

                    UNLOCK_WORKING_SET (CurrentThread, &MmSystemCacheWs);

                    LOCK_WORKING_SET (CurrentThread, Ws);

                    LOCK_PFN (OldIrql);
                }

                //
                // The world may have changed while we waited.
                //

                continue;
            }

            PteContents = *PointerProtoPte;

            if (PteContents.u.Long != 0) {

                if (PteContents.u.Hard.Valid == 1) {
                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
                    Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                }
                else {
                    PointerProtoPte->u.Soft.Protection = ProtectionMask;
    
                    if ((PteContents.u.Soft.Prototype == 0) &&
                        (PteContents.u.Soft.Transition == 1)) {
                        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
                        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                    }
                }
            }
        }
        else if (PteContents.u.Soft.Transition == 1) {

            //
            // This page is already private so if copy on write is specified,
            // skip this page.  This is very unusual.
            //

            if ((ProtectionMask & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK) {
                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                PointerPte->u.Soft.Protection = ProtectionMask;
            }
        }
        else {

            //
            // Must be page file space or demand zero.
            //

            //
            // This page is already private so if copy on write is specified,
            // skip this page.  This is very unusual.
            //

            if ((ProtectionMask & MM_COPY_ON_WRITE_MASK) != MM_COPY_ON_WRITE_MASK) {
                PointerPte->u.Soft.Protection = ProtectionMask;
            }
        }
        PointerPte += 1;
    }

    if (FlushCount == 0) {
        NOTHING;
    }
    else if (FlushCount == 1) {
        MI_FLUSH_SINGLE_TB (VaFlushList[0], TRUE);
    }
    else if (FlushCount < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (FlushCount, &VaFlushList[0], TRUE);
    }
    else {
        MI_FLUSH_ENTIRE_TB (0x17);
    }

    UNLOCK_PFN (OldIrql);

    UNLOCK_WORKING_SET (CurrentThread, Ws);

    return;
}

MM_PROTECTION_MASK
MiComputeDriverProtection (
    IN LOGICAL SessionDriver,
    IN ULONG SectionProtection
    )

/*++

Routine Description:

    This function computes the driver protection mask for an image.

Arguments:

    SessionDriver - Supplies TRUE if this is for a session space driver.

    SectionProtection - Supplies protection from the image header.

Return Value:

    None.

--*/

{
    MM_PROTECTION_MASK NewProtection;

    NewProtection = MM_ZERO_ACCESS;

    if (SectionProtection != 0) {

        //
        // Always make images executable (unless they're completely
        // no-access) for compatibility.
        //

#if !defined (_WIN64)
        SectionProtection |= IMAGE_SCN_MEM_EXECUTE;
#endif

        if (MmEnforceWriteProtection == 0) {
            SectionProtection |= (IMAGE_SCN_MEM_WRITE | IMAGE_SCN_MEM_EXECUTE);
        }
    }

    if (SectionProtection & IMAGE_SCN_MEM_EXECUTE) {
        NewProtection |= MM_EXECUTE;
    }

    if (SectionProtection & IMAGE_SCN_MEM_READ) {
        NewProtection |= MM_READONLY;
    }

    if (SectionProtection & IMAGE_SCN_MEM_WRITE) {

        if (SessionDriver == TRUE) {

            if (NewProtection & MM_EXECUTE) {
                NewProtection = MM_EXECUTE_WRITECOPY;
            }
            else {
                NewProtection = MM_WRITECOPY;
            }
        }
        else {
            if (NewProtection & MM_EXECUTE) {
                NewProtection = MM_EXECUTE_READWRITE;
            }
            else {
                NewProtection = MM_READWRITE;
            }
        }
    }

    if (NewProtection == MM_ZERO_ACCESS) {
        NewProtection = MM_NOACCESS;
    }

    return NewProtection;
}

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    )

/*++

Routine Description:

    This function sets the protection of a system component as specified.

Arguments:

    DllBase - Supplies the base address of the system component.

Return Value:

    None.

--*/

{
    ULONG RelevantSectionProtections;
    ULONG LastSectionProtection;
    ULONG SectionProtection;
    ULONG MergedSectionProtection;
    ULONG NumberOfSubsections;
    ULONG SectionVirtualSize;
    ULONG OffsetToSectionTable;
    PFN_NUMBER NumberOfPtes;
    ULONG_PTR VirtualAddress;
    PVOID LastVirtualAddress;
    PMMPTE PointerPte;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PMMPTE LastImagePte;
    PMMPTE MergedPte;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_FILE_HEADER FileHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    MM_PROTECTION_MASK LastProtection;
    MM_PROTECTION_MASK MergedProtection;
    LOGICAL SessionDriver;

    PAGED_CODE();

    if (MI_IS_PHYSICAL_ADDRESS (DllBase)) {
        return;
    }

    NtHeader = RtlImageNtHeader (DllBase);

    if (NtHeader == NULL) {
        return;
    }

    if (MI_IS_SESSION_ADDRESS (DllBase) == 0) {

        //
        // Images prior to Win2000 were not protected from stepping all over
        // their (and others) code and readonly data.  Here we somewhat
        // preserve that behavior, but don't allow them to step on anyone else.
        //

        if (NtHeader->OptionalHeader.MajorOperatingSystemVersion < 5) {
            return;
        }

        if (NtHeader->OptionalHeader.MajorImageVersion < 5) {
            return;
        }
        SessionDriver = FALSE;
    }
    else {
        SessionDriver = TRUE;
    }

    //
    // Don't include IMAGE_SCN_MEM_SHARED because it only applies to session
    // drivers and that's handled by MiSessionProcessGlobalSubsections.
    //

    RelevantSectionProtections = (IMAGE_SCN_MEM_WRITE | IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_EXECUTE);

    //
    // If the image has section alignment of at least PAGE_SIZE, then
    // the image section was created with individual subsections and
    // proper permissions already applied to the prototype PTEs.  However,
    // our caller may have been changing the individual PTE protections
    // in order to relocate the image, so march on regardless of section
    // alignment.
    //

    NumberOfPtes = BYTES_TO_PAGES (NtHeader->OptionalHeader.SizeOfImage);

    FileHeader = &NtHeader->FileHeader;

    NumberOfSubsections = FileHeader->NumberOfSections;

    ASSERT (NumberOfSubsections != 0);

    OffsetToSectionTable = sizeof(ULONG) +
                              sizeof(IMAGE_FILE_HEADER) +
                              FileHeader->SizeOfOptionalHeader;

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                            OffsetToSectionTable);

    //
    // Verify the image contains subsections ordered by increasing virtual
    // address and that there are no overlaps.
    //

    LastVirtualAddress = DllBase;

    for ( ; NumberOfSubsections > 0; NumberOfSubsections -= 1, SectionTableEntry += 1) {

        if (SectionTableEntry->Misc.VirtualSize == 0) {
            SectionVirtualSize = SectionTableEntry->SizeOfRawData;
        }
        else {
            SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
        }

        VirtualAddress = (ULONG_PTR)DllBase + SectionTableEntry->VirtualAddress;
        if ((PVOID)VirtualAddress <= LastVirtualAddress) {

            //
            // Subsections are not in an increasing virtual address ordering.
            // No protection is provided for such a poorly linked image.
            //

            KdPrint (("MM:sysload - Image at %p is badly linked\n", DllBase));
            return;
        }
        LastVirtualAddress = (PVOID)((PCHAR)VirtualAddress + SectionVirtualSize - 1);
    }

    NumberOfSubsections = FileHeader->NumberOfSections;
    ASSERT (NumberOfSubsections != 0);

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                            OffsetToSectionTable);

    LastVirtualAddress = (PVOID)((ULONG_PTR)(SectionTableEntry + NumberOfSubsections) - 1);

    //
    // Set writable PTE here so the image headers are excluded.  This is
    // needed so that locking down of sections can continue to edit the
    // image headers for counts.
    //

    LastSectionProtection = (IMAGE_SCN_MEM_WRITE | IMAGE_SCN_MEM_READ);

    FirstPte = MiGetPteAddress (DllBase);
    LastImagePte = MiGetPteAddress(DllBase) + NumberOfPtes;

    MergedPte = NULL;
    MergedSectionProtection = 0;

    for ( ; NumberOfSubsections > 0; NumberOfSubsections -= 1, SectionTableEntry += 1) {

        if (SectionTableEntry->Misc.VirtualSize == 0) {
            SectionVirtualSize = SectionTableEntry->SizeOfRawData;
        }
        else {
            SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
        }

        VirtualAddress = (ULONG_PTR)DllBase + SectionTableEntry->VirtualAddress;

        PointerPte = MiGetPteAddress ((PVOID)VirtualAddress);

        if ((MergedPte != NULL) && (PointerPte > MergedPte)) {

            //
            // Apply the merged protection to the prior straddling PTE.
            //

            MergedProtection = MiComputeDriverProtection (SessionDriver,
                                                          MergedSectionProtection);
            MiSetSystemCodeProtection (MergedPte, MergedPte, MergedProtection);

            if (MergedPte == FirstPte) {

                //
                // The merged PTE overlapped with the first PTE of the prior
                // subsection, so increment the first PTE now.
                //

                FirstPte += 1;
            }

            MergedPte = NULL;
            MergedSectionProtection = 0;
        }

        if (PointerPte >= LastImagePte) {

            //
            // Skip relocation subsections (which aren't given VA space).
            //

            break;
        }

        SectionProtection = (SectionTableEntry->Characteristics & RelevantSectionProtections);

        if (SectionProtection == LastSectionProtection) {

            //
            // Merge adjacent subsections that have the same protection.
            //

            LastVirtualAddress = (PVOID)((PCHAR)VirtualAddress + SectionVirtualSize - 1);
            continue;
        }

        //
        // This section has different permissions from the previous one so
        // set the protections on the previous one now.
        //

        //
        // Update the PTE protections.  Make sure if it's sharing
        // a PTE that the last PTE is set to the most relaxed
        // permission combination.
        //

        LastPte = (PVOID) MiGetPteAddress (LastVirtualAddress);

        if (LastPte == PointerPte) {

            //
            // Don't include the last PTE from the previous subsection as it
            // overlaps with this one (and may overlap with the next subsection
            // if this one is small!).
            //

            LastPte -= 1;

            //
            // Compute the most relaxed permission for the straddling PTE
            // by merging subsection protections, but don't apply it till
            // we have ascertained that there are no more subsections which
            // can overlap into this PTE.
            //

            if (MergedPte != NULL) {
                ASSERT (MergedPte == PointerPte);
            }

            MergedSectionProtection |= (SectionProtection | LastSectionProtection);

            MergedPte = PointerPte;
        }

        if (LastPte >= FirstPte) {

            ASSERT (FirstPte < LastImagePte);

            if (LastPte >= LastImagePte) {
                LastPte = LastImagePte - 1;
            }

            ASSERT (LastPte >= FirstPte);

            LastProtection = MiComputeDriverProtection (SessionDriver,
                                                        LastSectionProtection);

            MiSetSystemCodeProtection (FirstPte, LastPte, LastProtection);
        }

        //
        // Initialize variables to describe the current subsection as it
        // is the start of a new run.
        //

        FirstPte = PointerPte;
        LastVirtualAddress = (PVOID)((PCHAR)VirtualAddress + SectionVirtualSize - 1);
        LastSectionProtection = SectionProtection;
    }

    if (MergedPte != NULL) {

        //
        // Apply the merged protection to the prior straddling PTE.
        //

        MergedProtection = MiComputeDriverProtection (SessionDriver,
                                                      MergedSectionProtection);

        MiSetSystemCodeProtection (MergedPte, MergedPte, MergedProtection);

        if (MergedPte == FirstPte) {

            //
            // The merged PTE overlapped with the first PTE of the prior
            // subsection, so increment the first PTE now.
            //

            FirstPte += 1;
        }

        MergedPte = NULL;
        MergedSectionProtection = 0;
    }

    //
    // Set the protections on the last subsection's PTEs now.
    //

    LastPte = (PVOID) MiGetPteAddress (LastVirtualAddress);

    if ((FirstPte < LastImagePte) && (LastPte >= FirstPte)) {

        if (LastPte >= LastImagePte) {
            LastPte = LastImagePte - 1;
        }

        ASSERT (LastPte >= FirstPte);

        LastProtection = MiComputeDriverProtection (SessionDriver,
                                                    LastSectionProtection);

        MiSetSystemCodeProtection (FirstPte, LastPte, LastProtection);
    }

    return;
}


VOID
MiSessionProcessGlobalSubsections (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function sets the protection of a session driver's subsections
    to globally shared if their PE header specifies them as such.

Arguments:

    DataTableEntry - Supplies the loaded module list entry for the driver.

Return Value:

    None.

--*/

{
    PVOID DllBase;
    PSUBSECTION Subsection;
    PMMPTE RealPteBase;
    PMMPTE PrototypePteBase;
    PCONTROL_AREA ControlArea;
    PIMAGE_NT_HEADERS NtHeader;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    LOGICAL GlobalSubsectionSupport;
    PIMAGE_ENTRY_IN_SESSION Image;
    ULONG Count;

    PAGED_CODE();

    Image = MiSessionLookupImage (DataTableEntry->DllBase);

    if (Image != NULL) {
        ASSERT (MmSessionSpace->ImageLoadingCount >= 0);

        if (Image->ImageLoading == TRUE) {
            Image->ImageLoading = FALSE;
            ASSERT (MmSessionSpace->ImageLoadingCount > 0);
            InterlockedDecrement (&MmSessionSpace->ImageLoadingCount);
        }
    }
    else {
        ASSERT (FALSE);
    }

    DllBase = DataTableEntry->DllBase;

    ControlArea = ((PSECTION)DataTableEntry->SectionPointer)->Segment->ControlArea;

    ASSERT (MI_IS_PHYSICAL_ADDRESS(DllBase) == FALSE);

    ASSERT (MI_IS_SESSION_ADDRESS(DllBase));

    NtHeader = RtlImageNtHeader (DllBase);

    ASSERT (NtHeader);

    if (NtHeader->OptionalHeader.SectionAlignment < PAGE_SIZE) {
        if (Image->GlobalSubs != NULL) {
            ExFreePool (Image->GlobalSubs);
            Image->GlobalSubs = NULL;
        }
        return;
    }

    //
    // Win XP and Win2000 did not support global shared subsections
    // for session images.  To ensure backwards compatibility for existing
    // drivers, ensure that only newer ones get this feature.
    //

    GlobalSubsectionSupport = FALSE;

    if (NtHeader->OptionalHeader.MajorOperatingSystemVersion > 5) {
        GlobalSubsectionSupport = TRUE;
    }
    else if (NtHeader->OptionalHeader.MajorOperatingSystemVersion == 5) {

        if (NtHeader->OptionalHeader.MinorOperatingSystemVersion > 1) {
            GlobalSubsectionSupport = TRUE;
        }
        else if (NtHeader->OptionalHeader.MinorOperatingSystemVersion == 1) {
            if (NtHeader->OptionalHeader.MajorImageVersion > 5) {
                GlobalSubsectionSupport = TRUE;
            }
            else if (NtHeader->OptionalHeader.MajorImageVersion == 5) {
                if (NtHeader->OptionalHeader.MinorImageVersion > 1) {
                    GlobalSubsectionSupport = TRUE;
                }
                else if (NtHeader->OptionalHeader.MinorImageVersion == 1) {
                    if (NtHeader->OptionalHeader.MajorSubsystemVersion > 5) {
                        GlobalSubsectionSupport = TRUE;
                    }
                    else if (NtHeader->OptionalHeader.MajorSubsystemVersion == 5) {
                        if (NtHeader->OptionalHeader.MinorSubsystemVersion >= 2) {
                            GlobalSubsectionSupport = TRUE;
                        }
                    }
                }
            }
        }
    }

    if (GlobalSubsectionSupport == FALSE) {
        if (Image->GlobalSubs != NULL) {
            ExFreePool (Image->GlobalSubs);
            Image->GlobalSubs = NULL;
        }
        return;
    }

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    RealPteBase = MiGetPteAddress (DllBase);
    PrototypePteBase = Subsection->SubsectionBase;

    //
    // Loop through all the subsections.
    //

    if (ControlArea->u.Flags.Image == 1) {

        do {

            if (Subsection->u.SubsectionFlags.GlobalMemory == 1) {
    
                PointerPte = RealPteBase + (Subsection->SubsectionBase - PrototypePteBase);
                LastPte = PointerPte + Subsection->PtesInSubsection - 1;
    
                MiSetSystemCodeProtection (PointerPte,
                                           LastPte,
                                           Subsection->u.SubsectionFlags.Protection);
            }
    
            Subsection = Subsection->NextSubsection;
    
        } while (Subsection != NULL);
    }
    else if (Image->GlobalSubs != NULL) {

        Count = 0;
        ASSERT (Subsection->NextSubsection == NULL);

        while (Image->GlobalSubs[Count].PteCount != 0) {

            PointerPte = RealPteBase + Image->GlobalSubs[Count].PteIndex;
            LastPte = PointerPte + Image->GlobalSubs[Count].PteCount - 1;

            MiSetSystemCodeProtection (PointerPte,
                                       LastPte,
                                       Image->GlobalSubs[Count].Protection);
    
            Count += 1;
        }

        ExFreePool (Image->GlobalSubs);
        Image->GlobalSubs = NULL;
    }

    return;
}


VOID
MiUpdateThunks (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PVOID OldAddress,
    IN PVOID NewAddress,
    IN ULONG NumberOfBytes
    )

/*++

Routine Description:

    This function updates the IATs of all the loaded modules in the system
    to handle a newly relocated image.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

    OldAddress - Supplies the old address of the DLL which was just relocated.

    NewAddress - Supplies the new address of the DLL which was just relocated.

    NumberOfBytes - Supplies the number of bytes spanned by the DLL.

Return Value:

    None.

--*/

{
    PULONG_PTR ImportThunk;
    ULONG_PTR OldAddressHigh;
    ULONG_PTR AddressDifference;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLIST_ENTRY NextEntry;
    ULONG_PTR i;
    ULONG ImportSize;

    //
    // Note this routine must not call any modules outside the kernel.
    // This is because that module may itself be the one being relocated right
    // now.
    //

    OldAddressHigh = (ULONG_PTR)((PCHAR)OldAddress + NumberOfBytes - 1);
    AddressDifference = (ULONG_PTR)NewAddress - (ULONG_PTR)OldAddress;

    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    for ( ; NextEntry != &LoaderBlock->LoadOrderListHead; NextEntry = NextEntry->Flink) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        ImportThunk = (PULONG_PTR) RtlImageDirectoryEntryToData (
                                           DataTableEntry->DllBase,
                                           TRUE,
                                           IMAGE_DIRECTORY_ENTRY_IAT,
                                           &ImportSize);

        if (ImportThunk == NULL) {
            continue;
        }

        ImportSize /= sizeof(PULONG_PTR);

        for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {
            if (*ImportThunk >= (ULONG_PTR)OldAddress && *ImportThunk <= OldAddressHigh) {
                *ImportThunk += AddressDifference;
            }
        }
    }
}


VOID
MiReloadBootLoadedDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    The kernel, HAL and boot drivers are relocated by the loader.
    All the boot drivers are then relocated again here.

    This function relocates osloader-loaded images into system PTEs.  This
    gives these images the benefits that all other drivers already enjoy,
    including :

    1. Paging of the drivers (this is more than 500K today).
    2. Write-protection of their text sections.
    3. Automatic unload of drivers on last dereference.

    Note care must be taken when processing HIGHADJ relocations more than once.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 Initialization.

--*/

{
    LOGICAL UsedLargePage;
    LOGICAL HasRelocations;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLIST_ENTRY NextEntry;
    PIMAGE_FILE_HEADER FileHeader;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_DATA_DIRECTORY DataDirectory;
    ULONG_PTR i;
    ULONG RoundedNumberOfPtes;
    ULONG NumberOfPtes;
    ULONG NumberOfLoaderPtes;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LoaderPte;
    MMPTE PteContents;
    MMPTE TempPte;
    PVOID LoaderImageAddress;
    PVOID NewImageAddress;
    NTSTATUS Status;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PteFramePage;
    PMMPTE PteFramePointer;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PCHAR RelocatedVa;
    PCHAR NonRelocatedVa;
    LOGICAL StopMoving;

#if !defined (_X86_)

    //
    // Only try to preserve low memory on x86 machines.
    //

    MmMakeLowMemory = FALSE;
#endif
    StopMoving = FALSE;

    i = 0;
    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    for ( ; NextEntry != &LoaderBlock->LoadOrderListHead; NextEntry = NextEntry->Flink) {

        //
        // Skip the kernel and the HAL.  Note their relocation sections will
        // be automatically reclaimed.
        //

        i += 1;
        if (i <= 2) {
            continue;
        }

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        NtHeader = RtlImageNtHeader (DataTableEntry->DllBase);

        //
        // Ensure that the relocation section exists and that the loader
        // hasn't freed it already.
        //

        if (NtHeader == NULL) {
            continue;
        }

        FileHeader = &NtHeader->FileHeader;

        if (FileHeader->Characteristics & IMAGE_FILE_RELOCS_STRIPPED) {
            continue;
        }

        if (IMAGE_DIRECTORY_ENTRY_BASERELOC >= NtHeader->OptionalHeader.NumberOfRvaAndSizes) {
            continue;
        }

        DataDirectory = &NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_BASERELOC];

        if (DataDirectory->VirtualAddress == 0) {
            HasRelocations = FALSE;
        }
        else {

            if (DataDirectory->VirtualAddress + DataDirectory->Size > DataTableEntry->SizeOfImage) {

                //
                // The relocation section has already been freed, the user must
                // be using an old loader that didn't save the relocations.
                //

                continue;
            }
            HasRelocations = TRUE;
        }

        LoaderImageAddress = DataTableEntry->DllBase;
        LoaderPte = MiGetPteAddress(DataTableEntry->DllBase);
        NumberOfLoaderPtes = (ULONG)((ROUND_TO_PAGES(DataTableEntry->SizeOfImage)) >> PAGE_SHIFT);

        LOCK_PFN (OldIrql);

        PointerPte = LoaderPte;
        LastPte = PointerPte + NumberOfLoaderPtes;

        while (PointerPte < LastPte) {
            ASSERT (PointerPte->u.Hard.Valid == 1);
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            //
            // Mark the page as modified so boot drivers that call
            // MmPageEntireDriver don't lose their unmodified data !
            //

            MI_SET_MODIFIED (Pfn1, 1, 0x14);

            PointerPte += 1;
        }

        UNLOCK_PFN (OldIrql);

        NumberOfPtes = NumberOfLoaderPtes;

        NewImageAddress = LoaderImageAddress;

        UsedLargePage = MiUseLargeDriverPage (NumberOfPtes,
                                              &NewImageAddress,
                                              &DataTableEntry->BaseDllName,
                                              0);

        if (UsedLargePage == TRUE) {

            //
            // This image has been loaded into a large page mapping.
            //

            RelocatedVa = NewImageAddress;
            NonRelocatedVa = (PCHAR) DataTableEntry->DllBase;
            PointerPte -= NumberOfPtes;
            goto Fixup;
        }

        //
        // Extra PTEs are allocated here to map the relocation section at the
        // new address so the image can be relocated.
        //

        PointerPte = MiReserveSystemPtes (NumberOfPtes, SystemPteSpace);

        if (PointerPte == NULL) {
            continue;
        }

        LastPte = PointerPte + NumberOfPtes;

        NewImageAddress = MiGetVirtualAddressMappedByPte (PointerPte);

        //
        // This assert is important because the assumption is made that PTEs
        // (not superpages) are mapping these drivers.
        //

        ASSERT (InitializationPhase == 0);

        //
        // If the system is configured to make low memory available for ISA
        // type drivers, then copy the boot loaded drivers now.  Otherwise
        // only PTE adjustment is done.  Presumably some day when ISA goes
        // away this code can be removed.
        //

        RelocatedVa = NewImageAddress;
        NonRelocatedVa = (PCHAR) DataTableEntry->DllBase;

        while (PointerPte < LastPte) {

            PteContents = *LoaderPte;
            ASSERT (PteContents.u.Hard.Valid == 1);

            if (MmMakeLowMemory == TRUE) {
#if DBG
                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (LoaderPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->u1.WsIndex == 0);
#endif
                LOCK_PFN (OldIrql);

                if (MmAvailablePages < MM_HIGH_LIMIT) {
                    MiEnsureAvailablePageOrWait (NULL, OldIrql);
                }

                PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

                if (PageFrameIndex < (16*1024*1024)/PAGE_SIZE) {

                    //
                    // If the frames cannot be replaced with high pages
                    // then stop copying.
                    //

#if defined (_MI_MORE_THAN_4GB_)
                  if (MiNoLowMemory == 0)
#endif
                    StopMoving = TRUE;
                }

                MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                          PageFrameIndex,
                                          MM_EXECUTE_READWRITE,
                                          PointerPte);

                MI_SET_PTE_DIRTY (TempPte);
                MI_SET_ACCESSED_IN_PTE (&TempPte, 1);

                MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPte, TempPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                MI_SET_MODIFIED (Pfn1, 1, 0x15);

                //
                // Initialize the WsIndex just like the original page had it.
                //

                Pfn1->u1.WsIndex = 0;

                UNLOCK_PFN (OldIrql);
                RtlCopyMemory (RelocatedVa, NonRelocatedVa, PAGE_SIZE);
                RelocatedVa += PAGE_SIZE;
                NonRelocatedVa += PAGE_SIZE;
            }
            else {
                MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                          PteContents.u.Hard.PageFrameNumber,
                                          MM_EXECUTE_READWRITE,
                                          PointerPte);

                MI_SET_PTE_DIRTY (TempPte);

                MI_WRITE_VALID_PTE (PointerPte, TempPte);
            }

            PointerPte += 1;
            LoaderPte += 1;
        }
        PointerPte -= NumberOfPtes;

Fixup:

        ASSERT (*(PULONG)NewImageAddress == *(PULONG)LoaderImageAddress);

        //
        // Image is now mapped at the new address.  Relocate it (again).
        //

        NtHeader->OptionalHeader.ImageBase = (ULONG_PTR)LoaderImageAddress;
        if ((MmMakeLowMemory == TRUE) || (UsedLargePage == TRUE)) {
            PIMAGE_NT_HEADERS NtHeader2;

            NtHeader2 = (PIMAGE_NT_HEADERS)((PCHAR)NtHeader + (RelocatedVa - NonRelocatedVa));
            NtHeader2->OptionalHeader.ImageBase = (ULONG_PTR)LoaderImageAddress;
        }

        if (HasRelocations == TRUE) {
            Status = (NTSTATUS)LdrRelocateImage(NewImageAddress,
                                            (CONST PCHAR)"SYSLDR",
                                            (ULONG)STATUS_SUCCESS,
                                            (ULONG)STATUS_CONFLICTING_ADDRESSES,
                                            (ULONG)STATUS_INVALID_IMAGE_FORMAT
                                            );

            if (!NT_SUCCESS(Status)) {

                if (UsedLargePage == TRUE) {
                    ASSERT (MI_PDE_MAPS_LARGE_PAGE (MiGetPdeAddress (NewImageAddress)));
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPdeAddress (NewImageAddress)) + MiGetPteOffset (NewImageAddress);

                    RoundedNumberOfPtes = MI_ROUND_TO_SIZE (NumberOfPtes,
                                              MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT);
                    MiUnmapLargePages (NewImageAddress,
                                       RoundedNumberOfPtes << PAGE_SHIFT);

                    MiRemoveCachedRange (PageFrameIndex, PageFrameIndex + RoundedNumberOfPtes - 1);
                    MiFreeContiguousPages (PageFrameIndex, RoundedNumberOfPtes);
                }

                if (MmMakeLowMemory == TRUE) {

                    while (PointerPte < LastPte) {

                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

                        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

                        MI_SET_PFN_DELETED (Pfn1);
                        MiDecrementShareCount (Pfn1, PageFrameIndex);

                        PointerPte += 1;
                    }
                    PointerPte -= NumberOfPtes;
                }

                MiReleaseSystemPtes (PointerPte, NumberOfPtes, SystemPteSpace);

                if (StopMoving == TRUE) {
                    MmMakeLowMemory = FALSE;
                }

                continue;
            }
        }

        //
        // Update the IATs for all other loaded modules that reference this one.
        //

        NonRelocatedVa = (PCHAR) DataTableEntry->DllBase;
        DataTableEntry->DllBase = NewImageAddress;

        MiUpdateThunks (LoaderBlock,
                        LoaderImageAddress,
                        NewImageAddress,
                        DataTableEntry->SizeOfImage);


        //
        // Update the loaded module list entry.
        //

        DataTableEntry->Flags |= LDRP_SYSTEM_MAPPED;
        DataTableEntry->DllBase = NewImageAddress;
        DataTableEntry->EntryPoint =
            (PVOID)((PCHAR)NewImageAddress + NtHeader->OptionalHeader.AddressOfEntryPoint);
        DataTableEntry->SizeOfImage = NumberOfPtes << PAGE_SHIFT;

        //
        // Update the exception table data info
        //

        MiCaptureImageExceptionValues (DataTableEntry);

        //
        // Update the PFNs of the image to support trimming.
        // Note that the loader addresses are freed now so no references
        // to it are permitted after this point.
        //

        LoaderPte = MiGetPteAddress (NonRelocatedVa);

        LOCK_PFN (OldIrql);

        while (PointerPte < LastPte) {

            ASSERT ((UsedLargePage == TRUE) || (PointerPte->u.Hard.Valid == 1));

            if ((MmMakeLowMemory == TRUE) || (UsedLargePage == TRUE)) {

                ASSERT (LoaderPte->u.Hard.Valid == 1);
                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (LoaderPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

                //
                // Decrement the share count on the original page table
                // page so it can be freed.
                //

                MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

                MI_SET_PFN_DELETED (Pfn1);
                MiDecrementShareCount (Pfn1, PageFrameIndex);
                LoaderPte += 1;
            }
            else {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

                //
                // Decrement the share count on the original page table
                // page so it can be freed.
                //

                MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);
                Pfn1->PteAddress->u.Long = 0;

                //
                // Chain the PFN entry to its new page table.
                //

                PteFramePointer = MiGetPteAddress(PointerPte);
                PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);

                Pfn1->u4.PteFrame = PteFramePage;
                Pfn1->PteAddress = PointerPte;

                //
                // Increment the share count for the page table page that now
                // contains the PTE that was copied.
                //

                Pfn2 = MI_PFN_ELEMENT (PteFramePage);
                Pfn2->u2.ShareCount += 1;
            }

            PointerPte += 1;
        }

        UNLOCK_PFN (OldIrql);

        //
        // The physical pages mapping the relocation section are freed
        // later with the rest of the initialization code spanned by the
        // DataTableEntry->SizeOfImage.
        //

        if (StopMoving == TRUE) {
            MmMakeLowMemory = FALSE;
        }
    }

    //
    // Flush the instruction cache on all systems in the configuration.
    //

    KeSweepIcache (TRUE);
}

#if defined(_X86_) || defined(_AMD64_)
PMMPTE MiKernelResourceStartPte;
PMMPTE MiKernelResourceEndPte;
#endif

VOID
MiLocateKernelSections (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function locates the resource section in the kernel so it can be
    made readwrite if we bugcheck later, as the bugcheck code will write
    into it.

Arguments:

    DataTableEntry - Supplies the kernel's data table entry.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 Initialization.

--*/

{
    ULONG Span;
    PVOID CurrentBase;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    LONG i;
    PMMPTE PointerPte;
    PVOID SectionBaseAddress;

    CurrentBase = (PVOID) DataTableEntry->DllBase;

    NtHeader = RtlImageNtHeader (CurrentBase);

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                            sizeof(ULONG) +
                            sizeof(IMAGE_FILE_HEADER) +
                            NtHeader->FileHeader.SizeOfOptionalHeader);

    //
    // From the image header, locate the section named '.rsrc'.
    //

    i = NtHeader->FileHeader.NumberOfSections;

    PointerPte = NULL;

    while (i > 0) {

        SectionBaseAddress = SECTION_BASE_ADDRESS(SectionTableEntry);

        //
        // Generally, SizeOfRawData is larger than VirtualSize for each
        // section because it includes the padding to get to the subsection
        // alignment boundary.  However, if the image is linked with
        // subsection alignment == native page alignment, the linker will
        // have VirtualSize be much larger than SizeOfRawData because it
        // will account for all the bss.
        //

        Span = SectionTableEntry->SizeOfRawData;

        if (Span < SectionTableEntry->Misc.VirtualSize) {
            Span = SectionTableEntry->Misc.VirtualSize;
        }

#if defined(_X86_) || defined(_AMD64_)
        if (*(PULONG)SectionTableEntry->Name == 'rsr.') {

            MiKernelResourceStartPte = MiGetPteAddress ((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress);

            MiKernelResourceEndPte = MiGetPteAddress (ROUND_TO_PAGES((ULONG_PTR)CurrentBase +
                         SectionTableEntry->VirtualAddress + Span));
            break;
        }
#endif
        if (*(PULONG)SectionTableEntry->Name == 'LOOP') {
            if (*(PULONG)&SectionTableEntry->Name[4] == 'EDOC') {
                ExPoolCodeStart = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress);
                ExPoolCodeEnd = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress +
                                             Span);
            }
            else if (*(PUSHORT)&SectionTableEntry->Name[4] == 'IM') {
                MmPoolCodeStart = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress);
                MmPoolCodeEnd = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress +
                                             Span);
            }
        }
        else if ((*(PULONG)SectionTableEntry->Name == 'YSIM') &&
                 (*(PULONG)&SectionTableEntry->Name[4] == 'ETPS')) {
                MmPteCodeStart = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress);
                MmPteCodeEnd = (PVOID)((ULONG_PTR)CurrentBase +
                                             SectionTableEntry->VirtualAddress +
                                             Span);
        }

        i -= 1;
        SectionTableEntry += 1;
    }
}

VOID
MmMakeKernelResourceSectionWritable (
    VOID
    )

/*++

Routine Description:

    This function makes the kernel's resource section readwrite so the bugcheck
    code can write into it.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  Any IRQL.

--*/

{
#if defined(_X86_) || defined(_AMD64_)
    MMPTE TempPte;
    MMPTE PteContents;
    PMMPTE PointerPte;

    if (MiKernelResourceStartPte == NULL) {
        return;
    }

    PointerPte = MiKernelResourceStartPte;

    if (MI_IS_PHYSICAL_ADDRESS (MiGetVirtualAddressMappedByPte (PointerPte))) {

        //
        // Mapped physically, doesn't need to be made readwrite.
        //

        return;
    }

    //
    // Since the entry state and IRQL are unknown, just go through the
    // PTEs without a lock and make them all readwrite.
    //

    do {
        PteContents = *PointerPte;
#if defined(NT_UP)
        if (PteContents.u.Hard.Write == 0)
#else
        if (PteContents.u.Hard.Writable == 0)
#endif
        {
            MI_MAKE_VALID_KERNEL_PTE (TempPte,
                                      PteContents.u.Hard.PageFrameNumber,
                                      MM_READWRITE,
                                      PointerPte);
#if !defined(NT_UP)
            TempPte.u.Hard.Writable = 1;
#endif
            MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);
        }
        PointerPte += 1;
    } while (PointerPte < MiKernelResourceEndPte);

    //
    // Don't do this more than once.
    //

    MiKernelResourceStartPte = NULL;

    //
    // Only flush this processor as the state of the others is unknown.
    //

    MI_FLUSH_CURRENT_TB ();
#endif
}

#ifdef i386
PVOID PsNtosImageBase = (PVOID)0x80100000;
#else
PVOID PsNtosImageBase;
#endif

#if DBG
PVOID PsNtosImageEnd;
#endif

#if defined (_WIN64)

INVERTED_FUNCTION_TABLE PsInvertedFunctionTable = {
    0, MAXIMUM_INVERTED_FUNCTION_TABLE_SIZE, FALSE};

#endif

LIST_ENTRY PsLoadedModuleList;
ERESOURCE PsLoadedModuleResource;

LOGICAL
MiInitializeLoadedModuleList (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function initializes the loaded module list based on the LoaderBlock.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 Initialization.

--*/

{
    SIZE_T CommittedPages;
    SIZE_T DataTableEntrySize;
    PLIST_ENTRY NextEntry;
    PLIST_ENTRY NextEntryEnd;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry1;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry2;

    CommittedPages = 0;

    //
    // Initialize the loaded module list executive resource and spin lock.
    //

    ExInitializeResourceLite (&PsLoadedModuleResource);
    KeInitializeSpinLock (&PsLoadedModuleSpinLock);

    InitializeListHead (&PsLoadedModuleList);

    //
    // Scan the loaded module list and allocate and initialize a data table
    // entry for each module. The data table entry is inserted in the loaded
    // module list and the initialization order list in the order specified
    // in the loader parameter block. The data table entry is inserted in the
    // memory order list in memory order.
    //

    NextEntry = LoaderBlock->LoadOrderListHead.Flink;
    NextEntryEnd = &LoaderBlock->LoadOrderListHead;

    DataTableEntry2 = CONTAINING_RECORD (NextEntry,
                                         KLDR_DATA_TABLE_ENTRY,
                                         InLoadOrderLinks);

    PsNtosImageBase = DataTableEntry2->DllBase;

#if DBG
    PsNtosImageEnd = (PVOID) ((ULONG_PTR) DataTableEntry2->DllBase + DataTableEntry2->SizeOfImage);
#endif

    MiLocateKernelSections (DataTableEntry2);

    while (NextEntry != NextEntryEnd) {

        DataTableEntry2 = CONTAINING_RECORD(NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        //
        // Allocate a data table entry.
        //

        DataTableEntrySize = sizeof (KLDR_DATA_TABLE_ENTRY) +
            DataTableEntry2->BaseDllName.MaximumLength + sizeof(UNICODE_NULL);

        DataTableEntry1 = ExAllocatePoolWithTag (NonPagedPool,
                                                 DataTableEntrySize,
                                                 'dLmM');

        if (DataTableEntry1 == NULL) {
            return FALSE;
        }

        //
        // Copy the data table entry.
        //

        *DataTableEntry1 = *DataTableEntry2;

        //
        // Clear fields we may use later so they don't inherit irrelevant
        // loader values.
        //

        ((PKLDR_DATA_TABLE_ENTRY)DataTableEntry1)->NonPagedDebugInfo = NULL;
        DataTableEntry1->PatchInformation = NULL;

        DataTableEntry1->FullDllName.Buffer = ExAllocatePoolWithTag (PagedPool,
            DataTableEntry2->FullDllName.MaximumLength + sizeof(UNICODE_NULL),
            'TDmM');

        if (DataTableEntry1->FullDllName.Buffer == NULL) {
            ExFreePool (DataTableEntry1);
            return FALSE;
        }

        DataTableEntry1->BaseDllName.Buffer = (PWSTR)((ULONG_PTR)DataTableEntry1 + sizeof (KLDR_DATA_TABLE_ENTRY));

        //
        // Copy the strings.
        //

        RtlCopyMemory (DataTableEntry1->FullDllName.Buffer,
                       DataTableEntry2->FullDllName.Buffer,
                       DataTableEntry1->FullDllName.MaximumLength);

        RtlCopyMemory (DataTableEntry1->BaseDllName.Buffer,
                       DataTableEntry2->BaseDllName.Buffer,
                       DataTableEntry1->BaseDllName.MaximumLength);

        DataTableEntry1->BaseDllName.Buffer[DataTableEntry1->BaseDllName.Length/sizeof(WCHAR)] = UNICODE_NULL;

        //
        // Always charge commitment regardless of whether we were able to
        // relocate the driver, use large pages, etc.
        //

        CommittedPages += (DataTableEntry1->SizeOfImage >> PAGE_SHIFT);

        //
        // Calculate exception pointers
        //

        MiCaptureImageExceptionValues(DataTableEntry1);

        //
        // Insert the data table entry in the load order list in the order
        // they are specified.
        //

        InsertTailList (&PsLoadedModuleList,
                        &DataTableEntry1->InLoadOrderLinks);

#if defined (_WIN64)

        RtlInsertInvertedFunctionTable (&PsInvertedFunctionTable,
                                        DataTableEntry1->DllBase,
                                        DataTableEntry1->SizeOfImage);

#endif

        NextEntry = NextEntry->Flink;
    }

    //
    // Charge commitment for each boot loaded driver so that if unloads
    // later, the return will balance.  Note that the actual number of
    // free pages is not changing now so the commit limits need to be
    // bumped by the same amount.
    //
    // Resident available does not need to be charged here because it
    // has been already (by virtue of being snapped from available pages).
    //

    MM_TRACK_COMMIT (MM_DBG_COMMIT_LOAD_SYSTEM_IMAGE_TEMP, CommittedPages);

    MmTotalCommittedPages += CommittedPages;
    MmTotalCommitLimit += CommittedPages;
    MmTotalCommitLimitMaximum += CommittedPages;

    MiBuildImportsForBootDrivers ();

    return TRUE;
}

NTSTATUS
MmCallDllInitialize (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry,
    IN PLIST_ENTRY ModuleListHead
    )

/*++

Routine Description:

    This function calls the DLL's initialize routine.

Arguments:

    DataTableEntry - Supplies the kernel's data table entry.

Return Value:

    Various NTSTATUS error codes.

Environment:

    Kernel mode.

--*/

{
    NTSTATUS st;
    PWCHAR Dot;
    PMM_DLL_INITIALIZE Func;
    UNICODE_STRING RegistryPath;
    UNICODE_STRING ImportName;
    ULONG ThunksAdded;

    Func = (PMM_DLL_INITIALIZE)(ULONG_PTR)MiLocateExportName (DataTableEntry->DllBase, "DllInitialize");

    if (!Func) {
        return STATUS_SUCCESS;
    }

    ImportName.MaximumLength = DataTableEntry->BaseDllName.Length;
    ImportName.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                               ImportName.MaximumLength,
                                               'TDmM');

    if (ImportName.Buffer == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    ImportName.Length = DataTableEntry->BaseDllName.Length;
    RtlCopyMemory (ImportName.Buffer,
                   DataTableEntry->BaseDllName.Buffer,
                   ImportName.Length);

    RegistryPath.MaximumLength = (USHORT)(CmRegistryMachineSystemCurrentControlSetServices.Length +
                                    ImportName.Length +
                                    2*sizeof(WCHAR));

    RegistryPath.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                 RegistryPath.MaximumLength,
                                                 'TDmM');

    if (RegistryPath.Buffer == NULL) {
        ExFreePool (ImportName.Buffer);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RegistryPath.Length = CmRegistryMachineSystemCurrentControlSetServices.Length;
    RtlCopyMemory (RegistryPath.Buffer,
                   CmRegistryMachineSystemCurrentControlSetServices.Buffer,
                   CmRegistryMachineSystemCurrentControlSetServices.Length);

    RtlAppendUnicodeToString (&RegistryPath, (const PUSHORT)L"\\");
    Dot = wcschr (ImportName.Buffer, L'.');
    if (Dot) {
        ImportName.Length = (USHORT)((Dot - ImportName.Buffer) * sizeof(WCHAR));
    }

    RtlAppendUnicodeStringToString (&RegistryPath, &ImportName);
    ExFreePool (ImportName.Buffer);

    //
    // Save the number of verifier thunks currently added so we know
    // if this activation adds any.  To extend the thunk list, the module
    // performs an NtSetSystemInformation call which calls back to the
    // verifier's MmAddVerifierThunks, which increments MiVerifierThunksAdded.
    //

    ThunksAdded = MiVerifierThunksAdded;

    //
    // Invoke the DLL's initialization routine.
    //

    st = Func (&RegistryPath);

    ExFreePool (RegistryPath.Buffer);

    //
    // If the module's initialization routine succeeded, and if it extended
    // the verifier thunk list, and this is boot time, reapply the verifier
    // to the loaded modules.
    //
    // Note that boot time is the special case because after boot time, Mm
    // loads all the DLLs itself and a DLL initialize is thus guaranteed to
    // complete and add its thunks before the importing driver load finishes.
    // Since the importing driver is only thunked after its load finishes,
    // ordering implicitly guarantees that all DLL-registered thunks are
    // properly factored in to the importing driver.
    //
    // Boot time is special because the loader (not the Mm) already loaded
    // the DLLs *AND* the importing drivers so we have to look over our
    // shoulder and make it all right after the fact.
    //

    if ((NT_SUCCESS(st)) &&
        (MiFirstDriverLoadEver == 0) &&
        (MiVerifierThunksAdded != ThunksAdded)) {

        MiReApplyVerifierToLoadedModules (ModuleListHead);
    }

    return st;
}

NTKERNELAPI
PVOID
MmGetSystemRoutineAddress (
    __in PUNICODE_STRING SystemRoutineName
    )

/*++

Routine Description:

    This function returns the address of the argument function pointer if
    it is in the kernel or HAL, NULL if it is not.

Arguments:

    SystemRoutineName - Supplies the name of the desired routine.

Return Value:

    Non-NULL function pointer if successful.  NULL if not.

Environment:

    Kernel mode, PASSIVE_LEVEL, arbitrary process context.

--*/

{
    PKTHREAD CurrentThread;
    NTSTATUS Status;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ANSI_STRING AnsiString;
    PLIST_ENTRY NextEntry;
    UNICODE_STRING KernelString;
    UNICODE_STRING HalString;
    PVOID FunctionAddress;
    LOGICAL Found;
    ULONG EntriesChecked;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    EntriesChecked = 0;
    FunctionAddress = NULL;

    KernelString.Buffer = (const PUSHORT) KERNEL_NAME;
    KernelString.Length = sizeof (KERNEL_NAME) - sizeof (WCHAR);
    KernelString.MaximumLength = sizeof KERNEL_NAME;

    HalString.Buffer = (const PUSHORT) HAL_NAME;
    HalString.Length = sizeof (HAL_NAME) - sizeof (WCHAR);
    HalString.MaximumLength = sizeof HAL_NAME;

    do {
        Status = RtlUnicodeStringToAnsiString (&AnsiString,
                                               SystemRoutineName,
                                               TRUE);

        if (NT_SUCCESS (Status)) {
            break;
        }

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

    } while (TRUE);

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    //
    // Check only the kernel and the HAL for exports.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        Found = FALSE;

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        if (RtlEqualUnicodeString (&KernelString,
                                   &DataTableEntry->BaseDllName,
                                   TRUE)) {

            Found = TRUE;
            EntriesChecked += 1;

        }
        else if (RtlEqualUnicodeString (&HalString,
                                        &DataTableEntry->BaseDllName,
                                        TRUE)) {

            Found = TRUE;
            EntriesChecked += 1;
        }

        if (Found == TRUE) {

            FunctionAddress = MiFindExportedRoutineByName (DataTableEntry->DllBase,
                                                           &AnsiString);

            if (FunctionAddress != NULL) {
                break;
            }

            if (EntriesChecked == 2) {
                break;
            }
        }

        NextEntry = NextEntry->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    RtlFreeAnsiString (&AnsiString);

    return FunctionAddress;
}

PVOID
MiFindExportedRoutineByName (
    IN PVOID DllBase,
    IN PANSI_STRING AnsiImageRoutineName
    )

/*++

Routine Description:

    This function searches the argument module looking for the requested
    exported function name.

Arguments:

    DllBase - Supplies the base address of the requested module.

    AnsiImageRoutineName - Supplies the ANSI routine name being searched for.

Return Value:

    The virtual address of the requested routine or NULL if not found.

--*/

{
    USHORT OrdinalNumber;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PULONG Addr;
    LONG High;
    LONG Low;
    LONG Middle;
    LONG Result;
    ULONG ExportSize;
    PVOID FunctionAddress;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;

    PAGED_CODE();

    ExportDirectory = (PIMAGE_EXPORT_DIRECTORY) RtlImageDirectoryEntryToData (
                                DllBase,
                                TRUE,
                                IMAGE_DIRECTORY_ENTRY_EXPORT,
                                &ExportSize);

    if (ExportDirectory == NULL) {
        return NULL;
    }

    //
    // Initialize the pointer to the array of RVA-based ansi export strings.
    //

    NameTableBase = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNames);

    //
    // Initialize the pointer to the array of USHORT ordinal numbers.
    //

    NameOrdinalTableBase = (PUSHORT)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

    //
    // Lookup the desired name in the name table using a binary search.
    //

    Low = 0;
    Middle = 0;
    High = ExportDirectory->NumberOfNames - 1;

    while (High >= Low) {

        //
        // Compute the next probe index and compare the import name
        // with the export name entry.
        //

        Middle = (Low + High) >> 1;

        Result = strcmp (AnsiImageRoutineName->Buffer,
                         (PCHAR)DllBase + NameTableBase[Middle]);

        if (Result < 0) {
            High = Middle - 1;
        }
        else if (Result > 0) {
            Low = Middle + 1;
        }
        else {
            break;
        }
    }

    //
    // If the high index is less than the low index, then a matching
    // table entry was not found. Otherwise, get the ordinal number
    // from the ordinal table.
    //

    if (High < Low) {
        return NULL;
    }

    OrdinalNumber = NameOrdinalTableBase[Middle];

    //
    // If the OrdinalNumber is not within the Export Address Table,
    // then this image does not implement the function.  Return not found.
    //

    if ((ULONG)OrdinalNumber >= ExportDirectory->NumberOfFunctions) {
        return NULL;
    }

    //
    // Index into the array of RVA export addresses by ordinal number.
    //

    Addr = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfFunctions);

    FunctionAddress = (PVOID)((PCHAR)DllBase + Addr[OrdinalNumber]);

    //
    // Forwarders are not used by the kernel and HAL to each other.
    //

    ASSERT ((FunctionAddress <= (PVOID)ExportDirectory) ||
            (FunctionAddress >= (PVOID)((PCHAR)ExportDirectory + ExportSize)));

    return FunctionAddress;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\wslist.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   wslist.c

Abstract:

    This module contains routines which operate on the working
    set list structure.

--*/

#include "mi.h"

extern WSLE_NUMBER MmMaximumWorkingSetSize;

ULONG MmSystemCodePage;
ULONG MmSystemCachePage;
ULONG MmPagedPoolPage;
ULONG MmSystemDriverPage;

extern LOGICAL MiReplacing;
extern ULONG MmStandbyRePurposed;

#define MM_RETRY_COUNT 2

extern PFN_NUMBER MmTransitionSharedPages;
PFN_NUMBER MmTransitionSharedPagesPeak;

extern LOGICAL MiTrimRemovalPagesOnly;

typedef enum _WSLE_ALLOCATION_TYPE {
    WsleAllocationAny = 0,
    WsleAllocationReplace = 1,
    WsleAllocationDontTrim = 2
} WSLE_ALLOCATION_TYPE, *PWSLE_ALLOCATION_TYPE;

VOID
MiDoReplacement (
    IN PMMSUPPORT WsInfo,
    IN WSLE_ALLOCATION_TYPE Flags
    );

VOID
MiReplaceWorkingSetEntry (
    IN PMMSUPPORT WsInfo,
    IN WSLE_ALLOCATION_TYPE Flags
    );

VOID
MiUpdateWsle (
    IN OUT PWSLE_NUMBER DesiredIndex,
    IN PVOID VirtualAddress,
    IN PMMSUPPORT WsInfo,
    IN PMMPFN Pfn,
    IN ULONG_PTR WsleMask
    );

VOID
MiCheckWsleHash (
    IN PMMWSL WorkingSetList
    );

LOGICAL
MiEliminateWorkingSetEntry (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn,
    IN PMMSUPPORT WsInfo,
    IN LOGICAL Force
    );

ULONG
MiAddWorkingSetPage (
    IN PMMSUPPORT WsInfo
    );

VOID
MiRemoveWorkingSetPages (
    IN PMMSUPPORT WsInfo
    );

VOID
MiCheckNullIndex (
    IN PMMWSL WorkingSetList
    );

#if DBG
ULONG MiTbDebug;
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MiInitializeSessionWsSupport)
#pragma alloc_text(PAGE, MmAssignProcessToJob)
#pragma alloc_text(PAGE, MiInitializeWorkingSetList)
#pragma alloc_text(PAGELK, MmAdjustWorkingSetSize)
#pragma alloc_text(PAGELK, MmAdjustWorkingSetSizeEx)
#pragma alloc_text(PAGELK, MiSessionInitializeWorkingSetList)

#endif

ULONG MiWsleFailures;


WSLE_NUMBER
MiAllocateWsle (
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG_PTR WsleMask
    )

/*++

Routine Description:

    This function examines the working set list for the specified
    working set and locates an entry to contain a new page.
    
    If the memory is not tight, the new page is added without removing a page.

    If memory is tight (or this working set is at its limit), a page
    is removed from the working set and the new page added in its place.

Arguments:

    WsInfo - Supplies the working set list.

    PointerPte - Supplies the PTE of the virtual address to insert.

    Pfn1 - Supplies the PFN entry of the virtual address being inserted.  If
           this pointer has the low bit set, no trimming can be done at this
           time (because it is a WSLE hash table page insertion).  Strip the
           low bit for these cases.

    WsleMask - Supplies the mask to logical OR into the new working set entry.

Return Value:

    Returns the working set index which was used to insert the specified entry,
    or 0 if no index was available.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    PVOID VirtualAddress;
    PMMWSLE Wsle;
    PMMWSL WorkingSetList;
    WSLE_NUMBER WorkingSetIndex;

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    //
    // Update page fault counts.
    //

    WsInfo->PageFaultCount += 1;
    InterlockedIncrement (&KeGetCurrentPrcb ()->MmPageFaultCount);

    //
    // Determine if a page should be removed from the working set to make
    // room for the new page.  If so, remove it.
    //

    if ((ULONG_PTR)Pfn1 & 0x1) {
        MiDoReplacement (WsInfo, WsleAllocationDontTrim);
        if (WorkingSetList->FirstFree == WSLE_NULL_INDEX) {
            return 0;
        }
        Pfn1 = (PMMPFN)((ULONG_PTR)Pfn1 & ~0x1);
    }
    else {
        MiDoReplacement (WsInfo, WsleAllocationAny);
    
        if (WorkingSetList->FirstFree == WSLE_NULL_INDEX) {
    
            //
            // Add more pages to the working set list structure.
            //
    
            if (MiAddWorkingSetPage (WsInfo) == FALSE) {
    
                //
                // No page was added to the working set list structure.
                // We must replace a page within this working set.
                //
    
                MiDoReplacement (WsInfo, WsleAllocationReplace);
    
                if (WorkingSetList->FirstFree == WSLE_NULL_INDEX) {
                    MiWsleFailures += 1;
                    return 0;
                }
            }
        }
    }

    //
    // Get the working set entry from the free list.
    //

    ASSERT (WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle);

    ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

    WorkingSetIndex = WorkingSetList->FirstFree;
    WorkingSetList->FirstFree = (WSLE_NUMBER)(Wsle[WorkingSetIndex].u1.Long >> MM_FREE_WSLE_SHIFT);

    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

    ASSERT (WsInfo->WorkingSetSize <= (WorkingSetList->LastInitializedWsle + 1));
    WsInfo->WorkingSetSize += 1;

    if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
        InterlockedIncrementSizeT (&MmPagesAboveWsMinimum);
    }

    if (WsInfo->WorkingSetSize > WsInfo->PeakWorkingSetSize) {
        WsInfo->PeakWorkingSetSize = WsInfo->WorkingSetSize;
    }

    if (WsInfo == &MmSystemCacheWs) {
        if (WsInfo->WorkingSetSize + MmTransitionSharedPages > MmTransitionSharedPagesPeak) {
            MmTransitionSharedPagesPeak = WsInfo->WorkingSetSize + MmTransitionSharedPages;
        }
    }

    if (WorkingSetIndex > WorkingSetList->LastEntry) {
        WorkingSetList->LastEntry = WorkingSetIndex;
    }

    //
    // The returned entry is guaranteed to be available at this point.
    //

    ASSERT (Wsle[WorkingSetIndex].u1.e1.Valid == 0);

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    MiUpdateWsle (&WorkingSetIndex, VirtualAddress, WsInfo, Pfn1, WsleMask);

    if (WsleMask != 0) {
        Wsle[WorkingSetIndex].u1.Long |= WsleMask;
    }

#if DBG
    if (MI_IS_SYSTEM_CACHE_ADDRESS (VirtualAddress)) {
        ASSERT (MmSystemCacheWsle[WorkingSetIndex].u1.e1.Protection == MM_ZERO_ACCESS);
    }
#endif

    MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

    return WorkingSetIndex;
}

VOID
MiDoReplacement (
    IN PMMSUPPORT WsInfo,
    IN WSLE_ALLOCATION_TYPE Flags
    )

/*++

Routine Description:

    This function determines whether the working set should be
    grown or if a page should be replaced.  Replacement is
    done here if deemed necessary.

Arguments:

    WsInfo - Supplies the working set information structure to replace within.

    Flags - Supplies 0 if replacement is not required.
            Supplies 1 if replacement is desired.
            Supplies 2 if working set trimming must not be done here - this
            is only used for inserting new WSLE hash table pages as a trim
            would not know how to interpret them.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    KIRQL OldIrql;
    WSLE_NUMBER PagesTrimmed;
    ULONG MemoryMaker;
    PMMWSL WorkingSetList;
    WSLE_NUMBER CurrentSize;
    LARGE_INTEGER CurrentTime;
    PFN_NUMBER Dummy1;
    PFN_NUMBER Dummy2;
    WSLE_NUMBER Trim;
    ULONG TrimAge;
    ULONG GrowthSinceLastEstimate;
    WSLE_ALLOCATION_TYPE TrimFlags;

    TrimFlags = Flags;
    Flags &= ~WsleAllocationDontTrim;

    WorkingSetList = WsInfo->VmWorkingSetList;
    GrowthSinceLastEstimate = 1;

    PagesTrimmed = 0;

    //
    // Determine whether the working set can be grown or whether a
    // page needs to be replaced.
    //

recheck:

    if (WsInfo->WorkingSetSize >= WsInfo->MinimumWorkingSetSize) {

        if ((WsInfo->Flags.ForceTrim == 1) && (TrimFlags != WsleAllocationDontTrim)) {

            //
            // The working set manager cannot attach to this process
            // to trim it.  Force a trim now and update the working
            // set manager's fields properly to indicate a trim occurred.
            //

            Trim = WsInfo->Claim >>
                            ((WsInfo->Flags.MemoryPriority == MEMORY_PRIORITY_FOREGROUND)
                                ? MI_FOREGROUND_CLAIM_AVAILABLE_SHIFT
                                : MI_BACKGROUND_CLAIM_AVAILABLE_SHIFT);

            if (MmAvailablePages < MM_HIGH_LIMIT + 64) {
                if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
                    Trim = (WsInfo->WorkingSetSize - WsInfo->MinimumWorkingSetSize) >> 2;
                }
                TrimAge = MI_PASS4_TRIM_AGE;
            }
            else {
                TrimAge = MI_PASS0_TRIM_AGE;
            }

            PagesTrimmed += MiTrimWorkingSet (Trim, WsInfo, TrimAge);

            MiAgeWorkingSet (WsInfo,
                             TRUE,
                             NULL,
                             &Dummy1,
                             &Dummy2);

            KeQuerySystemTime (&CurrentTime);

            LOCK_EXPANSION (OldIrql);
            WsInfo->LastTrimTime = CurrentTime;
            WsInfo->Flags.ForceTrim = 0;
            UNLOCK_EXPANSION (OldIrql);

            goto recheck;
        }

        CurrentSize = WsInfo->WorkingSetSize;
        ASSERT (CurrentSize <= (WorkingSetList->LastInitializedWsle + 1));

        if ((WsInfo->Flags.MaximumWorkingSetHard) &&
            (CurrentSize >= WsInfo->MaximumWorkingSetSize)) {

            //
            // This is an enforced working set maximum triggering a replace.
            //

            MiReplaceWorkingSetEntry (WsInfo, Flags);

            return;
        }

        //
        // Don't grow if :
        //      - there aren't any pages to take
        //      - or if we are growing too much in this time interval
        //        and there isn't much memory available
        //

        MemoryMaker = PsGetCurrentThread()->MemoryMaker;

        if ((Flags == WsleAllocationReplace) ||
            ((MmAvailablePages < MM_VERY_HIGH_LIMIT) &&
                ((MmAvailablePages == 0) ||
                 (MI_WS_GROWING_TOO_FAST(WsInfo)) && (MemoryMaker == 0)))) {

            //
            // Can't grow this one.
            //

            MiReplacing = TRUE;

            if ((Flags == WsleAllocationReplace) || (MemoryMaker == 0)) {

                MiReplaceWorkingSetEntry (WsInfo, Flags);

                //
                // Set the must trim flag because this could be a realtime
                // thread where the fault straddles a page boundary.  If
                // it's realtime, the balance set manager will never get to
                // run and the thread will endlessly replace one WSL entry
                // with the other half of the straddler.  Setting this flag
                // guarantees the next fault will guarantee a forced trim
                // and allow a reasonable available page threshold trim
                // calculation since GrowthSinceLastEstimate will be
                // cleared.
                //

                LOCK_EXPANSION (OldIrql);
                WsInfo->Flags.ForceTrim = 1;
                UNLOCK_EXPANSION (OldIrql);

                GrowthSinceLastEstimate = 0;
            }
            else {

                //
                // If we've only trimmed a single page, then don't force
                // replacement on the next fault.  This prevents a single
                // instruction causing alternating faults on the referenced
                // code & data in a (realtime) thread from looping endlessly.
                //

                if (PagesTrimmed > 1) {

                    LOCK_EXPANSION (OldIrql);
                    WsInfo->Flags.ForceTrim = 1;
                    UNLOCK_EXPANSION (OldIrql);
                }
            }
        }
    }

    //
    // If there isn't enough memory to allow growth, find a good page
    // to remove and remove it.
    //

    WsInfo->GrowthSinceLastEstimate += GrowthSinceLastEstimate;

    return;
}


NTSTATUS
MmEnforceWorkingSetLimit (
    IN PEPROCESS Process,
    IN ULONG Flags
    )

/*++

Routine Description:

    This function enables/disables hard enforcement of the working set minimums
    and maximums for the specified WsInfo.

Arguments:

    Process - Supplies the target process.

    Flags - Supplies new flags (MM_WORKING_SET_MAX_HARD_ENABLE, etc).

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APCs disabled.  The working set mutex must NOT be held.
    The caller guarantees that the target WsInfo cannot go away.

--*/

{
    KIRQL OldIrql;
    PMMSUPPORT WsInfo;
    PETHREAD Thread;
    MMSUPPORT_FLAGS PreviousBits;
    MMSUPPORT_FLAGS TempBits = {0};
    LOGICAL Attached;
    NTSTATUS Status;
    KAPC_STATE ApcState;

    WsInfo = &Process->Vm;
    Status = STATUS_SUCCESS;

    if (Flags & MM_WORKING_SET_MIN_HARD_ENABLE) {
        Flags &= ~MM_WORKING_SET_MIN_HARD_DISABLE;
        TempBits.MinimumWorkingSetHard = 1;
    }

    if (Flags & MM_WORKING_SET_MAX_HARD_ENABLE) {
        Flags &= ~MM_WORKING_SET_MAX_HARD_DISABLE;
        TempBits.MaximumWorkingSetHard = 1;
    }

    Attached = FALSE;

    if (PsGetCurrentProcess() != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Thread = PsGetCurrentThread ();

    LOCK_WS (Thread, Process);

    LOCK_EXPANSION (OldIrql);

    if (Flags & MM_WORKING_SET_MIN_HARD_DISABLE) {
        WsInfo->Flags.MinimumWorkingSetHard = 0;
    }

    if (Flags & MM_WORKING_SET_MAX_HARD_DISABLE) {
        WsInfo->Flags.MaximumWorkingSetHard = 0;
    }

    PreviousBits = WsInfo->Flags;

    //
    // If the caller's request will result in hard enforcement of both limits
    // is enabled, then check whether the current minimum and maximum working
    // set values will guarantee forward progress even in pathological
    // scenarios.
    //

    if (PreviousBits.MinimumWorkingSetHard == 1) {
        TempBits.MinimumWorkingSetHard = 1;
    }

    if (PreviousBits.MaximumWorkingSetHard == 1) {
        TempBits.MaximumWorkingSetHard = 1;
    }

    if ((TempBits.MinimumWorkingSetHard == 1) &&
        (TempBits.MaximumWorkingSetHard == 1)) {

        //
        // Net result is hard enforcement on both limits so check that the
        // two limits cannot result in lack of forward progress.
        //

        if (WsInfo->MinimumWorkingSetSize + MM_FLUID_WORKING_SET >= WsInfo->MaximumWorkingSetSize) {
            Status = STATUS_BAD_WORKING_SET_LIMIT;
            goto bail;
        }
    }

    if (Flags & MM_WORKING_SET_MIN_HARD_ENABLE) {
        WsInfo->Flags.MinimumWorkingSetHard = 1;
    }

    if (Flags & MM_WORKING_SET_MAX_HARD_ENABLE) {
        WsInfo->Flags.MaximumWorkingSetHard = 1;
    }

bail:

    UNLOCK_EXPANSION (OldIrql);

    UNLOCK_WS (Thread, Process);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    return Status;
}


VOID
MiReplaceWorkingSetEntry (
    IN PMMSUPPORT WsInfo,
    IN WSLE_ALLOCATION_TYPE Flags
    )

/*++

Routine Description:

    This function tries to find a good working set entry to replace.

Arguments:

    WsInfo - Supplies the working set info pointer.

    Flags - Supplies 0 if replacement is not required.

            Supplies 1 if replacement is desired.  Note replacement cannot
                  be guaranteed (the entire existing working set may
                  be locked down) - if no entry can be released the caller
                  can detect this because MMWSL->FirstFree will not contain
                  any free entries - and so the caller should release the
                  working set mutex and retry the operation.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER FirstDynamic;
    WSLE_NUMBER LastEntry;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    ULONG NumberOfCandidates;
    PMMPTE PointerPte;
    WSLE_NUMBER TheNextSlot;
    WSLE_NUMBER OldestWorkingSetIndex;
    LONG OldestAge;

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    //
    // Toss a page out of the working set.
    //

    LastEntry = WorkingSetList->LastEntry;
    FirstDynamic = WorkingSetList->FirstDynamic;
    WorkingSetIndex = WorkingSetList->NextSlot;
    if (WorkingSetIndex > LastEntry || WorkingSetIndex < FirstDynamic) {
        WorkingSetIndex = FirstDynamic;
    }
    TheNextSlot = WorkingSetIndex;
    NumberOfCandidates = 0;

    OldestWorkingSetIndex = WSLE_NULL_INDEX;
    OldestAge = -1;

    while (TRUE) {

        //
        // Keep track of the oldest page along the way in case we
        // don't find one that's >= MI_IMMEDIATE_REPLACEMENT_AGE
        // before we've looked at MM_WORKING_SET_LIST_SEARCH
        // entries.
        //

        while (Wsle[WorkingSetIndex].u1.e1.Valid == 0) {
            WorkingSetIndex += 1;
            if (WorkingSetIndex > LastEntry) {
                WorkingSetIndex = FirstDynamic;
            }

            if (WorkingSetIndex == TheNextSlot) {
                    
                if (Flags == WsleAllocationAny) {
    
                    //
                    // Entire working set list has been searched, increase
                    // the working set size.  Note this may result in exceeding
                    // a hard maximum limit on the system cache (only)
                    // in the case where all the other entries (paged pool)
                    // have been locked (so there isn't one to replace).
                    //
    
                    WsInfo->GrowthSinceLastEstimate += 1;
                }
                return;
            }
        }

        if (OldestWorkingSetIndex == WSLE_NULL_INDEX) {

            //
            // First time through, so initialize the OldestWorkingSetIndex
            // to the first valid WSLE.  As we go along, this will be re-pointed
            // at the oldest candidate we come across.
            //

            OldestWorkingSetIndex = WorkingSetIndex;
            OldestAge = -1;
        }

        PointerPte = MiGetPteAddress(Wsle[WorkingSetIndex].u1.VirtualAddress);

        if ((Flags == WsleAllocationReplace) ||
            ((MI_GET_ACCESSED_IN_PTE(PointerPte) == 0) &&
            (OldestAge < (LONG) MI_GET_WSLE_AGE(PointerPte, &Wsle[WorkingSetIndex])))) {

            //
            // This one is not used and it's older.
            //

            OldestAge = MI_GET_WSLE_AGE(PointerPte, &Wsle[WorkingSetIndex]);
            OldestWorkingSetIndex = WorkingSetIndex;
        }

        //
        // If it's old enough or we've searched too much then use this entry.
        //

        if ((Flags == WsleAllocationReplace) ||
            OldestAge >= MI_IMMEDIATE_REPLACEMENT_AGE ||
            NumberOfCandidates > MM_WORKING_SET_LIST_SEARCH) {

            if (OldestWorkingSetIndex != WorkingSetIndex) {
                WorkingSetIndex = OldestWorkingSetIndex;
                PointerPte = MiGetPteAddress(Wsle[WorkingSetIndex].u1.VirtualAddress);
            }

            if (MiFreeWsle (WorkingSetIndex, WsInfo, PointerPte)) {

                //
                // This entry was removed.
                //

                WorkingSetList->NextSlot = WorkingSetIndex + 1;
                break;
            }

            //
            // We failed to remove a page, try the next one.
            //
            // Clear the OldestWorkingSetIndex so that
            // it gets set to the next valid entry above like the
            // first time around.
            //

            WorkingSetIndex = OldestWorkingSetIndex + 1;

            OldestWorkingSetIndex = WSLE_NULL_INDEX;
        }
        else {
            WorkingSetIndex += 1;
        }

        if (WorkingSetIndex > LastEntry) {
            WorkingSetIndex = FirstDynamic;
        }

        NumberOfCandidates += 1;


        if (WorkingSetIndex == TheNextSlot) {
                
            if (Flags == WsleAllocationAny) {

                //
                // Entire working set list has been searched, increase
                // the working set size.  Note this may result in exceeding
                // a hard maximum limit on the system cache (only)
                // in the case where all the other entries (paged pool)
                // have been locked (so there isn't one to replace).

                WsInfo->GrowthSinceLastEstimate += 1;
            }
            break;
        }
    }
    return;
}

ULONG
MiRemovePageFromWorkingSet (
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This function removes the page mapped by the specified PTE from
    the process's working set list.

Arguments:

    PointerPte - Supplies a pointer to the PTE mapping the page to
                 be removed from the working set list.

    Pfn1 - Supplies a pointer to the PFN database element referred to
           by the PointerPte.

Return Value:

    Returns TRUE if the specified page was locked in the working set,
    FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, working set pushlock held.

--*/

{
    WSLE_NUMBER WorkingSetIndex;
    PVOID VirtualAddress;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
#if DBG
    PVOID SwapVa;
#endif

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                    WorkingSetList,
                                    Pfn1->u1.WsIndex,
                                    TRUE);

    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

    MiEliminateWorkingSetEntry (WorkingSetIndex,
                                PointerPte,
                                Pfn1,
                                WsInfo,
                                TRUE);

    //
    // Check to see if this entry is locked in the working set
    // or locked in memory.
    //

    Locked = Wsle[WorkingSetIndex].u1.e1;

    MiRemoveWsle (WorkingSetIndex, WorkingSetList);

    //
    // Add this entry to the list of free working set entries
    // and adjust the working set count.
    //

    MiReleaseWsle ((WSLE_NUMBER)WorkingSetIndex, WsInfo);

    if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

        //
        // This entry is locked.
        //

        WorkingSetList->FirstDynamic -= 1;

        if (WorkingSetIndex != WorkingSetList->FirstDynamic) {

            Entry = WorkingSetList->FirstDynamic;

#if DBG
            SwapVa = Wsle[WorkingSetList->FirstDynamic].u1.VirtualAddress;
            SwapVa = PAGE_ALIGN (SwapVa);

            PointerPte = MiGetPteAddress (SwapVa);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

            ASSERT (Entry == MiLocateWsle (SwapVa, WorkingSetList, Pfn1->u1.WsIndex, FALSE));
#endif

            MiSwapWslEntries (Entry, WorkingSetIndex, WsInfo, FALSE);

        }
        return TRUE;
    }
    else {
        ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);
    }
    return FALSE;
}


VOID
MiReleaseWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This function releases a previously reserved working set entry to
    be reused.  A release occurs when a page fault is retried due to
    changes in PTEs and working sets during an I/O operation.

Arguments:

    WorkingSetIndex - Supplies the index of the working set entry to
                      release.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock held and PFN lock held.

--*/

{
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    MMWSLE WsleContents;

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    MM_WS_LOCK_ASSERT (WsInfo);

    ASSERT (WorkingSetIndex <= WorkingSetList->LastInitializedWsle);

    //
    // Put the entry on the free list and decrement the current size.
    //

    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

    WsleContents.u1.Long = WorkingSetList->FirstFree << MM_FREE_WSLE_SHIFT;

    MI_LOG_WSLE_CHANGE (WorkingSetList, WorkingSetIndex, WsleContents);

    Wsle[WorkingSetIndex] = WsleContents;
    WorkingSetList->FirstFree = WorkingSetIndex;
    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
    if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
        InterlockedDecrementSizeT (&MmPagesAboveWsMinimum);
    }
    WsInfo->WorkingSetSize -= 1;

    return;

}

VOID
MiUpdateWsle (
    IN OUT PWSLE_NUMBER DesiredIndex,
    IN PVOID VirtualAddress,
    IN PMMSUPPORT WsInfo,
    IN PMMPFN Pfn,
    IN ULONG_PTR WsleMask
    )

/*++

Routine Description:

    This routine updates a reserved working set entry to place it into
    the valid state.

Arguments:

    DesiredIndex - Supplies the index of the working set entry to update.

    VirtualAddress - Supplies the virtual address which the working set
                     entry maps.

    WsInfo - Supplies the relevant working set information to update.

    Pfn - Supplies a pointer to the PFN element for the page.

    WsleMask - Supplies the mask to logical OR into the new working set entry.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock held.

--*/

{
    ULONG_PTR OldValue;
    PMMWSLE Wsle;
    MMWSLE WsleContents;
    PMMWSL WorkingSetList;
    WSLE_NUMBER Index;
    WSLE_NUMBER WorkingSetIndex;

    MM_WS_LOCK_ASSERT (WsInfo);

    WorkingSetList = WsInfo->VmWorkingSetList;

    WorkingSetIndex = *DesiredIndex;

    ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);

    Wsle = WorkingSetList->Wsle;

    if (WorkingSetList == MmSystemCacheWorkingSetList) {

        //
        // This assert doesn't hold for NT64 as we can be adding page
        // directories and page tables for the system cache WSLE hash tables.
        //

        ASSERT32 ((VirtualAddress < (PVOID)PTE_BASE) ||
                  (VirtualAddress >= (PVOID)MM_SYSTEM_SPACE_START));

        //
        // Count system space inserts and removals.
        //

        if ((VirtualAddress >= MmPagedPoolStart) && (VirtualAddress <= MmPagedPoolEnd)) {
            MmPagedPoolPage += 1;
        }
        else if (MI_IS_SYSTEM_CACHE_ADDRESS (VirtualAddress)) {
            MmSystemCachePage += 1;
        }
        else if ((VirtualAddress <= MmSpecialPoolEnd) && (VirtualAddress >= MmSpecialPoolStart)) {
            MmPagedPoolPage += 1;
        }
        else if (VirtualAddress >= MiLowestSystemPteVirtualAddress) {
            MmSystemDriverPage += 1;
        }
        else {
            MmSystemCodePage += 1;
        }
    }
    else {
        ASSERT ((VirtualAddress < (PVOID)MM_SYSTEM_SPACE_START) ||
                (MI_IS_SESSION_ADDRESS (VirtualAddress)));
    }

    //
    // Make the WSLE valid, referring to the corresponding virtual
    // page number.
    //

#if DBG
    if (Pfn->u1.WsIndex <= WorkingSetList->LastInitializedWsle) {
        ASSERT ((PAGE_ALIGN(VirtualAddress) !=
                PAGE_ALIGN(Wsle[Pfn->u1.WsIndex].u1.VirtualAddress)) ||
                (Wsle[Pfn->u1.WsIndex].u1.e1.Valid == 0));
    }
#endif

    WsleContents.u1.VirtualAddress = PAGE_ALIGN (VirtualAddress);
    WsleContents.u1.e1.Valid = 1;

    //
    // The working set mutex is a process wide mutex and two threads in
    // different processes could be adding the same physical page to
    // their working sets.  Each one could see the WsIndex field in the
    // PFN as 0 and therefore want to set the direct bit.
    //
    // To solve this, the WsIndex field is updated with an interlocked
    // operation.  Note for private pages, there can be no race so a
    // simple update is enough.
    //

    if (Pfn->u1.Event == NULL) {

        //
        // Directly index into the WSL for this entry via the PFN database
        // element.
        //
        // The entire working set index union must be zeroed on NT64.  ie:
        // The WSLE_NUMBER is currently 32 bits and the PKEVENT is 64 - we
        // must zero the top 32 bits as well.  So instead of setting the
        // WsIndex field, set the overlaid Event field with appropriate casts.
        //

        if (Pfn->u3.e1.PrototypePte == 0) {

            //
            // This is a private page so this thread is the only one that
            // can be updating the PFN, so no need to use an interlocked update.
            // Note this is true even if the process is being forked because in
            // that case, the working set mutex is held throughout the fork so
            // this thread would have blocked earlier on that mutex first.
            //

            Pfn->u1.Event = (PKEVENT) (ULONG_PTR) WorkingSetIndex;
            ASSERT (Pfn->u1.Event == (PKEVENT) (ULONG_PTR) WorkingSetIndex);
            OldValue = 0;
        }
        else {

            //
            // This is a sharable page so a thread in another process could
            // be trying to update the PFN at the same time.  Use an interlocked
            // update so only one thread gets to set the value.
            //

#if defined (_WIN64)
            OldValue = InterlockedCompareExchange64 ((PLONGLONG)&Pfn->u1.Event,
                                                     (LONGLONG) (ULONG_PTR) WorkingSetIndex,
                                                     0);
#else
            OldValue = InterlockedCompareExchange ((PLONG)&Pfn->u1.Event,
                                                   WorkingSetIndex,
                                                   0);
#endif
        }

        if (OldValue == 0) {

            WsleContents.u1.e1.Direct = 1;

            MI_LOG_WSLE_CHANGE (WorkingSetList, WorkingSetIndex, WsleContents);

            Wsle[WorkingSetIndex] = WsleContents;

            Wsle[WorkingSetIndex].u1.Long |= WsleMask;

            return;
        }
    }

    MI_LOG_WSLE_CHANGE (WorkingSetList, WorkingSetIndex, WsleContents);

    Wsle[WorkingSetIndex] = WsleContents;

    //
    // Try to insert at WsIndex.
    //

    Index = Pfn->u1.WsIndex;

    if ((Index < WorkingSetList->LastInitializedWsle) &&
        (Index > WorkingSetList->FirstDynamic) &&
        (Index != WorkingSetIndex)) {

        if (Wsle[Index].u1.e1.Valid) {

            if (Wsle[Index].u1.e1.Direct) {

                //
                // Only move direct indexed entries.
                //

                MiSwapWslEntries (Index, WorkingSetIndex, WsInfo, TRUE);
                WorkingSetIndex = Index;
            }
        }
        else {

            //
            // On free list, try to remove quickly without walking
            // all the free pages.
            //

            WSLE_NUMBER FreeIndex;
            MMWSLE Temp;

            FreeIndex = 0;

            ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);
            ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);

            if (WorkingSetList->FirstFree == Index) {
                WorkingSetList->FirstFree = WorkingSetIndex;
                Temp = Wsle[WorkingSetIndex];
                MI_LOG_WSLE_CHANGE (WorkingSetList, WorkingSetIndex, Wsle[Index]);
                Wsle[WorkingSetIndex] = Wsle[Index];
                MI_LOG_WSLE_CHANGE (WorkingSetList, Index, Temp);
                Wsle[Index] = Temp;
                WorkingSetIndex = Index;
                ASSERT (((Wsle[WorkingSetList->FirstFree].u1.Long >> MM_FREE_WSLE_SHIFT)
                                 <= WorkingSetList->LastInitializedWsle) ||
                        ((Wsle[WorkingSetList->FirstFree].u1.Long >> MM_FREE_WSLE_SHIFT)
                                == WSLE_NULL_INDEX));
            }
            else if (Wsle[Index - 1].u1.e1.Valid == 0) {
                if ((Wsle[Index - 1].u1.Long >> MM_FREE_WSLE_SHIFT) == Index) {
                    FreeIndex = Index - 1;
                }
            }
            else if (Wsle[Index + 1].u1.e1.Valid == 0) {
                if ((Wsle[Index + 1].u1.Long >> MM_FREE_WSLE_SHIFT) == Index) {
                    FreeIndex = Index + 1;
                }
            }
            if (FreeIndex != 0) {

                //
                // Link the WSLE into the free list.
                //

                Temp = Wsle[WorkingSetIndex];
                Wsle[FreeIndex].u1.Long = WorkingSetIndex << MM_FREE_WSLE_SHIFT;

                MI_LOG_WSLE_CHANGE (WorkingSetList, WorkingSetIndex, Wsle[Index]);
                Wsle[WorkingSetIndex] = Wsle[Index];
                MI_LOG_WSLE_CHANGE (WorkingSetList, Index, Temp);
                Wsle[Index] = Temp;
                WorkingSetIndex = Index;

                ASSERT (((Wsle[FreeIndex].u1.Long >> MM_FREE_WSLE_SHIFT)
                                 <= WorkingSetList->LastInitializedWsle) ||
                        ((Wsle[FreeIndex].u1.Long >> MM_FREE_WSLE_SHIFT)
                                == WSLE_NULL_INDEX));
            }

        }
        *DesiredIndex = WorkingSetIndex;

        if (WorkingSetIndex > WorkingSetList->LastEntry) {
            WorkingSetList->LastEntry = WorkingSetIndex;
        }
    }

    ASSERT (Wsle[WorkingSetIndex].u1.e1.Valid == 1);
    ASSERT (Wsle[WorkingSetIndex].u1.e1.Direct != 1);

    Wsle[WorkingSetIndex].u1.Long |= WsleMask;
    Wsle[WorkingSetIndex].u1.e1.Hashed = 0;

    WorkingSetList->NonDirectCount += 1;

    if (WorkingSetIndex == Pfn->u1.WsIndex) {

        //
        // Don't waste a hash entry if we were able to share the direct index.
        //

        NOTHING;
    }
    else if (WorkingSetList->HashTable != NULL) {

        //
        // Insert the valid WSLE into the working set hash list.
        //

        MiInsertWsleHash (WorkingSetIndex, WsInfo);
    }

    return;
}


ULONG
MiFreeWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This routine frees the specified WSLE and decrements the share
    count for the corresponding page, putting the PTE into a transition
    state if the share count goes to 0.

Arguments:

    WorkingSetIndex - Supplies the index of the working set entry to free.

    WsInfo - Supplies a pointer to the working set structure.

    PointerPte - Supplies a pointer to the PTE for the working set entry.

Return Value:

    Returns TRUE if the WSLE was removed, FALSE if it was not removed.
        Pages with valid PTEs are not removed (i.e. page table pages
        that contain valid or transition PTEs).

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    PMMPFN Pfn1;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    MM_WS_LOCK_ASSERT (WsInfo);

    ASSERT (Wsle[WorkingSetIndex].u1.e1.Valid == 1);

    //
    // Check to see if the located entry is eligible for removal.
    //

    ASSERT (PointerPte->u.Hard.Valid == 1);

    ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);

    //
    // Check to see if this is a page table with valid PTEs.
    //
    // Note, don't clear the access bit for page table pages
    // with valid PTEs as this could cause an access trap fault which
    // would not be handled (it is only handled for PTEs not PDEs).
    //

    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

    //
    // Perform a preliminary check without the PFN lock so that lock
    // contention is avoided for cases that cannot possibly succeed.
    //

    if (WsInfo == &MmSystemCacheWs) {
        if (Pfn1->u3.e2.ReferenceCount > 1) {
            return FALSE;
        }
    }
    else {
        if ((Pfn1->u2.ShareCount > 1) && (Pfn1->u3.e1.PrototypePte == 0)) {
            return FALSE;
        }
    }

    //
    // Found a candidate, try to remove the page from the working set.
    //

    if (MiEliminateWorkingSetEntry (WorkingSetIndex,
                                    PointerPte,
                                    Pfn1,
                                    WsInfo,
                                    FALSE) == FALSE) {
        return FALSE;
    }

    //
    // Remove the working set entry from the working set.
    //

    MiRemoveWsle (WorkingSetIndex, WorkingSetList);

    ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

    ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);

    //
    // Put the entry on the free list and decrement the current size.
    //

    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
    Wsle[WorkingSetIndex].u1.Long = WorkingSetList->FirstFree << MM_FREE_WSLE_SHIFT;
    WorkingSetList->FirstFree = WorkingSetIndex;
    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

    if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
        InterlockedDecrementSizeT (&MmPagesAboveWsMinimum);
    }
    WsInfo->WorkingSetSize -= 1;

    return TRUE;
}

#define MI_INITIALIZE_WSLE(_VirtualAddress, _WslEntry) {           \
    PMMPFN _Pfn1;                                                   \
    _WslEntry->u1.VirtualAddress = (PVOID)(_VirtualAddress);        \
    _WslEntry->u1.e1.Valid = 1;                                     \
    _WslEntry->u1.e1.LockedInWs = 1;                                \
    _WslEntry->u1.e1.Direct = 1;                                    \
    _Pfn1 = MI_PFN_ELEMENT (MiGetPteAddress ((PVOID)(_VirtualAddress))->u.Hard.PageFrameNumber); \
    ASSERT (_Pfn1->u1.WsIndex == 0);                                \
    _Pfn1->u1.WsIndex = (WSLE_NUMBER)(_WslEntry - MmWsle);          \
    (_WslEntry) += 1;                                               \
}


VOID
MiInitializeWorkingSetList (
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine initializes a process's working set to the empty
    state.

Arguments:

    CurrentProcess - Supplies a pointer to the process to initialize.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PMMPFN Pfn1;
    WSLE_NUMBER i;
    PMMWSLE WslEntry;
    WSLE_NUMBER CurrentWsIndex;
    WSLE_NUMBER NumberOfEntriesMapped;
    PVOID VirtualAddress;

    WslEntry = MmWsle;

    //
    // Initialize the working set list control cells.
    //

    MmWorkingSetList->LastEntry = CurrentProcess->Vm.MinimumWorkingSetSize;
    MmWorkingSetList->HashTable = NULL;
    MmWorkingSetList->HashTableSize = 0;
    MmWorkingSetList->NumberOfImageWaiters = 0;
    MmWorkingSetList->Wsle = MmWsle;
    MmWorkingSetList->VadBitMapHint = 1;
    MmWorkingSetList->HashTableStart = 
       (PVOID)((PCHAR)PAGE_ALIGN (&MmWsle[MM_MAXIMUM_WORKING_SET]) + PAGE_SIZE);

#if defined(_X86_)
    MmWorkingSetList->HighestPermittedHashAddress = (PVOID)((ULONG_PTR)MmHyperSpaceEnd + 1);
#else
    MmWorkingSetList->HighestPermittedHashAddress = (PVOID)((ULONG_PTR)HYPER_SPACE_END + 1);
#endif

    //
    // Fill in the reserved slots.
    // Start with the top level page directory page.
    //

#if (_MI_PAGING_LEVELS >= 4)
    VirtualAddress = (PVOID) PXE_BASE;
#elif (_MI_PAGING_LEVELS >= 3)
    VirtualAddress = (PVOID) PDE_TBASE;
#else
    VirtualAddress = (PVOID) PDE_BASE;
#endif

    MI_INITIALIZE_WSLE (VirtualAddress, WslEntry);

#if defined (_X86PAE_)

    //
    // Fill in the additional page directory entries.
    //

    for (i = 1; i < PD_PER_SYSTEM; i += 1) {
        MI_INITIALIZE_WSLE (PDE_BASE + i * PAGE_SIZE, WslEntry);
    }

    VirtualAddress = (PVOID)((ULONG_PTR)VirtualAddress + ((PD_PER_SYSTEM - 1) * PAGE_SIZE));
#endif

    Pfn1 = MI_PFN_ELEMENT (MiGetPteAddress ((PVOID)(VirtualAddress))->u.Hard.PageFrameNumber);
    ASSERT (Pfn1->u4.PteFrame == (ULONG_PTR)MI_PFN_ELEMENT_TO_INDEX (Pfn1));
    Pfn1->u1.Event = (PVOID) CurrentProcess;

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Fill in the entry for the hyper space page directory parent page.
    //

    MI_INITIALIZE_WSLE (MiGetPpeAddress (HYPER_SPACE), WslEntry);

#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Fill in the entry for the hyper space page directory page.
    //

    MI_INITIALIZE_WSLE (MiGetPdeAddress (HYPER_SPACE), WslEntry);

#endif

    //
    // Fill in the entry for the page table page which maps hyper space.
    //

    MI_INITIALIZE_WSLE (MiGetPteAddress (HYPER_SPACE), WslEntry);

    //
    // Fill in the entry for the first VAD bitmap page.
    //
    // Note when booted /3GB, the second VAD bitmap page is automatically
    // inserted as part of the working set list page as the page is shared
    // by both.
    //

    MI_INITIALIZE_WSLE (VAD_BITMAP_SPACE, WslEntry);

    //
    // Fill in the entry for the page which contains the working set list.
    //

    MI_INITIALIZE_WSLE (MmWorkingSetList, WslEntry);

    NumberOfEntriesMapped = (PAGE_SIZE - BYTE_OFFSET (MmWsle)) / sizeof (MMWSLE);

    CurrentWsIndex = (WSLE_NUMBER)(WslEntry - MmWsle);

    CurrentProcess->Vm.WorkingSetSize = CurrentWsIndex;

    MmWorkingSetList->FirstFree = CurrentWsIndex;
    MmWorkingSetList->FirstDynamic = CurrentWsIndex;
    MmWorkingSetList->NextSlot = CurrentWsIndex;

    //
    //
    // Build the free list starting at the first dynamic entry.
    //

    i = CurrentWsIndex + 1;
    do {

        WslEntry->u1.Long = i << MM_FREE_WSLE_SHIFT;
        WslEntry += 1;
        i += 1;
    } while (i <= NumberOfEntriesMapped);

    //
    // Mark the end of the list.
    //

    WslEntry -= 1;
    WslEntry->u1.Long = WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT;

    MmWorkingSetList->LastInitializedWsle = NumberOfEntriesMapped - 1;

    return;
}


VOID
MiInitializeSessionWsSupport (
    VOID
    )

/*++

Routine Description:

    This routine initializes the session space working set support.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{
    //
    // This is the list of all session spaces ordered in a working set list.
    //

    InitializeListHead (&MiSessionWsList);
}


NTSTATUS
MiSessionInitializeWorkingSetList (
    VOID
    )

/*++

Routine Description:

    This function initializes the working set for the session space and adds
    it to the list of session space working sets.

Arguments:

    None.

Return Value:

    NT_SUCCESS if success or STATUS_NO_MEMORY on failure.

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{
    WSLE_NUMBER i;
    ULONG MaximumEntries;
    ULONG PageTableCost;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    MMPTE  TempPte;
    PMMWSLE WslEntry;
    PMMPFN Pfn1;
    ULONG PageColor;
    PFN_NUMBER ResidentPages;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER CurrentEntry;
    WSLE_NUMBER NumberOfEntriesMapped;
    WSLE_NUMBER NumberOfEntriesMappedByFirstPage;
    ULONG WorkingSetMaximum;
    PMM_SESSION_SPACE SessionGlobal;
    LOGICAL AllocatedPageTable;
    PMMWSL WorkingSetList;
    MMPTE DemandZeroWritePte;
#if (_MI_PAGING_LEVELS < 3)
    ULONG Index;
#endif

    //
    // Use the global address for pointer references by
    // MmWorkingSetManager before it attaches to the address space.
    //

    SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

    //
    // Set up the working set variables.
    //

    WorkingSetMaximum = MI_SESSION_SPACE_WORKING_SET_MAXIMUM;

    WorkingSetList = (PMMWSL) MiSessionSpaceWs;

    MmSessionSpace->Vm.VmWorkingSetList = WorkingSetList;
#if (_MI_PAGING_LEVELS >= 3)
    MmSessionSpace->Wsle = (PMMWSLE) (WorkingSetList + 1);
#else
    MmSessionSpace->Wsle = (PMMWSLE) (&WorkingSetList->UsedPageTableEntries[0]);
#endif

    //
    // Build the PDE entry for the working set - note that the global bit
    // must be turned off.
    //

    PointerPde = MiGetPdeAddress (WorkingSetList);

    //
    // The page table page for the working set and its first data page
    // are charged against MmResidentAvailablePages and for commitment.
    //

    if (PointerPde->u.Hard.Valid == 1) {

        //
        // The page directory entry for the working set is the same
        // as for another range in the session space.  Share the PDE.
        //

        ASSERT (PointerPde->u.Hard.Global == 0);

        AllocatedPageTable = FALSE;
        ResidentPages = 1;
    }
    else {
        AllocatedPageTable = TRUE;
        ResidentPages = 2;
    }


    PointerPte = MiGetPteAddress (WorkingSetList);

    //
    // The data pages needed to map up to maximum working set size are also
    // charged against MmResidentAvailablePages and for commitment.
    //

    NumberOfEntriesMappedByFirstPage = (WSLE_NUMBER)(
        ((PMMWSLE)((ULONG_PTR)WorkingSetList + PAGE_SIZE)) -
            MmSessionSpace->Wsle);

    if (MiChargeCommitment (ResidentPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        return STATUS_NO_MEMORY;
    }

    //
    // Use the global address for mutexes since the contained event
    // must be accessible from any address space.
    //

    ExInitializePushLock (&SessionGlobal->Vm.WorkingSetMutex);

    MmLockPageableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if ((SPFN_NUMBER)ResidentPages > MI_NONPAGEABLE_MEMORY_AVAILABLE() - 20) {

        UNLOCK_PFN (OldIrql);

        MmUnlockPageableImageSection (ExPageLockHandle);

        MiReturnCommitment (ResidentPages);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        return STATUS_NO_MEMORY;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_WS_INIT, ResidentPages);

    MI_DECREMENT_RESIDENT_AVAILABLE (ResidentPages,
                                     MM_RESAVAIL_ALLOCATE_INIT_SESSION_WS);

    if (AllocatedPageTable == TRUE) {

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_PAGETABLE_ALLOC, 1);

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, OldIrql);
        }

        PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

        PageFrameIndex = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

        //
        // The global bit is masked off since we need to make sure the TB entry
        // is flushed when we switch to a process in a different session space.
        //

        TempPte.u.Long = ValidKernelPdeLocal.u.Long;
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS < 3)

        //
        // Add this to the session structure so other processes can fault it in.
        //

        Index = MiGetPdeSessionIndex (WorkingSetList);

        MmSessionSpace->PageTables[Index] = TempPte;

#endif

        //
        // This page frame references the session space page table page.
        //

        MiInitializePfnForOtherProcess (PageFrameIndex,
                                        PointerPde,
                                        MmSessionSpace->SessionPageDirectoryIndex);

        KeZeroSinglePage (PointerPte);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // This page is never paged, ensure that its WsIndex stays clear so the
        // release of the page will be handled correctly.
        //

        ASSERT (Pfn1->u1.WsIndex == 0);
    }

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, OldIrql);
    }

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageFrameIndex = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_PAGE_ALLOC, (ULONG)(ResidentPages - 1));

#if DBG
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (Pfn1->u1.WsIndex == 0);
#endif

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPteLocal.u.Long;
    MI_SET_PTE_DIRTY (TempPte);
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MiInitializePfnAndMakePteValid (PageFrameIndex, PointerPte, TempPte);

#if DBG
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (Pfn1->u1.WsIndex == 0);
#endif

    UNLOCK_PFN (OldIrql);

#define MI_INITIALIZE_SESSION_WSLE(_VirtualAddress, _WslEntry) {   \
    PMMPFN _Pfn1;                                                   \
    _WslEntry->u1.VirtualAddress = (PVOID)(_VirtualAddress);        \
    _WslEntry->u1.e1.Valid = 1;                                     \
    _WslEntry->u1.e1.LockedInWs = 1;                                \
    _WslEntry->u1.e1.Direct = 1;                                    \
    _Pfn1 = MI_PFN_ELEMENT (MiGetPteAddress ((PVOID)(_VirtualAddress))->u.Hard.PageFrameNumber); \
    ASSERT (_Pfn1->u1.WsIndex == 0);                                \
    _Pfn1->u1.WsIndex = (WSLE_NUMBER)(_WslEntry - MmSessionSpace->Wsle); \
    (_WslEntry) += 1;                                               \
}

    //
    // Fill in the reserved slots starting with the 2 session data pages.
    //

    WslEntry = MmSessionSpace->Wsle;

    //
    // The first reserved slot is for the page table page mapping
    // the session data page.
    //

    MI_INITIALIZE_SESSION_WSLE (MiGetPteAddress (MmSessionSpace), WslEntry);

    //
    // The next reserved slot is for the working set page.
    //

    MI_INITIALIZE_SESSION_WSLE (WorkingSetList, WslEntry);

    if (AllocatedPageTable == TRUE) {

        //
        // The next reserved slot is for the page table page
        // mapping the working set page.
        //

        MI_INITIALIZE_SESSION_WSLE (PointerPte, WslEntry);
    }

    //
    // The next reserved slot is for the page table page
    // mapping the first session paged pool page.
    //

    MI_INITIALIZE_SESSION_WSLE (MiGetPteAddress (MmSessionSpace->PagedPoolStart), WslEntry);

    CurrentEntry = (WSLE_NUMBER)(WslEntry - MmSessionSpace->Wsle);

    MmSessionSpace->Vm.Flags.SessionSpace = 1;
    MmSessionSpace->Vm.MinimumWorkingSetSize = MI_SESSION_SPACE_WORKING_SET_MINIMUM;
    MmSessionSpace->Vm.MaximumWorkingSetSize = WorkingSetMaximum;

    WorkingSetList->LastEntry = MI_SESSION_SPACE_WORKING_SET_MINIMUM;
    WorkingSetList->HashTable = NULL;
    WorkingSetList->HashTableSize = 0;
    WorkingSetList->Wsle = MmSessionSpace->Wsle;

    //
    // Calculate the maximum number of entries dynamically as the size of
    // session space is registry configurable.  Then add in page table and
    // page directory overhead.
    //

    MaximumEntries = (ULONG)((MiSessionSpaceEnd - MmSessionBase) >> PAGE_SHIFT);
    PageTableCost = MaximumEntries / PTE_PER_PAGE + 1;
    MaximumEntries += PageTableCost;

    WorkingSetList->HashTableStart =
       (PVOID)((PCHAR)PAGE_ALIGN (&MmSessionSpace->Wsle[MaximumEntries]) + PAGE_SIZE);

#if defined (_X86PAE_)

    //
    // One less page table page is needed on PAE systems as the session
    // working set structures easily fit within 2MB.
    //

    WorkingSetList->HighestPermittedHashAddress =
        (PVOID)(MiSessionImageStart - MM_VA_MAPPED_BY_PDE);
#else
    WorkingSetList->HighestPermittedHashAddress =
        (PVOID)(MiSessionImageStart - MI_SESSION_SPACE_STRUCT_SIZE);
#endif

    DemandZeroWritePte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    NumberOfEntriesMapped = (WSLE_NUMBER)(((PMMWSLE)((ULONG_PTR)WorkingSetList +
                                PAGE_SIZE)) - MmSessionSpace->Wsle);

    MmSessionSpace->Vm.WorkingSetSize = CurrentEntry;
    WorkingSetList->FirstFree = CurrentEntry;
    WorkingSetList->FirstDynamic = CurrentEntry;
    WorkingSetList->NextSlot = CurrentEntry;

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_INIT_WS, (ULONG)ResidentPages);

    InterlockedExchangeAddSizeT (&MmSessionSpace->NonPageablePages,
                                 ResidentPages);

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                 ResidentPages);

    //
    // Initialize the following slots as free.
    //

    WslEntry = MmSessionSpace->Wsle + CurrentEntry;

    for (i = CurrentEntry + 1; i < NumberOfEntriesMapped; i += 1) {

        //
        // Build the free list, note that the first working
        // set entries (CurrentEntry) are not on the free list.
        // These entries are reserved for the pages which
        // map the working set and the page which contains the PDE.
        //

        WslEntry->u1.Long = i << MM_FREE_WSLE_SHIFT;
        WslEntry += 1;
    }

    WslEntry->u1.Long = WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT;  // End of list.

    WorkingSetList->LastInitializedWsle = NumberOfEntriesMapped - 1;

    //
    // Put this session's working set in lists using its global address.
    //

    ASSERT (SessionGlobal->Vm.WorkingSetExpansionLinks.Flink == NULL);
    ASSERT (SessionGlobal->Vm.WorkingSetExpansionLinks.Blink == NULL);

    LOCK_EXPANSION (OldIrql);

    ASSERT (SessionGlobal->WsListEntry.Flink == NULL);
    ASSERT (SessionGlobal->WsListEntry.Blink == NULL);

    InsertTailList (&MiSessionWsList, &SessionGlobal->WsListEntry);

    InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                    &SessionGlobal->Vm.WorkingSetExpansionLinks);

    UNLOCK_EXPANSION (OldIrql);

    MmUnlockPageableImageSection (ExPageLockHandle);

    return STATUS_SUCCESS;
}


LOGICAL
MmAssignProcessToJob (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine acquires the address space mutex so a consistent snapshot of
    the argument process' commit charges can be used by Ps when adding this
    process to a job.

    Note that the working set mutex is not acquired here so the argument
    process' working set sizes cannot be reliably snapped by Ps, but since Ps
    doesn't look at that anyway, it's not a problem.

Arguments:

    Process - Supplies a pointer to the process to operate upon.

Return Value:

    TRUE if the process is allowed to join the job, FALSE otherwise.

    Note that FALSE cannot be returned without changing the code in Ps.

Environment:

    Kernel mode, IRQL APC_LEVEL or below.  The caller provides protection
    from the target process going away.

--*/

{
    LOGICAL Attached;
    LOGICAL Status;
    KAPC_STATE ApcState;

    PAGED_CODE ();

    Attached = FALSE;

    if (PsGetCurrentProcess() != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    LOCK_ADDRESS_SPACE (Process);

    Status = PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, Process->CommitCharge);

    //
    // Join the job unconditionally.  If the process is over any limits, it
    // will be caught on its next request.
    //

    PS_SET_BITS (&Process->JobStatus, PS_JOB_STATUS_REPORT_COMMIT_CHANGES);

    UNLOCK_ADDRESS_SPACE (Process);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    //
    // Note that FALSE cannot be returned without changing the code in Ps.
    //

    return TRUE;
}


NTSTATUS
MmQueryWorkingSetInformation (
    IN PSIZE_T PeakWorkingSetSize,
    IN PSIZE_T WorkingSetSize,
    IN PSIZE_T MinimumWorkingSetSize,
    IN PSIZE_T MaximumWorkingSetSize,
    IN PULONG HardEnforcementFlags
    )

/*++

Routine Description:

    This routine returns various working set information fields for the
    current process.

Arguments:

    PeakWorkingSetSize - Supplies an address to receive the peak working set
                         size in bytes.

    WorkingSetSize - Supplies an address to receive the current working set
                     size in bytes.

    MinimumWorkingSetSize - Supplies an address to receive the minimum
                            working set size in bytes.

    MaximumWorkingSetSize - Supplies an address to receive the maximum
                            working set size in bytes.

    HardEnforcementFlags - Supplies an address to receive the current
                           working set enforcement policy.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, IRQL APC_LEVEL or below.

--*/

{
    PETHREAD Thread;
    PEPROCESS Process;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    *HardEnforcementFlags = 0;

    Thread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (Thread);

    LOCK_WS_SHARED (Thread, Process);

    *PeakWorkingSetSize = (SIZE_T) Process->Vm.PeakWorkingSetSize << PAGE_SHIFT;
    *WorkingSetSize = (SIZE_T) Process->Vm.WorkingSetSize << PAGE_SHIFT;
    *MinimumWorkingSetSize = (SIZE_T) Process->Vm.MinimumWorkingSetSize << PAGE_SHIFT;
    *MaximumWorkingSetSize = (SIZE_T) Process->Vm.MaximumWorkingSetSize << PAGE_SHIFT;

    if (Process->Vm.Flags.MinimumWorkingSetHard == 1) {
        *HardEnforcementFlags |= MM_WORKING_SET_MIN_HARD_ENABLE;
    }

    if (Process->Vm.Flags.MaximumWorkingSetHard == 1) {
        *HardEnforcementFlags |= MM_WORKING_SET_MAX_HARD_ENABLE;
    }

    UNLOCK_WS_SHARED (Thread, Process);

    return STATUS_SUCCESS;
}

VOID
MmQuerySystemCacheWorkingSetInformation (
    OUT PSYSTEM_FILECACHE_INFORMATION Info
    )

/*++

Routine Description:

    This routine returns various working set information fields for the
    system cache.

Arguments:

    Info - Supplies a pointer to captured (ie, system not user !) space to
           store the results in.  Note the fields in this structure are not
           consistent (some are bytes, some are pages) but since NT4 shipped
           this way we must maintain it.

Return Value:

    None.

Environment:

    Kernel mode, IRQL APC_LEVEL or below.

--*/

{
    PETHREAD Thread;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    Info->Flags = 0;

    Thread = PsGetCurrentThread ();

    LOCK_SYSTEM_WS (Thread);

    //
    // Fields using byte quantities.
    //

    Info->CurrentSize = ((SIZE_T)MmSystemCacheWs.WorkingSetSize) << PAGE_SHIFT;
    Info->PeakSize = ((SIZE_T)MmSystemCacheWs.PeakWorkingSetSize) << PAGE_SHIFT;
    Info->PageFaultCount = MmSystemCacheWs.PageFaultCount;

    //
    // Fields using page quantities.
    //

    Info->MinimumWorkingSet = MmSystemCacheWs.MinimumWorkingSetSize;
    Info->MaximumWorkingSet = MmSystemCacheWs.MaximumWorkingSetSize;
    Info->CurrentSizeIncludingTransitionInPages = MmSystemCacheWs.WorkingSetSize + MmTransitionSharedPages;
    Info->PeakSizeIncludingTransitionInPages = MmTransitionSharedPagesPeak;
    Info->TransitionRePurposeCount = MmStandbyRePurposed;

    if (MmSystemCacheWs.Flags.MinimumWorkingSetHard == 1) {
        Info->Flags |= MM_WORKING_SET_MIN_HARD_ENABLE;
    }

    if (MmSystemCacheWs.Flags.MaximumWorkingSetHard == 1) {
        Info->Flags |= MM_WORKING_SET_MAX_HARD_ENABLE;
    }

    UNLOCK_SYSTEM_WS (Thread);

    return;
}

NTSTATUS
MmAdjustWorkingSetSizeEx (
    IN SIZE_T WorkingSetMinimumInBytes,
    IN SIZE_T WorkingSetMaximumInBytes,
    IN ULONG SystemCache,
    IN BOOLEAN IncreaseOkay,
    IN ULONG Flags,
    OUT PBOOLEAN IncreaseRequested
    )

/*++

Routine Description:

    This routine adjusts the current size of a process's working set
    list.  If the maximum value is above the current maximum, pages
    are removed from the working set list.

    A failure status is returned if the limit cannot be granted.  This
    could occur if too many pages were locked in the process's
    working set.

    Note: if the minimum and maximum are both (SIZE_T)-1, the working set
          is purged, but the default sizes are not changed.

Arguments:

    WorkingSetMinimumInBytes - Supplies the new minimum working set size in
                               bytes.

    WorkingSetMaximumInBytes - Supplies the new maximum working set size in
                               bytes.

    SystemCache - Supplies TRUE if the system cache working set is being
                  adjusted, FALSE for all other working sets.

    IncreaseOkay - Supplies TRUE if this routine should allow increases to
                   the working set minimum.

    Flags - Supplies flags (MM_WORKING_SET_MAX_HARD_ENABLE, etc) for
            enabling/disabling hard enforcement of the working set minimums
            and maximums for the specified WsInfo.

    IncreaseRequested - The operation requested an increased in the working set min



Return Value:

    NTSTATUS.

Environment:

    Kernel mode, IRQL APC_LEVEL or below.

--*/

{
    ULONG TrimAge;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;
    PMMWSLE Wsle;
    KIRQL OldIrql;
    SPFN_NUMBER i;
    NTSTATUS ReturnStatus;
    LONG PagesAbove;
    LONG NewPagesAbove;
    LOGICAL ExtraWsFreed;
    PMMSUPPORT WsInfo;
    PMMWSL WorkingSetList;
    WSLE_NUMBER WorkingSetMinimum;
    WSLE_NUMBER WorkingSetMaximum;

    ExtraWsFreed = TRUE;
    *IncreaseRequested = FALSE;

    CurrentThread = PsGetCurrentThread ();

    if (SystemCache) {

        //
        // Initializing CurrentProcess is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        CurrentProcess = NULL;
        WsInfo = &MmSystemCacheWs;
    }
    else {
        CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);
        WsInfo = &CurrentProcess->Vm;
    }

    if ((WorkingSetMinimumInBytes == (SIZE_T)-1) &&
        (WorkingSetMaximumInBytes == (SIZE_T)-1)) {

        return MiEmptyWorkingSet (WsInfo, TRUE);
    }

    ReturnStatus = STATUS_SUCCESS;

    MmLockPageableSectionByHandle (ExPageLockHandle);

    //
    // Get the working set lock and disable APCs.
    //

    if (SystemCache) {
        LOCK_SYSTEM_WS (CurrentThread);
    }
    else {

        LOCK_WS (CurrentThread, CurrentProcess);

        if (CurrentProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            ReturnStatus = STATUS_PROCESS_IS_TERMINATING;
            goto Returns;
        }
    }

    if (WorkingSetMinimumInBytes == 0) {
        WorkingSetMinimum = WsInfo->MinimumWorkingSetSize;
    }
    else {
        WorkingSetMinimum = (WSLE_NUMBER)(WorkingSetMinimumInBytes >> PAGE_SHIFT);
    }

    if (WorkingSetMaximumInBytes == 0) {
        WorkingSetMaximum = WsInfo->MaximumWorkingSetSize;
    }
    else {
        WorkingSetMaximum = (WSLE_NUMBER)(WorkingSetMaximumInBytes >> PAGE_SHIFT);
    }

    if (WorkingSetMinimum > WorkingSetMaximum) {
        ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;
        goto Returns;
    }

    if (WorkingSetMaximum > MmMaximumWorkingSetSize) {
        WorkingSetMaximum = MmMaximumWorkingSetSize;
        ReturnStatus = STATUS_WORKING_SET_LIMIT_RANGE;
    }

    if (WorkingSetMinimum > MmMaximumWorkingSetSize) {
        WorkingSetMinimum = MmMaximumWorkingSetSize;
        ReturnStatus = STATUS_WORKING_SET_LIMIT_RANGE;
    }

    if (WorkingSetMinimum < MmMinimumWorkingSetSize) {
        WorkingSetMinimum = (ULONG)MmMinimumWorkingSetSize;
        ReturnStatus = STATUS_WORKING_SET_LIMIT_RANGE;
    }

    //
    // Make sure that the number of locked pages will not
    // make the working set not fluid.
    //

    if ((WsInfo->VmWorkingSetList->FirstDynamic + MM_FLUID_WORKING_SET) >=
         WorkingSetMaximum) {
        ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;
        goto Returns;
    }

    //
    // If hard working set limits are being enabled (or are already enabled),
    // then make sure the minimum and maximum will not starve this process.
    //

    if ((Flags & MM_WORKING_SET_MIN_HARD_ENABLE) ||

        ((WsInfo->Flags.MinimumWorkingSetHard == 1) &&
         ((Flags & MM_WORKING_SET_MIN_HARD_DISABLE) == 0)) ||

        (Flags & MM_WORKING_SET_MAX_HARD_ENABLE) ||

        ((WsInfo->Flags.MaximumWorkingSetHard == 1) &&
         ((Flags & MM_WORKING_SET_MAX_HARD_DISABLE) == 0))) {

        //
        // Working set maximum is (or will be hard) as well.
        //
        // Check whether the requested minimum and maximum working
        // set values will guarantee forward progress even in pathological
        // scenarios.
        //

        if (WorkingSetMinimum + MM_FLUID_WORKING_SET >= WorkingSetMaximum) {
            ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;
            goto Returns;
        }

        //
        // Make sure the system cache never gets a hard limit below 16mb
        // or so as this working set does work for all the others.
        //

        if ((SystemCache) && (WorkingSetMaximum < (16 * 1024 * 1024) / PAGE_SIZE)) {
            ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;
            goto Returns;
        }
    }

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    i = (SPFN_NUMBER)WorkingSetMinimum - (SPFN_NUMBER)WsInfo->MinimumWorkingSetSize;

    //
    // Check to make sure ample resident physical pages exist for
    // this operation.
    //

    LOCK_PFN (OldIrql);

    if (i > 0) {

        //
        // New minimum working set is greater than the old one.
        // Ensure that increasing is okay, and that we don't allow
        // this process' working set minimum to increase to a point
        // where subsequent nonpaged pool allocations could cause us
        // to run out of pages.  Additionally, leave 100 extra pages
        // around so the user can later bring up tlist and kill
        // processes if necessary.
        //

        *IncreaseRequested = TRUE;

        if (IncreaseOkay == FALSE) {
            UNLOCK_PFN (OldIrql);
            ReturnStatus = STATUS_PRIVILEGE_NOT_HELD;
            goto Returns;
        }

        if ((SPFN_NUMBER)((i / (PAGE_SIZE / sizeof (MMWSLE)))) >
            (SPFN_NUMBER)(MmAvailablePages - MM_HIGH_LIMIT)) {

            UNLOCK_PFN (OldIrql);
            ReturnStatus = STATUS_INSUFFICIENT_RESOURCES;
            goto Returns;
        }

        if (MI_NONPAGEABLE_MEMORY_AVAILABLE() - (2 * MM_HIGH_LIMIT) < i) {
            UNLOCK_PFN (OldIrql);
            ReturnStatus = STATUS_INSUFFICIENT_RESOURCES;
            goto Returns;
        }
    }

    //
    // Adjust the number of resident pages up or down dependent on
    // the size of the new minimum working set size versus the previous
    // minimum size.
    //

    MI_DECREMENT_RESIDENT_AVAILABLE (i, MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST);

    UNLOCK_PFN (OldIrql);

    if (WorkingSetMaximum < WorkingSetList->LastInitializedWsle) {

        //
        // The new working set maximum is less than the current working set
        // maximum.
        //

        if (WsInfo->WorkingSetSize > WorkingSetMaximum) {

            //
            // Remove some pages from the working set.
            //
            // Make sure that the number of locked pages will not
            // make the working set not fluid.
            //

            if ((WorkingSetList->FirstDynamic + MM_FLUID_WORKING_SET) >=
                 WorkingSetMaximum) {

                ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;

                MI_INCREMENT_RESIDENT_AVAILABLE (i, MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST2);

                goto Returns;
            }

            //
            // Attempt to remove the oldest working set pages first.
            //

            TrimAge = MI_PASS0_TRIM_AGE;
            do {
                ASSERT (TrimAge <= MI_PASS0_TRIM_AGE);
                MiTrimWorkingSet (WsInfo->WorkingSetSize - WorkingSetMaximum,
                                  WsInfo,
                                  TrimAge);

                if (WsInfo->WorkingSetSize <= WorkingSetMaximum) {
                    break;
                }

                if (TrimAge == 0) {
                    ExtraWsFreed = FALSE;

                    //
                    // Page table pages are not becoming free, give up
                    // and return an error.
                    //

                    ReturnStatus = STATUS_BAD_WORKING_SET_LIMIT;
                    break;
                }

                TrimAge -= 1;

            } while (TRUE);
        }
    }

    //
    // Adjust the number of pages above the working set minimum.
    //

    PagesAbove = (LONG)WsInfo->WorkingSetSize -
                               (LONG)WsInfo->MinimumWorkingSetSize;

    NewPagesAbove = (LONG)WsInfo->WorkingSetSize - (LONG)WorkingSetMinimum;

    if (PagesAbove > 0) {
        InterlockedExchangeAddSizeT (&MmPagesAboveWsMinimum, 0 - (PFN_NUMBER)PagesAbove);
    }
    if (NewPagesAbove > 0) {
        InterlockedExchangeAddSizeT (&MmPagesAboveWsMinimum, (PFN_NUMBER)NewPagesAbove);
    }

    if (ExtraWsFreed == TRUE) {
        WsInfo->MaximumWorkingSetSize = WorkingSetMaximum;
        WsInfo->MinimumWorkingSetSize = WorkingSetMinimum;

        //
        // A change in hard working set limits is being requested.
        //
        // If the caller's request will result in hard enforcement of both
        // limits, the minimum and maximum working set values must
        // guarantee forward progress even in pathological scenarios.
        // This was already checked above.
        //

        if (Flags != 0) {

            LOCK_EXPANSION (OldIrql);

            if (Flags & MM_WORKING_SET_MIN_HARD_ENABLE) {
                WsInfo->Flags.MinimumWorkingSetHard = 1;
            }
            else if (Flags & MM_WORKING_SET_MIN_HARD_DISABLE) {
                WsInfo->Flags.MinimumWorkingSetHard = 0;
            }

            if (Flags & MM_WORKING_SET_MAX_HARD_ENABLE) {
                WsInfo->Flags.MaximumWorkingSetHard = 1;
            }
            else if (Flags & MM_WORKING_SET_MAX_HARD_DISABLE) {
                WsInfo->Flags.MaximumWorkingSetHard = 0;
            }

            UNLOCK_EXPANSION (OldIrql);
        }
    }
    else {
        MI_INCREMENT_RESIDENT_AVAILABLE (i, MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST3);
    }

    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

Returns:

    if (SystemCache) {
        UNLOCK_SYSTEM_WS (CurrentThread);
    }
    else {
        UNLOCK_WS (CurrentThread, CurrentProcess);
    }

    MmUnlockPageableImageSection (ExPageLockHandle);

    return ReturnStatus;
}


NTSTATUS
MmAdjustWorkingSetSize (
    __in SIZE_T WorkingSetMinimumInBytes,
    __in SIZE_T WorkingSetMaximumInBytes,
    __in ULONG SystemCache,
    __in BOOLEAN IncreaseOkay
    )

/*++

Routine Description:

    This routine adjusts the current size of a process's working set
    list.  If the maximum value is above the current maximum, pages
    are removed from the working set list.

    A failure status is returned if the limit cannot be granted.  This
    could occur if too many pages were locked in the process's
    working set.

    Note: if the minimum and maximum are both (SIZE_T)-1, the working set
          is purged, but the default sizes are not changed.

Arguments:

    WorkingSetMinimumInBytes - Supplies the new minimum working set size in
                               bytes.

    WorkingSetMaximumInBytes - Supplies the new maximum working set size in
                               bytes.

    SystemCache - Supplies TRUE if the system cache working set is being
                  adjusted, FALSE for all other working sets.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, IRQL APC_LEVEL or below.

--*/

{
    BOOLEAN IncreasePerformed;

    return MmAdjustWorkingSetSizeEx (WorkingSetMinimumInBytes,
                                     WorkingSetMaximumInBytes,
                                     SystemCache,
                                     IncreaseOkay,
                                     0,
                                     &IncreasePerformed);
}

#define MI_ALLOCATED_PAGE_TABLE     0x1
#define MI_ALLOCATED_PAGE_DIRECTORY 0x2


ULONG
MiAddWorkingSetPage (
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This function grows the working set list above working set
    maximum during working set adjustment.  At most one page
    can be added at a time.

Arguments:

    None.

Return Value:

    Returns FALSE if no working set page could be added.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    WSLE_NUMBER SwapEntry;
    WSLE_NUMBER CurrentEntry;
    PMMWSLE WslEntry;
    WSLE_NUMBER i;
    PMMPTE PointerPte;
    PMMPTE Va;
    MMPTE TempPte;
    WSLE_NUMBER NumberOfEntriesMapped;
    PFN_NUMBER WorkingSetPage;
    WSLE_NUMBER WorkingSetIndex;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    ULONG PageTablePageAllocated;
    LOGICAL PfnHeld;
    ULONG NumberOfPages;
    MMPTE DemandZeroWritePte;
#if (_MI_PAGING_LEVELS >= 3)
    PVOID VirtualAddress;
    PMMPTE PointerPde;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPpe;
#endif

    //
    // Initializing OldIrql is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    MM_WS_LOCK_ASSERT (WsInfo);

    //
    // The maximum size of the working set is being increased, check
    // to ensure the proper number of pages are mapped to cover
    // the complete working set list.
    //

    PointerPte = MiGetPteAddress (&Wsle[WorkingSetList->LastInitializedWsle]);

    ASSERT (PointerPte->u.Hard.Valid == 1);

    PointerPte += 1;

    Va = (PMMPTE)MiGetVirtualAddressMappedByPte (PointerPte);

    if ((PVOID)Va >= WorkingSetList->HashTableStart) {

        //
        // Adding this entry would overrun the hash table.  The caller
        // must replace instead.
        //

        return FALSE;
    }

    //
    // Ensure enough commitment is available prior to acquiring pages.
    // Excess is released after the pages are acquired.
    //

    if (MiChargeCommitmentCantExpand (_MI_PAGING_LEVELS - 1, FALSE) == FALSE) {
        return FALSE;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES, _MI_PAGING_LEVELS - 1);
    PageTablePageAllocated = 0;
    PfnHeld = FALSE;
    NumberOfPages = 0;
    DemandZeroWritePte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    //
    // The PPE is guaranteed to always be resident for architectures using
    // 3 level lookup.  This is because the hash table PPE immediately
    // follows the working set PPE.
    //
    // For x86 PAE the same paradigm holds in guaranteeing that the PDE is
    // always resident.
    //
    // x86 non-PAE uses the same PDE and hence it also guarantees PDE residency.
    //
    // Architectures employing 4 level lookup use a single PXE for this, but
    // each PPE must be checked.
    //
    // All architectures must check for page table page residency.
    //

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Allocate a PPE if one is needed.
    //

    PointerPpe = MiGetPdeAddress (PointerPte);
    if (PointerPpe->u.Hard.Valid == 0) {

        ASSERT (WsInfo->Flags.SessionSpace == 0);

        //
        // Map in a new page directory for the working set expansion.
        // Continue holding the PFN lock until the entire hierarchy is
        // allocated.  This eliminates error recovery which would be needed
        // if the lock was released and then when reacquired it is discovered
        // that one of the pages cannot be allocated.
        //
    
        PfnHeld = TRUE;
        LOCK_PFN (OldIrql);
        if ((MmAvailablePages < MM_HIGH_LIMIT) ||
            (MI_NONPAGEABLE_MEMORY_AVAILABLE() < MM_HIGH_LIMIT)) {
    
            //
            // No pages are available, the caller will have to replace.
            //
    
            UNLOCK_PFN (OldIrql);
            MiReturnCommitment (_MI_PAGING_LEVELS - 1 - NumberOfPages);
            MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES,
                            _MI_PAGING_LEVELS - 1 - NumberOfPages);
            return FALSE;
        }
    
        //
        // Apply the resident available charge for the working set page
        // directory table page now before releasing the PFN lock.
        //

        MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_ADD_WS_PAGE);

        PageTablePageAllocated |= MI_ALLOCATED_PAGE_DIRECTORY;
        WorkingSetPage = MiRemoveZeroPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));

        MI_WRITE_INVALID_PTE (PointerPpe, DemandZeroWritePte);

        MiInitializePfn (WorkingSetPage, PointerPpe, 1);
    
        MI_MAKE_VALID_PTE (TempPte,
                           WorkingSetPage,
                           MM_READWRITE,
                           PointerPpe);
    
        MI_SET_PTE_DIRTY (TempPte);
        MI_WRITE_VALID_PTE (PointerPpe, TempPte);
        NumberOfPages += 1;
    }

#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Map in a new page table (if needed) for the working set expansion.
    //

    PointerPde = MiGetPteAddress (PointerPte);

    if (PointerPde->u.Hard.Valid == 0) {
        PageTablePageAllocated |= MI_ALLOCATED_PAGE_TABLE;

        if (PfnHeld == FALSE) {
            PfnHeld = TRUE;
            LOCK_PFN (OldIrql);
            if ((MmAvailablePages < MM_HIGH_LIMIT) ||
                (MI_NONPAGEABLE_MEMORY_AVAILABLE() < MM_HIGH_LIMIT)) {
    
                //
                // No pages are available, the caller will have to replace.
                //
    
                UNLOCK_PFN (OldIrql);
                MiReturnCommitment (_MI_PAGING_LEVELS - 1 - NumberOfPages);
                MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES,
                                 _MI_PAGING_LEVELS - 1 - NumberOfPages);
                return FALSE;
            }
        }

        //
        // Apply the resident available charge for the working set page table
        // page now before releasing the PFN lock.
        //

        MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_ADD_WS_PAGE);

        WorkingSetPage = MiRemoveZeroPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
        MI_WRITE_INVALID_PTE (PointerPde, DemandZeroWritePte);

        MiInitializePfn (WorkingSetPage, PointerPde, 1);
    
        MI_MAKE_VALID_PTE (TempPte,
                           WorkingSetPage,
                           MM_READWRITE,
                           PointerPde);
    
        MI_SET_PTE_DIRTY (TempPte);
        MI_WRITE_VALID_PTE (PointerPde, TempPte);
        NumberOfPages += 1;
    }

#endif
    
    ASSERT (PointerPte->u.Hard.Valid == 0);

    //
    // Finally allocate and map the actual working set page now.  The PFN lock
    // is only held if another page in the hierarchy needed to be allocated.
    //
    // Further down in this routine (once an actual working set page has been
    // allocated) the working set size will be increased by 1 to reflect
    // the working set size entry for the new page directory page.
    // The page directory page will be put in a working set entry which will
    // be locked into the working set.
    //

    if (PfnHeld == FALSE) {
        LOCK_PFN (OldIrql);
        if ((MmAvailablePages < MM_HIGH_LIMIT) ||
            (MI_NONPAGEABLE_MEMORY_AVAILABLE() < MM_HIGH_LIMIT)) {
    
            //
            // No pages are available, the caller will have to replace.
            //
    
            UNLOCK_PFN (OldIrql);
            MiReturnCommitment (_MI_PAGING_LEVELS - 1 - NumberOfPages);
            MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES,
                                       _MI_PAGING_LEVELS - 1 - NumberOfPages);
            return FALSE;
        }
    }

    //
    // Apply the resident available charge for the working set page now
    // before releasing the PFN lock.
    //

    MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_ADD_WS_PAGE);

    WorkingSetPage = MiRemoveZeroPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    MI_WRITE_INVALID_PTE (PointerPte, DemandZeroWritePte);

    MiInitializePfn (WorkingSetPage, PointerPte, 1);

    UNLOCK_PFN (OldIrql);

    NumberOfPages += 1;

    if (_MI_PAGING_LEVELS - 1 - NumberOfPages != 0) {
        MiReturnCommitment (_MI_PAGING_LEVELS - 1 - NumberOfPages);
        MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES,
                         _MI_PAGING_LEVELS - 1 - NumberOfPages);
    }

    MI_MAKE_VALID_PTE (TempPte, WorkingSetPage, MM_READWRITE, PointerPte);

    MI_SET_PTE_DIRTY (TempPte);
    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    NumberOfEntriesMapped = (WSLE_NUMBER)(((PMMWSLE)((PCHAR)Va + PAGE_SIZE)) - Wsle);

    if (WsInfo->Flags.SessionSpace == 1) {
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_WS_GROW, NumberOfPages);
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_PAGE_ALLOC_GROWTH, NumberOfPages);
        InterlockedExchangeAddSizeT (&MmSessionSpace->NonPageablePages,
                                     NumberOfPages);
        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     NumberOfPages);
    }

    CurrentEntry = WorkingSetList->LastInitializedWsle + 1;

    ASSERT (NumberOfEntriesMapped > CurrentEntry);

    WslEntry = &Wsle[CurrentEntry - 1];

    for (i = CurrentEntry; i < NumberOfEntriesMapped; i += 1) {

        //
        // Build the free list, note that the first working
        // set entries (CurrentEntry) are not on the free list.
        // These entries are reserved for the pages which
        // map the working set and the page which contains the PDE.
        //

        WslEntry += 1;
        WslEntry->u1.Long = (i + 1) << MM_FREE_WSLE_SHIFT;
    }

    WslEntry->u1.Long = WorkingSetList->FirstFree << MM_FREE_WSLE_SHIFT;

    ASSERT (CurrentEntry >= WorkingSetList->FirstDynamic);

    WorkingSetList->FirstFree = CurrentEntry;

    WorkingSetList->LastInitializedWsle = (NumberOfEntriesMapped - 1);

    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

    Pfn1->u1.Event = NULL;

    //
    // Get a working set entry.
    //

    ASSERT (WsInfo->WorkingSetSize <= (WorkingSetList->LastInitializedWsle + 1));
    WsInfo->WorkingSetSize += 1;

    ASSERT (WorkingSetList->FirstFree != WSLE_NULL_INDEX);
    ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

    WorkingSetIndex = WorkingSetList->FirstFree;
    WorkingSetList->FirstFree = (WSLE_NUMBER)(Wsle[WorkingSetIndex].u1.Long >> MM_FREE_WSLE_SHIFT);
    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

    if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
        InterlockedIncrementSizeT (&MmPagesAboveWsMinimum);
    }
    if (WorkingSetIndex > WorkingSetList->LastEntry) {
        WorkingSetList->LastEntry = WorkingSetIndex;
    }

    MiUpdateWsle (&WorkingSetIndex, Va, WsInfo, Pfn1, 0);

    MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

    //
    // Lock any created page table pages into the working set.
    //

    if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {

        SwapEntry = WorkingSetList->FirstDynamic;

        if (WorkingSetIndex != WorkingSetList->FirstDynamic) {

            //
            // Swap this entry with the one at first dynamic.
            //

            MiSwapWslEntries (WorkingSetIndex, SwapEntry, WsInfo, FALSE);
        }

        WorkingSetList->FirstDynamic += 1;

        Wsle[SwapEntry].u1.e1.LockedInWs = 1;
        ASSERT (Wsle[SwapEntry].u1.e1.Valid == 1);
    }

#if (_MI_PAGING_LEVELS >= 3)
    while (PageTablePageAllocated != 0) {
    
        if (PageTablePageAllocated & MI_ALLOCATED_PAGE_TABLE) {
            PageTablePageAllocated &= ~MI_ALLOCATED_PAGE_TABLE;
            Pfn1 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
            VirtualAddress = PointerPte;
        }
#if (_MI_PAGING_LEVELS >= 4)
        else if (PageTablePageAllocated & MI_ALLOCATED_PAGE_DIRECTORY) {
            PageTablePageAllocated &= ~MI_ALLOCATED_PAGE_DIRECTORY;
            Pfn1 = MI_PFN_ELEMENT (PointerPpe->u.Hard.PageFrameNumber);
            VirtualAddress = PointerPde;
        }
#endif
        else {
            ASSERT (FALSE);

            SATISFY_OVERZEALOUS_COMPILER (VirtualAddress = NULL);
        }
    
        Pfn1->u1.Event = NULL;
    
        //
        // Get a working set entry.
        //
    
        WsInfo->WorkingSetSize += 1;
    
        ASSERT (WorkingSetList->FirstFree != WSLE_NULL_INDEX);
        ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);
    
        WorkingSetIndex = WorkingSetList->FirstFree;
        WorkingSetList->FirstFree = (WSLE_NUMBER)(Wsle[WorkingSetIndex].u1.Long >> MM_FREE_WSLE_SHIFT);
        ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
                (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
    
        if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
            InterlockedIncrementSizeT (&MmPagesAboveWsMinimum);
        }
        if (WorkingSetIndex > WorkingSetList->LastEntry) {
            WorkingSetList->LastEntry = WorkingSetIndex;
        }
    
        MiUpdateWsle (&WorkingSetIndex, VirtualAddress, WsInfo, Pfn1, 0);
    
        MI_SET_PTE_IN_WORKING_SET (MiGetPteAddress (VirtualAddress),
                                   WorkingSetIndex);
    
        //
        // Lock the created page table page into the working set.
        //
    
        if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {
    
            SwapEntry = WorkingSetList->FirstDynamic;
    
            if (WorkingSetIndex != WorkingSetList->FirstDynamic) {
    
                //
                // Swap this entry with the one at first dynamic.
                //
    
                MiSwapWslEntries (WorkingSetIndex, SwapEntry, WsInfo, FALSE);
            }
    
            WorkingSetList->FirstDynamic += 1;
    
            Wsle[SwapEntry].u1.e1.LockedInWs = 1;
            ASSERT (Wsle[SwapEntry].u1.e1.Valid == 1);
        }
    }
#endif

    ASSERT ((MiGetPteAddress(&Wsle[WorkingSetList->LastInitializedWsle]))->u.Hard.Valid == 1);

    if ((WorkingSetList->HashTable == NULL) &&
        (MmAvailablePages > MM_HIGH_LIMIT)) {

        //
        // Add a hash table to support shared pages in the working set to
        // eliminate costly lookups.
        //

        WsInfo->Flags.GrowWsleHash = 1;
    }

    return TRUE;
}

LOGICAL
MiAddWsleHash (
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This function adds a page directory, page table or actual mapping page
    for hash table creation (or expansion) for the current process.

Arguments:

    WsInfo - Supplies a pointer to the working set info structure.

    PointerPte - Supplies a pointer to the PTE to be filled.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock held.

--*/
{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    WSLE_NUMBER SwapEntry;
    MMPTE TempPte;
    PMMWSLE Wsle;
    PFN_NUMBER WorkingSetPage;
    WSLE_NUMBER WorkingSetIndex;
    PMMWSL WorkingSetList;
    MMPTE DemandZeroWritePte;

    if (MiChargeCommitmentCantExpand (1, FALSE) == FALSE) {
        return FALSE;
    }

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    ASSERT (PointerPte->u.Hard.Valid == 0);

    DemandZeroWritePte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    LOCK_PFN (OldIrql);

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (1);
        return FALSE;
    }

    if (MI_NONPAGEABLE_MEMORY_AVAILABLE() < MM_HIGH_LIMIT) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (1);
        return FALSE;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_WSLE_HASH);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_HASHPAGES, 1);

    WorkingSetPage = MiRemoveZeroPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    MI_WRITE_INVALID_PTE (PointerPte, DemandZeroWritePte);

    MiInitializePfn (WorkingSetPage, PointerPte, 1);

    UNLOCK_PFN (OldIrql);

    MI_MAKE_VALID_PTE (TempPte,
                       WorkingSetPage,
                       MM_READWRITE,
                       PointerPte);

    MI_SET_PTE_DIRTY (TempPte);
    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    //
    // As we have grown the working set, take the
    // next free WSLE from the list and use it.
    //

    Pfn1 = MI_PFN_ELEMENT (WorkingSetPage);

    Pfn1->u1.Event = NULL;

    //
    // Set the low bit in the PFN pointer to indicate that the working set
    // should not be trimmed during the WSLE allocation as the PTEs for the
    // new hash pages are valid but we are still in the midst of making all
    // the associated fields valid.
    //

    WorkingSetIndex = MiAllocateWsle (WsInfo,
                                      PointerPte,
                                      (PMMPFN)((ULONG_PTR)Pfn1 | 0x1),
                                      0);

    if (WorkingSetIndex == 0) {

        //
        // No working set index was available, flush the PTE and the page,
        // and decrement the count on the containing page.
        //

        ASSERT (Pfn1->u3.e1.PrototypePte == 0);

        LOCK_PFN (OldIrql);
        MI_SET_PFN_DELETED (Pfn1);
        UNLOCK_PFN (OldIrql);

        MiTrimPte (MiGetVirtualAddressMappedByPte (PointerPte),
                   PointerPte,
                   Pfn1,
                   PsGetCurrentProcess (),
                   ZeroPte);

        MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_WSLE_HASH);
        MiReturnCommitment (1);

        return FALSE;
    }

    //
    // Lock any created page table pages into the working set.
    //

    if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {

        SwapEntry = WorkingSetList->FirstDynamic;

        if (WorkingSetIndex != WorkingSetList->FirstDynamic) {

            //
            // Swap this entry with the one at first dynamic.
            //

            MiSwapWslEntries (WorkingSetIndex, SwapEntry, WsInfo, FALSE);
        }

        WorkingSetList->FirstDynamic += 1;

        Wsle[SwapEntry].u1.e1.LockedInWs = 1;
        ASSERT (Wsle[SwapEntry].u1.e1.Valid == 1);
    }

    if (WsInfo->Flags.SessionSpace == 1) {
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_HASH_GROW, 1);
        InterlockedIncrementSizeT (&MmSessionSpace->NonPageablePages);
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_HASHPAGE_ALLOC, 1);
        InterlockedIncrementSizeT (&MmSessionSpace->CommittedPages);
    }

    return TRUE;
}

VOID
MiGrowWsleHash (
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This function grows (or adds) a hash table to the working set list
    to allow direct indexing for WSLEs than cannot be located via the
    PFN database WSINDEX field.

    The hash table is located AFTER the WSLE array and the pages are
    locked into the working set just like standard WSLEs.

    Note that the hash table is expanded by setting the hash table
    field in the working set to NULL, but leaving the size as non-zero.
    This indicates that the hash should be expanded and the initial
    portion of the table zeroed.

Arguments:

    WsInfo - Supplies a pointer to the working set info structure.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock held.

--*/
{
    LONG Size;
    PMMPTE StartPte;
    PMMPTE EndPte;
    PMMPTE PointerPte;
    ULONG First;
    ULONG NewSize;
    PMMWSLE_HASH Table;
    PMMWSLE_HASH OriginalTable;
    ULONG j;
    PMMWSL WorkingSetList;
    PVOID EntryHashTableEnd;
    PVOID VirtualAddress;
    KIRQL OldIrql;
    PVOID TempVa;
    PEPROCESS CurrentProcess;
    LOGICAL LoopStart;
    PMMPTE AllocatedPde;
    PMMPTE AllocatedPpe;
    PMMPTE AllocatedPxe;
    PMMPTE PointerPde;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
#endif

    WorkingSetList = WsInfo->VmWorkingSetList;

    Table = WorkingSetList->HashTable;
    OriginalTable = WorkingSetList->HashTable;

    First = WorkingSetList->HashTableSize;

    if (Table == NULL) {

        NewSize = PtrToUlong(PAGE_ALIGN (((1 + WorkingSetList->NonDirectCount) *
                            2 * sizeof(MMWSLE_HASH)) + PAGE_SIZE - 1));

        //
        // Note that the Table may be NULL and the HashTableSize/PTEs nonzero
        // in the case where the hash has been contracted.
        //

        j = First * sizeof(MMWSLE_HASH);

        //
        // Don't try for additional hash pages if we already have
        // the right amount (or too many).
        //

        if ((j + PAGE_SIZE > NewSize) && (j != 0)) {
            WsInfo->Flags.GrowWsleHash = 0;
            return;
        }

        Table = (PMMWSLE_HASH)(WorkingSetList->HashTableStart);
        EntryHashTableEnd = &Table[WorkingSetList->HashTableSize];

        WorkingSetList->HashTableSize = 0;
    }
    else {

        //
        // Attempt to increase by 1/4 of the indirect count.  If this is less
        // than 4 pages, then grow by 4.  Make sure the working set list has
        // enough free entries for the growth size though.
        //

        NewSize = MI_ROUND_TO_SIZE ((WorkingSetList->NonDirectCount >> 2) * sizeof (MMWSLE_HASH), PAGE_SIZE);

        if (NewSize < 4 * PAGE_SIZE) {
            NewSize = 4 * PAGE_SIZE;
        }

        j = WorkingSetList->LastInitializedWsle - WsInfo->WorkingSetSize;

        if (NewSize / PAGE_SIZE >= j) {
            NewSize = PAGE_SIZE;
        }

        EntryHashTableEnd = &Table[WorkingSetList->HashTableSize];
    }

    if ((PCHAR)EntryHashTableEnd + NewSize > (PCHAR)WorkingSetList->HighestPermittedHashAddress) {
        NewSize =
            (ULONG)((PCHAR)(WorkingSetList->HighestPermittedHashAddress) -
                ((PCHAR)EntryHashTableEnd));
        if (NewSize == 0) {
            if (OriginalTable == NULL) {
                WorkingSetList->HashTableSize = First;
            }
            WsInfo->Flags.GrowWsleHash = 0;
            return;
        }
    }


#if (_MI_PAGING_LEVELS >= 4)
    ASSERT64 ((MiGetPxeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPpeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPdeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPteAddress(EntryHashTableEnd)->u.Hard.Valid == 0));
#else
    ASSERT64 ((MiGetPpeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPdeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPteAddress(EntryHashTableEnd)->u.Hard.Valid == 0));
#endif

    ASSERT32 ((EntryHashTableEnd == WorkingSetList->HighestPermittedHashAddress) ||
              (MiGetPdeAddress(EntryHashTableEnd)->u.Hard.Valid == 0) ||
              (MiGetPteAddress(EntryHashTableEnd)->u.Hard.Valid == 0));

    Size = NewSize;
    PointerPte = MiGetPteAddress (EntryHashTableEnd);
    StartPte = PointerPte;
    EndPte = PointerPte + (NewSize >> PAGE_SHIFT);

    PointerPde = NULL;
    LoopStart = TRUE;
    AllocatedPde = NULL;

#if (_MI_PAGING_LEVELS >= 3)
    AllocatedPpe = NULL;
    AllocatedPxe = NULL;
    PointerPpe = NULL;
    PointerPxe = NULL;
#endif

    do {

        if ((LoopStart == TRUE) || MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPteAddress (PointerPte);

#if (_MI_PAGING_LEVELS >= 3)
            PointerPxe = MiGetPpeAddress (PointerPte);
            PointerPpe = MiGetPdeAddress (PointerPte);

#if (_MI_PAGING_LEVELS >= 4)
            if (PointerPxe->u.Hard.Valid == 0) {
                if (MiAddWsleHash (WsInfo, PointerPxe) == FALSE) {
                    break;
                }
                AllocatedPxe = PointerPxe;
            }
#endif

            if (PointerPpe->u.Hard.Valid == 0) {
                if (MiAddWsleHash (WsInfo, PointerPpe) == FALSE) {
                    break;
                }
                AllocatedPpe = PointerPpe;
            }
#endif

            if (PointerPde->u.Hard.Valid == 0) {
                if (MiAddWsleHash (WsInfo, PointerPde) == FALSE) {
                    break;
                }
                AllocatedPde = PointerPde;
            }

            LoopStart = FALSE;
        }
        else {
            AllocatedPde = NULL;
            AllocatedPpe = NULL;
            AllocatedPxe = NULL;
        }

        if (PointerPte->u.Hard.Valid == 0) {
            if (MiAddWsleHash (WsInfo, PointerPte) == FALSE) {
                break;
            }
        }

        PointerPte += 1;
        Size -= PAGE_SIZE;
    } while (Size > 0);

    //
    // If MiAddWsleHash was unable to allocate memory above, then roll back
    // any extra PPEs & PDEs that may have been created.  Note NewSize must
    // be recalculated to handle the fact that memory may have run out.
    //

    if (PointerPte != EndPte) {

        CurrentProcess = PsGetCurrentProcess ();

        //
        // Clean up the last allocated PPE/PDE as they are not needed.
        // Note that the system cache and the session space working sets
        // have no current process (which MiDeletePte requires) which is
        // needed for WSLE and PrivatePages adjustments.
        //

        if (AllocatedPde != NULL) {

            ASSERT (AllocatedPde->u.Hard.Valid == 1);
            TempVa = MiGetVirtualAddressMappedByPte (AllocatedPde);

            if (WsInfo->VmWorkingSetList == MmWorkingSetList) {

                LOCK_PFN (OldIrql);
                MiDeletePte (AllocatedPde,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL,
                             OldIrql);
                UNLOCK_PFN (OldIrql);

                //
                // Add back in the private page MiDeletePte subtracted.
                //

                CurrentProcess->NumberOfPrivatePages += 1;
            }
            else {
                LOCK_PFN (OldIrql);
                MiDeleteValidSystemPte (AllocatedPde,
                                        TempVa,
                                        WsInfo,
                                        NULL);
                UNLOCK_PFN (OldIrql);
            }

            MiReturnCommitment (1);
            MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_WSLE_HASH);
        }
    
#if (_MI_PAGING_LEVELS >= 3)
        if (AllocatedPpe != NULL) {

            ASSERT (AllocatedPpe->u.Hard.Valid == 1);
            TempVa = MiGetVirtualAddressMappedByPte (AllocatedPpe);

            if (WsInfo->VmWorkingSetList == MmWorkingSetList) {
                LOCK_PFN (OldIrql);

                MiDeletePte (AllocatedPpe,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL,
                             OldIrql);

                UNLOCK_PFN (OldIrql);

                //
                // Add back in the private page MiDeletePte subtracted.
                //

                CurrentProcess->NumberOfPrivatePages += 1;
            }
            else {
                LOCK_PFN (OldIrql);
                MiDeleteValidSystemPte (AllocatedPpe,
                                        TempVa,
                                        WsInfo,
                                        NULL);
                UNLOCK_PFN (OldIrql);
            }

            MiReturnCommitment (1);
            MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_WSLE_HASH);
        }

        if (AllocatedPxe != NULL) {

            ASSERT (AllocatedPxe->u.Hard.Valid == 1);
            TempVa = MiGetVirtualAddressMappedByPte (AllocatedPxe);

            if (WsInfo->VmWorkingSetList == MmWorkingSetList) {
                LOCK_PFN (OldIrql);

                MiDeletePte (AllocatedPxe,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL,
                             OldIrql);

                UNLOCK_PFN (OldIrql);

                //
                // Add back in the private page MiDeletePte subtracted.
                //

                CurrentProcess->NumberOfPrivatePages += 1;
            }
            else {
                LOCK_PFN (OldIrql);
                MiDeleteValidSystemPte (AllocatedPxe,
                                        TempVa,
                                        WsInfo,
                                        NULL);
                UNLOCK_PFN (OldIrql);
            }

            MiReturnCommitment (1);
            MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_WSLE_HASH);
        }
#endif

        if (PointerPte == StartPte) {
            if (OriginalTable == NULL) {
                WorkingSetList->HashTableSize = First;
            }
            WsInfo->Flags.GrowWsleHash = 0;
            return;
        }
    }

    NewSize = (ULONG)((PointerPte - StartPte) << PAGE_SHIFT);

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    ASSERT ((VirtualAddress == WorkingSetList->HighestPermittedHashAddress) ||
            (MiIsAddressValid (VirtualAddress, FALSE) == FALSE));

    WorkingSetList->HashTableSize = First + NewSize / sizeof (MMWSLE_HASH);
    WorkingSetList->HashTable = Table;

    VirtualAddress = &Table[WorkingSetList->HashTableSize];

    ASSERT ((VirtualAddress == WorkingSetList->HighestPermittedHashAddress) ||
            (MiIsAddressValid (VirtualAddress, FALSE) == FALSE));

    if (First != 0) {
        RtlZeroMemory (Table, First * sizeof(MMWSLE_HASH));
    }

    WsInfo->Flags.GrowWsleHash = 0;

    //
    // Fill hash table.
    //

    if (WorkingSetList->NonDirectCount != 0) {
        MiFillWsleHash (WorkingSetList);
        return;
    }
}


WSLE_NUMBER
MiFreeWsleList (
    IN PMMSUPPORT WsInfo,
    IN PMMWSLE_FLUSH_LIST WsleFlushList
    )

/*++

Routine Description:

    This routine frees the specified list of WSLEs decrementing the share
    count for the corresponding pages, putting each PTE into a transition
    state if the share count goes to 0.

Arguments:

    WsInfo - Supplies a pointer to the working set structure.

    WsleFlushList - Supplies the list of WSLEs to flush.

Return Value:

    Returns the number of WSLEs that were NOT removed.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    WSLE_NUMBER i;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER NumberNotFlushed;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    PMMPTE ContainingPageTablePage;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PEPROCESS Process;
    PMMPFN Pfn2;
    MMPTE_FLUSH_LIST PteFlushList;
    BOOLEAN AllProcessors;

    PteFlushList.Count = 0;
    SATISFY_OVERZEALOUS_COMPILER (PteFlushList.FlushVa[0] = NULL);

    ASSERT (WsleFlushList->Count != 0);

    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    MM_WS_LOCK_ASSERT (WsInfo);

    if (Wsle == MmWsle) {
        AllProcessors = FALSE;
    }
    else {

        //
        // Must be the system cache or a session space.
        //

        AllProcessors = TRUE;
    }

    LOCK_PFN (OldIrql);

    for (i = 0; i < WsleFlushList->Count; i += 1) {

        WorkingSetIndex = WsleFlushList->FlushIndex[i];

        ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);
        ASSERT (Wsle[WorkingSetIndex].u1.e1.Valid == 1);

        PointerPte = MiGetPteAddress (Wsle[WorkingSetIndex].u1.VirtualAddress);

        TempPte = *PointerPte;
        ASSERT (TempPte.u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Check to see if the located entry is eligible for removal.
        //
        // Note, don't clear the access bit for page table pages
        // with valid PTEs as this could cause an access trap fault which
        // would not be handled (it is only handled for PTEs not PDEs).
        //
        // If the PTE is a page table page with non-zero share count or
        // within the system cache with its reference count greater
        // than 1, don't remove it.
        //

        if (WsInfo == &MmSystemCacheWs) {
            if (Pfn1->u3.e2.ReferenceCount > 1) {
                WsleFlushList->FlushIndex[i] = 0;
                continue;
            }
        }
        else {
            if ((Pfn1->u2.ShareCount > 1) && (Pfn1->u3.e1.PrototypePte == 0)) {

#if DBG
                if (WsInfo->Flags.SessionSpace == 1) {
                    ASSERT (MI_IS_SESSION_ADDRESS (Wsle[WorkingSetIndex].u1.VirtualAddress));
                }
                else {
                    ASSERT32 ((Wsle[WorkingSetIndex].u1.VirtualAddress >= (PVOID)PTE_BASE) &&
                     (Wsle[WorkingSetIndex].u1.VirtualAddress<= (PVOID)PTE_TOP));
                }
#endif

                //
                // Don't remove page table pages from the working set until
                // all transition pages have exited.
                //

                WsleFlushList->FlushIndex[i] = 0;
                continue;
            }
        }

        //
        // Found a candidate, remove the page from the working set.
        //

        ASSERT (MI_IS_PFN_DELETED (Pfn1) == 0);

#ifdef _X86_
#if DBG
#if !defined(NT_UP)
        if (TempPte.u.Hard.Writable == 1) {
            ASSERT (TempPte.u.Hard.Dirty == 1);
        }
#endif //NTUP
#endif //DBG
#endif //X86

        //
        // This page is being removed from the working set, the dirty
        // bit must be ORed into the modify bit in the PFN element.
        //

        MI_CAPTURE_DIRTY_BIT_TO_PFN (&TempPte, Pfn1);

        if (Pfn1->u3.e1.PrototypePte) {

            //
            // This is a prototype PTE.  The PFN database does not contain
            // the contents of this PTE it contains the contents of the
            // prototype PTE.  This PTE must be reconstructed to contain
            // a pointer to the prototype PTE.
            //
            // The working set list entry contains information about
            // how to reconstruct the PTE.
            //

            if (Wsle[WorkingSetIndex].u1.e1.Protection != MM_ZERO_ACCESS) {

                //
                // The protection for the prototype PTE is in the WSLE.
                //

                ASSERT (Wsle[WorkingSetIndex].u1.e1.Protection != 0);

                TempPte.u.Long = 0;
                TempPte.u.Soft.Protection =
                    MI_GET_PROTECTION_FROM_WSLE (&Wsle[WorkingSetIndex]);
                TempPte.u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;
            }
            else {

                //
                // The protection is in the prototype PTE.
                //

                TempPte.u.Long = MiProtoAddressForPte (Pfn1->PteAddress);
            }
        
            TempPte.u.Proto.Prototype = 1;

            //
            // Decrement the share count of the containing page table
            // page as the PTE for the removed page is no longer valid
            // or in transition.
            //

            ContainingPageTablePage = MiGetPteAddress (PointerPte);
#if (_MI_PAGING_LEVELS >= 3)
            ASSERT (ContainingPageTablePage->u.Hard.Valid == 1);
#else
            if (ContainingPageTablePage->u.Hard.Valid == 0) {
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                                  0x61940, 
                                  (ULONG_PTR)PointerPte,
                                  (ULONG_PTR)ContainingPageTablePage->u.Long,
                                  (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
                }
            }
#endif
            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPageTablePage);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
        }
        else {

            //
            // This is a private page, make it transition.
            //
            // If the PTE indicates the page has been modified (this is
            // different from the PFN indicating this), then ripple it
            // back to the write watch bitmap now since we are still in
            // the correct process context.
            //

            if ((MI_IS_PTE_DIRTY(TempPte)) && (Wsle == MmWsle)) {

                Process = CONTAINING_RECORD (WsInfo, EPROCESS, Vm);

                ASSERT (Process == PsGetCurrentProcess ());

                if (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) {

                    //
                    // This process has (or had) write watch VADs.  Search now
                    // for a write watch region encapsulating the PTE being
                    // invalidated.
                    //

                    MiCaptureWriteWatchDirtyBit (Process,
                                           Wsle[WorkingSetIndex].u1.VirtualAddress);
                }
            }

            //
            // Assert that the share count is 1 for all user mode pages.
            //

            ASSERT ((Pfn1->u2.ShareCount == 1) ||
                    (Wsle[WorkingSetIndex].u1.VirtualAddress >
                            (PVOID)MM_HIGHEST_USER_ADDRESS));

            //
            // Set the working set index to zero.  This allows page table
            // pages to be brought back in with the proper WSINDEX.
            //

            ASSERT (Pfn1->u1.WsIndex != 0);
            MI_ZERO_WSINDEX (Pfn1);
            MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                          Pfn1->OriginalPte.u.Soft.Protection);
        }

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushVa[PteFlushList.Count] =
                Wsle[WorkingSetIndex].u1.VirtualAddress;
            PteFlushList.Count += 1;
        }

        //
        // Flush the translation buffer and decrement the number of valid
        // PTEs within the containing page table page.  Note that for a
        // private page, the page table page is still needed because the
        // page is in transition.
        //

        MiDecrementShareCountInline (Pfn1, PageFrameIndex);
    }

    if (PteFlushList.Count == 0) {
        NOTHING;
    }
    else if (PteFlushList.Count == 1) {
        MI_FLUSH_SINGLE_TB (PteFlushList.FlushVa[0], AllProcessors);
    }
    else if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
        MI_FLUSH_MULTIPLE_TB (PteFlushList.Count,
                              &PteFlushList.FlushVa[0],
                              AllProcessors);
    }
    else {
        if (AllProcessors) {
            MI_FLUSH_ENTIRE_TB (0x1A);
        }
        else {
            MI_FLUSH_PROCESS_TB (FALSE);
        }
    }

    UNLOCK_PFN (OldIrql);

    NumberNotFlushed = 0;

    //
    // Remove the working set entries (the PFN lock is not needed for this).
    //

    for (i = 0; i < WsleFlushList->Count; i += 1) {

        WorkingSetIndex = WsleFlushList->FlushIndex[i];

        if (WorkingSetIndex == 0) {
            NumberNotFlushed += 1;
            continue;
        }

        ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);
        ASSERT (Wsle[WorkingSetIndex].u1.e1.Valid == 1);

        MiRemoveWsle (WorkingSetIndex, WorkingSetList);

        ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

        ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);

        //
        // Put the entry on the free list and decrement the current size.
        //

        ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
                (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

        Wsle[WorkingSetIndex].u1.Long = WorkingSetList->FirstFree << MM_FREE_WSLE_SHIFT;
        WorkingSetList->FirstFree = WorkingSetIndex;

        ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
                (WorkingSetList->FirstFree == WSLE_NULL_INDEX));

        if (WsInfo->WorkingSetSize > WsInfo->MinimumWorkingSetSize) {
            InterlockedDecrementSizeT (&MmPagesAboveWsMinimum);
        }

        WsInfo->WorkingSetSize -= 1;
    }

    return NumberNotFlushed;
}

WSLE_NUMBER
MiTrimWorkingSet (
    IN WSLE_NUMBER Reduction,
    IN PMMSUPPORT WsInfo,
    IN ULONG TrimAge
    )

/*++

Routine Description:

    This function reduces the working set by the specified amount.

Arguments:

    Reduction - Supplies the number of pages to remove from the working set.

    WsInfo - Supplies a pointer to the working set information to trim.

    TrimAge - Supplies the age value to use - ie: pages of this age or older
              will be removed.

Return Value:

    Returns the actual number of pages removed.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    WSLE_NUMBER TryToFree;
    WSLE_NUMBER StartEntry;
    WSLE_NUMBER LastEntry;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    PMMPTE PointerPte;
    WSLE_NUMBER NumberLeftToRemove;
    WSLE_NUMBER NumberNotFlushed;
    MMWSLE_FLUSH_LIST WsleFlushList;

    WsleFlushList.Count = 0;

    NumberLeftToRemove = Reduction;
    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    MM_WS_LOCK_ASSERT (WsInfo);

    LastEntry = WorkingSetList->LastEntry;

    TryToFree = WorkingSetList->NextSlot;
    if (TryToFree > LastEntry || TryToFree < WorkingSetList->FirstDynamic) {
        TryToFree = WorkingSetList->FirstDynamic;
    }

    StartEntry = TryToFree;

TrimMore:

    while (NumberLeftToRemove != 0) {
        if (Wsle[TryToFree].u1.e1.Valid == 1) {
            PointerPte = MiGetPteAddress (Wsle[TryToFree].u1.VirtualAddress);
            ASSERT (PointerPte->u.Hard.Valid == 1);

            if ((TrimAge == 0) ||
                ((MI_GET_ACCESSED_IN_PTE (PointerPte) == 0) &&
                (MI_GET_WSLE_AGE(PointerPte, &Wsle[TryToFree]) >= TrimAge))) {

                WsleFlushList.FlushIndex[WsleFlushList.Count] = TryToFree;
                WsleFlushList.Count += 1;
                NumberLeftToRemove -= 1;

                if (WsleFlushList.Count == MM_MAXIMUM_FLUSH_COUNT) {
                    NumberNotFlushed = MiFreeWsleList (WsInfo, &WsleFlushList);
                    WsleFlushList.Count = 0;
                    NumberLeftToRemove += NumberNotFlushed;
                }
            }
        }
        TryToFree += 1;

        if (TryToFree > LastEntry) {
            TryToFree = WorkingSetList->FirstDynamic;
        }

        if (TryToFree == StartEntry) {
            break;
        }
    }

    if (WsleFlushList.Count != 0) {
        NumberNotFlushed = MiFreeWsleList (WsInfo, &WsleFlushList);
        NumberLeftToRemove += NumberNotFlushed;

        if (NumberLeftToRemove != 0) {
            if (TryToFree != StartEntry) {
                WsleFlushList.Count = 0;
                goto TrimMore;
            }
        }
    }

    WorkingSetList->NextSlot = TryToFree;

    //
    // See if the working set list can be contracted.
    //
    // Make sure we are at least a page above the working set maximum.
    //

    if (WorkingSetList->FirstDynamic == WsInfo->WorkingSetSize) {
        MiRemoveWorkingSetPages (WsInfo);
    }
    else {

        if ((WsInfo->WorkingSetSize + 15 + (PAGE_SIZE / sizeof(MMWSLE))) <
                                                WorkingSetList->LastEntry) {
            if ((WsInfo->MaximumWorkingSetSize + 15 + (PAGE_SIZE / sizeof(MMWSLE))) <
                 WorkingSetList->LastEntry ) {

                MiRemoveWorkingSetPages (WsInfo);
            }
        }
    }

    return Reduction - NumberLeftToRemove;
}

LOGICAL
MiEliminateWorkingSetEntry (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn,
    IN PMMSUPPORT WsInfo,
    IN LOGICAL Force
    )

/*++

Routine Description:

    This routine removes the specified working set list entry
    from the working set, flushes the TB for the page, decrements
    the share count for the physical page, and, if necessary turns
    the PTE into a transition PTE.

Arguments:

    WorkingSetIndex - Supplies the working set index to remove.

    PointerPte - Supplies a pointer to the PTE corresponding to the virtual
                 address in the working set.

    Pfn - Supplies a pointer to the PFN element corresponding to the PTE.

    WsInfo - Supplies the working set to operate on.

    Force - Supplies TRUE if the entry should be removed regardless.

Return Value:

    TRUE if the entry was eliminated, FALSE if not.

Environment:

    Kernel mode, Working set pushlock held, APCs disabled.

--*/

{
    KIRQL OldIrql;
    BOOLEAN FlushAllProcessors;
    PMMWSLE Wsle;
    MMWSLE WsleContents;
    PMMPTE ContainingPageTablePage;
    MMPTE TempPte;
    MMPTE PreviousPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PEPROCESS Process;
    PVOID VirtualAddress;
    PMMPFN Pfn2;

    Wsle = WsInfo->VmWorkingSetList->Wsle;
    
    if (Wsle == MmWsle) {
        FlushAllProcessors = FALSE;
    }
    else {

        //
        // Must be the system cache or a session space.
        //

        FlushAllProcessors = TRUE;
    }

    Wsle += WorkingSetIndex;
    WsleContents = *Wsle;

    TempPte = *PointerPte;
    ASSERT (TempPte.u.Hard.Valid == 1);
    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);

    ASSERT (Pfn == MI_PFN_ELEMENT (PageFrameIndex));
    ASSERT (MI_IS_PFN_DELETED (Pfn) == 0);

#if defined(_X86_) || defined(_AMD64_)
#if DBG
#if !defined(NT_UP)
    if (TempPte.u.Hard.Writable == 1) {
        ASSERT (TempPte.u.Hard.Dirty == 1);
    }
#endif //NTUP
#endif //DBG
#endif //X86

    //
    // Construct the invalid PTE format.
    //

    if (Pfn->u3.e1.PrototypePte) {

        //
        // This is a prototype PTE.  The PFN database does not contain
        // the contents of this PTE it contains the contents of the
        // prototype PTE.  This PTE must be reconstructed to contain
        // a pointer to the prototype PTE.
        //
        // The working set list entry contains information about
        // how to reconstruct the PTE.
        //

        if (WsleContents.u1.e1.Protection != MM_ZERO_ACCESS) {

            //
            // The protection for the prototype PTE is in the WSLE.
            //

            ASSERT (WsleContents.u1.e1.Protection != 0);

            TempPte.u.Long = 0;
            TempPte.u.Soft.Protection = MI_GET_PROTECTION_FROM_WSLE (&WsleContents);
            TempPte.u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;
        }
        else {

            //
            // The protection is in the prototype PTE.
            //

            TempPte.u.Long = MiProtoAddressForPte (Pfn->PteAddress);
        }
    
        TempPte.u.Proto.Prototype = 1;

        //
        // Obtain the containing page table page as its share count will
        // need to be decremented (when the PFN lock is held) as the PTE
        // for the removed page is no longer valid or in transition.
        //

        ContainingPageTablePage = MiGetPteAddress (PointerPte);
#if (_MI_PAGING_LEVELS >= 3)
        ASSERT (ContainingPageTablePage->u.Hard.Valid == 1);
#else
        if (ContainingPageTablePage->u.Hard.Valid == 0) {
            if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x61940, 
                              (ULONG_PTR)PointerPte,
                              (ULONG_PTR)ContainingPageTablePage->u.Long,
                              (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
            }
        }
#endif
        PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPageTablePage);
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
    }
    else {

        //
        // This is a private page, make it transition.
        //

        //
        // Assert that the share count is 1 for all user mode pages.
        //

        ASSERT ((Pfn->u2.ShareCount == 1) ||
                (WsleContents.u1.VirtualAddress > (PVOID)MM_HIGHEST_USER_ADDRESS));

        MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                      Pfn->OriginalPte.u.Soft.Protection);

        SATISFY_OVERZEALOUS_COMPILER (PageTableFrameIndex = 0);
        SATISFY_OVERZEALOUS_COMPILER (Pfn2 = NULL);
    }

    LOCK_PFN (OldIrql);

    //
    // If the PTE is a page table page with non-zero share count or
    // within the system cache with its reference count greater
    // than 1, don't remove it.
    //

    if (Force == FALSE) {
        if (WsInfo == &MmSystemCacheWs) {
            if (Pfn->u3.e2.ReferenceCount > 1) {
                UNLOCK_PFN (OldIrql);
                return FALSE;
            }
        }
        else {
            if ((Pfn->u2.ShareCount > 1) && (Pfn->u3.e1.PrototypePte == 0)) {
    
#if DBG
                if (WsInfo->Flags.SessionSpace == 1) {
                    ASSERT (MI_IS_SESSION_ADDRESS (WsleContents.u1.VirtualAddress));
                }
                else {
                    ASSERT32 ((WsleContents.u1.VirtualAddress >= (PVOID)PTE_BASE) &&
                              (WsleContents.u1.VirtualAddress <= (PVOID)PTE_TOP));
                }
#endif
    
                //
                // Don't remove page table pages from the working set until
                // all transition pages have exited.
                //
    
                UNLOCK_PFN (OldIrql);
                return FALSE;
            }
        }
    }
    
    //
    // Remove the page from the working set.
    //

    if (Pfn->u3.e1.PrototypePte) {

        //
        // Decrement the share count of the containing page table
        // page as the PTE for the removed page is no longer valid
        // or in transition.
        //

        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
    }
    else {

        //
        // Set the working set index to zero.  This allows page table
        // pages to be brought back in with the proper WSINDEX.
        //

        ASSERT (Pfn->u1.WsIndex != 0);
        MI_ZERO_WSINDEX (Pfn);
    }

    PreviousPte = *PointerPte;

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

    //
    // Flush the translation buffer.
    //

    MI_FLUSH_SINGLE_TB (WsleContents.u1.VirtualAddress, FlushAllProcessors);

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    //
    // A page is being removed from the working set, on certain
    // hardware the dirty bit should be ORed into the modify bit in
    // the PFN element.
    //

    MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn);

    //
    // If the PTE indicates the page has been modified (this is different
    // from the PFN indicating this), then ripple it back to the write watch
    // bitmap now since we are still in the correct process context.
    //

    if ((Pfn->u3.e1.PrototypePte == 0) && (MI_IS_PTE_DIRTY(PreviousPte))) {

        Process = PsGetCurrentProcess ();

        if (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) {

            //
            // This process has (or had) write watch VADs.  Search now
            // for a write watch region encapsulating the PTE being
            // invalidated.
            //

            VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
            MiCaptureWriteWatchDirtyBit (Process, VirtualAddress);
        }
    }

    //
    // Decrement the share count on the page.  Note that for a
    // private page, the page table page is still needed because the
    // page is in transition.
    //

    MiDecrementShareCountInline (Pfn, PageFrameIndex);

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MiRemoveWorkingSetPages (
    IN PMMSUPPORT WsInfo
    )

/*++

Routine Description:

    This routine compresses the WSLEs into the front of the working set
    and frees the pages for unneeded working set entries.

Arguments:

    WsInfo - Supplies a pointer to the working set structure to compress.

Return Value:

    None.

Environment:

    Kernel mode, Working set lock held, APCs disabled.

--*/

{
    LOGICAL MovedOne;
    PFN_NUMBER PageFrameIndex;
    PMMWSLE FreeEntry;
    PMMWSLE LastEntry;
    PMMWSLE Wsle;
    WSLE_NUMBER DynamicEntries;
    WSLE_NUMBER LockedEntries;
    WSLE_NUMBER FreeIndex;
    WSLE_NUMBER LastIndex;
    PMMPTE OldPte;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    ULONG NewSize;
    PMMWSLE_HASH Table;
    MMWSLE WsleContents;
    PMMWSL WorkingSetList;

    WorkingSetList = WsInfo->VmWorkingSetList;

#if DBG
    MiCheckNullIndex (WorkingSetList);
#endif

Top:

    //
    // Check to see if the WSLE hash table should be contracted.
    //

    if (WorkingSetList->HashTable) {

        Table = WorkingSetList->HashTable;

#if DBG
        if ((PVOID)(&Table[WorkingSetList->HashTableSize]) < WorkingSetList->HighestPermittedHashAddress) {
            ASSERT (MiIsAddressValid (&Table[WorkingSetList->HashTableSize], FALSE) == FALSE);
        }
#endif

        if (WsInfo->WorkingSetSize < 200) {
            NewSize = 0;
        }
        else {
            NewSize = PtrToUlong(PAGE_ALIGN ((WorkingSetList->NonDirectCount * 2 *
                                       sizeof(MMWSLE_HASH)) + PAGE_SIZE - 1));
    
            NewSize = NewSize / sizeof(MMWSLE_HASH);
        }

        if (NewSize < WorkingSetList->HashTableSize) {

            if (NewSize != 0) {
                WsInfo->Flags.GrowWsleHash = 1;
            }

            //
            // Remove pages from hash table.
            //

            ASSERT (((ULONG_PTR)&WorkingSetList->HashTable[NewSize] &
                                                    (PAGE_SIZE - 1)) == 0);

            PointerPte = MiGetPteAddress (&WorkingSetList->HashTable[NewSize]);

            LastPte = MiGetPteAddress (WorkingSetList->HighestPermittedHashAddress);
            //
            // Set the hash table to null indicating that no hashing
            // is going on.
            //

            WorkingSetList->HashTable = NULL;
            WorkingSetList->HashTableSize = NewSize;

            MiDeletePteRange (WsInfo, PointerPte, LastPte, FALSE);
        }
#if (_MI_PAGING_LEVELS >= 4)

        //
        // For NT64, the page tables and page directories are also
        // deleted during contraction.
        //

        ASSERT ((MiGetPxeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPpeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPdeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPteAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0));

#elif (_MI_PAGING_LEVELS >= 3)

        //
        // For NT64, the page tables and page directories are also
        // deleted during contraction.
        //

        ASSERT ((MiGetPpeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPdeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPteAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0));

#else

        ASSERT ((&Table[WorkingSetList->HashTableSize] == WorkingSetList->HighestPermittedHashAddress) ||
                (MiGetPdeAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0) ||
                (MiGetPteAddress(&Table[WorkingSetList->HashTableSize])->u.Hard.Valid == 0));

#endif
    }

    //
    // Compress all the valid working set entries to the front of the list.
    //

    Wsle = WorkingSetList->Wsle;

    LockedEntries = WorkingSetList->FirstDynamic;

    if (WsInfo == &MmSystemCacheWs) {

        //
        // The first entry of the system cache working set list is never used
        // because WSL index 0 in a PFN is treated specially by the fault
        // processing code (and trimming) to mean that the entry should be
        // inserted into the WSL by the current thread.
        //
        // This is not an issue for process or session working sets because
        // for them, entry 0 is the top level page directory which must already
        // be resident in order for the process to even get to run (ie: so
        // the process cannot fault on it).
        //

        ASSERT (WorkingSetList->FirstDynamic != 0);

        LockedEntries -= 1;
    }

    ASSERT (WsInfo->WorkingSetSize >= LockedEntries);

    MovedOne = FALSE;
    DynamicEntries = WsInfo->WorkingSetSize - LockedEntries;

    if (DynamicEntries == 0) {

        //
        // If the only pages in the working set are locked pages (that
        // is all pages are BEFORE first dynamic, just reorganize the
        // free list).
        //

        LastIndex = WorkingSetList->FirstDynamic;
        LastEntry = &Wsle[LastIndex];
    }
    else {

        //
        // Start from the first dynamic and move towards the end looking
        // for free entries.  At the same time start from the end and
        // move towards first dynamic looking for valid entries.
        //

        FreeIndex = WorkingSetList->FirstDynamic;
        FreeEntry = &Wsle[FreeIndex];
        LastIndex = WorkingSetList->LastEntry;
        LastEntry = &Wsle[LastIndex];

        while (FreeEntry < LastEntry) {
            if (FreeEntry->u1.e1.Valid == 1) {
                FreeEntry += 1;
                FreeIndex += 1;
                DynamicEntries -= 1;
            }
            else if (LastEntry->u1.e1.Valid == 0) {
                LastEntry -= 1;
                LastIndex -= 1;
            }
            else {

                //
                // Move the WSLE at LastEntry to the free slot at FreeEntry.
                //

                MovedOne = TRUE;
                WsleContents = *LastEntry;
                PointerPte = MiGetPteAddress (LastEntry->u1.VirtualAddress);
                ASSERT (PointerPte->u.Hard.Valid == 1);
                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

                MI_LOG_WSLE_CHANGE (WorkingSetList, FreeIndex, WsleContents);

                *FreeEntry = WsleContents;

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                if (WsleContents.u1.e1.Direct) {
                    Pfn1->u1.WsIndex = FreeIndex;
                }
                else {

                    //
                    // This last working set entry is not direct.
                    // Remove it from there and re-insert it in the hash at the
                    // lowest free slot.
                    //

                    MiRemoveWsle (LastIndex, WorkingSetList);

                    FreeEntry->u1.e1.Hashed = 0;

                    WorkingSetList->NonDirectCount += 1;

                    if (Pfn1->u1.WsIndex != FreeIndex) {
                        MiInsertWsleHash (FreeIndex, WsInfo);
                    }
                }

                MI_SET_PTE_IN_WORKING_SET (PointerPte, FreeIndex);
                LastEntry->u1.Long = 0;
                LastEntry -= 1;
                LastIndex -= 1;
                FreeEntry += 1;
                FreeIndex += 1;
                DynamicEntries -= 1;
            }

            if (DynamicEntries == 0) {

                //
                // The last dynamic entry has been processed, no need to look
                // at any more - the rest are all invalid.
                //

                LastEntry = FreeEntry;
                LastIndex = FreeIndex;
                break;
            }
        }
    }

    //
    // Reorganize the free list.  Make last entry the first free.
    //

    ASSERT (((LastEntry - 1)->u1.e1.Valid == 1) ||
            (WsInfo->WorkingSetSize == 0) ||
            ((WsInfo == &MmSystemCacheWs) && (LastEntry - 1 == WorkingSetList->Wsle)));

    if (LastEntry->u1.e1.Valid == 1) {
        LastEntry += 1;
        LastIndex += 1;
    }

    ASSERT (((LastEntry - 1)->u1.e1.Valid == 1) || (WsInfo->WorkingSetSize == 0));

    ASSERT ((MiIsAddressValid (LastEntry, FALSE) == FALSE) || (LastEntry->u1.e1.Valid == 0));

    //
    // If the working set valid & free entries are already compressed optimally
    // (or fit into a single page) then bail.
    //

    if ((MovedOne == FALSE) &&

        ((WorkingSetList->LastInitializedWsle + 1 == (PAGE_SIZE - BYTE_OFFSET (WorkingSetList->Wsle)) / sizeof (MMWSLE)) ||

         ((WorkingSetList->FirstFree == LastIndex) &&
         ((WorkingSetList->LastEntry == LastIndex - 1) ||
         (WorkingSetList->LastEntry == WorkingSetList->FirstDynamic))))) {

#if DBG
        while (LastIndex < WorkingSetList->LastInitializedWsle) {
            ASSERT (LastEntry->u1.e1.Valid == 0);
            LastIndex += 1;
            LastEntry += 1;
        }
#endif

        return;
    }

    WorkingSetList->LastEntry = LastIndex - 1;

    if (WorkingSetList->FirstFree != WSLE_NULL_INDEX) {
        WorkingSetList->FirstFree = LastIndex;
    }

    //
    // Point free entry to the first invalid page.
    //

    FreeEntry = LastEntry;

    while (LastIndex < WorkingSetList->LastInitializedWsle) {

        //
        // Put the remainder of the WSLEs on the free list.
        //

        ASSERT (LastEntry->u1.e1.Valid == 0);
        LastIndex += 1;
        LastEntry->u1.Long = LastIndex << MM_FREE_WSLE_SHIFT;
        LastEntry += 1;
    }

    //
    // Calculate the start and end of the working set pages at the end
    // that we will delete shortly.  Don't delete them until after
    // LastInitializedWsle is reduced so that debug WSL validation code
    // in MiReleaseWsle (called from MiDeletePte) will see a
    // consistent snapshot.
    //

    LastPte = MiGetPteAddress (&Wsle[WorkingSetList->LastInitializedWsle]) + 1;

    PointerPte = MiGetPteAddress (FreeEntry) + 1;

    //
    // Mark the last working set entry in the list as free.  Note if the list
    // has no free entries, the marker is in FirstFree (and cannot be put into
    // the list anyway because there is no space).
    //

    if (WorkingSetList->FirstFree == WSLE_NULL_INDEX) {
        FreeEntry -= 1;
        PointerPte -= 1;
    }
    else {

        ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);
    
        LastEntry = (PMMWSLE)((PCHAR)(PAGE_ALIGN(FreeEntry)) + PAGE_SIZE);
        LastEntry -= 1;
    
        ASSERT (LastEntry->u1.e1.Valid == 0);
    
        //
        // Insert the end of list delimiter.
        //

        LastEntry->u1.Long = WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT;
        ASSERT (LastEntry > &Wsle[0]);
    
        ASSERT (WsInfo->WorkingSetSize <= (WSLE_NUMBER)(LastEntry - &Wsle[0] + 1));
    
        WorkingSetList->LastInitializedWsle = (WSLE_NUMBER)(LastEntry - &Wsle[0]);
    }

    WorkingSetList->NextSlot = WorkingSetList->FirstDynamic;

    ASSERT (WorkingSetList->LastEntry <= WorkingSetList->LastInitializedWsle);

    ASSERT ((MiGetPteAddress(&Wsle[WorkingSetList->LastInitializedWsle]))->u.Hard.Valid == 1);
    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
#if DBG
    MiCheckNullIndex (WorkingSetList);
#endif

    //
    // Delete the working set pages at the end.  Note that since these are
    // in the working set themselves, that after this deletion, we must
    // reprocess the working set itself again to look for excess pages.
    //

    ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

    OldPte = MiGetPteAddress (WorkingSetList->Wsle + WsInfo->WorkingSetSize);

    MiDeletePteRange (WsInfo, PointerPte, LastPte, FALSE);

    ASSERT (WorkingSetList->FirstFree >= WorkingSetList->FirstDynamic);

    ASSERT (WorkingSetList->LastEntry <= WorkingSetList->LastInitializedWsle);

    ASSERT ((MiGetPteAddress(&Wsle[WorkingSetList->LastInitializedWsle]))->u.Hard.Valid == 1);
    ASSERT ((WorkingSetList->FirstFree <= WorkingSetList->LastInitializedWsle) ||
            (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
#if DBG
    MiCheckNullIndex (WorkingSetList);
#endif

    //
    // The working set size will be reduced by the number of PTEs just deleted
    // above.  If this enables us to shrink the table size then it is mandatory
    // to do so since this is called from the process exit path to remove
    // everything possible (anything that isn't will be leaked!).
    //

    PointerPte = MiGetPteAddress (WorkingSetList->Wsle + WsInfo->WorkingSetSize);

    if (PointerPte != OldPte) {
        ASSERT (PointerPte < OldPte);

        goto Top;
    }
    return;
}


NTSTATUS
MiEmptyWorkingSet (
    IN PMMSUPPORT WsInfo,
    IN LOGICAL NeedLock
    )

/*++

Routine Description:

    This routine frees all pages from the working set.

Arguments:

    WsInfo - Supplies the working set information entry to trim.

    NeedLock - Supplies TRUE if the caller needs us to acquire mutex
               synchronization for the working set.  Supplies FALSE if the
               caller has already acquired synchronization.

Return Value:

    Status of operation.

Environment:

    Kernel mode. No locks.  For session operations, the caller is responsible
    for attaching into the proper session.

--*/

{
    PETHREAD Thread;
    PMMPTE PointerPte;
    WSLE_NUMBER Entry;
    WSLE_NUMBER FirstDynamic;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;
    PMMPFN Pfn1;
    MMWSLE_FLUSH_LIST WsleFlushList;

    WsleFlushList.Count = 0;

    Thread = PsGetCurrentThread ();
    WorkingSetList = WsInfo->VmWorkingSetList;
    Wsle = WorkingSetList->Wsle;

    if (NeedLock == TRUE) {
        LOCK_WORKING_SET (Thread, WsInfo);
    }
    else {
        MM_WS_LOCK_ASSERT (WsInfo);
    }

    if (WsInfo->VmWorkingSetList == MmWorkingSetList) {
        if (PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            if (NeedLock == TRUE) {
                UNLOCK_WORKING_SET (Thread, WsInfo);
            }
            return STATUS_PROCESS_IS_TERMINATING;
        }
    }

    //
    // Attempt to remove the pages starting at the top to keep the free list
    // compressed as entries are added to the freelist in FILO order.
    //

    FirstDynamic = WorkingSetList->FirstDynamic;

    for (Entry = WorkingSetList->LastEntry; Entry >= FirstDynamic; Entry -= 1) {

        if (Wsle[Entry].u1.e1.Valid != 0) {

            PointerPte = MiGetPteAddress (Wsle[Entry].u1.VirtualAddress);
            ASSERT (PointerPte->u.Hard.Valid == 1);
            Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte));

            if (MiTrimRemovalPagesOnly == TRUE) {
                if (Pfn1->u3.e1.RemovalRequested == 0) {
                    Pfn1 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
                    if (Pfn1->u3.e1.RemovalRequested == 0) {
#if (_MI_PAGING_LEVELS >= 3)
                        Pfn1 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
                        if (Pfn1->u3.e1.RemovalRequested == 0) {
                            continue;
                        }
#else
                        continue;
#endif
                    }
                }
            }

            WsleFlushList.FlushIndex[WsleFlushList.Count] = Entry;
            WsleFlushList.Count += 1;

            if (WsleFlushList.Count == MM_MAXIMUM_FLUSH_COUNT) {
                MiFreeWsleList (WsInfo, &WsleFlushList);
                WsleFlushList.Count = 0;
            }
        }
    }

    if (WsleFlushList.Count != 0) {
        MiFreeWsleList (WsInfo, &WsleFlushList);
    }

    MiRemoveWorkingSetPages (WsInfo);

    if (NeedLock == TRUE) {
        UNLOCK_WORKING_SET (Thread, WsInfo);

#if DBG
        if (MiTbDebug) {
    
            KeEnterCriticalRegionThread (&Thread->Tcb);
        
            MiFlushAllPages ();
        
            KeLeaveCriticalRegionThread (&Thread->Tcb);
        
            //
            // Run the transition list and free all the entries so transition
            // faults are not satisfied for any of the modified pages that were
            // just written.  This will generally cause faults to be satisfied
            // using a different page from the original one, which is a useful
            // mode to us when we are trying to track down TB flush problems.
            //
        
            MiPurgeTransitionList ();
        }
#endif
    }

    return STATUS_SUCCESS;
}


#if DBG
VOID
MiCheckNullIndex (
    IN PMMWSL WorkingSetList
    )

{
    PMMWSLE Wsle;
    ULONG j;
    ULONG Nulls = 0;

    Wsle = WorkingSetList->Wsle;
    for (j = 0;j <= WorkingSetList->LastInitializedWsle; j += 1) {
        if ((((Wsle[j].u1.Long)) >> MM_FREE_WSLE_SHIFT) == WSLE_NULL_INDEX) {
            Nulls += 1;
        }
    }
    ASSERT ((Nulls == 1) || (WorkingSetList->FirstFree == WSLE_NULL_INDEX));
    return;
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\vadtree.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

    vadtree.c

Abstract:

    This module contains the routines to manipulate the virtual address
    descriptor tree.

Environment:

    Kernel mode only, APCs disabled.

--*/

#include "mi.h"

VOID
VadTreeWalk (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE, MmPerfVadTreeWalk)
#pragma alloc_text(PAGE, MiInsertVadCharges)
#pragma alloc_text(PAGE, MiRemoveVadCharges)
#if DBG
#pragma alloc_text(PAGE, VadTreeWalk)
#endif
#endif


NTSTATUS
MiInsertVadCharges (
    IN PMMVAD Vad,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This function charges various quotas for the specified virtual address
    descriptor.

Arguments:

    Vad - Supplies a pointer to a virtual address descriptor.

    Process - Supplies a pointer to the current process.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, address creation mutex held, but not the working set pushlock.

--*/

{
    ULONG StartBit;
    ULONG EndBit;
    SIZE_T RealCharge;
    SIZE_T PageCharge;
    SIZE_T PagesReallyCharged;
    ULONG FirstPage;
    ULONG LastPage;
    SIZE_T PagedPoolCharge;
    LOGICAL ChargedJobCommit;
    NTSTATUS Status;
    RTL_BITMAP VadBitMap;
#if (_MI_PAGING_LEVELS >= 3)
    ULONG FirstPdPage;
    ULONG LastPdPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    ULONG FirstPpPage;
    ULONG LastPpPage;
#endif

    ASSERT (Vad->EndingVpn >= Vad->StartingVpn);

    ASSERT (CurrentProcess == PsGetCurrentProcess ());

    //
    // Commit charge of MAX_COMMIT means don't charge quota.
    //

    if (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT) {

        PageCharge = 0;
        PagedPoolCharge = 0;
        ChargedJobCommit = FALSE;

        //
        // Charge quota for the nonpaged pool for the VAD.  This is
        // done here rather than by using ExAllocatePoolWithQuota
        // so the process object is not referenced by the quota charge.
        //

        Status = PsChargeProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));
        if (!NT_SUCCESS(Status)) {
            return STATUS_COMMITMENT_LIMIT;
        }

        //
        // Charge quota for the prototype PTEs if this is a mapped view.
        //

        if ((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) {

            PagedPoolCharge =
              (Vad->EndingVpn - Vad->StartingVpn + 1) << PTE_SHIFT;

            Status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                                    PagedPoolCharge);

            if (!NT_SUCCESS(Status)) {
                PagedPoolCharge = 0;
                RealCharge = 0;
                goto Failed;
            }
        }

        //
        // Add in the charge for page table pages.
        //

        FirstPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPage <= LastPage) {

#if (_MI_PAGING_LEVELS >= 3)
            ASSERT (FirstPage < MmWorkingSetList->MaximumUserPageTablePages);
#endif
            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                               FirstPage)) {
                PageCharge += 1;
            }
            FirstPage += 1;
        }

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Add in the charge for page directory parent pages.
        //

        FirstPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPpPage <= LastPpPage) {

            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                               FirstPpPage)) {
                PageCharge += 1;
            }
            FirstPpPage += 1;
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Add in the charge for page directory pages.
        //

        FirstPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPdPage <= LastPdPage) {

#if (_MI_PAGING_LEVELS >= 4)
            ASSERT (FirstPdPage < MmWorkingSetList->MaximumUserPageDirectoryPages);
#endif
            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                               FirstPdPage)) {
                PageCharge += 1;
            }
            FirstPdPage += 1;
        }
#endif

        RealCharge = Vad->u.VadFlags.CommitCharge + PageCharge;

        if (RealCharge != 0) {

            Status = PsChargeProcessPageFileQuota (CurrentProcess, RealCharge);
            if (!NT_SUCCESS (Status)) {
                RealCharge = 0;
                goto Failed;
            }

            if (CurrentProcess->CommitChargeLimit) {
                if (CurrentProcess->CommitCharge + RealCharge > CurrentProcess->CommitChargeLimit) {
                    if (CurrentProcess->Job) {
                        PsReportProcessMemoryLimitViolation ();
                    }
                    goto Failed;
                }
            }
            if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                if (PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, RealCharge) == FALSE) {
                    goto Failed;
                }
                ChargedJobCommit = TRUE;
            }

            if (MiChargeCommitment (RealCharge, NULL) == FALSE) {
                goto Failed;
            }

            CurrentProcess->CommitCharge += RealCharge;
            if (CurrentProcess->CommitCharge > CurrentProcess->CommitChargePeak) {
                CurrentProcess->CommitChargePeak = CurrentProcess->CommitCharge;
            }

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (RealCharge);

            ASSERT (RealCharge == Vad->u.VadFlags.CommitCharge + PageCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD, Vad->u.VadFlags.CommitCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD_PT, PageCharge);
        }

        if (PageCharge != 0) {

            //
            // Since the commitment was successful, charge the page
            // table pages.
            //

            PagesReallyCharged = 0;

            FirstPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPage <= LastPage) {

#if (_MI_PAGING_LEVELS >= 3)
                ASSERT (FirstPage < MmWorkingSetList->MaximumUserPageTablePages);
#endif
                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                                   FirstPage)) {
                    MI_SET_BIT (MmWorkingSetList->CommittedPageTables,
                                FirstPage);
                    MmWorkingSetList->NumberOfCommittedPageTables += 1;

                    ASSERT32 (MmWorkingSetList->NumberOfCommittedPageTables <
                                                 PD_PER_SYSTEM * PDE_PER_PAGE);
                    PagesReallyCharged += 1;
                }
                FirstPage += 1;
            }

#if (_MI_PAGING_LEVELS >= 3)

            //
            // Charge the page directory pages.
            //

            FirstPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPdPage <= LastPdPage) {

#if (_MI_PAGING_LEVELS >= 4)
                ASSERT (FirstPdPage < MmWorkingSetList->MaximumUserPageDirectoryPages);
#endif
                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                                   FirstPdPage)) {

                    MI_SET_BIT (MmWorkingSetList->CommittedPageDirectories,
                                FirstPdPage);
                    MmWorkingSetList->NumberOfCommittedPageDirectories += 1;
#if (_MI_PAGING_LEVELS == 3)
                    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectories <
                                                                 PDE_PER_PAGE);
#endif
                    PagesReallyCharged += 1;
                }
                FirstPdPage += 1;
            }
#endif

#if (_MI_PAGING_LEVELS >= 4)

            //
            // Charge the page directory parent pages.
            //

            FirstPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPpPage <= LastPpPage) {

                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                   FirstPpPage)) {

                    MI_SET_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                FirstPpPage);
                    MmWorkingSetList->NumberOfCommittedPageDirectoryParents += 1;
                    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectoryParents <
                                                                 PDE_PER_PAGE);
                    PagesReallyCharged += 1;
                }
                FirstPpPage += 1;
            }
#endif

            ASSERT (PageCharge == PagesReallyCharged);
        }
    }

    //
    // Set the relevant fields in the Vad bitmap.
    //

    StartBit = (ULONG)(((ULONG_PTR) MI_64K_ALIGN (MI_VPN_TO_VA (Vad->StartingVpn))) / X64K);
    EndBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (MI_VPN_TO_VA (Vad->EndingVpn))) / X64K);

    //
    // Initialize the bitmap inline for speed.
    //

    VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
    VadBitMap.Buffer = VAD_BITMAP_SPACE;

    //
    // Note VADs like the PEB & TEB start on page (not 64K) boundaries so
    // for these, the relevant bits may already be set.
    //

#if defined (_WIN64) || defined (_X86PAE_)
    if (EndBit > MiLastVadBit) {
        EndBit = MiLastVadBit;
    }

    //
    // Only the first (PAGE_SIZE*8*64K) of VA space on NT64 is bitmapped.
    //

    if (StartBit <= MiLastVadBit) {
        RtlSetBits (&VadBitMap, StartBit, EndBit - StartBit + 1);
    }
#else
    RtlSetBits (&VadBitMap, StartBit, EndBit - StartBit + 1);
#endif

    if (MmWorkingSetList->VadBitMapHint == StartBit) {
        MmWorkingSetList->VadBitMapHint = EndBit + 1;
    }

    if ((CurrentProcess->VadFreeHint != NULL) &&
        (((ULONG_PTR)((PMMVAD)CurrentProcess->VadFreeHint)->EndingVpn + MI_VA_TO_VPN (X64K)) >= Vad->StartingVpn)) {

        CurrentProcess->VadFreeHint = Vad;
    }

    return STATUS_SUCCESS;

Failed:

    //
    // Return any quotas charged thus far.
    //

    PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));

    if (PagedPoolCharge != 0) {
        PsReturnProcessPagedPoolQuota (CurrentProcess, PagedPoolCharge);
    }

    if (RealCharge != 0) {
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
    }

    if (ChargedJobCommit == TRUE) {
        PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)RealCharge);
    }

    return STATUS_COMMITMENT_LIMIT;
}


VOID
MiRemoveVadCharges (
    IN PMMVAD Vad,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This function removes a virtual address descriptor's commit charges.
    The VAD remains in the tree.

Arguments:

    Vad - Supplies a pointer to a virtual address descriptor.

    Process - Supplies a pointer to the current process.

Return Value:

    None.

Environment:

    Kernel mode, address creation mutex held, but not the working set pushlock.

--*/

{
    SIZE_T RealCharge;
    PLIST_ENTRY Next;
    PMMSECURE_ENTRY Entry;

    ASSERT (!MM_ANY_WS_LOCK_HELD (PsGetCurrentThread()));

    ASSERT (CurrentProcess == PsGetCurrentProcess ());

    //
    // Commit charge of MAX_COMMIT means quota was not charged.
    //

    if (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT) {

        //
        // Return the quota charge to the process.
        //

        PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));

        if ((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) {
            PsReturnProcessPagedPoolQuota (CurrentProcess,
                                           (Vad->EndingVpn - Vad->StartingVpn + 1) << PTE_SHIFT);
        }

        RealCharge = Vad->u.VadFlags.CommitCharge;

        if (RealCharge != 0) {

            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);

            MiReturnCommitment (RealCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_VAD, RealCharge);
            if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)RealCharge);
            }
            CurrentProcess->CommitCharge -= RealCharge;

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - RealCharge);
        }
    }

    if (Vad->u.VadFlags.NoChange) {
        if (Vad->u2.VadFlags2.MultipleSecured) {

           //
           // Free the outstanding pool allocations.
           //

            Next = ((PMMVAD_LONG) Vad)->u3.List.Flink;
            do {
                Entry = CONTAINING_RECORD( Next,
                                           MMSECURE_ENTRY,
                                           List);

                Next = Entry->List.Flink;
                ExFreePool (Entry);
            } while (Next != &((PMMVAD_LONG)Vad)->u3.List);
        }
    }

    if (Vad == CurrentProcess->VadFreeHint) {
        CurrentProcess->VadFreeHint = MiGetPreviousVad (Vad);
    }

    return;
}


NTSTATUS
MiFindEmptyAddressRange (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN ULONG QuickCheck,
    IN PVOID *Base
    )

/*++

Routine Description:

    The function examines the virtual address descriptors to locate
    an unused range of the specified size and returns the starting
    address of the range.

Arguments:

    SizeOfRange - Supplies the size in bytes of the range to locate.

    Alignment - Supplies the alignment for the address.  Must be
                 a power of 2 and greater than the page_size.

    QuickCheck - Supplies a zero if a quick check for free memory
                 after the VadFreeHint exists, non-zero if checking
                 should start at the lowest address.

    Base - Receives the starting address of a suitable range on success.

Return Value:

    NTSTATUS.

--*/

{
    ULONG FirstBitValue;
    ULONG StartPosition;
    ULONG BitsNeeded;
    PMMVAD NextVad;
    PMMVAD FreeHint;
    PEPROCESS CurrentProcess;
    PVOID StartingVa;
    PVOID EndingVa;
    NTSTATUS Status;
    RTL_BITMAP VadBitMap;

    CurrentProcess = PsGetCurrentProcess ();

    if ((QuickCheck == 0) && (Alignment == X64K)) {
                    
        //
        // Initialize the bitmap inline for speed.
        //

        VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
        VadBitMap.Buffer = VAD_BITMAP_SPACE;

        //
        // Skip the first bit here as we don't generally recommend
        // that applications map virtual address zero.
        //

        FirstBitValue = *((PULONG)VAD_BITMAP_SPACE);

        *((PULONG)VAD_BITMAP_SPACE) = (FirstBitValue | 0x1);

        BitsNeeded = (ULONG) ((MI_ROUND_TO_64K (SizeOfRange)) / X64K);

        StartPosition = RtlFindClearBits (&VadBitMap,
                                          BitsNeeded,
                                          MmWorkingSetList->VadBitMapHint);

        if (FirstBitValue & 0x1) {
            FirstBitValue = (ULONG)-1;
        }
        else {
            FirstBitValue = (ULONG)~0x1;
        }

        *((PULONG)VAD_BITMAP_SPACE) &= FirstBitValue;

        if (StartPosition != NO_BITS_FOUND) {
            *Base = (PVOID) (((ULONG_PTR)StartPosition) * X64K);
#if DBG
            if (MiCheckForConflictingVad (CurrentProcess, *Base, (ULONG_PTR)*Base + SizeOfRange - 1) != NULL) {
                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_ERROR_LEVEL, 
                    "MiFindEmptyAddressRange: overlapping VAD %p %p\n", *Base, SizeOfRange);
                DbgBreakPoint ();
            }
#endif
            return STATUS_SUCCESS;
        }

        FreeHint = CurrentProcess->VadFreeHint;

        if (FreeHint != NULL) {

            EndingVa = MI_VPN_TO_VA_ENDING (FreeHint->EndingVpn);
            NextVad = MiGetNextVad (FreeHint);

            if (NextVad == NULL) {

                if (SizeOfRange <
                    (((ULONG_PTR)MM_HIGHEST_USER_ADDRESS + 1) -
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa, Alignment))) {
                    *Base = (PVOID) MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,
                                                         Alignment);
                    return STATUS_SUCCESS;
                }
            }
            else {
                StartingVa = MI_VPN_TO_VA (NextVad->StartingVpn);

                if (SizeOfRange <
                    ((ULONG_PTR)StartingVa -
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa, Alignment))) {

                    //
                    // Check to ensure that the ending address aligned upwards
                    // is not greater than the starting address.
                    //

                    if ((ULONG_PTR)StartingVa >
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,Alignment)) {

                        *Base = (PVOID)MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,
                                                           Alignment);
                        return STATUS_SUCCESS;
                    }
                }
            }
        }
    }

    Status = MiFindEmptyAddressRangeInTree (
                   SizeOfRange,
                   Alignment,
                   &CurrentProcess->VadRoot,
                   (PMMADDRESS_NODE *)&CurrentProcess->VadFreeHint,
                   Base);

    return Status;
}

#if DBG

VOID
MiNodeTreeWalk (
    IN PMM_AVL_TABLE Table
    );

VOID
VadTreeWalk (
    VOID
    )

{
    MiNodeTreeWalk (&PsGetCurrentProcess ()->VadRoot);

    return;
}
#endif

LOGICAL
MiCheckForConflictingVadExistence (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    The function determines if any addresses between a given starting and
    ending address is contained within a virtual address descriptor.

Arguments:

    StartingAddress - Supplies the virtual address to locate a containing
                      descriptor.

    EndingAddress - Supplies the virtual address to locate a containing
                      descriptor.

Return Value:

    TRUE if the VAD if found, FALSE if not.

Environment:

    Kernel mode, process address creation mutex held.

--*/

{
    if (MiCheckForConflictingVad (Process, StartingAddress, EndingAddress) != NULL) {
        return TRUE;
    }

    return FALSE;
}

PFILE_OBJECT *
MmPerfVadTreeWalk (
    IN PEPROCESS TargetProcess
    )

/*++

Routine Description:

    This routine walks through the VAD tree to find all files mapped
    into the specified process.  It returns a pointer to a pool allocation 
    containing the referenced file object pointers.

Arguments:

    TargetProcess - Supplies the process to walk.  Note this is usually NOT
                    the same as the current process.

Return Value:

    Returns a pointer to a NULL terminated pool allocation containing 
    the file object pointers which have been referenced in the process, 
    NULL if the memory could not be allocated.

    It is also the responsibility of the caller to dereference each
    file object in the list and then free the returned pool.

Environment:

    PASSIVE_LEVEL, arbitrary thread context.

--*/

{
    PMMVAD Vad;
    ULONG VadCount;
    PFILE_OBJECT *File;
    PFILE_OBJECT *FileObjects;
    PMM_AVL_TABLE Table;
    PVOID RestartKey;
    PMMADDRESS_NODE NewNode;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    
    Table = &TargetProcess->VadRoot;
    RestartKey = NULL;

    LOCK_ADDRESS_SPACE (TargetProcess);

    if (Table->NumberGenericTableElements == 0) {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        return NULL;
    }

    //
    // Allocate one additional entry for the NULL terminator.
    //

    VadCount = (ULONG)(Table->NumberGenericTableElements + 1);

    FileObjects = (PFILE_OBJECT *) ExAllocatePoolWithTag (
                                            PagedPool,
                                            VadCount * sizeof(PFILE_OBJECT),
                                            '01pM');

    if (FileObjects == NULL) {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        return NULL;
    }

    File = FileObjects;

    do {

        NewNode = MiEnumerateGenericTableWithoutSplayingAvl (Table,
                                                             &RestartKey);

        if (NewNode == NULL) {
            break;
        }

        Vad = (PMMVAD) NewNode;

        if ((!Vad->u.VadFlags.PrivateMemory) &&
            (Vad->ControlArea != NULL) &&
            (Vad->ControlArea->FilePointer != NULL)) {

            *File = Vad->ControlArea->FilePointer;
            ObReferenceObject (*File);
            File += 1;
        }

    } while (TRUE);

    ASSERT (File < FileObjects + VadCount);

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    *File = NULL;

    return FileObjects;
}


BOOLEAN
MmCheckForSafeExecution (
    IN PVOID InstructionPointer,
    IN PVOID StackPointer,
    IN PVOID BranchTarget,
    IN BOOLEAN PermitStackExecution
    )

/*++

Routine Description:

    This routine compares two virtual addresses (Va1 and Va2) to determine whether they
    fall within the same VAD in the current process or the target address (Va3) falls
    inside an image file.

Arguments:

    InstructionPointer - Supplies the address of the "thunk" to execute

    StackPointer - Supplies the stack pointer at the time of thunk execution.
    
    BranchTarget - Supplies the calculated target address of the thunk.

    PermitStackExecution - Indicates whether the thunk may reside on the
        stack.

Return Value:

    Returns TRUE if execution of the thunk is permitted, FALSE otherwise.

Environment:

    PASSIVE_LEVEL, arbitrary thread context.  Address space lock not taken.

--*/

{
    PEPROCESS CurrentProcess;
    BOOLEAN RetValue;
    PMMVAD InstructionVad;
    PMMVAD StackVad;
    PMMVAD TargetVad;

    UNREFERENCED_PARAMETER (InstructionPointer);
    UNREFERENCED_PARAMETER (StackPointer);

    CurrentProcess = PsGetCurrentProcess ();
    RetValue = TRUE;

    LOCK_ADDRESS_SPACE (CurrentProcess);

    if (PermitStackExecution == FALSE) {

        //
        // Ensure that the instruction pointer does not refer to the
        // same VAD as the stack pointer.
        //
        // And that the instruction pointer does not reside in an
        // image section.
        //

        InstructionVad = MiLocateAddress (InstructionPointer);
        StackVad = MiLocateAddress (StackPointer);

        if ((InstructionVad == NULL) ||
            (StackVad == NULL) ||
            (InstructionVad == StackVad) ||
            (InstructionVad->u.VadFlags.VadType == VadImageMap)) {

            RetValue = FALSE;
        }
    }

    if (RetValue != FALSE) {

        //
        // Ensure that the branch target is backed by an image section.
        //
            
        TargetVad = MiLocateAddress (BranchTarget);
        if (TargetVad == NULL ||
            TargetVad->u.VadFlags.VadType != VadImageMap) {

            RetValue = FALSE;
        }
    }

    UNLOCK_ADDRESS_SPACE (CurrentProcess);

    return RetValue;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\wrtwatch.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   wrtwatch.c

Abstract:

    This module contains the routines to support write watch.

--*/

#include "mi.h"

#define COPY_STACK_SIZE 256


NTSTATUS
NtGetWriteWatch (
    __in HANDLE ProcessHandle,
    __in ULONG Flags,
    __in PVOID BaseAddress,
    __in SIZE_T RegionSize,
    __out_ecount(*EntriesInUserAddressArray) PVOID *UserAddressArray,
    __inout PULONG_PTR EntriesInUserAddressArray,
    __out PULONG Granularity
    )

/*++

Routine Description:

    This function returns the write watch status of the argument region.
    UserAddressArray is filled with the base address of each page that has
    been written to since the last NtResetWriteWatch call (or if no
    NtResetWriteWatch calls have been made, then each page written since
    this address space was created).

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    Flags - Supplies WRITE_WATCH_FLAG_RESET or nothing.

    BaseAddress - An address within a region of pages to be queried. This
                  value must lie within a private memory region with the
                  write-watch attribute already set.

    RegionSize - The size of the region in bytes beginning at the base address
                 specified.

    UserAddressArray - Supplies a pointer to user memory to store the user
                       addresses modified since the last reset.

    UserAddressArrayEntries - Supplies a pointer to how many user addresses
                              can be returned in this call.  This is then filled
                              with the exact number of addresses actually
                              returned.

    Granularity - Supplies a pointer to a variable to receive the size of
                  modified granule in bytes.
        
Return Value:

    Various NTSTATUS codes.

--*/

{
    PMMPFN Pfn1;
    LOGICAL First;
    LOGICAL UserWritten;
    PVOID EndAddress;
    PMMVAD Vad;
    KIRQL OldIrql;
    PEPROCESS Process;
    PMMPTE NextPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE EndPte;
    NTSTATUS Status;
    PVOID PoolArea;
    PVOID *PoolAreaPointer;
    ULONG_PTR StackArray[COPY_STACK_SIZE];
    MMPTE PteContents;
    ULONG_PTR NumberOfBytes;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    ULONG NextBitMapIndex;
    PMI_PHYSICAL_VIEW PhysicalView;
    ULONG_PTR PagesWritten;
    ULONG_PTR NumberOfPages;
    LOGICAL Attached;
    KPROCESSOR_MODE PreviousMode;
    PFN_NUMBER PageFrameIndex;
    ULONG WorkingSetIndex;
    MMPTE TempPte;
    MMPTE PreviousPte;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;
    MMPTE_FLUSH_LIST PteFlushList;
    TABLE_SEARCH_RESULT SearchResult;

    PteFlushList.Count = 0;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if ((Flags & ~WRITE_WATCH_FLAG_RESET) != 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread (&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        if (PreviousMode != KernelMode) {

            //
            // Make sure the specified starting and ending addresses are
            // within the user part of the virtual address space.
            //
        
            if (BaseAddress > MM_HIGHEST_VAD_ADDRESS) {
                return STATUS_INVALID_PARAMETER_2;
            }
        
            if ((((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) - (ULONG_PTR)BaseAddress) <
                    RegionSize) {
                return STATUS_INVALID_PARAMETER_3;
            }

            //
            // Capture the number of pages.
            //

            ProbeForWriteUlong_ptr (EntriesInUserAddressArray);

            NumberOfPages = *EntriesInUserAddressArray;

            if (NumberOfPages == 0) {
                return STATUS_INVALID_PARAMETER_5;
            }

            if (NumberOfPages > (MAXULONG_PTR / sizeof(ULONG_PTR))) {
                return STATUS_INVALID_PARAMETER_5;
            }

            ProbeForWrite (UserAddressArray,
                           NumberOfPages * sizeof (PVOID),
                           sizeof(PVOID));

            ProbeForWriteUlong (Granularity);
        }
        else {
            NumberOfPages = *EntriesInUserAddressArray;
            ASSERT (NumberOfPages != 0);
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Carefully probe and capture the user virtual address array.
    //

    PoolArea = (PVOID)&StackArray[0];

    NumberOfBytes = NumberOfPages * sizeof(ULONG_PTR);

    if (NumberOfPages > COPY_STACK_SIZE) {
        PoolArea = ExAllocatePoolWithTag (NonPagedPool,
                                                 NumberOfBytes,
                                                 'cGmM');

        if (PoolArea == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    PoolAreaPointer = (PVOID *)PoolArea;

    Attached = FALSE;

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        Status = ObReferenceObjectByHandle (ProcessHandle,
                                            PROCESS_VM_OPERATION,
                                            PsProcessType,
                                            PreviousMode,
                                            (PVOID *)&Process,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            goto ErrorReturn0;
        }
    }

    EndAddress = (PVOID)((PCHAR)BaseAddress + RegionSize - 1);

    PagesWritten = 0;

    if (BaseAddress > EndAddress) {
        Status = STATUS_INVALID_PARAMETER_4;
        goto ErrorReturn;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Vad = NULL;

    SATISFY_OVERZEALOUS_COMPILER (PhysicalView = NULL);

    First = TRUE;

    PointerPte = MiGetPteAddress (BaseAddress);
    EndPte = MiGetPteAddress (EndAddress);

    PointerPde = MiGetPdeAddress (BaseAddress);
    PointerPpe = MiGetPpeAddress (BaseAddress);
    PointerPxe = MiGetPxeAddress (BaseAddress);

    LOCK_WS (CurrentThread, Process);

    if (Process->PhysicalVadRoot == NULL) {
        UNLOCK_WS (CurrentThread, Process);
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    //
    // Lookup the element and save the result.
    //

    SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                       MI_VA_TO_VPN (BaseAddress),
                                       (PMMADDRESS_NODE *) &PhysicalView);

    if ((SearchResult == TableFoundNode) &&
        (PhysicalView->Vad->u.VadFlags.VadType == VadWriteWatch) &&
        (BaseAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
        (EndAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

        Vad = PhysicalView->Vad;
    }
    else {

        //
        // No virtual address is marked for write-watch at the specified base
        // address, return an error.
        //

        UNLOCK_WS (CurrentThread, Process);
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // Extract the write watch status for each page in the range.
    // Note the PFN lock must be held to ensure atomicity.
    //

    BitMap = PhysicalView->u.BitMap;

    BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    BitMapIndex = (ULONG)(((PCHAR)BaseAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);

    ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
    ASSERT (BitMapIndex + (EndPte - PointerPte) < BitMap->SizeOfBitMap);

    LOCK_PFN (OldIrql);

    while (PointerPte <= EndPte) {

        ASSERT (BitMapIndex < BitMap->SizeOfBitMap);

        UserWritten = FALSE;

        //
        // If the PTE is marked dirty (or writable) OR the BitMap says it's
        // dirtied, then let the caller know.
        //

        if (RtlCheckBit (BitMap, BitMapIndex) == 1) {
            UserWritten = TRUE;

            //
            // Note that a chunk of bits cannot be cleared at once because
            // the user array may overflow at any time.  If the user specifies
            // a bad address and the results cannot be written out, then it's
            // his own fault that he won't know which bits were cleared !
            //

            if (Flags & WRITE_WATCH_FLAG_RESET) {
                RtlClearBit (BitMap, BitMapIndex);
                goto ClearPteIfValid;
            }
        }
        else {

ClearPteIfValid:

            //
            // If the page table page is not present, then the dirty bit
            // has already been captured to the write watch bitmap.
            // Unfortunately all the entries in the page cannot be skipped
            // as the write watch bitmap must be checked for each PTE.
            //
    
#if (_MI_PAGING_LEVELS >= 4)
            if (PointerPxe->u.Hard.Valid == 0) {

                //
                // Skip the entire extended page parent if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif
#if (_MI_PAGING_LEVELS >= 3)
            if (PointerPpe->u.Hard.Valid == 0) {

                //
                // Skip the entire page parent if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif
            if (PointerPde->u.Hard.Valid == 0) {

                //
                // Skip the entire page directory if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPde += 1;
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }

            PteContents = *PointerPte;

            if ((PteContents.u.Hard.Valid == 1) &&
                (MI_IS_PTE_DIRTY(PteContents))) {

                ASSERT (MI_PFN_ELEMENT(MI_GET_PAGE_FRAME_FROM_PTE(&PteContents))->u3.e1.PrototypePte == 0);

                UserWritten = TRUE;
                if (Flags & WRITE_WATCH_FLAG_RESET) {

                    //
                    // For the uniprocessor x86, just the dirty bit is
                    // cleared.  For all other platforms, the PTE writable
                    // bit must be disabled now so future writes trigger
                    // write watch updates.
                    //
        
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
                    ASSERT (Pfn1->u3.e1.PrototypePte == 0);
        
                    MI_MAKE_VALID_USER_PTE (TempPte,
                                            PageFrameIndex,
                                            Pfn1->OriginalPte.u.Soft.Protection,
                                            PointerPte);
        
                    WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);
                    MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);
        
                    //
                    // Flush the TB as the protection of a valid PTE is
                    // being changed.
                    //
        
                    PreviousPte = *PointerPte;

                    ASSERT (PreviousPte.u.Hard.Valid == 1);

                    MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

                    if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                        PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
                        PteFlushList.Count += 1;
                    }
                
                    ASSERT (PreviousPte.u.Hard.Valid == 1);
                
                    //
                    // A page's protection is being changed, on certain
                    // hardware the dirty bit should be ORed into the
                    // modify bit in the PFN element.
                    //
                    
                    MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);
                }
            }
        }

        if (UserWritten == TRUE) {
            *PoolAreaPointer = BaseAddress;
            PoolAreaPointer += 1;
            PagesWritten += 1;
            if (PagesWritten == NumberOfPages) {

                //
                // User array isn't big enough to take any more.  The API
                // (inherited from Win9x) is defined to return at this point.
                //

                break;
            }
        }

        PointerPte += 1;
        if (MiIsPteOnPdeBoundary(PointerPte)) {
            PointerPde = MiGetPteAddress (PointerPte);
            if (MiIsPteOnPdeBoundary(PointerPde)) {
                PointerPpe = MiGetPdeAddress (PointerPte);
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary(PointerPpe)) {
                    PointerPxe = MiGetPpeAddress (PointerPte);
                }
#endif
            }
        }
        BitMapIndex += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
    }

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

    UNLOCK_PFN (OldIrql);

    UNLOCK_WS (CurrentThread, Process);

    Status = STATUS_SUCCESS;

ErrorReturn:

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    if (Status == STATUS_SUCCESS) {

        //
        // Return all results to the caller.
        //
    
        try {
    
            RtlCopyMemory (UserAddressArray,
                           PoolArea,
                           PagesWritten * sizeof (PVOID));

            *EntriesInUserAddressArray = PagesWritten;

            *Granularity = PAGE_SIZE;
    
        } except (ExSystemExceptionFilter()) {
    
            Status = GetExceptionCode();
        }
    }
    
ErrorReturn0:

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return Status;
}

NTSTATUS
NtResetWriteWatch (
    __in HANDLE ProcessHandle,
    __in PVOID BaseAddress,
    __in SIZE_T RegionSize
    )

/*++

Routine Description:

    This function clears the write watch status of the argument region.
    This allows callers to "forget" old writes and only see new ones from
    this point on.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - An address within a region of pages to be reset.  This
                  value must lie within a private memory region with the
                  write-watch attribute already set.

    RegionSize - The size of the region in bytes beginning at the base address
                 specified.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PVOID EndAddress;
    PMMVAD Vad;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE EndPte;
    NTSTATUS Status;
    MMPTE PreviousPte;
    MMPTE PteContents;
    MMPTE TempPte;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    PMI_PHYSICAL_VIEW PhysicalView;
    LOGICAL First;
    LOGICAL Attached;
    KPROCESSOR_MODE PreviousMode;
    PFN_NUMBER PageFrameIndex;
    ULONG WorkingSetIndex;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;
    MMPTE_FLUSH_LIST PteFlushList;
    TABLE_SEARCH_RESULT SearchResult;

    PteFlushList.Count = 0;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (BaseAddress > MM_HIGHEST_VAD_ADDRESS) {
        return STATUS_INVALID_PARAMETER_2;
    }

    if ((((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) - (ULONG_PTR)BaseAddress) <
            RegionSize) {
        return STATUS_INVALID_PARAMETER_3;
    }

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread(CurrentThread);

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

        Status = ObReferenceObjectByHandle (ProcessHandle,
                                            PROCESS_VM_OPERATION,
                                            PsProcessType,
                                            PreviousMode,
                                            (PVOID *)&Process,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    Attached = FALSE;

    EndAddress = (PVOID)((PCHAR)BaseAddress + RegionSize - 1);
    
    if (BaseAddress > EndAddress) {
        Status = STATUS_INVALID_PARAMETER_3;
        goto ErrorReturn;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Vad = NULL;
    First = TRUE;

    SATISFY_OVERZEALOUS_COMPILER (PhysicalView = NULL);

    PointerPte = MiGetPteAddress (BaseAddress);
    EndPte = MiGetPteAddress (EndAddress);

    LOCK_WS (CurrentThread, Process);

    if (Process->PhysicalVadRoot == NULL) {
        UNLOCK_WS (CurrentThread, Process);
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    //
    // Lookup the element and save the result.
    //

    SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                       MI_VA_TO_VPN (BaseAddress),
                                       (PMMADDRESS_NODE *) &PhysicalView);

    if ((SearchResult == TableFoundNode) &&
        (PhysicalView->Vad->u.VadFlags.VadType == VadWriteWatch) &&
        (BaseAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
        (EndAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

        Vad = PhysicalView->Vad;
    }
    else {

        //
        // No virtual address is marked for write-watch at the specified base
        // address, return an error.
        //

        UNLOCK_WS (CurrentThread, Process);
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // Clear the write watch status (and PTE writable/dirty bits) for each page
    // in the range.  Note if the PTE is not currently valid, then the write
    // watch bit has already been captured to the bitmap.  Hence only valid PTEs
    // need adjusting.
    //
    // The PFN lock must be held to ensure atomicity.
    //

    BitMap = PhysicalView->u.BitMap;

    BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    BitMapIndex = (ULONG)(((PCHAR)BaseAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);

    ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
    ASSERT (BitMapIndex + (EndPte - PointerPte) < BitMap->SizeOfBitMap);

    RtlClearBits (BitMap, BitMapIndex, (ULONG)(EndPte - PointerPte + 1));

    LOCK_PFN (OldIrql);

    while (PointerPte <= EndPte) {

        //
        // If the page table page is not present, then the dirty bit
        // has already been captured to the write watch bitmap.  So skip it.
        //

        if ((First == TRUE) || MiIsPteOnPdeBoundary(PointerPte)) {
            First = FALSE;

            PointerPpe = MiGetPpeAddress (BaseAddress);
            PointerPxe = MiGetPxeAddress (BaseAddress);

#if (_MI_PAGING_LEVELS >= 4)
            if (PointerPxe->u.Hard.Valid == 0) {
                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif

#if (_MI_PAGING_LEVELS >= 3)
            if (PointerPpe->u.Hard.Valid == 0) {
                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif

            PointerPde = MiGetPdeAddress (BaseAddress);

            if (PointerPde->u.Hard.Valid == 0) {
                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
        }

        //
        // If the PTE is marked dirty (or writable) OR the BitMap says it's
        // dirtied, then let the caller know.
        //

        PteContents = *PointerPte;

        if ((PteContents.u.Hard.Valid == 1) &&
            (MI_IS_PTE_DIRTY(PteContents))) {

            //
            // For the uniprocessor x86, just the dirty bit is cleared.
            // For all other platforms, the PTE writable bit must be
            // disabled now so future writes trigger write watch updates.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 0);

            MI_MAKE_VALID_USER_PTE (TempPte,
                                    PageFrameIndex,
                                    Pfn1->OriginalPte.u.Soft.Protection,
                                    PointerPte);

            WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);
            MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);

            //
            // Flush the TB as the protection of a valid PTE is being changed.
            //

            PreviousPte = *PointerPte;

            ASSERT (PreviousPte.u.Hard.Valid == 1);

            MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

            if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
                PteFlushList.Count += 1;
            }

            ASSERT (PreviousPte.u.Hard.Valid == 1);
        
            //
            // A page's protection is being changed, on certain
            // hardware the dirty bit should be ORed into the
            // modify bit in the PFN element.
            //
        
            MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);
        }

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
    }

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList);
    }

    UNLOCK_PFN (OldIrql);

    UNLOCK_WS (CurrentThread, Process);

    Status = STATUS_SUCCESS;

ErrorReturn:

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    return Status;
}

VOID
MiCaptureWriteWatchDirtyBit (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine sets the write watch bit corresponding to the argument
    virtual address.

Arguments:

    Process - Supplies a pointer to an executive process structure.

    VirtualAddress - Supplies the modified virtual address.

Return Value:

    None.

Environment:

    Kernel mode, working set mutex and PFN lock held.

--*/

{
    PMMVAD Vad;
    PMI_PHYSICAL_VIEW PhysicalView;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    TABLE_SEARCH_RESULT SearchResult;

    MM_PFN_LOCK_ASSERT();

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // This process has (or had) write watch VADs.  Search now
    // for a write watch region encapsulating the PTE being
    // invalidated.
    //

    ASSERT (Process->PhysicalVadRoot != NULL);

    SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                       MI_VA_TO_VPN (VirtualAddress),
                                       (PMMADDRESS_NODE *) &PhysicalView);

    if ((SearchResult == TableFoundNode) &&
        (PhysicalView->Vad->u.VadFlags.VadType == VadWriteWatch) &&
        (VirtualAddress >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
        (VirtualAddress <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {

        //
        // The write watch bitmap must be updated.
        //

        Vad = PhysicalView->Vad;
        BitMap = PhysicalView->u.BitMap;

        BitMapIndex = (ULONG)(((PCHAR)VirtualAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);
    
        ASSERT (BitMapIndex < BitMap->SizeOfBitMap);

        MI_SET_BIT (BitMap->Buffer, BitMapIndex);
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\wrtfault.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   wrtfault.c

Abstract:

    This module contains the copy on write routine for memory management.

--*/

#include "mi.h"


LOGICAL
FASTCALL
MiCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This routine performs a copy on write operation for the specified
    virtual address.

Arguments:

    FaultingAddress - Supplies the virtual address which caused the fault.

    PointerPte - Supplies the pointer to the PTE which caused the page fault.

Return Value:

    Returns TRUE if the page was actually split, FALSE if not.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    MMPTE TempPte;
    MMPTE TempPte2;
    PMMPTE MappingPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NewPageIndex;
    PVOID CopyTo;
    PVOID CopyFrom;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PEPROCESS CurrentProcess;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    WSLE_NUMBER WorkingSetIndex;
    LOGICAL FakeCopyOnWrite;
    PMMWSL WorkingSetList;
    PVOID SessionSpace;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;

    //
    // This is called from MmAccessFault, the PointerPte is valid
    // and the working set mutex ensures it cannot change state.
    //
    // Capture the PTE contents to TempPte.
    //

    TempPte = *PointerPte;
    ASSERT (TempPte.u.Hard.Valid == 1);

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Check to see if this is a prototype PTE with copy on write enabled.
    //

    FakeCopyOnWrite = FALSE;
    CurrentProcess = PsGetCurrentProcess ();
    CloneBlock = NULL;

    if (FaultingAddress >= (PVOID) MmSessionBase) {

        WorkingSetList = MmSessionSpace->Vm.VmWorkingSetList;
        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
        SessionSpace = (PVOID) MmSessionSpace;

        MM_SESSION_SPACE_WS_LOCK_ASSERT ();

        if (MmSessionSpace->ImageLoadingCount != 0) {

            NextEntry = MmSessionSpace->ImageList.Flink;
    
            while (NextEntry != &MmSessionSpace->ImageList) {
    
                Image = CONTAINING_RECORD (NextEntry, IMAGE_ENTRY_IN_SESSION, Link);
    
                if ((FaultingAddress >= Image->Address) &&
                    (FaultingAddress <= Image->LastAddress)) {
    
                    if (Image->ImageLoading) {
    
                        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
    
                        TempPte.u.Hard.CopyOnWrite = 0;
                        TempPte.u.Hard.Write = 1;
    
                        //
                        // The page is no longer copy on write, update the PTE
                        // setting both the dirty bit and the accessed bit.
                        //
                        // Even though the page's current backing is the image
                        // file, the modified writer will convert it to
                        // pagefile backing when it notices the change later.
                        //
    
                        MI_SET_PTE_DIRTY (TempPte);
                        MI_SET_ACCESSED_IN_PTE (&TempPte, 1);
    
                        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);
    
                        //
                        // The TB entry must be flushed as the valid PTE with
                        // the dirty bit clear has been fetched into the TB. If
                        // it isn't flushed, another fault is generated as the
                        // dirty bit is not set in the cached TB entry.
                        //
    
                        MI_FLUSH_SINGLE_TB (FaultingAddress, TRUE);
    
                        return FALSE;
                    }
                    break;
                }
    
                NextEntry = NextEntry->Flink;
            }
        }
    }
    else {
        WorkingSetList = MmWorkingSetList;
        SessionSpace = NULL;

        //
        // If a fork operation is in progress, block until the fork is
        // completed, then retry the whole operation as the state of
        // everything may have changed between when the mutexes were
        // released and reacquired.
        //

        if (CurrentProcess->ForkInProgress != NULL) {
            if (MiWaitForForkToComplete (CurrentProcess) == TRUE) {
                return FALSE;
            }
        }

        if (TempPte.u.Hard.CopyOnWrite == 0) {

            //
            // This is a fork page which is being made private in order
            // to change the protection of the page.
            // Do not make the page writable.
            //

            FakeCopyOnWrite = TRUE;
        }
    }

    WorkingSetIndex = MiLocateWsle (FaultingAddress,
                                    WorkingSetList,
                                    Pfn1->u1.WsIndex,
                                    FALSE);

    //
    // The page must be copied into a new page.
    //

    LOCK_PFN (OldIrql);

    if ((MmAvailablePages < MM_HIGH_LIMIT) &&
        (MiEnsureAvailablePageOrWait (SessionSpace != NULL ? HYDRA_PROCESS : CurrentProcess, OldIrql))) {

        //
        // A wait operation was performed to obtain an available
        // page and the working set mutex and PFN lock have
        // been released and various things may have changed for
        // the worse.  Rather than examine all the conditions again,
        // return and if things are still proper, the fault will
        // be taken again.
        //

        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

    //
    // This must be a prototype PTE.  Perform the copy on write.
    //

    ASSERT (Pfn1->u3.e1.PrototypePte == 1);

    //
    // A page is being copied and made private, the global state of
    // the shared page needs to be updated at this point on certain
    // hardware.  This is done by ORing the dirty bit into the modify bit in
    // the PFN element.
    //
    // Note that a session page cannot be dirty (no POSIX-style forking is
    // supported for these drivers).
    //

    if (SessionSpace != NULL) {
        ASSERT ((TempPte.u.Hard.Valid == 1) && (TempPte.u.Hard.Write == 0));
        ASSERT (!MI_IS_PTE_DIRTY (TempPte));

        NewPageIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_SESSION(MmSessionSpace));
    }
    else {
        MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);
        CloneBlock = (PMMCLONE_BLOCK) Pfn1->PteAddress;

        //
        // Get a new page with the same color as this page.
        //

        NewPageIndex = MiRemoveAnyPage (
                        MI_PAGE_COLOR_PTE_PROCESS(PageFrameIndex,
                                              &CurrentProcess->NextPageColor));
    }

    MiInitializeCopyOnWritePfn (NewPageIndex,
                                PointerPte,
                                WorkingSetIndex,
                                WorkingSetList);

    UNLOCK_PFN (OldIrql);

    InterlockedIncrement (&KeGetCurrentPrcb ()->MmCopyOnWriteCount);

    CopyFrom = PAGE_ALIGN (FaultingAddress);

    MappingPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MappingPte != NULL) {

        MI_MAKE_VALID_KERNEL_PTE (TempPte2,
                                  NewPageIndex,
                                  MM_READWRITE,
                                  MappingPte);

        MI_SET_PTE_DIRTY (TempPte2);

        if (Pfn1->u3.e1.CacheAttribute == MiNonCached) {
            MI_DISABLE_CACHING (TempPte2);
        }
        else if (Pfn1->u3.e1.CacheAttribute == MiWriteCombined) {
            MI_SET_PTE_WRITE_COMBINE (TempPte2);
        }

        MI_WRITE_VALID_PTE (MappingPte, TempPte2);

        CopyTo = MiGetVirtualAddressMappedByPte (MappingPte);
    }
    else {

        CopyTo = MiMapPageInHyperSpace (CurrentProcess,
                                        NewPageIndex,
                                        &OldIrql);
    }

    KeCopyPage (CopyTo, CopyFrom);

    if (MappingPte != NULL) {
        MiReleaseSystemPtes (MappingPte, 1, SystemPteSpace);
    }
    else {
        MiUnmapPageInHyperSpace (CurrentProcess, CopyTo, OldIrql);
    }

    if (!FakeCopyOnWrite) {

        //
        // If the page was really a copy on write page, make it
        // accessed, dirty and writable.  Also, clear the copy-on-write
        // bit in the PTE.
        //

        MI_SET_PTE_DIRTY (TempPte);
        TempPte.u.Hard.Write = 1;
        MI_SET_ACCESSED_IN_PTE (&TempPte, 1);
        TempPte.u.Hard.CopyOnWrite = 0;
    }

    //
    // Regardless of whether the page was really a copy on write,
    // the frame field of the PTE must be updated.
    //

    TempPte.u.Hard.PageFrameNumber = NewPageIndex;

    //
    // If the modify bit is set in the PFN database for the
    // page, the data cache must be flushed.  This is due to the
    // fact that this process may have been cloned and the cache
    // still contains stale data destined for the page we are
    // going to remove.
    //

    ASSERT (TempPte.u.Hard.Valid == 1);

    MI_WRITE_VALID_PTE_NEW_PAGE (PointerPte, TempPte);

    //
    // Flush the TB entry for this page.
    //

    if (SessionSpace == NULL) {

        MI_FLUSH_SINGLE_TB (FaultingAddress, FALSE);

        //
        // Increment the number of private pages.
        //

        CurrentProcess->NumberOfPrivatePages += 1;
    }
    else {

        MI_FLUSH_SINGLE_TB (FaultingAddress, TRUE);

        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
    }

    //
    // Decrement the share count for the page which was copied
    // as this PTE no longer refers to it.
    //

    LOCK_PFN (OldIrql);

    MiDecrementShareCount (Pfn1, PageFrameIndex);

    if (SessionSpace == NULL) {

        CloneDescriptor = MiLocateCloneAddress (CurrentProcess,
                                                (PVOID)CloneBlock);

        if (CloneDescriptor != NULL) {

            //
            // Decrement the reference count for the clone block,
            // note that this could release and reacquire the mutexes.
            //

            MiDecrementCloneBlockReference (CloneDescriptor,
                                            CloneBlock,
                                            CurrentProcess,
                                            NULL,
                                            OldIrql);
        }
    }

    UNLOCK_PFN (OldIrql);
    return TRUE;
}


#if !defined(NT_UP)

LOGICAL
MiSetDirtyBit (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN ULONG PfnHeld
    )

/*++

Routine Description:

    This routine sets dirty in the specified PTE and the modify bit in the
    corresponding PFN element.  If any page file space is allocated, it
    is deallocated.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies a pointer to the corresponding valid PTE.

    PfnHeld - Supplies TRUE if the PFN lock is already held.

Return Value:

    TRUE if action was taken, FALSE if not.

Environment:

    Kernel mode, APCs disabled, working set pushlock held.

--*/

{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;

    //
    // The page is NOT copy on write, update the PTE setting both the
    // dirty bit and the accessed bit. Note, that as this PTE is in
    // the TB, the TB must be flushed.
    //

    TempPte = *PointerPte;

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);

    //
    // This may be a PTE from a rotate physical frame so there may be no
    // corresponding PFN for it.
    //

    if (!MI_IS_PFN (PageFrameIndex)) {
        return FALSE;
    }

    MI_SET_PTE_DIRTY (TempPte);
    MI_SET_ACCESSED_IN_PTE (&TempPte, 1);

    MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

    //
    // Check state of PFN lock and if not held, don't update PFN database.
    //

    if (PfnHeld) {

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Set the modified field in the PFN database, also, if the physical
        // page is currently in a paging file, free up the page file space
        // as the contents are now worthless.
        //

        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
            (Pfn1->u3.e1.WriteInProgress == 0)) {

            //
            // This page is in page file format, deallocate the page file space.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);

            //
            // Change original PTE to indicate no page file space is reserved,
            // otherwise the space will be deallocated when the PTE is
            // deleted.
            //

            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        MI_SET_MODIFIED (Pfn1, 1, 0x17);
    }

    //
    // The TB entry must be flushed as the valid PTE with the dirty bit clear
    // has been fetched into the TB.  If it isn't flushed, another fault
    // is generated as the dirty bit is not set in the cached TB entry.
    //

    KeFillEntryTb (FaultingAddress);
    return TRUE;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\umapview.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   umapview.c

Abstract:

    This module contains the routines which implement the
    NtUnmapViewOfSection service.

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtUnmapViewOfSection)
#pragma alloc_text(PAGE,MmUnmapViewOfSection)
#endif


NTSTATUS
NtUnmapViewOfSection(
    __in HANDLE ProcessHandle,
    __in PVOID BaseAddress
    )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies the base address of the view.

Return Value:

    NTSTATUS.

--*/

{
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;

    PAGED_CODE ();

    PreviousMode = KeGetPreviousMode ();

    if ((PreviousMode == UserMode) && (BaseAddress > MM_HIGHEST_USER_ADDRESS)) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    Status = ObReferenceObjectByHandle (ProcessHandle,
                                        PROCESS_VM_OPERATION,
                                        PsProcessType,
                                        PreviousMode,
                                        (PVOID *)&Process,
                                        NULL);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    Status = MiUnmapViewOfSection (Process, BaseAddress, 0);
    ObDereferenceObject (Process);

    return Status;
}

NTSTATUS
MiUnmapViewOfSection (
    IN PEPROCESS Process,
    IN PVOID BaseAddress,
    IN ULONG Flags
    )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    Process - Supplies a referenced pointer to a process object.

    BaseAddress - Supplies the base address of the view.

    Flags - Supplies a bitfield directive.

Return Value:

    NTSTATUS.

--*/

{
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PETHREAD Thread;
    PEPROCESS CurrentProcess;
    SIZE_T RegionSize;
    PVOID UnMapImageBase;
    PVOID StartingVa;
    PVOID EndingVa;
    NTSTATUS status;
    LOGICAL Attached;
    KAPC_STATE ApcState;

    PAGED_CODE();

    Attached = FALSE;
    UnMapImageBase = NULL;

    Thread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (Thread);

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be removed.  Raise IRQL to block APCs.
    //

    if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
        LOCK_ADDRESS_SPACE (Process);
    }

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    //
    // Find the associated vad.
    //

    Vad = MiLocateAddress (BaseAddress);

    if ((Vad == NULL) || (Vad->u.VadFlags.PrivateMemory)) {

        //
        // No Virtual Address Descriptor located for Base Address.
        //

        if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        status = STATUS_NOT_MAPPED_VIEW;
        goto ErrorReturn;
    }

    StartingVa = MI_VPN_TO_VA (Vad->StartingVpn);
    EndingVa = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

    //
    // If this Vad is for an image section, then
    // get the base address of the section.
    //

    ASSERT (Process == PsGetCurrentProcess ());

    if (Vad->u.VadFlags.VadType == VadImageMap) {
        UnMapImageBase = StartingVa;
    }

    RegionSize = PAGE_SIZE + ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT);

    if (Vad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is being made to delete a secured VAD, check
        // the whole VAD to see if this deletion is allowed.
        //

        status = MiCheckSecuredVad (Vad,
                                    StartingVa,
                                    RegionSize,
                                    MM_SECURE_DELETE_CHECK);

        if (!NT_SUCCESS (status)) {
            if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
                UNLOCK_ADDRESS_SPACE (Process);
            }
            goto ErrorReturn;
        }
    }

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);

    if (Vad->u.VadFlags.VadType == VadRotatePhysical) {

        if ((Flags & UNMAP_ROTATE_PHYSICAL_OK) == 0) {

            //
            // This caller is treating the VA space via the section APIs,
            // but he's not supposed to know that internally we converted
            // the private memory request into a section.  So fail this.
            //

            if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
                UNLOCK_ADDRESS_SPACE (Process);
            }
            status = STATUS_NOT_MAPPED_VIEW;
            goto ErrorReturn;
        }

        MiRemoveVadCharges (Vad, Process);

        LOCK_WS_UNSAFE (Thread, Process);

        MiPhysicalViewRemover (Process, Vad);
    }
    else {
        MiRemoveVadCharges (Vad, Process);

        LOCK_WS_UNSAFE (Thread, Process);
    }

    MiRemoveVad (Vad, Process);

    //
    // NOTE: MiRemoveMappedView releases the working set pushlock
    // before returning.
    //

    MiRemoveMappedView (Process, Vad);

    //
    // Return commitment for page table pages if possible.
    //

    MiReturnPageTablePageCommitment (StartingVa,
                                     EndingVa,
                                     Process,
                                     PreviousVad,
                                     NextVad);

    //
    // Update the current virtual size in the process header.
    //

    Process->VirtualSize -= RegionSize;
    if ((Flags & UNMAP_ADDRESS_SPACE_HELD) == 0) {
        UNLOCK_ADDRESS_SPACE (Process);
    }

    ExFreePool (Vad);
    status = STATUS_SUCCESS;

ErrorReturn:

    if (UnMapImageBase) {
        DbgkUnMapViewOfSection (UnMapImageBase);
    }
    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    return status;
}

NTSTATUS
MmUnmapViewOfSection (
    __in PEPROCESS Process,
    __in PVOID BaseAddress
     )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    Process - Supplies a referenced pointer to a process object.

    BaseAddress - Supplies the base address of the view.

Return Value:

    NTSTATUS.

--*/

{
    return MiUnmapViewOfSection (Process, BaseAddress, 0);
}

VOID
MiDecrementSubsections (
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection OPTIONAL
    )
/*++

Routine Description:

    This function decrements the subsections, inserting them on the unused
    subsection list if they qualify.

Arguments:

    FirstSubsection - Supplies the subsection to start at.

    LastSubsection - Supplies the last subsection to insert.  Supplies NULL
                     to decrement all the subsections in the chain.

Return Value:

    None.

Environment:

    PFN lock held.

--*/
{
    PMSUBSECTION MappedSubsection;

    ASSERT ((FirstSubsection->ControlArea->u.Flags.Image == 0) &&
            (FirstSubsection->ControlArea->FilePointer != NULL) &&
            (FirstSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {
        MappedSubsection = (PMSUBSECTION) FirstSubsection;

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

        ASSERT (((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if (ARGUMENT_PRESENT (LastSubsection)) {
            if (FirstSubsection == LastSubsection) {
                break;
            }
        }
        else {
            if (FirstSubsection->NextSubsection == NULL) {
                break;
            }
        }

        FirstSubsection = FirstSubsection->NextSubsection;
    } while (TRUE);
}


VOID
MiRemoveMappedView (
    IN PEPROCESS CurrentProcess,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function removes the mapping from the current process's
    address space.  The physical VAD may be a normal mapping (backed by
    a control area) or it may have no control area (it was mapped by a driver).

Arguments:

    CurrentProcess - Supplies a referenced pointer to the current process object.

    Vad - Supplies the VAD which maps the view.

Return Value:

    None.

Environment:

    APC level, working set mutex pushlock and address creation mutex held.

    NOTE:  THE WORKING SET PUSHLOCK IS RELEASED BEFORE RETURNING !!!!

           The working set pushlock must be acquired UNSAFE by the caller.

--*/

{
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PMMPTE LastPte;
    PFN_NUMBER PdePage;
    PVOID TempVa;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID UsedPageTableHandle;
    PMMPFN Pfn2;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;
    PMMPFN Pfn1;
    PMMPFN EndPfn;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPTE EndPde;
    PETHREAD CurrentThread;
#if DBG
    PMMPTE EndProto;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
    PVOID UsedPageDirectoryHandle;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PVOID UsedPageDirectoryParentHandle;
#endif

    ControlArea = Vad->ControlArea;
    CurrentThread = PsGetCurrentThread ();

    if (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory) {

        if (((PMMVAD_LONG)Vad)->u4.Banked != NULL) {
            ExFreePool (((PMMVAD_LONG)Vad)->u4.Banked);
        }

        //
        // This is a physical memory view.  The pages map physical memory
        // and are not accounted for in the working set list or in the PFN
        // database.
        //

        MiPhysicalViewRemover (CurrentProcess, Vad);

        //
        // Set count so only flush entire TB operations are performed.
        //

        PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;

        PointerPde = MiGetPdeAddress (MI_VPN_TO_VA (Vad->StartingVpn));
        PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));

        ASSERT (PointerPde->u.Hard.Valid == 1);

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {

            MiFreeLargePages (MI_VPN_TO_VA (Vad->StartingVpn),
                              MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                              TRUE);

            UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
            LOCK_PFN (OldIrql);
        }
        else {
    
            //
            // Remove the PTES from the address space.
            //
    
            PdePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
    
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MI_VPN_TO_VA (Vad->StartingVpn));
    
            LOCK_PFN (OldIrql);

            while (PointerPte <= LastPte) {
    
                if (MiIsPteOnPdeBoundary (PointerPte)) {
    
                    PointerPde = MiGetPteAddress (PointerPte);
                    PdePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
    
                    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));
                }
    
                //
                // Decrement the count of non-zero page table entries for this
                // page table.
                //
    
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
    
                MI_WRITE_ZERO_PTE (PointerPte);
    
                Pfn2 = MI_PFN_ELEMENT (PdePage);
                MiDecrementShareCountInline (Pfn2, PdePage);
    
                //
                // If all the entries have been eliminated from the previous
                // page table page, delete the page table page itself.  And if
                // this results in an empty page directory page, then delete
                // that too.
                //
    
                if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageTableHandle) == 0) {
    
                    TempVa = MiGetVirtualAddressMappedByPte(PointerPde);
    
                    PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;
    
#if (_MI_PAGING_LEVELS >= 3)
                    UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
    
                    MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif
    
                    MiDeletePte (PointerPde,
                                 TempVa,
                                 FALSE,
                                 CurrentProcess,
                                 NULL,
                                 &PteFlushList,
                                 OldIrql);
    
                    //
                    // Add back in the private page MiDeletePte subtracted.
                    //
    
                    CurrentProcess->NumberOfPrivatePages += 1;
    
#if (_MI_PAGING_LEVELS >= 3)
    
                    if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageDirectoryHandle) == 0) {
    
                        PointerPpe = MiGetPdeAddress (PointerPte);
                        TempVa = MiGetVirtualAddressMappedByPte (PointerPpe);
    
                        PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;
    
#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageDirectoryParentHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
    
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryParentHandle);
#endif
    
                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     CurrentProcess,
                                     NULL,
                                     &PteFlushList,
                                     OldIrql);
    
                        //
                        // Add back in the private page MiDeletePte subtracted.
                        //
        
                        CurrentProcess->NumberOfPrivatePages += 1;
    
#if (_MI_PAGING_LEVELS >= 4)
    
                        if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageDirectoryParentHandle) == 0) {
    
                            PointerPxe = MiGetPpeAddress(PointerPte);
                            TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
    
                            PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;
    
                            MiDeletePte (PointerPxe,
                                         TempVa,
                                         FALSE,
                                         CurrentProcess,
                                         NULL,
                                         &PteFlushList,
                                         OldIrql);
    
                            //
                            // Add back in the private page MiDeletePte subtracted.
                            //
        
                            CurrentProcess->NumberOfPrivatePages += 1;
                        }
#endif
    
                    }
#endif
                }
                PointerPte += 1;
            }
            MI_FLUSH_PROCESS_TB (FALSE);
            UNLOCK_PFN (OldIrql);
            UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
            LOCK_PFN (OldIrql);
        }
    }
    else {

        if (Vad->u2.VadFlags2.ExtendableFile) {
            PMMEXTEND_INFO ExtendedInfo;
            PMMVAD_LONG VadLong;

            //
            // Release the working set pushlock before referencing the
            // segment since it is pageable.
            //

            UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);

            ExtendedInfo = NULL;
            VadLong = (PMMVAD_LONG) Vad;

            KeAcquireGuardedMutexUnsafe (&MmSectionBasedMutex);
            ASSERT (Vad->ControlArea->Segment->ExtendInfo == VadLong->u4.ExtendedInfo);
            VadLong->u4.ExtendedInfo->ReferenceCount -= 1;
            if (VadLong->u4.ExtendedInfo->ReferenceCount == 0) {
                ExtendedInfo = VadLong->u4.ExtendedInfo;
                VadLong->ControlArea->Segment->ExtendInfo = NULL;
            }
            KeReleaseGuardedMutexUnsafe (&MmSectionBasedMutex);
            if (ExtendedInfo != NULL) {
                ExFreePool (ExtendedInfo);
            }

            LOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
        }

        FirstSubsection = NULL;
        LastSubsection = NULL;

        if (Vad->u.VadFlags.VadType != VadImageMap) {

            if (ControlArea->FilePointer != NULL) {

                if ((Vad->u.VadFlags.Protection == MM_READWRITE) ||
                    (Vad->u.VadFlags.Protection == MM_EXECUTE_READWRITE)) {

                    //
                    // Adjust the count of writable user mappings
                    // to support transactions.
                    //
    
                    InterlockedDecrement ((PLONG)&ControlArea->WritableUserReferences);
                }

                FirstSubsection = MiLocateSubsection (Vad, Vad->StartingVpn);

                ASSERT (FirstSubsection != NULL);

                //
                // Note LastSubsection may be NULL for extendable VADs when the
                // EndingVpn is past the end of the section.  In this case,
                // all the subsections can be safely decremented.
                //

                LastSubsection = MiLocateSubsection (Vad, Vad->EndingVpn);
            }
        }
        else {

            //
            // Handle this specially if it is an image view using large pages.
            //

            TempVa = MI_VPN_TO_VA (Vad->StartingVpn);
            PointerPde = MiGetPdeAddress (TempVa);

            if (
#if (_MI_PAGING_LEVELS>=4)
                (MiGetPxeAddress(TempVa)->u.Hard.Valid == 1) &&
#endif
#if (_MI_PAGING_LEVELS>=3)
                (MiGetPpeAddress(TempVa)->u.Hard.Valid == 1) &&
#endif
                (PointerPde->u.Hard.Valid == 1) &&
                (MI_PDE_MAPS_LARGE_PAGE (PointerPde))) {

                MiFreeLargePages (TempVa,
                                  MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                  FALSE);

                UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);

                MiReleasePhysicalCharges (Vad->EndingVpn - Vad->StartingVpn + 1,
                                          CurrentProcess);

                //
                // Decrement the count of the number of views for the
                // control area.  This requires the PFN lock to be held.
                //
            
                LOCK_PFN (OldIrql);

                ControlArea->NumberOfMappedViews -= 1;
                ControlArea->NumberOfUserReferences -= 1;
            
                //
                // Check to see if the control area should be deleted.
                // This routine releases the PFN lock.
                //
            
                MiCheckControlArea (ControlArea, OldIrql);

                return;
            }
        }

        if (Vad->u.VadFlags.VadType == VadLargePageSection) {

            PointerPde = MiGetPdeAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            EndPde = MiGetPdeAddress (MI_VPN_TO_VA_ENDING (Vad->EndingVpn));

            ControlArea = Vad->ControlArea;

#if DBG
            //
            // Release the working set pushlock before referencing the
            // segment since it is pageable.
            //

            UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
            ASSERT (ControlArea->Segment->SegmentFlags.LargePages == 1);
            LOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
#endif

            ProtoPte = Vad->FirstPrototypePte;

            do {
                TempPte = *PointerPde;
                ASSERT (TempPte.u.Hard.Valid == 1);
                ASSERT (MI_PDE_MAPS_LARGE_PAGE (&TempPte));

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                EndPfn = Pfn1 + (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);

                ASSERT (PageFrameIndex % (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT) == 0);

#if DBG
                //
                // Release the working set pushlock before referencing the
                // prototype PTEs since they are pageable.
                //

                UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);

                ASSERT (ProtoPte->u.Hard.Valid == 1);
                ASSERT (PageFrameIndex == MI_GET_PAGE_FRAME_FROM_PTE (ProtoPte));
                EndProto = ProtoPte + (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);

                while (ProtoPte < EndProto) {
                    TempPte = *ProtoPte;
                    ASSERT (TempPte.u.Hard.Valid == 1);

                    ASSERT (MI_GET_PAGE_FRAME_FROM_PTE (&TempPte) == PageFrameIndex);
                    ASSERT (MI_PFN_ELEMENT(PageFrameIndex)->PteAddress == ProtoPte);

                    ProtoPte += 1;
                    PageFrameIndex += 1;
                }

                LOCK_WS_UNSAFE (CurrentThread, CurrentProcess);
#endif

                LOCK_PFN (OldIrql);

                do {
                    ASSERT (Pfn1->u2.ShareCount >= 1);
                    Pfn1->u2.ShareCount -= 1;
                    Pfn1 += 1;
                } while (Pfn1 < EndPfn);

                MI_WRITE_ZERO_PTE (PointerPde);

                MI_FLUSH_PROCESS_TB (FALSE);

                UNLOCK_PFN (OldIrql);

                PointerPde += 1;

            } while (PointerPde <= EndPde);

#if defined (_WIN64)
            MiDeletePageDirectoriesForPhysicalRange (
                                    MI_VPN_TO_VA (Vad->StartingVpn),
                                    MI_VPN_TO_VA_ENDING (Vad->EndingVpn));
#endif
        }
        else {
            MiDeleteVirtualAddresses (MI_VPN_TO_VA (Vad->StartingVpn),
                                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                      Vad);
        }

        UNLOCK_WS_UNSAFE (CurrentThread, CurrentProcess);

        if (FirstSubsection != NULL) {

            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            LOCK_PFN (OldIrql);
            MiDecrementSubsections (FirstSubsection, LastSubsection);
        }
        else {
            LOCK_PFN (OldIrql);
        }
    }

    //
    // Only physical VADs mapped by drivers don't have control areas.
    // If this view has a control area, the view count must be decremented now.
    //

    if (ControlArea) {

        //
        // Decrement the count of the number of views for the
        // Segment object.  This requires the PFN lock to be held (it is
        // already).
        //
    
        ControlArea->NumberOfMappedViews -= 1;
        ControlArea->NumberOfUserReferences -= 1;
    
        //
        // Check to see if the control area (segment) should be deleted.
        // This routine releases the PFN lock.
        //
    
        MiCheckControlArea (ControlArea, OldIrql);
    }
    else {

        UNLOCK_PFN (OldIrql);

        //
        // Even though it says short VAD in VadFlags, it better be a long VAD.
        //

        ASSERT (Vad->u.VadFlags.VadType == VadDevicePhysicalMemory);
        ASSERT (((PMMVAD_LONG)Vad)->u4.Banked == NULL);
        ASSERT (Vad->ControlArea == NULL);
        ASSERT (Vad->FirstPrototypePte == NULL);
    }
    
    return;
}

VOID
MiPurgeImageSection (
    IN PCONTROL_AREA ControlArea,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This function locates subsections within an image section that
    contain global memory and resets the global memory back to
    the initial subsection contents.

    Note, that for this routine to be called the section is not
    referenced nor is it mapped in any process.

Arguments:

    ControlArea - Supplies a pointer to the control area for the section.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    None.

Environment:

    PFN LOCK held, working set pushlock NOT held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageTableFrameIndex;
    MMPTE PteContents;
    MMPTE NewContents;
    MMPTE NewContentsDemandZero;
    ULONG SizeOfRawData;
    ULONG OffsetIntoSubsection;
    PSUBSECTION Subsection;
#if DBG
    ULONG DelayCount = 0;
#endif

    ASSERT (ControlArea->u.Flags.Image != 0);

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Loop through all the subsections.
    //

    do {

        if (Subsection->u.SubsectionFlags.GlobalMemory == 1) {

            NewContents.u.Long = 0;
            NewContentsDemandZero.u.Long = 0;
            SizeOfRawData = 0;
            OffsetIntoSubsection = 0;

            //
            // Purge this section.
            //

            if (Subsection->StartingSector != 0) {

                //
                // This is not a demand zero section.
                //

                NewContents.u.Long = MiGetSubsectionAddressForPte(Subsection);
                NewContents.u.Soft.Prototype = 1;

                SizeOfRawData = (Subsection->NumberOfFullSectors << MMSECTOR_SHIFT) |
                               Subsection->u.SubsectionFlags.SectorEndOffset;
            }

            NewContents.u.Soft.Protection =
                                       Subsection->u.SubsectionFlags.Protection;
            NewContentsDemandZero.u.Soft.Protection =
                                        NewContents.u.Soft.Protection;

            PointerPte = Subsection->SubsectionBase;
            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
            ControlArea = Subsection->ControlArea;

            if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
            }

            while (PointerPte < LastPte) {

                if (MiIsPteOnPdeBoundary(PointerPte)) {

                    //
                    // We are on a page boundary, make sure this PTE is resident.
                    //

                    if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                    }
                }

                PteContents = *PointerPte;
                if (PteContents.u.Long == 0) {

                    //
                    // No more valid PTEs to deal with.
                    //

                    break;
                }

                ASSERT (PteContents.u.Hard.Valid == 0);

                if ((PteContents.u.Soft.Prototype == 0) &&
                    (PteContents.u.Soft.Transition == 1)) {

                    //
                    // The prototype PTE is in transition format.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                    //
                    // If the prototype PTE is no longer pointing to
                    // the original image page (not in protopte format),
                    // or has been modified, remove it from memory.
                    //

                    if ((Pfn1->u3.e1.Modified == 1) ||
                        (Pfn1->OriginalPte.u.Soft.Prototype == 0)) {

                        ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                        //
                        // This is a transition PTE which has been
                        // modified or is no longer in protopte format.
                        //

                        if (Pfn1->u3.e2.ReferenceCount != 0) {

                            //
                            // There must be an I/O in progress on this
                            // page.  Wait for the I/O operation to complete.
                            //

                            UNLOCK_PFN (OldIrql);

                            //
                            // Drain the deferred lists as these pages may be
                            // sitting in there right now.
                            //

                            MiDeferredUnlockPages (0);

                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

                            //
                            // Redo the loop.
                            //
#if DBG
                            if ((DelayCount % 1024) == 0) {
                                DbgPrintEx (DPFLTR_MM_ID, DPFLTR_WARNING_LEVEL, 
                                    "MMFLUSHSEC: waiting for i/o to complete PFN %p\n",
                                    Pfn1);
                            }
                            DelayCount += 1;
#endif

                            PointerPde = MiGetPteAddress (PointerPte);
                            LOCK_PFN (OldIrql);

                            if (PointerPde->u.Hard.Valid == 0) {
                                MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                            }
                            continue;
                        }

                        ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                           (Pfn1->OriginalPte.u.Soft.Transition == 1)));

                        MI_WRITE_INVALID_PTE (PointerPte, Pfn1->OriginalPte);
                        ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                        //
                        // Only reduce the number of PFN references if
                        // the original PTE is still in prototype PTE
                        // format.
                        //

                        if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {
                            ControlArea->NumberOfPfnReferences -= 1;
                            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
                        }
                        MiUnlinkPageFromList (Pfn1);

                        MI_SET_PFN_DELETED (Pfn1);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // If the reference count for the page is zero, insert
                        // it into the free page list, otherwise leave it alone
                        // and when the reference count is decremented to zero
                        // the page will go to the free list.
                        //

                        if (Pfn1->u3.e2.ReferenceCount == 0) {
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                        }

                        MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                    }
                } else {

                    //
                    // Prototype PTE is not in transition format.
                    //

                    if (PteContents.u.Soft.Prototype == 0) {

                        //
                        // This refers to a page in the paging file,
                        // as it no longer references the image,
                        // restore the PTE contents to what they were
                        // at the initial image creation.
                        //

                        if (PteContents.u.Long != NoAccessPte.u.Long) {
                            MiReleasePageFileSpace (PteContents);
                            MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                        }
                    }
                }
                PointerPte += 1;
                OffsetIntoSubsection += PAGE_SIZE;

                if (OffsetIntoSubsection >= SizeOfRawData) {

                    //
                    // There are trailing demand zero pages in this
                    // subsection, set the PTE contents to be demand
                    // zero for the remainder of the PTEs in this
                    // subsection.
                    //

                    NewContents = NewContentsDemandZero;
                }

#if DBG
                DelayCount = 0;
#endif

            }
        }

        Subsection = Subsection->NextSubsection;

    } while (Subsection != NULL);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Windows Kernel Source Code like\WindowsResearchKernel-WRK\WRK-v1.2\base\ntos\mm\wsmanage.c ===
/*++

Copyright (c) Microsoft Corporation. All rights reserved. 

You may only use this code if you agree to the terms of the Windows Research Kernel Source Code License agreement (see License.txt).
If you do not agree to the terms, do not use the code.


Module Name:

   wsmanage.c

Abstract:

    This module contains routines which manage the set of active working
    set lists.

    Working set management is accomplished by a parallel group of actions

        1. Writing modified pages.

        2. Trimming working sets by :

            a) Aging pages by turning off access bits and incrementing age
               counts for pages which haven't been accessed.
            b) Estimating the number of unused pages in a working set and
               keeping a global count of that estimate.
            c) When getting tight on memory, replacing rather than adding
               pages in a working set when a fault occurs in a working set
               that has a significant proportion of unused pages.
            d) When memory is tight, reducing (trimming) working sets which
               are above their maximum towards their minimum.  This is done
               especially if there are a large number of available pages
               in it.

    The metrics are set such that writing modified pages is typically
    accomplished before trimming working sets, however, under certain cases
    where modified pages are being generated at a very high rate, working
    set trimming will be initiated to free up more pages.

    Once a process has had its working set raised above the minimum
    specified, the process is put on the Working Set Expanded list and
    is now eligible for trimming.  Note that at this time the FLINK field
    in the WorkingSetExpansionLink has an address value.

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MiAdjustWorkingSetManagerParameters)
#pragma alloc_text(PAGE, MmIsMemoryAvailable)
#endif

KEVENT  MiWorkingSetRequestEvent;

#define MI_TRIM_ALL_WORKING_SETS                    0x01
#define MI_AGE_ALL_WORKING_SETS                     0x02
#define MI_EMPTY_ALL_WORKING_SETS                   0x04
#define MI_CAPTURE_ALL_ACCESS_BITS                  0x08
#define MI_CAPTURE_AND_RESET_ALL_ACCESS_BITS        0x10

#define MI_WORKING_SET_FLAGS                        \
        (MI_AGE_ALL_WORKING_SETS |                  \
         MI_TRIM_ALL_WORKING_SETS |                 \
         MI_EMPTY_ALL_WORKING_SETS |                \
         MI_CAPTURE_ALL_ACCESS_BITS |               \
         MI_CAPTURE_AND_RESET_ALL_ACCESS_BITS)

ULONG MiWorkingSetRequestFlags;

LOGICAL MiReplacing = FALSE;

extern ULONG MmStandbyRePurposed;
ULONG MiLastStandbyRePurposed;

extern ULONG MiActiveVerifies;

PFN_NUMBER MmPlentyFreePages = 400;

PFN_NUMBER MmPlentyFreePagesValue;

#define MI_MAXIMUM_AGING_SHIFT 7

ULONG MiAgingShift = 4;
ULONG MiEstimationShift = 5;
PFN_NUMBER MmTotalClaim = 0;
PFN_NUMBER MmTotalEstimatedAvailable = 0;

LARGE_INTEGER MiLastAdjustmentOfClaimParams;

//
// Sixty seconds.
//

const LARGE_INTEGER MmClaimParameterAdjustUpTime = {60 * 1000 * 1000 * 10, 0};

//
// 2 seconds.
//

const LARGE_INTEGER MmClaimParameterAdjustDownTime = {2 * 1000 * 1000 * 10, 0};

LOGICAL MiHardTrim = FALSE;

WSLE_NUMBER MiMaximumWslesPerSweep = (1024 * 1024 * 1024) / PAGE_SIZE;

#define MI_MAXIMUM_SAMPLE 8192

#define MI_MINIMUM_SAMPLE 64
#define MI_MINIMUM_SAMPLE_SHIFT 7

#if DBG
PETHREAD MmWorkingSetThread;
#endif

typedef struct _MMWS_TRIM_CRITERIA {
    UCHAR NumPasses;
    UCHAR TrimAge;
    UCHAR DoAging;
    UCHAR TrimAllPasses;
    PFN_NUMBER DesiredFreeGoal;
    PFN_NUMBER NewTotalClaim;
    PFN_NUMBER NewTotalEstimatedAvailable;
} MMWS_TRIM_CRITERIA, *PMMWS_TRIM_CRITERIA;

ULONG
MiComputeSystemTrimCriteria (
    IN OUT PMMWS_TRIM_CRITERIA Criteria
    );

LOGICAL
MiCheckSystemTrimEndCriteria (
    IN OUT PMMWS_TRIM_CRITERIA Criteria,
    IN KIRQL OldIrql
    );

WSLE_NUMBER
MiDetermineTrimAmount (
    IN PMMWS_TRIM_CRITERIA Criteria,
    IN PMMSUPPORT VmSupport
    );

VOID
MiProcessWorkingSets (
    IN ULONG WorkingSetRequest,
    IN PMMWS_TRIM_CRITERIA TrimCriteria
    );

VOID
MiAdjustClaimParameters (
    IN LOGICAL EnoughPages
    );

VOID
MiRearrangeWorkingSetExpansionList (
    VOID
    );

VOID
MiCaptureAndResetWorkingSetAccessBits (
    IN PMMSUPPORT WsInfo,
    IN ULONG Flags
    );

VOID
MiAdjustWorkingSetManagerParameters (
    IN LOGICAL WorkStation
    )

/*++

Routine Description:

    This function is called from MmInitSystem to adjust the working set manager
    trim algorithms based on system type and size.

Arguments:

    WorkStation - TRUE if this is a workstation, FALSE if not.

Return Value:

    None.

Environment:

    Kernel mode, INIT time only.

--*/
{
    if (WorkStation && MmNumberOfPhysicalPages <= 257*1024*1024/PAGE_SIZE) {
        MiAgingShift = 3;
        MiEstimationShift = 4;
    }
    else {
        MiAgingShift = 5;
        MiEstimationShift = 6;
    }

    if (MmNumberOfPhysicalPages >= 63*1024*1024/PAGE_SIZE) {
        MmPlentyFreePages *= 2;
    }

    MmPlentyFreePagesValue = MmPlentyFreePages;

    MiWorkingSetRequestFlags = 0;
    KeInitializeEvent (&MiWorkingSetRequestEvent, NotificationEvent, TRUE);
}


VOID
MiObtainFreePages (
    VOID
    )

/*++

Routine Description:

    This function examines the size of the modified list and the
    total number of pages in use because of working set increments
    and obtains pages by writing modified pages and/or reducing
    working sets.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set and PFN mutexes held.

--*/

{

    //
    // Check to see if there are enough modified pages to institute a
    // write.
    //

    if (MmModifiedPageListHead.Total >= MmModifiedWriteClusterSize) {

        //
        // Start the modified page writer.
        //

        KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
    }

    //
    // See if there are enough working sets above the minimum
    // threshold to make working set trimming worthwhile.
    //

    if ((MmPagesAboveWsMinimum > MmPagesAboveWsThreshold) ||
        (MmAvailablePages < 5)) {

        //
        // Start the working set manager to reduce working sets.
        //

        KeSetEvent (&MmWorkingSetManagerEvent, 0, FALSE);
    }
}

LOGICAL
MmIsMemoryAvailable (
    IN PFN_NUMBER PagesDesired
    )

/*++

Routine Description:

    This function checks whether there are sufficient available pages based
    on the caller's request.  If currently active pages are needed to satisfy
    this request and non-useful ones can be taken, then trimming is initiated
    here to do so.

Arguments:

    PagesRequested - Supplies the number of pages desired.

Return Value:

    TRUE if sufficient pages exist to satisfy the request.
    FALSE if not.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    LOGICAL Status;
    PFN_NUMBER PageTarget;
    PFN_NUMBER PagePlentyTarget;
    ULONG i;
    PFN_NUMBER CurrentAvailablePages;
    PFN_NUMBER CurrentTotalClaim;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    CurrentAvailablePages = MmAvailablePages;

    //
    // If twice the pages that the caller asked for are available
    // without trimming anything, return TRUE.
    //

    PageTarget = PagesDesired * 2;
    if (CurrentAvailablePages >= PageTarget) {
        return TRUE;
    }

    CurrentTotalClaim = MmTotalClaim;

    //
    // If there are few pages available or claimable, we adjust to do 
    // a hard trim.
    //

    if (CurrentAvailablePages + CurrentTotalClaim < PagesDesired) {
        MiHardTrim = TRUE;
    }

    //
    // Active pages must be trimmed to satisfy this request and it is believed
    // that non-useful pages can be taken to accomplish this.
    //
    // Set the PagePlentyTarget to 125% of the readlist size and kick it off.
    // Our actual trim goal will be 150% of the PagePlentyTarget.
    //

    PagePlentyTarget = PagesDesired + (PagesDesired >> 2);
    MmPlentyFreePages = PagePlentyTarget;

    KeSetEvent (&MmWorkingSetManagerEvent, 0, FALSE);

    Status = FALSE;
    for (i = 0; i < 10; i += 1) {
        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
        if (MmAvailablePages >= PagesDesired) {
            Status = TRUE;
            break;
        }
    }

    MmPlentyFreePages = MmPlentyFreePagesValue;
    MiHardTrim = FALSE;

    return Status;
}

LOGICAL
MiAttachAndLockWorkingSet (
    IN PMMSUPPORT VmSupport
    )

/*++

Routine Description:

    This function attaches to the proper address space and acquires the
    relevant working set mutex for the address space being trimmed.

    If successful, this routine returns with APCs blocked as well.

    On failure, this routine returns without any APCs blocked, no working
    set mutex acquired and no address space attached to.

Arguments:

    VmSupport - Supplies the working set to attach to and lock.

Return Value:

    TRUE if successful, FALSE if not.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    ULONG count;
    PETHREAD Thread;
    PEPROCESS ProcessToTrim;
    LOGICAL Attached;
    PMM_SESSION_SPACE SessionSpace;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    Thread = PsGetCurrentThread ();

    if (VmSupport == &MmSystemCacheWs) {

        ASSERT (VmSupport->Flags.SessionSpace == 0);
        ASSERT (VmSupport->Flags.TrimHard == 0);

        //
        // System cache,
        //

        KeEnterGuardedRegionThread (&Thread->Tcb);

        if (ExTryAcquirePushLockExclusive (&VmSupport->WorkingSetMutex) == FALSE) {

            //
            // System working set mutex was not granted, don't trim
            // the system cache.
            //

            KeLeaveGuardedRegionThread (&Thread->Tcb);
            return FALSE;
        }

        PsGetCurrentThread ()->OwnsSystemWorkingSetExclusive = 1;

        MM_SYSTEM_WS_LOCK_TIMESTAMP ();

        return TRUE;
    }

    if (VmSupport->Flags.SessionSpace == 0) {

        ProcessToTrim = CONTAINING_RECORD (VmSupport, EPROCESS, Vm);

        ASSERT ((ProcessToTrim->Flags & PS_PROCESS_FLAGS_VM_DELETED) == 0);

        //
        // Attach to the process in preparation for trimming.
        //

        Attached = 0;
        if (ProcessToTrim != PsInitialSystemProcess) {

            Attached = KeForceAttachProcess (&ProcessToTrim->Pcb);

            if (Attached == 0) {
                return FALSE;
            }

            if (ProcessToTrim->Flags & PS_PROCESS_FLAGS_OUTSWAP_ENABLED) {

                //
                // We have effectively performed an inswap of the process
                // due to the force attach.  Mark the process (and session)
                // accordingly.
                //

                ASSERT ((ProcessToTrim->Flags & PS_PROCESS_FLAGS_OUTSWAPPED) == 0);

                if (ProcessToTrim->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
                    MiSessionInSwapProcess (ProcessToTrim, TRUE);
                }

                PS_CLEAR_BITS (&ProcessToTrim->Flags,
                               PS_PROCESS_FLAGS_OUTSWAP_ENABLED);
            }
        }

        //
        // Attempt to acquire the working set pushlock. If the
        // lock cannot be acquired, skip over this process.
        //

        count = 0;
        do {
            KeEnterGuardedRegionThread (&Thread->Tcb);

            if (ExTryAcquirePushLockExclusive (&VmSupport->WorkingSetMutex) != FALSE) {
                ASSERT (VmSupport->WorkingSetExpansionLinks.Flink == MM_WS_TRIMMING);
                LOCK_WS_TIMESTAMP (ProcessToTrim);
                Thread->OwnsProcessWorkingSetExclusive = 1;
                return TRUE;
            }

            KeLeaveGuardedRegionThread (&Thread->Tcb);

            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
            count += 1;
        } while (count < 3);

        //
        // Could not get the lock, skip this process.
        //

        if (Attached) {
            KeDetachProcess ();
        }

        return FALSE;
    }

    SessionSpace = CONTAINING_RECORD (VmSupport, MM_SESSION_SPACE, Vm);

    //
    // Attach directly to the session space to be trimmed.
    //

    MiAttachSession (SessionSpace);

    //
    // Try for the session working set mutex.
    //

    KeEnterGuardedRegionThread (&Thread->Tcb);

    if (ExTryAcquirePushLockExclusive (&VmSupport->WorkingSetMutex) == FALSE) {

        //
        // This session space's working set mutex was not
        // granted, don't trim it.
        //

        KeLeaveGuardedRegionThread (&Thread->Tcb);

        MiDetachSession ();

        return FALSE;
    }

    Thread->OwnsSessionWorkingSetExclusive = 1;

    return TRUE;
}

VOID
MiDetachAndUnlockWorkingSet (
    IN PMMSUPPORT VmSupport
    )

/*++

Routine Description:

    This function detaches from the target address space and releases the
    relevant working set mutex for the address space that was trimmed.

Arguments:

    VmSupport - Supplies the working set to detach from and unlock.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL.

--*/

{
    PETHREAD Thread;
    PEPROCESS ProcessToTrim;

    ASSERT (KeAreAllApcsDisabled () == TRUE);

    Thread = PsGetCurrentThread ();

    UNLOCK_WORKING_SET (Thread, VmSupport);

    if (VmSupport == &MmSystemCacheWs) {
        ASSERT (VmSupport->Flags.SessionSpace == 0);
    }
    else if (VmSupport->Flags.SessionSpace == 0) {

        ProcessToTrim = CONTAINING_RECORD (VmSupport, EPROCESS, Vm);

        ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

        if (ProcessToTrim != PsInitialSystemProcess) {
            KeDetachProcess ();
        }
    }
    else {
        MiDetachSession ();
    }

    return;
}


VOID
MmWorkingSetManager (
    VOID
    )

/*++

Routine Description:

    Implements the NT working set manager thread.  When the number
    of free pages becomes critical and ample pages can be obtained by
    reducing working sets, the working set manager's event is set, and
    this thread becomes active.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG WorkingSetRequestFlags;
    MMWS_TRIM_CRITERIA TrimCriteria;
    static ULONG Initialized = 0;

    if (Initialized == 0) {
        PsGetCurrentThread()->MemoryMaker = 1;
        Initialized = 1;
    }

#if DBG
    MmWorkingSetThread = PsGetCurrentThread ();
#endif

    ASSERT (MmIsAddressValid (MmSessionSpace) == FALSE);

    //
    // Set the trim criteria: If there are plenty of pages, the existing
    // sets are aged and FALSE is returned to signify no trim is necessary.
    // Otherwise, the working set expansion list is ordered so the best
    // candidates for trimming are placed at the front and TRUE is returned.
    //

    WorkingSetRequestFlags = MiComputeSystemTrimCriteria (&TrimCriteria);

    if (WorkingSetRequestFlags != 0) {

        ASSERT (MmIsAddressValid (MmSessionSpace) == FALSE);

        if (WorkingSetRequestFlags & (MI_TRIM_ALL_WORKING_SETS | MI_EMPTY_ALL_WORKING_SETS)) {

            //
            // Clear the deferred entry list to free up some pages.
            //

            MiDeferredUnlockPages (0);
        }

        MiProcessWorkingSets (WorkingSetRequestFlags, &TrimCriteria);
    }
            
    //
    // If memory is critical then when we trimmed any dirty pages, the modified
    // writer was signaled during the insertion on the modified list.
    //
    // If there are a significant number of modified pages to be written then
    // signal the modified writer now, regardless of the above to avoid
    // generating large backlogs.
    //

    if (MmModifiedPageListHead.Total >= MmModifiedPageMaximum) {
        KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
    }

    return;
}

ULONG
MiComputeSystemTrimCriteria (
    IN PMMWS_TRIM_CRITERIA Criteria
    )

/*++

Routine Description:

    Decide whether to trim, age or adjust claim estimations at this time.

Arguments:

    Criteria - Supplies a pointer to the trim criteria information.  Various
               fields in this structure are set as needed by this routine.

Return Value:

    The actions our caller should apply to the working sets.

Environment:

    Kernel mode.  No locks held.  APC level or below.

    This is called at least once per second on entry to MmWorkingSetManager.

--*/

{
    KIRQL OldIrql;
    ULONG OutFlags;
    PFN_NUMBER Available;
    ULONG StandbyRemoved;
    ULONG StandbyTemp;
    PFN_NUMBER PlentyFreePages;

    //
    // Check the number of pages available to see if any trimming (or aging)
    // is really required.
    //

    Available = MmAvailablePages;

    StandbyRemoved = MmStandbyRePurposed;

    //
    // If the counter wrapped, it's ok to just ignore it this time around.
    //

    if (StandbyRemoved <= MiLastStandbyRePurposed) {
        MiLastStandbyRePurposed = StandbyRemoved;
        StandbyRemoved = 0;
    }
    else {

        //
        // The value is nonzero, we need to synchronize so we get a coordinated
        // snapshot of both values.
        //

        LOCK_PFN (OldIrql);
        Available = MmAvailablePages;
        StandbyRemoved = MmStandbyRePurposed;
        UNLOCK_PFN (OldIrql);

        if (StandbyRemoved <= MiLastStandbyRePurposed) {
            MiLastStandbyRePurposed = StandbyRemoved;
            StandbyRemoved = 0;
        }
        else {
            StandbyTemp = StandbyRemoved;
            StandbyRemoved -= MiLastStandbyRePurposed;
            MiLastStandbyRePurposed = StandbyTemp;
        }
    }

    //
    // MmPlentyFreePages can change dynamically, so snap it now.
    //

    PlentyFreePages = MmPlentyFreePages;

    //
    // If we're low on pages, or we've been replacing within a given
    // working set, or we've been cannibalizing a large number of standby
    // pages, then trim now.
    //

    if ((Available <= PlentyFreePages) ||
        (MiReplacing == TRUE) ||
        (StandbyRemoved >= (Available >> 2))) {

        //
        // Only pulse the event if it is not already set.  This saves
        // on dispatcher lock contention, but more importantly since
        // KePulse always clears the event it saves us having to check
        // whether to set it (and potentially do the setting) afterwards.
        //

        if (MiLowMemoryEvent->Header.SignalState == 0) {
            LOCK_PFN (OldIrql);
            if (MiLowMemoryEvent->Header.SignalState == 0) {
                KePulseEvent (MiLowMemoryEvent, 0, FALSE);
            }
            UNLOCK_PFN (OldIrql);
        }

        //
        // Inform our caller to start trimming since we're below
        // plenty pages - order the list so the bigger working sets are
        // in front so our caller trims those first.
        //

        Criteria->NumPasses = 0;
        Criteria->DesiredFreeGoal = PlentyFreePages + (PlentyFreePages / 2);
        Criteria->NewTotalClaim = 0;
        Criteria->NewTotalEstimatedAvailable = 0;

        //
        // If more than 25% of the available pages were recycled standby
        // pages, then trim more aggressively in an attempt to get more of the
        // cold pages into standby for the next pass.
        //

        if (StandbyRemoved >= (Available >> 2)) {
            Criteria->TrimAllPasses = TRUE;
        }
        else {
            Criteria->TrimAllPasses = FALSE;
        }

#if DBG
        if (MmDebug & MM_DBG_WS_EXPANSION) {
            DbgPrintEx (DPFLTR_MM_ID, DPFLTR_TRACE_LEVEL, 
                "\nMM-wsmanage: Desired = %ld, Avail %ld\n",
                    Criteria->DesiredFreeGoal, MmAvailablePages);
        }
#endif

        //
        // No need to lock synchronize the MiReplacing clearing as it
        // gets set every time a page replacement happens anyway.
        //

        MiReplacing = FALSE;

        //
        // Start trimming the bigger working sets first.
        //

        MiRearrangeWorkingSetExpansionList ();

        OutFlags = MI_TRIM_ALL_WORKING_SETS;
    }
    else if (Available < MM_ENORMOUS_LIMIT) {

        //
        // Don't trim but do age unused pages and estimate
        // the amount available in working sets.
        //

        OutFlags = MI_AGE_ALL_WORKING_SETS;
    }
    else {

        //
        // There is an overwhelming surplus of memory and this is a big
        // server then don't even bother aging at this point (note the claim
        // and estimated available are not cleared so they may contain stale
        // values, but at this level it doesn't really matter).
        //

        OutFlags = 0;
    }

    //
    // See if any working set requests have been queued to us.
    //

    OutFlags |= MiWorkingSetRequestFlags;

    ASSERT ((OutFlags & ~MI_WORKING_SET_FLAGS) == 0);

    return OutFlags;
}

LOGICAL
MiCheckSystemTrimEndCriteria (
    IN PMMWS_TRIM_CRITERIA Criteria,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

     Check the ending criteria.  If we're not done, delay for a little
     bit to let the modified writes catch up.

Arguments:

    Criteria - Supplies the trim criteria information.

    OldIrql - Supplies the old IRQL to lower to if the expansion lock needs
              to be released.

Return Value:

    TRUE if trimming can be stopped, FALSE otherwise.

Environment:

    Kernel mode.  Expansion lock held.  APC level or below.

--*/

{
    LOGICAL FinishedTrimming;

    if ((MmAvailablePages > Criteria->DesiredFreeGoal) ||
        (Criteria->NumPasses >= MI_MAX_TRIM_PASSES)) {

        //
        // We have enough pages or we trimmed as many as we're going to get.
        //

        return TRUE;
    }

    //
    // Update the global claim and estimate before we wait.
    //

    MmTotalClaim = Criteria->NewTotalClaim;
    MmTotalEstimatedAvailable = Criteria->NewTotalEstimatedAvailable;

    //
    // We don't have enough pages - give the modified page writer
    // 10 milliseconds to catch up.  The wait is also important because a
    // thread may have the system cache locked but has been preempted
    // by the balance set manager due to its higher priority.  We must
    // give this thread a shot at running so it can release the system
    // cache lock (all the trimmable pages may reside in the system cache).
    //

    UNLOCK_EXPANSION (OldIrql);

    KeDelayExecutionThread (KernelMode,
                            FALSE,
                            (PLARGE_INTEGER)&MmShortTime);

    //
    // Check again to see if we've met the criteria to stop trimming.
    //

    if (MmAvailablePages > Criteria->DesiredFreeGoal) {

        //
        // Now we have enough pages so break out.
        //

        FinishedTrimming = TRUE;
    }
    else {

        //
        // We don't have enough pages so let's do another pass.
        // Go get the next working set list which is probably the
        // one we put back before we gave up the processor.
        //

        FinishedTrimming = FALSE;

        if (Criteria->NumPasses == 0) {
            MiAdjustClaimParameters (FALSE);
        }

        Criteria->NumPasses += 1;
        Criteria->NewTotalClaim = 0;
        Criteria->NewTotalEstimatedAvailable = 0;

    }

    LOCK_EXPANSION (OldIrql);

    return FinishedTrimming;
}


VOID
MiProcessWorkingSets (
    IN ULONG WorkingSetRequestFlags,
    IN PMMWS_TRIM_CRITERIA TrimCriteria
    )

/*++

Routine Description:

    Walk through the sets on the working set expansion list, taking action
    as specified by the argument request.

Arguments:

    WorkingSetRequestFlags - Supplies the working set request flags to apply to
                             all the working sets.

    TrimCriteria - Supplies relevant trim/aging criteria.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.  PFN lock NOT held.

--*/

{
    PLIST_ENTRY ListEntry;
    WSLE_NUMBER Trim;
    KIRQL OldIrql;
    PMMSUPPORT VmSupport;
    LARGE_INTEGER CurrentTime;
    WSLE_NUMBER WslesScanned;
    ULONG WorkingSetRequestFlagsDone;

    WslesScanned = 0;
    WorkingSetRequestFlagsDone = 0;

    if (WorkingSetRequestFlags & MI_AGE_ALL_WORKING_SETS) {
        ASSERT ((WorkingSetRequestFlags & MI_TRIM_ALL_WORKING_SETS) == 0);
    }

    KeQuerySystemTime (&CurrentTime);

    LOCK_EXPANSION (OldIrql);

    while (!IsListEmpty (&MmWorkingSetExpansionHead.ListHead)) {

        //
        // Remove the entry at the head and operate on it.
        //

        ListEntry = RemoveHeadList (&MmWorkingSetExpansionHead.ListHead);

        VmSupport = CONTAINING_RECORD (ListEntry,
                                       MMSUPPORT,
                                       WorkingSetExpansionLinks);

        //
        // Note that other routines that set this bit must remove the
        // entry from the expansion list first.
        //

        ASSERT (VmSupport->WorkingSetExpansionLinks.Flink != MM_WS_TRIMMING);

        //
        // Check to see if we've been here before.
        //

        if (VmSupport->LastTrimTime.QuadPart == CurrentTime.QuadPart) {

            InsertHeadList (&MmWorkingSetExpansionHead.ListHead,
                            &VmSupport->WorkingSetExpansionLinks);

            //
            // If we aren't finished we may sleep in MiCheck.
            //

            if ((WorkingSetRequestFlags & MI_TRIM_ALL_WORKING_SETS) &&
                (MiCheckSystemTrimEndCriteria (TrimCriteria, OldIrql)) == FALSE) {


                //
                // Start a new round of trimming.
                //

                KeQuerySystemTime (&CurrentTime);

                continue;
            }

            //
            // No more pages are needed.  If no new requests have
            // been queued, then we're done.
            //

            WorkingSetRequestFlags &= ~(MI_TRIM_ALL_WORKING_SETS | MI_AGE_ALL_WORKING_SETS);

            WorkingSetRequestFlagsDone |= WorkingSetRequestFlags;

            if (WorkingSetRequestFlagsDone != MiWorkingSetRequestFlags) {

                //
                // Some other thread has made an additional request(s) so pick
                // up the new bits and process them now before returning.
                //

                WorkingSetRequestFlags = (WorkingSetRequestFlagsDone ^
                                           MiWorkingSetRequestFlags);

                ASSERT ((WorkingSetRequestFlags & ~MI_WORKING_SET_FLAGS) == 0);
                ASSERT (WorkingSetRequestFlags != 0);

                KeQuerySystemTime (&CurrentTime);

                continue;
            }

            if (MiWorkingSetRequestFlags != 0) {

                //
                // Clear all the flags and wake any waiters
                // since every request has been processed.
                //

                MiWorkingSetRequestFlags = 0;

                KeSetEvent (&MiWorkingSetRequestEvent, 0, FALSE);
            }

            break;
        }

        //
        // Only attach if the working set is worth examining.  This is
        // not just an optimization, as care must be taken not to attempt
        // an attach to a process which is a candidate for being currently
        // (or already) swapped out because if we attach to a page
        // directory that is in transition it's all over.
        //

        if ((VmSupport->WorkingSetSize <= MM_PROCESS_COMMIT_CHARGE) &&
            (VmSupport != &MmSystemCacheWs) &&
            (VmSupport->Flags.SessionSpace == 0)) {

            InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                            &VmSupport->WorkingSetExpansionLinks);
            continue;
        }

        VmSupport->LastTrimTime = CurrentTime;
        VmSupport->WorkingSetExpansionLinks.Flink = MM_WS_TRIMMING;
        VmSupport->WorkingSetExpansionLinks.Blink = NULL;

        UNLOCK_EXPANSION (OldIrql);

        if (MiAttachAndLockWorkingSet (VmSupport) == TRUE) {

            //
            // Order is important when processing the flag bits ...
            //

            if (WorkingSetRequestFlags & MI_EMPTY_ALL_WORKING_SETS) {
                MiEmptyWorkingSet (VmSupport, FALSE);
            }

            if (WorkingSetRequestFlags & MI_TRIM_ALL_WORKING_SETS) {

                ASSERT ((WorkingSetRequestFlags & MI_AGE_ALL_WORKING_SETS) == 0);

                //
                // Determine how many pages to trim from this working set.
                //
    
                Trim = MiDetermineTrimAmount (TrimCriteria, VmSupport);
    
                //
                // If there's something to trim...
                //
    
                if ((Trim != 0) &&
                    ((TrimCriteria->TrimAllPasses > TrimCriteria->NumPasses) ||
                     (MmAvailablePages < TrimCriteria->DesiredFreeGoal))) {
    
                    //
                    // We haven't reached our goal, so trim now.
                    //
    
                    Trim = MiTrimWorkingSet (Trim,
                                             VmSupport,
                                             TrimCriteria->TrimAge);
                }
    
                //
                // Estimating the current claim is always done here by taking a
                // sample of the working set.  Aging is only done if the trim
                // pass warrants it (ie: the first pass only).
                //
    
                MiAgeWorkingSet (VmSupport,
                                 TrimCriteria->DoAging,
                                 NULL,
                                 &TrimCriteria->NewTotalClaim,
                                 &TrimCriteria->NewTotalEstimatedAvailable);
            }
            else if (WorkingSetRequestFlags & MI_AGE_ALL_WORKING_SETS) {
                MiAgeWorkingSet (VmSupport,
                                 TRUE,
                                 &WslesScanned,
                                 &TrimCriteria->NewTotalClaim,
                                 &TrimCriteria->NewTotalEstimatedAvailable);
            }

            if (WorkingSetRequestFlags & MI_CAPTURE_AND_RESET_ALL_ACCESS_BITS) {

                MiCaptureAndResetWorkingSetAccessBits (VmSupport,
                                                       WorkingSetRequestFlags);
            }
            else if (WorkingSetRequestFlags & MI_CAPTURE_ALL_ACCESS_BITS) {

                //
                // This mode could be handled by acquiring the working set
                // pushlock shared instead of exclusive.  Note that if this
                // is implemented, only do it if this is the only flag bit
                // set (as the other bits require exclusive).
                //

                MiCaptureAndResetWorkingSetAccessBits (VmSupport,
                                                       WorkingSetRequestFlags);
            }

            MiDetachAndUnlockWorkingSet (VmSupport);

            LOCK_EXPANSION (OldIrql);
        }
        else {

            //
            // Unable to attach to the working set presumably because
            // some other thread has it locked.  Set the ForceTrim flag
            // so it will be trimmed later by whoever owns it (or whoever
            // tries to insert the next entry).
            //

            LOCK_EXPANSION (OldIrql);

            if (WorkingSetRequestFlags & MI_TRIM_ALL_WORKING_SETS) {
                VmSupport->Flags.ForceTrim = 1;
            }
        }

        ASSERT (VmSupport->WorkingSetExpansionLinks.Flink == MM_WS_TRIMMING);
        if (VmSupport->WorkingSetExpansionLinks.Blink == NULL) {

            //
            // Reinsert this working set at the tail of the list.
            //

            InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                            &VmSupport->WorkingSetExpansionLinks);
        }
        else {

            //
            // The process is terminating - the value in the blink
            // is the address of an event to set.
            //

            ASSERT (VmSupport != &MmSystemCacheWs);

            VmSupport->WorkingSetExpansionLinks.Flink = MM_WS_NOT_LISTED;

            KeSetEvent ((PKEVENT)VmSupport->WorkingSetExpansionLinks.Blink,
                        0,
                        FALSE);
        }
    }

    if (WorkingSetRequestFlags & (MI_TRIM_ALL_WORKING_SETS | MI_AGE_ALL_WORKING_SETS)) {
        MmTotalClaim = TrimCriteria->NewTotalClaim;
        MmTotalEstimatedAvailable = TrimCriteria->NewTotalEstimatedAvailable;
    }

    UNLOCK_EXPANSION (OldIrql);

    if (WorkingSetRequestFlags & MI_AGE_ALL_WORKING_SETS) {
        MiAdjustClaimParameters (TRUE);
    }

    return;
}


WSLE_NUMBER
MiDetermineTrimAmount (
    PMMWS_TRIM_CRITERIA Criteria,
    PMMSUPPORT VmSupport
    )

/*++

Routine Description:

     Determine whether this process should be trimmed.

Arguments:

    Criteria - Supplies the trim criteria information.

    VmSupport - Supplies the working set information for the candidate.

Return Value:

    TRUE if trimming should be done on this process, FALSE if not.

Environment:

    Kernel mode.  Expansion lock held.  APC level or below.

--*/

{
    PMMWSL WorkingSetList;
    WSLE_NUMBER MaxTrim;
    WSLE_NUMBER Trim;
    LOGICAL OutswapEnabled;
    PEPROCESS ProcessToTrim;
    PMM_SESSION_SPACE SessionSpace;

    WorkingSetList = VmSupport->VmWorkingSetList;

    MaxTrim = VmSupport->WorkingSetSize;

    if (MaxTrim <= WorkingSetList->FirstDynamic) {
        return 0;
    }

    OutswapEnabled = FALSE;

    if (VmSupport == &MmSystemCacheWs) {
    }
    else if (VmSupport->Flags.SessionSpace == 0) {

        ProcessToTrim = CONTAINING_RECORD (VmSupport, EPROCESS, Vm);

        if (ProcessToTrim->Flags & PS_PROCESS_FLAGS_OUTSWAP_ENABLED) {
            OutswapEnabled = TRUE;
        }

        if (VmSupport->Flags.MinimumWorkingSetHard == 1) {
            if (MaxTrim <= VmSupport->MinimumWorkingSetSize) {
                return 0;
            }
            OutswapEnabled = FALSE;
        }

    }
    else {
        if (VmSupport->Flags.TrimHard == 1) {
            OutswapEnabled = TRUE;
        }

        SessionSpace = CONTAINING_RECORD(VmSupport,
                                         MM_SESSION_SPACE,
                                         Vm);

    }

    if (OutswapEnabled == FALSE) {

        //
        // Don't trim the cache or non-swapped sessions or processes
        // below their minimum.
        //

        MaxTrim -= VmSupport->MinimumWorkingSetSize;
    }

    switch (Criteria->NumPasses) {
    case 0:
        Trim = VmSupport->Claim >>
                    ((VmSupport->Flags.MemoryPriority == MEMORY_PRIORITY_FOREGROUND)
                        ? MI_FOREGROUND_CLAIM_AVAILABLE_SHIFT
                        : MI_BACKGROUND_CLAIM_AVAILABLE_SHIFT);
        Criteria->TrimAge = MI_PASS0_TRIM_AGE;
        Criteria->DoAging = TRUE;
        break;
    case 1:
        Trim = VmSupport->Claim >>
                    ((VmSupport->Flags.MemoryPriority == MEMORY_PRIORITY_FOREGROUND)
                        ? MI_FOREGROUND_CLAIM_AVAILABLE_SHIFT
                        : MI_BACKGROUND_CLAIM_AVAILABLE_SHIFT);
        Criteria->TrimAge = MI_PASS1_TRIM_AGE;
        Criteria->DoAging = FALSE;
        break;
    case 2:
        Trim = VmSupport->Claim;
        Criteria->TrimAge = MI_PASS2_TRIM_AGE;
        Criteria->DoAging = FALSE;
        break;
    case 3:
        Trim = VmSupport->EstimatedAvailable;
        Criteria->TrimAge = MI_PASS3_TRIM_AGE;
        Criteria->DoAging = FALSE;
        break;
    default:
        Trim = VmSupport->EstimatedAvailable;
        Criteria->TrimAge = MI_PASS3_TRIM_AGE;
        Criteria->DoAging = FALSE;

        if (MiHardTrim == TRUE || MmAvailablePages < MM_HIGH_LIMIT + 64) {
            if (VmSupport->WorkingSetSize > VmSupport->MinimumWorkingSetSize) {
                Trim = (VmSupport->WorkingSetSize - VmSupport->MinimumWorkingSetSize) >> 2;
             