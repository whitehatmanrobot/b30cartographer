                               1);

                MiFillMemoryPte (MiGetVirtualAddressMappedByPte(StartPpe),
                                 PAGE_SIZE,
                                 ZeroKernelPte.u.Long);
            }
#endif

            ASSERT (StartPde->u.Hard.Valid == 0);

            //
            // Map in a page table page.
            //

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (StartPde));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (StartPde, TempPte);

            MiInitializePfn (PageFrameIndex, StartPde, 1);

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte(StartPde),
                             PAGE_SIZE,
                             ZeroKernelPte.u.Long);

            StartPde += 1;
        }

        UNLOCK_PFN (OldIrql);

        //
        // Initialize the system cache.  Only set the large system cache if
        // we have a large amount of physical memory.
        //

        if (MmLargeSystemCache != 0 && MmNumberOfPhysicalPages > 0x7FF0) {
            if ((MmAvailablePages >
                    MmSystemCacheWsMaximum + ((64*1024*1024) >> PAGE_SHIFT))) {
                MmSystemCacheWsMaximum =
                            MmAvailablePages - ((32*1024*1024) >> PAGE_SHIFT);
                ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
            }
        }

        if (MmSystemCacheWsMaximum > (MM_MAXIMUM_WORKING_SET - 5)) {
            MmSystemCacheWsMaximum = MM_MAXIMUM_WORKING_SET - 5;
        }

        if (MmSystemCacheWsMaximum > MmSizeOfSystemCacheInPages) {
            MmSystemCacheWsMaximum = MmSizeOfSystemCacheInPages;
            if ((MmSystemCacheWsMinimum + 500) > MmSystemCacheWsMaximum) {
                MmSystemCacheWsMinimum = MmSystemCacheWsMaximum - 500;
            }
        }

        MiInitializeSystemCache ((ULONG)MmSystemCacheWsMinimum,
                                 (ULONG)MmSystemCacheWsMaximum);

        MmAttemptForCantExtend.Segment = NULL;
        MmAttemptForCantExtend.RequestedExpansionSize = 1;
        MmAttemptForCantExtend.ActualExpansion = 0;
        MmAttemptForCantExtend.InProgress = FALSE;
        MmAttemptForCantExtend.PageFileNumber = MI_EXTEND_ANY_PAGEFILE;

        KeInitializeEvent (&MmAttemptForCantExtend.Event,
                           NotificationEvent,
                           FALSE);

        //
        // Now that we have booted far enough, replace the temporary
        // commit limits with real ones: set the initial commit page
        // limit to the number of available pages.  This value is
        // updated as paging files are created.
        //

        MmTotalCommitLimit = MmAvailablePages;

        if (MmTotalCommitLimit > 1024) {
            MmTotalCommitLimit -= 1024;
        }

        MmTotalCommitLimitMaximum = MmTotalCommitLimit;

        //
        // Set maximum working set size to 512 pages less than the
        // total available memory.
        //

        MmMaximumWorkingSetSize = (WSLE_NUMBER)(MmAvailablePages - 512);

        if (MmMaximumWorkingSetSize > (MM_MAXIMUM_WORKING_SET - 5)) {
            MmMaximumWorkingSetSize = MM_MAXIMUM_WORKING_SET - 5;
        }

        //
        // Create the modified page writer event.
        //

        KeInitializeEvent (&MmModifiedPageWriterEvent, NotificationEvent, FALSE);

        //
        // Build paged pool.
        //

        MiBuildPagedPool ();

        //
        // Initialize the loaded module list.  This cannot be done until
        // paged pool has been built.
        //

        if (MiInitializeLoadedModuleList (LoaderBlock) == FALSE) {
#if DBG
            DbgPrint("Loaded module list initialization failed\n");
#endif
            return FALSE;
        }

        //
        // Initialize the unused segment threshold.  Attempt to keep pool usage
        // below this percentage (by trimming the cache) if pool requests
        // can fail.
        //

        if (MmConsumedPoolPercentage == 0) {
            MmConsumedPoolPercentage = 80;
        }
        else if (MmConsumedPoolPercentage < 5) {
            MmConsumedPoolPercentage = 5;
        }
        else if (MmConsumedPoolPercentage > 100) {
            MmConsumedPoolPercentage = 100;
        }
    
        //
        // Add more system PTEs if this is a large memory system.
        // Note that 64 bit systems can determine the right value at the
        // beginning since there is no virtual address space crunch.
        //

#if !defined (_WIN64)
        if (MmNumberOfPhysicalPages > ((127*1024*1024) >> PAGE_SHIFT)) {

            PMMPTE StartingPte;

            PointerPde = MiGetPdeAddress ((PCHAR)MmPagedPoolEnd + 1);
            StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);
            j = 0;

            TempPte = ValidKernelPde;
            LOCK_PFN (OldIrql);
            while (PointerPde->u.Hard.Valid == 0) {

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_EXTRA_SYSTEM_PTES, 1);

                PageFrameIndex = MiRemoveZeroPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPde, TempPte);
                MiInitializePfn (PageFrameIndex, PointerPde, 1);
                PointerPde += 1;
                StartingPte += PAGE_SIZE / sizeof(MMPTE);
                j += PAGE_SIZE / sizeof(MMPTE);
            }

            UNLOCK_PFN (OldIrql);

            if (j != 0) {
                StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);
                MmNonPagedSystemStart = MiGetVirtualAddressMappedByPte (StartingPte);
                MmNumberOfSystemPtes += j;
                MiAddSystemPtes (StartingPte, j, SystemPteSpace);
                MiIncrementSystemPtes (j);
            }
        }
#endif

#if defined (_MI_DEBUG_SUB)
        if (MiTrackSubs != 0) {
            MiSubsectionTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackSubs * sizeof (MI_SUB_TRACES),
                                   'tCmM');
        }
#endif

#if defined (_MI_DEBUG_DIRTY)
        if (MiTrackDirtys != 0) {
            MiDirtyTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackDirtys * sizeof (MI_DIRTY_TRACES),
                                   'tCmM');
        }
#endif

#if defined (_MI_DEBUG_DATA)
        if (MiTrackData != 0) {
            MiDataTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackData * sizeof (MI_DATA_TRACES),
                                   'tCmM');
        }
#endif

#if DBG
        if (MmDebug & MM_DBG_DUMP_BOOT_PTES) {
            MiDumpValidAddresses ();
            MiDumpPfn ();
        }
#endif

        MmPageFaultNotifyRoutine = NULL;

#ifdef _MI_MESSAGE_SERVER
        MiInitializeMessageQueue ();
#endif

        return TRUE;
    }

    if (Phase == 1) {

#if DBG
        MmDebug |= MM_DBG_CHECK_PFN_LOCK;
#endif

#if defined(_X86_) || defined(_AMD64_)
        MiInitMachineDependent (LoaderBlock);
#endif
        MiMapBBTMemory(LoaderBlock);

        if (!MiSectionInitialization ()) {
            return FALSE;
        }

        Process = PsGetCurrentProcess ();
        if (Process->PhysicalVadList.Flink == NULL) {
            InitializeListHead (&Process->PhysicalVadList);
        }

#if defined(MM_SHARED_USER_DATA_VA)

        //
        // Create double mapped page between kernel and user mode.
        // The PTE is deliberately allocated from paged pool so that
        // it will always have a PTE itself instead of being superpaged.
        // This way, checks throughout the fault handler can assume that
        // the PTE can be checked without having to special case this.
        //

        MmSharedUserDataPte = ExAllocatePoolWithTag (PagedPool,
                                                     sizeof(MMPTE),
                                                     '  mM');

        if (MmSharedUserDataPte == NULL) {
            return FALSE;
        }

        PointerPte = MiGetPteAddress ((PVOID)KI_USER_SHARED_DATA);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READONLY,
                           PointerPte);

        *MmSharedUserDataPte = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        LOCK_PFN (OldIrql);

        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        UNLOCK_PFN (OldIrql);

#ifdef _X86_
        if (MmHighestUserAddress < (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // Install the PTE mapping now as faults will not because the
            // shared user data is in the system portion of the address space.
            // Note the pagetable page has already been allocated and locked
            // down.
            //

            //
            // Make the mapping user accessible.
            //

            ASSERT (MmSharedUserDataPte->u.Hard.Owner == 0);
            MmSharedUserDataPte->u.Hard.Owner = 1;

            PointerPde = MiGetPdeAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPde->u.Hard.Owner == 0);
            PointerPde->u.Hard.Owner = 1;

            ASSERT (MiUseMaximumSystemSpace != 0);
            PointerPte = MiGetPteAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPte->u.Hard.Valid == 0);
            MI_WRITE_VALID_PTE (PointerPte, *MmSharedUserDataPte);
        }
#endif

#endif

        MiSessionWideInitializeAddresses ();
        MiInitializeSessionWsSupport ();
        MiInitializeSessionIds ();

        //
        // Start the modified page writer.
        //

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread(
                        &ThreadHandle,
                        THREAD_ALL_ACCESS,
                        &ObjectAttributes,
                        0L,
                        NULL,
                        MiModifiedPageWriter,
                        NULL
                        ))) {
            return FALSE;
        }
        ZwClose (ThreadHandle);

        //
        // Initialize the low and high memory events.  This must be done
        // before starting the working set manager.
        //

        if (MiInitializeMemoryEvents () == FALSE) {
            return FALSE;
        }

        //
        // Start the balance set manager.
        //
        // The balance set manager performs stack swapping and working
        // set management and requires two threads.
        //

        KeInitializeEvent (&MmWorkingSetManagerEvent,
                           SynchronizationEvent,
                           FALSE);

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread(
                        &ThreadHandle,
                        THREAD_ALL_ACCESS,
                        &ObjectAttributes,
                        0L,
                        NULL,
                        KeBalanceSetManager,
                        NULL
                        ))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

        if (!NT_SUCCESS(PsCreateSystemThread(
                        &ThreadHandle,
                        THREAD_ALL_ACCESS,
                        &ObjectAttributes,
                        0L,
                        NULL,
                        KeSwapProcessOrStack,
                        NULL
                        ))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

#ifndef NO_POOL_CHECKS
        MiInitializeSpecialPoolCriteria ();
#endif

#if defined(_X86_)
        MiEnableKernelVerifier ();
#endif

        ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

        NextEntry = PsLoadedModuleList.Flink;

        for ( ; NextEntry != &PsLoadedModuleList; NextEntry = NextEntry->Flink) {

            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            NtHeaders = RtlImageNtHeader(DataTableEntry->DllBase);

            if ((NtHeaders->OptionalHeader.MajorOperatingSystemVersion >= 5) &&
                (NtHeaders->OptionalHeader.MajorImageVersion >= 5)) {
                DataTableEntry->Flags |= LDRP_ENTRY_NATIVE;
            }

            MiWriteProtectSystemImage (DataTableEntry->DllBase);
        }
        ExReleaseResourceLite (&PsLoadedModuleResource);

        InterlockedDecrement (&MiTrimInProgressCount);

        return TRUE;
    }

    if (Phase == 2) {
        MiEnablePagingTheExecutive();
        return TRUE;
    }

    return FALSE;
}

VOID
MiMapBBTMemory (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function walks through the loader block's memory descriptor list
    and maps memory reserved for the BBT buffer into the system.

    The mapped PTEs are PDE-aligned and made user accessible.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PVOID Va;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER NumberOfPagesMapped;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPde;
    MMPTE TempPte;

    if (BBTPagesToReserve <= 0) {
        return;
    }

    //
    // Request enough PTEs such that protection can be applied to the PDEs.
    //

    NumberOfPages = (BBTPagesToReserve + (PTE_PER_PAGE - 1)) & ~(PTE_PER_PAGE - 1);

    PointerPte = MiReserveAlignedSystemPtes ((ULONG)NumberOfPages,
                                             SystemPteSpace,
                                             MM_VA_MAPPED_BY_PDE);

    if (PointerPte == NULL) {
        BBTPagesToReserve = 0;
        return;
    }

    //
    // Allow user access to the buffer.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    LastPde = MiGetPteAddress (PointerPte + NumberOfPages);

    ASSERT (LastPde != PointerPde);

    do {
        TempPte = *PointerPde;
        TempPte.u.Long |= MM_PTE_OWNER_MASK;
        MI_WRITE_VALID_PTE (PointerPde, TempPte);
        PointerPde += 1;
    } while (PointerPde < LastPde);

    KeFlushEntireTb (TRUE, TRUE);

    Va = MiGetVirtualAddressMappedByPte (PointerPte);

    TempPte = ValidUserPte;
    NumberOfPagesMapped = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType == LoaderBBTMemory) {

            PageFrameIndex = MemoryDescriptor->BasePage;
            NumberOfPages = MemoryDescriptor->PageCount;

            if (NumberOfPagesMapped + NumberOfPages > BBTPagesToReserve) {
                NumberOfPages = BBTPagesToReserve - NumberOfPagesMapped;
            }

            NumberOfPagesMapped += NumberOfPages;

            do {

                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                PointerPte += 1;
                PageFrameIndex += 1;
                NumberOfPages -= 1;
            } while (NumberOfPages);

            if (NumberOfPagesMapped == BBTPagesToReserve) {
                break;
            }
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    RtlZeroMemory(Va, BBTPagesToReserve << PAGE_SHIFT);

    //
    // Tell BBT_Init how many pages were allocated.
    //

    if (NumberOfPagesMapped < BBTPagesToReserve) {
        BBTPagesToReserve = (ULONG)NumberOfPagesMapped;
    }
    *(PULONG)Va = BBTPagesToReserve;

    //
    // At this point instrumentation code will detect the existence of
    // buffer and initialize the structures.
    //

    BBTBuffer = Va;

    PERFINFO_MMINIT_START();
}


PPHYSICAL_MEMORY_DESCRIPTOR
MmInitializeMemoryLimits (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType,
    IN OUT PPHYSICAL_MEMORY_DESCRIPTOR InputMemory OPTIONAL
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and builds a list of contiguous physical
    memory blocks of the desired types.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in return.

    Memory - If non-NULL, supplies the physical memory blocks to place the
             search results in.  If NULL, pool is allocated to hold the
             returned search results in - the caller must free this pool.

Return Value:

    A pointer to the physical memory blocks for the requested search or NULL
    on failure.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PLIST_ENTRY NextMd;
    ULONG i;
    ULONG InitialAllocation;
    PFN_NUMBER NextPage;
    PFN_NUMBER TotalPages;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory2;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;

    InitialAllocation = 0;

    if (ARGUMENT_PRESENT (InputMemory)) {
        Memory = InputMemory;
    }
    else {

        //
        // The caller wants us to allocate the return result buffer.  Size it
        // by allocating the maximum possibly needed as this should not be
        // very big (relatively).  It is the caller's responsibility to free
        // this.  Obviously this option can only be requested after pool has
        // been initialized.
        //

        NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

        while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
            InitialAllocation += 1;
            MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                                 MEMORY_ALLOCATION_DESCRIPTOR,
                                                 ListEntry);
            NextMd = MemoryDescriptor->ListEntry.Flink;
        }

        Memory = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (InitialAllocation - 1),
                                        'lMmM');

        if (Memory == NULL) {
            return NULL;
        }

        Memory->NumberOfRuns = InitialAllocation;
    }

    //
    // Walk through the memory descriptors and build the physical memory list.
    //

    i = 0;
    TotalPages = 0;
    NextPage = (PFN_NUMBER) -1;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;

            //
            // Merge runs whenever possible.
            //

            if (MemoryDescriptor->BasePage == NextPage) {
                ASSERT (MemoryDescriptor->PageCount != 0);
                Memory->Run[i - 1].PageCount += MemoryDescriptor->PageCount;
                NextPage += MemoryDescriptor->PageCount;
            }
            else {
                Memory->Run[i].BasePage = MemoryDescriptor->BasePage;
                Memory->Run[i].PageCount = MemoryDescriptor->PageCount;
                NextPage = Memory->Run[i].BasePage + Memory->Run[i].PageCount;
                i += 1;
            }
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    ASSERT (i <= Memory->NumberOfRuns);

    if (i == 0) {

        //
        // Don't bother shrinking this as the caller will be freeing it
        // shortly as it is just an empty list.
        //

        Memory->Run[i].BasePage = 0;
        Memory->Run[i].PageCount = 0;
    }
    else if (!ARGUMENT_PRESENT (InputMemory)) {

        //
        // Shrink the buffer (if possible) now that the final size is known.
        //

        if (InitialAllocation > i) {
            Memory2 = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (i - 1),
                                            'lMmM');

            if (Memory2 != NULL) {
                RtlCopyMemory (Memory2->Run,
                               Memory->Run,
                               sizeof(PHYSICAL_MEMORY_RUN) * i);

                ExFreePool (Memory);
                Memory = Memory2;
            }
        }
    }

    Memory->NumberOfRuns = i;
    Memory->NumberOfPages = TotalPages;

    return Memory;
}


PFN_NUMBER
MiPagesInLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and returns the number of pages of the desired type.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in the returned count.

Return Value:

    The number of pages of the requested type in the loader block list.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER TotalPages;

    //
    // Walk through the memory descriptors counting pages.
    //

    TotalPages = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    return TotalPages;
}


static
VOID
MiMemoryLicense (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function walks through the loader block's memory descriptor list
    and based on the system's license, ensures only the proper amount of
    physical memory is used.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PLIST_ENTRY NextMd;
    PFN_NUMBER TotalPagesAllowed;
    PFN_NUMBER PageCount;
    ULONG VirtualBias;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;

    //
    // The default configuration gets a maximum of 4gb physical memory.
    // On PAE machines the system continues to operate in 8-byte PTE mode.
    //

    TotalPagesAllowed = MI_DEFAULT_MAX_PAGES;

    //
    // If properly licensed (ie: DataCenter) and booted without the
    // 3gb switch, then use all available physical memory.
    //

    if (ExVerifySuite(DataCenter) == TRUE) {

        //
        // Note MmVirtualBias has not yet been initialized at the time of the
        // first call to this routine, so use the LoaderBlock directly.
        //

#if defined(_X86_)
        VirtualBias = LoaderBlock->u.I386.VirtualBias;
#else
        VirtualBias = 0;
#endif

        if (VirtualBias == 0) {

            //
            // Limit the maximum physical memory to the amount we have
            // actually physically seen in a machine inhouse.
            //

            TotalPagesAllowed = MI_DTC_MAX_PAGES;
        }
        else {

            //
            // The system is booting /3gb, so don't use more than 16gb of
            // physical memory.  This ensures enough virtual space to map
            // the PFN database.
            //

            TotalPagesAllowed = MI_DTC_BOOTED_3GB_MAX_PAGES;
        }
    }
    else if ((MmProductType != 0x00690057) &&
             (ExVerifySuite(Enterprise) == TRUE)) {

        //
        // Enforce the Advanced Server physical memory limit.
        // On PAE machines the system continues to operate in 8-byte PTE mode.
        //

        TotalPagesAllowed = MI_ADS_MAX_PAGES;
    }
    else if (ExVerifySuite(Blade) == TRUE) {

        //
        // Enforce the Blade physical memory limit.
        //

        TotalPagesAllowed = MI_BLADE_MAX_PAGES;
    }

    //
    // Walk through the memory descriptors and remove or truncate descriptors
    // that exceed the maximum physical memory to be used.
    //

    PageCount = 0;
    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;
    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if ((MemoryDescriptor->MemoryType == LoaderFirmwarePermanent) ||
            (MemoryDescriptor->MemoryType == LoaderBBTMemory) ||
            (MemoryDescriptor->MemoryType == LoaderBad) ||
            (MemoryDescriptor->MemoryType == LoaderSpecialMemory)) {

                NextMd = MemoryDescriptor->ListEntry.Flink;
                continue;
        }

        PageCount += MemoryDescriptor->PageCount;

        if (PageCount <= TotalPagesAllowed) {
            NextMd = MemoryDescriptor->ListEntry.Flink;
            continue;
        }

        //
        // This descriptor needs to be removed or truncated.
        //

        if (PageCount - MemoryDescriptor->PageCount >= TotalPagesAllowed) {

            //
            // Completely remove this descriptor.
            //
            // Note since this only adjusts the links and since the entry is
            // not freed, it can still be safely referenced again below to
            // obtain the NextMd.  N.B.  This keeps the memory descriptors
            // sorted in ascending order.
            //

            RemoveEntryList (NextMd);
        }
        else {

            //
            // Truncate this descriptor.
            //

            ASSERT (PageCount - MemoryDescriptor->PageCount < TotalPagesAllowed);
            MemoryDescriptor->PageCount -= (ULONG)(PageCount - TotalPagesAllowed);
            PageCount = TotalPagesAllowed;
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    return;
}


VOID
MmFreeLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function is called as the last routine in phase 1 initialization.
    It frees memory used by the OsLoader.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PLIST_ENTRY NextMd;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    ULONG i;
    PFN_NUMBER NextPhysicalPage;
    PFN_NUMBER PagesFreed;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RUN RunBase;
    PPHYSICAL_MEMORY_RUN Runs;

    i = 0;
    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
        i += 1;
        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    RunBase = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(PHYSICAL_MEMORY_RUN) * i,
                                     'lMmM');

    if (RunBase == NULL) {
        return;
    }

    Runs = RunBase;

    //
    //
    // Walk through the memory descriptors and add pages to the
    // free list in the PFN database.
    //

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);


        switch (MemoryDescriptor->MemoryType) {
            case LoaderOsloaderHeap:
            case LoaderRegistryData:
            case LoaderNlsData:
            //case LoaderMemoryData:  //this has page table and other stuff.

                //
                // Capture the data to temporary storage so we won't
                // free memory we are referencing.
                //

                Runs->BasePage = MemoryDescriptor->BasePage;
                Runs->PageCount = MemoryDescriptor->PageCount;
                Runs += 1;

                break;

            default:

                break;
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    PagesFreed = 0;

    LOCK_PFN (OldIrql);

    if (Runs != RunBase) {
        Runs -= 1;
        do {
            i = (ULONG)Runs->PageCount;
            NextPhysicalPage = Runs->BasePage;

#if defined (_MI_MORE_THAN_4GB_)
            if (MiNoLowMemory != 0) {
                if (NextPhysicalPage < MiNoLowMemory) {

                    //
                    // Don't free this run as it is below the memory threshold
                    // configured for this system.
                    //

                    Runs -= 1;
                    continue;
                }
            }
#endif

            Pfn1 = MI_PFN_ELEMENT (NextPhysicalPage);
            PagesFreed += i;
            while (i != 0) {

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    if (Pfn1->u1.Flink == 0) {

                        //
                        // Set the PTE address to the physical page for
                        // virtual address alignment checking.
                        //

                        Pfn1->PteAddress =
                                   (PMMPTE)(NextPhysicalPage << PTE_SHIFT);

                        MiDetermineNode (NextPhysicalPage, Pfn1);

                        MiInsertPageInFreeList (NextPhysicalPage);
                    }
                }
                else {

                    if (NextPhysicalPage != 0) {
                        //
                        // Remove PTE and insert into the free list.  If it is
                        // a physical address within the PFN database, the PTE
                        // element does not exist and therefore cannot be updated.
                        //

                        if (!MI_IS_PHYSICAL_ADDRESS (
                                MiGetVirtualAddressMappedByPte (Pfn1->PteAddress))) {

                            //
                            // Not a physical address.
                            //

                            *(Pfn1->PteAddress) = ZeroPte;
                        }

                        MI_SET_PFN_DELETED (Pfn1);
                        MiDecrementShareCountOnly (NextPhysicalPage);
                    }
                }

                Pfn1 += 1;
                i -= 1;
                NextPhysicalPage += 1;
            }
            Runs -= 1;
        } while (Runs >= RunBase);
    }

    //
    // Since systemwide commitment was determined early in Phase 0 and
    // excluded the ranges just freed, add them back in now.
    //

    if (PagesFreed != 0) {
        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, PagesFreed);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, PagesFreed);
    }

#if defined(_X86_)

    if (MmVirtualBias != 0) {

        //
        // If the kernel has been biased to allow for 3gb of user address space,
        // then the first 16mb of memory is doubly mapped to KSEG0_BASE and to
        // ALTERNATE_BASE. Therefore, the KSEG0_BASE entries must be unmapped.
        //

        PMMPTE Pde;
        ULONG NumberOfPdes;

        NumberOfPdes = MmBootImageSize / MM_VA_MAPPED_BY_PDE;

        Pde = MiGetPdeAddress((PVOID)KSEG0_BASE);

        for (i = 0; i < NumberOfPdes; i += 1) {
            MI_WRITE_INVALID_PTE (Pde, ZeroKernelPte);
            Pde += 1;
        }
    }

#endif

    KeFlushEntireTb (TRUE, TRUE);
    UNLOCK_PFN (OldIrql);
    ExFreePool (RunBase);
    return;
}

VOID
MiBuildPagedPool (
    VOID
    )

/*++

Routine Description:

    This function is called to build the structures required for paged
    pool and initialize the pool.  Once this routine is called, paged
    pool may be allocated.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    SIZE_T Size;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE PointerPde;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER ContainingFrame;
    SIZE_T AdditionalCommittedPages;
    KIRQL OldIrql;
    ULONG i;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PMMPTE PointerPxeEnd;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID LastVa;
    PMMPTE PointerPpe;
    PMMPTE PointerPpeEnd;
#else
    PMMPFN Pfn1;
#endif

    i = 0;
    AdditionalCommittedPages = 0;

#if (_MI_PAGING_LEVELS < 3)

    //
    // Double map system page directory page.
    //

    PointerPte = MiGetPteAddress(PDE_BASE);

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        MmSystemPageDirectory[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT(MmSystemPageDirectory[i]);
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
        PointerPte += 1;
    }

    //
    // Was not mapped physically, map it virtually in system space.
    //

    PointerPte = MiReserveSystemPtes (PD_PER_SYSTEM, SystemPteSpace);

    if (PointerPte == NULL) {
        MiIssueNoPtesBugcheck (PD_PER_SYSTEM, SystemPteSpace);
    }

    MmSystemPagePtes = (PMMPTE)MiGetVirtualAddressMappedByPte (PointerPte);

    TempPte = ValidKernelPde;

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        TempPte.u.Hard.PageFrameNumber = MmSystemPageDirectory[i];
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
    }

#endif

    if (MmPagedPoolMaximumDesired == TRUE) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }
    else if (MmSizeOfPagedPoolInBytes == 0) {

        //
        // A size of 0 means size the pool based on physical memory.
        //

        MmSizeOfPagedPoolInBytes = 2 * MmMaximumNonPagedPoolInBytes;
#if (_MI_PAGING_LEVELS >= 3)
        MmSizeOfPagedPoolInBytes *= 2;
#endif
    }

    if (MmIsThisAnNtAsSystem()) {
        if ((MmNumberOfPhysicalPages > ((24*1024*1024) >> PAGE_SHIFT)) &&
            (MmSizeOfPagedPoolInBytes < MM_MINIMUM_PAGED_POOL_NTAS)) {

            MmSizeOfPagedPoolInBytes = MM_MINIMUM_PAGED_POOL_NTAS;
        }
    }

    if (MmSizeOfPagedPoolInBytes >
              (ULONG_PTR)((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart)) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }

    Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);

    if (Size < MM_MIN_INITIAL_PAGED_POOL) {
        Size = MM_MIN_INITIAL_PAGED_POOL;
    }

    if (Size > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        Size = MM_MAX_PAGED_POOL >> PAGE_SHIFT;
    }

#if defined (_WIN64)

    //
    // NT64 places system mapped views directly after paged pool.  Ensure
    // enough VA space is available.
    //

    if (Size + (MmSystemViewSize >> PAGE_SHIFT) > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        ASSERT (MmSizeOfPagedPoolInBytes > 2 * MmSystemViewSize);
        MmSizeOfPagedPoolInBytes -= MmSystemViewSize;
        Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);
    }
#endif

    Size = (Size + (PTE_PER_PAGE - 1)) / PTE_PER_PAGE;
    MmSizeOfPagedPoolInBytes = (ULONG_PTR)Size * PAGE_SIZE * PTE_PER_PAGE;

    //
    // Set size to the number of pages in the pool.
    //

    Size = Size * PTE_PER_PAGE;

    //
    // If paged pool is really nonpagable then limit the size based
    // on how much physical memory is actually present.  Disable this
    // feature if not enough physical memory is present to do it.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        Size = MmSizeOfPagedPoolInBytes / PAGE_SIZE;

        if ((MI_NONPAGABLE_MEMORY_AVAILABLE() < 2048) ||
            (MmAvailablePages < 2048)) {
                Size = 0;
        }
        else {
            if ((SPFN_NUMBER)(Size) > MI_NONPAGABLE_MEMORY_AVAILABLE() - 2048) {
                Size = (MI_NONPAGABLE_MEMORY_AVAILABLE() - 2048);
            }

            if (Size > MmAvailablePages - 2048) {
                Size = MmAvailablePages - 2048;
            }
        }

        Size = ((Size * PAGE_SIZE) / MM_VA_MAPPED_BY_PDE) * MM_VA_MAPPED_BY_PDE;

        if ((((Size / 5) * 4) >= MmSizeOfPagedPoolInBytes) &&
            (Size >= MM_MIN_INITIAL_PAGED_POOL)) {

            MmSizeOfPagedPoolInBytes = Size;
        }
        else {
            MmDisablePagingExecutive &= ~MM_PAGED_POOL_LOCKED_DOWN;
        }

        Size = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;
    }

    ASSERT ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart) <=
            (PCHAR)MmNonPagedSystemStart);

    ASSERT64 ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart + MmSystemViewSize) <=
              (PCHAR)MmNonPagedSystemStart);

    MmPagedPoolEnd = (PVOID)(((PUCHAR)MmPagedPoolStart +
                            MmSizeOfPagedPoolInBytes) - 1);

    MmPageAlignedPoolBase[PagedPool] = MmPagedPoolStart;

    //
    // Build page table page for paged pool.
    //

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

    TempPte = ValidKernelPde;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Map in all the page directory pages to span all of paged pool.
    // This removes the need for a system lookup directory.
    //

    LastVa = (PVOID)((PCHAR)MmPagedPoolEnd + MmSystemViewSize);
    PointerPpe = MiGetPpeAddress (MmPagedPoolStart);
    PointerPpeEnd = MiGetPpeAddress (LastVa);

    MiSystemViewStart = (ULONG_PTR)MmPagedPoolEnd + 1;

    PointerPde = MiGetPdeAddress (MmPagedPoolEnd) + 1;
    LastPde = MiGetPdeAddress (LastVa);

    LOCK_PFN (OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
    PointerPxe = MiGetPxeAddress (MmPagedPoolStart);
    PointerPxeEnd = MiGetPxeAddress (LastVa);

    while (PointerPxe <= PointerPxeEnd) {

        if (PointerPxe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveAnyPage(
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPxe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPxe, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPxe, 1);

            //
            // Make all entries no access since the PDEs may not fill the page.
            //

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPxe),
                             PAGE_SIZE,
                             MM_KERNEL_NOACCESS_PTE);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPxe += 1;
    }
#endif

    while (PointerPpe <= PointerPpeEnd) {

        if (PointerPpe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveAnyPage(
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPpe, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPpe, 1);

            //
            // Make all entries no access since the PDEs may not fill the page.
            //

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPpe),
                             PAGE_SIZE,
                             MM_KERNEL_NOACCESS_PTE);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPpe += 1;
    }

    //
    // Initialize the system view page table pages.
    //

    MmResidentAvailablePages -= (LastPde - PointerPde + 1);
    AdditionalCommittedPages += (LastPde - PointerPde + 1);

    while (PointerPde <= LastPde) {

        ASSERT (PointerPde->u.Hard.Valid == 0);

        PageFrameIndex = MiRemoveAnyPage(
                            MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (PointerPde, TempPte);

        MiInitializePfn (PageFrameIndex, PointerPde, 1);

        MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPde),
                        PAGE_SIZE,
                        ZeroKernelPte.u.Long);

        PointerPde += 1;
    }

    UNLOCK_PFN (OldIrql);

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

#endif

    PointerPte = MiGetPteAddress (MmPagedPoolStart);
    MmPagedPoolInfo.FirstPteForPagedPool = PointerPte;
    MmPagedPoolInfo.LastPteForPagedPool = MiGetPteAddress (MmPagedPoolEnd);

    MiFillMemoryPte (PointerPde,
                     sizeof(MMPTE) *
                         (1 + MiGetPdeAddress (MmPagedPoolEnd) - PointerPde),
                     MM_KERNEL_NOACCESS_PTE);

    LOCK_PFN (OldIrql);

    //
    // Map in a page table page.
    //

    PageFrameIndex = MiRemoveAnyPage(
                            MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
    ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPpeAddress (MmPagedPoolStart));
#else
    ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

    MiInitializePfnForOtherProcess (PageFrameIndex,
                                    PointerPde,
                                    ContainingFrame);

    MiFillMemoryPte (PointerPte, PAGE_SIZE, MM_KERNEL_NOACCESS_PTE);

    UNLOCK_PFN (OldIrql);

    MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde + 1;

    //
    // Build bitmaps for paged pool.
    //

    MiCreateBitMap (&MmPagedPoolInfo.PagedPoolAllocationMap, Size, NonPagedPool);
    RtlSetAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

    //
    // Indicate first page worth of PTEs are available.
    //

    RtlClearBits (MmPagedPoolInfo.PagedPoolAllocationMap, 0, PTE_PER_PAGE);

    MiCreateBitMap (&MmPagedPoolInfo.EndOfPagedPoolBitmap, Size, NonPagedPool);
    RtlClearAllBits (MmPagedPoolInfo.EndOfPagedPoolBitmap);

    //
    // If verifier is present then build the verifier paged pool bitmap.
    //

    if (MmVerifyDriverBufferLength != (ULONG)-1) {
        MiCreateBitMap (&VerifierLargePagedPoolMap, Size, NonPagedPool);
        RtlClearAllBits (VerifierLargePagedPoolMap);
    }

    //
    // Initialize paged pool.
    //

    InitializePool (PagedPool, 0L);

    //
    // If paged pool is really nonpagable then allocate the memory now.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        PointerPde = MiGetPdeAddress (MmPagedPoolStart);
        PointerPde += 1;
        LastPde = MiGetPdeAddress (MmPagedPoolEnd);
        TempPte = ValidKernelPde;

        PointerPte = MiGetPteAddress (MmPagedPoolStart);
        LastPte = MiGetPteAddress (MmPagedPoolEnd);

        ASSERT (MmPagedPoolCommit == 0);
        MmPagedPoolCommit = (ULONG)(LastPte - PointerPte + 1);

        ASSERT (MmPagedPoolInfo.PagedPoolCommit == 0);
        MmPagedPoolInfo.PagedPoolCommit = MmPagedPoolCommit;

#if DBG
        //
        // Ensure no paged pool has been allocated yet.
        //

        for (i = 0; i < PTE_PER_PAGE; i += 1) {
            ASSERT (!RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
        }

        while (i < MmSizeOfPagedPoolInBytes / PAGE_SIZE) {
            ASSERT (RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
            i += 1;
        }
#endif

        RtlClearAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

        LOCK_PFN (OldIrql);

        //
        // Map in the page table pages.
        //

        MmResidentAvailablePages -= (LastPde - PointerPde + 1);
        AdditionalCommittedPages += (LastPde - PointerPde + 1);

        while (PointerPde <= LastPde) {

            ASSERT (PointerPde->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
            ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPteAddress (PointerPde));
#else
            ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

            MiInitializePfnForOtherProcess (PageFrameIndex,
                                            MiGetPteAddress (PointerPde),
                                            ContainingFrame);

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPde),
                             PAGE_SIZE,
                             MM_KERNEL_NOACCESS_PTE);

            PointerPde += 1;
        }

        MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde;

        TempPte = ValidKernelPte;
        MI_SET_PTE_DIRTY (TempPte);

        ASSERT (MmAvailablePages > (PFN_COUNT)(LastPte - PointerPte + 1));
        ASSERT (MmResidentAvailablePages > (SPFN_NUMBER)(LastPte - PointerPte + 1));
        MmResidentAvailablePages -= (LastPte - PointerPte + 1);
        AdditionalCommittedPages += (LastPte - PointerPte + 1);

        while (PointerPte <= LastPte) {

            ASSERT (PointerPte->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPte, 1);

            PointerPte += 1;
        }

        UNLOCK_PFN (OldIrql);
    }

    //
    // Since the commitment return path is lock free, the total committed
    // page count must be atomically incremented.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommittedPages, AdditionalCommittedPages);

    MiInitializeSpecialPool (NonPagedPool);

    //
    // Allow mapping of views into system space.
    //

    MiInitializeSystemSpaceMap (NULL);

    return;
}


VOID
MiFindInitializationCode (
    OUT PVOID *StartVa,
    OUT PVOID *EndVa
    )

/*++

Routine Description:

    This function locates the start and end of the kernel initialization
    code.  This code resides in the "init" section of the kernel image.

Arguments:

    StartVa - Returns the starting address of the init section.

    EndVa - Returns the ending address of the init section.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PVOID InitStart;
    PVOID InitEnd;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    PIMAGE_SECTION_HEADER LastDiscard;
    LONG i;
    LOGICAL DiscardSection;
    PVOID MiFindInitializationCodeAddress;
    PKTHREAD CurrentThread;

    MiFindInitializationCodeAddress = MmGetProcedureAddress((PVOID)(ULONG_PTR)&MiFindInitializationCode);

#if defined(_IA64_)

    //
    // One more indirection is needed due to the PLABEL.
    //

    MiFindInitializationCodeAddress = (PVOID)(*((PULONGLONG)MiFindInitializationCodeAddress));

#endif

    *StartVa = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);
    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {
        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->SectionPointer != NULL) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already
            // had its init section removed.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;
        NtHeader = RtlImageNtHeader(CurrentBase);

        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the sections named 'INIT',
        // PAGEVRF* and PAGESPEC.  INIT always goes, the others go depending
        // on registry configuration.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        InitStart = NULL;
        while (i > 0) {

#if DBG
            if ((*(PULONG)SectionTableEntry->Name == 'tini') ||
                (*(PULONG)SectionTableEntry->Name == 'egap')) {
                DbgPrint("driver %wZ has lower case sections (init or pagexxx)\n",
                    &LdrDataTableEntry->FullDllName);
            }
#endif

            DiscardSection = FALSE;

            //
            // Free any INIT sections (or relocation sections that haven't
            // been already).  Note a driver may have a relocation section
            // but not have any INIT code.
            //

            if ((*(PULONG)SectionTableEntry->Name == 'TINI') ||
                ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0)) {
                DiscardSection = TRUE;
            }
            else if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                     (SectionTableEntry->Name[4] == 'V') &&
                     (SectionTableEntry->Name[5] == 'R') &&
                     (SectionTableEntry->Name[6] == 'F')) {

                //
                // Discard PAGEVRF* if no drivers are being instrumented.
                //

                if (MmVerifyDriverBufferLength == (ULONG)-1) {
                    DiscardSection = TRUE;
                }
            }
            else if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS')) {

                //
                // Discard PAGESPEC special pool code if it's not enabled.
                //

                if (MiSpecialPoolFirstPte == NULL) {
                    DiscardSection = TRUE;
                }
            }

            if (DiscardSection == TRUE) {

                InitStart = (PVOID)((PCHAR)CurrentBase + SectionTableEntry->VirtualAddress);
                InitEnd = (PVOID)((PCHAR)InitStart + SectionTableEntry->SizeOfRawData - 1);
                InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                        (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                InitStart = (PVOID)ROUND_TO_PAGES (InitStart);

                //
                // Check if more sections are discardable after this one so
                // even small INIT sections can be discarded.
                //

                if (i == 1) {
                    LastDiscard = SectionTableEntry;
                }
                else {
                    LastDiscard = NULL;
                    do {
                        i -= 1;
                        SectionTableEntry += 1;

                        if ((SectionTableEntry->Characteristics &
                             IMAGE_SCN_MEM_DISCARDABLE) != 0) {

                            //
                            // Discard this too.
                            //

                            LastDiscard = SectionTableEntry;
                        }
                        else {
                            break;
                        }
                    } while (i > 1);
                }

                if (LastDiscard) {
                    InitEnd = (PVOID)(((PCHAR)CurrentBase +
                                       LastDiscard->VirtualAddress) +
                                      (LastDiscard->SizeOfRawData - 1));

                    //
                    // If this isn't the last section in the driver then the
                    // the next section is not discardable.  So the last
                    // section is not rounded down, but all others must be.
                    //

                    if (i != 1) {
                        InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                                                             (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                    }
                }

                if (InitEnd > (PVOID)((PCHAR)CurrentBase +
                                      LdrDataTableEntry->SizeOfImage)) {
                    InitEnd = (PVOID)(((ULONG_PTR)CurrentBase +
                                       (LdrDataTableEntry->SizeOfImage - 1)) |
                                      (PAGE_SIZE - 1));
                }

                if (InitStart <= InitEnd) {
                    if ((MiFindInitializationCodeAddress >= InitStart) &&
                        (MiFindInitializationCodeAddress <= InitEnd)) {

                        //
                        // This init section is in the kernel, don't free it
                        // now as it would free this code!
                        //

                        ASSERT (*StartVa == NULL);
                        *StartVa = InitStart;
                        *EndVa = InitEnd;
                    }
                    else {
                        MiFreeInitializationCode (InitStart, InitEnd);
                    }
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }
        Next = Next->Flink;
    }
    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);
    return;
}


VOID
MiFreeInitializationCode (
    IN PVOID StartVa,
    IN PVOID EndVa
    )

/*++

Routine Description:

    This function is called to delete the initialization code.

Arguments:

    StartVa - Supplies the starting address of the range to delete.

    EndVa - Supplies the ending address of the range to delete.

Return Value:

    None.

Environment:

    Kernel Mode Only.  Runs after system initialization.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PagesFreed;
    KIRQL OldIrql;

    ASSERT(ExPageLockHandle);

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {

        //
        // Don't free this range as the kernel is always below the memory
        // threshold configured for this system.
        //

        return;
    }
#endif

    PagesFreed = 0;
    MmLockPagableSectionByHandle(ExPageLockHandle);

    if (MI_IS_PHYSICAL_ADDRESS(StartVa)) {
        LOCK_PFN (OldIrql);
        while (StartVa < EndVa) {

            //
            // On certain architectures (e.g., MIPS) virtual addresses
            // may be physical and hence have no corresponding PTE.
            //

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (StartVa);

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            Pfn1->u2.ShareCount = 0;
            Pfn1->u3.e2.ReferenceCount = 0;
            MI_SET_PFN_DELETED (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
            StartVa = (PVOID)((PUCHAR)StartVa + PAGE_SIZE);
            PagesFreed += 1;
        }
        UNLOCK_PFN (OldIrql);
    }
    else {
        PointerPte = MiGetPteAddress (StartVa);
        PagesFreed = MiDeleteSystemPagableVm (PointerPte,
                                 (PFN_NUMBER) (1 + MiGetPteAddress (EndVa) -
                                     PointerPte),
                                 ZeroKernelPte,
                                 FALSE,
                                 NULL);
    }
    MmUnlockPagableImageSection(ExPageLockHandle);

    //
    // Since systemwide commitment was determined early in Phase 0 and
    // excluded the ranges just freed, add them back in now.
    //

    if (PagesFreed != 0) {

        //
        // Since systemwide commitment was determined early in Phase 0
        // and excluded the ranges just freed, increase the limits
        // accordingly now.  Note that there is no commitment to be
        // returned (as none was ever charged earlier) for boot
        // loaded drivers.
        //

        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, PagesFreed);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, PagesFreed);
    }

    return;
}


VOID
MiEnablePagingTheExecutive (
    VOID
    )

/*++

Routine Description:

    This function locates the start and end of the kernel initialization
    code.  This code resides in the "init" section of the kernel image.

Arguments:

    StartVa - Returns the starting address of the init section.

    EndVa - Returns the ending address of the init section.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    PVOID StartVa;
    PETHREAD CurrentThread;
    PLONG SectionLockCountPointer;
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER StartSectionTableEntry;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    LONG i;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE SubsectionStartPte;
    PMMPTE SubsectionLastPte;
    LOGICAL PageSection;
    PVOID SectionBaseAddress;
    LOGICAL AlreadyLockedOnce;
    ULONG Waited;
    PEPROCESS CurrentProcess;

    //
    // Don't page kernel mode code if customer does not want it paged or if
    // this is a diskless remote boot client.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

#if defined(REMOTE_BOOT)
    if (IoRemoteBootClient && IoCscInitializationFailed) {
        return;
    }
#endif

    //
    // Initializing LastPte is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    LastPte = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);
    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);
    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {

        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->SectionPointer != NULL) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already paged.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;

        if (MI_IS_PHYSICAL_ADDRESS (CurrentBase)) {

            //
            // Mapped physically, can't be paged.
            //

            Next = Next->Flink;
            continue;
        }

        NtHeader = RtlImageNtHeader (CurrentBase);

restart:

        StartSectionTableEntry = NULL;
        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the section named 'PAGE' or '.edata'.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        PointerPte = NULL;

        while (i > 0) {

            SectionBaseAddress = SECTION_BASE_ADDRESS(SectionTableEntry);

            if ((PUCHAR)SectionBaseAddress ==
                            ((PUCHAR)CurrentBase + SectionTableEntry->VirtualAddress)) {
                AlreadyLockedOnce = TRUE;

                //
                // This subsection has already been locked down (and possibly
                // unlocked as well) at least once.  If it is NOT locked down
                // right now and the pages are not in the system working set
                // then include it in the chunk to be paged.
                //

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (SectionTableEntry);

                if (*SectionLockCountPointer == 0) {

                    SubsectionStartPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));

                    SubsectionLastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                                 SectionTableEntry->VirtualAddress +
                                 (NtHeader->OptionalHeader.SectionAlignment - 1) +
                                 SectionTableEntry->SizeOfRawData -
                                 PAGE_SIZE));

                    if (SubsectionLastPte >= SubsectionStartPte) {
                        AlreadyLockedOnce = FALSE;
                    }
                }
            }
            else {
                AlreadyLockedOnce = FALSE;
            }

            PageSection = ((*(PULONG)SectionTableEntry->Name == 'EGAP') ||
                          (*(PULONG)SectionTableEntry->Name == 'ade.')) &&
                           (AlreadyLockedOnce == FALSE);

            if (*(PULONG)SectionTableEntry->Name == 'EGAP' &&
                SectionTableEntry->Name[4] == 'K'  &&
                SectionTableEntry->Name[5] == 'D') {

                //
                // Only pageout PAGEKD if KdPitchDebugger is TRUE.
                //

                PageSection = KdPitchDebugger;
            }

            if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                     (SectionTableEntry->Name[4] == 'V') &&
                     (SectionTableEntry->Name[5] == 'R') &&
                     (SectionTableEntry->Name[6] == 'F')) {

                //
                // Pageout PAGEVRF* if no drivers are being instrumented.
                //

                if (MmVerifyDriverBufferLength != (ULONG)-1) {
                    PageSection = FALSE;
                }
            }

            if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS')) {

                //
                // Pageout PAGESPEC special pool code if it's not enabled.
                //

                if (MiSpecialPoolFirstPte != NULL) {
                    PageSection = FALSE;
                }
            }

            if (PageSection) {

                 //
                 // This section is pagable, save away the start and end.
                 //

                 if (PointerPte == NULL) {

                     //
                     // Previous section was NOT pagable, get the start address.
                     //

                     ASSERT (StartSectionTableEntry == NULL);
                     StartSectionTableEntry = SectionTableEntry;
                     PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));
                 }
                 LastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                             SectionTableEntry->VirtualAddress +
                             (NtHeader->OptionalHeader.SectionAlignment - 1) +
                             SectionTableEntry->SizeOfRawData -
                             PAGE_SIZE));
            }
            else {

                //
                // This section is not pagable, if the previous section was
                // pagable, enable it.
                //

                if (PointerPte != NULL) {

                    ASSERT (StartSectionTableEntry != NULL);
                    LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
                    LOCK_PFN (OldIrql);

                    StartVa = PAGE_ALIGN (StartSectionTableEntry);
                    while (StartVa < (PVOID) SectionTableEntry) {
                        Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa);
                        if (Waited != 0) {

                            //
                            // Restart at the top as the locks were released.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS (OldIrqlWs);
                            goto restart;
                        }
                        StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
                    }

                    //
                    // Now that we're holding the proper locks, rewalk all
                    // the sections to make sure they weren't locked down
                    // after we checked above.
                    //

                    while (StartSectionTableEntry < SectionTableEntry) {
                        SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                        SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                        if (((PUCHAR)SectionBaseAddress ==
                                        ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                        (*SectionLockCountPointer != 0)) {

                            //
                            // Restart at the top as the section has been
                            // explicitly locked by a driver since we first
                            // checked above.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS (OldIrqlWs);
                            goto restart;
                        }
                        StartSectionTableEntry += 1;
                    }

                    MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (OldIrqlWs);

                    PointerPte = NULL;
                    StartSectionTableEntry = NULL;
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }

        if (PointerPte != NULL) {
            ASSERT (StartSectionTableEntry != NULL);
            LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
            LOCK_PFN (OldIrql);

            StartVa = PAGE_ALIGN (StartSectionTableEntry);
            while (StartVa < (PVOID) SectionTableEntry) {
                Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa);
                if (Waited != 0) {

                    //
                    // Restart at the top as the locks were released.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (OldIrqlWs);
                    goto restart;
                }
                StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
            }

            //
            // Now that we're holding the proper locks, rewalk all
            // the sections to make sure they weren't locked down
            // after we checked above.
            //

            while (StartSectionTableEntry < SectionTableEntry) {
                SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                if (((PUCHAR)SectionBaseAddress ==
                                ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                (*SectionLockCountPointer != 0)) {

                    //
                    // Restart at the top as the section has been
                    // explicitly locked by a driver since we first
                    // checked above.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS (OldIrqlWs);
                    goto restart;
                }
                StartSectionTableEntry += 1;
            }
            MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS (OldIrqlWs);
        }

        Next = Next->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    return;
}


VOID
MiEnablePagingOfDriverAtInit (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This routine marks the specified range of PTEs as pagable.

Arguments:

    PointerPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

Return Value:

    None.

Environment:

    Working set mutex AND PFN lock held.

--*/

{
    PVOID Base;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    MMPTE TempPte;
    LOGICAL SessionAddress;

    MM_PFN_LOCK_ASSERT();

    Base = MiGetVirtualAddressMappedByPte (PointerPte);
    SessionAddress = MI_IS_SESSION_PTE (PointerPte);

    while (PointerPte <= LastPte) {

        //
        // The PTE must be carefully checked as drivers may call MmPageEntire
        // during their DriverEntry yet faults may occur prior to this routine
        // running which cause pages to already be resident and in the working
        // set at this point.  So checks for validity and wsindex must be
        // applied.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn->u2.ShareCount == 1);

            if (Pfn->u1.WsIndex == 0) {

                //
                // Set the working set index to zero.  This allows page table
                // pages to be brought back in with the proper WSINDEX.
                //

                MI_ZERO_WSINDEX (Pfn);

                //
                // Original PTE may need to be set for drivers loaded via
                // ntldr.
                //

                if (Pfn->OriginalPte.u.Long == 0) {
                    Pfn->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
                    Pfn->OriginalPte.u.Soft.Protection |= MM_EXECUTE;
                }

                MI_SET_MODIFIED (Pfn, 1, 0x11);

                TempPte = *PointerPte;

                MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                              Pfn->OriginalPte.u.Soft.Protection);

                KeFlushSingleTb (Base,
                                 TRUE,
                                 TRUE,
                                 (PHARDWARE_PTE)PointerPte,
                                 TempPte.u.Flush);

                //
                // Flush the translation buffer and decrement the number of valid
                // PTEs within the containing page table page.  Note that for a
                // private page, the page table page is still needed because the
                // page is in transition.
                //

                MiDecrementShareCount (PageFrameIndex);

                MmResidentAvailablePages += 1;
                MmTotalSystemCodePages += 1;
            }
            else {

                //
                // This would need to be taken out of the WSLEs so skip it for
                // now and let the normal paging algorithms remove it if we
                // run into memory pressure.
                //
            }

        }
        Base = (PVOID)((PCHAR)Base + PAGE_SIZE);
        PointerPte += 1;
    }

    if (SessionAddress == TRUE) {

        //
        // Session space has no ASN - flush the entire TB.
        //

        MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
    }

    return;
}


MM_SYSTEMSIZE
MmQuerySystemSize(
    VOID
    )
{
    //
    // 12Mb  is small
    // 12-19 is medium
    // > 19 is large
    //
    return MmSystemSize;
}

NTKERNELAPI
BOOLEAN
MmIsThisAnNtAsSystem(
    VOID
    )
{
    return (BOOLEAN)MmProductType;
}

NTKERNELAPI
VOID
FASTCALL
MmSetPageFaultNotifyRoutine(
    PPAGE_FAULT_NOTIFY_ROUTINE NotifyRoutine
    )
{
    MmPageFaultNotifyRoutine = NotifyRoutine;
}

#ifdef _MI_MESSAGE_SERVER

LIST_ENTRY MiMessageInfoListHead;
KSPIN_LOCK MiMessageLock;
KEVENT MiMessageEvent;
ULONG MiMessageCount;

VOID
MiInitializeMessageQueue (
    VOID
    )
{
    MiMessageCount = 0;
    InitializeListHead (&MiMessageInfoListHead);
    KeInitializeSpinLock (&MiMessageLock);

    //
    // Use a synchronization event so the event's signal state is
    // auto cleared on a successful wait.
    //

    KeInitializeEvent (&MiMessageEvent, SynchronizationEvent, FALSE);
}

LOGICAL
MiQueueMessage (
    IN PVOID Message
    )
{
    KIRQL OldIrql;
    LOGICAL Enqueued;

    Enqueued = TRUE;
    ExAcquireSpinLock (&MiMessageLock, &OldIrql);

    if (MiMessageCount <= 500) {
        InsertTailList (&MiMessageInfoListHead, (PLIST_ENTRY)Message);
        MiMessageCount += 1;
    }
    else {
        Enqueued = FALSE;
    }

    ExReleaseSpinLock (&MiMessageLock, OldIrql);

    if (Enqueued == TRUE) {
        KeSetEvent (&MiMessageEvent, 0, FALSE);
    }
    else {
        ExFreePool (Message);
    }

    return Enqueued;
}

//
// sr: free the pool when done.
//

PVOID
MiRemoveMessage (
    VOID
    )
{
    PVOID Message;
    KIRQL OldIrql;
    NTSTATUS Status;

    Message = NULL;

    //
    // N.B.  waiting with a timeout and return so caller can support unload.
    //

    Status = KeWaitForSingleObject (&MiMessageEvent,
                                    WrVirtualMemory,
                                    KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER) &MmTwentySeconds);

    if (Status != STATUS_TIMEOUT) {

        ExAcquireSpinLock (&MiMessageLock, &OldIrql);

        if (!IsListEmpty (&MiMessageInfoListHead)) {

            Message = (PVOID)RemoveHeadList(&MiMessageInfoListHead);
            MiMessageCount -= 1;

            if (!IsListEmpty (&MiMessageInfoListHead)) {

                //
                // The list still contains entries so undo the event autoreset.
                //

                KeSetEvent (&MiMessageEvent, 0, FALSE);
            }
        }

        ExReleaseSpinLock (&MiMessageLock, OldIrql);
    }

    return Message;
}

#endif

#define CONSTANT_UNICODE_STRING(s)   { sizeof( s ) - sizeof( WCHAR ), sizeof( s ), s }

LOGICAL
MiInitializeMemoryEvents (
    VOID
    )
{
    KIRQL OldIrql;
    NTSTATUS Status;
    UNICODE_STRING LowMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowMemoryCondition");
    UNICODE_STRING HighMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighMemoryCondition");

    //
    // The thresholds may be set in the registry, if so, they are interpreted
    // in megabytes so convert them to pages now.
    //
    // If the user modifies the registry to introduce his own values, don't
    // bother error checking them as they can't hurt the system regardless (bad
    // values just may result in events not getting signaled or staying
    // signaled when they shouldn't, but that's not fatal).
    //

    if (MmLowMemoryThreshold != 0) {
        MmLowMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {

        //
        // Scale the threshold so on servers the low threshold is
        // approximately 32MB per 4GB, capping it at 64MB.
        //

        MmLowMemoryThreshold = MmPlentyFreePages;

        if (MmNumberOfPhysicalPages > 0x40000) {
            MmLowMemoryThreshold = (32 * 1024 * 1024) / PAGE_SIZE;
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x40000) >> 7);
        }
        else if (MmNumberOfPhysicalPages > 0x8000) {
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x8000) >> 5);
        }

        if (MmLowMemoryThreshold > (64 * 1024 * 1024) / PAGE_SIZE) {
            MmLowMemoryThreshold = (64 * 1024 * 1024) / PAGE_SIZE;
        }
    }

    if (MmHighMemoryThreshold != 0) {
        MmHighMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {
        MmHighMemoryThreshold = 3 * MmLowMemoryThreshold;
        ASSERT (MmHighMemoryThreshold > MmLowMemoryThreshold);
    }

    if (MmHighMemoryThreshold < MmLowMemoryThreshold) {
        MmHighMemoryThreshold = MmLowMemoryThreshold;
    }

    Status = MiCreateMemoryEvent (&LowMem, &MiLowMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighMem, &MiHighMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    //
    // Initialize the event values.
    //

    LOCK_PFN (OldIrql);

    MiNotifyMemoryEvents ();

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

extern POBJECT_TYPE ExEventObjectType;

NTSTATUS
MiCreateMemoryEvent (
    IN PUNICODE_STRING EventName,
    OUT PKEVENT *Event
    )
{
    PACL Dacl;
    HANDLE EventHandle;
    ULONG DaclLength;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES ObjectAttributes;
    SECURITY_DESCRIPTOR SecurityDescriptor;

    Status = RtlCreateSecurityDescriptor (&SecurityDescriptor,
                                          SECURITY_DESCRIPTOR_REVISION);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    DaclLength = sizeof (ACL) + sizeof (ACCESS_ALLOWED_ACE) * 3 +
                 RtlLengthSid (SeLocalSystemSid) +
                 RtlLengthSid (SeAliasAdminsSid) +
                 RtlLengthSid (SeWorldSid);

    Dacl = ExAllocatePoolWithTag (PagedPool, DaclLength, 'lcaD');

    if (Dacl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Status = RtlCreateAcl (Dacl, DaclLength, ACL_REVISION);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeAliasAdminsSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeLocalSystemSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     SYNCHRONIZE|EVENT_QUERY_STATE|READ_CONTROL,
                                     SeWorldSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }
  
    Status = RtlSetDaclSecurityDescriptor (&SecurityDescriptor,
                                           TRUE,
                                           Dacl,
                                           FALSE);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }
  
    InitializeObjectAttributes (&ObjectAttributes,
                                EventName,
                                OBJ_KERNEL_HANDLE | OBJ_PERMANENT,
                                NULL,
                                &SecurityDescriptor);

    Status = ZwCreateEvent (&EventHandle,
                            EVENT_ALL_ACCESS,
                            &ObjectAttributes,
                            NotificationEvent,
                            FALSE);

    ExFreePool (Dacl);

    if (NT_SUCCESS (Status)) {
        Status = ObReferenceObjectByHandle (EventHandle,
                                            EVENT_MODIFY_STATE,
                                            ExEventObjectType,
                                            KernelMode,
                                            (PVOID *)Event,
                                            NULL);
    }

    ZwClose (EventHandle);

    return Status;
}

VOID
MiNotifyMemoryEvents (
    VOID
    )
// PFN lock is held.
{
    if (MmAvailablePages <= MmLowMemoryThreshold) {

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) == 0) {
            KeSetEvent (MiLowMemoryEvent, 0, FALSE);
        }
    }
    else if (MmAvailablePages < MmHighMemoryThreshold) {

        //
        // Gray zone, make sure both events are cleared.
        //

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }
    else {
        if (KeReadStateEvent (MiHighMemoryEvent) == 0) {
            KeSetEvent (MiHighMemoryEvent, 0, FALSE);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }

    return;
}

VOID
MiInitializeCacheOverrides (
    VOID
    )
{
#if defined (_WIN64)

    ULONG NumberOfBytes;
    NTSTATUS Status;
    HAL_PLATFORM_INFORMATION Information;

    //
    // Gather platform information from the HAL.
    //

    Status = HalQuerySystemInformation (HalPlatformInformation, 
                                        sizeof (Information),
                                        &Information,
                                        &NumberOfBytes);

    if (!NT_SUCCESS (Status)) {
        return;
    }

    //
    // Apply mapping modifications based on platform information flags.
    //
    // It would be better if the platform returned what the new cachetype
    // should be.
    //

    if (Information.PlatformFlags & HAL_PLATFORM_DISABLE_UC_MAIN_MEMORY) {
          MI_SET_CACHETYPE_TRANSLATION (MmNonCached, 0, MiCached);
    }
#endif

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mirror.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mirror.c

Abstract:

    This module contains the routines to support memory mirroring.

Author:

    Landy Wang (landyw) 17-Jan-2000

Revision History:

--*/

#include "mi.h"


#define MIRROR_MAX_PHASE_ZERO_PASSES 8

//
// This is set via the registry.
//

ULONG MmMirroring = 0;

//
// These bitmaps are allocated at system startup if the
// registry key above is set.
//

PRTL_BITMAP MiMirrorBitMap;
PRTL_BITMAP MiMirrorBitMap2;

//
// This is set if a mirroring operation is in progress.
//

LOGICAL MiMirroringActive = FALSE;

extern LOGICAL MiZeroingDisabled;

#if DBG
ULONG MiMirrorDebug = 1;
ULONG MiMirrorPassMax[2];
#endif

#pragma alloc_text(PAGELK, MmCreateMirror)

NTSTATUS
MmCreateMirror (
    VOID
    )
{
    KIRQL OldIrql;
    KIRQL ExitIrql;
    ULONG Limit;
    ULONG Color;
    ULONG IterationCount;
    PMMPFN Pfn1;
    PMMPFNLIST ListHead;
    PFN_NUMBER PreviousPage;
    PFN_NUMBER ThisPage;
    PFN_NUMBER PageFrameIndex;
    MMLISTS MemoryList;
    ULONG LengthOfClearRun;
    ULONG LengthOfSetRun;
    ULONG StartingRunIndex;
    ULONG BitMapIndex;
    ULONG BitMapHint;
    ULONG BitMapBytes;
    PULONG BitMap1;
    PULONG BitMap2;
    PHYSICAL_ADDRESS PhysicalAddress;
    LARGE_INTEGER PhysicalBytes;
    NTSTATUS Status;
    ULONG BitMapSize;
    PFN_NUMBER PagesWritten;
    PFN_NUMBER PagesWrittenLast;
#if DBG
    ULONG PassMaxRun;
    PFN_NUMBER PagesVerified;
#endif

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if ((MmMirroring & MM_MIRRORING_ENABLED) == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    if (MiMirrorBitMap == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if ((ExVerifySuite(DataCenter) == TRUE) ||
        ((MmProductType != 0x00690057) && (ExVerifySuite(Enterprise) == TRUE))) {
        //
        // DataCenter and Advanced Server are the only appropriate mirroring
        // platforms, allow them to proceed.
        //

        NOTHING;
    }
    else {
        return STATUS_LICENSE_VIOLATION;
    }

    //
    // Serialize here with dynamic memory additions and removals.
    //

    ExAcquireFastMutex (&MmDynamicMemoryMutex);

    ASSERT (MiMirroringActive == FALSE);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Setting all the bits here states all the pages need to be mirrored.
    // In the Phase0 loop below, the bits will be cleared as pages are
    // found on the lists and marked to be sent to the mirror. Bits are 
    // set again if the pages are reclaimed for active use.
    //

    RtlSetAllBits (MiMirrorBitMap2);

    //
    // Put all readonly nonpaged kernel and HAL subsection pages into the
    // Phase0 list.  The only way these could get written between Phase0
    // starting and Phase1 ending is via debugger breakpoints and those
    // don't matter.  This is worth a couple of megabytes and could be done
    // at some point in the future if a reasonable perf gain can be shown.
    //

    MiZeroingDisabled = TRUE;
    IterationCount = 0;

    //
    // Compute initial "pages copied" so convergence can be ascertained
    // in the main loop below.
    //

    PagesWrittenLast = 0;

#if DBG
    if (MiMirrorDebug != 0) {
        for (MemoryList = ZeroedPageList; MemoryList <= ModifiedNoWritePageList; MemoryList += 1) {
            PagesWrittenLast += (PFN_COUNT)MmPageLocationList[MemoryList]->Total;
        }
	    DbgPrint ("Mirror P0 starting with %x pages\n", PagesWrittenLast);
        PagesWrittenLast = 0;
	}
#endif

    //
    // Initiate Phase0 copying.
    // Inform the HAL so it can initialize if need be.
    //

    Status = HalStartMirroring ();

    if (!NT_SUCCESS(Status)) {
        MmUnlockPagableImageSection(ExPageLockHandle);
        MiZeroingDisabled = FALSE;
        ASSERT (MiMirroringActive == FALSE);
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return Status;
    }
    
    //
    // Scan system memory and mirror pages until a pass
    // doesn't find many pages to transfer.
    //

    do {

        //
        // The list of pages to be transferred on this iteration will be
        // formed in the MiMirrorBitMap array.  Clear out prior usage.
        //

        RtlClearAllBits (MiMirrorBitMap);

        //
        // Trim all pages from all process working sets so that as many pages
        // as possible will be on the standby, modified and modnowrite lists.
        // These lists are written during Phase0 mirroring where locks are
        // not held and thus the system is still somewhat operational from
        // an application's perspective.
        //

        MmEmptyAllWorkingSets ();
    
        MiFreeAllExpansionNonPagedPool (FALSE);
    
        LOCK_PFN (OldIrql);
    
        //
        // Scan all the page lists so they can be copied during Phase0
        // mirroring.
        //
        
        for (MemoryList = ZeroedPageList; MemoryList <= ModifiedNoWritePageList; MemoryList += 1) {
    
            ListHead = MmPageLocationList[MemoryList];
    
            if (ListHead->Total == 0) {
                continue;
            }
    
            if ((MemoryList == ModifiedPageList) &&
                (ListHead->Total == MmTotalPagesForPagingFile)) {
                    continue;
            }
    
            PageFrameIndex = ListHead->Flink;
    
            do {
    
                //
                // The scan is operating via the lists rather than the PFN
                // entries as read-in-progress pages are not on lists and
                // therefore do not have to be special cased here and elsewhere.
                //
    
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                //
                // Setting the bit in BitMap means this page is to be copied
                // in this Phase0 iteration.  If it is reused after this
                // point (as indicated by its bit being set again in BitMap2),
                // it will be recopied on a later iteration or in Phase1.
                //

                if (RtlCheckBit(MiMirrorBitMap2, (ULONG)PageFrameIndex)) {
                    RtlSetBit (MiMirrorBitMap, (ULONG)PageFrameIndex);
                    RtlClearBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
                }
    
                PageFrameIndex = Pfn1->u1.Flink;
            } while (PageFrameIndex != MM_EMPTY_LIST);
        }
    
        //
        // Scan for modified pages destined for the paging file.
        //
    
        for (Color = 0; Color < MM_MAXIMUM_NUMBER_OF_COLORS; Color += 1) {
    
            ListHead = &MmModifiedPageListByColor[Color];
    
            if (ListHead->Total == 0) {
                continue;
            }
    
            PageFrameIndex = ListHead->Flink;
    
            do {
    
                //
                // The scan is operating via the lists rather than the PFN
                // entries as read-in-progress are not on lists.  Thus this
                // case does not have to be handled here and just works out.
                //
    
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                //
                // Setting the bit in BitMap means this page is to be copied
                // on this iteration of Phase0.  If it is reused after this
                // point (as indicated by its bit being set again in BitMap2),
                // it will be recopied on a later iteration or in Phase1.
                //

                if (RtlCheckBit(MiMirrorBitMap2, (ULONG)PageFrameIndex)) {
                    RtlSetBit (MiMirrorBitMap, (ULONG)PageFrameIndex);
                    RtlClearBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
                }
    
                PageFrameIndex = Pfn1->u1.Flink;
            } while (PageFrameIndex != MM_EMPTY_LIST);
        }
    
#if DBG
        if (MiMirrorDebug != 0) {
            DbgPrint ("Mirror P0 pass %d: Transfer %x pages\n", 
		      IterationCount,
		      RtlNumberOfSetBits(MiMirrorBitMap));
        }
#endif
    
        MiMirroringActive = TRUE;
    
        //
        // The dirty PFN bitmap has been initialized and the flag set.
        // There are very intricate rules governing how different places in
        // memory management MUST update the bitmap when we are in this mode.
        //
        // The rules are:
        //
        // Anyone REMOVING a page from the zeroed, free, transition, modified
        // or modnowrite lists must update the bitmap IFF that page could
        // potentially be subsequently modified.  Pages that are in transition
        // BUT NOT on one of these lists (ie inpages, freed pages that are
        // dangling due to nonzero reference counts, etc) do NOT need to
        // update the bitmap as they are not one of these lists.  If the page
        // is removed from one of the five lists just to be immediately
        // placed--without modification--on another list, then the bitmap
        // does NOT need updating.
        //
        // Therefore :
        //
        // MiUnlinkPageFromList updates the bitmap.  While some callers
        // immediately put a page acquired this way back on one of the 3 lists
        // above, this is generally rare.  Having this routine update the
        // bitmap means cases like restoring a transition PTE "just work".
        //
        // Callers of MiRemovePageFromList where list >= Transition, must do the
        // bitmap updates as only they know if the page is immediately going
        // back to one of the five lists above or being
        // reused (reused == update REQUIRED).
        //
        // MiRemoveZeroPage updates the bitmap as the page is immediately
        // going to be modified.  MiRemoveAnyPage does this also.
        //
        // Inserts into ANY list do not need to update bitmaps, as a remove had
        // to occur first (which would do the update) or it wasn't on a list to
        // begin with and thus wasn't subtracted above and therefore doesn't
        // need to update the bitmap either.
        //
    
        UNLOCK_PFN (OldIrql);
    
        BitMapHint = 0;
        PagesWritten = 0;
#if DBG
        PassMaxRun = 0;
#endif
    
        do {
    
            BitMapIndex = RtlFindSetBits (MiMirrorBitMap, 1, BitMapHint);
        
            if (BitMapIndex < BitMapHint) {
                break;
            }
        
            if (BitMapIndex == NO_BITS_FOUND) {
                break;
            }
    
            //
            // Found at least one page to copy - try for a cluster.
            //
    
            LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap,
                                                           BitMapIndex,
                                                           &StartingRunIndex);
    
            if (LengthOfClearRun != 0) {
                LengthOfSetRun = StartingRunIndex - BitMapIndex;
            }
            else {
                LengthOfSetRun = MiMirrorBitMap->SizeOfBitMap - BitMapIndex;
            }

            PagesWritten += LengthOfSetRun;
    
#if DBG
            if (LengthOfSetRun > PassMaxRun) {
                PassMaxRun = LengthOfSetRun;
            }
#endif
            //
            // Write out the page(s).
            //
    
            PhysicalAddress.QuadPart = BitMapIndex;
            PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    
            PhysicalBytes.QuadPart = LengthOfSetRun;
            PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;
    
            Status = HalMirrorPhysicalMemory (PhysicalAddress, PhysicalBytes);
    
            if (!NT_SUCCESS(Status)) {
                MiZeroingDisabled = FALSE;
                MmUnlockPagableImageSection(ExPageLockHandle);
                MiMirroringActive = FALSE;
                ExReleaseFastMutex (&MmDynamicMemoryMutex);
                return Status;
            }
    
            BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;
    
        } while (BitMapHint < MiMirrorBitMap->SizeOfBitMap);
    
        ASSERT (RtlNumberOfSetBits(MiMirrorBitMap) == PagesWritten);
    
#if DBG
        if (PassMaxRun > MiMirrorPassMax[0]) {
            MiMirrorPassMax[0] = PassMaxRun;
        }

        if (MiMirrorDebug != 0) {
            DbgPrint ("Mirror P0 pass %d: ended with %x (last= %x) pages\n", 
                  IterationCount, PagesWritten, PagesWrittenLast);
        }
#endif

        ASSERT (MiMirroringActive == TRUE);

	    //
	    // Stop when PagesWritten by the current pass is not somewhat
	    // better than the preceeding pass.  If improvement is vanishing,
	    // the method is at the steady state where working set removals and
	    // transition faults are in balance.  Also stop if PagesWritten is
	    // small in absolute terms.  Finally, there is a limit on iterations
	    // for the misbehaving cases.
	    //

        if (((PagesWritten > PagesWrittenLast - 256) && (IterationCount > 0)) ||
            (PagesWritten < 1024)) {
            break;
        }

        ASSERT (MiMirroringActive == TRUE);
        PagesWrittenLast = PagesWritten;

        IterationCount += 1;

    } while (IterationCount < MIRROR_MAX_PHASE_ZERO_PASSES);

    ASSERT (MiMirroringActive == TRUE);

    //
    // Notify the HAL that Phase0 is complete.  The HAL is responsible for
    // doing things like disabling interrupts, processors and preparing the
    // hardware for Phase1.  Note that some HALs may return from this
    // call at DISPATCH_LEVEL, so snap current IRQL now.
    //

    ExitIrql = KeGetCurrentIrql ();
    ASSERT (ExitIrql == APC_LEVEL);

    Status = HalEndMirroring (0);

    if (!NT_SUCCESS(Status)) {
        ASSERT (KeGetCurrentIrql () == APC_LEVEL);
        MmUnlockPagableImageSection(ExPageLockHandle);
        MiZeroingDisabled = FALSE;
        MiMirroringActive = FALSE;
        ExReleaseFastMutex (&MmDynamicMemoryMutex);
        return Status;
    }

    ASSERT ((KeGetCurrentIrql () == APC_LEVEL) ||
            (KeGetCurrentIrql () == DISPATCH_LEVEL));
    
    //
    // Phase0 copying is now complete.
    //
    // BitMap2 contains the list of safely transmitted (bit == 0) and
    // pages needing transmission (bit == 1).
    //
    // BitMap content is obsolete and if mirror verification is enabled,
    // BitMap will be reused below to accumulate the pages needing
    // verification in the following steps.
    //
    // Prepare for Phase1:
    //
    //  1.  Assume all pages are to be verified (set all bits in BitMap).
    //  2.  Synchronize list updates by acquiring the PFN lock.
    //  3.  Exclude all holes in the PFN database.
    //
    // Phase 1:
    //
    //  4.  Copy all the remaining pages whose bits are set.
    //  5.  Transmit the list of pages to be verified if so configured.
    //

    BitMapBytes = (ULONG)((((MiMirrorBitMap->SizeOfBitMap) + 31) / 32) * 4);

    BitMap1 = MiMirrorBitMap->Buffer;
    BitMap2 = MiMirrorBitMap2->Buffer;

    BitMapSize = MiMirrorBitMap->SizeOfBitMap;
    ASSERT (BitMapSize == MiMirrorBitMap2->SizeOfBitMap);

    //
    // Step 1:  Assume all pages are to be verified (set all bits in BitMap).
    //

    if (MmMirroring & MM_MIRRORING_VERIFYING) {
        RtlSetAllBits(MiMirrorBitMap);
    }

    //
    //  Step 2:  Synchronize list updates by acquiring the PFN lock.
    //

    LOCK_PFN2 (OldIrql);

    //
    // No more updates of the bitmaps are needed - we've already snapped the
    // information we need and are going to hold the PFN lock from here until
    // we're done.
    //

    MiMirroringActive = FALSE;

    //
    // Step 3: Exclude any memory gaps.
    //

    Limit = 0;
    PreviousPage = 0;

    do {

        ThisPage = MmPhysicalMemoryBlock->Run[Limit].BasePage;

        if (ThisPage != PreviousPage) {
            RtlClearBits (MiMirrorBitMap2,
                          (ULONG)PreviousPage,
                          (ULONG)(ThisPage - PreviousPage));
	    
            if (MmMirroring & MM_MIRRORING_VERIFYING) {
                RtlClearBits (MiMirrorBitMap,
                          (ULONG)PreviousPage,
                          (ULONG)(ThisPage - PreviousPage));
            }
        }

        PreviousPage = ThisPage + MmPhysicalMemoryBlock->Run[Limit].PageCount;
        Limit += 1;

    } while (Limit != MmPhysicalMemoryBlock->NumberOfRuns);

    if (PreviousPage != MmHighestPossiblePhysicalPage + 1) {

        RtlClearBits (MiMirrorBitMap2,
                      (ULONG)PreviousPage,
                      (ULONG)(MmHighestPossiblePhysicalPage + 1 - PreviousPage));
        if (MmMirroring & MM_MIRRORING_VERIFYING) {
            RtlClearBits (MiMirrorBitMap,
                          (ULONG)PreviousPage,
                          (ULONG)(MmHighestPossiblePhysicalPage + 1 - PreviousPage));
        }
    }

    //
    // Step 4: Initiate Phase1 copying.
    //
    // N.B.  If this code or code that it calls, writes to non-stack
    // memory between this point and the completion of the call to
    // HalEndMirroring(1), the mirror *BREAKS*, because MmCreateMirror
    // does not know when that non-stack data will be transferred to
    // the new memory. [This rule can be broken if special arrangements
    // are made to re-copy the memory after the final write takes place.]
    //
    // N.B.  The HAL *MUST* handle the writes into this routine's stack
    // frame at the same time it deals with the stack frame of HalEndMirroring
    // and any other frames pushed by the HAL.
    //

    BitMapHint = 0;
#if DBG
    PagesWritten = 0;
    PassMaxRun = 0;
#endif

    do {

        BitMapIndex = RtlFindSetBits (MiMirrorBitMap2, 1, BitMapHint);
    
        if (BitMapIndex < BitMapHint) {
            break;
        }
    
        if (BitMapIndex == NO_BITS_FOUND) {
            break;
        }

        //
        // Found at least one page to copy - try for a cluster.
        //

        LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap2,
                                                       BitMapIndex,
                                                       &StartingRunIndex);

        if (LengthOfClearRun != 0) {
            LengthOfSetRun = StartingRunIndex - BitMapIndex;
        }
        else {
            LengthOfSetRun = MiMirrorBitMap2->SizeOfBitMap - BitMapIndex;
        }

#if DBG
        PagesWritten += LengthOfSetRun;

        if (LengthOfSetRun > PassMaxRun) {
            PassMaxRun = LengthOfSetRun;
        }
#endif

        //
        // Write out the page(s).
        //

        PhysicalAddress.QuadPart = BitMapIndex;
        PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;

        PhysicalBytes.QuadPart = LengthOfSetRun;
        PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;

        Status = HalMirrorPhysicalMemory (PhysicalAddress, PhysicalBytes);

        if (!NT_SUCCESS(Status)) {
            UNLOCK_PFN2 (ExitIrql);
            MiZeroingDisabled = FALSE;
            MmUnlockPagableImageSection(ExPageLockHandle);
            ExReleaseFastMutex (&MmDynamicMemoryMutex);
            return Status;
        }

        BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;

    } while (BitMapHint < MiMirrorBitMap2->SizeOfBitMap);

    //
    // Phase1 copying is now complete.
    //

    //
    // Step 5:
    //
    // If HAL verification is enabled, inform the HAL of the ranges the
    // systems expects were mirrored.  Any range not in this list means
    // that the system doesn't care if it was mirrored and the contents may
    // very well be different between the mirrors.  Note the PFN lock is still
    // held so that the HAL can see things consistently.
    //

#if DBG
    PagesVerified = 0;
#endif

    if (MmMirroring & MM_MIRRORING_VERIFYING) {
        BitMapHint = 0;

        do {
    
            BitMapIndex = RtlFindSetBits (MiMirrorBitMap, 1, BitMapHint);
        
            if (BitMapIndex < BitMapHint) {
                break;
            }
        
            if (BitMapIndex == NO_BITS_FOUND) {
                break;
            }
    
            //
            // Found at least one page in this mirror range - try for a cluster.
            //
    
            LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap,
                                                           BitMapIndex,
                                                           &StartingRunIndex);
    
            if (LengthOfClearRun != 0) {
                LengthOfSetRun = StartingRunIndex - BitMapIndex;
            }
            else {
                LengthOfSetRun = MiMirrorBitMap->SizeOfBitMap - BitMapIndex;
            }
    
#if DBG
            PagesVerified += LengthOfSetRun;
#endif
    
            //
            // Tell the HAL that this range must be in a mirrored state.
            //
    
            PhysicalAddress.QuadPart = BitMapIndex;
            PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    
            PhysicalBytes.QuadPart = LengthOfSetRun;
            PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;
    
            Status = HalMirrorVerify (PhysicalAddress, PhysicalBytes);
    
            if (!NT_SUCCESS(Status)) {
                UNLOCK_PFN2 (ExitIrql);
                MiZeroingDisabled = FALSE;
                MmUnlockPagableImageSection(ExPageLockHandle);
                ExReleaseFastMutex (&MmDynamicMemoryMutex);
                return Status;
            }
    
            BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;
    
        } while (BitMapHint < MiMirrorBitMap->SizeOfBitMap);
    }
    
    //
    // Phase1 verification is now complete.
    //

    //
    // Notify the HAL that everything's done while still holding
    // the PFN lock - the HAL will now complete copying of all pages and
    // any other needed state before returning from this call.
    //

    Status = HalEndMirroring (1);

    UNLOCK_PFN2 (ExitIrql);

#if DBG
    if (MiMirrorDebug != 0) {
        DbgPrint ("Mirror P1: %x pages copied\n", PagesWritten);
        if (MmMirroring & MM_MIRRORING_VERIFYING) {
            DbgPrint ("Mirror P1: %x pages verified\n", PagesVerified);
        }
    }
    if (PassMaxRun > MiMirrorPassMax[1]) {
        MiMirrorPassMax[1] = PassMaxRun;
    }
#endif

    MiZeroingDisabled = FALSE;

    MmUnlockPagableImageSection(ExPageLockHandle);

    ExReleaseFastMutex (&MmDynamicMemoryMutex);

    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mmsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mmsup.c

Abstract:

    This module contains the various routines for miscellaneous support
    operations for memory management.

Author:

    Lou Perazzoli (loup) 31-Aug-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE, MmHibernateInformation)
#pragma alloc_text(PAGE, MiMakeSystemAddressValid)
#pragma alloc_text(PAGE, MiIsPteDecommittedPage)
#endif

#if _WIN64
#if DBGXX
VOID
MiCheckPageTableTrim(
    IN PMMPTE PointerPte
);
#endif
#endif


ULONG
FASTCALL
MiIsPteDecommittedPage (
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This function checks the contents of a PTE to determine if the
    PTE is explicitly decommitted.

    If the PTE is a prototype PTE and the protection is not in the
    prototype PTE, the value FALSE is returned.

Arguments:

    PointerPte - Supplies a pointer to the PTE to examine.

Return Value:

    TRUE if the PTE is in the explicit decommitted state.
    FALSE if the PTE is not in the explicit decommitted state.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    MMPTE PteContents;

    PteContents = *PointerPte;

    //
    // If the protection in the PTE is not decommitted, return FALSE.
    //

    if (PteContents.u.Soft.Protection != MM_DECOMMIT) {
        return FALSE;
    }

    //
    // Check to make sure the protection field is really being interpreted
    // correctly.
    //

    if (PteContents.u.Hard.Valid == 1) {

        //
        // The PTE is valid and therefore cannot be decommitted.
        //

        return FALSE;
    }

    if ((PteContents.u.Soft.Prototype == 1) &&
         (PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED)) {

        //
        // The PTE's protection is not known as it is in
        // prototype PTE format.  Return FALSE.
        //

        return FALSE;
    }

    //
    // It is a decommitted PTE.
    //

    return TRUE;
}

//
// Data for is protection compatible.
//

ULONG MmCompatibleProtectionMask[8] = {
            PAGE_NOACCESS,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY,
            PAGE_NOACCESS | PAGE_EXECUTE,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_EXECUTE |
                PAGE_EXECUTE_READ,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_READWRITE,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_READWRITE |
                PAGE_EXECUTE | PAGE_EXECUTE_READ | PAGE_EXECUTE_READWRITE |
                PAGE_EXECUTE_WRITECOPY,
            PAGE_NOACCESS | PAGE_READONLY | PAGE_WRITECOPY | PAGE_EXECUTE |
                PAGE_EXECUTE_READ | PAGE_EXECUTE_WRITECOPY
            };



ULONG
FASTCALL
MiIsProtectionCompatible (
    IN ULONG OldProtect,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function takes two user supplied page protections and checks
    to see if the new protection is compatible with the old protection.

   protection        compatible protections
    NoAccess          NoAccess
    ReadOnly          NoAccess, ReadOnly, ReadWriteCopy
    ReadWriteCopy     NoAccess, ReadOnly, ReadWriteCopy
    ReadWrite         NoAccess, ReadOnly, ReadWriteCopy, ReadWrite
    Execute           NoAccess, Execute
    ExecuteRead       NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy
    ExecuteWrite      NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy, ReadWrite, ExecuteWrite
    ExecuteWriteCopy  NoAccess, ReadOnly, ReadWriteCopy, Execute, ExecuteRead,
                        ExecuteWriteCopy

Arguments:

    OldProtect - Supplies the protection to be compatible with.

    NewProtect - Supplies the protection to check out.


Return Value:

    Returns TRUE if the protection is compatible, FALSE if not.

Environment:

    Kernel Mode.

--*/

{
    ULONG Mask;
    ULONG ProtectMask;
    ULONG PteProtection;

    PteProtection = MiMakeProtectionMask (OldProtect);

    if (PteProtection == MM_INVALID_PROTECTION) {
        return FALSE;
    }

    Mask = PteProtection & 0x7;

    ProtectMask = MmCompatibleProtectionMask[Mask] | PAGE_GUARD | PAGE_NOCACHE;

    if ((ProtectMask | NewProtect) != ProtectMask) {
        return FALSE;
    }
    return TRUE;
}


ULONG
FASTCALL
MiIsPteProtectionCompatible (
    IN ULONG PteProtection,
    IN ULONG NewProtect
    )
{
    ULONG Mask;
    ULONG ProtectMask;

    Mask = PteProtection & 0x7;

    ProtectMask = MmCompatibleProtectionMask[Mask] | PAGE_GUARD | PAGE_NOCACHE;

    if ((ProtectMask | NewProtect) != ProtectMask) {
        return FALSE;
    }
    return TRUE;
}


//
// Protection data for MiMakeProtectionMask
//

CCHAR MmUserProtectionToMask1[16] = {
                                 0,
                                 MM_NOACCESS,
                                 MM_READONLY,
                                 -1,
                                 MM_READWRITE,
                                 -1,
                                 -1,
                                 -1,
                                 MM_WRITECOPY,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1 };

CCHAR MmUserProtectionToMask2[16] = {
                                 0,
                                 MM_EXECUTE,
                                 MM_EXECUTE_READ,
                                 -1,
                                 MM_EXECUTE_READWRITE,
                                 -1,
                                 -1,
                                 -1,
                                 MM_EXECUTE_WRITECOPY,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1,
                                 -1 };


ULONG
FASTCALL
MiMakeProtectionMask (
    IN ULONG Protect
    )

/*++

Routine Description:

    This function takes a user supplied protection and converts it
    into a 5-bit protection code for the PTE.

Arguments:

    Protect - Supplies the protection.

Return Value:

    Returns the protection code for use in the PTE.  Note that
    MM_INVALID_PROTECTION (-1) is returned for an invalid protection
    request.  Since valid PTE protections fit in 5 bits and are
    zero-extended, it's easy for callers to distinguish this.

Environment:

    Kernel Mode.

--*/

{
    ULONG Field1;
    ULONG Field2;
    ULONG ProtectCode;

    if (Protect >= (PAGE_NOCACHE * 2)) {
        return MM_INVALID_PROTECTION;
    }

    Field1 = Protect & 0xF;
    Field2 = (Protect >> 4) & 0xF;

    //
    // Make sure at least one field is set.
    //

    if (Field1 == 0) {
        if (Field2 == 0) {

            //
            // Both fields are zero, return failure.
            //

            return MM_INVALID_PROTECTION;
        }
        ProtectCode = MmUserProtectionToMask2[Field2];
    } else {
        if (Field2 != 0) {
            //
            //  Both fields are non-zero, raise failure.
            //

            return MM_INVALID_PROTECTION;
        }
        ProtectCode = MmUserProtectionToMask1[Field1];
    }

    if (ProtectCode == -1) {
        return MM_INVALID_PROTECTION;
    }

    if (Protect & PAGE_GUARD) {
        if (ProtectCode == MM_NOACCESS) {

            //
            // Invalid protection, no access and no_cache.
            //

            return MM_INVALID_PROTECTION;
        }

        ProtectCode |= MM_GUARD_PAGE;
    }

    if (Protect & PAGE_NOCACHE) {

        if (ProtectCode == MM_NOACCESS) {

            //
            // Invalid protection, no access and no cache.
            //

            return MM_INVALID_PROTECTION;
        }

        ProtectCode |= MM_NOCACHE;
    }

    return ProtectCode;
}


ULONG
MiDoesPdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld,
    OUT PULONG Waited
    )

/*++

Routine Description:

    This routine examines the specified Page Directory Entry to determine
    if the page table page mapped by the PDE exists.

    If the page table page exists and is not currently in memory, the
    working set mutex and, if held, the PFN lock are released and the
    page table page is faulted into the working set.  The mutexes are
    reacquired.

    If the PDE exists, the function returns TRUE.

Arguments:

    PointerPde - Supplies a pointer to the PDE to examine and potentially
                 bring into the working set.

    TargetProcess - Supplies a pointer to the current process.

    PfnLockHeld - Supplies the value TRUE if the PFN lock is held, FALSE
                  otherwise.

    Waited - Supplies a pointer to a ULONG to increment if the mutex is released
             and reacquired.  Note this value may be incremented more than once.

Return Value:

    TRUE if the PDE exists, FALSE if the PDE is zero.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    PMMPTE PointerPte;
    KIRQL OldIrql;

    OldIrql = APC_LEVEL;

    if (PointerPde->u.Long == 0) {

        //
        // This page directory entry doesn't exist, return FALSE.
        //

        return FALSE;
    }

    if (PointerPde->u.Hard.Valid == 1) {

        //
        // Already valid.
        //

        return TRUE;
    }

    //
    // Page directory entry exists, it is either valid, in transition
    // or in the paging file.  Fault it in.
    //

    if (PfnLockHeld) {
        UNLOCK_PFN (OldIrql);
        *Waited += 1;
    }

    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

    *Waited += MiMakeSystemAddressValid (PointerPte, TargetProcess);

    if (PfnLockHeld) {
        LOCK_PFN (OldIrql);
    }
    return TRUE;
}

#if (_MI_PAGING_LEVELS >= 4)

VOID
MiMakePxeExistAndMakeValid (
    IN PMMPTE PointerPxe,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    )

/*++

Routine Description:

    This routine examines the extended Page Directory Parent Entry to
    determine if the page directory parent page mapped by the PXE exists.

    If the page directory parent page exists and is not currently in memory, the
    working set mutex and, if held, the PFN lock are released and the
    page directory parent page is faulted into the working set.  The mutex and
    lock are then reacquired.

    If the PXE does not exist, a zero filled page directory parent is created
    and it is brought into the working set.

Arguments:

    PointerPxe - Supplies a pointer to the PXE to examine and bring
                 into the working set.

    TargetProcess - Supplies a pointer to the current process.

    PfnLockHeld - Supplies the value TRUE if the PFN lock is held, FALSE
                  otherwise.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    PMMPTE PointerPpe;
    KIRQL OldIrql;

    if (PointerPxe->u.Hard.Valid == 1) {

        //
        // Already valid.
        //

        return;
    }

    //
    // Deal with the page directory page first.
    //

    if (PfnLockHeld) {
        OldIrql = APC_LEVEL;
        UNLOCK_PFN (OldIrql);
    }

    //
    // Fault it in.
    //

    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
    MiMakeSystemAddressValid (PointerPpe, TargetProcess);

    ASSERT (PointerPxe->u.Hard.Valid == 1);

    if (PfnLockHeld) {
        LOCK_PFN (OldIrql);
    }

    return;
}
#endif

#if (_MI_PAGING_LEVELS >= 3)

VOID
MiMakePpeExistAndMakeValid (
    IN PMMPTE PointerPpe,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    )

/*++

Routine Description:

    This routine examines the specified Page Directory Parent Entry to
    determine if the page directory page mapped by the PPE exists.

    If the page directory page exists and is not currently in memory, the
    working set mutex and, if held, the PFN lock are released and the
    page directory page is faulted into the working set.  The mutex and lock
    are then reacquired.

    If the PPE does not exist, a zero filled page directory is created
    and it is brought into the working set.

Arguments:

    PointerPpe - Supplies a pointer to the PPE to examine and bring
                 into the working set.

    TargetProcess - Supplies a pointer to the current process.

    PfnLockHeld - Supplies the value TRUE if the PFN lock is held, FALSE
                  otherwise.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    PMMPTE PointerPde;
    PMMPTE PointerPxe;
    KIRQL OldIrql;

    PointerPxe = MiGetPteAddress (PointerPpe);

    if ((PointerPxe->u.Hard.Valid == 1) &&
        (PointerPpe->u.Hard.Valid == 1)) {

        //
        // Already valid.
        //

        return;
    }

    //
    // Deal with the page directory and parent pages first.
    //

    if (PfnLockHeld) {
        OldIrql = APC_LEVEL;
        UNLOCK_PFN (OldIrql);
    }

    //
    // Fault it in.
    //

    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
    MiMakeSystemAddressValid (PointerPde, TargetProcess);

    ASSERT (PointerPxe->u.Hard.Valid == 1);
    ASSERT (PointerPpe->u.Hard.Valid == 1);

    if (PfnLockHeld) {
        LOCK_PFN (OldIrql);
    }

    return;
}
#endif

VOID
MiMakePdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN LOGICAL PfnLockHeld
    )

/*++

Routine Description:

    This routine examines the specified Page Directory Parent Entry to
    determine if the page directory page mapped by the PPE exists.  If it does,
    then it examines the specified Page Directory Entry to determine if
    the page table page mapped by the PDE exists.

    If the page table page exists and is not currently in memory, the
    working set mutex and, if held, the PFN lock are released and the
    page table page is faulted into the working set.  The mutexes are
    reacquired.

    If the PDE does not exist, a zero filled PTE is created and it
    too is brought into the working set.

Arguments:

    PointerPde - Supplies a pointer to the PDE to examine and bring
                 into the working set.

    TargetProcess - Supplies a pointer to the current process.

    PfnLockHeld - Supplies the value TRUE if the PFN lock is held, FALSE
                  otherwise.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    KIRQL OldIrql;

    PointerPpe = MiGetPteAddress (PointerPde);
    PointerPxe = MiGetPdeAddress (PointerPde);

    if ((PointerPxe->u.Hard.Valid == 1) &&
        (PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1)) {

        //
        // Already valid.
        //

        return;
    }

    //
    // Page directory parent (or extended parent) entry not valid,
    // make it valid.
    //

    OldIrql = APC_LEVEL;

    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

    do {

        if (PfnLockHeld) {
            UNLOCK_PFN (OldIrql);
        }

        //
        // Fault it in.
        //

        MiMakeSystemAddressValid (PointerPpe, TargetProcess);

        ASSERT (PointerPxe->u.Hard.Valid == 1);

        //
        // Now deal with the page directory page.
        //

        MiMakeSystemAddressValid (PointerPde, TargetProcess);

        ASSERT (PointerPxe->u.Hard.Valid == 1);
        ASSERT (PointerPpe->u.Hard.Valid == 1);

        //
        // Now deal with the page table page.
        //

        ASSERT (OldIrql == APC_LEVEL);

        //
        // Fault it in.
        //

        MiMakeSystemAddressValid (PointerPte, TargetProcess);

        ASSERT (PointerPxe->u.Hard.Valid == 1);
        ASSERT (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);

        if (PfnLockHeld) {
            LOCK_PFN (OldIrql);
        }

    } while ((PointerPxe->u.Hard.Valid == 0) ||
             (PointerPpe->u.Hard.Valid == 0) ||
             (PointerPde->u.Hard.Valid == 0));

    return;
}

ULONG
FASTCALL
MiMakeSystemAddressValid (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    CurrentProcess - Supplies a pointer to the current process.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock held.

--*/

{
    NTSTATUS status;
    LOGICAL WsHeldSafe;
    ULONG Waited;

    Waited = FALSE;

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    ASSERT ((VirtualAddress < MM_PAGED_POOL_START) ||
        (VirtualAddress > MmPagedPoolEnd));

    while (!MmIsAddressValid(VirtualAddress)) {

        //
        // The virtual address is not present.  Release
        // the working set mutex and fault it in.
        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Handle both cases here and below.
        //

        UNLOCK_WS_REGARDLESS(CurrentProcess, WsHeldSafe);

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, (PVOID)0);
        if (!NT_SUCCESS(status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          1,
                          (ULONG)status,
                          (ULONG_PTR)CurrentProcess,
                          (ULONG_PTR)VirtualAddress);
        }

        LOCK_WS_REGARDLESS(CurrentProcess, WsHeldSafe);

        Waited = TRUE;
    }

    return Waited;
}


ULONG
FASTCALL
MiMakeSystemAddressValidPfnWs (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess OPTIONAL
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    CurrentProcess - Supplies a pointer to the current process, if the
                     working set lock is not held, this value is NULL.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, PFN lock held, working set lock held
       if CurrentProcess != NULL.

--*/

{
    NTSTATUS status;
    ULONG Waited;
    KIRQL OldIrql;
    LOGICAL WsHeldSafe;

    Waited = FALSE;
    OldIrql = APC_LEVEL;

    //
    // Initializing WsHeldSafe is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    WsHeldSafe = FALSE;

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    while (!MmIsAddressValid(VirtualAddress)) {

        //
        // The virtual address is not present.  Release
        // the working set mutex and fault it in.
        //

        UNLOCK_PFN (OldIrql);
        if (CurrentProcess != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Handle both cases here and below.
            //

            UNLOCK_WS_REGARDLESS(CurrentProcess, WsHeldSafe);
        }
        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, (PVOID)0);
        if (!NT_SUCCESS(status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          2,
                          (ULONG)status,
                          (ULONG_PTR)CurrentProcess,
                          (ULONG_PTR)VirtualAddress);
        }
        if (CurrentProcess != NULL) {
            LOCK_WS_REGARDLESS(CurrentProcess, WsHeldSafe);
        }
        LOCK_PFN (OldIrql);

        Waited = TRUE;
    }
    return Waited;
}

ULONG
FASTCALL
MiMakeSystemAddressValidPfnSystemWs (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, PFN lock held, system working set lock held.

--*/

{
    NTSTATUS status;
    ULONG Waited;
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    LOGICAL SessionSpace;

    Waited = FALSE;
    OldIrql = APC_LEVEL;
    OldIrqlWs = APC_LEVEL;

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    SessionSpace = MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress);

    while (!MmIsAddressValid(VirtualAddress)) {

        //
        // The virtual address is not present.  Release
        // the working set mutex and fault it in.
        //

        UNLOCK_PFN (OldIrql);

        if (SessionSpace == TRUE) {
            UNLOCK_SESSION_SPACE_WS (OldIrqlWs);
        }
        else {
            UNLOCK_SYSTEM_WS (OldIrqlWs);
        }

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, (PVOID)0);
        if (!NT_SUCCESS(status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          2,
                          (ULONG)status,
                          (ULONG_PTR)0,
                          (ULONG_PTR)VirtualAddress);
        }
        if (SessionSpace == TRUE) {
            LOCK_SESSION_SPACE_WS (OldIrqlWs, PsGetCurrentThread ());
        }
        else {
            LOCK_SYSTEM_WS (OldIrqlWs, PsGetCurrentThread ());
        }
        LOCK_PFN (OldIrql);

        Waited = TRUE;
    }
    return Waited;
}

ULONG
FASTCALL
MiMakeSystemAddressValidPfn (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode, APCs disabled, only the PFN Lock held.

--*/

{
    NTSTATUS status;
    KIRQL OldIrql = APC_LEVEL;

    ULONG Waited = FALSE;

    ASSERT (VirtualAddress > MM_HIGHEST_USER_ADDRESS);

    while (!MmIsAddressValid(VirtualAddress)) {

        //
        // The virtual address is not present.  Release
        // the working set mutex and fault it in.
        //

        UNLOCK_PFN (OldIrql);

        status = MmAccessFault (FALSE, VirtualAddress, KernelMode, (PVOID)0);
        if (!NT_SUCCESS(status)) {
            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                          3,
                          (ULONG)status,
                          (ULONG_PTR)VirtualAddress,
                          0);
        }

        LOCK_PFN (OldIrql);

        Waited = TRUE;
    }

    return Waited;
}

ULONG
FASTCALL
MiLockPagedAddress (
    IN PVOID VirtualAddress,
    IN ULONG PfnLockHeld
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.

    CurrentProcess - Supplies a pointer to the current process.

Return Value:

    Returns TRUE if lock released and wait performed, FALSE otherwise.

Environment:

    Kernel mode.

--*/

{

    PMMPTE PointerPte;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    ULONG Waited;

    //
    // Initializing OldIrql is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

    Waited = FALSE;

    PointerPte = MiGetPteAddress(VirtualAddress);

    //
    // The address must be within paged pool.
    //

    if (PfnLockHeld == FALSE) {
        LOCK_PFN2 (OldIrql);
    }

    if (PointerPte->u.Hard.Valid == 0) {

        Waited = MiMakeSystemAddressValidPfn (
                                MiGetVirtualAddressMappedByPte(PointerPte));

    }

    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
    MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 6);
    Pfn1->u3.e2.ReferenceCount += 1;

    if (PfnLockHeld == FALSE) {
        UNLOCK_PFN2 (OldIrql);
    }
    return Waited;
}


VOID
FASTCALL
MiUnlockPagedAddress (
    IN PVOID VirtualAddress,
    IN ULONG PfnLockHeld
    )

/*++

Routine Description:

    This routine checks to see if the virtual address is valid, and if
    not makes it valid.

Arguments:

    VirtualAddress - Supplies the virtual address to make valid.


Return Value:

    None.

Environment:

    Kernel mode.  PFN LOCK MUST NOT BE HELD.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;

    PointerPte = MiGetPteAddress(VirtualAddress);

    //
    // Initializing OldIrql is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

    //
    // Address must be within paged pool.
    //

    if (PfnLockHeld == FALSE) {
        LOCK_PFN2 (OldIrql);
    }

    ASSERT (PointerPte->u.Hard.Valid == 1);
    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 7);

    if (PfnLockHeld == FALSE) {
        UNLOCK_PFN2 (OldIrql);
    }
    return;
}

VOID
FASTCALL
MiZeroPhysicalPage (
    IN PFN_NUMBER PageFrameIndex,
    IN ULONG PageColor
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and fills the page with zeros.

Arguments:

    PageFrameIndex - Supplies the physical page number to fill with zeroes.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    PVOID VirtualAddress;
    PEPROCESS Process;

    UNREFERENCED_PARAMETER (PageColor);

    Process = PsGetCurrentProcess ();

    VirtualAddress = MiMapPageInHyperSpace (Process, PageFrameIndex, &OldIrql);
    KeZeroPage (VirtualAddress);
    MiUnmapPageInHyperSpace (Process, VirtualAddress, OldIrql);

    return;
}

VOID
FASTCALL
MiRestoreTransitionPte (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure restores the original contents into the PTE (which could
    be a prototype PTE) referred to by the PFN database for the specified
    physical page.  It also updates all necessary data structures to
    reflect the fact that the referenced PTE is no longer in transition.

    The physical address of the referenced PTE is mapped into hyper space
    of the current process and the PTE is then updated.

Arguments:

    PageFrameIndex - Supplies the physical page number which refers to a
                    transition PTE.

Return Value:

    none.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PointerPte;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PEPROCESS Process;
    PFN_NUMBER PageTableFrameIndex;

    Process = NULL;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    ASSERT (Pfn1->u3.e1.PageLocation == StandbyPageList);
    ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

    if (Pfn1->u3.e1.PrototypePte) {

        if (MmIsAddressValid (Pfn1->PteAddress)) {
            PointerPte = Pfn1->PteAddress;
        } else {

            //
            // The page containing the prototype PTE is not valid,
            // map the page into hyperspace and reference it that way.
            //

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) &&
                 (PointerPte->u.Hard.Valid == 0));

        //
        // This page is referenced by a prototype PTE.  The
        // segment structures need to be updated when the page
        // is removed from the transition state.
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype) {

            //
            // The prototype PTE is in subsection format, calculate the
            // address of the control area for the subsection and decrement
            // the number of PFN references to the control area.
            //
            // Calculate address of subsection for this prototype PTE.
            //

            Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
            ControlArea = Subsection->ControlArea;
            ControlArea->NumberOfPfnReferences -= 1;
            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

            MiCheckForControlAreaDeletion (ControlArea);
        }

    } else {

        //
        // The page points to a page or page table page which may not be
        // for the current process.  Map the page into hyperspace and
        // reference it through hyperspace.  If the page resides in
        // system space (but not session space), it does not need to be
        // mapped as all PTEs for system space must be resident.  Session
        // space PTEs are only mapped per session so access to them must
        // also go through hyperspace.
        //

        PointerPte = Pfn1->PteAddress;

        if (PointerPte < MiGetPteAddress ((PVOID)MM_SYSTEM_SPACE_START) ||
	       MI_IS_SESSION_PTE (PointerPte)) {

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                       MiGetByteOffset(Pfn1->PteAddress));
        }
        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) &&
                 (PointerPte->u.Hard.Valid == 0));

        MI_CAPTURE_USED_PAGETABLE_ENTRIES (Pfn1);

#if _WIN64
#if DBGXX
        MiCheckPageTableTrim(PointerPte);
#endif
#endif
    }

    ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);
    ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
             (Pfn1->OriginalPte.u.Soft.Transition == 1)));

    MI_WRITE_INVALID_PTE (PointerPte, Pfn1->OriginalPte);

    if (Process != NULL) {
        MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
    }

    Pfn1->u3.e1.CacheAttribute = MiNotMapped;

    //
    // The PTE has been restored to its original contents and is
    // no longer in transition.  Decrement the share count on
    // the page table page which contains the PTE.
    //

    PageTableFrameIndex = Pfn1->u4.PteFrame;
    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

    return;
}

PSUBSECTION
MiGetSubsectionAndProtoFromPte (
    IN PMMPTE PointerPte,
    OUT PMMPTE *ProtoPte
    )

/*++

Routine Description:

    This routine examines the contents of the supplied PTE (which must
    map a page within a section) and determines the address of the
    subsection in which the PTE is contained.

Arguments:

    PointerPte - Supplies a pointer to the PTE.

    ProtoPte - Supplies a pointer to a PMMPTE which receives the
               address of the prototype PTE which is mapped by the supplied
               PointerPte.

Return Value:

    Returns the pointer to the subsection for this PTE.

Environment:

    Kernel mode - Must be holding the PFN database lock and
                  working set mutex (acquired safely) with APCs disabled.

--*/

{
    PMMPTE PointerProto;
    PMMPFN Pfn1;

    if (PointerPte->u.Hard.Valid == 1) {
        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
        *ProtoPte = Pfn1->PteAddress;
        return MiGetSubsectionAddress (&Pfn1->OriginalPte);
    }

    PointerProto = MiPteToProto (PointerPte);
    *ProtoPte = PointerProto;

    MiMakeSystemAddressValidPfn (PointerProto);

    if (PointerProto->u.Hard.Valid == 1) {
        //
        // Prototype Pte is valid.
        //

        Pfn1 = MI_PFN_ELEMENT (PointerProto->u.Hard.PageFrameNumber);
        return MiGetSubsectionAddress (&Pfn1->OriginalPte);
    }

    if ((PointerProto->u.Soft.Transition == 1) &&
         (PointerProto->u.Soft.Prototype == 0)) {

        //
        // Prototype Pte is in transition.
        //

        Pfn1 = MI_PFN_ELEMENT (PointerProto->u.Trans.PageFrameNumber);
        return MiGetSubsectionAddress (&Pfn1->OriginalPte);
    }

    ASSERT (PointerProto->u.Soft.Prototype == 1);
    return MiGetSubsectionAddress (PointerProto);
}

BOOLEAN
MmIsNonPagedSystemAddressValid (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    For a given virtual address this function returns TRUE if the address
    is within the nonpagable portion of the system's address space,
    FALSE otherwise.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if the address is within the nonpagable portion of the system
    address space, FALSE otherwise.

Environment:

    Kernel mode.

--*/

{
    //
    // Return TRUE if address is within the nonpagable portion
    // of the system.  Check limits for paged pool and if not within
    // those limits, return TRUE.
    //

    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress <= MmPagedPoolEnd)) {
        return FALSE;
    }

    //
    // Check special pool before checking session space because on NT64
    // nonpaged session pool exists in session space (on NT32, nonpaged
    // session requests are satisfied from systemwide nonpaged pool instead).
    //

    if (MmIsSpecialPoolAddress (VirtualAddress)) {
        if (MiIsSpecialPoolAddressNonPaged (VirtualAddress)) {
            return TRUE;
        }
        return FALSE;
    }

    if ((VirtualAddress >= (PVOID) MmSessionBase) &&
        (VirtualAddress < (PVOID) MiSessionSpaceEnd)) {
        return FALSE;
    }

    return TRUE;
}

VOID
MmHibernateInformation (
    IN PVOID    MemoryMap,
    OUT PULONG_PTR  HiberVa,
    OUT PPHYSICAL_ADDRESS HiberPte
    )
{
    //
    // Mark PTE page where the 16 dump PTEs reside as needing cloning.
    //

    PoSetHiberRange (MemoryMap, PO_MEM_CLONE, MmCrashDumpPte, 1, ' etP');

    //
    // Return the dump PTEs to the loader (as it needs to use them
    // to map it's relocation code into the kernel space on the
    // final bit of restoring memory).
    //

    *HiberVa = (ULONG_PTR) MiGetVirtualAddressMappedByPte(MmCrashDumpPte);
    *HiberPte = MmGetPhysicalAddress(MmCrashDumpPte);
}

#if _WIN64
#if DBGXX

ULONG zok[16];

VOID
MiCheckPageTableTrim(
    IN PMMPTE PointerPte
)
{
    ULONG i;
    PFN_NUMBER Frame;
    PMMPFN Pfn;
    PMMPTE FrameData;
    PMMPTE p;
    ULONG count;

    Frame = MI_GET_PAGE_FRAME_FROM_PTE(PointerPte);
    Pfn = MI_PFN_ELEMENT (Frame);

    if (Pfn->UsedPageTableEntries) {

        count = 0;

        p = FrameData = (PMMPTE)KSEG_ADDRESS (Frame);

        for (i = 0; i < PTE_PER_PAGE; i += 1, p += 1) {
            if (p->u.Long != 0) {
                count += 1;
            }
        }

        DbgPrint ("MiCheckPageTableTrim: %I64X %I64X %I64X\n",
            PointerPte, Pfn, Pfn->UsedPageTableEntries);

        if (count != Pfn->UsedPageTableEntries) {
            DbgPrint ("MiCheckPageTableTrim1: %I64X %I64X %I64X %I64X\n",
                PointerPte, Pfn, Pfn->UsedPageTableEntries, count);
            DbgBreakPoint();
        }
        zok[0] += 1;
    }
    else {
        zok[1] += 1;
    }
}

VOID
MiCheckPageTableInPage(
    IN PMMPFN Pfn,
    IN PMMINPAGE_SUPPORT Support
)
{
    ULONG i;
    PFN_NUMBER Frame;
    PMMPTE FrameData;
    PMMPTE p;
    ULONG count;

    if (Support->UsedPageTableEntries) {

        Frame = (PFN_NUMBER)((PMMPFN)Pfn - (PMMPFN)MmPfnDatabase);

        count = 0;

        p = FrameData = (PMMPTE)KSEG_ADDRESS (Frame);

        for (i = 0; i < PTE_PER_PAGE; i += 1, p += 1) {
            if (p->u.Long != 0) {
                count += 1;
            }
        }

        DbgPrint ("MiCheckPageTableIn: %I64X %I64X %I64X\n",
            FrameData, Pfn, Support->UsedPageTableEntries);

        if (count != Support->UsedPageTableEntries) {
            DbgPrint ("MiCheckPageTableIn1: %I64X %I64X %I64X %I64X\n",
                FrameData, Pfn, Support->UsedPageTableEntries, count);
            DbgBreakPoint();
        }
        zok[2] += 1;
    }
    else {
        zok[3] += 1;
    }
}
#endif
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\mmquota.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mmquota.c

Abstract:

    This module contains the routines which implement the quota and
    commitment charging for memory management.

Author:

    Lou Perazzoli (loup) 12-December-89
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

#define MM_MINIMAL_COMMIT_INCREASE 512

SIZE_T MmPeakCommitment;

LONG MiCommitPopups[2];

ULONG MiChargeCommitmentFailures[2];

extern ULONG_PTR MmAllocatedPagedPool;

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeCommitment)
#pragma alloc_text(PAGE,MiCalculatePageCommitment)
#pragma alloc_text(PAGE,MiReturnPageTablePageCommitment)
#endif

SIZE_T MmSystemCommitReserve = (5 * 1024 * 1024) / PAGE_SIZE;

VOID
MiInitializeCommitment (
    VOID
    )
{
    if (MmNumberOfPhysicalPages < (33 * 1024 * 1024) / PAGE_SIZE) {
        MmSystemCommitReserve = (1 * 1024 * 1024) / PAGE_SIZE;
    }

#if defined (_MI_DEBUG_COMMIT_LEAKS)
    MiCommitTraces = ExAllocatePoolWithTag (NonPagedPool,
                           MI_COMMIT_TRACE_MAX * sizeof (MI_COMMIT_TRACES),
                           'tCmM');
#endif
}

LOGICAL
FASTCALL
MiChargeCommitment (
    IN SIZE_T QuotaCharge,
    IN PEPROCESS Process OPTIONAL
    )

/*++

Routine Description:

    This routine checks to ensure the system has sufficient page file
    space remaining.

    Since this routine is generally used to charge commitment on behalf of
    usermode or other optional actions, this routine does not allow the
    caller to use up the very last morsels of commit on the premise that
    the operating system and drivers can put those to better use than any
    application in order to prevent the appearance of system hangs.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

    Process - Optionally supplies the current process IF AND ONLY IF
              the working set mutex is held.  If the paging file
              is being extended, the working set mutex is released if
              this is non-null.

Return Value:

    TRUE if there is sufficient space, FALSE if not.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;
    SIZE_T CommitLimit;
    MMPAGE_FILE_EXPANSION PageExtend;
    LOGICAL WsHeldSafe;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

#if DBG
    if (InitializationPhase > 1) {
        ULONG i;
        PKTHREAD Thread;

        Thread = KeGetCurrentThread ();
        for (i = 0; i < (ULONG)KeNumberProcessors; i += 1) {
            if (KiProcessorBlock[i]->IdleThread == Thread) {
                DbgPrint ("MMQUOTA: %x %p\n", i, Thread);
                DbgBreakPoint ();
            }
        }
    }
#endif

    //
    // Initializing WsHeldSafe is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    WsHeldSafe = FALSE;

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        while (NewCommitValue + MmSystemCommitReserve > MmTotalCommitLimit) {

            //
            // If the pagefiles are already at the maximum, then don't
            // bother trying to extend them, but do trim the cache.
            //

            if (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum) {

                MiChargeCommitmentFailures[1] += 1;

                MiTrimSegmentCache ();

                if (MmTotalCommitLimit >= MmTotalCommitLimitMaximum) {
                    MiCauseOverCommitPopup ();
                    return FALSE;
                }
            }

            if (Process != NULL) {

                //
                // The working set lock may have been acquired safely or
                // unsafely by our caller.  Handle both cases here and below.
                //

                UNLOCK_WS_REGARDLESS(Process, WsHeldSafe);
            }

            //
            // Queue a message to the segment dereferencing / pagefile extending
            // thread to see if the page file can be extended.  This is done
            // in the context of a system thread due to mutexes which may
            // currently be held.
            //

            PageExtend.InProgress = 1;
            PageExtend.ActualExpansion = 0;
            PageExtend.RequestedExpansionSize = QuotaCharge;
            PageExtend.Segment = NULL;
            PageExtend.PageFileNumber = MI_EXTEND_ANY_PAGEFILE;
            KeInitializeEvent (&PageExtend.Event, NotificationEvent, FALSE);

            if ((MiIssuePageExtendRequest (&PageExtend) == FALSE) ||
                (PageExtend.ActualExpansion == 0)) {

                MiCauseOverCommitPopup ();

                MiChargeCommitmentFailures[0] += 1;

                if (Process != NULL) {
                    LOCK_WS_REGARDLESS(Process, WsHeldSafe);
                }

                return FALSE;
            }

            if (Process != NULL) {
                LOCK_WS_REGARDLESS(Process, WsHeldSafe);
            }

            OldCommitValue = MmTotalCommittedPages;

            NewCommitValue = OldCommitValue + QuotaCharge;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    //
    // Success.
    //

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_NORMAL, QuotaCharge);

    if (MmTotalCommittedPages > MmPeakCommitment) {
        MmPeakCommitment = MmTotalCommittedPages;
    }

    //
    // Success.  If system commit exceeds 90%, attempt a preemptive pagefile
    // increase anyway.
    //

    NewCommitValue = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    if (NewCommitValue > ((CommitLimit/10)*9)) {

        if (CommitLimit < MmTotalCommitLimitMaximum) {

            //
            // Attempt to expand the paging file, but don't wait
            // to see if it succeeds.
            //

            NewCommitValue = NewCommitValue - ((CommitLimit/100)*85);

            MiIssuePageExtendRequestNoWait (NewCommitValue);
        }
        else {

            //
            // If the pagefiles are already at the maximum, then don't
            // bother trying to extend them, but do trim the cache.
            //

            if (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum) {
                MiTrimSegmentCache ();
            }
        }
    }

    return TRUE;
}

LOGICAL
FASTCALL
MiChargeCommitmentCantExpand (
    IN SIZE_T QuotaCharge,
    IN ULONG MustSucceed
    )

/*++

Routine Description:

    This routine charges the specified commitment without attempting
    to expand paging files and waiting for the expansion.  The routine
    determines if the paging file space is exhausted, and if so,
    it attempts to ascertain if the paging file space could be expanded.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

    MustSucceed - Supplies TRUE if the charge must succeed.

Return Value:

    TRUE if the commitment was permitted, FALSE if not.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    SIZE_T CommitLimit;
    SIZE_T ExtendAmount;
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

    ASSERT32 ((QuotaCharge < 0x100000) || (QuotaCharge < MmTotalCommitLimit));

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        if ((NewCommitValue > MmTotalCommitLimit) && (!MustSucceed)) {

            if ((NewCommitValue < MmTotalCommittedPages) ||
                (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum)) {

                MiChargeCommitmentFailures[1] += 1;
                return FALSE;
            }

            //
            // Attempt to expand the paging file, but don't wait
            // to see if it succeeds.
            //

            MiChargeCommitmentFailures[0] += 1;
            MiIssuePageExtendRequestNoWait (MM_MINIMAL_COMMIT_INCREASE);
            return FALSE;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_CANT_EXPAND, QuotaCharge);

    //
    // Success.  If system commit exceeds 90%, attempt a preemptive pagefile
    // increase anyway.
    //

    NewCommitValue = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    if ((NewCommitValue > ((CommitLimit/10)*9)) &&
        (CommitLimit < MmTotalCommitLimitMaximum)) {

        //
        // Attempt to expand the paging file, but don't wait
        // to see if it succeeds.
        //
        // Queue a message to the segment dereferencing / pagefile extending
        // thread to see if the page file can be extended.  This is done
        // in the context of a system thread due to mutexes which may
        // currently be held.
        //

        ExtendAmount = NewCommitValue - ((CommitLimit/100)*85);

        if (QuotaCharge > ExtendAmount) {
            ExtendAmount = QuotaCharge;
        }

        MiIssuePageExtendRequestNoWait (ExtendAmount);
    }

    return TRUE;
}


LOGICAL
FASTCALL
MiChargeTemporaryCommitmentForReduction (
    IN SIZE_T QuotaCharge
    )

/*++

Routine Description:

    This routine attempts to charge the specified commitment without
    expanding the paging file.

    This is typically called just prior to reducing the pagefile size.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

Return Value:

    TRUE if the commitment was permitted, FALSE if not.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

    ASSERT32 (QuotaCharge < 0x100000);

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        if (NewCommitValue > MmTotalCommitLimit) {
            return FALSE;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    //
    // Success.
    //

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_NORMAL, QuotaCharge);

    if (MmTotalCommittedPages > MmPeakCommitment) {
        MmPeakCommitment = MmTotalCommittedPages;
    }

    return TRUE;
}


SIZE_T
MiCalculatePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine examines the range of pages from the starting address
    up to and including the ending address and returns the commit charge
    for the pages within the range.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the virtual address descriptor which describes the range.

    Process - Supplies the current process.

Return Value:

    Commitment charge for the range.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    SIZE_T NumberOfCommittedPages;
    ULONG Waited;

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);

    LastPte = MiGetPteAddress (EndingAddress);

    if (Vad->u.VadFlags.MemCommit == 1) {

        //
        // All the pages are committed within this range.
        //

        NumberOfCommittedPages = BYTES_TO_PAGES ((PCHAR)EndingAddress -
                                                       (PCHAR)StartingAddress);

        //
        // Examine the PTEs to determine how many pages are committed.
        //

        do {

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif

            while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                Process,
                                                FALSE,
                                                &Waited)) {
    
                //
                // No PXE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
            }

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                Process,
                                                FALSE,
                                                &Waited)) {
    
                //
                // No PPE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary (PointerPpe)) {
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    goto retry;
                }
#endif
            }

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                Process,
                                                FALSE,
                                                &Waited)) {
    
                //
                // No PDE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
#if (_MI_PAGING_LEVELS >= 3)
                if (MiIsPteOnPdeBoundary (PointerPde)) {
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);
                    Waited = 1;
                    break;
                }
#endif
            }

        } while (Waited != 0);

restart:

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                //
                // This is a PDE boundary, check to see if the all the
                // PXE/PPE/PDE pages exist.
                //

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPteAddress (PointerPpe);

                do {

                    if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                     Process,
                                                     FALSE,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
    
#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif
    
                    if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                     Process,
                                                     FALSE,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPpe += 1;
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
    
#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif
    
                    if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                     Process,
                                                     FALSE,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPde += 1;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
                } while (Waited != 0);
            }

            //
            // The PDE exists, examine the PTE.
            //

            if (PointerPte->u.Long != 0) {

                //
                // Has this page been explicitly decommitted?
                //

                if (MiIsPteDecommittedPage (PointerPte)) {

                    //
                    // This page is decommitted, remove it from the count.
                    //

                    NumberOfCommittedPages -= 1;

                }
            }

            PointerPte += 1;
        }

        return NumberOfCommittedPages;
    }

    //
    // Examine non committed range.
    //

    NumberOfCommittedPages = 0;

    do {

#if (_MI_PAGING_LEVELS >= 4)
retry2:
#endif
        while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                            Process,
                                            FALSE,
                                            &Waited)) {
    
    
            //
            // No PXE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
        }

#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                            Process,
                                            FALSE,
                                            &Waited)) {
    
    
            //
            // No PPE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto retry2;
            }
#endif
        }

#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

        while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                            Process,
                                            FALSE,
                                            &Waited)) {
    
            //
            // No PDE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);
                Waited = 1;
                break;
            }
#endif
        }

    } while (Waited != 0);

restart2:

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            //
            // This is a PDE boundary, check to see if the entire
            // PXE/PPE/PDE pages exist.
            //

            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPteAddress (PointerPde);
            PointerPxe = MiGetPdeAddress (PointerPde);

            do {

                if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                 Process,
                                                 FALSE,
                                                 &Waited)) {
    
                    //
                    // No PXE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPxe += 1;
                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }
    
#if (_MI_PAGING_LEVELS >= 4)
                Waited = 0;
#endif

                if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                 Process,
                                                 FALSE,
                                                 &Waited)) {
    
                    //
                    // No PPE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPpe += 1;
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }
    
#if (_MI_PAGING_LEVELS < 4)
                Waited = 0;
#endif
    
                if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                 Process,
                                                 FALSE,
                                                 &Waited)) {
    
                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPde += 1;
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }

            } while (Waited != 0);
        }

        //
        // The PDE exists, examine the PTE.
        //

        if ((PointerPte->u.Long != 0) &&
             (!MiIsPteDecommittedPage (PointerPte))) {

            //
            // This page is committed, count it.
            //

            NumberOfCommittedPages += 1;
        }

        PointerPte += 1;
    }

    return NumberOfCommittedPages;
}

VOID
MiReturnPageTablePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PEPROCESS CurrentProcess,
    IN PMMVAD PreviousVad,
    IN PMMVAD NextVad
    )

/*++

Routine Description:

    This routine returns commitment for COMPLETE page table pages which
    span the virtual address range.  For example (assuming 4k pages),
    if the StartingAddress =  64k and the EndingAddress = 5mb, no
    page table charges would be freed as a complete page table page is
    not covered by the range.  However, if the StartingAddress was 4mb
    and the EndingAddress was 9mb, 1 page table page would be freed.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    CurrentProcess - Supplies a pointer to the current process.

    PreviousVad - Supplies a pointer to the previous VAD, NULL if none.

    NextVad - Supplies a pointer to the next VAD, NULL if none.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    RTL_BITMAP VadBitMap;
    ULONG NumberToClear;
    ULONG StartBit;
    ULONG EndBit;
    LONG FirstPage;
    LONG LastPage;
    LONG PreviousPage;
    LONG NextPage;
#if (_MI_PAGING_LEVELS >= 3)
    LONG FirstPdPage;
    LONG LastPdPage;
    LONG PreviousPdPage;
    LONG NextPdPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    LONG FirstPpPage;
    LONG LastPpPage;
    LONG PreviousPpPage;
    LONG NextPpPage;
#endif

    //
    // Check to see if any page table pages would be freed.
    //

    ASSERT (StartingAddress != EndingAddress);

    StartBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (StartingAddress)) / X64K);
    EndBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (EndingAddress)) / X64K);

    if (PreviousVad == NULL) {
        PreviousPage = -1;
#if (_MI_PAGING_LEVELS >= 3)
        PreviousPdPage = -1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PreviousPpPage = -1;
#endif
    }
    else {
        PreviousPage = MiGetPdeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#if (_MI_PAGING_LEVELS >= 3)
        PreviousPdPage = MiGetPpeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PreviousPpPage = MiGetPxeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#endif
        if (MI_64K_ALIGN (MI_VPN_TO_VA (PreviousVad->EndingVpn)) ==
            MI_64K_ALIGN (StartingAddress)) {
                StartBit += 1;
        }
    }

    if (NextVad == NULL) {
        NextPage = MiGetPdeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#if (_MI_PAGING_LEVELS >= 3)
        NextPdPage = MiGetPpeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        NextPpPage = MiGetPxeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#endif
    }
    else {
        NextPage = MiGetPdeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#if (_MI_PAGING_LEVELS >= 3)
        NextPdPage = MiGetPpeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#endif
#if (_MI_PAGING_LEVELS >= 4)
        NextPpPage = MiGetPxeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#endif
        if (MI_64K_ALIGN (MI_VPN_TO_VA (NextVad->StartingVpn)) ==
            MI_64K_ALIGN (EndingAddress)) {
                EndBit -= 1;
        }
    }

    ASSERT (PreviousPage <= NextPage);
    ASSERT64 (PreviousPdPage <= NextPdPage);
#if (_MI_PAGING_LEVELS >= 4)
    ASSERT64 (PreviousPpPage <= NextPpPage);
#endif

    FirstPage = MiGetPdeIndex (StartingAddress);

    LastPage = MiGetPdeIndex (EndingAddress);

    if (PreviousPage == FirstPage) {

        //
        // A VAD is within the starting page table page.
        //

        FirstPage += 1;
    }

    if (NextPage == LastPage) {

        //
        // A VAD is within the ending page table page.
        //

        LastPage -= 1;
    }

    if (StartBit <= EndBit) {

        //
        // Initialize the bitmap inline for speed.
        //

        VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
        VadBitMap.Buffer = VAD_BITMAP_SPACE;

#if defined (_WIN64) || defined (_X86PAE_)

        //
        // Only the first (PAGE_SIZE*8*64K) of VA space on NT64 is bitmapped.
        //

        if (EndBit > MiLastVadBit) {
            EndBit = MiLastVadBit;
        }

        if (StartBit <= MiLastVadBit) {
            RtlClearBits (&VadBitMap, StartBit, EndBit - StartBit + 1);

            if (MmWorkingSetList->VadBitMapHint > StartBit) {
                MmWorkingSetList->VadBitMapHint = StartBit;
            }
        }
#else
        RtlClearBits (&VadBitMap, StartBit, EndBit - StartBit + 1);

        if (MmWorkingSetList->VadBitMapHint > StartBit) {
            MmWorkingSetList->VadBitMapHint = StartBit;
        }
#endif
    }

    //
    // Indicate that the page table page is not in use.
    //

    if (FirstPage > LastPage) {
        return;
    }

    NumberToClear = 1 + LastPage - FirstPage;

    while (FirstPage <= LastPage) {
        ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                              FirstPage));

        MI_CLEAR_BIT (MmWorkingSetList->CommittedPageTables, FirstPage);
        FirstPage += 1;
    }

    MmWorkingSetList->NumberOfCommittedPageTables -= NumberToClear;

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Return page directory parent charges here.
    //

    FirstPpPage = MiGetPxeIndex (StartingAddress);

    LastPpPage = MiGetPxeIndex (EndingAddress);

    if (PreviousPpPage == FirstPpPage) {

        //
        // A VAD is within the starting page directory parent page.
        //

        FirstPpPage += 1;
    }

    if (NextPpPage == LastPpPage) {

        //
        // A VAD is within the ending page directory parent page.
        //

        LastPpPage -= 1;
    }

    //
    // Indicate that the page directory page parent is not in use.
    //

    if (FirstPpPage <= LastPpPage) {

        MmWorkingSetList->NumberOfCommittedPageDirectoryParents -= (1 + LastPpPage - FirstPpPage);

        NumberToClear += (1 + LastPpPage - FirstPpPage);
    
        while (FirstPpPage <= LastPpPage) {
            ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                  FirstPpPage));
    
            MI_CLEAR_BIT (MmWorkingSetList->CommittedPageDirectoryParents, FirstPpPage);
            FirstPpPage += 1;
        }
    }
    
#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Return page directory charges here.
    //

    FirstPdPage = MiGetPpeIndex (StartingAddress);

    LastPdPage = MiGetPpeIndex (EndingAddress);

    if (PreviousPdPage == FirstPdPage) {

        //
        // A VAD is within the starting page directory page.
        //

        FirstPdPage += 1;
    }

    if (NextPdPage == LastPdPage) {

        //
        // A VAD is within the ending page directory page.
        //

        LastPdPage -= 1;
    }

    //
    // Indicate that the page directory page is not in use.
    //

    if (FirstPdPage <= LastPdPage) {

        MmWorkingSetList->NumberOfCommittedPageDirectories -= (1 + LastPdPage - FirstPdPage);

        NumberToClear += (1 + LastPdPage - FirstPdPage);
    
        while (FirstPdPage <= LastPdPage) {
            ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                                  FirstPdPage));
    
            MI_CLEAR_BIT (MmWorkingSetList->CommittedPageDirectories, FirstPdPage);
            FirstPdPage += 1;
        }
    }
    
#endif

    MiReturnCommitment (NumberToClear);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PAGETABLES, NumberToClear);
    PsReturnProcessPageFileQuota (CurrentProcess, NumberToClear);

    if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
        PsChangeJobMemoryUsage(-(SSIZE_T)NumberToClear);
    }
    CurrentProcess->CommitCharge -= NumberToClear;

    MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - NumberToClear);

    return;
}


VOID
MiCauseOverCommitPopup (
    VOID
    )

/*++

Routine Description:

    This function causes an over commit popup to occur (if the popup has never
    been sent before).

Arguments:

    None.

Return Value:

    None.

--*/

{
    LONG PopupNumber;

    //
    // Give the user a meaningful message - either to increase the minimum,
    // maximum, or both.
    //

    if (MmTotalCommittedPages > MmTotalCommitLimitMaximum - 100) {
        if (InterlockedIncrement (&MiCommitPopups[0]) > 1) {
            InterlockedDecrement (&MiCommitPopups[0]);
            return;
        }
        PopupNumber = STATUS_COMMITMENT_LIMIT;
    }
    else {
        if (InterlockedIncrement (&MiCommitPopups[1]) > 1) {
            InterlockedDecrement (&MiCommitPopups[1]);
            return;
        }
        PopupNumber = STATUS_COMMITMENT_MINIMUM;
    }

    IoRaiseInformationalHardError (PopupNumber, NULL, NULL);
}


SIZE_T MmTotalPagedPoolQuota;
SIZE_T MmTotalNonPagedPoolQuota;

BOOLEAN
MmRaisePoolQuota(
    IN POOL_TYPE PoolType,
    IN SIZE_T OldQuotaLimit,
    OUT PSIZE_T NewQuotaLimit
    )

/*++

Routine Description:

    This function is called (with a spinlock) whenever PS detects a quota
    limit has been exceeded. The purpose of this function is to attempt to
    increase the specified quota.

Arguments:

    PoolType - Supplies the pool type of the quota to be raised

    OldQuotaLimit - Supplies the current quota limit for this pool type

    NewQuotaLimit - Returns the new limit

Return Value:

    TRUE - The API succeeded and the quota limit was raised.

    FALSE - We were unable to raise the quota limit.

Environment:

    Kernel mode, QUOTA SPIN LOCK HELD!!

--*/

{
    SIZE_T Limit;
    PMM_PAGED_POOL_INFO PagedPoolInfo;

    if (PoolType == PagedPool) {

        //
        // Check commit limit and make sure at least 1mb is available.
        // Check to make sure 4mb of paged pool still exists.
        //

        PagedPoolInfo = &MmPagedPoolInfo;

        if ((MmSizeOfPagedPoolInBytes >> PAGE_SHIFT) <
            (PagedPoolInfo->AllocatedPagedPool + ((MMPAGED_QUOTA_CHECK) >> PAGE_SHIFT))) {

            return FALSE;
        }

        MmTotalPagedPoolQuota += (MMPAGED_QUOTA_INCREASE);
        *NewQuotaLimit = OldQuotaLimit + (MMPAGED_QUOTA_INCREASE);
        return TRUE;

    } else {

        if ( (ULONG_PTR)(MmAllocatedNonPagedPool + ((1*1024*1024) >> PAGE_SHIFT)) < (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT)) {
            goto aok;
            }

        //
        // Make sure 200 pages and 5mb of nonpaged pool expansion
        // available.  Raise quota by 64k.
        //

        if ((MmAvailablePages < 200) ||
            (MmResidentAvailablePages < ((MMNONPAGED_QUOTA_CHECK) >> PAGE_SHIFT))) {

            return FALSE;
        }

        if (MmAvailablePages > ((4*1024*1024) >> PAGE_SHIFT)) {
            Limit = (1*1024*1024) >> PAGE_SHIFT;
        } else {
            Limit = (4*1024*1024) >> PAGE_SHIFT;
        }

        if ((ULONG_PTR)((MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT)) <
            (MmAllocatedNonPagedPool + Limit)) {

            return FALSE;
        }
aok:
        MmTotalNonPagedPoolQuota += (MMNONPAGED_QUOTA_INCREASE);
        *NewQuotaLimit = OldQuotaLimit + (MMNONPAGED_QUOTA_INCREASE);
        return TRUE;
    }
}


VOID
MmReturnPoolQuota(
    IN POOL_TYPE PoolType,
    IN SIZE_T ReturnedQuota
    )

/*++

Routine Description:

    Returns pool quota.

Arguments:

    PoolType - Supplies the pool type of the quota to be returned.

    ReturnedQuota - Number of bytes returned.

Return Value:

    NONE.

Environment:

    Kernel mode, QUOTA SPIN LOCK HELD!!

--*/

{

    if (PoolType == PagedPool) {
        MmTotalPagedPoolQuota -= ReturnedQuota;
    } else {
        MmTotalNonPagedPoolQuota -= ReturnedQuota;
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\modwrite.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    modwrite.c

Abstract:

    This module contains the modified page writer for memory management.

Author:

    Lou Perazzoli (loup) 10-Jun-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"
#include "ntiodump.h"

typedef enum _MODIFIED_WRITER_OBJECT {
    NormalCase,
    MappedPagesNeedWriting,
    ModifiedWriterMaximumObject
} MODIFIED_WRITER_OBJECT;

typedef struct _MM_WRITE_CLUSTER {
    ULONG Count;
    ULONG StartIndex;
    ULONG Cluster[2 * (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE) + 1];
} MM_WRITE_CLUSTER, *PMM_WRITE_CLUSTER;

typedef struct _MM_LDW_WORK_CONTEXT {
    WORK_QUEUE_ITEM WorkItem;
    PFILE_OBJECT FileObject;
} MM_LDW_WORK_CONTEXT, *PMM_LDW_WORK_CONTEXT;

ULONG MmWriteAllModifiedPages;
LOGICAL MiFirstPageFileCreatedAndReady = FALSE;

LOGICAL MiDrainingMappedWrites = FALSE;

ULONG MmNumberOfMappedMdls;
#if DBG
ULONG MmNumberOfMappedMdlsInUse;
ULONG MmNumberOfMappedMdlsInUsePeak;
#endif

ULONG MiClusterWritesDisabled;

#define MI_SLOW_CLUSTER_WRITES   10

#define ONEMB_IN_PAGES  ((1024 * 1024) / PAGE_SIZE)

VOID
MiClusterWritePages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex,
    IN PMM_WRITE_CLUSTER WriteCluster,
    IN ULONG Size
    );

VOID
MiExtendPagingFileMaximum (
    IN ULONG PageFileNumber,
    IN PRTL_BITMAP NewBitmap
    );

VOID
MiLdwPopupWorker (
    IN PVOID Context
    );

SIZE_T
MiAttemptPageFileExtension (
    IN ULONG PageFileNumber,
    IN SIZE_T SizeNeeded,
    IN LOGICAL Maximum
    );

NTSTATUS
MiZeroPageFileFirstPage (
    IN PFILE_OBJECT File
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtCreatePagingFile)
#pragma alloc_text(PAGE,MmGetPageFileInformation)
#pragma alloc_text(PAGE,MmGetSystemPageFile)
#pragma alloc_text(PAGE,MiLdwPopupWorker)
#pragma alloc_text(PAGE,MiAttemptPageFileExtension)
#pragma alloc_text(PAGE,MiExtendPagingFiles)
#pragma alloc_text(PAGE,MiZeroPageFileFirstPage)
#pragma alloc_text(PAGELK,MiModifiedPageWriter)
#pragma alloc_text(PAGELK,MiFlushAllPages)
#endif


extern POBJECT_TYPE IoFileObjectType;

extern SIZE_T MmSystemCommitReserve;

LIST_ENTRY MmMappedPageWriterList;

KEVENT MmMappedPageWriterEvent;

KEVENT MmMappedFileIoComplete;

ULONG MmSystemShutdown;

BOOLEAN MmSystemPageFileLocated;

NTSTATUS
MiCheckPageFileMapping (
    IN PFILE_OBJECT File
    );

VOID
MiInsertPageFileInList (
    VOID
    );

PFN_NUMBER
MiGatherMappedPages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    );

PFN_NUMBER
MiGatherPagefilePages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiPageFileFull (
    VOID
    );

#if DBG
ULONG_PTR MmPagingFileDebug[8192];
#endif

extern PFN_NUMBER MmMoreThanEnoughFreePages;

#define MINIMUM_PAGE_FILE_SIZE ((ULONG)(256*PAGE_SIZE))

VOID
MiModifiedPageWriterWorker (
    VOID
    );


VOID
MiReleaseModifiedWriter (
    VOID
    )

/*++

Routine Description:

    Nonpagable wrapper to signal the modified writer when the first pagefile
    creation has completely finished.

--*/

{
    KIRQL OldIrql;
    LOCK_PFN (OldIrql);
    MiFirstPageFileCreatedAndReady = TRUE;
    UNLOCK_PFN (OldIrql);
}

NTSTATUS
MiZeroPageFileFirstPage (
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    This routine zeroes the first page of the newly created paging file
    to ensure no stale crashdump signatures get to live on.

Arguments:

    File - Supplies a pointer to the file object for the paging file.

Return Value:

    NTSTATUS.

--*/

{
    PMDL Mdl;
    LARGE_INTEGER Offset = {0};
    PULONG Block;
    IO_STATUS_BLOCK IoStatus;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    KEVENT Event;

    Mdl = (PMDL)&MdlHack[0];

    MmCreateMdl (Mdl, NULL, PAGE_SIZE);

    Mdl->MdlFlags |= MDL_PAGES_LOCKED;

    Page = (PPFN_NUMBER)(Mdl + 1);

    *Page = MiGetPageForHeader ();

    Block = MmGetSystemAddressForMdl (Mdl);

    RtlZeroMemory (Block, PAGE_SIZE);

    KeInitializeEvent (&Event, NotificationEvent, FALSE);

    Status = IoSynchronousPageWrite (File,
                                     Mdl,
                                     &Offset,
                                     &Event,
                                     &IoStatus);

    if (NT_SUCCESS (Status)) {

        KeWaitForSingleObject (&Event,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               NULL);

        Status = IoStatus.Status;
    }

    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
    }

    MiRemoveImageHeaderPage (*Page);

    return Status;
}


NTSTATUS
NtCreatePagingFile (
    IN PUNICODE_STRING PageFileName,
    IN PLARGE_INTEGER MinimumSize,
    IN PLARGE_INTEGER MaximumSize,
    IN ULONG Priority OPTIONAL
    )

/*++

Routine Description:

    This routine opens the specified file, attempts to write a page
    to the specified file, and creates the necessary structures to
    use the file as a paging file.

    If this file is the first paging file, the modified page writer
    is started.

    This system service requires the caller to have SeCreatePagefilePrivilege.

Arguments:

    PageFileName - Supplies the fully qualified file name.

    MinimumSize - Supplies the starting size of the paging file.
                  This value is rounded up to the host page size.

    MaximumSize - Supplies the maximum number of bytes to write to the file.
                  This value is rounded up to the host page size.

    Priority - Supplies the relative priority of this paging file.

Return Value:

    tbs

--*/

{
    PFILE_OBJECT File;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES PagingFileAttributes;
    HANDLE FileHandle;
    IO_STATUS_BLOCK IoStatus;
    UNICODE_STRING CapturedName;
    PWSTR CapturedBuffer;
    LARGE_INTEGER CapturedMaximumSize;
    LARGE_INTEGER CapturedMinimumSize;
    FILE_END_OF_FILE_INFORMATION EndOfFileInformation;
    KPROCESSOR_MODE PreviousMode;
    FILE_FS_DEVICE_INFORMATION FileDeviceInfo;
    ULONG ReturnedLength;
    ULONG PageFileNumber;
    ULONG NewMaxSizeInPages;
    ULONG NewMinSizeInPages;
    PMMPAGING_FILE FoundExisting;
    PMMPAGING_FILE NewPagingFile;
    PRTL_BITMAP NewBitmap;
    PRTL_BITMAP OldBitmap;
    PDEVICE_OBJECT deviceObject;
    MMPAGE_FILE_EXPANSION PageExtend;
    SECURITY_DESCRIPTOR SecurityDescriptor;
    ULONG DaclLength;
    PACL Dacl;

    DBG_UNREFERENCED_PARAMETER (Priority);

    PAGED_CODE();

    CapturedBuffer = NULL;
    Dacl = NULL;

    if (MmNumberOfPagingFiles == MAX_PAGE_FILES) {

        //
        // The maximum number of paging files is already in use.
        //

        return STATUS_TOO_MANY_PAGING_FILES;
    }

    PreviousMode = KeGetPreviousMode();

    if (PreviousMode != KernelMode) {

        //
        // Make sure the caller has the proper privilege for this.
        //

        if (!SeSinglePrivilegeCheck (SeCreatePagefilePrivilege, PreviousMode)) {
            return STATUS_PRIVILEGE_NOT_HELD;
        }

        //
        // Probe arguments.
        //

        try {

#if !defined (_WIN64)

            //
            // Note we only probe for byte alignment because early releases
            // of NT did and we don't want to break user apps
            // that had bad alignment if they worked before.
            //

            ProbeForReadSmallStructure (PageFileName,
                                        sizeof(*PageFileName),
                                        sizeof(UCHAR));
#else
            ProbeForReadSmallStructure (PageFileName,
                                        sizeof(*PageFileName),
                                        PROBE_ALIGNMENT (UNICODE_STRING));
#endif

            ProbeForReadSmallStructure (MaximumSize,
                                        sizeof(LARGE_INTEGER),
                                        PROBE_ALIGNMENT (LARGE_INTEGER));

            ProbeForReadSmallStructure (MinimumSize,
                                        sizeof(LARGE_INTEGER),
                                        PROBE_ALIGNMENT (LARGE_INTEGER));

            //
            // Capture arguments.
            //

            CapturedMinimumSize = *MinimumSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {

        //
        // Capture arguments.
        //

        CapturedMinimumSize = *MinimumSize;
    }

    if ((CapturedMinimumSize.QuadPart > MI_MAXIMUM_PAGEFILE_SIZE) ||
        (CapturedMinimumSize.LowPart < MINIMUM_PAGE_FILE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    if (PreviousMode != KernelMode) {

        try {
            CapturedMaximumSize = *MaximumSize;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {
        CapturedMaximumSize = *MaximumSize;
    }

    if (CapturedMaximumSize.QuadPart > MI_MAXIMUM_PAGEFILE_SIZE) {
        return STATUS_INVALID_PARAMETER_3;
    }

    if (CapturedMinimumSize.QuadPart > CapturedMaximumSize.QuadPart) {
        return STATUS_INVALID_PARAMETER_3;
    }

    if (PreviousMode != KernelMode) {
        try {
            CapturedName = *PageFileName;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }
    else {
        CapturedName = *PageFileName;
    }

    CapturedName.MaximumLength = CapturedName.Length;

    if ((CapturedName.Length == 0) ||
        (CapturedName.Length > MAXIMUM_FILENAME_LENGTH )) {
        return STATUS_OBJECT_NAME_INVALID;
    }

    CapturedBuffer = ExAllocatePoolWithTag (PagedPool,
                                            (ULONG)CapturedName.Length,
                                            '  mM');

    if (CapturedBuffer == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (PreviousMode != KernelMode) {
        try {

            ProbeForRead (CapturedName.Buffer,
                          CapturedName.Length,
                          sizeof (UCHAR));

            //
            // Copy the string to the allocated buffer.
            //

            RtlCopyMemory (CapturedBuffer,
                           CapturedName.Buffer,
                           CapturedName.Length);

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            ExFreePool (CapturedBuffer);

            return GetExceptionCode();
        }
    }
    else {

        //
        // Copy the string to the allocated buffer.
        //

        RtlCopyMemory (CapturedBuffer,
                       CapturedName.Buffer,
                       CapturedName.Length);
    }

    //
    // Point the buffer to the string that was just copied.
    //

    CapturedName.Buffer = CapturedBuffer;

    //
    // Create a security descriptor to protect the pagefile.
    //
    Status = RtlCreateSecurityDescriptor (&SecurityDescriptor,
                                          SECURITY_DESCRIPTOR_REVISION);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
    DaclLength = sizeof (ACL) + sizeof (ACCESS_ALLOWED_ACE) * 2 +
                 RtlLengthSid (SeLocalSystemSid) +
                 RtlLengthSid (SeAliasAdminsSid);

    Dacl = ExAllocatePoolWithTag (PagedPool, DaclLength, 'lcaD');

    if (Dacl == NULL) {
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn1;
    }

    Status = RtlCreateAcl (Dacl, DaclLength, ACL_REVISION);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     FILE_ALL_ACCESS,
                                     SeAliasAdminsSid);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     FILE_ALL_ACCESS,
                                     SeLocalSystemSid);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
  
    Status = RtlSetDaclSecurityDescriptor (&SecurityDescriptor,
                                           TRUE,
                                           Dacl,
                                           FALSE);

    if (!NT_SUCCESS (Status)) {
        goto ErrorReturn1;
    }
  

    //
    // Open a paging file and get the size.
    //

    InitializeObjectAttributes (&PagingFileAttributes,
                                &CapturedName,
                                (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                NULL,
                                NULL);

//
// Note this macro cannot use ULONG_PTR as it must also work on PAE.
//

#define ROUND64_TO_PAGES(Size)  (((ULONG64)(Size) + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))

    EndOfFileInformation.EndOfFile.QuadPart =
                                ROUND64_TO_PAGES (CapturedMinimumSize.QuadPart);

    Status = IoCreateFile (&FileHandle,
                           FILE_READ_DATA | FILE_WRITE_DATA | WRITE_DAC | SYNCHRONIZE,
                           &PagingFileAttributes,
                           &IoStatus,
                           &CapturedMinimumSize,
                           FILE_ATTRIBUTE_HIDDEN | FILE_ATTRIBUTE_SYSTEM,
                           FILE_SHARE_WRITE,
                           FILE_SUPERSEDE,
                           FILE_NO_INTERMEDIATE_BUFFERING | FILE_NO_COMPRESSION | FILE_DELETE_ON_CLOSE,
                           NULL,
                           0L,
                           CreateFileTypeNone,
                           NULL,
                           IO_OPEN_PAGING_FILE | IO_NO_PARAMETER_CHECKING);

    if (NT_SUCCESS(Status)) {

        //
        // Update the DACL in case there was a pre-existing regular file named
        // pagefile.sys (even supersede above does not do this).
        //

        if (NT_SUCCESS(IoStatus.Status)) {

            Status = ZwSetSecurityObject (FileHandle,
                                          DACL_SECURITY_INFORMATION,
                                          &SecurityDescriptor);

            if (!NT_SUCCESS(Status)) {
                goto ErrorReturn2;
            }
        }
    }
    else {

        //
        // Treat this as an extension of an existing pagefile maximum -
        // and try to open rather than create the paging file specified.
        //

        Status = IoCreateFile (&FileHandle,
                           FILE_WRITE_DATA | SYNCHRONIZE,
                           &PagingFileAttributes,
                           &IoStatus,
                           &CapturedMinimumSize,
                           FILE_ATTRIBUTE_HIDDEN | FILE_ATTRIBUTE_SYSTEM,
                           FILE_SHARE_READ | FILE_SHARE_WRITE,
                           FILE_OPEN,
                           FILE_NO_INTERMEDIATE_BUFFERING | FILE_NO_COMPRESSION,
                           (PVOID) NULL,
                           0L,
                           CreateFileTypeNone,
                           (PVOID) NULL,
                           IO_OPEN_PAGING_FILE | IO_NO_PARAMETER_CHECKING);

        if (!NT_SUCCESS(Status)) {

#if DBG
            if (Status != STATUS_DISK_FULL) {
                DbgPrint("MM MODWRITE: unable to open paging file %wZ - status = %X \n", &CapturedName, Status);
            }
#endif

            goto ErrorReturn1;
        }

        Status = ObReferenceObjectByHandle (FileHandle,
                                            FILE_READ_DATA | FILE_WRITE_DATA,
                                            IoFileObjectType,
                                            KernelMode,
                                            (PVOID *)&File,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            goto ErrorReturn2;
        }

        FoundExisting = NULL;

        ExAcquireFastMutex (&MmPageFileCreationLock);

        for (PageFileNumber = 0; PageFileNumber < MmNumberOfPagingFiles; PageFileNumber += 1) {
            if (MmPagingFile[PageFileNumber]->File->SectionObjectPointer == File->SectionObjectPointer) {
                FoundExisting = MmPagingFile[PageFileNumber];
                break;
            }
        }

        if (FoundExisting == NULL) {
            Status = STATUS_NOT_FOUND;
            goto ErrorReturn4;
        }

        //
        // Check for increases in the minimum or the maximum paging file sizes.
        // Decreasing either paging file size on the fly is not allowed.
        //

        NewMaxSizeInPages = (ULONG)(CapturedMaximumSize.QuadPart >> PAGE_SHIFT);
        NewMinSizeInPages = (ULONG)(CapturedMinimumSize.QuadPart >> PAGE_SHIFT);

        if (FoundExisting->MinimumSize > NewMinSizeInPages) {
            Status = STATUS_INVALID_PARAMETER_2;
            goto ErrorReturn4;
        }

        if (FoundExisting->MaximumSize > NewMaxSizeInPages) {
            Status = STATUS_INVALID_PARAMETER_3;
            goto ErrorReturn4;
        }

        if (NewMaxSizeInPages > FoundExisting->MaximumSize) {

            //
            // Make sure that the pagefile increase doesn't cause the commit
            // limit (in pages) to wrap.  Currently this can only happen on
            // PAE systems where 16 pagefiles of 16TB (==256TB) is greater
            // than the 32-bit commit variable (max is 16TB).
            //

            if (MmTotalCommitLimitMaximum + (NewMaxSizeInPages - FoundExisting->MaximumSize) <= MmTotalCommitLimitMaximum) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn4;
            }

            //
            // Handle the increase to the maximum paging file size.
            //

            MiCreateBitMap (&NewBitmap, NewMaxSizeInPages, NonPagedPool);

            if (NewBitmap == NULL) {
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn4;
            }

            OldBitmap = FoundExisting->Bitmap;

            MiExtendPagingFileMaximum (PageFileNumber, NewBitmap);

            MiRemoveBitMap (&OldBitmap);

            //
            // We may be low on commitment and/or may have put a temporary
            // stopgate on things.  Clear up the logjam now by forcing an
            // extension and immediately returning it.
            //

            if (MmTotalCommittedPages + 100 > MmTotalCommitLimit) {
                if (MiChargeCommitment (200, NULL) == TRUE) {
                    MiReturnCommitment (200);
                }
            }
        }

        if (NewMinSizeInPages > FoundExisting->MinimumSize) {

            //
            // Handle the increase to the minimum paging file size.
            //

            if (NewMinSizeInPages > FoundExisting->Size) {

                //
                // Queue a message to the segment dereferencing / pagefile
                // extending thread to see if the page file can be extended.
                //

                PageExtend.InProgress = 1;
                PageExtend.ActualExpansion = 0;
                PageExtend.RequestedExpansionSize = NewMinSizeInPages - FoundExisting->Size;
                PageExtend.Segment = NULL;
                PageExtend.PageFileNumber = PageFileNumber;
                KeInitializeEvent (&PageExtend.Event, NotificationEvent, FALSE);

                MiIssuePageExtendRequest (&PageExtend);
            }

            //
            // The current size is now greater than the new desired minimum.
            // Ensure subsequent contractions obey this new minimum.
            //

            if (FoundExisting->Size >= NewMinSizeInPages) {
                ASSERT (FoundExisting->Size >= FoundExisting->MinimumSize);
                ASSERT (NewMinSizeInPages >= FoundExisting->MinimumSize);
                FoundExisting->MinimumSize = NewMinSizeInPages;
            }
            else {

                //
                // The pagefile could not be expanded to handle the new minimum.
                // No easy way to undo any maximum raising that may have been
                // done as the space may have already been used, so just set
                // Status so our caller knows it didn't all go perfectly.
                //

                Status = STATUS_INSUFFICIENT_RESOURCES;
            }
        }

        goto ErrorReturn4;
    }

    if (!NT_SUCCESS(IoStatus.Status)) {
        KdPrint(("MM MODWRITE: unable to open paging file %wZ - iosb %lx\n", &CapturedName, IoStatus.Status));
        Status = IoStatus.Status;
        goto ErrorReturn1;
    }

    //
    // Make sure that the pagefile increase doesn't cause the commit
    // limit (in pages) to wrap.  Currently this can only happen on
    // PAE systems where 16 pagefiles of 16TB (==256TB) is greater
    // than the 32-bit commit variable (max is 16TB).
    //

    if (MmTotalCommitLimitMaximum + (CapturedMaximumSize.QuadPart >> PAGE_SHIFT)
        <= MmTotalCommitLimitMaximum) {
        Status = STATUS_INVALID_PARAMETER_3;
        goto ErrorReturn2;
    }

    Status = ZwSetInformationFile (FileHandle,
                                   &IoStatus,
                                   &EndOfFileInformation,
                                   sizeof(EndOfFileInformation),
                                   FileEndOfFileInformation);

    if (!NT_SUCCESS(Status)) {
        KdPrint(("MM MODWRITE: unable to set length of paging file %wZ status = %X \n",
                 &CapturedName, Status));
        goto ErrorReturn2;
    }

    if (!NT_SUCCESS(IoStatus.Status)) {
        KdPrint(("MM MODWRITE: unable to set length of paging file %wZ - iosb %lx\n",
                &CapturedName, IoStatus.Status));
        Status = IoStatus.Status;
        goto ErrorReturn2;
    }

    Status = ObReferenceObjectByHandle ( FileHandle,
                                         FILE_READ_DATA | FILE_WRITE_DATA,
                                         IoFileObjectType,
                                         KernelMode,
                                         (PVOID *)&File,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        KdPrint(("MM MODWRITE: Unable to reference paging file - %wZ\n",
                 &CapturedName));
        goto ErrorReturn2;
    }

    //
    // Get the address of the target device object and ensure
    // the specified file is of a suitable type.
    //

    deviceObject = IoGetRelatedDeviceObject (File);

    if ((deviceObject->DeviceType != FILE_DEVICE_DISK_FILE_SYSTEM) &&
        (deviceObject->DeviceType != FILE_DEVICE_NETWORK_FILE_SYSTEM) &&
        (deviceObject->DeviceType != FILE_DEVICE_DFS_VOLUME) &&
        (deviceObject->DeviceType != FILE_DEVICE_DFS_FILE_SYSTEM)) {
            KdPrint(("MM MODWRITE: Invalid paging file type - %x\n",
                     deviceObject->DeviceType));
            Status = STATUS_UNRECOGNIZED_VOLUME;
            goto ErrorReturn3;
    }

    //
    // Make sure the specified file is not currently being used
    // as a mapped data file.
    //

    Status = MiCheckPageFileMapping (File);
    if (!NT_SUCCESS(Status)) {
        goto ErrorReturn3;
    }

    //
    // Make sure the volume is not a floppy disk.
    //

    Status = IoQueryVolumeInformation ( File,
                                        FileFsDeviceInformation,
                                        sizeof(FILE_FS_DEVICE_INFORMATION),
                                        &FileDeviceInfo,
                                        &ReturnedLength
                                      );

    if (FILE_FLOPPY_DISKETTE & FileDeviceInfo.Characteristics) {
        Status = STATUS_FLOPPY_VOLUME;
        goto ErrorReturn3;
    }

    //
    // Check with all of the drivers along the path to the file to ensure
    // that they are willing to follow the rules required of them and to
    // give them a chance to lock down code and data that needs to be locked.
    // If any of the drivers along the path refuses to participate, fail the
    // pagefile creation.
    //

    Status = PpPagePathAssign(File);
    if (!NT_SUCCESS(Status)) {
        KdPrint(( "PpPagePathAssign(%wZ) FAILED: %x\n", &CapturedName, Status ));
        //
        // Fail the pagefile creation if the storage stack tells us to.
        //

        goto ErrorReturn3;
    }

    NewPagingFile = ExAllocatePoolWithTag (NonPagedPool,
                                           sizeof(MMPAGING_FILE),
                                           '  mM');

    if (NewPagingFile == NULL) {

        //
        // Allocate pool failed.
        //

        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    RtlZeroMemory (NewPagingFile, sizeof(MMPAGING_FILE));

    NewPagingFile->File = File;
    NewPagingFile->FileHandle = FileHandle;
    NewPagingFile->Size = (PFN_NUMBER)(CapturedMinimumSize.QuadPart >> PAGE_SHIFT);
    NewPagingFile->MinimumSize = NewPagingFile->Size;
    NewPagingFile->FreeSpace = NewPagingFile->Size - 1;

    NewPagingFile->MaximumSize = (PFN_NUMBER)(CapturedMaximumSize.QuadPart >>
                                                PAGE_SHIFT);

    //
    // Adjust the commit page limit to reflect the new page file space.
    //

    NewPagingFile->Entry[0] = ExAllocatePoolWithTag (NonPagedPool,
                                            sizeof(MMMOD_WRITER_MDL_ENTRY) +
                                            MmModifiedWriteClusterSize *
                                            sizeof(PFN_NUMBER),
                                            '  mM');

    if (NewPagingFile->Entry[0] == NULL) {

        //
        // Allocate pool failed.
        //

        ExFreePool (NewPagingFile);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    RtlZeroMemory (NewPagingFile->Entry[0], sizeof(MMMOD_WRITER_MDL_ENTRY));

    NewPagingFile->Entry[0]->PagingListHead = &MmPagingFileHeader;

    NewPagingFile->Entry[0]->PagingFile = NewPagingFile;

    NewPagingFile->Entry[1] = ExAllocatePoolWithTag (NonPagedPool,
                                            sizeof(MMMOD_WRITER_MDL_ENTRY) +
                                            MmModifiedWriteClusterSize *
                                            sizeof(PFN_NUMBER),
                                            '  mM');

    if (NewPagingFile->Entry[1] == NULL) {

        //
        // Allocate pool failed.
        //

        ExFreePool (NewPagingFile->Entry[0]);
        ExFreePool (NewPagingFile);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    RtlZeroMemory (NewPagingFile->Entry[1], sizeof(MMMOD_WRITER_MDL_ENTRY));

    NewPagingFile->Entry[1]->PagingListHead = &MmPagingFileHeader;

    NewPagingFile->Entry[1]->PagingFile = NewPagingFile;

    NewPagingFile->PageFileName = CapturedName;

    MiCreateBitMap (&NewPagingFile->Bitmap,
                    NewPagingFile->MaximumSize,
                    NonPagedPool);

    if (NewPagingFile->Bitmap == NULL) {

        //
        // Allocate pool failed.
        //

        ExFreePool (NewPagingFile->Entry[0]);
        ExFreePool (NewPagingFile->Entry[1]);
        ExFreePool (NewPagingFile);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    Status = MiZeroPageFileFirstPage (File);

    if (!NT_SUCCESS (Status)) {

        //
        // The storage stack could not zero the first page of the file.
        // This means an old crashdump signature could still be around so
        // fail the create.
        //

        ExFreePool (NewPagingFile->Entry[0]);
        ExFreePool (NewPagingFile->Entry[1]);
        ExFreePool (NewPagingFile);
        MiRemoveBitMap (&NewPagingFile->Bitmap);
        goto ErrorReturn3;
    }

    RtlSetAllBits (NewPagingFile->Bitmap);

    //
    // Set the first bit as 0 is an invalid page location, clear the
    // following bits.
    //

    RtlClearBits (NewPagingFile->Bitmap,
                  1,
                  (ULONG)(NewPagingFile->Size - 1));

    //
    // See if this pagefile is on the boot partition, and if so, mark it
    // so we can find it later if someone enables crashdump.
    //

    if (File->DeviceObject->Flags & DO_SYSTEM_BOOT_PARTITION) {
        NewPagingFile->BootPartition = TRUE;
    }
    else {
        NewPagingFile->BootPartition = FALSE;
    }

    //
    // Acquire the global page file creation mutex.
    //

    ExAcquireFastMutex (&MmPageFileCreationLock);

    PageFileNumber = MmNumberOfPagingFiles;

    MmPagingFile[PageFileNumber] = NewPagingFile;

    NewPagingFile->PageFileNumber = PageFileNumber;

    MiInsertPageFileInList ();

    if (PageFileNumber == 0) {

        //
        // The first paging file has been created and reservation of any
        // crashdump pages has completed, signal the modified
        // page writer.
        //

        MiReleaseModifiedWriter ();
    }

    ExReleaseFastMutex (&MmPageFileCreationLock);

    //
    // Note that the file handle (a kernel handle) is not closed during the
    // create path (it IS duped and closed in the pagefile size extending path)
    // to prevent the paging file from being deleted or opened again.  It is
    // also kept open so that extensions of existing pagefiles can be detected
    // because successive IoCreateFile calls will fail.
    //

    if ((!MmSystemPageFileLocated) &&
        (File->DeviceObject->Flags & DO_SYSTEM_BOOT_PARTITION)) {
        MmSystemPageFileLocated = IoInitializeCrashDump (FileHandle);
    }

    return STATUS_SUCCESS;

    //
    // Error returns:
    //

ErrorReturn4:
    ExReleaseFastMutex (&MmPageFileCreationLock);

ErrorReturn3:
    ObDereferenceObject (File);

ErrorReturn2:
    ZwClose (FileHandle);

ErrorReturn1:
    if (Dacl != NULL) {
        ExFreePool (Dacl);
    }
    ExFreePool (CapturedBuffer);

    return Status;
}


HANDLE
MmGetSystemPageFile (
    VOID
    )
/*++

Routine Description:

    Returns a filehandle to the paging file on the system boot partition. This is
    used by crashdump to enable crashdump after the system has already booted.

Arguments:

    None

Return Value:

    A filehandle to the paging file on the system boot partition

    NULL if no such pagefile exists

--*/

{
    HANDLE FileHandle;
    ULONG PageFileNumber;

    PAGED_CODE();

    FileHandle = NULL;

    ExAcquireFastMutex (&MmPageFileCreationLock);
    for (PageFileNumber = 0; PageFileNumber < MmNumberOfPagingFiles; PageFileNumber += 1) {
        if (MmPagingFile[PageFileNumber]->BootPartition) {
            FileHandle = MmPagingFile[PageFileNumber]->FileHandle;
        }
    }
    ExReleaseFastMutex (&MmPageFileCreationLock);

    return FileHandle;
}


LOGICAL
MmIsFileObjectAPagingFile (
    IN PFILE_OBJECT FileObject
    )
/*++

Routine Description:

    Returns TRUE if the file object refers to a paging file, FALSE if not.

Arguments:

    FileObject - Supplies the file object in question.

Return Value:

    Returns TRUE if the file object refers to a paging file, FALSE if not.

--*/

{
    PMMPAGING_FILE PageFile;
    PMMPAGING_FILE *PagingFile;
    PMMPAGING_FILE *PagingFileEnd;

    //
    // It's ok to check without synchronization.
    //

    PagingFile = MmPagingFile;
    PagingFileEnd = PagingFile + MmNumberOfPagingFiles;

    while (PagingFile < PagingFileEnd) {
        PageFile = *PagingFile;
        if (PageFile->File == FileObject) {
            return TRUE;
        }
        PagingFile += 1;
    }

    return FALSE;
}


VOID
MiExtendPagingFileMaximum (
    IN ULONG PageFileNumber,
    IN PRTL_BITMAP NewBitmap
    )

/*++

Routine Description:

    This routine switches from the old bitmap to the new (larger) bitmap.

Arguments:

    PageFileNumber - Supplies the paging file number to be extended.

    NewBitmap - Supplies the new bitmap to use.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, MmPageFileCreationLock held.

--*/

{
    KIRQL OldIrql;
    PRTL_BITMAP OldBitmap;
    SIZE_T Delta;

    OldBitmap = MmPagingFile[PageFileNumber]->Bitmap;

    RtlSetAllBits (NewBitmap);

    LOCK_PFN (OldIrql);

    //
    // Copy the bits from the existing map.
    //

    RtlCopyMemory (NewBitmap->Buffer,
                   OldBitmap->Buffer,
                   ((OldBitmap->SizeOfBitMap + 31) / 32) * sizeof (ULONG));

    Delta = NewBitmap->SizeOfBitMap - OldBitmap->SizeOfBitMap;

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, Delta);

    MmPagingFile[PageFileNumber]->MaximumSize = NewBitmap->SizeOfBitMap;

    MmPagingFile[PageFileNumber]->Bitmap = NewBitmap;

    //
    // If any MDLs are waiting for space, get them up now.
    //

    if (!IsListEmpty (&MmFreePagingSpaceLow)) {
        MiUpdateModifiedWriterMdls (PageFileNumber);
    }

    UNLOCK_PFN (OldIrql);
}


VOID
MiFinishPageFileExtension (
    IN ULONG PageFileNumber,
    IN PFN_NUMBER AdditionalAllocation
    )

/*++

Routine Description:

    This routine finishes the specified page file extension.

Arguments:

    PageFileNumber - Supplies the page file number to attempt to extend.

    SizeNeeded - Supplies the number of pages to extend the file by.

    Maximum - Supplies TRUE if the page file should be extended
              by the maximum size possible, but not to exceed
              SizeNeeded.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPAGING_FILE PagingFile;

    //
    // Clear bits within the paging file bitmap to allow the extension
    // to take effect.
    //

    PagingFile = MmPagingFile[PageFileNumber];

    LOCK_PFN (OldIrql);

    ASSERT (RtlCheckBit (PagingFile->Bitmap, PagingFile->Size) == 1);

    RtlClearBits (PagingFile->Bitmap,
                  (ULONG)PagingFile->Size,
                  (ULONG)AdditionalAllocation);

    PagingFile->Size += AdditionalAllocation;
    PagingFile->FreeSpace += AdditionalAllocation;

    MiUpdateModifiedWriterMdls (PageFileNumber);

    UNLOCK_PFN (OldIrql);

    return;
}


SIZE_T
MiAttemptPageFileExtension (
    IN ULONG PageFileNumber,
    IN SIZE_T SizeNeeded,
    IN LOGICAL Maximum
    )

/*++

Routine Description:

    This routine attempts to extend the specified page file by SizeNeeded.

Arguments:

    PageFileNumber - Supplies the page file number to attempt to extend.

    SizeNeeded - Supplies the number of pages to extend the file by.

    Maximum - Supplies TRUE if the page file should be extended
              by the maximum size possible, but not to exceed
              SizeNeeded.

Return Value:

    Returns the size of the extension.  Zero if the page file cannot
    be extended.

--*/

{

    NTSTATUS status;
    FILE_FS_SIZE_INFORMATION FileInfo;
    FILE_END_OF_FILE_INFORMATION EndOfFileInformation;
    ULONG AllocSize;
    ULONG ReturnedLength;
    PFN_NUMBER PagesAvailable;
    SIZE_T SizeToExtend;
    SIZE_T MinimumExtension;
    LARGE_INTEGER BytesAvailable;

    //
    // Check to see if this page file is at the maximum.
    //

    if (MmPagingFile[PageFileNumber]->Size ==
                                    MmPagingFile[PageFileNumber]->MaximumSize) {
        return 0;
    }

    //
    // Find out how much free space is on this volume.
    //

    status = IoQueryVolumeInformation (MmPagingFile[PageFileNumber]->File,
                                       FileFsSizeInformation,
                                       sizeof(FileInfo),
                                       &FileInfo,
                                       &ReturnedLength);

    if (!NT_SUCCESS (status)) {

        //
        // The volume query did not succeed - return 0 indicating
        // the paging file was not extended.
        //

        return 0;
    }

    //
    // Attempt to extend by at least 16 megabytes, if that fails then attempt
    // for at least a megabyte.
    //

    MinimumExtension = MmPageFileExtension << 4;

retry:

    SizeToExtend = SizeNeeded;

    if (SizeNeeded < MinimumExtension) {
        SizeToExtend = MinimumExtension;
    }
    else {
        MinimumExtension = MmPageFileExtension;
    }

    //
    // Don't go over the maximum size for the paging file.
    //

    ASSERT (MmPagingFile[PageFileNumber]->MaximumSize >= MmPagingFile[PageFileNumber]->Size);

    PagesAvailable = MmPagingFile[PageFileNumber]->MaximumSize -
                     MmPagingFile[PageFileNumber]->Size;

    if (SizeToExtend > PagesAvailable) {
        SizeToExtend = PagesAvailable;

        if ((SizeToExtend < SizeNeeded) && (Maximum == FALSE)) {

            //
            // Can't meet the requested (mandatory) requirement.
            //

            return 0;
        }
    }

    //
    // See if there is enough space on the volume for the extension.
    //

    AllocSize = FileInfo.SectorsPerAllocationUnit * FileInfo.BytesPerSector;

    BytesAvailable = RtlExtendedIntegerMultiply (
                        FileInfo.AvailableAllocationUnits,
                        AllocSize);

    if ((UINT64)BytesAvailable.QuadPart > (UINT64)MmMinimumFreeDiskSpace) {

        BytesAvailable.QuadPart = BytesAvailable.QuadPart -
                                    (LONGLONG)MmMinimumFreeDiskSpace;

        if ((UINT64)BytesAvailable.QuadPart > (UINT64)(SizeToExtend << PAGE_SHIFT)) {
            BytesAvailable.QuadPart = (LONGLONG)(SizeToExtend << PAGE_SHIFT);
        }

        PagesAvailable = (PFN_NUMBER)(BytesAvailable.QuadPart >> PAGE_SHIFT);

        if ((Maximum == FALSE) && (PagesAvailable < SizeNeeded)) {

            //
            // Can't meet the requested (mandatory) requirement.
            //

            return 0;
        }

    }
    else {

        //
        // Not enough space is available period.
        //

        return 0;
    }

#if defined (_WIN64) || defined (_X86PAE_)
    EndOfFileInformation.EndOfFile.QuadPart =
              ((ULONG64)MmPagingFile[PageFileNumber]->Size + PagesAvailable) * PAGE_SIZE;
#else
    EndOfFileInformation.EndOfFile.LowPart =
              (MmPagingFile[PageFileNumber]->Size + PagesAvailable) * PAGE_SIZE;

    //
    // Set high part to zero as paging files are limited to 4GB.
    //

    EndOfFileInformation.EndOfFile.HighPart = 0;
#endif

    //
    // Attempt to extend the file by setting the end-of-file position.
    //

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    status = IoSetInformation (MmPagingFile[PageFileNumber]->File,
                               FileEndOfFileInformation,
                               sizeof(FILE_END_OF_FILE_INFORMATION),
                               &EndOfFileInformation);

    if (status != STATUS_SUCCESS) {
        KdPrint(("MM MODWRITE: page file extension failed %p %lx\n",PagesAvailable,status));

        if (MinimumExtension != MmPageFileExtension) {
            MinimumExtension = MmPageFileExtension;
            goto retry;
        }

        return 0;
    }

    MiFinishPageFileExtension (PageFileNumber, PagesAvailable);

    return PagesAvailable;
}

SIZE_T
MiExtendPagingFiles (
    IN PMMPAGE_FILE_EXPANSION PageExpand
    )

/*++

Routine Description:

    This routine attempts to extend the paging files to provide
    SizeNeeded bytes.

    Note - Page file expansion and page file reduction are synchronized
           because a single thread is responsible for performing the
           operation.  Hence, while expansion is occurring, a reduction
           request will be queued to the thread.

Arguments:

    PageFileNumber - Supplies the page file number to extend.
                     MI_EXTEND_ANY_PAGFILE indicates to extend any page file.

Return Value:

    Returns the size of the extension.  Zero if the page file(s) cannot
    be extended.

--*/

{
    SIZE_T DesiredQuota;
    ULONG PageFileNumber;
    SIZE_T ExtendedSize;
    SIZE_T SizeNeeded;
    ULONG i;
    SIZE_T CommitLimit;
    SIZE_T CommittedPages;

    DesiredQuota = PageExpand->RequestedExpansionSize;
    PageFileNumber = PageExpand->PageFileNumber;

    ASSERT (PageExpand->ActualExpansion == 0);

    ASSERT (PageFileNumber < MmNumberOfPagingFiles || PageFileNumber == MI_EXTEND_ANY_PAGEFILE);

    if (MmNumberOfPagingFiles == 0) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    if (PageFileNumber < MmNumberOfPagingFiles) {
        i = PageFileNumber;
        ExtendedSize = MmPagingFile[i]->MaximumSize - MmPagingFile[i]->Size;
        if (ExtendedSize < DesiredQuota) {
            InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
            return 0;
        }

        ExtendedSize = MiAttemptPageFileExtension (i, DesiredQuota, FALSE);
        goto alldone;
    }

    //
    // Snap the globals into locals so calculations will be consistent from
    // step to step.  It is ok to snap the globals unsynchronized with respect
    // to each other as even when pagefile expansion occurs, the expansion
    // space is not reserved for the caller - any process could consume
    // the expansion prior to this routine returning.
    //

    CommittedPages = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    SizeNeeded = CommittedPages + DesiredQuota + MmSystemCommitReserve;

    //
    // Check to make sure the request does not wrap.
    //

    if (SizeNeeded < CommittedPages) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Check to see if ample space already exists.
    //

    if (SizeNeeded <= CommitLimit) {
        PageExpand->ActualExpansion = 1;
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 1;
    }

    //
    // Calculate the additional pages needed.
    //

    SizeNeeded -= CommitLimit;
    if (SizeNeeded > MmSystemCommitReserve) {
        SizeNeeded -= MmSystemCommitReserve;
    }

    //
    // Make sure ample space exists within the paging files.
    //

    i = 0;
    ExtendedSize = 0;

    do {
        ExtendedSize += MmPagingFile[i]->MaximumSize - MmPagingFile[i]->Size;
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    if (ExtendedSize < SizeNeeded) {
        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Attempt to extend only one of the paging files.
    //

    i = 0;
    do {
        ExtendedSize = MiAttemptPageFileExtension (i, SizeNeeded, FALSE);
        if (ExtendedSize != 0) {
            goto alldone;
        }
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    ASSERT (ExtendedSize == 0);

    if (MmNumberOfPagingFiles == 1) {

        //
        // If the attempt didn't succeed for one (not enough disk space free) -
        // don't try to set it to the maximum size.
        //

        InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
        return 0;
    }

    //
    // Attempt to extend all paging files.
    //

    i = 0;
    do {
        ASSERT (SizeNeeded > ExtendedSize);
        ExtendedSize += MiAttemptPageFileExtension (i,
                                                    SizeNeeded - ExtendedSize,
                                                    TRUE);
        if (ExtendedSize >= SizeNeeded) {
            goto alldone;
        }
        i += 1;
    } while (i < MmNumberOfPagingFiles);

    //
    // Not enough space is available.
    //

    InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);
    return 0;

alldone:

    ASSERT (ExtendedSize != 0);

    PageExpand->ActualExpansion = ExtendedSize;

    //
    // Increase the systemwide commit limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, ExtendedSize);

    //
    // Clear the in progress flag - if this is the global cantexpand structure
    // it is possible for it to be immediately reused.
    //

    InterlockedExchange ((PLONG)&PageExpand->InProgress, 0);

    return ExtendedSize;
}

VOID
MiContractPagingFiles (
    VOID
    )

/*++

Routine Description:

    This routine checks to see if ample space is no longer committed
    and if so, does enough free space exist in any paging file.

    IFF the answer to both these is affirmative, a reduction in the
    paging file size(s) is attempted.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PMMPAGE_FILE_EXPANSION PageReduce;

    //
    // This is an unsynchronized check but that's ok.  The real check is
    // made when the packet below is processed by the dereference thread.
    //

    if (MmTotalCommittedPages >= ((MmTotalCommitLimit/10)*8)) {
        return;
    }

    if ((MmTotalCommitLimit - MmMinimumPageFileReduction) <=
                                                       MmTotalCommittedPages) {
        return;
    }

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        if (MmPagingFile[i]->Size != MmPagingFile[i]->MinimumSize) {
            if (MmPagingFile[i]->FreeSpace > MmMinimumPageFileReduction) {
                break;
            }
        }
    }

    if (i == MmNumberOfPagingFiles) {
        return;
    }

    PageReduce = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(MMPAGE_FILE_EXPANSION),
                                        '  mM');

    if (PageReduce == NULL) {
        return;
    }

    PageReduce->Segment = NULL;
    PageReduce->RequestedExpansionSize = MI_CONTRACT_PAGEFILES;

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    InsertTailList (&MmDereferenceSegmentHeader.ListHead,
                    &PageReduce->DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore, 0L, 1L, FALSE);
    return;
}

VOID
MiAttemptPageFileReduction (
    VOID
    )

/*++

Routine Description:

    This routine attempts to reduce the size of the paging files to
    their minimum levels.

    Note - Page file expansion and page file reduction are synchronized
           because a single thread is responsible for performing the
           operation.  Hence, while expansion is occurring, a reduction
           request will be queued to the thread.

Arguments:

    None.

Return Value:

    None.

--*/

{
    SIZE_T CommitLimit;
    SIZE_T CommittedPages;
    SIZE_T SafetyMargin;
    KIRQL OldIrql;
    ULONG i;
    PFN_NUMBER StartReduction;
    PFN_NUMBER ReductionSize;
    PFN_NUMBER TryBit;
    PFN_NUMBER TryReduction;
    PMMPAGING_FILE PagingFile;
    SIZE_T MaxReduce;
    FILE_ALLOCATION_INFORMATION FileAllocationInfo;
    NTSTATUS status;

    //
    // Snap the globals into locals so calculations will be consistent from
    // step to step.  It is ok to snap the globals unsynchronized with respect
    // to each other.
    //

    CommittedPages = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    //
    // Make sure the commit limit is significantly greater than the number
    // of committed pages to avoid thrashing.
    //

    SafetyMargin = 2 * MmMinimumPageFileReduction;

    if (CommittedPages + SafetyMargin >= ((CommitLimit/10)*8)) {
        return;
    }

    MaxReduce = ((CommitLimit/10)*8) - (CommittedPages + SafetyMargin);

    ASSERT ((SSIZE_T)MaxReduce > 0);
    ASSERT ((LONG_PTR)MaxReduce >= 0);

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {

        if (MaxReduce < MmMinimumPageFileReduction) {

            //
            // Don't reduce any more paging files.
            //

            break;
        }

        PagingFile = MmPagingFile[i];

        if (PagingFile->Size == PagingFile->MinimumSize) {
            continue;
        }

        //
        // This unsynchronized check is ok because a synchronized check is
        // made later.
        //

        if (PagingFile->FreeSpace < MmMinimumPageFileReduction) {
            continue;
        }

        //
        // Lock the PFN database and check to see if ample pages
        // are free at the end of the paging file.
        //

        TryBit = PagingFile->Size - MmMinimumPageFileReduction;
        TryReduction = MmMinimumPageFileReduction;

        if (TryBit <= PagingFile->MinimumSize) {
            TryBit = PagingFile->MinimumSize;
            TryReduction = PagingFile->Size - PagingFile->MinimumSize;
        }

        StartReduction = 0;
        ReductionSize = 0;

        LOCK_PFN (OldIrql);

        do {

            //
            // Try to reduce.
            //

            if ((ReductionSize + TryReduction) > MaxReduce) {

                //
                // The reduction attempt would remove more
                // than MaxReduce pages.
                //

                break;
            }

            if (RtlAreBitsClear (PagingFile->Bitmap,
                                 (ULONG)TryBit,
                                 (ULONG)TryReduction)) {

                //
                // Can reduce it by TryReduction, see if it can
                // be made smaller.
                //

                StartReduction = TryBit;
                ReductionSize += TryReduction;

                if (StartReduction == PagingFile->MinimumSize) {
                    break;
                }

                TryBit = StartReduction - MmMinimumPageFileReduction;

                if (TryBit <= PagingFile->MinimumSize) {
                    TryReduction -= PagingFile->MinimumSize - TryBit;
                    TryBit = PagingFile->MinimumSize;
                }
                else {
                    TryReduction = MmMinimumPageFileReduction;
                }
            }
            else {

                //
                // Reduction has failed.
                //

                break;
            }

        } while (TRUE);

        //
        // Make sure there are no outstanding writes to
        // pages within the start reduction range.
        //

        if (StartReduction != 0) {

            //
            // There is an outstanding write past where the
            // new end of the paging file should be.  This
            // is a very rare condition, so just punt shrinking
            // the file.
            //

            ASSERT (MM_PAGING_FILE_MDLS == 2);

            if ((PagingFile->Entry[0]->LastPageToWrite > StartReduction) ||
                (PagingFile->Entry[1]->LastPageToWrite > StartReduction)) {

                StartReduction = 0;
            }
        }

        //
        // If there are no pages to remove, march on to the next pagefile.
        //

        if (StartReduction == 0) {
            UNLOCK_PFN (OldIrql);
            continue;
        }

        //
        // Reduce the paging file's size and free space.
        //

        ASSERT (ReductionSize == (PagingFile->Size - StartReduction));

        PagingFile->Size = StartReduction;
        PagingFile->FreeSpace -= ReductionSize;

        RtlSetBits (PagingFile->Bitmap,
                    (ULONG)StartReduction,
                    (ULONG)ReductionSize );

        //
        // Release the PFN lock now that the size info
        // has been updated.
        //

        UNLOCK_PFN (OldIrql);

        MaxReduce -= ReductionSize;
        ASSERT ((LONG)MaxReduce >= 0);

        //
        // Change the commit limit to reflect the returned page file space.
        // First try to charge the reduction amount to confirm that the
        // reduction is still a sensible thing to do.
        //

        if (MiChargeTemporaryCommitmentForReduction (ReductionSize + SafetyMargin) == FALSE) {

            LOCK_PFN (OldIrql);

            PagingFile->Size = StartReduction + ReductionSize;
            PagingFile->FreeSpace += ReductionSize;

            RtlClearBits (PagingFile->Bitmap,
                          (ULONG)StartReduction,
                          (ULONG)ReductionSize );

            UNLOCK_PFN (OldIrql);

            ASSERT ((LONG)(MaxReduce + ReductionSize) >= 0);

            break;
        }

        //
        // Reduce the systemwide commit limit - note this is carefully done
        // *PRIOR* to returning this commitment so no one else (including a DPC
        // in this very thread) can consume past the limit.
        //

        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, 0 - ReductionSize);

        //
        // Now that the systemwide commit limit has been lowered, the amount
        // we have removed can be safely returned.
        //

        MiReturnCommitment (ReductionSize + SafetyMargin);

#if defined (_WIN64) || defined (_X86PAE_)
        FileAllocationInfo.AllocationSize.QuadPart =
                                       ((ULONG64)StartReduction << PAGE_SHIFT);

#else
        FileAllocationInfo.AllocationSize.LowPart = StartReduction * PAGE_SIZE;

        //
        // Set high part to zero, paging files are limited to 4gb.
        //

        FileAllocationInfo.AllocationSize.HighPart = 0;
#endif

        //
        // Reduce the allocated size of the paging file
        // thereby actually freeing the space and
        // setting a new end of file.
        //

        ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

        status = IoSetInformation (PagingFile->File,
                                   FileAllocationInformation,
                                   sizeof(FILE_ALLOCATION_INFORMATION),
                                   &FileAllocationInfo);
#if DBG
        //
        // Ignore errors on truncating the paging file
        // as we can always have less space in the bitmap
        // than the pagefile holds.
        //

        if (status != STATUS_SUCCESS) {
            DbgPrint ("MM: pagefile truncate status %lx\n", status);
        }
#endif
    }

    return;
}


VOID
MiLdwPopupWorker (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine is the worker routine to send a lost delayed write data popup
    for a given control area/file.

Arguments:

    Context - Supplies a pointer to the MM_LDW_WORK_CONTEXT for the failed I/O.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    NTSTATUS Status;
    PFILE_OBJECT FileObject;
    PMM_LDW_WORK_CONTEXT LdwContext;
    POBJECT_NAME_INFORMATION FileNameInfo;

    PAGED_CODE();

    LdwContext = (PMM_LDW_WORK_CONTEXT) Context;
    FileObject = LdwContext->FileObject;
    FileNameInfo = NULL;

    ExFreePool (LdwContext);

    //
    // Throw the popup with the user-friendly form, if possible.
    // If everything fails, the user probably couldn't have figured
    // out what failed either.
    //

    Status = IoQueryFileDosDeviceName (FileObject, &FileNameInfo);

    if (Status == STATUS_SUCCESS) {

        IoRaiseInformationalHardError (STATUS_LOST_WRITEBEHIND_DATA,
                                       &FileNameInfo->Name,
                                       NULL);

    }
    else {
        if ((FileObject->FileName.Length) &&
            (FileObject->FileName.MaximumLength) &&
            (FileObject->FileName.Buffer)) {

            IoRaiseInformationalHardError (STATUS_LOST_WRITEBEHIND_DATA,
                                           &FileObject->FileName,
                                           NULL);
        }
    }

    //
    // Now drop the reference to the file object and clean up.
    //

    ObDereferenceObject (FileObject);

    if (FileNameInfo != NULL) {
        ExFreePool(FileNameInfo);
    }
}


VOID
MiWriteComplete (
    IN PVOID Context,
    IN PIO_STATUS_BLOCK IoStatus,
    IN ULONG Reserved
    )

/*++

Routine Description:

    This routine is the APC write completion procedure.  It is invoked
    at APC_LEVEL when a page write operation is completed.

Arguments:

    Context - Supplies a pointer to the MOD_WRITER_MDL_ENTRY which was
              used for this I/O.

    IoStatus - Supplies a pointer to the IO_STATUS_BLOCK which was used
               for this I/O.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL.

--*/

{

    PMMMOD_WRITER_MDL_ENTRY WriterEntry;
    PMMMOD_WRITER_MDL_ENTRY NextWriterEntry;
    PPFN_NUMBER Page;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    LONG ByteCount;
    NTSTATUS status;
    PCONTROL_AREA ControlArea;
    ULONG FailAllIo;
    PFILE_OBJECT FileObject;
    PERESOURCE FileResource;

    UNREFERENCED_PARAMETER (Reserved);

    FailAllIo = FALSE;

#if DBG
    if (MmDebug & MM_DBG_MOD_WRITE) {
        DbgPrint("MM MODWRITE: modified page write completed\n");
    }
#endif

    //
    // A page write has completed, at this time the pages are not
    // on any lists, write-in-progress is set in the PFN database,
    // and the reference count was incremented.
    //

    WriterEntry = (PMMMOD_WRITER_MDL_ENTRY)Context;
    ByteCount = (LONG)WriterEntry->Mdl.ByteCount;
    Page = &WriterEntry->Page[0];

    if (WriterEntry->Mdl.MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
        MmUnmapLockedPages (WriterEntry->Mdl.MappedSystemVa,
                            &WriterEntry->Mdl);
    }

    //
    // Get the PFN lock so the PFN database can be manipulated.
    //

    status = IoStatus->Status;
    ControlArea = WriterEntry->ControlArea;

    LOCK_PFN (OldIrql);

    //
    // Indicate that the write is complete.
    //

    WriterEntry->LastPageToWrite = 0;


    while (ByteCount > 0) {

        Pfn1 = MI_PFN_ELEMENT (*Page);
        ASSERT (Pfn1->u3.e1.WriteInProgress == 1);
#if DBG
#if !defined (_WIN64)
        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            ULONG Offset;
            Offset = GET_PAGING_FILE_OFFSET(Pfn1->OriginalPte);
            if ((Offset < 8192) &&
                    (GET_PAGING_FILE_NUMBER(Pfn1->OriginalPte) == 0)) {
                ASSERT ((MmPagingFileDebug[Offset] & 1) != 0);
                if (!MI_IS_PFN_DELETED(Pfn1)) {
                    if ((GET_PAGING_FILE_NUMBER (Pfn1->OriginalPte)) == 0) {
                        if ((MmPagingFileDebug[Offset] & ~0x1f) !=
                                   ((ULONG_PTR)Pfn1->PteAddress << 3)) {
                            if (Pfn1->PteAddress != MiGetPteAddress(PDE_BASE)) {

                                //
                                // Make sure this isn't a PTE that was forked
                                // during the I/O.
                                //

                                if ((Pfn1->PteAddress < (PMMPTE)PTE_TOP) ||
                                    ((Pfn1->OriginalPte.u.Soft.Protection &
                                            MM_COPY_ON_WRITE_MASK) ==
                                                MM_PROTECTION_WRITE_MASK)) {
                                    DbgPrint("MMWRITE: Mismatch Pfn1 %p Offset %lx info %p\n",
                                             Pfn1,
                                             Offset,
                                             MmPagingFileDebug[Offset]);

                                    DbgBreakPoint();

                                }
                                else {
                                    MmPagingFileDebug[Offset] &= 0x1f;
                                    MmPagingFileDebug[Offset] |=
                                        ((ULONG_PTR)Pfn1->PteAddress << 3);
                                }
                            }

                        }
                    }
                }
            }
        }
#endif
#endif //DBG

        Pfn1->u3.e1.WriteInProgress = 0;

        if (NT_ERROR(status)) {

            //
            // If the file object is over the network, assume that this
            // I/O operation can never complete and mark the pages as
            // clean and indicate in the control area all I/O should fail.
            // Note that the modified bit in the PFN database is not set.
            //
            // If the user changes the protection on the volume containing the
            // file to readonly, this puts us in a problematic situation.  We
            // cannot just keep retrying the writes because if there are no
            // other pages that can be written, not writing these can cause the
            // system to run out of pages, ie: bugcheck 4D.  So throw away
            // these pages just as if they were on a network that has
            // disappeared.
            //

            if (((status != STATUS_FILE_LOCK_CONFLICT) &&
                (ControlArea != NULL) &&
                (ControlArea->u.Flags.Networked == 1))
                            ||
                (status == STATUS_FILE_INVALID)
                            ||
                ((status == STATUS_MEDIA_WRITE_PROTECTED) &&
                 (ControlArea != NULL))) {

                if (ControlArea->u.Flags.FailAllIo == 0) {
                    ControlArea->u.Flags.FailAllIo = 1;
                    FailAllIo = TRUE;

                    KdPrint(("MM MODWRITE: failing all io, controlarea %p status %lx\n",
                          ControlArea, status));
                }
            }
            else {

                //
                // The modified write operation failed, SET the modified bit
                // for each page which was written and free the page file
                // space.
                //

#if DBG
                if ((status != STATUS_FILE_LOCK_CONFLICT) &&
                   ((MmDebug & MM_DBG_PRINTS_MODWRITES) == 0)) {
                    KdPrint(("MM MODWRITE: modified page write iosb failed - status 0x%lx\n",
                            status));
                }
#endif

                MI_SET_MODIFIED (Pfn1, 1, 0x9);
            }
        }

        if ((Pfn1->u3.e1.Modified == 1) &&
            (Pfn1->OriginalPte.u.Soft.Prototype == 0)) {

            //
            // This page was modified since the write was done,
            // release the page file space.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 15);

#if DBG
        *Page = 0xF0FFFFFF;
#endif

        Page += 1;
        ByteCount -= (LONG)PAGE_SIZE;
    }

    //
    // Check to which list to insert this entry into depending on
    // the amount of free space left in the paging file.
    //

    FileObject = WriterEntry->File;
    FileResource = WriterEntry->FileResource;

    if ((WriterEntry->PagingFile != NULL) &&
        (WriterEntry->PagingFile->FreeSpace < MM_USABLE_PAGES_FREE)) {

        InsertTailList (&MmFreePagingSpaceLow, &WriterEntry->Links);
        WriterEntry->CurrentList = &MmFreePagingSpaceLow;
        MmNumberOfActiveMdlEntries -= 1;

        if (MmNumberOfActiveMdlEntries == 0) {

            //
            // If we leave this entry on the list, there will be
            // no more paging.  Locate all entries which are non
            // zero and pull them from the list.
            //

            WriterEntry = (PMMMOD_WRITER_MDL_ENTRY)MmFreePagingSpaceLow.Flink;

            while ((PLIST_ENTRY)WriterEntry != &MmFreePagingSpaceLow) {

                NextWriterEntry =
                            (PMMMOD_WRITER_MDL_ENTRY)WriterEntry->Links.Flink;

                if (WriterEntry->PagingFile->FreeSpace != 0) {

                    RemoveEntryList (&WriterEntry->Links);

                    //
                    // Insert this into the active list.
                    //

                    if (IsListEmpty (&WriterEntry->PagingListHead->ListHead)) {
                        KeSetEvent (&WriterEntry->PagingListHead->Event,
                                    0,
                                    FALSE);
                    }

                    InsertTailList (&WriterEntry->PagingListHead->ListHead,
                                    &WriterEntry->Links);
                    WriterEntry->CurrentList = &MmPagingFileHeader.ListHead;
                    MmNumberOfActiveMdlEntries += 1;
                }

                WriterEntry = NextWriterEntry;
            }

        }
    }
    else {

#if DBG
        if (WriterEntry->PagingFile == NULL) {
            MmNumberOfMappedMdlsInUse -= 1;
        }
#endif
        //
        // Ample space exists, put this on the active list.
        //

        if (IsListEmpty (&WriterEntry->PagingListHead->ListHead)) {
            KeSetEvent (&WriterEntry->PagingListHead->Event, 0, FALSE);
        }

        InsertTailList (&WriterEntry->PagingListHead->ListHead,
                        &WriterEntry->Links);
    }

    ASSERT (((ULONG_PTR)WriterEntry->Links.Flink & 1) == 0);

    UNLOCK_PFN (OldIrql);

    if (FileResource != NULL) {
        FsRtlReleaseFileForModWrite (FileObject, FileResource);
    }

    if (FailAllIo) {

        PMM_LDW_WORK_CONTEXT LdwContext;

        //
        // Reference our fileobject and queue the popup.
        // The DOS name translation must occur at PASSIVE_LEVEL - we're at APC.
        //

        LdwContext = ExAllocatePoolWithTag (NonPagedPool,
                                            sizeof(MM_LDW_WORK_CONTEXT),
                                            'pdmM');

        if (LdwContext != NULL) {
            LdwContext->FileObject = ControlArea->FilePointer;
            ObReferenceObject (LdwContext->FileObject);

            ExInitializeWorkItem (&LdwContext->WorkItem,
                                  MiLdwPopupWorker,
                                  (PVOID)LdwContext);

            ExQueueWorkItem (&LdwContext->WorkItem, DelayedWorkQueue);
        }
    }

    if (ControlArea != NULL) {

        LOCK_PFN (OldIrql);

        //
        // A write to a mapped file just completed, check to see if
        // there are any waiters on the completion of this i/o.
        //

        ControlArea->ModifiedWriteCount -= 1;
        ASSERT ((SHORT)ControlArea->ModifiedWriteCount >= 0);
        if (ControlArea->u.Flags.SetMappedFileIoComplete != 0) {
            KePulseEvent (&MmMappedFileIoComplete,
                          0,
                          FALSE);
        }

        if (MiDrainingMappedWrites == TRUE) {
            if (MmModifiedPageListHead.Flink != MM_EMPTY_LIST) {
                MiTimerPending = TRUE;
                KeSetEvent (&MiMappedPagesTooOldEvent, 0, FALSE);
            }
            else {
                MiDrainingMappedWrites = FALSE;
            }
        }

        ControlArea->NumberOfPfnReferences -= 1;

        if (ControlArea->NumberOfPfnReferences == 0) {

            //
            // This routine return with the PFN lock released!.
            //

            MiCheckControlArea (ControlArea, NULL, OldIrql);
        }
        else {
            UNLOCK_PFN (OldIrql);
        }
    }

    if (NT_ERROR(status)) {

        //
        // Wait for a short time so other processing can continue.
        //

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&Mm30Milliseconds);

        if (MmIsRetryIoStatus(status)) {

            //
            // Low resource scenarios are a chicken and egg problem.  The
            // mapped and modified writers must make forward progress to
            // alleviate low memory situations.  If these threads are
            // unable to write data due to resource problems in the driver
            // stack then temporarily fall back to single page I/Os as
            // the stack guarantees forward progress with those.  This
            // causes the low memory situation to persist slightly longer
            // but ensures that it won't become terminal either.
            //

            LOCK_PFN (OldIrql);
            MiClusterWritesDisabled = MI_SLOW_CLUSTER_WRITES;
            UNLOCK_PFN (OldIrql);
        }
    }
    else {

        //
        // Check first without lock synchronization so the common case is
        // not slowed.
        //

        if (MiClusterWritesDisabled != 0) {

            LOCK_PFN (OldIrql);

            //
            // Recheck now that the lock is held.
            //

            if (MiClusterWritesDisabled != 0) {
                ASSERT (MiClusterWritesDisabled <= MI_SLOW_CLUSTER_WRITES);
                MiClusterWritesDisabled -= 1;
            }

            UNLOCK_PFN (OldIrql);
        }
    }

    return;
}

LOGICAL
MiCancelWriteOfMappedPfn (
    IN PFN_NUMBER PageToStop
    )

/*++

Routine Description:

    This routine attempts to stop a pending mapped page writer write for the
    specified PFN.  Note that if the write can be stopped, any other pages
    that may be clustered with the write are also stopped.

Arguments:

    PageToStop - Supplies the frame number that the caller wants to stop.

Return Value:

    TRUE if the write was stopped, FALSE if not.

Environment:

    Kernel mode, PFN lock held.  The PFN lock is released and reacquired if
    the write was stopped.

    N.B.  No other locks may be held as IRQL is lowered to APC_LEVEL here.

--*/

{
    ULONG i;
    ULONG PageCount;
    KIRQL OldIrql;
    PPFN_NUMBER Page;
    PLIST_ENTRY NextEntry;
    PMDL MemoryDescriptorList;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    //
    // Walk the MmMappedPageWriterList looking for an MDL which contains
    // the argument page.  If found, remove it and cancel the write.
    //

    NextEntry = MmMappedPageWriterList.Flink;
    while (NextEntry != &MmMappedPageWriterList) {

        ModWriterEntry = CONTAINING_RECORD(NextEntry,
                                           MMMOD_WRITER_MDL_ENTRY,
                                           Links);

        MemoryDescriptorList = &ModWriterEntry->Mdl;
        PageCount = (MemoryDescriptorList->ByteCount >> PAGE_SHIFT);
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        for (i = 0; i < PageCount; i += 1) {
            if (*Page == PageToStop) {
                RemoveEntryList (NextEntry);
                goto CancelWrite;
            }
            Page += 1;
        }

        NextEntry = NextEntry->Flink;
    }

    return FALSE;

CancelWrite:

    UNLOCK_PFN (APC_LEVEL);

    //
    // File lock conflict to indicate an error has occurred,
    // but that future I/Os should be allowed.  Keep APCs disabled and
    // call the write completion routine.
    //

    ModWriterEntry->u.IoStatus.Status = STATUS_FILE_LOCK_CONFLICT;
    ModWriterEntry->u.IoStatus.Information = 0;

    MiWriteComplete ((PVOID)ModWriterEntry,
                     &ModWriterEntry->u.IoStatus,
                     0 );

    LOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MiModifiedPageWriter (
    IN PVOID StartContext
    )

/*++

Routine Description:

    Implements the NT modified page writer thread.  When the modified
    page threshold is reached, or memory becomes overcommitted the
    modified page writer event is set, and this thread becomes active.

Arguments:

    StartContext - not used.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    HANDLE ThreadHandle;
    OBJECT_ATTRIBUTES ObjectAttributes;
    PMMMOD_WRITER_MDL_ENTRY ModWriteEntry;

    PAGED_CODE();

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Initialize listheads as empty.
    //

    MmSystemShutdown = 0;
    KeInitializeEvent (&MmPagingFileHeader.Event, NotificationEvent, FALSE);
    KeInitializeEvent (&MmMappedFileHeader.Event, NotificationEvent, FALSE);

    InitializeListHead(&MmPagingFileHeader.ListHead);
    InitializeListHead(&MmMappedFileHeader.ListHead);
    InitializeListHead(&MmFreePagingSpaceLow);

    //
    // Allocate enough MDLs such that 2% of system memory can be pending
    // at any point in time in mapped writes.  Even smaller memory systems
    // get 20 MDLs as the minimum.
    //

    MmNumberOfMappedMdls = MmNumberOfPhysicalPages / (32 * 1024);

    if (MmNumberOfMappedMdls < 20) {
        MmNumberOfMappedMdls = 20;
    }

    for (i = 0; i < MmNumberOfMappedMdls; i += 1) {
        ModWriteEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MMMOD_WRITER_MDL_ENTRY) +
                                                MmModifiedWriteClusterSize *
                                                    sizeof(PFN_NUMBER),
                                                'eWmM');

        if (ModWriteEntry == NULL) {
            break;
        }

        ModWriteEntry->PagingFile = NULL;
        ModWriteEntry->PagingListHead = &MmMappedFileHeader;

        InsertTailList (&MmMappedFileHeader.ListHead, &ModWriteEntry->Links);
    }

    MmNumberOfMappedMdls = i;

    //
    // Make this a real time thread.
    //

    KeSetPriorityThread (&PsGetCurrentThread()->Tcb, LOW_REALTIME_PRIORITY + 1);

    //
    // Start a secondary thread for writing mapped file pages.  This
    // is required as the writing of mapped file pages could cause
    // page faults resulting in requests for free pages.  But there
    // could be no free pages - hence a dead lock.  Rather than deadlock
    // the whole system waiting on the modified page writer, creating
    // a secondary thread allows that thread to block without affecting
    // on going page file writes.
    //

    KeInitializeEvent (&MmMappedPageWriterEvent, NotificationEvent, FALSE);
    InitializeListHead(&MmMappedPageWriterList);
    InitializeObjectAttributes( &ObjectAttributes, NULL, 0, NULL, NULL );

    PsCreateSystemThread (&ThreadHandle,
                          THREAD_ALL_ACCESS,
                          &ObjectAttributes,
                          0L,
                          NULL,
                          MiMappedPageWriter,
                          NULL );
    ZwClose (ThreadHandle);
    MiModifiedPageWriterWorker();

    //
    // Shutdown in progress, wait forever.
    //

    {
        LARGE_INTEGER Forever;

        //
        // System has shutdown, go into LONG wait.
        //

        Forever.LowPart = 0;
        Forever.HighPart = 0xF000000;
        KeDelayExecutionThread (KernelMode, FALSE, &Forever);
    }

    return;
}


VOID
MiModifiedPageWriterTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )

/*++

Routine Description:

    This routine is executed whenever modified mapped pages are waiting to
    be written.  Its job is to signal the Modified Page Writer to write
    these out.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    DeferredContext - Optional deferred context;  not used.

    SystemArgument1 - Optional argument 1;  not used.

    SystemArgument2 - Optional argument 2;  not used.

Return Value:

    None.

--*/

{
    UNREFERENCED_PARAMETER (Dpc);
    UNREFERENCED_PARAMETER (DeferredContext);
    UNREFERENCED_PARAMETER (SystemArgument1);
    UNREFERENCED_PARAMETER (SystemArgument2);

    LOCK_PFN_AT_DPC ();

    MiTimerPending = TRUE;
    KeSetEvent (&MiMappedPagesTooOldEvent, 0, FALSE);

    UNLOCK_PFN_FROM_DPC ();
}


VOID
MiModifiedPageWriterWorker (
    VOID
    )

/*++

Routine Description:

    Implements the NT modified page writer thread.  When the modified
    page threshold is reached, or memory becomes overcommitted the
    modified page writer event is set, and this thread becomes active.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PagesWritten;
    KIRQL OldIrql;
    ULONG NextColor;
    static KWAIT_BLOCK WaitBlockArray[ModifiedWriterMaximumObject];
    PVOID WaitObjects[ModifiedWriterMaximumObject];
    NTSTATUS WakeupStatus;

    PsGetCurrentThread()->MemoryMaker = 1;

    //
    // Wait for the modified page writer event or the mapped pages event.
    //

    WaitObjects[NormalCase] = (PVOID)&MmModifiedPageWriterEvent;
    WaitObjects[MappedPagesNeedWriting] = (PVOID)&MiMappedPagesTooOldEvent;

    for (;;) {

        WakeupStatus = KeWaitForMultipleObjects(ModifiedWriterMaximumObject,
                                          &WaitObjects[0],
                                          WaitAny,
                                          WrFreePage,
                                          KernelMode,
                                          FALSE,
                                          NULL,
                                          &WaitBlockArray[0]);

        //
        // Switch on the wait status.
        //

        switch (WakeupStatus) {

        case NormalCase:
                break;

        case MappedPagesNeedWriting:

                //
                // Our mapped pages DPC went off, only deal with those pages.
                // Write all the mapped pages (ONLY), then clear the flag
                // and come back to the top.
                //

                break;

        default:
                break;

        }

        //
        // Indicate that the hint values have not been reset in
        // the paging files.
        //

        if (MmNumberOfPagingFiles != 0) {
            i = 0;
            do {
                MmPagingFile[i]->HintSetToZero = FALSE;
                i += 1;
            } while (i < MmNumberOfPagingFiles);
        }

        NextColor = 0;
        PagesWritten = 0;

        LOCK_PFN (OldIrql);

        for (;;) {

            //
            // Modified page writer was signalled.
            //

            if (MmModifiedPageListHead.Total == 0) {

                //
                // No more pages, clear the event(s) and wait again...
                // Note we can clear both events regardless of why we woke up
                // since no modified pages of any type exist.
                //

                if (MiTimerPending == TRUE) {
                    MiTimerPending = FALSE;
                    KeClearEvent (&MiMappedPagesTooOldEvent);
                }

                UNLOCK_PFN (OldIrql);

                KeClearEvent (&MmModifiedPageWriterEvent);

                break;
            }

            //
            // If we didn't wake up explicitly to deal with mapped pages,
            // then determine which type of pages are the most popular:
            // page file backed pages, or mapped file backed pages.
            //

            if (WakeupStatus == MappedPagesNeedWriting) {
                PageFrameIndex = MmModifiedPageListHead.Flink;
                if (PageFrameIndex == MM_EMPTY_LIST) {

                    //
                    // No more modified mapped pages (there may still be
                    // modified pagefile-destined pages), so clear only the
                    // mapped pages event and check for directions at the top
                    // again.
                    //

                    MiTimerPending = FALSE;
                    KeClearEvent (&MiMappedPagesTooOldEvent);

                    UNLOCK_PFN (OldIrql);

                    break;
                }
                MiDrainingMappedWrites = TRUE;
            }
            else if (MmTotalPagesForPagingFile >=
                (MmModifiedPageListHead.Total - MmTotalPagesForPagingFile)) {

                //
                // More pages are destined for the paging file.
                //

                MI_GET_MODIFIED_PAGE_ANY_COLOR (PageFrameIndex, NextColor);

            }
            else {

                //
                // More pages are destined for mapped files.
                //

                PageFrameIndex = MmModifiedPageListHead.Flink;
            }

            //
            // Check to see what type of page (section file backed or page
            // file backed) and write out that page and more if possible.
            //

            //
            // Check to see if this page is destined for a paging file or
            // a mapped file.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {
                if (IsListEmpty (&MmMappedFileHeader.ListHead)) {

                    //
                    // Make sure page is destined for paging file as there
                    // are no MDLs for mapped writes free.
                    //

                    if (WakeupStatus != MappedPagesNeedWriting) {

                        MI_GET_MODIFIED_PAGE_ANY_COLOR (PageFrameIndex, NextColor);

                        //
                        // No pages are destined for the paging file, get the
                        // first page destined for a mapped file.
                        //

                        if (PageFrameIndex == MM_EMPTY_LIST) {

                            //
                            // Select the first page from the list anyway.
                            //

                            PageFrameIndex = MmModifiedPageListHead.Flink;
                        }

                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    }
                }
            }
            else if ((IsListEmpty(&MmPagingFileHeader.ListHead)) ||
                       (MiFirstPageFileCreatedAndReady == FALSE)) {

                //
                // Try for a dirty section-backed page as no paging file MDLs
                // are available.
                //

                if (MmModifiedPageListHead.Flink != MM_EMPTY_LIST) {
                    ASSERT (MmTotalPagesForPagingFile != MmModifiedPageListHead.Total);
                    PageFrameIndex = MmModifiedPageListHead.Flink;
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                }
                else {
                    ASSERT (MmTotalPagesForPagingFile == MmModifiedPageListHead.Total);
                    if ((MiFirstPageFileCreatedAndReady == FALSE) &&
                        (MmNumberOfPagingFiles != 0)) {

                        //
                        // The first paging has been created but the reservation
                        // checking for crashdumps has not finished yet.  Delay
                        // a bit as this will finish shortly and then restart.
                        //

                        UNLOCK_PFN (OldIrql);
                        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                        LOCK_PFN (OldIrql);
                        continue;
                    }
                }
            }

            if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {

                if (IsListEmpty(&MmMappedFileHeader.ListHead)) {

                    if (WakeupStatus == MappedPagesNeedWriting) {

                        //
                        // Since we woke up only to take care of mapped pages,
                        // don't wait for an MDL below because drivers may take
                        // an inordinate amount of time processing the
                        // outstanding ones.  We might have to wait too long,
                        // resulting in the system running out of pages.
                        //

                        if (MiTimerPending == TRUE) {

                            //
                            // This should be normal case - the reason we must
                            // first check timer pending above is for the rare
                            // case - when this thread first ran for normal
                            // modified page processing and took
                            // care of all the pages including the mapped ones.
                            // Then this thread woke up again for the mapped
                            // reason and here we are.
                            //

                            MiTimerPending = FALSE;
                            KeClearEvent (&MiMappedPagesTooOldEvent);
                        }

                        MiTimerPending = TRUE;

                        (VOID) KeSetTimerEx( &MiModifiedPageWriterTimer, MiModifiedPageLife, 0, &MiModifiedPageWriterTimerDpc );
                        UNLOCK_PFN (OldIrql);
                        break;
                    }

                    //
                    // Reset the event indicating no mapped files in
                    // the list, drop the PFN lock and wait for an
                    // I/O operation to complete with a one second
                    // timeout.
                    //

                    KeClearEvent (&MmMappedFileHeader.Event);

                    UNLOCK_PFN (OldIrql);
                    KeWaitForSingleObject( &MmMappedFileHeader.Event,
                                           WrPageOut,
                                           KernelMode,
                                           FALSE,
                                           (PLARGE_INTEGER)&Mm30Milliseconds);
                    LOCK_PFN (OldIrql);

                    //
                    // Don't go on as the old PageFrameIndex at the
                    // top of the ModifiedList may have changed states.
                    //

                    continue;
                }

                PagesWritten += MiGatherMappedPages (Pfn1, PageFrameIndex);
            }
            else {

                PagesWritten += MiGatherPagefilePages (Pfn1, PageFrameIndex);
            }

            if (MmSystemShutdown) {

                //
                // Shutdown has returned.  Stop the modified page writer.
                //

                UNLOCK_PFN (OldIrql);
                return;
            }

            //
            // If this is a mapped page timer, then keep on writing till there's
            // nothing left.
            //

            if (WakeupStatus == MappedPagesNeedWriting) {
                continue;
            }

            //
            // If this is a request to write all modified pages, then keep on
            // writing.
            //

            if (MmWriteAllModifiedPages) {
                continue;
            }

            if ((MmAvailablePages > MmFreeGoal) &&
                (MmModifiedPageListHead.Total < MmFreeGoal)) {

                //
                // There are ample pages, clear the event and wait again...
                //

                UNLOCK_PFN (OldIrql);

                KeClearEvent (&MmModifiedPageWriterEvent);
                break;
            }

            if (MmAvailablePages > MmMoreThanEnoughFreePages) {
                if (PagesWritten >= 96) { 

                    //
                    // Always try to write at least 96 pages worth
                    // to make our chore worthwhile.  Otherwise on laptops,
                    // waking up once per second to write a single page
                    // needlessly drains battery life.
                    //

                    UNLOCK_PFN (OldIrql);

                    KeClearEvent (&MmModifiedPageWriterEvent);
                    break;
                }
            }
        } // end for

    } // end for
}

PFN_NUMBER
MiGatherMappedPages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine processes the specified modified page by examining
    the prototype PTE for that page and the adjacent prototype PTEs
    building a cluster of modified pages destined for a mapped file.
    Once the cluster is built, it is sent to the mapped writer thread
    to be processed.

Arguments:

    Pfn1 - Supplies a pointer to the PFN element for the corresponding
           page.

    PageFrameIndex - Supplies the physical page frame to write.

Return Value:

    The number of pages in the attempted write.

Environment:

    PFN lock held.

--*/

{
    PMMPFN Pfn2;
    PFN_NUMBER PagesWritten;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PPFN_NUMBER Page;
    PMMPTE LastPte;
    PMMPTE BasePte;
    PMMPTE NextPte;
    PMMPTE PointerPte;
    PMMPTE StartingPte;
    MMPTE PteContents;
    KIRQL OldIrql;
    PVOID HyperMapped;
    PEPROCESS Process;

    //
    // This page is destined for a mapped file, check to see if
    // there are any physically adjacent pages are also in the
    // modified page list and write them out at the same time.
    //

    Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
    ControlArea = Subsection->ControlArea;

    if (ControlArea->u.Flags.NoModifiedWriting) {

        //
        // This page should not be written out, add it to the
        // tail of the modified NO WRITE list and get the next page.
        //

        MiUnlinkPageFromList (Pfn1);
        MiInsertPageInList (&MmModifiedNoWritePageListHead,
                            PageFrameIndex);
        return 0;
    }

    if (ControlArea->u.Flags.Image) {

#if 0
        //
        // Assert that there are no dangling shared global pages
        // for an image section that is not being used.
        //
        // This assert can be re-enabled when the segment dereference
        // thread list re-insertion is fixed.  Note the recovery code is
        // fine, so disabling the assert is benign.
        //

        ASSERT ((ControlArea->NumberOfMappedViews != 0) ||
                (ControlArea->NumberOfSectionReferences != 0) ||
                (ControlArea->u.Flags.FloppyMedia != 0));
#endif

        //
        // This is an image section, writes are not
        // allowed to an image section.
        //

        //
        // Change page contents to look like it's a demand zero
        // page and put it back into the modified list.
        //

        //
        // Decrement the count for PfnReferences to the
        // segment as paging file pages are not counted as
        // "image" references.
        //

        ControlArea->NumberOfPfnReferences -= 1;
        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
        MiUnlinkPageFromList (Pfn1);

        Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        Pfn1->OriginalPte.u.Soft.Prototype = 0;
        Pfn1->OriginalPte.u.Soft.Transition = 0;

        //
        // Insert the page at the tail of the list and get
        // color update performed.
        //

        MiInsertPageInList (&MmModifiedPageListHead, PageFrameIndex);
        return 0;
    }

    //
    // Look at backwards at previous prototype PTEs to see if
    // this can be clustered into a larger write operation.
    //

    PointerPte = Pfn1->PteAddress;
    NextPte = PointerPte - (MmModifiedWriteClusterSize - 1);

    //
    // Make sure NextPte is in the same page.
    //

    if (NextPte < (PMMPTE)PAGE_ALIGN (PointerPte)) {
        NextPte = (PMMPTE)PAGE_ALIGN (PointerPte);
    }

    //
    // Make sure NextPte is within the subsection.
    //

    if (NextPte < Subsection->SubsectionBase) {
        NextPte = Subsection->SubsectionBase;
    }

    //
    // If the prototype PTEs are not currently mapped,
    // map them via hyperspace.  BasePte refers to the
    // prototype PTEs for nonfaulting references.
    //

    if (MmIsAddressValid (PointerPte)) {
        Process = NULL;
        HyperMapped = NULL;
        BasePte = PointerPte;
    }
    else {
        Process = PsGetCurrentProcess ();
        HyperMapped = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
        BasePte = (PMMPTE)((PCHAR)HyperMapped + BYTE_OFFSET (PointerPte));
    }

    ASSERT (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (BasePte) == PageFrameIndex);

    PointerPte -= 1;
    BasePte -= 1;

    if (MiClusterWritesDisabled != 0) {
        NextPte = PointerPte + 1;
    }

    //
    // Don't go before the start of the subsection nor cross
    // a page boundary.
    //

    while (PointerPte >= NextPte) {

        PteContents = *BasePte;

        //
        // If the page is not in transition, exit loop.
        //

        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Transition == 0) ||
            (PteContents.u.Soft.Prototype == 1)) {

            break;
        }

        Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        //
        // Make sure page is modified and on the modified list.
        //

        if ((Pfn2->u3.e1.Modified == 0 ) ||
            (Pfn2->u3.e2.ReferenceCount != 0)) {
            break;
        }
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        PointerPte -= 1;
        BasePte -= 1;
    }

    StartingPte = PointerPte + 1;
    BasePte = BasePte + 1;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (StartingPte == Pfn1->PteAddress);
    MiUnlinkPageFromList (Pfn1);

    //
    // Get an entry from the list and fill it in.
    //

    ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                    &MmMappedFileHeader.ListHead);

#if DBG
    MmNumberOfMappedMdlsInUse += 1;
    if (MmNumberOfMappedMdlsInUse > MmNumberOfMappedMdlsInUsePeak) {
        MmNumberOfMappedMdlsInUsePeak = MmNumberOfMappedMdlsInUse;
    }
#endif

    ModWriterEntry->File = ControlArea->FilePointer;
    ModWriterEntry->ControlArea = ControlArea;

    //
    // Calculate the offset to read into the file.
    //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
    //

    ModWriterEntry->WriteOffset.QuadPart = MiStartingOffset (Subsection,
                                                             Pfn1->PteAddress);

    MmInitializeMdl(&ModWriterEntry->Mdl, NULL, PAGE_SIZE);

    ModWriterEntry->Mdl.MdlFlags |= MDL_PAGES_LOCKED;

    ModWriterEntry->Mdl.Size = (CSHORT)(sizeof(MDL) +
                      (sizeof(PFN_NUMBER) * MmModifiedWriteClusterSize));

    Page = &ModWriterEntry->Page[0];

    //
    // Up the reference count for the physical page as there
    // is I/O in progress.
    //

    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 14);
    Pfn1->u3.e2.ReferenceCount += 1;

    //
    // Clear the modified bit for the page and set the write
    // in progress bit.
    //

    MI_SET_MODIFIED (Pfn1, 0, 0x23);

    Pfn1->u3.e1.WriteInProgress = 1;

    //
    // Put this physical page into the MDL.
    //

    *Page = PageFrameIndex;

    //
    // See if any adjacent pages are also modified and in
    // the transition state and if so, write them out at
    // the same time.
    //

    LastPte = StartingPte + MmModifiedWriteClusterSize;

    //
    // Look at the last PTE, ensuring a page boundary is not crossed.
    //
    // If LastPte is not in the same page as the StartingPte,
    // set LastPte so the cluster will not cross.
    //

    if (StartingPte < (PMMPTE)PAGE_ALIGN(LastPte)) {
        LastPte = (PMMPTE)PAGE_ALIGN(LastPte);
    }

    //
    // Make sure LastPte is within the subsection.
    //

    if (LastPte > &Subsection->SubsectionBase[Subsection->PtesInSubsection]) {
        LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
    }

    //
    // Look forwards.
    //

    NextPte = BasePte + 1;
    PointerPte = StartingPte + 1;

    if (MiClusterWritesDisabled != 0) {
        LastPte = PointerPte;
    }

    //
    // Loop until an MDL is filled, the end of a subsection
    // is reached, or a page boundary is reached.
    // Note, PointerPte points to the PTE. NextPte points
    // to where it is mapped in hyperspace (if required).
    //

    while (PointerPte < LastPte) {

        PteContents = *NextPte;

        //
        // If the page is not in transition, exit loop.
        //

        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Transition == 0) ||
            (PteContents.u.Soft.Prototype == 1)) {

            break;
        }

        Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        if ((Pfn2->u3.e1.Modified == 0 ) ||
            (Pfn2->u3.e2.ReferenceCount != 0)) {

            //
            // Page is not dirty or not on the modified list,
            // end clustering operation.
            //

            break;
        }
        Page += 1;

        //
        // Add physical page to MDL.
        //

        *Page = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        ASSERT (PointerPte == Pfn2->PteAddress);
        MiUnlinkPageFromList (Pfn2);

        //
        // Up the reference count for the physical page as there
        // is I/O in progress.
        //

        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn2, 14);
        Pfn2->u3.e2.ReferenceCount += 1;

        //
        // Clear the modified bit for the page and set the
        // write in progress bit.
        //

        MI_SET_MODIFIED (Pfn2, 0, 0x24);

        Pfn2->u3.e1.WriteInProgress = 1;

        ModWriterEntry->Mdl.ByteCount += PAGE_SIZE;

        NextPte += 1;
        PointerPte += 1;

    }

    if (HyperMapped != NULL) {
        MiUnmapPageInHyperSpaceFromDpc (Process, HyperMapped);
    }

    ASSERT (BYTES_TO_PAGES (ModWriterEntry->Mdl.ByteCount) <= MmModifiedWriteClusterSize);

    ModWriterEntry->u.LastByte.QuadPart = ModWriterEntry->WriteOffset.QuadPart +
                        ModWriterEntry->Mdl.ByteCount;

    ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

#if DBG
    if ((ULONG)ModWriterEntry->Mdl.ByteCount >
                                ((1+MmModifiedWriteClusterSize)*PAGE_SIZE)) {
        DbgPrint("Mdl %p, MDL End Offset %lx %lx Subsection %p\n",
            ModWriterEntry->Mdl,
            ModWriterEntry->u.LastByte.LowPart,
            ModWriterEntry->u.LastByte.HighPart,
            Subsection);
        DbgBreakPoint();
    }
#endif

    PagesWritten = (ModWriterEntry->Mdl.ByteCount >> PAGE_SHIFT);

    MmInfoCounters.MappedWriteIoCount += 1;
    MmInfoCounters.MappedPagesWriteCount += (ULONG)PagesWritten;

    //
    // Increment the count of modified page writes outstanding
    // in the control area.
    //

    ControlArea->ModifiedWriteCount += 1;

    //
    // Increment the number of PFN references.  This allows the file
    // system to purge (i.e. call MmPurgeSection) modified writes.
    //

    ControlArea->NumberOfPfnReferences += 1;

    ModWriterEntry->FileResource = NULL;

    if (ControlArea->u.Flags.BeingPurged == 1) {
        UNLOCK_PFN (PASSIVE_LEVEL);
        ModWriterEntry->u.IoStatus.Status = STATUS_FILE_LOCK_CONFLICT;
        ModWriterEntry->u.IoStatus.Information = 0;
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        MiWriteComplete ((PVOID)ModWriterEntry,
                         &ModWriterEntry->u.IoStatus,
                         0 );
        KeLowerIrql (OldIrql);
        LOCK_PFN (OldIrql);
        return PagesWritten;
    }

    //
    // Send the entry for the MappedPageWriter.
    //

    InsertTailList (&MmMappedPageWriterList, &ModWriterEntry->Links);

    KeSetEvent (&MmMappedPageWriterEvent, 0, FALSE);

#if 0

    UNLOCK_PFN (OldIrql);

    ModWriterEntry->FileResource = NULL;

    if (ModWriterEntry->ControlArea->u.Flags.FailAllIo == 1) {
        Status = STATUS_UNSUCCESSFUL;
    }
    else {

        Status = FsRtlAcquireFileForModWriteEx (ModWriterEntry->File,
                                                &ModWriterEntry->u.LastByte,
                                                &ModWriterEntry->FileResource);
        if (NT_SUCCESS(Status)) {

            //
            // Issue the write request.
            //

            Status = IoAsynchronousPageWrite (ModWriterEntry->File,
                                              &ModWriterEntry->Mdl,
                                              &ModWriterEntry->WriteOffset,
                                              MiWriteComplete,
                                              (PVOID)ModWriterEntry,
                                              &ModWriterEntry->IoStatus,
                                              &ModWriterEntry->Irp);
        }
        else {

            //
            // Unable to get the file system resources, set error status
            // to lock conflict (ignored by MiWriteComplete) so the APC
            // routine is explicitly called.
            //

            Status = STATUS_FILE_LOCK_CONFLICT;
        }
    }

    if (NT_ERROR(Status)) {

        //
        // An error has occurred, disable APCs and
        // call the write completion routine.
        //

        ModWriterEntry->IoStatus.Status = Status;
        ModWriterEntry->IoStatus.Information = 0;
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        MiWriteComplete ((PVOID)ModWriterEntry,
                         &ModWriterEntry->IoStatus,
                         0 );
        KeLowerIrql (OldIrql);
    }

    LOCK_PFN (OldIrql);
#endif //0
    return PagesWritten;
}

PFN_NUMBER
MiGatherPagefilePages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine processes the specified modified page by getting
    that page and gather any other pages on the modified list destined
    for the paging file in a large write cluster.  This cluster is
    then written to the paging file.

Arguments:

    Pfn1 - Supplies a pointer to the PFN element for the corresponding page.

    PageFrameIndex - Supplies the physical page frame to write.

Return Value:

    The number of pages in the attempted write.

Environment:

    PFN lock held.

--*/

{
    PFILE_OBJECT File;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;
    PMMPAGING_FILE CurrentPagingFile;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    ULONG StartBit;
    LARGE_INTEGER StartingOffset;
    PFN_NUMBER ClusterSize;
    PFN_NUMBER ThisCluster;
    MMPTE LongPte;
    KIRQL OldIrql;
    ULONG NextColor;
    LOGICAL PageFileFull;
    //MM_WRITE_CLUSTER WriteCluster;

    OldIrql = PASSIVE_LEVEL;

    if (IsListEmpty(&MmPagingFileHeader.ListHead)) {

        //
        // Reset the event indicating no paging files MDLs in
        // the list, drop the PFN lock and wait for an
        // I/O operation to complete.
        //

        KeClearEvent (&MmPagingFileHeader.Event);
        UNLOCK_PFN (OldIrql);
        KeWaitForSingleObject( &MmPagingFileHeader.Event,
                               WrPageOut,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)&Mm30Milliseconds);
        LOCK_PFN (OldIrql);

        //
        // Don't go on as the old PageFrameIndex at the
        // top of the ModifiedList may have changed states.
        //

        return 0;
    }

    //
    // Page is destined for the paging file.
    // Find the paging file with the most free space and get a cluster.
    //

#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
    NextColor = Pfn1->u3.e1.PageColor;
#else
    NextColor = 0;
#endif

    ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                    &MmPagingFileHeader.ListHead);
#if DBG
    ModWriterEntry->Links.Flink = MM_IO_IN_PROGRESS;
#endif
    CurrentPagingFile = ModWriterEntry->PagingFile;

    File = ModWriterEntry->PagingFile->File;


    if (MiClusterWritesDisabled == 0) {
        ThisCluster = MmModifiedWriteClusterSize;
    }
    else {
        ThisCluster = 1;
    }

    PageFileFull = FALSE;

    do {
        //
        // Attempt to cluster MmModifiedWriteClusterSize pages
        // together.  Reduce by one half until we succeed or
        // can't find a single page free in the paging file.
        //

        if (((CurrentPagingFile->Hint + MmModifiedWriteClusterSize) >
                                CurrentPagingFile->MinimumSize)
             &&
            (CurrentPagingFile->HintSetToZero == FALSE)) {

            CurrentPagingFile->HintSetToZero = TRUE;
            CurrentPagingFile->Hint = 0;
        }

        StartBit = RtlFindClearBitsAndSet (CurrentPagingFile->Bitmap,
                                           (ULONG)ThisCluster,
                                           (ULONG)CurrentPagingFile->Hint);

        if (StartBit != NO_BITS_FOUND) {
            break;
        }
        if (CurrentPagingFile->Hint != 0) {

            //
            // Start looking from front of the file.
            //

            CurrentPagingFile->Hint = 0;
        }
        else {
            ThisCluster = ThisCluster >> 1;
            PageFileFull = TRUE;
        }

    } while (ThisCluster != 0);

    if (StartBit == NO_BITS_FOUND) {

        //
        // Paging file must be full.
        //

        KdPrint(("MM MODWRITE: page file full\n"));
        ASSERT(CurrentPagingFile->FreeSpace == 0);

        //
        // Move this entry to the not enough space list,
        // and try again.
        //

        InsertTailList (&MmFreePagingSpaceLow,
                        &ModWriterEntry->Links);
        ModWriterEntry->CurrentList = &MmFreePagingSpaceLow;
        MmNumberOfActiveMdlEntries -= 1;
        UNLOCK_PFN (OldIrql);
        MiPageFileFull ();
        LOCK_PFN (OldIrql);
        return 0;
    }

    CurrentPagingFile->FreeSpace -= ThisCluster;
    CurrentPagingFile->CurrentUsage += ThisCluster;
    if (CurrentPagingFile->FreeSpace < 32) {
        PageFileFull = TRUE;
    }

    StartingOffset.QuadPart = (UINT64)StartBit << PAGE_SHIFT;

    MmInitializeMdl(&ModWriterEntry->Mdl, NULL, PAGE_SIZE);

    ModWriterEntry->Mdl.MdlFlags |= MDL_PAGES_LOCKED;

    ModWriterEntry->Mdl.Size = (CSHORT)(sizeof(MDL) +
                    sizeof(PFN_NUMBER) * MmModifiedWriteClusterSize);

    Page = &ModWriterEntry->Page[0];

    ClusterSize = 0;

    //
    // Search through the modified page list looking for other
    // pages destined for the paging file and build a cluster.
    //

    while (ClusterSize != ThisCluster) {

        //
        // Is this page destined for a paging file?
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

#if 0  //********* commented out

            MiClusterWritePages (Pfn1,
                                 PageFrameIndex,
                                 &WriteCluster,
                                 ThisCluster - ClusterSize);
            do {

                PageFrameIndex = WriteCluster.Cluster[WriteCluster.StartIndex];
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
#endif //0
                *Page = PageFrameIndex;

                //
                // Remove the page from the modified list. Note that
                // write-in-progress marks the state.
                //

                //
                // Unlink the page so the same page won't be found
                // on the modified page list by color.
                //

                MiUnlinkPageFromList (Pfn1);
                NextColor = MI_GET_NEXT_COLOR(NextColor);

                MI_GET_MODIFIED_PAGE_BY_COLOR (PageFrameIndex,
                                               NextColor);

                //
                // Up the reference count for the physical page as there
                // is I/O in progress.
                //

                MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 16);
                Pfn1->u3.e2.ReferenceCount += 1;

                //
                // Clear the modified bit for the page and set the
                // write in progress bit.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x25);

                Pfn1->u3.e1.WriteInProgress = 1;
                ASSERT (Pfn1->OriginalPte.u.Soft.PageFileHigh == 0);

                MI_SET_PAGING_FILE_INFO (LongPte,
                                         Pfn1->OriginalPte,
                                         CurrentPagingFile->PageFileNumber,
                                         StartBit);

#if DBG
                if ((StartBit < 8192) &&
                    (CurrentPagingFile->PageFileNumber == 0)) {
                    ASSERT ((MmPagingFileDebug[StartBit] & 1) == 0);
                    MmPagingFileDebug[StartBit] =
                        (((ULONG_PTR)Pfn1->PteAddress << 3) |
                            ((ClusterSize & 0xf) << 1) | 1);
                }
#endif

                //
                // Change the original PTE contents to refer to
                // the paging file offset where this was written.
                //

                Pfn1->OriginalPte = LongPte;

                ClusterSize += 1;
                Page += 1;
                StartBit += 1;
#if 0 // COMMENTED OUT
                WriteCluster.Count -= 1;
                WriteCluster.StartIndex += 1;

            } while (WriteCluster.Count != 0);
#endif //0
        }
        else {

            //
            // This page was not destined for a paging file,
            // get another page.
            //
            // Get a page of the same color as the one which
            // was not usable.
            //

            MI_GET_MODIFIED_PAGE_BY_COLOR (PageFrameIndex,
                                           NextColor);
        }

        if (PageFrameIndex == MM_EMPTY_LIST) {
            break;
        }

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    } //end while

    if (ClusterSize != ThisCluster) {

        //
        // A complete cluster could not be located, free the
        // excess page file space that was reserved and adjust
        // the size of the packet.
        //

        RtlClearBits (CurrentPagingFile->Bitmap,
                      StartBit,
                      (ULONG)(ThisCluster - ClusterSize));

        CurrentPagingFile->FreeSpace += ThisCluster - ClusterSize;
        CurrentPagingFile->CurrentUsage -= ThisCluster - ClusterSize;

        //
        // If there are no pages to write, don't issue a write
        // request and restart the scan loop.
        //

        if (ClusterSize == 0) {

            //
            // No pages to write.  Insert the entry back in the list.
            //

            if (IsListEmpty (&ModWriterEntry->PagingListHead->ListHead)) {
                KeSetEvent (&ModWriterEntry->PagingListHead->Event,
                            0,
                            FALSE);
            }

            InsertTailList (&ModWriterEntry->PagingListHead->ListHead,
                            &ModWriterEntry->Links);

            return 0;
        }
    }

    if (CurrentPagingFile->PeakUsage <
                                CurrentPagingFile->CurrentUsage) {
        CurrentPagingFile->PeakUsage =
                                CurrentPagingFile->CurrentUsage;
    }

    ModWriterEntry->Mdl.ByteCount = (ULONG)(ClusterSize * PAGE_SIZE);
    ModWriterEntry->LastPageToWrite = StartBit - 1;

    MmInfoCounters.DirtyWriteIoCount += 1;
    MmInfoCounters.DirtyPagesWriteCount += (ULONG)ClusterSize;

    //
    // For now release the PFN lock and wait for the write to complete.
    //

    UNLOCK_PFN (OldIrql);

#if DBG
    if (MmDebug & MM_DBG_MOD_WRITE) {
        DbgPrint("MM MODWRITE: modified page write begun @ %08lx by %08lx\n",
                StartingOffset.LowPart, ModWriterEntry->Mdl.ByteCount);
    }
#endif

    //
    // Issue the write request.
    //

    Status = IoAsynchronousPageWrite (File,
                                      &ModWriterEntry->Mdl,
                                      &StartingOffset,
                                      MiWriteComplete,
                                      (PVOID)ModWriterEntry,
                                      &ModWriterEntry->u.IoStatus,
                                      &ModWriterEntry->Irp);

    if (NT_ERROR(Status)) {
        KdPrint(("MM MODWRITE: modified page write failed %lx\n", Status));

        //
        // An error has occurred, disable APCs and
        // call the write completion routine.
        //

        ModWriterEntry->u.IoStatus.Status = Status;
        ModWriterEntry->u.IoStatus.Information = 0;
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        MiWriteComplete ((PVOID)ModWriterEntry,
                         &ModWriterEntry->u.IoStatus,
                         0 );
        KeLowerIrql (OldIrql);
    }

    if (PageFileFull == TRUE) {
        MiPageFileFull ();
    }

    LOCK_PFN (OldIrql);

    return ClusterSize;
}


#if 0 // COMMENTED OUT **************************************************
ULONG ClusterCounts[20];
ULONG ClusterSizes[20];
VOID
MiClusterWritePages (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex,
    IN PMM_WRITE_CLUSTER WriteCluster,
    IN ULONG Size
    )

{
    PMMPTE PointerClusterPte;
    PMMPTE OriginalPte;
    PMMPTE StopPte;
    PMMPTE ThisPage;
    PMMPTE BasePage;
    ULONG Start;
    PMMPFN Pfn2;
    KIRQL OldIrql = 99;

    Start = MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE;
    WriteCluster->Cluster[Start] = PageFrameIndex;
    WriteCluster->Count = 1;
    ClusterSizes[Size] += 1;
    if (Size == 1) {
        WriteCluster->StartIndex = Start;
        return;
    }

    //
    // The page points to a page table page which may not be
    // for the current process.  Map the page into hyperspace
    // reference it through hyperspace.
    //

    PointerClusterPte = Pfn1->PteAddress;
    BasePage = (PMMPTE)((ULONG_PTR)PointerClusterPte & ~(PAGE_SIZE - 1));
    ThisPage = BasePage;

    if ((PointerClusterPte < (PMMPTE)PDE_TOP) ||
        (!MmIsAddressValid (PointerClusterPte))) {

        //
        // Map page into hyperspace as it is either a page table
        // page or nonresident paged pool.
        //

        PointerClusterPte = (PMMPTE)((PCHAR)MiMapPageInHyperSpace (
                                        Pfn1->PteFrame, &OldIrql)
                                        +
                                BYTE_OFFSET (PointerClusterPte));
        ThisPage = (PMMPTE)((ULONG_PTR)PointerClusterPte & ~(PAGE_SIZE - 1));
    }

    OriginalPte = PointerClusterPte;
    ASSERT (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerClusterPte) == PageFrameIndex);

    //
    // Check backwards and forwards for other pages from this process
    // destined for the paging file.
    //

    StopPte = PointerClusterPte - (Size - 1);
    if (StopPte < ThisPage) {
        StopPte = ThisPage;
    }

    while (PointerClusterPte > StopPte) {
        PointerClusterPte -= 1;

        //
        // Look for the pointer at start of segment, quit as this is NOT
        // a prototype PTE.  Normal PTEs will not match this.
        //

        if (BasePage != (PMMPTE)
                        (ULONG_PTR)(PointerClusterPte->u.Long & ~(PAGE_SIZE - 1))) {

            if ((PointerClusterPte->u.Hard.Valid == 0) &&
                (PointerClusterPte->u.Soft.Prototype == 0) &&
                (PointerClusterPte->u.Soft.Transition == 1))  {

                //
                // PTE is in transition state, see if it is modified.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerClusterPte);
                Pfn2 = MI_PFN_ELEMENT(PageFrameIndex);
                ASSERT (Pfn2->OriginalPte.u.Soft.Prototype == 0);
                if ((Pfn2->u3.e1.Modified != 0 ) &&
                    (Pfn2->u3.e2.ReferenceCount == 0)) {

                    Start -= 1;
                    WriteCluster->Count += 1;
                    WriteCluster->Cluster[Start] = PageFrameIndex;
                }
            }
        }
        break;
    }

    WriteCluster->StartIndex = Start;
    PointerClusterPte = OriginalPte + 1;
    Start = MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE;

    //
    // Remove pages looking forward from PointerClusterPte until
    // a cluster is filled or a PTE is not on the modified list.
    //

    ThisPage = (PMMPTE)((PCHAR)ThisPage + PAGE_SIZE);

    while ((WriteCluster->Count < Size) &&
           (PointerClusterPte < ThisPage)) {

        if ((PointerClusterPte->u.Hard.Valid == 0) &&
            (PointerClusterPte->u.Soft.Prototype == 0) &&
            (PointerClusterPte->u.Soft.Transition == 1))  {

            //
            // PTE is in transition state, see if it is modified.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerClusterPte);
            Pfn2 = MI_PFN_ELEMENT(PageFrameIndex);
            ASSERT (Pfn2->OriginalPte.u.Soft.Prototype == 0);
            if ((Pfn2->u3.e1.Modified != 0 ) &&
                (Pfn2->u3.e2.ReferenceCount == 0)) {

                Start += 1;
                WriteCluster->Count += 1;
                WriteCluster->Cluster[Start] = PageFrameIndex;
                PointerClusterPte += 1;
                continue;
            }
        }
        break;
    }

    if (OldIrql != 99) {
        MiUnmapPageInHyperSpace (OldIrql);
    }
    ClusterCounts[WriteCluster->Count] += 1;
    return;
}
#endif // COMMENTED OUT **************************************************


VOID
MiMappedPageWriter (
    IN PVOID StartContext
    )

/*++

Routine Description:

    Implements the NT secondary modified page writer thread.
    Requests for writes to mapped files are sent to this thread.
    This is required as the writing of mapped file pages could cause
    page faults resulting in requests for free pages.  But there
    could be no free pages - hence a dead lock.  Rather than deadlock
    the whole system waiting on the modified page writer, creating
    a secondary thread allows that thread to block without affecting
    on going page file writes.

Arguments:

    StartContext - not used.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    NTSTATUS Status;
    KEVENT TempEvent;
    PETHREAD CurrentThread;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Make this a real time thread.
    //

    CurrentThread = PsGetCurrentThread ();

    KeSetPriorityThread (&CurrentThread->Tcb, LOW_REALTIME_PRIORITY + 1);

    CurrentThread->MemoryMaker = 1;

    //
    // Let the file system know that we are getting resources.
    //

    FsRtlSetTopLevelIrpForModWriter();

    KeInitializeEvent (&TempEvent, NotificationEvent, FALSE);

    OldIrql = PASSIVE_LEVEL;

    while (TRUE) {
        KeWaitForSingleObject (&MmMappedPageWriterEvent,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        LOCK_PFN (OldIrql);
        if (IsListEmpty (&MmMappedPageWriterList)) {
            KeClearEvent (&MmMappedPageWriterEvent);
            UNLOCK_PFN (OldIrql);
        }
        else {

            ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                                &MmMappedPageWriterList);

            UNLOCK_PFN (OldIrql);

            if (ModWriterEntry->ControlArea->u.Flags.FailAllIo == 1) {
                Status = STATUS_UNSUCCESSFUL;
            }
            else {
                Status = FsRtlAcquireFileForModWriteEx (ModWriterEntry->File,
                                                        &ModWriterEntry->u.LastByte,
                                                        &ModWriterEntry->FileResource);
                if (NT_SUCCESS(Status)) {

                    //
                    // Issue the write request.
                    //

                    Status = IoAsynchronousPageWrite (ModWriterEntry->File,
                                                      &ModWriterEntry->Mdl,
                                                      &ModWriterEntry->WriteOffset,
                                                      MiWriteComplete,
                                                      (PVOID)ModWriterEntry,
                                                      &ModWriterEntry->u.IoStatus,
                                                      &ModWriterEntry->Irp);
                }
                else {

                    //
                    // Unable to get the file system resources, set error status
                    // to lock conflict (ignored by MiWriteComplete) so the APC
                    // routine is explicitly called.
                    //

                    Status = STATUS_FILE_LOCK_CONFLICT;
                }
            }

            if (NT_ERROR(Status)) {

                //
                // An error has occurred, disable APC's and
                // call the write completion routine.
                //

                ModWriterEntry->u.IoStatus.Status = Status;
                ModWriterEntry->u.IoStatus.Information = 0;
                KeRaiseIrql (APC_LEVEL, &OldIrql);
                MiWriteComplete ((PVOID)ModWriterEntry,
                                 &ModWriterEntry->u.IoStatus,
                                 0 );
                KeLowerIrql (OldIrql);
            }
#if 0
    //TEMPORARY code to use synchronous I/O here.

            //
            // Issue the write request.
            //

            Status = IoSynchronousPageWrite (
                                   ModWriterEntry->File,
                                   &ModWriterEntry->Mdl,
                                   &ModWriterEntry->WriteOffset,
                                   &TempEvent,
                                   &ModWriterEntry->u.IoStatus );

            if (NT_ERROR(Status)) {
                ModWriterEntry->u.IoStatus.Status = Status;
                ModWriterEntry->u.IoStatus.Information = 0;
            }

            if (NT_ERROR(ModWriterEntry->u.IoStatus.Status)) {
                KdPrint(("MM MODWRITE: modified page write failed %lx\n", Status));
            }

            //
            // Call the write completion routine.
            //

            KeRaiseIrql (APC_LEVEL, &OldIrql);
            MiWriteComplete ((PVOID)ModWriterEntry,
                             &ModWriterEntry->IoStatus,
                             0 );
            KeLowerIrql (OldIrql);
#endif //0

        }

    }
}


BOOLEAN
MmDisableModifiedWriteOfSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer
    )

/*++

Routine Description:

    This function disables page writing by the modified page writer for
    the section which is mapped by the specified file object pointer.

    This should only be used for files which CANNOT be mapped by user
    programs, e.g., volume files, directory files, etc.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects


Return Value:

    Returns TRUE if the operation was a success, FALSE if either
    the there is no section or the section already has a view.

--*/

{
    KIRQL OldIrql;
    BOOLEAN state;
    PCONTROL_AREA ControlArea;

    state = TRUE;

    LOCK_PFN (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    if (ControlArea != NULL) {
        if (ControlArea->NumberOfMappedViews == 0) {

            //
            // There are no views to this section, indicate no modified
            // page writing is allowed.
            //

            ControlArea->u.Flags.NoModifiedWriting = 1;
        }
        else {

            //
            // Return the current modified page writing state.
            //

            state = (BOOLEAN)ControlArea->u.Flags.NoModifiedWriting;
        }
    }
    else {

        //
        // This file no longer has an associated segment.
        //

        state = 0;
    }

    UNLOCK_PFN (OldIrql);
    return state;
}


BOOLEAN
MmEnableModifiedWriteOfSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer
    )

/*++

Routine Description:

    This function enables page writing by the modified page writer for
    the section which is mapped by the specified file object pointer.

    This should only be used for files which have previously been disabled.
    Normal sections are created allowing modified write.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects


Return Value:

    Returns TRUE if the operation was a success, FALSE if either
    the there is no section or the section already has a view.

--*/

{
    KIRQL OldIrql;
    BOOLEAN state;
    PCONTROL_AREA ControlArea;

    state = TRUE;

    LOCK_PFN2 (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    if (ControlArea != NULL) {
        if (ControlArea->NumberOfMappedViews == 0) {

            //
            // There are no views to this section, indicate modified
            // page writing is allowed.
            //

            ControlArea->u.Flags.NoModifiedWriting = 0;
        }
        else {

            //
            // Return the current modified page writing state.
            //

            state = (BOOLEAN)!ControlArea->u.Flags.NoModifiedWriting;
        }
    }

    UNLOCK_PFN2 (OldIrql);
    return state;
}


#define ROUND_UP(VALUE,ROUND) ((ULONG)(((ULONG)VALUE + \
                               ((ULONG)ROUND - 1L)) & (~((ULONG)ROUND - 1L))))
NTSTATUS
MmGetPageFileInformation (
    OUT PVOID SystemInformation,
    IN ULONG SystemInformationLength,
    OUT PULONG Length
    )

/*++

Routine Description:

    This routine returns information about the currently active paging
    files.

Arguments:

    SystemInformation - Returns the paging file information.

    SystemInformationLength - Supplies the length of the SystemInformation
                              buffer.

    Length - Returns the length of the paging file information placed in the
             buffer.

Return Value:

    Returns the status of the operation.

--*/

{
    PSYSTEM_PAGEFILE_INFORMATION PageFileInfo;
    ULONG NextEntryOffset = 0;
    ULONG TotalSize = 0;
    ULONG i;
    UNICODE_STRING UserBufferPageFileName;

    PAGED_CODE();

    *Length = 0;
    PageFileInfo = (PSYSTEM_PAGEFILE_INFORMATION)SystemInformation;

    PageFileInfo->TotalSize = 0;

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        PageFileInfo = (PSYSTEM_PAGEFILE_INFORMATION)(
                                (PUCHAR)PageFileInfo + NextEntryOffset);
        NextEntryOffset = sizeof(SYSTEM_PAGEFILE_INFORMATION);
        TotalSize += sizeof(SYSTEM_PAGEFILE_INFORMATION);

        if (TotalSize > SystemInformationLength) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }

        PageFileInfo->TotalSize = (ULONG)MmPagingFile[i]->Size;
        PageFileInfo->TotalInUse = (ULONG)MmPagingFile[i]->CurrentUsage;
        PageFileInfo->PeakUsage = (ULONG)MmPagingFile[i]->PeakUsage;

        //
        // The PageFileName portion of the UserBuffer must be saved locally
        // to protect against a malicious thread changing the contents.  This
        // is because we will reference the contents ourselves when the actual
        // string is copied out carefully below.
        //

        UserBufferPageFileName.Length = MmPagingFile[i]->PageFileName.Length;
        UserBufferPageFileName.MaximumLength = (USHORT)(MmPagingFile[i]->PageFileName.Length + sizeof(WCHAR));
        UserBufferPageFileName.Buffer = (PWCHAR)(PageFileInfo + 1);

        PageFileInfo->PageFileName = UserBufferPageFileName;

        TotalSize += ROUND_UP (UserBufferPageFileName.MaximumLength,
                               sizeof(ULONG));
        NextEntryOffset += ROUND_UP (UserBufferPageFileName.MaximumLength,
                                     sizeof(ULONG));

        if (TotalSize > SystemInformationLength) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }

        //
        // Carefully reference the user buffer here.
        //

        RtlCopyMemory(UserBufferPageFileName.Buffer,
                      MmPagingFile[i]->PageFileName.Buffer,
                      MmPagingFile[i]->PageFileName.Length);
        UserBufferPageFileName.Buffer[
                    MmPagingFile[i]->PageFileName.Length/sizeof(WCHAR)] = UNICODE_NULL;
        PageFileInfo->NextEntryOffset = NextEntryOffset;
    }
    PageFileInfo->NextEntryOffset = 0;
    *Length = TotalSize;
    return STATUS_SUCCESS;
}


NTSTATUS
MiCheckPageFileMapping (
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    Non-pagable routine to check to see if a given file has
    no sections and therefore is eligible to become a paging file.

Arguments:

    File - Supplies a pointer to the file object.

Return Value:

    Returns STATUS_SUCCESS if the file can be used as a paging file.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    if (File->SectionObjectPointer == NULL) {
        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    if ((File->SectionObjectPointer->DataSectionObject != NULL) ||
        (File->SectionObjectPointer->ImageSectionObject != NULL)) {

        UNLOCK_PFN (OldIrql);
        return STATUS_INCOMPATIBLE_FILE_MAP;
    }
    UNLOCK_PFN (OldIrql);
    return STATUS_SUCCESS;

}


VOID
MiInsertPageFileInList (
    VOID
    )

/*++

Routine Description:

    Non-pagable routine to add a page file into the list
    of system wide page files.

Arguments:

    None, implicitly found through page file structures.

Return Value:

    None.  Operation cannot fail.

--*/

{
    KIRQL OldIrql;
    SIZE_T FreeSpace;
    SIZE_T MaximumSize;

    LOCK_PFN (OldIrql);

    MmNumberOfPagingFiles += 1;

    if (IsListEmpty (&MmPagingFileHeader.ListHead)) {
        KeSetEvent (&MmPagingFileHeader.Event, 0, FALSE);
    }

    InsertTailList (&MmPagingFileHeader.ListHead,
                    &MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[0]->Links);

    MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[0]->CurrentList =
                                                &MmPagingFileHeader.ListHead;

    InsertTailList (&MmPagingFileHeader.ListHead,
                    &MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[1]->Links);

    MmPagingFile[MmNumberOfPagingFiles - 1]->Entry[1]->CurrentList =
                                                &MmPagingFileHeader.ListHead;

    FreeSpace = MmPagingFile[MmNumberOfPagingFiles - 1]->FreeSpace;
    MaximumSize = MmPagingFile[MmNumberOfPagingFiles - 1]->MaximumSize;

    MmNumberOfActiveMdlEntries += 2;

    UNLOCK_PFN (OldIrql);

    //
    // Increase the systemwide commit limit maximum first.  Then increase
    // the current limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, MaximumSize);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, FreeSpace);

    return;
}

VOID
MiPageFileFull (
    VOID
    )

/*++

Routine Description:

    This routine is called when no space can be found in a paging file.
    It looks through all the paging files to see if ample space is
    available and if not, tries to expand the paging files.

    If more than 90% of all the paging files are in use, the commitment limit
    is set to the total and then 100 pages are added.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG i;
    PFN_NUMBER Total;
    PFN_NUMBER Free;
    SIZE_T QuotaCharge;

    if (MmNumberOfPagingFiles == 0) {
        return;
    }

    Total = 0;
    Free = 0;

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        Total += MmPagingFile[i]->Size;
        Free += MmPagingFile[i]->FreeSpace;
    }

    //
    // Check to see if more than 90% of the total space has been used.
    //

    if (((Total >> 5) + (Total >> 4)) >= Free) {

        //
        // Try to expand the paging files.
        //
        // Check (unsynchronized is ok) commit limits of each pagefile.
        // If all the pagefiles are already at their maximums, then don't
        // make things worse by setting commit to the maximum - this gives
        // systems with lots of memory a longer lease on life when they have
        // small pagefiles.
        //

        i = 0;

        do {
            if (MmPagingFile[i]->MaximumSize > MmPagingFile[i]->Size) {
                break;
            }
            i += 1;
        } while (i < MmNumberOfPagingFiles);

        if (i == MmNumberOfPagingFiles) {

            //
            // No pagefiles can be extended,
            // display a popup if we haven't before.
            //

            MiCauseOverCommitPopup ();

            return;
        }

        QuotaCharge = MmTotalCommitLimit - MmTotalCommittedPages;

        //
        // IFF the total number of committed pages is less than the limit,
        // or in any event, no more than 50 pages past the limit,
        // then charge pages against the commitment to trigger pagefile
        // expansion.
        //
        // If the total commit is more than 50 past the limit, then don't
        // bother trying to extend the pagefile.
        //

        if ((SSIZE_T)QuotaCharge + 50 > 0) {

            if ((SSIZE_T)QuotaCharge < 50) {
                QuotaCharge = 50;
            }

            MiChargeCommitmentCantExpand (QuotaCharge, TRUE);

            MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGEFILE_FULL, QuotaCharge);

            //
            // Display a popup if we haven't before.
            //

            MiCauseOverCommitPopup ();

            MiReturnCommitment (QuotaCharge);

            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PAGEFILE_FULL, QuotaCharge);
        }
    }
    return;
}

VOID
MiFlushAllPages (
    VOID
    )

/*++

Routine Description:

    Forces a write of all modified pages.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  APC_LEVEL or less.

--*/

{
    ULONG j;

    //
    // If there are no paging files, then no sense in waiting for
    // modified writes to complete.
    //

    if (MmNumberOfPagingFiles == 0) {
        return;
    }

    MmWriteAllModifiedPages = TRUE;
    KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);

    j = 0xff;

    do {
        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
        j -= 1;
    } while ((MmModifiedPageListHead.Total > 50) && (j > 0));

    MmWriteAllModifiedPages = FALSE;
    return;
}


LOGICAL
MiIssuePageExtendRequest (
    IN PMMPAGE_FILE_EXPANSION PageExtend
    )

/*++

Routine Description:

    Queue a message to the segment dereferencing / pagefile extending
    thread to see if the page file can be extended.  Extension is done
    in the context of a system thread due to mutexes which the current
    thread may be holding.

Arguments:

    PageExtend - Supplies a pointer to the page file extension request.

Return Value:

    TRUE indicates the request completed.  FALSE indicates the request timed
    out and was removed.

Environment:

    Kernel mode.  No locks held.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    NTSTATUS status;
    PLIST_ENTRY NextEntry;
    PETHREAD Thread;

    Thread = PsGetCurrentThread ();

    //
    // The segment dereference thread cannot wait for itself !
    //

    if (Thread->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread) {
        return FALSE;
    }

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    InsertHeadList (&MmDereferenceSegmentHeader.ListHead,
                    &PageExtend->DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                        0L,
                        1L,
                        TRUE);

    //
    // Wait for the thread to extend the paging file.
    //

    status = KeWaitForSingleObject (&PageExtend->Event,
                                    Executive,
                                    KernelMode,
                                    FALSE,
                                    (PageExtend->RequestedExpansionSize < 10) ?
                                        (PLARGE_INTEGER)&MmOneSecond : (PLARGE_INTEGER)&MmTwentySeconds);

    if (status == STATUS_TIMEOUT) {

        //
        // The wait has timed out, if this request has not
        // been processed, remove it from the list and check
        // to see if we should allow this request to succeed.
        // This prevents a deadlock between the file system
        // trying to allocate memory in the FSP and the
        // segment dereferencing thread trying to close a
        // file object, and waiting in the file system.
        //

        KdPrint(("MiIssuePageExtendRequest: wait timed out, page-extend= %p, quota = %lx\n",
                   PageExtend, PageExtend->RequestedExpansionSize));

        ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

        NextEntry = MmDereferenceSegmentHeader.ListHead.Flink;

        while (NextEntry != &MmDereferenceSegmentHeader.ListHead) {

            //
            // Check to see if this is the entry we are waiting for.
            //

            if (NextEntry == &PageExtend->DereferenceList) {

                //
                // Remove this entry.
                //

                RemoveEntryList (&PageExtend->DereferenceList);
                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);
                return FALSE;
            }
            NextEntry = NextEntry->Flink;
        }

        ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

        //
        // Entry is being processed, wait for completion.
        //

        KdPrint (("MiIssuePageExtendRequest: rewaiting...\n"));

        KeWaitForSingleObject (&PageExtend->Event,
                               Executive,
                               KernelMode,
                               FALSE,
                               NULL);
    }

    return TRUE;
}


VOID
MiIssuePageExtendRequestNoWait (
    IN PFN_NUMBER SizeInPages
    )

/*++

Routine Description:

    Queue a message to the segment dereferencing / pagefile extending
    thread to see if the page file can be extended.  Extension is done
    in the context of a system thread due to mutexes which the current
    thread may be holding.

Arguments:

    SizeInPages - Supplies the size in pages to increase the page file(s) by.
                  This is rounded up to a 1MB multiple by this routine.

Return Value:

    TRUE indicates the request completed.  FALSE indicates the request timed
    out and was removed.

Environment:

    Kernel mode.  No locks held.  DISPATCH_LEVEL or less.

    Note this routine must be very careful to not use any paged
    pool as the only reason it is being called is because pool is depleted.

--*/

{
    KIRQL OldIrql;
    LONG OriginalInProgress;

    OriginalInProgress = InterlockedCompareExchange (
                            &MmAttemptForCantExtend.InProgress, 1, 0);

    if (OriginalInProgress != 0) {

        //
        // An expansion request is already in progress, assume
        // it will help enough (another can always be issued later) and
        // that it will succeed.
        //

        return;
    }

    ASSERT (MmAttemptForCantExtend.InProgress == 1);

    SizeInPages = (SizeInPages + ONEMB_IN_PAGES - 1) & ~(ONEMB_IN_PAGES - 1);

    MmAttemptForCantExtend.ActualExpansion = 0;
    MmAttemptForCantExtend.RequestedExpansionSize = SizeInPages;

    ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

    InsertHeadList (&MmDereferenceSegmentHeader.ListHead,
                    &MmAttemptForCantExtend.DereferenceList);

    ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

    KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                        0L,
                        1L,
                        FALSE);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\nolowmem.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    nolowmem.c

Abstract:

    This module contains routines which remove physical memory below 4GB
    to make testing for driver addressing errors easier.

Author:

    Landy Wang (landyw) 30-Nov-1998

Revision History:

--*/

#include "mi.h"

//
// If /NOLOWMEM is used, this is set to the boundary PFN (pages below this
// value are not used whenever possible).
//

PFN_NUMBER MiNoLowMemory;

#if defined (_MI_MORE_THAN_4GB_)

VOID
MiFillRemovedPages (
    IN ULONG StartPage,
    IN ULONG NumberOfPages
    );

ULONG
MiRemoveModuloPages (
    IN ULONG StartPage,
    IN ULONG LastPage
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiRemoveLowPages)
#pragma alloc_text(INIT,MiFillRemovedPages)
#pragma alloc_text(INIT,MiRemoveModuloPages)
#endif

PRTL_BITMAP MiLowMemoryBitMap;

LOGICAL MiFillModuloPages = FALSE;


VOID
MiFillRemovedPages (
    IN ULONG StartPage,
    IN ULONG NumberOfPages
    )

/*++

Routine Description:

    This routine fills low pages with a recognizable pattern.  Thus, if the
    page is ever mistakenly used by a broken component, it will be easy to
    see exactly which bytes were corrupted.

Arguments:

    StartPage - Supplies the low page to fill.

    NumberOfPages - Supplies the number of pages to fill.

Return Value:

    None.

Environment:

    Phase 0 initialization.

--*/

{
    ULONG Page;
    ULONG LastPage;
    PVOID LastChunkVa;
    ULONG MaxPageChunk;
    ULONG ThisPageChunk;
    PVOID TempVa;
    PVOID BaseVa;
    SIZE_T NumberOfBytes;
    PHYSICAL_ADDRESS PhysicalAddress;

    //
    // Do 256MB at a time when possible (don't want to overflow unit
    // conversions or fail to allocate system PTEs needlessly).
    //

    MaxPageChunk = (256 * 1024 * 1024) / PAGE_SIZE;

    LastPage = StartPage + NumberOfPages;

    PhysicalAddress.QuadPart = StartPage;
    PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;

    Page = StartPage;

    while (Page < LastPage) {

        if (NumberOfPages > MaxPageChunk) {
            ThisPageChunk = MaxPageChunk;
        }
        else {
            ThisPageChunk = NumberOfPages;
        }

        NumberOfBytes = ThisPageChunk << PAGE_SHIFT;

        BaseVa = MmMapIoSpace (PhysicalAddress, NumberOfBytes, MmCached);

        if (BaseVa != NULL) {

            //
            // Fill the actual page with a recognizable data pattern.  No
            // one should write to these pages unless they are allocated for
            // a contiguous memory request.
            //

            TempVa = BaseVa;
            LastChunkVa = (PVOID)((ULONG_PTR)BaseVa + NumberOfBytes);

            while (TempVa < LastChunkVa) {

                RtlFillMemoryUlong (TempVa,
                                    PAGE_SIZE,
                                    (ULONG)Page | MI_LOWMEM_MAGIC_BIT);

                TempVa = (PVOID)((ULONG_PTR)TempVa + PAGE_SIZE);
                Page += 1;
            }

            MmUnmapIoSpace (BaseVa, NumberOfBytes);
        }
        else {
            MaxPageChunk /= 2;
            if (MaxPageChunk == 0) {
#if DBG
                DbgPrint ("Not even one PTE available for filling lowmem pages\n");
                DbgBreakPoint ();
#endif
                break;
            }
        }
    }
}
    

ULONG
MiRemoveModuloPages (
    IN ULONG StartPage,
    IN ULONG LastPage
    )

/*++

Routine Description:

    This routine removes pages above 4GB.

    For every page below 4GB that could not be reclaimed, don't use the
    high modulo-4GB equivalent page.  The motivation is to prevent
    code bugs that drop the high bits from destroying critical
    system data in the unclaimed pages (like the GDT, IDT, kernel code
    and data, etc).

Arguments:

    StartPage - Supplies the low page to modulo-ize and remove.

    LastPage - Supplies the final low page to modulo-ize and remove.

Return Value:

    None.

Environment:

    Phase 0 initialization.

--*/

{
    PEPROCESS Process;
    ULONG Page;
    ULONG PagesRemoved;
    PVOID TempVa;
    KIRQL OldIrql;
    PFN_NUMBER HighPage;
    PMMPFN Pfn1;

    //
    // Removing modulo pages can take a long (on the order of 30 minutes!) on
    // large memory systems because the various PFN lists generally need to 
    // linearly walked in order to find and cross-remove from the colored chains
    // the requested pages.  Since actually putting these pages out of
    // circulation is of dubious benefit, default this behavior to disabled
    // but leave the data variable so a questionable machine can have this
    // enabled without needing a new kernel.
    //

    if (MiFillModuloPages == FALSE) {
        return 0;
    }

    Process = PsGetCurrentProcess ();
    PagesRemoved = 0;

#if DBG
    DbgPrint ("Removing modulo pages %x %x\n", StartPage, LastPage);
#endif

    for (Page = StartPage; Page < LastPage; Page += 1) {

        //
        // Search for any high modulo pages and remove them.
        //

        HighPage = Page + MiNoLowMemory;

        LOCK_PFN (OldIrql);

        while (HighPage <= MmHighestPhysicalPage) {

            Pfn1 = MI_PFN_ELEMENT (HighPage);

            if ((MmIsAddressValid(Pfn1)) &&
                (MmIsAddressValid((PCHAR)Pfn1 + sizeof(MMPFN) - 1)) &&
                ((ULONG)Pfn1->u3.e1.PageLocation <= (ULONG)StandbyPageList) &&
                (Pfn1->u1.Flink != 0) &&
                (Pfn1->u2.Blink != 0) &&
                (Pfn1->u3.e2.ReferenceCount == 0) &&
                (MmAvailablePages > 0)) {

                    //
                    // Systems utilizing memory compression may have more
                    // pages on the zero, free and standby lists than we
                    // want to give out.  Explicitly check MmAvailablePages
                    // above instead (and recheck whenever the PFN lock is
                    // released and reacquired).
                    //

                    //
                    // This page can be taken.
                    //

                    if (Pfn1->u3.e1.PageLocation == StandbyPageList) {
                        MiUnlinkPageFromList (Pfn1);
                        MiRestoreTransitionPte (HighPage);
                    }
                    else {
                        MiUnlinkFreeOrZeroedPage (HighPage);
                    }

                    Pfn1->u3.e2.ShortFlags = 0;
                    Pfn1->u3.e2.ReferenceCount = 1;
                    Pfn1->u2.ShareCount = 1;
                    Pfn1->PteAddress = (PMMPTE)(ULONG_PTR)0xFFFFFFF8;
                    Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                    Pfn1->u4.PteFrame = MI_MAGIC_4GB_RECLAIM;
                    Pfn1->u3.e1.PageLocation = ActiveAndValid;
                    Pfn1->u3.e1.CacheAttribute = MiNotMapped;
                    Pfn1->u4.VerifierAllocation = 0;
                    Pfn1->u3.e1.LargeSessionAllocation = 0;
                    Pfn1->u3.e1.StartOfAllocation = 1;
                    Pfn1->u3.e1.EndOfAllocation = 1;

                    //
                    // Fill the actual page with a recognizable data
                    // pattern.  No one else should write to these
                    // pages unless they are allocated for
                    // a contiguous memory request.
                    //

                    MmNumberOfPhysicalPages -= 1;
                    UNLOCK_PFN (OldIrql);

                    TempVa = (PULONG)MiMapPageInHyperSpace (Process,
                                                            HighPage,
                                                            &OldIrql);
                    RtlFillMemoryUlong (TempVa,
                                        PAGE_SIZE,
                                        (ULONG)HighPage | MI_LOWMEM_MAGIC_BIT);

                    MiUnmapPageInHyperSpace (Process, TempVa, OldIrql);

                    PagesRemoved += 1;
                    LOCK_PFN (OldIrql);
            }
            HighPage += MiNoLowMemory;
        }

        UNLOCK_PFN (OldIrql);
    }

#if DBG
    DbgPrint ("Done removing modulo pages %x %x\n", StartPage, LastPage);
#endif

    return PagesRemoved;
}
    
VOID
MiRemoveLowPages (
    ULONG RemovePhase
    )

/*++

Routine Description:

    This routine removes all pages below physical 4GB in the system.  This lets
    us find problems with device drivers by putting all accesses high.

Arguments:

    RemovePhase - Supplies the current phase of page removal.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    ULONG i;
    ULONG BitMapIndex;
    ULONG BitMapHint;
    ULONG LengthOfClearRun;
    ULONG LengthOfSetRun;
    ULONG StartingRunIndex;
    ULONG ModuloRemoved;
    ULONG PagesRemoved;
    PFN_COUNT PageCount;
    PMMPFN PfnNextColored;
    PMMPFN PfnNextFlink;
    PMMPFN PfnLastColored;
    PFN_NUMBER PageNextColored;
    PFN_NUMBER PageNextFlink;
    PFN_NUMBER PageLastColored;
    PFN_NUMBER Page;
    PMMPFN Pfn1;
    PMMPFNLIST ListHead;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    PFN_NUMBER MovedPage;

    if (RemovePhase == 0) {

        MiCreateBitMap (&MiLowMemoryBitMap, (ULONG)MiNoLowMemory, NonPagedPool);

        if (MiLowMemoryBitMap != NULL) {
            RtlClearAllBits (MiLowMemoryBitMap);
            MmMakeLowMemory = TRUE;
        }
    }

    if (MiLowMemoryBitMap == NULL) {
        return;
    }

    ListHead = &MmFreePageListHead;
    PageCount = 0;

    LOCK_PFN (OldIrql);

    for (Color = 0; Color < MmSecondaryColors; Color += 1) {
        ColorHead = &MmFreePagesByColor[FreePageList][Color];

        MovedPage = MM_EMPTY_LIST;

        while (ColorHead->Flink != MM_EMPTY_LIST) {

            Page = ColorHead->Flink;

            Pfn1 = MI_PFN_ELEMENT(Page);

            ASSERT ((MMLISTS)Pfn1->u3.e1.PageLocation == FreePageList);

            //
            // The Flink and Blink must be nonzero here for the page
            // to be on the listhead.  Only code that scans the
            // MmPhysicalMemoryBlock has to check for the zero case.
            //

            ASSERT (Pfn1->u1.Flink != 0);
            ASSERT (Pfn1->u2.Blink != 0);

            //
            // See if the page is below 4GB - if not, skip it.
            //

            if (Page >= MiNoLowMemory) {

                //
                // Put page on end of list and if first time, save pfn.
                //

                if (MovedPage == MM_EMPTY_LIST) {
                    MovedPage = Page;
                }
                else if (Page == MovedPage) {

                    //
                    // No more pages available in this colored chain.
                    //

                    break;
                }

                //
                // If the colored chain has more than one entry then
                // put this page on the end.
                //

                PageNextColored = (PFN_NUMBER)Pfn1->OriginalPte.u.Long;

                if (PageNextColored == MM_EMPTY_LIST) {

                    //
                    // No more pages available in this colored chain.
                    //

                    break;
                }

                ASSERT (Pfn1->u1.Flink != 0);
                ASSERT (Pfn1->u1.Flink != MM_EMPTY_LIST);
                ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);

                PfnNextColored = MI_PFN_ELEMENT(PageNextColored);
                ASSERT ((MMLISTS)PfnNextColored->u3.e1.PageLocation == FreePageList);
                ASSERT (PfnNextColored->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);

                //
                // Adjust the free page list so Page
                // follows PageNextFlink.
                //

                PageNextFlink = Pfn1->u1.Flink;
                PfnNextFlink = MI_PFN_ELEMENT(PageNextFlink);

                ASSERT ((MMLISTS)PfnNextFlink->u3.e1.PageLocation == FreePageList);
                ASSERT (PfnNextFlink->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);

                PfnLastColored = ColorHead->Blink;
                ASSERT (PfnLastColored != (PMMPFN)MM_EMPTY_LIST);
                ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                ASSERT (PfnLastColored->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);
                ASSERT (PfnLastColored->u2.Blink != MM_EMPTY_LIST);

                ASSERT ((MMLISTS)PfnLastColored->u3.e1.PageLocation == FreePageList);
                PageLastColored = PfnLastColored - MmPfnDatabase;

                if (ListHead->Flink == Page) {

                    ASSERT (Pfn1->u2.Blink == MM_EMPTY_LIST);
                    ASSERT (ListHead->Blink != Page);

                    ListHead->Flink = PageNextFlink;

                    PfnNextFlink->u2.Blink = MM_EMPTY_LIST;
                }
                else {

                    ASSERT (Pfn1->u2.Blink != MM_EMPTY_LIST);
                    ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);
                    ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u3.e1.PageLocation == FreePageList);

                    MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink = PageNextFlink;
                    PfnNextFlink->u2.Blink = Pfn1->u2.Blink;
                }

#if DBG
                if (PfnLastColored->u1.Flink == MM_EMPTY_LIST) {
                    ASSERT (ListHead->Blink == PageLastColored);
                }
#endif

                Pfn1->u1.Flink = PfnLastColored->u1.Flink;
                Pfn1->u2.Blink = PageLastColored;

                if (ListHead->Blink == PageLastColored) {
                    ListHead->Blink = Page;
                }

                //
                // Adjust the colored chains.
                //

                if (PfnLastColored->u1.Flink != MM_EMPTY_LIST) {
                    ASSERT (MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u4.PteFrame != MI_MAGIC_4GB_RECLAIM);
                    ASSERT ((MMLISTS)(MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u3.e1.PageLocation) == FreePageList);
                    MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u2.Blink = Page;
                }

                PfnLastColored->u1.Flink = Page;

                ColorHead->Flink = PageNextColored;
                Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

                ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                PfnLastColored->OriginalPte.u.Long = Page;
                ColorHead->Blink = Pfn1;

                continue;
            }

            //
            // Page is below 4GB so reclaim it.
            //

            ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
            MiUnlinkFreeOrZeroedPage (Page);

            Pfn1->u3.e2.ReferenceCount = 1;
            Pfn1->u2.ShareCount = 1;
            MI_SET_PFN_DELETED(Pfn1);
            Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
            Pfn1->u4.PteFrame = MI_MAGIC_4GB_RECLAIM;
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiNotMapped;

            Pfn1->u3.e1.StartOfAllocation = 1;
            Pfn1->u3.e1.EndOfAllocation = 1;
            Pfn1->u4.VerifierAllocation = 0;
            Pfn1->u3.e1.LargeSessionAllocation = 0;

            ASSERT (Page < MiLowMemoryBitMap->SizeOfBitMap);
            ASSERT (RtlCheckBit (MiLowMemoryBitMap, Page) == 0);
            RtlSetBit (MiLowMemoryBitMap, (ULONG)Page);
            PageCount += 1;
        }
    }

    MmNumberOfPhysicalPages -= PageCount;

    UNLOCK_PFN (OldIrql);

#if DBG
    DbgPrint ("Removed 0x%x pages from low memory for LOW MEMORY testing\n", PageCount);
#endif

    ModuloRemoved = 0;

    if (RemovePhase == 1) {

        //
        // For every page below 4GB that could not be reclaimed, don't use the
        // high modulo-4GB equivalent page.  The motivation is to prevent
        // code bugs that drop the high bits from destroying critical
        // system data in the unclaimed pages (like the GDT, IDT, kernel code
        // and data, etc).
        //

        BitMapHint = 0;
        PagesRemoved = 0;
        StartingRunIndex = 0;
        LengthOfClearRun = 0;

#if DBG
        DbgPrint ("%x Unclaimable Pages below 4GB are:\n\n",
            MiLowMemoryBitMap->SizeOfBitMap - RtlNumberOfSetBits (MiLowMemoryBitMap));
        DbgPrint ("StartPage EndPage  Length\n");
#endif

        do {
    
            BitMapIndex = RtlFindSetBits (MiLowMemoryBitMap, 1, BitMapHint);
        
            if (BitMapIndex < BitMapHint) {
                break;
            }
        
            if (BitMapIndex == NO_BITS_FOUND) {
                break;
            }
    
            //
            // Print the page run that was clear as we didn't get those pages.
            //
    
            if (BitMapIndex != 0) {
#if DBG
                DbgPrint ("%08lx  %08lx %08lx\n",
                            StartingRunIndex,
                            BitMapIndex - 1,
                            BitMapIndex - StartingRunIndex);
#endif

                //
                // Also remove high modulo pages corresponding to the low ones
                // we couldn't get.
                //

                ModuloRemoved += MiRemoveModuloPages (StartingRunIndex,
                                                      BitMapIndex);
            }

            //
            // Found at least one page to copy - try for a cluster.
            //
    
            LengthOfClearRun = RtlFindNextForwardRunClear (MiLowMemoryBitMap,
                                                           BitMapIndex,
                                                           &StartingRunIndex);
    
            if (LengthOfClearRun != 0) {
                LengthOfSetRun = StartingRunIndex - BitMapIndex;
            }
            else {
                LengthOfSetRun = MiLowMemoryBitMap->SizeOfBitMap - BitMapIndex;
            }

            PagesRemoved += LengthOfSetRun;
    
            //
            // Fill the page run with unique patterns.
            //
    
            MiFillRemovedPages (BitMapIndex, LengthOfSetRun);

            //
            // Clear the cache attribute bit in each page as MmMapIoSpace
            // will have set it, but no one else has cleared it.
            //

            Pfn1 = MI_PFN_ELEMENT(BitMapIndex);
            i = LengthOfSetRun;

            LOCK_PFN (OldIrql);

            do {
                Pfn1->u3.e1.CacheAttribute = MiNotMapped;
                Pfn1 += 1;
                i -= 1;
            } while (i != 0);

            UNLOCK_PFN (OldIrql);

            BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;
    
        } while (BitMapHint < MiLowMemoryBitMap->SizeOfBitMap);
    
        if (LengthOfClearRun != 0) {
#if DBG
            DbgPrint ("%08lx  %08lx %08lx\n",
                        StartingRunIndex,
                        StartingRunIndex + LengthOfClearRun - 1,
                        LengthOfClearRun);
#endif

            ModuloRemoved += MiRemoveModuloPages (StartingRunIndex,
                                                  StartingRunIndex + LengthOfClearRun);
        }

        ASSERT (RtlNumberOfSetBits(MiLowMemoryBitMap) == PagesRemoved);
    }

#if DBG
    if (ModuloRemoved != 0) {
        DbgPrint ("Total 0x%x Above-4GB Alias Pages also reclaimed\n\n",
            ModuloRemoved);
    }
#endif

}

PVOID
MiAllocateLowMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PVOID CallingAddress,
    IN MEMORY_CACHING_TYPE CacheType,
    IN ULONG Tag
    )

/*++

Routine Description:

    This is a special routine for allocating contiguous physical memory below
    4GB on a system that has been booted in test mode where all this memory
    has been made generally unavailable to all components.  This lets us find
    problems with device drivers.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    LowestAcceptablePfn - Supplies the lowest page frame number
                          which is valid for the allocation.

    HighestAcceptablePfn - Supplies the highest page frame number
                           which is valid for the allocation.

    BoundaryPfn - Supplies the page frame number multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    CallingAddress - Supplies the calling address of the allocator.

    CacheType - Supplies the type of cache mapping that will be used for the
                memory.

    Tag - Supplies the tag to tie to this allocation.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the system PTEs portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PFN_NUMBER Page;
    PFN_NUMBER BoundaryMask;
    PVOID BaseAddress;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN StartPfn;
    ULONG BitMapHint;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER StartPage;
    PFN_NUMBER LastPage;
    PMMPTE PointerPte;
    PMMPTE DummyPte;
    PHYSICAL_ADDRESS PhysicalAddress;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    PAGED_CODE();

    UNREFERENCED_PARAMETER (Tag);
    UNREFERENCED_PARAMETER (CallingAddress);

    //
    // This cast is ok because the callers check the PFNs first.
    //

    ASSERT64 (LowestAcceptablePfn < _4gb);
    BitMapHint = (ULONG)LowestAcceptablePfn;

    SizeInPages = BYTES_TO_PAGES (NumberOfBytes);
    BoundaryMask = ~(BoundaryPfn - 1);

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    LOCK_PFN (OldIrql);

    do {
        Page = RtlFindSetBits (MiLowMemoryBitMap, (ULONG)SizeInPages, BitMapHint);

        if (Page == (ULONG)-1) {
            UNLOCK_PFN (OldIrql);
            return NULL;
        }

        if (BoundaryPfn == 0) {
            break;
        }

        //
        // If a noncachable mapping is requested, none of the pages in the
        // requested MDL can reside in a large page.  Otherwise we would be
        // creating an incoherent overlapping TB entry as the same physical
        // page would be mapped by 2 different TB entries with different
        // cache attributes.
        //

        if (CacheAttribute != MiCached) {
            for (PageFrameIndex = Page; PageFrameIndex < Page + SizeInPages; PageFrameIndex += 1) {
                if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {

                    MiNonCachedCollisions += 1;

                    //
                    // Keep it simple and just march one page at a time.
                    //

                    BitMapHint += 1;
                    goto FindNext;
                }
            }
        }

        if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) == 0) {

            //
            // This portion of the range meets the alignment requirements.
            //

            break;
        }

        BitMapHint = (ULONG)((Page & BoundaryMask) + BoundaryPfn);

FindNext:

        if ((BitMapHint >= MiLowMemoryBitMap->SizeOfBitMap) ||
            (BitMapHint + SizeInPages > HighestAcceptablePfn)) {
            UNLOCK_PFN (OldIrql);
            return NULL;
        }

    } while (TRUE);

    if (Page + SizeInPages > HighestAcceptablePfn) {
        UNLOCK_PFN (OldIrql);
        return NULL;
    }

    RtlClearBits (MiLowMemoryBitMap, (ULONG)Page, (ULONG)SizeInPages);

    //
    // No need to update ResidentAvailable or commit as these pages were
    // never added to either.
    //

    Pfn1 = MI_PFN_ELEMENT (Page);
    StartPfn = Pfn1;
    StartPage = Page;
    LastPage = Page + SizeInPages;

    DummyPte = MiGetPteAddress (MmNonPagedPoolExpansionStart);

    do {
        ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->OriginalPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE);
        ASSERT (Pfn1->u4.VerifierAllocation == 0);
        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);

        MiDetermineNode (Page, Pfn1);

        Pfn1->u3.e1.CacheAttribute = CacheAttribute;
        Pfn1->u3.e1.EndOfAllocation = 0;

        //
        // Initialize PteAddress so an MiIdentifyPfn scan
        // won't crash.  The real value is put in after the loop.
        //

        Pfn1->PteAddress = DummyPte;

        Pfn1 += 1;
        Page += 1;
    } while (Page < LastPage);

    Pfn1 -= 1;
    Pfn1->u3.e1.EndOfAllocation = 1;
    StartPfn->u3.e1.StartOfAllocation = 1;
    UNLOCK_PFN (OldIrql);

    PhysicalAddress.QuadPart = StartPage;
    PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;

    BaseAddress = MmMapIoSpace (PhysicalAddress,
                                SizeInPages << PAGE_SHIFT,
                                CacheType);

    if (BaseAddress == NULL) {

        //
        // Release the actual pages.
        //

        LOCK_PFN (OldIrql);
        ASSERT (Pfn1->u3.e1.EndOfAllocation == 1);
        Pfn1->u3.e1.EndOfAllocation = 0;
        Pfn1->u3.e1.CacheAttribute = MiNotMapped;
        RtlSetBits (MiLowMemoryBitMap, (ULONG)StartPage, (ULONG)SizeInPages);
        UNLOCK_PFN (OldIrql);

        return NULL;
    }

    PointerPte = MiGetPteAddress (BaseAddress);
    do {
        StartPfn->PteAddress = PointerPte;
        StartPfn->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPte));
        StartPfn += 1;
        PointerPte += 1;
    } while (StartPfn <= Pfn1);

#if 0
    MiInsertContiguousTag (BaseAddress,
                           SizeInPages << PAGE_SHIFT,
                           CallingAddress);
#endif

    return BaseAddress;
}

LOGICAL
MiFreeLowMemory (
    IN PVOID BaseAddress,
    IN ULONG Tag
    )

/*++

Routine Description:

    This is a special routine which returns allocated contiguous physical
    memory below 4GB on a system that has been booted in test mode where
    all this memory has been made generally unavailable to all components.
    This lets us find problems with device drivers.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    Tag - Supplies the tag for this address.

Return Value:

    TRUE if the allocation was freed by this routine, FALSE if not.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PFN_NUMBER Page;
    PFN_NUMBER StartPage;
    KIRQL OldIrql;
    KIRQL OldIrqlHyper;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER SizeInPages;
    PMMPTE PointerPte;
    PMMPTE StartPte;
    PULONG TempVa;
    PEPROCESS Process;

    PAGED_CODE();

    UNREFERENCED_PARAMETER (Tag);

    //
    // If the address is superpage mapped then it must be a regular pool
    // address.
    //

    if (MI_IS_PHYSICAL_ADDRESS(BaseAddress)) {
        return FALSE;
    }

    Process = PsGetCurrentProcess ();
    PointerPte = MiGetPteAddress (BaseAddress);
    StartPte = PointerPte;

    ASSERT (PointerPte->u.Hard.Valid == 1);

    Page = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    //
    // Only free allocations here that really were obtained from the low pool.
    //

    if (Page >= MiNoLowMemory) {
        return FALSE;
    }

    StartPage = Page;
    Pfn1 = MI_PFN_ELEMENT (Page);

    ASSERT (Pfn1->u3.e1.StartOfAllocation == 1);

    //
    // The PFNs can be walked without the PFN lock as no one can be changing
    // the allocation bits while this allocation is being freed.
    //

    Pfn2 = Pfn1;

    while (Pfn2->u3.e1.EndOfAllocation == 0) {
        Pfn2 += 1;
    }

    SizeInPages = Pfn2 - Pfn1 + 1;

    MmUnmapIoSpace (BaseAddress, SizeInPages << PAGE_SHIFT);

    LOCK_PFN (OldIrql);

    Pfn1->u3.e1.StartOfAllocation = 0;

    do {
        ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->OriginalPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE);
        ASSERT (Pfn1->u4.VerifierAllocation == 0);
        ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);

        while (Pfn1->u3.e2.ReferenceCount != 1) {

            //
            // A driver is still transferring data even though the caller
            // is freeing the memory.   Wait a bit before filling this page.
            //

            UNLOCK_PFN (OldIrql);

            //
            // Drain the deferred lists as these pages may be
            // sitting in there right now.
            //

            MiDeferredUnlockPages (0);

            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

            LOCK_PFN (OldIrql);

            ASSERT (Pfn1->u3.e1.StartOfAllocation == 0);
            continue;
        }

        Pfn1->u4.PteFrame = MI_MAGIC_4GB_RECLAIM;
        Pfn1->u3.e1.CacheAttribute = MiNotMapped;

        //
        // Fill the actual page with a recognizable data
        // pattern.  No one else should write to these
        // pages unless they are allocated for
        // a contiguous memory request.
        //

        TempVa = (PULONG)MiMapPageInHyperSpace (Process, Page, &OldIrqlHyper);

        RtlFillMemoryUlong (TempVa, PAGE_SIZE, (ULONG)Page | MI_LOWMEM_MAGIC_BIT);

        MiUnmapPageInHyperSpace (Process, TempVa, OldIrqlHyper);

        if (Pfn1 == Pfn2) {
            break;
        }

        Pfn1 += 1;
        Page += 1;

    } while (TRUE);

    Pfn1->u3.e1.EndOfAllocation = 0;

    //
    // Note the clearing of the bitmap range cannot be done until all the
    // PFNs above are finished.
    //

    ASSERT (RtlAreBitsClear (MiLowMemoryBitMap, (ULONG)StartPage, (ULONG)SizeInPages) == TRUE);
    RtlSetBits (MiLowMemoryBitMap, (ULONG)StartPage, (ULONG)SizeInPages);

    //
    // No need to update ResidentAvailable or commit as these pages were
    // never added to either.
    //

    UNLOCK_PFN (OldIrql);

    return TRUE;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\pagfault.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   pagfault.c

Abstract:

    This module contains the pager for memory management.

Author:

    Lou Perazzoli (loup) 10-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if defined ( _WIN64)
#if DBGXX
VOID
MiCheckPageTableInPage(
    IN PMMPFN Pfn,
    IN PMMINPAGE_SUPPORT Support
);
#endif
#endif

#define STATUS_PTE_CHANGED      0x87303000
#define STATUS_REFAULT          0xC7303001

ULONG MmInPageSupportMinimum = 4;

ULONG MiInPageSinglePages;

extern PMMPTE MmSharedUserDataPte;

extern PVOID MmSpecialPoolStart;
extern PVOID MmSpecialPoolEnd;

ULONG MiFaultRetries;
ULONG MiUserFaultRetries;

ULONG MmClusterPageFileReads;

#define MI_PROTOTYPE_WSINDEX    ((ULONG)-1)

VOID
MiHandleBankedSection (
    IN PVOID VirtualAddress,
    IN PMMVAD Vad
    );

NTSTATUS
MiCompleteProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte
    );


NTSTATUS
MiDispatchFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN PEPROCESS Process,
    OUT PLOGICAL ApcNeeded
    )

/*++

Routine Description:

    This routine dispatches a page fault to the appropriate
    routine to complete the fault.

Arguments:

    FaultStatus - Supplies fault status information bits.

    VirtualAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in,
                      NULL if no prototype PTE exists.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.
              If this parameter is HYDRA_PROCESS, then the fault is for session
              space and the process's working set lock is not held - rather
              the session space's working set lock is held.

    ApcNeeded - Supplies a pointer to a location set to TRUE if an I/O
                completion APC is needed to complete partial IRPs that
                collided.

                It is the caller's responsibility to initialize this (usually
                to FALSE) on entry.  However, since this routine may be called
                multiple times for a single fault (for the page directory,
                page table and the page itself), it is possible for it to
                occasionally be TRUE on entry.

                If it is FALSE on exit, no completion APC is needed.

Return Value:

    status.

Environment:

    Kernel mode, working set lock held.

--*/

{
    MMPTE TempPte;
    NTSTATUS status;
    PMMINPAGE_SUPPORT ReadBlock;
    MMPTE SavedPte;
    PMMINPAGE_SUPPORT CapturedEvent;
    KIRQL OldIrql;
    PPFN_NUMBER Page;
    PFN_NUMBER PageFrameIndex;
    LONG NumberOfBytes;
    PMMPTE CheckPte;
    PMMPTE ReadPte;
    PMMPFN PfnClusterPage;
    PMMPFN Pfn1;
    LOGICAL WsLockChanged;
    PETHREAD CurrentThread;
    PERFINFO_HARDPAGEFAULT_INFORMATION HardFaultEvent;
    LARGE_INTEGER IoStartTime;
    LARGE_INTEGER IoCompleteTime;
    LOGICAL PerfInfoLogHardFault;
    PETHREAD Thread;
    ULONG_PTR StoreInstruction;

    WsLockChanged = FALSE;
    StoreInstruction = MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus);

    //
    // Initializing ReadBlock & ReadPte is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    ReadPte = NULL;
    ReadBlock = NULL;

    if (PointerProtoPte != NULL) {

        ASSERT (!MI_IS_PHYSICAL_ADDRESS(PointerProtoPte));

        CheckPte = MiGetPteAddress (PointerProtoPte);

        //
        // Acquire the PFN lock to synchronize access to prototype PTEs.
        // This is required as the working set lock will not prevent
        // multiple processes from operating on the same prototype PTE.
        //

        LOCK_PFN (OldIrql);

        //
        // Make sure the prototype PTEs are in memory.  For
        // user mode faults, this should already be the case.
        //

        if (CheckPte->u.Hard.Valid == 0) {

            ASSERT ((Process == NULL) || (Process == HYDRA_PROCESS));

            UNLOCK_PFN (OldIrql);

            VirtualAddress = PointerProtoPte;
            PointerPte = CheckPte;
            PointerProtoPte = NULL;

            //
            // The page that contains the prototype PTE is not in memory.
            //

            if (Process == HYDRA_PROCESS) {

                //
                // We were called while holding this session space's
                // working set lock.  But we need to fault in a
                // prototype PTE which is in system paged pool. This
                // must be done under the system working set lock.
                //
                // So we release the session space WSL lock and get
                // the system working set lock.  When done
                // we return STATUS_MORE_PROCESSING_REQUIRED
                // so our caller will call us again to handle the
                // actual prototype PTE fault.
                //

                UNLOCK_SESSION_SPACE_WS (APC_LEVEL);

                //
                // Clear Process as the system working set is now held.
                //

                Process = NULL;

                WsLockChanged = TRUE;

                ASSERT (MI_IS_SESSION_ADDRESS (VirtualAddress) == FALSE);

                LOCK_SYSTEM_WS_UNSAFE (PsGetCurrentThread ());
            }

            goto NonProtoFault;
        }

        if (PointerPte->u.Hard.Valid == 1) {

            //
            // PTE was already made valid by the cache manager support
            // routines.
            //

            UNLOCK_PFN (OldIrql);

            return STATUS_SUCCESS;
        }

        ReadPte = PointerProtoPte;

        status = MiResolveProtoPteFault (StoreInstruction,
                                         VirtualAddress,
                                         PointerPte,
                                         PointerProtoPte,
                                         &ReadBlock,
                                         Process,
                                         ApcNeeded);
        //
        // Returns with PFN lock released.
        //

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    }
    else {

NonProtoFault:

        TempPte = *PointerPte;
        ASSERT (TempPte.u.Long != 0);

        if (TempPte.u.Soft.Transition != 0) {

            //
            // This is a transition page.
            //

            CapturedEvent = NULL;
            status = MiResolveTransitionFault (VirtualAddress,
                                               PointerPte,
                                               Process,
                                               FALSE,
                                               ApcNeeded,
                                               &CapturedEvent);
            if (CapturedEvent != NULL) {
                MiFreeInPageSupportBlock (CapturedEvent);
            }

        }
        else if (TempPte.u.Soft.PageFileHigh == 0) {

            //
            // Demand zero fault.
            //

            status = MiResolveDemandZeroFault (VirtualAddress,
                                               PointerPte,
                                               Process,
                                               FALSE);
        }
        else {

            //
            // Page resides in paging file.
            //

            ReadPte = PointerPte;
            LOCK_PFN (OldIrql);
            status = MiResolvePageFileFault (VirtualAddress,
                                             PointerPte,
                                             &ReadBlock,
                                             Process);
        }
    }

    ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    if (NT_SUCCESS(status)) {

        if (WsLockChanged == TRUE) {
            UNLOCK_SYSTEM_WS (APC_LEVEL);
            LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
        }

        return status;
    }

    if (status == STATUS_ISSUE_PAGING_IO) {

        ASSERT (ReadPte != NULL);
        ASSERT (ReadBlock != NULL);

        SavedPte = *ReadPte;

        CapturedEvent = (PMMINPAGE_SUPPORT)ReadBlock->Pfn->u1.Event;

        CurrentThread = NULL;

        if (Process == HYDRA_PROCESS) {
            UNLOCK_SESSION_SPACE_WS(APC_LEVEL);
        }
        else if (Process != NULL) {

            //
            // APCs must be explicitly disabled to prevent suspend APCs from
            // interrupting this thread before the I/O has been issued.
            // Otherwise a shared page I/O can stop any other thread that
            // references it indefinitely until the suspend is released.
            //

            CurrentThread = PsGetCurrentThread();

            ASSERT (CurrentThread->NestedFaultCount <= 2);
            CurrentThread->NestedFaultCount += 1;

            KeEnterCriticalRegionThread (&CurrentThread->Tcb);
            UNLOCK_WS (Process);
        }
        else {
            UNLOCK_SYSTEM_WS(APC_LEVEL);
        }

#if DBG
        if (MmDebug & MM_DBG_PAGEFAULT) {
            DbgPrint ("MMFAULT: va: %p size: %lx process: %s file: %Z\n",
                VirtualAddress,
                ReadBlock->Mdl.ByteCount,
                Process == HYDRA_PROCESS ? (PUCHAR)"Session Space" : (Process ? Process->ImageFileName : (PUCHAR)"SystemVa"),
                &ReadBlock->FilePointer->FileName
            );
        }
#endif //DBG

        if (PERFINFO_IS_GROUP_ON(PERF_FILE_IO)) {
            PerfInfoLogHardFault = TRUE;

            PerfTimeStamp (IoStartTime);
        }
        else {
            PerfInfoLogHardFault = FALSE;

            //
            // Initializing these is not needed for correctness, but
            // without it the compiler cannot compile this code W4 to check
            // for use of uninitialized variables.
            //

            IoStartTime.QuadPart = 0;
        }

        IoCompleteTime.QuadPart = 0;

        //
        // Assert no reads issued here are marked as prefetched.
        //

        ASSERT (ReadBlock->u1.e1.PrefetchMdlHighBits == 0);

        //
        // Issue the read request.
        //

        status = IoPageRead (ReadBlock->FilePointer,
                             &ReadBlock->Mdl,
                             &ReadBlock->ReadOffset,
                             &ReadBlock->Event,
                             &ReadBlock->IoStatus);

        if (!NT_SUCCESS(status)) {

            //
            // Set the event as the I/O system doesn't set it on errors.
            //

            ReadBlock->IoStatus.Status = status;
            ReadBlock->IoStatus.Information = 0;
            KeSetEvent (&ReadBlock->Event, 0, FALSE);
        }

        //
        // Initializing PageFrameIndex is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        PageFrameIndex = (PFN_NUMBER)-1;

        //
        // Wait for the I/O operation.
        //

        status = MiWaitForInPageComplete (ReadBlock->Pfn,
                                          ReadPte,
                                          VirtualAddress,
                                          &SavedPte,
                                          CapturedEvent,
                                          Process);

        if (CurrentThread != NULL) {
            KeLeaveCriticalRegionThread ((PKTHREAD)CurrentThread);

            ASSERT (CurrentThread->NestedFaultCount <= 3);
            ASSERT (CurrentThread->NestedFaultCount != 0);

            CurrentThread->NestedFaultCount -= 1;

            if ((CurrentThread->ApcNeeded == 1) &&
                (CurrentThread->NestedFaultCount == 0)) {
                *ApcNeeded = TRUE;
                CurrentThread->ApcNeeded = 0;
            }
        }

        if (PerfInfoLogHardFault) {
            PerfTimeStamp (IoCompleteTime);
        }

        //
        // MiWaitForInPageComplete RETURNS WITH THE WORKING SET LOCK
        // AND PFN LOCK HELD!!!
        //

        //
        // This is the thread which owns the event, clear the event field
        // in the PFN database.
        //

        Pfn1 = ReadBlock->Pfn;
        Page = &ReadBlock->Page[0];
        NumberOfBytes = (LONG)ReadBlock->Mdl.ByteCount;
        CheckPte = ReadBlock->BasePte;

        while (NumberOfBytes > 0) {

            //
            // Don't remove the page we just brought in to
            // satisfy this page fault.
            //

            if (CheckPte != ReadPte) {
                PfnClusterPage = MI_PFN_ELEMENT (*Page);
                MI_SNAP_DATA (PfnClusterPage, PfnClusterPage->PteAddress, 0xB);
                ASSERT (PfnClusterPage->u4.PteFrame == Pfn1->u4.PteFrame);
#if DBG
                if (PfnClusterPage->u4.InPageError) {
                    ASSERT (status != STATUS_SUCCESS);
                }
#endif
                if (PfnClusterPage->u3.e1.ReadInProgress != 0) {

                    ASSERT (PfnClusterPage->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                    PfnClusterPage->u3.e1.ReadInProgress = 0;

                    if (PfnClusterPage->u4.InPageError == 0) {
                        PfnClusterPage->u1.Event = NULL;
                    }
                }
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnClusterPage, 9);
            }
            else {
                PageFrameIndex = *Page;
                MI_SNAP_DATA (MI_PFN_ELEMENT (PageFrameIndex),
                              MI_PFN_ELEMENT (PageFrameIndex)->PteAddress,
                              0xC);
            }

            CheckPte += 1;
            Page += 1;
            NumberOfBytes -= PAGE_SIZE;
        }

        if (status != STATUS_SUCCESS) {

            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(MI_PFN_ELEMENT(PageFrameIndex), 9);

            if (status == STATUS_PTE_CHANGED) {

                //
                // State of PTE changed during I/O operation, just
                // return success and refault.
                //

                UNLOCK_PFN (APC_LEVEL);

                if (WsLockChanged == TRUE) {
                    UNLOCK_SYSTEM_WS (APC_LEVEL);
                    LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
                }

                MiFreeInPageSupportBlock (CapturedEvent);

                return STATUS_SUCCESS;

            }

            //
            // An I/O error occurred during the page read
            // operation.  All the pages which were just
            // put into transition should be put onto the
            // free list if InPageError is set, and their
            // PTEs restored to the proper contents.
            //

            Page = &ReadBlock->Page[0];

            NumberOfBytes = ReadBlock->Mdl.ByteCount;

            while (NumberOfBytes > 0) {

                PfnClusterPage = MI_PFN_ELEMENT (*Page);

                if (PfnClusterPage->u4.InPageError == 1) {

                    if (PfnClusterPage->u3.e2.ReferenceCount == 0) {

                        PfnClusterPage->u4.InPageError = 0;

                        //
                        // Only restore the transition PTE if the address
                        // space still exists.  Another thread may have
                        // deleted the VAD while this thread waited for the
                        // fault to complete - in this case, the frame
                        // will be marked as free already.
                        //

                        if (PfnClusterPage->u3.e1.PageLocation != FreePageList) {
                            ASSERT (PfnClusterPage->u3.e1.PageLocation ==
                                                            StandbyPageList);
                            MiUnlinkPageFromList (PfnClusterPage);
                            MiRestoreTransitionPte (*Page);
                            MiInsertPageInFreeList (*Page);
                        }
                    }
                }
                Page += 1;
                NumberOfBytes -= PAGE_SIZE;
            }
            UNLOCK_PFN (APC_LEVEL);

            if (WsLockChanged == TRUE) {
                UNLOCK_SYSTEM_WS (APC_LEVEL);
                LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
            }

            MiFreeInPageSupportBlock (CapturedEvent);

            if (status == STATUS_REFAULT) {

                //
                // The I/O operation to bring in a system page failed
                // due to insufficent resources.  Delay a bit, then
                // return success and refault.
                //

                KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                return STATUS_SUCCESS;
            }

            return status;
        }

        //
        // PTE is still in transition state, same protection, etc.
        //

        ASSERT (Pfn1->u4.InPageError == 0);

        if (Pfn1->u2.ShareCount == 0) {
            MI_REMOVE_LOCKED_PAGE_CHARGE (Pfn1, 9);
        }

        Pfn1->u2.ShareCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        Pfn1->u3.e1.CacheAttribute = MiCached;

        MI_MAKE_TRANSITION_PTE_VALID (TempPte, ReadPte);
        if (StoreInstruction && TempPte.u.Hard.Write) {
            MI_SET_PTE_DIRTY (TempPte);
        }
        MI_WRITE_VALID_PTE (ReadPte, TempPte);

        if (PointerProtoPte != NULL) {

#if DBG
            NTSTATUS oldstatus = status;
#endif

            //
            // The prototype PTE has been made valid, now make the
            // original PTE valid.  The original PTE must still be invalid
            // otherwise MiWaitForInPageComplete would have returned
            // a collision status.
            //

            ASSERT (PointerPte->u.Hard.Valid == 0);

            //
            // PTE is not valid, continue with operation.
            //

            status = MiCompleteProtoPteFault (StoreInstruction,
                                              VirtualAddress,
                                              PointerPte,
                                              PointerProtoPte);

            //
            // Returns with PFN lock released!
            //

#if DBG
            if (PointerPte->u.Hard.Valid == 0) {
                DbgPrint ("MM:PAGFAULT - va %p  %p  %p  status:%lx\n",
                    VirtualAddress, PointerPte, PointerProtoPte, oldstatus);
            }
#endif
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);
        }
        else {

            if (Pfn1->u1.Event == 0) {
                Pfn1->u1.Event = (PVOID)PsGetCurrentThread();
            }

            UNLOCK_PFN (APC_LEVEL);
            MiAddValidPageToWorkingSet (VirtualAddress,
                                        ReadPte,
                                        Pfn1,
                                        0);
            ASSERT (KeGetCurrentIrql() == APC_LEVEL);
        }

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);

        if (PerfInfoLogHardFault) {
            Thread = PsGetCurrentThread();

            HardFaultEvent.ReadOffset = ReadBlock->ReadOffset;
            HardFaultEvent.IoTime.QuadPart = IoCompleteTime.QuadPart - IoStartTime.QuadPart;
            HardFaultEvent.VirtualAddress = VirtualAddress;
            HardFaultEvent.FileObject = ReadBlock->FilePointer;
            HardFaultEvent.ThreadId = HandleToUlong(Thread->Cid.UniqueThread);
            HardFaultEvent.ByteCount = ReadBlock->Mdl.ByteCount;

            PerfInfoLogBytes(PERFINFO_LOG_TYPE_HARDFAULT, &HardFaultEvent, sizeof(HardFaultEvent));
        }

        MiFreeInPageSupportBlock (CapturedEvent);

        if (status == STATUS_SUCCESS) {
            status = STATUS_PAGE_FAULT_PAGING_FILE;
        }
    }

    //
    // Stop high priority threads from consuming the CPU on collided
    // faults for pages that are still marked with inpage errors.  All
    // the threads must let go of the page so it can be freed and the
    // inpage I/O reissued to the filesystem.
    //

    if (MmIsRetryIoStatus(status)) {
        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        status = STATUS_SUCCESS;
    }

    if ((status == STATUS_REFAULT) ||
        (status == STATUS_PTE_CHANGED)) {
        status = STATUS_SUCCESS;
    }

    ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    if (WsLockChanged == TRUE) {
        UNLOCK_SYSTEM_WS (APC_LEVEL);
        LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
    }

    return status;
}



NTSTATUS
MiResolveDemandZeroFault (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN ULONG PrototypePte
    )

/*++

Routine Description:

    This routine resolves a demand zero page fault.

    If the PrototypePte argument is TRUE, the PFN lock is
    held, the lock cannot be dropped, and the page should
    not be added to the working set at this time.

Arguments:

    VirtualAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    PrototypePte - Supplies TRUE if this is a prototype PTE.

Return Value:

    status, either STATUS_SUCCESS or STATUS_REFAULT.

Environment:

    Kernel mode, PFN lock held conditionally.

--*/


{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    MMPTE TempPte;
    ULONG PageColor;
    KIRQL OldIrql;
    LOGICAL NeedToZero;
    LOGICAL BarrierNeeded;
    ULONG BarrierStamp;

    NeedToZero = FALSE;
    BarrierNeeded = FALSE;

    PERFINFO_PRIVATE_PAGE_DEMAND_ZERO(VirtualAddress);

    //
    // Check to see if a page is available, if a wait is
    // returned, do not continue, just return success.
    //

    if (!PrototypePte) {
        LOCK_PFN (OldIrql);
    }

    MM_PFN_LOCK_ASSERT();

    if (PointerPte->u.Hard.Valid == 0) {
        if (!MiEnsureAvailablePageOrWait (Process,
                                          VirtualAddress)) {

            //
            // Initializing BarrierStamp is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            BarrierStamp = 0;

            if (Process != NULL && Process != HYDRA_PROCESS && (!PrototypePte)) {
                //
                // If a fork operation is in progress and the faulting thread
                // is not the thread performing the fork operation, block until
                // the fork is completed.
                //

                if (Process->ForkInProgress != NULL) {
                    if (MiWaitForForkToComplete (Process, TRUE) == TRUE) {
                        UNLOCK_PFN (APC_LEVEL);
                        return STATUS_REFAULT;
                    }
                }

                Process->NumberOfPrivatePages += 1;
                PageColor = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                                   &Process->NextPageColor);

                ASSERT (MI_IS_PAGE_TABLE_ADDRESS(PointerPte));

                PageFrameIndex = MiRemoveZeroPageIfAny (PageColor);
                if (PageFrameIndex) {

                    //
                    // This barrier check is needed after zeroing the page
                    // and before setting the PTE valid.  Note since the PFN
                    // database entry is used to hold the sequence timestamp,
                    // it must be captured now.  Check it at the last possible
                    // moment.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    BarrierStamp = (ULONG)Pfn1->u4.PteFrame;
                }
                else {
                    PageFrameIndex = MiRemoveAnyPage (PageColor);
                    NeedToZero = TRUE;
                }
                BarrierNeeded = TRUE;

            }
            else {
                PageColor = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                                      &MI_SYSTEM_PAGE_COLOR);
                //
                // As this is a system page, there is no need to
                // remove a page of zeroes, it must be initialized by
                // the system before being used.
                //

                if (PrototypePte) {
                    PageFrameIndex = MiRemoveZeroPage (PageColor);
                }
                else {
                    PageFrameIndex = MiRemoveAnyPage (PageColor);
                }
            }

            MmInfoCounters.DemandZeroCount += 1;

            MiInitializePfn (PageFrameIndex, PointerPte, 1);

            if (!PrototypePte) {
                UNLOCK_PFN (APC_LEVEL);
            }

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if (NeedToZero) {

                MiZeroPhysicalPage (PageFrameIndex, PageColor);

                //
                // Note the stamping must occur after the page is zeroed.
                //

                MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
            }

            //
            // As this page is demand zero, set the modified bit in the
            // PFN database element and set the dirty bit in the PTE.
            //

            PERFINFO_SOFTFAULT(Pfn1, VirtualAddress, PERFINFO_LOG_TYPE_DEMANDZEROFAULT)

            MI_SNAP_DATA (Pfn1, PointerPte, 5);

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               PointerPte->u.Soft.Protection,
                               PointerPte);

            if (TempPte.u.Hard.Write != 0) {
                MI_SET_PTE_DIRTY (TempPte);
            }

            if (BarrierNeeded) {
                MI_BARRIER_SYNCHRONIZE (BarrierStamp);
            }

            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            if (!PrototypePte) {
                ASSERT (Pfn1->u1.Event == 0);
                Pfn1->u1.Event = (PVOID)PsGetCurrentThread();
                MiAddValidPageToWorkingSet (VirtualAddress,
                                            PointerPte,
                                            Pfn1,
                                            0);
            }
            return STATUS_PAGE_FAULT_DEMAND_ZERO;
        }
    }
    if (!PrototypePte) {
        UNLOCK_PFN (APC_LEVEL);
    }
    return STATUS_REFAULT;
}


NTSTATUS
MiResolveTransitionFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS CurrentProcess,
    IN ULONG PfnLockHeld,
    OUT PLOGICAL ApcNeeded,
    OUT PMMINPAGE_SUPPORT *InPageBlock
    )

/*++

Routine Description:

    This routine resolves a transition page fault.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    CurrentProcess - Supplies a pointer to the process object.  If this
                     parameter is NULL, then the fault is for system
                     space and the process's working set lock is not held.

    PfnLockHeld - Supplies TRUE if the PFN lock is held, FALSE if not.

    ApcNeeded - Supplies a pointer to a location set to TRUE if an I/O
                completion APC is needed to complete partial IRPs that
                collided.

                It is the caller's responsibility to initialize this (usually
                to FALSE) on entry.  However, since this routine may be called
                multiple times for a single fault (for the page directory,
                page table and the page itself), it is possible for it to
                occasionally be TRUE on entry.

                If it is FALSE on exit, no completion APC is needed.

    InPageBlock - Supplies a pointer to an inpage block pointer.  The caller
                  must initialize this to NULL on entry.  This routine
                  sets this to a non-NULL value to signify an inpage block
                  the caller must free when the caller releases the PFN lock.

Return Value:

    status, either STATUS_SUCCESS, STATUS_REFAULT or an I/O status
    code.

Environment:

    Kernel mode, PFN lock may optionally be held.

--*/

{
    MMPFNENTRY PfnFlags;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    MMPTE TempPte;
    NTSTATUS status;
    NTSTATUS PfnStatus;
    PMMINPAGE_SUPPORT CapturedEvent;
    KIRQL OldIrql;
    PETHREAD CurrentThread;
    PMMPTE PointerToPteForProtoPage;

    //
    // ***********************************************************
    //      Transition PTE.
    // ***********************************************************
    //

    //
    // A transition PTE is either on the free or modified list,
    // on neither list because of its ReferenceCount
    // or currently being read in from the disk (read in progress).
    // If the page is read in progress, this is a collided page
    // and must be handled accordingly.
    //

    ASSERT (*InPageBlock == NULL);

    if (!PfnLockHeld) {
        LOCK_PFN (OldIrql);
    }

    TempPte = *PointerPte;

    if ((TempPte.u.Soft.Valid == 0) &&
        (TempPte.u.Soft.Prototype == 0) &&
        (TempPte.u.Soft.Transition == 1)) {

        //
        // Still in transition format.
        //

        MmInfoCounters.TransitionCount += 1;

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if (Pfn1->u4.InPageError) {

            //
            // There was an in-page read error and there are other
            // threads colliding for this page, delay to let the
            // other threads complete and return.  Snap relevant PFN fields
            // before releasing the lock as the page may immediately get 
            // reused.
            //

            PfnFlags = Pfn1->u3.e1;
            status = Pfn1->u1.ReadStatus;

            if (!PfnLockHeld) {
                UNLOCK_PFN (APC_LEVEL);
            }

            if (PfnFlags.ReadInProgress) {

                //
                // This only occurs when the page is being reclaimed by the
                // compression reaper.  In this case, the page is still on the
                // transition list (so the ReadStatus is really a flink) so
                // substitute a retry status which will induce a delay so the
                // compression reaper can finish taking the page (and PTE).
                //

                return STATUS_NO_MEMORY;
            }

            ASSERT (!NT_SUCCESS(status));

            return status;
        }

        if (Pfn1->u3.e1.ReadInProgress) {

            //
            // Collided page fault.
            //

#if DBG
            if (MmDebug & MM_DBG_COLLIDED_PAGE) {
                DbgPrint("MM:collided page fault\n");
            }
#endif

            CapturedEvent = (PMMINPAGE_SUPPORT)Pfn1->u1.Event;

            CurrentThread = PsGetCurrentThread();

            if (CapturedEvent->Thread == CurrentThread) {

                //
                // This detects MmCopyToCachedPage deadlocks where both the
                // user and system address point at the same physical page.
                //
                // It also detects when the Io APC completion routine accesses
                // the same user page (ie: during an overlapped I/O) that
                // the user thread has already faulted on.
                //
                // Both cases above can result in fatal deadlocks and so must
                // be detected here.  Return a unique status code so the
                // (legitimate) callers know this has happened so it can be
                // handled properly.  In the first case above this means
                // restarting the entire operation immediately.  In the second
                // case above it means requesting a callback from the Mm
                // once the first fault has completed.
                //
                // Note that non-legitimate callers must get back a failure
                // status so the thread can be terminated.
                //

                ASSERT ((CurrentThread->NestedFaultCount == 1) ||
                        (CurrentThread->NestedFaultCount == 2));

                CurrentThread->ApcNeeded = 1;

                if (!PfnLockHeld) {
                    UNLOCK_PFN (APC_LEVEL);
                }
                return STATUS_MULTIPLE_FAULT_VIOLATION;
            }

            //
            // Increment the reference count for the page so it won't be
            // reused until all collisions have been completed.
            //

            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
            ASSERT (Pfn1->u3.e1.LockCharged == 1);

            Pfn1->u3.e2.ReferenceCount += 1;

            //
            // Careful synchronization is applied to the WaitCount field so
            // that freeing of the inpage block can occur lock-free.  Note
            // that the ReadInProgress bit on each PFN is set and cleared while
            // holding the PFN lock.  Inpage blocks are always (and must be)
            // freed _AFTER_ the ReadInProgress bit is cleared.
            //

            InterlockedIncrement(&CapturedEvent->WaitCount);

            UNLOCK_PFN (APC_LEVEL);

            if (CurrentProcess == HYDRA_PROCESS) {
                CurrentThread = NULL;
                UNLOCK_SESSION_SPACE_WS (APC_LEVEL);
            }
            else if (CurrentProcess != NULL) {

                //
                // APCs must be explicitly disabled to prevent suspend APCs from
                // interrupting this thread before the wait has been issued.
                // Otherwise the APC can result in this page being locked
                // indefinitely until the suspend is released.
                //

                ASSERT (CurrentThread->NestedFaultCount <= 2);
                CurrentThread->NestedFaultCount += 1;

                KeEnterCriticalRegionThread (&CurrentThread->Tcb);
                UNLOCK_WS (CurrentProcess);
            }
            else {
                CurrentThread = NULL;
                UNLOCK_SYSTEM_WS (APC_LEVEL);
            }

            //
            // Set the inpage block address as the waitcount was incremented
            // above and therefore the free must be done by our caller.
            //

            *InPageBlock = CapturedEvent;

            status = MiWaitForInPageComplete (Pfn1,
                                              PointerPte,
                                              FaultingAddress,
                                              &TempPte,
                                              CapturedEvent,
                                              CurrentProcess);

            //
            // MiWaitForInPageComplete RETURNS WITH THE WORKING SET LOCK
            // AND PFN LOCK HELD!!!
            //

            if (CurrentThread != NULL) {
                KeLeaveCriticalRegionThread ((PKTHREAD)CurrentThread);

                ASSERT (CurrentThread->NestedFaultCount <= 3);
                ASSERT (CurrentThread->NestedFaultCount != 0);

                CurrentThread->NestedFaultCount -= 1;

                if ((CurrentThread->ApcNeeded == 1) &&
                    (CurrentThread->NestedFaultCount == 0)) {
                    *ApcNeeded = TRUE;
                    CurrentThread->ApcNeeded = 0;
                }
            }

            ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

            if (status != STATUS_SUCCESS) {
                PfnStatus = Pfn1->u1.ReadStatus;
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 9);

                //
                // Check to see if an I/O error occurred on this page.
                // If so, try to free the physical page, wait a
                // half second and return a status of PTE_CHANGED.
                // This will result in success being returned to
                // the user and the fault will occur again and should
                // not be a transition fault this time.
                //

                if (Pfn1->u4.InPageError == 1) {
                    ASSERT (!NT_SUCCESS(PfnStatus));
                    status = PfnStatus;
                    if (Pfn1->u3.e2.ReferenceCount == 0) {

                        Pfn1->u4.InPageError = 0;

                        //
                        // Only restore the transition PTE if the address
                        // space still exists.  Another thread may have
                        // deleted the VAD while this thread waited for the
                        // fault to complete - in this case, the frame
                        // will be marked as free already.
                        //

                        if (Pfn1->u3.e1.PageLocation != FreePageList) {
                            ASSERT (Pfn1->u3.e1.PageLocation ==
                                                            StandbyPageList);
                            MiUnlinkPageFromList (Pfn1);
                            MiRestoreTransitionPte (PageFrameIndex);
                            MiInsertPageInFreeList (PageFrameIndex);
                        }
                    }
                }

#if DBG
                if (MmDebug & MM_DBG_COLLIDED_PAGE) {
                    DbgPrint("MM:decrement ref count - PTE changed\n");
                    MiFormatPfn(Pfn1);
                }
#endif
                if (!PfnLockHeld) {
                    UNLOCK_PFN (APC_LEVEL);
                }

                //
                // Instead of returning status, always return STATUS_REFAULT.
                // This is to support filesystems that save state in the
                // ETHREAD of the thread that serviced the fault !  Since
                // collided threads never enter the filesystem, their ETHREADs
                // haven't been hacked up.  Since this only matters when
                // errors occur (specifically STATUS_VERIFY_REQUIRED today),
                // retry any failed I/O in the context of each collider
                // to give the filesystems ample opportunity.
                //

                return STATUS_REFAULT;
            }

        }
        else {

            //
            // PTE refers to a normal transition PTE.
            //

            ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

            if (MmAvailablePages == 0) {

                //
                // This can only happen if the system is utilizing a hardware
                // compression cache.  This ensures that only a safe amount
                // of the compressed virtual cache is directly mapped so that
                // if the hardware gets into trouble, we can bail it out.
                //

                if (!PfnLockHeld) {
                    UNLOCK_PFN (APC_LEVEL);
                }

                //
                // Note our caller will delay execution after releasing the
                // working set mutex in order to make pages available.
                //

                return STATUS_NO_MEMORY;
            }

            ASSERT (Pfn1->u4.InPageError == 0);
            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

                //
                // This page must contain an MmSt allocation of prototype PTEs.
                // Because these types of pages reside in paged pool (or special
                // pool) and are part of the system working set, they can be
                // trimmed at any time regardless of the share count.  However,
                // if the share count is nonzero, then the page state will
                // remain active and the page will remain in memory - but the
                // PTE will be set to the transition state.  Make the page
                // valid without incrementing the reference count, but
                // increment the share count.
                //

                ASSERT (((Pfn1->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                        ((Pfn1->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

                //
                // Don't increment the valid PTE count for the
                // page table page.
                //

                ASSERT (Pfn1->u2.ShareCount != 0);
                ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

            }
            else {

                MiUnlinkPageFromList (Pfn1);
                ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

                //
                // Update the PFN database - the reference count must be
                // incremented as the share count is going to go from zero to 1.
                //

                ASSERT (Pfn1->u2.ShareCount == 0);

                //
                // The PFN reference count will be 1 already here if the
                // modified writer has begun a write of this page.  Otherwise
                // it's ordinarily 0.
                //

                MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 8);

                Pfn1->u3.e2.ReferenceCount += 1;
            }
        }

        //
        // Join with collided page fault code to handle updating
        // the transition PTE.
        //

        ASSERT (Pfn1->u4.InPageError == 0);

        if (Pfn1->u2.ShareCount == 0) {
            MI_REMOVE_LOCKED_PAGE_CHARGE (Pfn1, 9);
        }

        Pfn1->u2.ShareCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        //
        // Paged pool is trimmed without regard to sharecounts.
        // This means a paged pool PTE can be in transition while
        // the page is still marked active.
        //
        // Note this check only needs to be done for system space addresses
        // as user space address faults lock down the page containing the
        // prototype PTE entries before processing the fault.
        //
        // One example is a system cache fault - the FaultingAddress is a
        // system cache virtual address, the PointerPte points at the pool
        // allocation containing the relevant prototype PTEs.  This page
        // may have been trimmed because it isn't locked down during
        // processing of system space virtual address faults.
        //

        if (FaultingAddress >= MmSystemRangeStart) {

            PointerToPteForProtoPage = MiGetPteAddress (PointerPte);

            TempPte = *PointerToPteForProtoPage;

            if ((TempPte.u.Hard.Valid == 0) &&
                (TempPte.u.Soft.Transition == 1)) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&TempPte);
                Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT ((Pfn2->u3.e1.ReadInProgress == 0) &&
                    (Pfn2->u4.InPageError));

                ASSERT (Pfn2->u3.e1.PageLocation == ActiveAndValid);

                ASSERT (((Pfn2->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                        (Pfn2->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                        ((Pfn2->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                        (Pfn2->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

                //
                // Don't increment the valid PTE count for the
                // paged pool page.
                //

                ASSERT (Pfn2->u2.ShareCount != 0);
                ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

                Pfn2->u3.e1.PageLocation = ActiveAndValid;
                ASSERT (Pfn2->u3.e1.CacheAttribute == MiCached);

                MI_MAKE_VALID_PTE (TempPte,
                                   PageFrameIndex,
                                   Pfn2->OriginalPte.u.Soft.Protection,
                                   PointerToPteForProtoPage);

                MI_WRITE_VALID_PTE (PointerToPteForProtoPage, TempPte);
            }
        }

        MI_MAKE_TRANSITION_PTE_VALID (TempPte, PointerPte);

        //
        // If the modified field is set in the PFN database and this
        // page is not copy on modify, then set the dirty bit.
        // This can be done as the modified page will not be
        // written to the paging file until this PTE is made invalid.
        //

        if (Pfn1->u3.e1.Modified && TempPte.u.Hard.Write &&
                        (TempPte.u.Hard.CopyOnWrite == 0)) {
            MI_SET_PTE_DIRTY (TempPte);
        }
        else {
            MI_SET_PTE_CLEAN (TempPte);
        }

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        if (!PfnLockHeld) {

            if (Pfn1->u1.Event == 0) {
               Pfn1->u1.Event = (PVOID)PsGetCurrentThread();
            }

            UNLOCK_PFN (APC_LEVEL);

            PERFINFO_SOFTFAULT(Pfn1, FaultingAddress, PERFINFO_LOG_TYPE_TRANSITIONFAULT)

            MiAddValidPageToWorkingSet (FaultingAddress,
                                        PointerPte,
                                        Pfn1,
                                        0);
        }
        return STATUS_PAGE_FAULT_TRANSITION;
    }
    else {
        if (!PfnLockHeld) {
            UNLOCK_PFN (APC_LEVEL);
        }
    }
    return STATUS_REFAULT;
}


NTSTATUS
MiResolvePageFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine builds the MDL and other structures to allow a
    read operation on a page file for a page fault.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

Return Value:

    status.  A status value of STATUS_ISSUE_PAGING_IO is returned
    if this function completes successfully.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    PMDL Mdl;
    ULONG i;
    PMMPTE BasePte;
    PMMPTE CheckPte;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PSUBSECTION Subsection;
    ULONG ReadSize;
    LARGE_INTEGER StartingOffset;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER MdlPage;
    ULONG PageFileNumber;
    ULONG ClusterSize;
    ULONG BackwardPageCount;
    ULONG ForwardPageCount;
    ULONG MaxForwardPageCount;
    ULONG MaxBackwardPageCount;
    WSLE_NUMBER WorkingSetIndex;
    ULONG PageColor;
    MMPTE TempPte;
    MMPTE ComparePte;
    PMMINPAGE_SUPPORT ReadBlockLocal;
    PETHREAD CurrentThread;
    PMMVAD Vad;

    // **************************************************
    //    Page File Read
    // **************************************************

    //
    // Calculate the VBN for the in-page operation.
    //

    TempPte = *PointerPte;

    if (TempPte.u.Hard.Valid == 1) {
        UNLOCK_PFN (APC_LEVEL);
        return STATUS_REFAULT;
    }

    ASSERT (TempPte.u.Soft.Prototype == 0);
    ASSERT (TempPte.u.Soft.Transition == 0);

    MM_PFN_LOCK_ASSERT();

    if (MiEnsureAvailablePageOrWait (Process, FaultingAddress)) {

        //
        // A wait operation was performed which dropped the locks,
        // repeat this fault.
        //

        UNLOCK_PFN (APC_LEVEL);
        return STATUS_REFAULT;
    }

    ReadBlockLocal = MiGetInPageSupportBlock (TRUE, Process);

    if (ReadBlockLocal == NULL) {
        UNLOCK_PFN (APC_LEVEL);
        return STATUS_REFAULT;
    }

    MmInfoCounters.PageReadCount += 1;
    MmInfoCounters.PageReadIoCount += 1;

    //
    // Transition collisions rely on the entire PFN (including the event field)
    // being initialized, the ReadBlockLocal's event being not-signaled,
    // and the ReadBlockLocal's thread and waitcount being initialized.
    //
    // All of this has been done by MiGetInPageSupportBlock already except
    // the PFN settings.  The PFN lock can be safely released once
    // this is done.
    //

    ReadSize = 1;
    BasePte = NULL;

    if (MI_IS_PAGE_TABLE_ADDRESS(PointerPte)) {
        WorkingSetIndex = 1;
    }
    else {
        WorkingSetIndex = MI_PROTOTYPE_WSINDEX;
    }

    //
    // Capture the desired cluster size.
    //

    ClusterSize = MmClusterPageFileReads;
    ASSERT (ClusterSize <= MM_MAXIMUM_READ_CLUSTER_SIZE);

    if (MiInPageSinglePages != 0) {
        MiInPageSinglePages -= 1;
    }
    else if ((ClusterSize > 1) && (MmAvailablePages > 256)) {

        //
        // Maybe this condition should be only on free+zeroed pages (ie: don't
        // include standby).  Maybe it should look at the recycle rate of
        // the standby list, etc, etc.
        //

        ASSERT (ClusterSize <= MmAvailablePages);

        //
        // Attempt to cluster ahead and behind.
        //

        MaxForwardPageCount = PTE_PER_PAGE - (BYTE_OFFSET (PointerPte) / sizeof (MMPTE));
        ASSERT (MaxForwardPageCount != 0);
        MaxBackwardPageCount = PTE_PER_PAGE - MaxForwardPageCount;
        MaxForwardPageCount -= 1;

        if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {

            //
            // This is a pagefile read for a shared memory (prototype PTE)
            // backed section.   Stay within the prototype PTE pool allocation.
            //
            // The prototype PTE pool start and end must be carefully
            // calculated (remember the user's view may be smaller or larger
            // than this).  Don't bother walking the entire VAD tree if it is
            // very large as this can take a significant amount of time.
            //

            if ((FaultingAddress <= MM_HIGHEST_USER_ADDRESS) &&
                (Process->NumberOfVads < 128)) {

                Vad = MiLocateAddress (FaultingAddress);

                if (Vad != NULL) {
                    Subsection = MiLocateSubsection (Vad,
                                            MI_VA_TO_VPN(FaultingAddress));

                    if (Subsection != NULL) {
                        FirstPte = &Subsection->SubsectionBase[0];
                        LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                        if ((ULONG)(LastPte - PointerPte - 1) < MaxForwardPageCount) {
                            MaxForwardPageCount = (ULONG)(LastPte - PointerPte - 1);
                        }

                        if ((ULONG)(PointerPte - FirstPte) < MaxBackwardPageCount) {
                            MaxBackwardPageCount = (ULONG)(PointerPte - FirstPte);
                        }
                    }
                    else {
                        ClusterSize = 0;
                    }
                }
                else {
                    ClusterSize = 0;
                }
            }
            else {
                ClusterSize = 0;
            }
        }

        CurrentThread = PsGetCurrentThread();

        if (CurrentThread->ForwardClusterOnly) {

            MaxBackwardPageCount = 0;

            if (MaxForwardPageCount == 0) {

                //
                // This PTE is the last one in the page table page and
                // no backwards clustering is enabled for this thread so
                // no clustering can be done.
                //

                ClusterSize = 0;
            }
        }

        if (ClusterSize != 0) {

            if (MaxForwardPageCount > ClusterSize) {
                MaxForwardPageCount = ClusterSize;
            }

            ComparePte = TempPte;
            CheckPte = PointerPte + 1;
            ForwardPageCount = MaxForwardPageCount;

            //
            // Try to cluster forward within the page of PTEs.
            //

            while (ForwardPageCount != 0) {

                ASSERT (MiIsPteOnPdeBoundary (CheckPte) == 0);

                ComparePte.u.Soft.PageFileHigh += 1;

                if (CheckPte->u.Long != ComparePte.u.Long) {
                    break;
                }

                ForwardPageCount -= 1;
                CheckPte += 1;
            }

            ReadSize += (MaxForwardPageCount - ForwardPageCount);

            //
            // Try to cluster backward within the page of PTEs.  Donate
            // any unused forward cluster space to the backwards gathering
            // but keep the entire transfer within the MDL.
            //

            ClusterSize -= (MaxForwardPageCount - ForwardPageCount);

            if (MaxBackwardPageCount > ClusterSize) {
                MaxBackwardPageCount = ClusterSize;
            }

            ComparePte = TempPte;
            BasePte = PointerPte;
            CheckPte = PointerPte;
            BackwardPageCount = MaxBackwardPageCount;

            while (BackwardPageCount != 0) {

                ASSERT (MiIsPteOnPdeBoundary(CheckPte) == 0);

                CheckPte -= 1;
                ComparePte.u.Soft.PageFileHigh -= 1;

                if (CheckPte->u.Long != ComparePte.u.Long) {
                    break;
                }

                BackwardPageCount -= 1;
            }

            ReadSize += (MaxBackwardPageCount - BackwardPageCount);
            BasePte -= (MaxBackwardPageCount - BackwardPageCount);
        }
    }

    if (ReadSize == 1) {

        //
        // Get a page and put the PTE into the transition state with the
        // read-in-progress flag set.
        //

        if (Process == HYDRA_PROCESS) {
            PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
        }
        else if (Process == NULL) {
            PageColor = MI_GET_PAGE_COLOR_FROM_VA(FaultingAddress);
        }
        else {
            PageColor = MI_PAGE_COLOR_VA_PROCESS (FaultingAddress,
                                                  &Process->NextPageColor);
        }

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        MiInitializeReadInProgressSinglePfn (PageFrameIndex,
                                             PointerPte,
                                             &ReadBlockLocal->Event,
                                             WorkingSetIndex);

        MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE (ReadBlockLocal, &TempPte);
    }
    else {

        Mdl = &ReadBlockLocal->Mdl;
        MdlPage = &ReadBlockLocal->Page[0];

        ASSERT (ReadSize <= MmAvailablePages);

        for (i = 0; i < ReadSize; i += 1) {

            //
            // Get a page and put the PTE into the transition state with the
            // read-in-progress flag set.
            //

            if (Process == HYDRA_PROCESS) {
                PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
            }
            else if (Process == NULL) {
                PageColor = MI_GET_PAGE_COLOR_FROM_VA(FaultingAddress);
            }
            else {
                PageColor = MI_PAGE_COLOR_VA_PROCESS (FaultingAddress,
                                                      &Process->NextPageColor);
            }

            *MdlPage = MiRemoveAnyPage (PageColor);
            MdlPage += 1;
        }

        ReadSize *= PAGE_SIZE;

        //
        // Note PageFrameIndex is the actual frame that was requested by
        // this caller.  All the other frames will be put in transition
        // when the inpage completes (provided there are no colliding threads).
        //

        MdlPage = &ReadBlockLocal->Page[0];
        PageFrameIndex = *(MdlPage + (PointerPte - BasePte));

        //
        // Initialize the MDL for this request.
        //

        MmInitializeMdl (Mdl,
                         MiGetVirtualAddressMappedByPte (BasePte),
                         ReadSize);

        Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

        //
        // Set PointerPte and TempPte to the base of the cluster so the
        // correct starting offset can be calculated below.  Note this must
        // be done before MiInitializeReadInProgressPfn overwrites the PTEs.
        //

        PointerPte = BasePte;
        TempPte = *PointerPte;
        ASSERT (TempPte.u.Soft.Prototype == 0);
        ASSERT (TempPte.u.Soft.Transition == 0);

        //
        // Put the PTEs into the transition state with the
        // read-in-progress flag set.
        //

        MiInitializeReadInProgressPfn (Mdl,
                                       BasePte,
                                       &ReadBlockLocal->Event,
                                       WorkingSetIndex);

        MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(ReadBlockLocal);
    }

    UNLOCK_PFN (APC_LEVEL);

    *ReadBlock = ReadBlockLocal;

    PageFileNumber = GET_PAGING_FILE_NUMBER (TempPte);
    StartingOffset.LowPart = GET_PAGING_FILE_OFFSET (TempPte);

    ASSERT (StartingOffset.LowPart <= MmPagingFile[PageFileNumber]->Size);

    StartingOffset.HighPart = 0;
    StartingOffset.QuadPart = StartingOffset.QuadPart << PAGE_SHIFT;

    ReadBlockLocal->FilePointer = MmPagingFile[PageFileNumber]->File;

#if DBG

    if (((StartingOffset.QuadPart >> PAGE_SHIFT) < 8192) && (PageFileNumber == 0)) {
        if ((MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT] & ~0x1f) !=
               ((ULONG_PTR)PointerPte << 3)) {
            if ((MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT] & ~0x1f) !=
                  ((ULONG_PTR)(MiGetPteAddress(FaultingAddress)) << 3)) {

                DbgPrint("MMINPAGE: Mismatch PointerPte %p Offset %I64X info %p\n",
                         PointerPte,
                         StartingOffset.QuadPart >> PAGE_SHIFT,
                         MmPagingFileDebug[StartingOffset.QuadPart >> PAGE_SHIFT]);

                DbgBreakPoint();
            }
        }
    }

#endif //DBG

    ReadBlockLocal->ReadOffset = StartingOffset;
    ReadBlockLocal->BasePte = PointerPte;

    //
    // Build a single page MDL for the request unless it was a cluster -
    // clustered MDLs have already been constructed.
    //

    if (ReadSize == 1) {
        MmInitializeMdl (&ReadBlockLocal->Mdl, PAGE_ALIGN(FaultingAddress), PAGE_SIZE);
        ReadBlockLocal->Mdl.MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);
        ReadBlockLocal->Page[0] = PageFrameIndex;
    }

    ReadBlockLocal->Pfn = MI_PFN_ELEMENT (PageFrameIndex);

    return STATUS_ISSUE_PAGING_IO;
}

NTSTATUS
MiResolveProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process,
    OUT PLOGICAL ApcNeeded
    )

/*++

Routine Description:

    This routine resolves a prototype PTE fault.

Arguments:

    StoreInstruction - Supplies nonzero if the instruction is trying
                       to modify the faulting address (i.e. write
                       access required).

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

    ApcNeeded - Supplies a pointer to a location set to TRUE if an I/O
                completion APC is needed to complete partial IRPs that
                collided.

Return Value:

    status, either STATUS_SUCCESS, STATUS_REFAULT, or an I/O status
    code.

Environment:

    Kernel mode, PFN lock held.

--*/
{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS status;
    ULONG CopyOnWrite;
    LOGICAL PfnHeld;
    PMMINPAGE_SUPPORT CapturedEvent;

    CapturedEvent = NULL;

    //
    // Note the PFN lock must be held as the routine to locate a working
    // set entry decrements the share count of PFN elements.
    //

    MM_PFN_LOCK_ASSERT();

#if DBG
    if (MmDebug & MM_DBG_PTE_UPDATE) {
        DbgPrint("MM:actual fault %p va %p\n",PointerPte, FaultingAddress);
        MiFormatPte(PointerPte);
    }
#endif //DBG

    ASSERT (PointerPte->u.Soft.Prototype == 1);
    TempPte = *PointerProtoPte;

    //
    // The page containing the prototype PTE is resident,
    // handle the fault referring to the prototype PTE.
    // If the prototype PTE is already valid, make this
    // PTE valid and up the share count etc.
    //

    if (TempPte.u.Hard.Valid) {

        //
        // Prototype PTE is valid.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
        Pfn1->u2.ShareCount += 1;
        status = STATUS_SUCCESS;

        //
        // Count this as a transition fault.
        //

        MmInfoCounters.TransitionCount += 1;
        PfnHeld = TRUE;

        PERFINFO_SOFTFAULT(Pfn1, FaultingAddress, PERFINFO_LOG_TYPE_ADDVALIDPAGETOWS)

    }
    else {

        //
        // Check to make sure the prototype PTE is committed.
        //

        if (TempPte.u.Long == 0) {

#if DBG
            if (MmDebug & MM_DBG_STOP_ON_ACCVIO) {
                DbgPrint("MM:access vio2 - %p\n",FaultingAddress);
                MiFormatPte(PointerPte);
                DbgBreakPoint();
            }
#endif //DEBUG

            UNLOCK_PFN (APC_LEVEL);
            return STATUS_ACCESS_VIOLATION;
        }

        //
        // If the PTE indicates that the protection field to be
        // checked is in the prototype PTE, check it now.
        //

        CopyOnWrite = FALSE;

        if (PointerPte->u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {
            if (PointerPte->u.Proto.ReadOnly == 0) {

                //
                // Check for kernel mode access, we have already verified
                // that the user has access to the virtual address.
                //

#if 0 // removed this assert since mapping drivers via MmMapViewInSystemSpace
      // file violates the assert.

                {
                    PSUBSECTION Sub;
                    if (PointerProtoPte->u.Soft.Prototype == 1) {
                        Sub = MiGetSubsectionAddress (PointerProtoPte);
                        ASSERT (Sub->u.SubsectionFlags.Protection ==
                                    PointerProtoPte->u.Soft.Protection);
                    }
                }

#endif //DBG

                status = MiAccessCheck (PointerProtoPte,
                                        StoreInstruction,
                                        KernelMode,
                                        MI_GET_PROTECTION_FROM_SOFT_PTE (PointerProtoPte),
                                        TRUE);

                if (status != STATUS_SUCCESS) {
#if DBG
                    if (MmDebug & MM_DBG_STOP_ON_ACCVIO) {
                        DbgPrint("MM:access vio3 - %p\n",FaultingAddress);
                        MiFormatPte(PointerPte);
                        MiFormatPte(PointerProtoPte);
                        DbgBreakPoint();
                    }
#endif
                    UNLOCK_PFN (APC_LEVEL);
                    return status;
                }
                if ((PointerProtoPte->u.Soft.Protection & MM_COPY_ON_WRITE_MASK) ==
                     MM_COPY_ON_WRITE_MASK) {
                    CopyOnWrite = TRUE;
                }
            }
        }
        else {
            if ((PointerPte->u.Soft.Protection & MM_COPY_ON_WRITE_MASK) ==
                 MM_COPY_ON_WRITE_MASK) {
                CopyOnWrite = TRUE;
            }
        }

        if ((!IS_PTE_NOT_DEMAND_ZERO(TempPte)) && (CopyOnWrite)) {

            //
            // The prototype PTE is demand zero and copy on
            // write.  Make this PTE a private demand zero PTE.
            //

            ASSERT (Process != NULL);

            PointerPte->u.Long = MM_DEMAND_ZERO_WRITE_PTE;

            UNLOCK_PFN (APC_LEVEL);

            status = MiResolveDemandZeroFault (FaultingAddress,
                                               PointerPte,
                                               Process,
                                               FALSE);
            return status;
        }

        //
        // Make the prototype PTE valid, the prototype PTE is in
        // one of these 4 states:
        //
        //   demand zero
        //   transition
        //   paging file
        //   mapped file
        //

        if (TempPte.u.Soft.Prototype == 1) {

            //
            // Mapped File.
            //

            status = MiResolveMappedFileFault (FaultingAddress,
                                               PointerProtoPte,
                                               ReadBlock,
                                               Process);

            //
            // Returns with PFN lock held.
            //

            PfnHeld = TRUE;

        }
        else if (TempPte.u.Soft.Transition == 1) {

            //
            // Transition.
            //

            status = MiResolveTransitionFault (FaultingAddress,
                                               PointerProtoPte,
                                               Process,
                                               TRUE,
                                               ApcNeeded,
                                               &CapturedEvent);
            //
            // Returns with PFN lock held.
            //

            PfnHeld = TRUE;

        }
        else if (TempPte.u.Soft.PageFileHigh == 0) {

            //
            // Demand Zero
            //

            status = MiResolveDemandZeroFault (FaultingAddress,
                                               PointerProtoPte,
                                               Process,
                                               TRUE);

            //
            // Returns with PFN lock held.
            //

            PfnHeld = TRUE;

        }
        else {

            //
            // Paging file.
            //

            status = MiResolvePageFileFault (FaultingAddress,
                                             PointerProtoPte,
                                             ReadBlock,
                                             Process);

            //
            // Returns with PFN lock released.
            //

            ASSERT (KeGetCurrentIrql() == APC_LEVEL);
            PfnHeld = FALSE;
        }
    }

    if (NT_SUCCESS(status)) {

        ASSERT (PointerPte->u.Hard.Valid == 0);

        MiCompleteProtoPteFault (StoreInstruction,
                                 FaultingAddress,
                                 PointerPte,
                                 PointerProtoPte);

        if (CapturedEvent != NULL) {
            MiFreeInPageSupportBlock (CapturedEvent);
        }

    }
    else {

        if (PfnHeld == TRUE) {
            UNLOCK_PFN (APC_LEVEL);
        }

        ASSERT (KeGetCurrentIrql() == APC_LEVEL);

        if (CapturedEvent != NULL) {
            MiFreeInPageSupportBlock (CapturedEvent);
        }

        //
        // Stop high priority threads from consuming the CPU on collided
        // faults for pages that are still marked with inpage errors.  All
        // the threads must let go of the page so it can be freed and the
        // inpage I/O reissued to the filesystem.
        //

        if (MmIsRetryIoStatus(status)) {
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
            status = STATUS_REFAULT;
        }
    }

    return status;
}



NTSTATUS
MiCompleteProtoPteFault (
    IN ULONG_PTR StoreInstruction,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte
    )

/*++

Routine Description:

    This routine completes a prototype PTE fault.  It is invoked
    after a read operation has completed bringing the data into
    memory.

Arguments:

    StoreInstruction - Supplies nonzero if the instruction is trying
                       to modify the faulting address (i.e. write
                       access required).

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PointerProtoPte - Supplies a pointer to the prototype PTE to fault in,
                      NULL if no prototype PTE exists.

Return Value:

    status.

Environment:

    Kernel mode, PFN lock held.

--*/
{
    MMPTE TempPte;
    MMWSLE ProtoProtect;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE ContainingPageTablePointer;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;
    PSUBSECTION Subsection;
    MMSECTION_FLAGS ControlAreaFlags;
    ULONG Flags;

    MM_PFN_LOCK_ASSERT();

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerProtoPte);
    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
    Pfn1->u3.e1.PrototypePte = 1;

    //
    // Capture prefetch fault information.
    //

    Subsection = NULL;

    if (CCPF_IS_PREFETCHER_ACTIVE()) {

        TempPte = Pfn1->OriginalPte;

        if (TempPte.u.Soft.Prototype == 1) {

            Subsection = MiGetSubsectionAddress (&TempPte);
        }
    }

    //
    // Prototype PTE is now valid, make the PTE valid.
    //

    ASSERT (PointerProtoPte->u.Hard.Valid == 1);

    //
    // A PTE just went from not present, not transition to
    // present.  The share count and valid count must be
    // updated in the page table page which contains this PTE.
    //

    ContainingPageTablePointer = MiGetPteAddress(PointerPte);
    Pfn2 = MI_PFN_ELEMENT(ContainingPageTablePointer->u.Hard.PageFrameNumber);
    Pfn2->u2.ShareCount += 1;

    ProtoProtect.u1.Long = 0;
    if (PointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

        //
        // The protection code for the real PTE comes from the real PTE as
        // it was placed there earlier during the handling of this fault.
        //

        ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(PointerPte);
    }
    else if ((MI_IS_SESSION_IMAGE_ADDRESS (FaultingAddress)) &&
             (PointerPte->u.Proto.ReadOnly == 0)) {

        //
        // Session image addresses must be treated specially.  This is
        // because we only encode the readonly bit in the PTEs in the
        // native pagetables (ie: not in the prototype PTEs themselves).
        //
        // Normally MiWaitForInPageComplete checks to make sure that collided
        // faults are processed properly by seeing if the prototype PTE
        // state before and after the fault is the same.  This is not enough
        // because for the session image range, the readonly attribute of the
        // native PTE must also be taken into account because it must be
        // preserved here.  Consider:
        //
        // Thread A faults on a session image address that is *data* (ie:
        // readwrite bss).  The native PTE is set to LOOKUP_NEEDED and execute-
        // writecopy is set in the native PTE as well at the start of the
        // fault.  The prototype PTE is put in transition and an inpage
        // initiated.
        //
        // Then thread B collides on the same address, eventually racing
        // ahead of thread A after the inpage completes and makes both the
        // prototype PTE and the hardware PTE valid, and puts the session
        // image VA into the session working set list.
        //
        // Now the working set trimmer executes and trims the newly inserted
        // session image VA.  The hardware PTE is repointed back to the
        // prototype PTE *WITHOUT* the readonly bit set (this is correct),
        // and the prototype PTE continues to point at the same transition
        // page because the reference count on the PFN is still held by
        // thread A.
        //
        // Then thread A resumes to process the initial fault, unaware
        // that thread B and the trimmer thread ran while thread A was waiting
        // for the inpage to complete.  The first check above will see the
        // hardware PTE is not encoded with lookup needed and thus assume
        // that the protection should be set to the prototype PTE below.
        // This would be wrong as the session address referred to data !
        //
        // This is the edge condition the code in this if statement handles.
        //

        ProtoProtect.u1.e1.Protection = MM_EXECUTE_WRITECOPY;
    }
    else {

        //
        // Use the protection in the prototype PTE to initialize the real PTE.
        //

        ProtoProtect.u1.e1.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn1->OriginalPte);
        ProtoProtect.u1.e1.SameProtectAsProto = 1;

        MI_ASSERT_NOT_SESSION_DATA (PointerPte);

        if ((StoreInstruction != 0) &&
            ((ProtoProtect.u1.e1.Protection & MM_PROTECTION_WRITE_MASK) == 0)) {

            //
            // This is the errant case where the user is trying to write
            // to a readonly subsection in the image.  Since we're more than
            // halfway through the fault, take the easy way to clean this up -
            // treat the access as a read for the rest of this trip through
            // the fault.  We'll then immediately refault when the instruction
            // is rerun (because it's really a write), and then we'll notice
            // that the user's PTE is not copy-on-write (or even writable!)
            // and return a clean access violation.
            //

#if DBG
            DbgPrint("MM: user tried to write to a readonly subsection in the image! %p %p %p\n",
                FaultingAddress,
                PointerPte,
                PointerProtoPte);
#endif
            StoreInstruction = 0;
        }
    }

    MI_SNAP_DATA (Pfn1, PointerProtoPte, 6);

    MI_MAKE_VALID_PTE (TempPte,
                       PageFrameIndex,
                       ProtoProtect.u1.e1.Protection,
                       PointerPte);

    //
    // If this is a store instruction and the page is not copy on
    // write, then set the modified bit in the PFN database and
    // the dirty bit in the PTE.  The PTE is not set dirty even
    // if the modified bit is set so writes to the page can be
    // tracked for FlushVirtualMemory.
    //

    if ((StoreInstruction != 0) && (TempPte.u.Hard.CopyOnWrite == 0)) {

#if DBG
        PVOID Va;
        MMPTE TempPte2;
        PSUBSECTION Subsection2;
        PCONTROL_AREA ControlArea;

        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        //
        // Session space backed by the filesystem is not writable.
        //

        ASSERT (!MI_IS_SESSION_IMAGE_ADDRESS (Va));

        TempPte2 = Pfn1->OriginalPte;

        if (TempPte2.u.Soft.Prototype == 1) {

            Subsection2 = MiGetSubsectionAddress (&TempPte2);
            ControlArea = Subsection2->ControlArea;

            if (ControlArea->DereferenceList.Flink != NULL) {
                if (!KdDebuggerNotPresent) {
                    DbgPrint ("MM: page fault completing to dereferenced CA %p %p %p\n",
                                    ControlArea, Pfn1, PointerPte);
                    DbgBreakPoint ();
                }
            }
        }
#endif

        MI_SET_MODIFIED (Pfn1, 1, 0xA);

        MI_SET_PTE_DIRTY (TempPte);

        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                      (Pfn1->u3.e1.WriteInProgress == 0)) {
             MiReleasePageFileSpace (Pfn1->OriginalPte);
             Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }
    }

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    if (Pfn1->u1.Event == NULL) {
        Pfn1->u1.Event = (PVOID)PsGetCurrentThread();
    }

    UNLOCK_PFN (APC_LEVEL);

    PERFINFO_SOFTFAULT(Pfn1, FaultingAddress, PERFINFO_LOG_TYPE_PROTOPTEFAULT);

    MiAddValidPageToWorkingSet (FaultingAddress,
                                PointerPte,
                                Pfn1,
                                (ULONG) ProtoProtect.u1.Long);

    //
    // Log prefetch fault information now that the PFN lock has been released
    // and the PTE has been made valid.  This minimizes PFN lock contention,
    // allows CcPfLogPageFault to allocate (and fault on) pool, and allows other
    // threads in this process to execute without faulting on this address.
    //
    // Note that the process' working set mutex is still held so any other
    // faults or operations on user addresses by other threads in this process
    // will block for the duration of this call.
    //

    if (Subsection != NULL) {
        FileObject = Subsection->ControlArea->FilePointer;
        FileOffset = MiStartingOffset (Subsection, PointerProtoPte);
        ControlAreaFlags = Subsection->ControlArea->u.Flags;

        Flags = 0;
        if (ControlAreaFlags.Image) {

            if ((Subsection->StartingSector == 0) &&
                (Subsection->SubsectionBase != Subsection->ControlArea->Segment->PrototypePte)) {
                //
                // This is an image that was built with a linker pre-1995
                // (version 2.39 is one example) that put bss into a separate
                // subsection with zero as a starting file offset field
                // in the on-disk image.  The prefetcher will fetch from the
                // wrong offset trying to satisfy these ranges (which are
                // actually demand zero when the fault occurs) so don't let
                // the prefetcher know about ANY access within this subsection.
                //

                goto Finish;
            }

            Flags |= CCPF_TYPE_IMAGE;
        }
        if (ControlAreaFlags.Rom) {
            Flags |= CCPF_TYPE_ROM;
        }
        CcPfLogPageFault (FileObject, FileOffset, Flags);
    }

Finish:

    ASSERT (PointerPte == MiGetPteAddress(FaultingAddress));

    return STATUS_SUCCESS;
}


NTSTATUS
MiResolveMappedFileFault (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    OUT PMMINPAGE_SUPPORT *ReadBlock,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine builds the MDL and other structures to allow a
    read operation on a mapped file for a page fault.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    ReadBlock - Supplies a pointer to put the address of the read block which
                needs to be completed before an I/O can be issued.

    Process - Supplies a pointer to the process object.  If this
              parameter is NULL, then the fault is for system
              space and the process's working set lock is not held.

Return Value:

    status.  A status value of STATUS_ISSUE_PAGING_IO is returned
    if this function completes successfully.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PMDL Mdl;
    ULONG ReadSize;
    PETHREAD CurrentThread;
    PPFN_NUMBER Page;
    PPFN_NUMBER EndPage;
    PMMPTE BasePte;
    PMMPTE CheckPte;
    LARGE_INTEGER StartingOffset;
    LARGE_INTEGER TempOffset;
    PPFN_NUMBER FirstMdlPage;
    PMMINPAGE_SUPPORT ReadBlockLocal;
    ULONG PageColor;
    ULONG ClusterSize;
    ULONG Result;
    PFN_COUNT AvailablePages;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;

    ClusterSize = 0;

    ASSERT (PointerPte->u.Soft.Prototype == 1);

    // *********************************************
    //   Mapped File (subsection format)
    // *********************************************

    Result = MiEnsureAvailablePageOrWait (Process, FaultingAddress);

    if (Result) {

        //
        // A wait operation was performed which dropped the locks,
        // repeat this fault.
        //

        return STATUS_REFAULT;
    }

#if DBG
    if (MmDebug & MM_DBG_PTE_UPDATE) {
        MiFormatPte (PointerPte);
    }
#endif

    //
    // Calculate address of subsection for this prototype PTE.
    //

    Subsection = MiGetSubsectionAddress (PointerPte);

#ifdef LARGE_PAGES

    //
    // Check to see if this subsection maps a large page, if
    // so, just fill the TB and return a status of PTE_CHANGED.
    //

    if (Subsection->u.SubsectionFlags.LargePages == 1) {
        KeFlushEntireTb (TRUE, TRUE);
        KeFillLargeEntryTb ((PHARDWARE_PTE)(Subsection + 1),
                            FaultingAddress,
                            Subsection->StartingSector);

        return STATUS_REFAULT;
    }
#endif

    ControlArea = Subsection->ControlArea;

    if (ControlArea->u.Flags.FailAllIo) {
        return STATUS_IN_PAGE_ERROR;
    }

    if (PointerPte >= &Subsection->SubsectionBase[Subsection->PtesInSubsection]) {

        //
        // Attempt to read past the end of this subsection.
        //

        return STATUS_ACCESS_VIOLATION;
    }

    if (ControlArea->u.Flags.Rom == 1) {
		ASSERT (XIPConfigured == TRUE);

        //
        // Calculate the offset to read into the file.
        //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
        //

        StartingOffset.QuadPart = MiStartingOffset (Subsection, PointerPte);

        TempOffset = MiEndingOffset(Subsection);

        ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

        //
        // Check to see if the read will go past the end of the file,
        // If so, correct the read size and get a zeroed page instead.
        //

        if ((ControlArea->u.Flags.Image) &&
            (((UINT64)StartingOffset.QuadPart + PAGE_SIZE) > (UINT64)TempOffset.QuadPart)) {

            ReadBlockLocal = MiGetInPageSupportBlock (TRUE, Process);
            if (ReadBlockLocal == NULL) {
                return STATUS_REFAULT;
            }
            *ReadBlock = ReadBlockLocal;

            CurrentThread = PsGetCurrentThread();

            //
            // Build an MDL for the request.
            //

            Mdl = &ReadBlockLocal->Mdl;

            FirstMdlPage = &ReadBlockLocal->Page[0];
            Page = FirstMdlPage;

#if DBG
            RtlFillMemoryUlong (Page,
                                (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof(PFN_NUMBER),
                                0xf1f1f1f1);
#endif

            ReadSize = PAGE_SIZE;
            BasePte = PointerPte;

            ClusterSize = 1;

            goto UseSingleRamPage;
        }

        PageFrameIndex = (PFN_NUMBER) (StartingOffset.QuadPart >> PAGE_SHIFT);
        PageFrameIndex += ((PLARGE_CONTROL_AREA)ControlArea)->StartingFrame;

        //
        // Increment the PFN reference count in the control area for
        // the subsection (the PFN lock is required to modify this field).
        //

        ControlArea->NumberOfPfnReferences += 1;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u3.e1.Rom == 1);

        if (Pfn1->u3.e1.PageLocation != 0) {

            ASSERT (Pfn1->u3.e1.PageLocation == StandbyPageList);

            MiUnlinkPageFromList (Pfn1);

            //
            // Update the PFN database - the reference count must be
            // incremented as the share count is going to go from zero to 1.
            //

            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

            Pfn1->u3.e2.ReferenceCount += 1;
            Pfn1->u2.ShareCount += 1;
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiCached;
            ASSERT (Pfn1->PteAddress == PointerPte);
            ASSERT (Pfn1->u1.Event == NULL);

            //
            // Determine the page frame number of the page table page which
            // contains this PTE.
            //

            PteFramePointer = MiGetPteAddress (PointerPte);
            if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x61940,
                              (ULONG_PTR)PointerPte,
                              (ULONG_PTR)PteFramePointer->u.Long,
                              0);
#if (_MI_PAGING_LEVELS < 3)
                }
#endif
            }

            PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
            ASSERT (Pfn1->u4.PteFrame == PteFramePage);

            //
            // Increment the share count for the page table page containing
            // this PTE as the PTE is going to be made valid.
            //

            ASSERT (PteFramePage != 0);
            Pfn2 = MI_PFN_ELEMENT (PteFramePage);
            Pfn2->u2.ShareCount += 1;
        }
        else {
            ASSERT (Pfn1->u4.InPageError == 0);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);
            ASSERT (Pfn1->u1.Event == NULL);
            MiInitializePfn (PageFrameIndex, PointerPte, 0);
        }

        //
        // Put the prototype PTE into the valid state.
        //

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           PointerPte->u.Soft.Protection,
                           PointerPte);

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        return STATUS_PAGE_FAULT_TRANSITION;
    }

    CurrentThread = PsGetCurrentThread();

    ReadBlockLocal = MiGetInPageSupportBlock (TRUE, Process);
    if (ReadBlockLocal == NULL) {
        return STATUS_REFAULT;
    }
    *ReadBlock = ReadBlockLocal;

    //
    // Build an MDL for the request.
    //

    Mdl = &ReadBlockLocal->Mdl;

    FirstMdlPage = &ReadBlockLocal->Page[0];
    Page = FirstMdlPage;

#if DBG
    RtlFillMemoryUlong (Page, (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof(PFN_NUMBER), 0xf1f1f1f1);
#endif //DBG

    ReadSize = PAGE_SIZE;
    BasePte = PointerPte;

    //
    // Should we attempt to perform page fault clustering?
    //

    AvailablePages = MmAvailablePages;

    if (MiInPageSinglePages != 0) {
        AvailablePages = 0;
        MiInPageSinglePages -= 1;
    }

    if ((!CurrentThread->DisablePageFaultClustering) &&
        (PERFINFO_DO_PAGEFAULT_CLUSTERING()) &&
        (ControlArea->u.Flags.NoModifiedWriting == 0)) {

        if ((AvailablePages > (MmFreeGoal * 2))
                 ||
         (((ControlArea->u.Flags.Image != 0) ||
            (CurrentThread->ForwardClusterOnly)) &&
         (AvailablePages > (MM_MAXIMUM_READ_CLUSTER_SIZE + 16)))) {

            //
            // Cluster up to n pages.  This one + n-1.
            //

            if (ControlArea->u.Flags.Image == 0) {
                ASSERT (CurrentThread->ReadClusterSize <=
                            MM_MAXIMUM_READ_CLUSTER_SIZE);
                ClusterSize = CurrentThread->ReadClusterSize;
            }
            else {
                ClusterSize = MmDataClusterSize;
                if (Subsection->u.SubsectionFlags.Protection &
                                            MM_PROTECTION_EXECUTE_MASK ) {
                    ClusterSize = MmCodeClusterSize;
                }
            }
            EndPage = Page + ClusterSize;

            CheckPte = PointerPte + 1;

            //
            // Try to cluster within the page of PTEs.
            //

            while ((MiIsPteOnPdeBoundary(CheckPte) == 0) &&
               (Page < EndPage) &&
               (CheckPte <
                 &Subsection->SubsectionBase[Subsection->PtesInSubsection])
                      && (CheckPte->u.Long == BasePte->u.Long)) {

                ControlArea->NumberOfPfnReferences += 1;
                ReadSize += PAGE_SIZE;
                Page += 1;
                CheckPte += 1;
            }

            if ((Page < EndPage) && (!CurrentThread->ForwardClusterOnly)) {

                //
                // Attempt to cluster going backwards from the PTE.
                //

                CheckPte = PointerPte - 1;

                while ((((ULONG_PTR)CheckPte & (PAGE_SIZE - 1)) !=
                                            (PAGE_SIZE - sizeof(MMPTE))) &&
                        (Page < EndPage) &&
                         (CheckPte >= Subsection->SubsectionBase) &&
                         (CheckPte->u.Long == BasePte->u.Long)) {

                    ControlArea->NumberOfPfnReferences += 1;
                    ReadSize += PAGE_SIZE;
                    Page += 1;
                    CheckPte -= 1;
                }
                BasePte = CheckPte + 1;
            }
        }
    }

    //
    //
    // Calculate the offset to read into the file.
    //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
    //

    StartingOffset.QuadPart = MiStartingOffset (Subsection, BasePte);

    TempOffset = MiEndingOffset(Subsection);

    ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

UseSingleRamPage:

    //
    // Remove pages to fill in the MDL.  This is done here as the
    // base PTE has been determined and can be used for virtual
    // aliasing checks.
    //

    EndPage = FirstMdlPage;
    CheckPte = BasePte;

    while (EndPage < Page) {
        if (Process == HYDRA_PROCESS) {
            PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
        }
        else if (Process == NULL) {
            PageColor = MI_GET_PAGE_COLOR_FROM_PTE (CheckPte);
        }
        else {
            PageColor = MI_PAGE_COLOR_PTE_PROCESS (CheckPte,
                                                   &Process->NextPageColor);
        }
        *EndPage = MiRemoveAnyPage (PageColor);

        EndPage += 1;
        CheckPte += 1;
    }

    if (Process == HYDRA_PROCESS) {
        PageColor = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);
    }
    else if (Process == NULL) {
        PageColor = MI_GET_PAGE_COLOR_FROM_PTE (CheckPte);
    }
    else {
        PageColor = MI_PAGE_COLOR_PTE_PROCESS (CheckPte,
                                               &Process->NextPageColor);
    }

    //
    // Check to see if the read will go past the end of the file,
    // If so, correct the read size and get a zeroed page.
    //

    MmInfoCounters.PageReadIoCount += 1;
    MmInfoCounters.PageReadCount += ReadSize >> PAGE_SHIFT;

    if ((ControlArea->u.Flags.Image) &&
        (((UINT64)StartingOffset.QuadPart + ReadSize) > (UINT64)TempOffset.QuadPart)) {

        ASSERT ((ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart)
                > (ReadSize - PAGE_SIZE));

        ReadSize = (ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart);

        //
        // Round the offset to a 512-byte offset as this will help filesystems
        // optimize the transfer.  Note that filesystems will always zero fill
        // the remainder between VDL and the next 512-byte multiple and we have
        // already zeroed the whole page.
        //

        ReadSize = ((ReadSize + MMSECTOR_MASK) & ~MMSECTOR_MASK);

        PageFrameIndex = MiRemoveZeroPage (PageColor);

    }
    else {

        //
        // We are reading a complete page, no need to get a zeroed page.
        //

        PageFrameIndex = MiRemoveAnyPage (PageColor);
    }

    //
    // Increment the PFN reference count in the control area for
    // the subsection (the PFN lock is required to modify this field).
    //

    ControlArea->NumberOfPfnReferences += 1;
    *Page = PageFrameIndex;

    PageFrameIndex = *(FirstMdlPage + (PointerPte - BasePte));

    //
    // Get a page and put the PTE into the transition state with the
    // read-in-progress flag set.
    //

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Initialize MDL for request.
    //

    MmInitializeMdl (Mdl,
                     MiGetVirtualAddressMappedByPte (BasePte),
                     ReadSize);
    Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

#if DBG
    if (ReadSize > ((ClusterSize + 1) << PAGE_SHIFT)) {
        KeBugCheckEx (MEMORY_MANAGEMENT, 0x777,(ULONG_PTR)Mdl, (ULONG_PTR)Subsection,
                        (ULONG)TempOffset.LowPart);
    }
#endif //DBG

    MiInitializeReadInProgressPfn (Mdl,
                                   BasePte,
                                   &ReadBlockLocal->Event,
                                   MI_PROTOTYPE_WSINDEX);

    MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(ReadBlockLocal);

    ReadBlockLocal->ReadOffset = StartingOffset;
    ReadBlockLocal->FilePointer = ControlArea->FilePointer;
    ReadBlockLocal->BasePte = BasePte;
    ReadBlockLocal->Pfn = Pfn1;

    return STATUS_ISSUE_PAGING_IO;
}

NTSTATUS
MiWaitForInPageComplete (
    IN PMMPFN Pfn2,
    IN PMMPTE PointerPte,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPteContents,
    IN PMMINPAGE_SUPPORT InPageSupport,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    Waits for a page read to complete.

Arguments:

    Pfn - Supplies a pointer to the PFN element for the page being read.

    PointerPte - Supplies a pointer to the PTE that is in the transition
                 state.  This can be a prototype PTE address.

    FaultingAddress - Supplies the faulting address.

    PointerPteContents - Supplies the contents of the PTE before the
                         working set lock was released.

    InPageSupport - Supplies a pointer to the inpage support structure
                    for this read operation.

Return Value:

    Returns the status of the inpage operation.

    Note that the working set mutex and PFN lock are held upon return !!!

Environment:

    Kernel mode, APCs disabled.  Neither the working set lock nor
    the PFN lock may be held.

--*/

{
    PMMPTE NewPointerPte;
    PMMPTE ProtoPte;
    PMMPFN Pfn1;
    PMMPFN Pfn;
    PULONG Va;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    ULONG Offset;
    ULONG Protection;
    PMDL Mdl;
    KIRQL OldIrql;
    NTSTATUS status;
    NTSTATUS status2;
    PEPROCESS Process;

    //
    // Wait for the I/O to complete.  Note that we can't wait for all
    // the objects simultaneously as other threads/processes could be
    // waiting for the same event.  The first thread which completes
    // the wait and gets the PFN lock may reuse the event for another
    // fault before this thread completes its wait.
    //

    KeWaitForSingleObject( &InPageSupport->Event,
                           WrPageIn,
                           KernelMode,
                           FALSE,
                           NULL);

    if (CurrentProcess == HYDRA_PROCESS) {
        LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
    }
    else if (CurrentProcess == PREFETCH_PROCESS) {
        NOTHING;
    }
    else if (CurrentProcess != NULL) {
        LOCK_WS (CurrentProcess);
    }
    else {
        LOCK_SYSTEM_WS (OldIrql, PsGetCurrentThread ());
    }

    LOCK_PFN (OldIrql);

    ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

    //
    // Check to see if this is the first thread to complete the in-page
    // operation.
    //

    Pfn = InPageSupport->Pfn;
    if (Pfn2 != Pfn) {
        ASSERT (Pfn2->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        Pfn2->u3.e1.ReadInProgress = 0;
    }

    //
    // Another thread has already serviced the read, check the
    // io-error flag in the PFN database to ensure the in-page
    // was successful.
    //

    if (Pfn2->u4.InPageError == 1) {
        ASSERT (!NT_SUCCESS(Pfn2->u1.ReadStatus));

        if (MmIsRetryIoStatus(Pfn2->u1.ReadStatus)) {
            return STATUS_REFAULT;
        }
        return Pfn2->u1.ReadStatus;
    }

    if (InPageSupport->u1.e1.Completed == 0) {

        //
        // The ReadInProgress bit for the dummy page is constantly cleared
        // below as there are generally multiple inpage blocks pointing to
        // the same dummy page.
        //

        ASSERT ((Pfn->u3.e1.ReadInProgress == 1) ||
                (Pfn->PteAddress == MI_PF_DUMMY_PAGE_PTE));

        InPageSupport->u1.e1.Completed = 1;

        Mdl = &InPageSupport->Mdl;

        if (InPageSupport->u1.e1.PrefetchMdlHighBits != 0) {

            //
            // This is a prefetcher-issued read.
            //

            Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        ASSERT (Pfn->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        Pfn->u3.e1.ReadInProgress = 0;
        Pfn->u1.Event = NULL;

#if defined (_WIN64)
        //
        // Page directory and page table pages are never clustered,
        // ensure this is never violated as only one UsedPageTableEntries
        // is kept in the inpage support block.
        //

        if (InPageSupport->UsedPageTableEntries) {
            Page = (PPFN_NUMBER)(Mdl + 1);
            LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);
            ASSERT (Page == LastPage);
        }

#if DBGXX
        MiCheckPageTableInPage (Pfn, InPageSupport);
#endif
#endif

        MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(Pfn, InPageSupport);

        //
        // Check the IO_STATUS_BLOCK to ensure the in-page completed successfully.
        //

        if (!NT_SUCCESS(InPageSupport->IoStatus.Status)) {

            if (InPageSupport->IoStatus.Status == STATUS_END_OF_FILE) {

                //
                // An attempt was made to read past the end of file
                // zero all the remaining bytes in the read.
                //

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);

                while (Page <= LastPage) {
                    MiZeroPhysicalPage (*Page, 0);

                    MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(MI_PFN_ELEMENT(*Page));

                    Page += 1;
                }

            }
            else {

                //
                // In page io error occurred.
                //

                status = InPageSupport->IoStatus.Status;
                status2 = InPageSupport->IoStatus.Status;

                if (status != STATUS_VERIFY_REQUIRED) {

                    LOGICAL Retry;

                    Retry = FALSE;
#if DBG
                    DbgPrint ("MM: inpage I/O error %X\n",
                                    InPageSupport->IoStatus.Status);
#endif

                    //
                    // If this page is for paged pool or for paged
                    // kernel code or page table pages, bugcheck.
                    //

                    if ((FaultingAddress > MM_HIGHEST_USER_ADDRESS) &&
                        (!MI_IS_SYSTEM_CACHE_ADDRESS(FaultingAddress))) {

                        if (MmIsRetryIoStatus(status)) {

                            if (MiInPageSinglePages == 0) {
                                MiInPageSinglePages = 30;
                            }

                            MiFaultRetries -= 1;
                            if (MiFaultRetries & MiFaultRetryMask) {
                                Retry = TRUE;
                            }
                        }

                        if (Retry == FALSE) {

                            ULONG_PTR PteContents;

                            //
                            // The prototype PTE resides in paged pool which may
                            // not be resident at this point.  Check first.
                            //

                            if (MmIsAddressValid (PointerPte) == TRUE) {
                                PteContents = *(PULONG_PTR)PointerPte;
                            }
                            else {
                                PteContents = (ULONG_PTR)-1;
                            }

                            KeBugCheckEx (KERNEL_DATA_INPAGE_ERROR,
                                          (ULONG_PTR)PointerPte,
                                          status,
                                          (ULONG_PTR)FaultingAddress,
                                          PteContents);
                        }
                        status2 = STATUS_REFAULT;
                    }
                    else {

                        if (MmIsRetryIoStatus(status)) {

                            if (MiInPageSinglePages == 0) {
                                MiInPageSinglePages = 30;
                            }

                            MiUserFaultRetries -= 1;
                            if (MiUserFaultRetries & MiUserFaultRetryMask) {
                                Retry = TRUE;
                            }
                        }

                        if (Retry == TRUE) {
                            status2 = STATUS_REFAULT;
                        }
                    }
                }

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);

#if DBG
                Process = PsGetCurrentProcess ();
#endif
                while (Page <= LastPage) {
                    Pfn1 = MI_PFN_ELEMENT (*Page);
                    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
                    Pfn1->u4.InPageError = 1;
                    Pfn1->u1.ReadStatus = status;

#if DBG
                    Va = (PULONG)MiMapPageInHyperSpaceAtDpc (Process, *Page);
                    RtlFillMemoryUlong (Va, PAGE_SIZE, 0x50444142);
                    MiUnmapPageInHyperSpaceFromDpc (Process, Va);
#endif
                    Page += 1;
                }

                return status2;
            }
        }
        else {

            MiFaultRetries = 0;
            MiUserFaultRetries = 0;

            if (InPageSupport->IoStatus.Information != Mdl->ByteCount) {

                ASSERT (InPageSupport->IoStatus.Information != 0);

                //
                // Less than a full page was read - zero the remainder
                // of the page.
                //

                Page = (PPFN_NUMBER)(Mdl + 1);
                LastPage = Page + ((Mdl->ByteCount - 1) >> PAGE_SHIFT);
                Page += ((InPageSupport->IoStatus.Information - 1) >> PAGE_SHIFT);

                Offset = BYTE_OFFSET (InPageSupport->IoStatus.Information);

                if (Offset != 0) {
                    Process = PsGetCurrentProcess ();
                    Va = (PULONG)((PCHAR)MiMapPageInHyperSpaceAtDpc (Process, *Page)
                                + Offset);

                    RtlZeroMemory (Va, PAGE_SIZE - Offset);
                    MiUnmapPageInHyperSpaceFromDpc (Process, Va);
                }

                //
                // Zero any remaining pages within the MDL.
                //

                Page += 1;

                while (Page <= LastPage) {
                    MiZeroPhysicalPage (*Page, 0);
                    Page += 1;
                }
            }

            //
            // If any filesystem return non-zeroed data for any slop
            // after the VDL but before the next 512-byte offset then this
            // non-zeroed data will overwrite our zeroed page.  This would
            // need to be checked for and cleaned up here.  Note that the only
            // reason Mm even rounds the MDL request up to a 512-byte offset
            // is so filesystems receive a transfer they can handle optimally,
            // but any transfer size has always worked (although non-512 byte
            // multiples end up getting posted by the filesystem).
            //
        }
    }

    //
    // Prefetcher-issued reads only put prototype PTEs into transition and
    // never fill actual hardware PTEs so these can be returned now.
    //

    if (CurrentProcess == PREFETCH_PROCESS) {
        return STATUS_SUCCESS;
    }

    //
    // Check to see if the faulting PTE has changed.
    //

    NewPointerPte = MiFindActualFaultingPte (FaultingAddress);

    //
    // If this PTE is in prototype PTE format, make the pointer to the
    // PTE point to the prototype PTE.
    //

    if (NewPointerPte == NULL) {
        return STATUS_PTE_CHANGED;
    }

    if (NewPointerPte != PointerPte) {

        //
        // Check to make sure the NewPointerPte is not a prototype PTE
        // which refers to the page being made valid.
        //

        if (NewPointerPte->u.Soft.Prototype == 1) {
            if (NewPointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

                ProtoPte = MiCheckVirtualAddress (FaultingAddress,
                                                  &Protection);

            }
            else {
                ProtoPte = MiPteToProto (NewPointerPte);
            }

            //
            // Make sure the prototype PTE refers to the PTE made valid.
            //

            if (ProtoPte != PointerPte) {
                return STATUS_PTE_CHANGED;
            }

            //
            // If the only difference is the owner mask, everything is okay.
            //

            if (ProtoPte->u.Long != PointerPteContents->u.Long) {
                return STATUS_PTE_CHANGED;
            }
        }
        else {
            return STATUS_PTE_CHANGED;
        }
    }
    else {

        if (NewPointerPte->u.Long != PointerPteContents->u.Long) {
            return STATUS_PTE_CHANGED;
        }
    }
    return STATUS_SUCCESS;
}

PMMPTE
MiFindActualFaultingPte (
    IN PVOID FaultingAddress
    )

/*++

Routine Description:

    This routine locates the actual PTE which must be made resident in order
    to complete this fault.  Note that for certain cases multiple faults
    are required to make the final page resident.

Arguments:

    FaultingAddress - Supplies the virtual address which caused the fault.

Return Value:

    The PTE to be made valid to finish the fault, NULL if the fault should
    be retried.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    PMMPTE ProtoPteAddress;
    PMMPTE PointerPte;
    PMMPTE PointerFaultingPte;
    ULONG Protection;

    if (MI_IS_PHYSICAL_ADDRESS(FaultingAddress)) {
        return NULL;
    }

#if (_MI_PAGING_LEVELS >= 4)

    PointerPte = MiGetPxeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page directory parent page is not valid.
        //

        return PointerPte;
    }

#endif

#if (_MI_PAGING_LEVELS >= 3)

    PointerPte = MiGetPpeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page directory page is not valid.
        //

        return PointerPte;
    }

#endif

    PointerPte = MiGetPdeAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 0) {

        //
        // Page table page is not valid.
        //

        return PointerPte;
    }

    PointerPte = MiGetPteAddress (FaultingAddress);

    if (PointerPte->u.Hard.Valid == 1) {

        //
        // Page is already valid, no need to fault it in.
        //

        return NULL;
    }

    if (PointerPte->u.Soft.Prototype == 0) {

        //
        // Page is not a prototype PTE, make this PTE valid.
        //

        return PointerPte;
    }

    //
    // Check to see if the PTE which maps the prototype PTE is valid.
    //

    if (PointerPte->u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {

        //
        // Protection is here, PTE must be located in VAD.
        //

        ProtoPteAddress = MiCheckVirtualAddress (FaultingAddress,
                                                 &Protection);

        if (ProtoPteAddress == NULL) {

            //
            // No prototype PTE means another thread has deleted the VAD while
            // this thread waited for the inpage to complete.  Certainly NULL
            // must be returned so a stale PTE is not modified - the instruction
            // will then be reexecuted and an access violation delivered.
            //

            return NULL;
        }

    }
    else {

        //
        // Protection is in ProtoPte.
        //

        ProtoPteAddress = MiPteToProto (PointerPte);
    }

    PointerFaultingPte = MiFindActualFaultingPte (ProtoPteAddress);

    if (PointerFaultingPte == (PMMPTE)NULL) {
        return PointerPte;
    }

    return PointerFaultingPte;
}

PMMPTE
MiCheckVirtualAddress (
    IN PVOID VirtualAddress,
    OUT PULONG ProtectCode
    )

/*++

Routine Description:

    This function examines the virtual address descriptors to see
    if the specified virtual address is contained within any of
    the descriptors.  If a virtual address descriptor is found
    which contains the specified virtual address, a PTE is built
    from information within the virtual address descriptor and
    returned to the caller.

Arguments:

    VirtualAddress - Supplies the virtual address to locate within
                     a virtual address descriptor.

Return Value:

    Returns the PTE which corresponds to the supplied virtual address.
    If no virtual address descriptor is found, a zero PTE is returned.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    PMMVAD Vad;
    PMMPTE PointerPte;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;

    if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {

#if defined(MM_SHARED_USER_DATA_VA)

        if (PAGE_ALIGN(VirtualAddress) == (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // This is the page that is double mapped between
            // user mode and kernel mode.  Map in as read only.
            // On MIPS this is hardwired in the TB.
            //

            *ProtectCode = MM_READONLY;

#if defined(_X86PAE_)

            if (MmPaeMask != 0) {

                //
                // For some 32 bit architectures, the fast system call
                // instruction sequence lives in this page hence we must
                // ensure it is executable.
                //

                *ProtectCode = MM_EXECUTE_READ;
            }

#endif

            return MmSharedUserDataPte;
        }

#endif

        Vad = MiLocateAddress (VirtualAddress);
        if (Vad == (PMMVAD)NULL) {

            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        //
        // A virtual address descriptor which contains the virtual address
        // has been located.  Build the PTE from the information within
        // the virtual address descriptor.
        //

#ifdef LARGE_PAGES

        if (Vad->u.VadFlags.LargePages == 1) {

            KIRQL OldIrql;
            PSUBSECTION Subsection;

            //
            // The first prototype PTE points to the subsection for the
            // large page mapping.

            Subsection = (PSUBSECTION)Vad->FirstPrototypePte;

            ASSERT (Subsection->u.SubsectionFlags.LargePages == 1);

            KeRaiseIrql (DISPATCH_LEVEL, &OldIrql);
            KeFlushEntireTb (TRUE, TRUE);
            KeFillLargeEntryTb ((PHARDWARE_PTE)(Subsection + 1),
                                 VirtualAddress,
                                 Subsection->StartingSector);

            KeLowerIrql (OldIrql);
            *ProtectCode = MM_LARGE_PAGES;
            return NULL;
        }
#endif //LARGE_PAGES

        if (Vad->u.VadFlags.PhysicalMapping == 1) {

#if defined(_IA64_)

            //
            // This is a banked section for all platforms except IA64.  This
            // is because only IA64 (in the MmX86Fault handler for 32-bit apps)
            // calls this routine without first checking for a valid PTE and
            // just returning.
            //

            if (((PMMVAD_LONG)Vad)->u4.Banked == NULL) {

                //
                // This is a physical (non-banked) section which is allowed to
                // take a TB miss, but never a legitimate call to this routine
                // because the corresponding PPE/PDE/PTE must always be valid.
                //

                ASSERT (MiGetPpeAddress(VirtualAddress)->u.Hard.Valid == 1);
                ASSERT (MiGetPdeAddress(VirtualAddress)->u.Hard.Valid == 1);

                PointerPte = MiGetPteAddress(VirtualAddress);
                ASSERT (PointerPte->u.Hard.Valid == 1);

                KeFillEntryTb ((PHARDWARE_PTE)PointerPte, VirtualAddress, FALSE);
                *ProtectCode = MM_NOACCESS;
                return NULL;
            }

#endif

            //
            // This is definitely a banked section.
            //

            MiHandleBankedSection (VirtualAddress, Vad);
            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        if (Vad->u.VadFlags.PrivateMemory == 1) {

            //
            // This is a private region of memory.  Check to make
            // sure the virtual address has been committed.  Note that
            // addresses are dense from the bottom up.
            //

            if (Vad->u.VadFlags.UserPhysicalPages == 1) {

                //
                // These mappings only fault if the access is bad.
                //

#if 0
                //
                // Note the PTE can only be checked if the PXE, PPE and PDE are
                // all valid, so just comment out the assert for now.
                //

                ASSERT (MiGetPteAddress(VirtualAddress)->u.Long == ZeroPte.u.Long);
#endif
                *ProtectCode = MM_NOACCESS;
                return NULL;
            }

            if (Vad->u.VadFlags.MemCommit == 1) {
                *ProtectCode = MI_GET_PROTECTION_FROM_VAD(Vad);
                return NULL;
            }

            //
            // The address is reserved but not committed.
            //

            *ProtectCode = MM_NOACCESS;
            return NULL;

        }
        else {

            //
            // This virtual address descriptor refers to a
            // section, calculate the address of the prototype PTE
            // and construct a pointer to the PTE.
            //
            //*******************************************************
            //*******************************************************
            // well here's an interesting problem, how do we know
            // how to set the attributes on the PTE we are creating
            // when we can't look at the prototype PTE without
            // potentially incurring a page fault.  In this case
            // PteTemplate would be zero.
            //*******************************************************
            //*******************************************************
            //

            if (Vad->u.VadFlags.ImageMap == 1) {

                //
                // PTE and proto PTEs have the same protection for images.
                //

                *ProtectCode = MM_UNKNOWN_PROTECTION;
            }
            else {
                *ProtectCode = MI_GET_PROTECTION_FROM_VAD(Vad);
            }
            PointerPte = (PMMPTE)MiGetProtoPteAddress(Vad,
                                                MI_VA_TO_VPN (VirtualAddress));
            if (PointerPte == NULL) {
                *ProtectCode = MM_NOACCESS;
            }
            if (Vad->u2.VadFlags2.ExtendableFile) {

                //
                // Make sure the data has been committed.
                //

                if ((MI_VA_TO_VPN (VirtualAddress) - Vad->StartingVpn) >
                    (ULONG_PTR)((((PMMVAD_LONG)Vad)->u4.ExtendedInfo->CommittedSize - 1)
                                                 >> PAGE_SHIFT)) {
                    *ProtectCode = MM_NOACCESS;
                }
            }
            return PointerPte;
        }

    }
    else if (MI_IS_PAGE_TABLE_ADDRESS(VirtualAddress)) {

        //
        // The virtual address is within the space occupied by PDEs,
        // make the PDE valid.
        //

        if (((PMMPTE)VirtualAddress >= MiGetPteAddress (MM_PAGED_POOL_START)) &&
            ((PMMPTE)VirtualAddress <= MmPagedPoolInfo.LastPteForPagedPool)) {

            *ProtectCode = MM_NOACCESS;
            return NULL;
        }

        *ProtectCode = MM_READWRITE;
        return NULL;
    }
    else if (MI_IS_SESSION_ADDRESS (VirtualAddress) == TRUE) {

        //
        // See if the session space address is copy on write.
        //

        MM_SESSION_SPACE_WS_LOCK_ASSERT ();

        PointerPte = NULL;
        *ProtectCode = MM_NOACCESS;

        NextEntry = MmSessionSpace->ImageList.Flink;

        while (NextEntry != &MmSessionSpace->ImageList) {

            Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

            if ((VirtualAddress >= Image->Address) && (VirtualAddress <= Image->LastAddress)) {
                PointerPte = Image->PrototypePtes +
                    (((PCHAR)VirtualAddress - (PCHAR)Image->Address) >> PAGE_SHIFT);
                *ProtectCode = MM_EXECUTE_WRITECOPY;
                break;
            }

            NextEntry = NextEntry->Flink;
        }

        return PointerPte;
    }

    //
    // Address is in system space.
    //

    *ProtectCode = MM_NOACCESS;
    return NULL;
}

#if (_MI_PAGING_LEVELS < 3)

NTSTATUS
FASTCALL
MiCheckPdeForPagedPool (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function copies the Page Table Entry for the corresponding
    virtual address from the system process's page directory.

    This allows page table pages to be lazily evaluated for things
    like paged pool and per-session mappings.

Arguments:

    VirtualAddress - Supplies the virtual address in question.

Return Value:

    Either success or access violation.

Environment:

    Kernel mode, DISPATCH level or below.

--*/
{
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    NTSTATUS status;

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) == TRUE) {

         //
         // Virtual address in the session space range.
         //

         return MiCheckPdeForSessionSpace (VirtualAddress);
    }

    if (MI_IS_SESSION_PTE (VirtualAddress) == TRUE) {

         //
         // PTE for the session space range.
         //

         return MiCheckPdeForSessionSpace (VirtualAddress);
    }

    status = STATUS_SUCCESS;

    if (MI_IS_KERNEL_PAGE_TABLE_ADDRESS(VirtualAddress)) {

        //
        // PTE for paged pool.
        //

        PointerPde = MiGetPteAddress (VirtualAddress);
        status = STATUS_WAIT_1;
    }
    else if (VirtualAddress < MmSystemRangeStart) {

        return STATUS_ACCESS_VIOLATION;

    }
    else {

        //
        // Virtual address in paged pool range.
        //

        PointerPde = MiGetPdeAddress (VirtualAddress);
    }

    //
    // Locate the PDE for this page and make it valid.
    //

    if (PointerPde->u.Hard.Valid == 0) {
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        MI_WRITE_VALID_PTE (PointerPde,
                            MmSystemPagePtes [((ULONG_PTR)PointerPde &
                               (PD_PER_SYSTEM * (sizeof(MMPTE) * PDE_PER_PAGE) - 1)) / sizeof(MMPTE)]);
        KeFillEntryTb ((PHARDWARE_PTE)PointerPde, PointerPte, FALSE);
    }
    return status;
}


NTSTATUS
FASTCALL
MiCheckPdeForSessionSpace(
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function copies the Page Table Entry for the corresponding
    session virtual address from the current session's data structures.

    This allows page table pages to be lazily evaluated for session mappings.
    The caller must check for the current process having a session space.

Arguments:

    VirtualAddress - Supplies the virtual address in question.

Return Value:

    STATUS_WAIT_1 - The mapping has been made valid, retry the fault.

    STATUS_SUCCESS - Did not handle the fault, continue further processing.

    !STATUS_SUCCESS - An access violation has occurred - raise an exception.

Environment:

    Kernel mode, DISPATCH level or below.

--*/

{
    PMMPTE PointerPde;
    PVOID  SessionVirtualAddress;
    ULONG  Index;

    //
    // First check whether the reference was to a page table page which maps
    // session space.  If so, the PDE is retrieved from the session space
    // data structure and made valid.
    //

    if (MI_IS_SESSION_PTE (VirtualAddress) == TRUE) {

        //
        // Verify that the current process has a session space.
        //

        PointerPde = MiGetPdeAddress (MmSessionSpace);

        if (PointerPde->u.Hard.Valid == 0) {

#if DBG
            DbgPrint("MiCheckPdeForSessionSpace: No current session for PTE %p\n",
                VirtualAddress);

            DbgBreakPoint();
#endif
            return STATUS_ACCESS_VIOLATION;
        }

        SessionVirtualAddress = MiGetVirtualAddressMappedByPte ((PMMPTE) VirtualAddress);

        PointerPde = MiGetPteAddress (VirtualAddress);

        if (PointerPde->u.Hard.Valid == 1) {

            //
            // The PDE is already valid - another thread must have
            // won the race.  Just return.
            //

            return STATUS_WAIT_1;
        }

        //
        // Calculate the session space PDE index and load the
        // PDE from the session space table for this session.
        //

        Index = MiGetPdeSessionIndex (SessionVirtualAddress);

        PointerPde->u.Long = MmSessionSpace->PageTables[Index].u.Long;

        if (PointerPde->u.Hard.Valid == 1) {
            KeFillEntryTb ((PHARDWARE_PTE)PointerPde, VirtualAddress, FALSE);
            return STATUS_WAIT_1;
        }

#if DBG
        DbgPrint("MiCheckPdeForSessionSpace: No Session PDE for PTE %p, %p\n",
            PointerPde->u.Long, SessionVirtualAddress);

        DbgBreakPoint();
#endif
        return STATUS_ACCESS_VIOLATION;
    }

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) == FALSE) {

        //
        // Not a session space fault - tell the caller to try other handlers.
        //

        return STATUS_SUCCESS;
    }

    //
    // Handle PDE faults for references in the session space.
    // Verify that the current process has a session space.
    //

    PointerPde = MiGetPdeAddress (MmSessionSpace);

    if (PointerPde->u.Hard.Valid == 0) {

#if DBG
        DbgPrint("MiCheckPdeForSessionSpace: No current session for VA %p\n",
            VirtualAddress);

        DbgBreakPoint();
#endif
        return STATUS_ACCESS_VIOLATION;
    }

    PointerPde = MiGetPdeAddress (VirtualAddress);

    if (PointerPde->u.Hard.Valid == 0) {

        //
        // Calculate the session space PDE index and load the
        // PDE from the session space table for this session.
        //

        Index = MiGetPdeSessionIndex (VirtualAddress);

        PointerPde->u.Long = MmSessionSpace->PageTables[Index].u.Long;

        if (PointerPde->u.Hard.Valid == 1) {

            KeFillEntryTb ((PHARDWARE_PTE)PointerPde,
                           MiGetPteAddress(VirtualAddress),
                           FALSE);

            return STATUS_WAIT_1;
        }

#if DBG
        DbgPrint("MiCheckPdeForSessionSpace: No Session PDE for VA %p, %p\n",
            PointerPde->u.Long, VirtualAddress);

        DbgBreakPoint();
#endif

        return STATUS_ACCESS_VIOLATION;
    }

    //
    // Tell the caller to continue with other fault handlers.
    //

    return STATUS_SUCCESS;
}
#endif


VOID
MiInitializePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN ULONG ModifiedState
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state.

Arguments:

    PageFrameIndex - Supplies the page frame number to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    ModifiedState - Supplies the state to set the modified field in the PFN
                    element for this page, either 0 or 1.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;

    //
    // If the PTE is currently valid, an address space is being built,
    // just make the original PTE demand zero.
    //

    if (PointerPte->u.Hard.Valid == 1) {
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

#if defined(_X86PAE_)
        if (MmPaeMask != 0) {
            if ((PointerPte->u.Long & MmPaeMask) == 0) {
                Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
            }
        }
#endif

#if defined(_IA64_)
        if (PointerPte->u.Hard.Execute == 1) {
            Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
        }
#endif

        if (MI_IS_CACHING_DISABLED (PointerPte)) {
            Pfn1->OriginalPte.u.Soft.Protection = MM_READWRITE | MM_NOCACHE;
        }

    }
    else {
        Pfn1->OriginalPte = *PointerPte;
        ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                    (Pfn1->OriginalPte.u.Soft.Transition == 1)));
    }

    Pfn1->u3.e2.ReferenceCount += 1;

#if DBG
    if (Pfn1->u3.e2.ReferenceCount > 1) {
        DbgPrint("MM:incrementing ref count > 1 \n");
        MiFormatPfn(Pfn1);
        MiFormatPte(PointerPte);
    }
#endif

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;
    Pfn1->u3.e1.CacheAttribute = MiCached;

    if (ModifiedState == 1) {
        MI_SET_MODIFIED (Pfn1, 1, 0xB);
    }
    else {
        MI_SET_MODIFIED (Pfn1, 0, 0x26);
    }

#if defined (_WIN64)
    Pfn1->UsedPageTableEntries = 0;
#endif

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress(PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }
    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    ASSERT (PteFramePage != 0);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Increment the share count for the page table page containing
    // this PTE.
    //

    Pfn2 = MI_PFN_ELEMENT (PteFramePage);

    Pfn2->u2.ShareCount += 1;

    return;
}

VOID
MiInitializeReadInProgressSinglePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition / read-in-progress state for an in-page operation.

Arguments:

    PageFrameIndex - Supplies the page frame to initialize.

    BasePte - Supplies the pointer to the PTE for the page frame.

    Event - Supplies the event which is to be set when the I/O operation
            completes.

    WorkingSetIndex - Supplies the working set index flag, a value of
                      -1 indicates no WSLE is required because
                      this is a prototype PTE.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->u1.Event = Event;
    Pfn1->PteAddress = BasePte;
    Pfn1->OriginalPte = *BasePte;
    if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {
        Pfn1->u3.e1.PrototypePte = 1;
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 10);
    Pfn1->u3.e2.ReferenceCount += 1;

    Pfn1->u2.ShareCount = 0;
    Pfn1->u3.e1.ReadInProgress = 1;
    Pfn1->u3.e1.CacheAttribute = MiCached;
    Pfn1->u4.InPageError = 0;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress(BasePte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (BasePte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)BasePte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(BasePte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Put the PTE into the transition state, no cache flush needed as
    // PTE is still not valid.
    //

    MI_MAKE_TRANSITION_PTE (TempPte,
                            PageFrameIndex,
                            BasePte->u.Soft.Protection,
                            BasePte);

    MI_WRITE_INVALID_PTE (BasePte, TempPte);

    //
    // Increment the share count for the page table page containing
    // this PTE as the PTE just went into the transition state.
    //

    ASSERT (PteFramePage != 0);

    Pfn1 = MI_PFN_ELEMENT (PteFramePage);
    Pfn1->u2.ShareCount += 1;

    return;
}

VOID
MiInitializeReadInProgressPfn (
    IN PMDL Mdl,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition / read-in-progress state for an in-page operation.


Arguments:

    Mdl - Supplies a pointer to the MDL.

    BasePte - Supplies the pointer to the PTE which the first page in
              the MDL maps.

    Event - Supplies the event which is to be set when the I/O operation
            completes.

    WorkingSetIndex - Supplies the working set index flag, a value of
                      -1 indicates no WSLE is required because
                      this is a prototype PTE.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;
    LONG NumberOfBytes;
    PPFN_NUMBER Page;

    MM_PFN_LOCK_ASSERT();

    Page = (PPFN_NUMBER)(Mdl + 1);

    NumberOfBytes = Mdl->ByteCount;

    while (NumberOfBytes > 0) {

        Pfn1 = MI_PFN_ELEMENT (*Page);
        Pfn1->u1.Event = Event;
        Pfn1->PteAddress = BasePte;
        Pfn1->OriginalPte = *BasePte;
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        if (WorkingSetIndex == MI_PROTOTYPE_WSINDEX) {
            Pfn1->u3.e1.PrototypePte = 1;
        }

        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 10);
        Pfn1->u3.e2.ReferenceCount += 1;

        Pfn1->u2.ShareCount = 0;
        Pfn1->u3.e1.ReadInProgress = 1;
        Pfn1->u3.e1.CacheAttribute = MiCached;
        Pfn1->u4.InPageError = 0;

        //
        // Determine the page frame number of the page table page which
        // contains this PTE.
        //

        PteFramePointer = MiGetPteAddress(BasePte);
        if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
            if (!NT_SUCCESS(MiCheckPdeForPagedPool (BasePte))) {
#endif
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x61940,
                              (ULONG_PTR)BasePte,
                              (ULONG_PTR)PteFramePointer->u.Long,
                              (ULONG_PTR)MiGetVirtualAddressMappedByPte(BasePte));
#if (_MI_PAGING_LEVELS < 3)
            }
#endif
        }

        PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
        Pfn1->u4.PteFrame = PteFramePage;

        //
        // Put the PTE into the transition state, no cache flush needed as
        // PTE is still not valid.
        //

        MI_MAKE_TRANSITION_PTE (TempPte,
                                *Page,
                                BasePte->u.Soft.Protection,
                                BasePte);
        MI_WRITE_INVALID_PTE (BasePte, TempPte);

        //
        // Increment the share count for the page table page containing
        // this PTE as the PTE just went into the transition state.
        //

        ASSERT (PteFramePage != 0);
        Pfn2 = MI_PFN_ELEMENT (PteFramePage);
        Pfn2->u2.ShareCount += 1;

        NumberOfBytes -= PAGE_SIZE;
        Page += 1;
        BasePte += 1;
    }

    return;
}

VOID
MiInitializeTransitionPfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    transition state.  Main use is by MapImageFile to make the
    page which contains the image header transition in the
    prototype PTEs.

Arguments:

    PageFrameIndex - Supplies the page frame index to be initialized.

    PointerPte - Supplies an invalid, non-transition PTE to initialize.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    MMPTE TempPte;

    MM_PFN_LOCK_ASSERT();
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->u1.Event = NULL;
    Pfn1->PteAddress = PointerPte;
    Pfn1->OriginalPte = *PointerPte;
    ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
              (Pfn1->OriginalPte.u.Soft.Transition == 1)));

    //
    // Don't change the reference count (it should already be 1).
    //

    Pfn1->u2.ShareCount = 0;

    //
    // No WSLE is required because this is a prototype PTE.
    //

    Pfn1->u3.e1.PrototypePte = 1;

    Pfn1->u3.e1.PageLocation = TransitionPage;
    Pfn1->u3.e1.CacheAttribute = MiCached;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress(PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Put the PTE into the transition state, no cache flush needed as
    // PTE is still not valid.
    //

    MI_MAKE_TRANSITION_PTE (TempPte,
                            PageFrameIndex,
                            PointerPte->u.Soft.Protection,
                            PointerPte);

    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

    //
    // Increment the share count for the page table page containing
    // this PTE as the PTE just went into the transition state.
    //

    Pfn2 = MI_PFN_ELEMENT (PteFramePage);
    ASSERT (PteFramePage != 0);
    Pfn2->u2.ShareCount += 1;

    return;
}

VOID
MiInitializeCopyOnWritePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN WSLE_NUMBER WorkingSetIndex,
    IN PVOID SessionPointer
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state for a copy on write operation.

    In this case the page table page which contains the PTE has
    the proper ShareCount.

Arguments:

    PageFrameIndex - Supplies the page frame number to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    WorkingSetIndex - Supplies the working set index for the corresponding
                      virtual address.

    SessionPointer - Supplies the session space pointer if this fault is for
                     a session space page or NULL if this is for a user page.


Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PteFramePointer;
    PFN_NUMBER PteFramePage;
    PVOID VirtualAddress;
    PMM_SESSION_SPACE SessionSpace;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;

    //
    // Get the protection for the page.
    //

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    Pfn1->OriginalPte.u.Long = 0;

    if (SessionPointer) {
        Pfn1->OriginalPte.u.Soft.Protection = MM_EXECUTE_READWRITE;
        SessionSpace = (PMM_SESSION_SPACE) SessionPointer;
        SessionSpace->Wsle[WorkingSetIndex].u1.e1.Protection =
            MM_EXECUTE_READWRITE;
    }
    else {
        Pfn1->OriginalPte.u.Soft.Protection =
                MI_MAKE_PROTECT_NOT_WRITE_COPY (
                                    MmWsle[WorkingSetIndex].u1.e1.Protection);
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount += 1;
    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;
    Pfn1->u3.e1.CacheAttribute = MiCached;
    Pfn1->u1.WsIndex = WorkingSetIndex;

    //
    // Determine the page frame number of the page table page which
    // contains this PTE.
    //

    PteFramePointer = MiGetPteAddress(PointerPte);
    if (PteFramePointer->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteFramePointer->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PteFramePage = MI_GET_PAGE_FRAME_FROM_PTE (PteFramePointer);
    ASSERT (PteFramePage != 0);

    Pfn1->u4.PteFrame = PteFramePage;

    //
    // Set the modified flag in the PFN database as we are writing
    // into this page and the dirty bit is already set in the PTE.
    //

    MI_SET_MODIFIED (Pfn1, 1, 0xC);

    return;
}

BOOLEAN
MmIsAddressValid (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    For a given virtual address this function returns TRUE if no page fault
    will occur for a read operation on the address, FALSE otherwise.

    Note that after this routine was called, if appropriate locks are not
    held, a non-faulting address could fault.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if no page fault would be generated reading the virtual address,
    FALSE otherwise.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;
#if defined(_IA64_)
    ULONG Region;

    Region = (ULONG)(((ULONG_PTR) VirtualAddress & VRN_MASK) >> 61);

    if ((Region == 0) || (Region == 1) || (Region == 4) || (Region == 7)) {
        NOTHING;
    }
    else {
        return FALSE;
    }

    if (MiIsVirtualAddressMappedByTr (VirtualAddress) == TRUE) {
        return TRUE;
    }

    if (MiMappingsInitialized == FALSE) {
        return FALSE;
    }
#endif

#if defined (_AMD64_)

    //
    // If this is within the physical addressing range, just return TRUE.
    //

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {

        PFN_NUMBER PageFrameIndex;

        //
        // Only bound with MmHighestPhysicalPage once Mm has initialized.
        //

        if (MmHighestPhysicalPage != 0) {

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN(VirtualAddress);

            if (PageFrameIndex > MmHighestPhysicalPage) {
                return FALSE;
            }
        }

        return TRUE;
    }

#endif

    //
    // If the address is not canonical then return FALSE as the caller (which
    // may be the kernel debugger) is not expecting to get an unimplemented
    // address bit fault.
    //

    if (MI_RESERVED_BITS_CANONICAL(VirtualAddress) == FALSE) {
        return FALSE;
    }

#if (_MI_PAGING_LEVELS >= 4)
    PointerPte = MiGetPxeAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }
#endif

#if (_MI_PAGING_LEVELS >= 3)
    PointerPte = MiGetPpeAddress (VirtualAddress);

    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }
#endif

    PointerPte = MiGetPdeAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }
#ifdef _X86_
    if (PointerPte->u.Hard.LargePage == 1) {
        return TRUE;
    }
#endif //_X86_

    PointerPte = MiGetPteAddress (VirtualAddress);
    if (PointerPte->u.Hard.Valid == 0) {
        return FALSE;
    }

#ifdef _X86_
    //
    // Make sure we're not treating a page directory as a page table here for
    // the case where the page directory is mapping a large page.  This is
    // because the large page bit is valid in PDE formats, but reserved in
    // PTE formats and will cause a trap.  A virtual address like c0200000
    // triggers this case.  It's not enough to just check the large page bit
    // in the PTE below because of course that bit's been reused by other
    // steppings of the processor so we have to look at the address too.
    //
    if (PointerPte->u.Hard.LargePage == 1) {
        PVOID Va;

        Va = MiGetVirtualAddressMappedByPde (PointerPte);
        if (MI_IS_PHYSICAL_ADDRESS(Va)) {
            return FALSE;
        }
    }
#endif

#if defined(_IA64_)
    if (MI_GET_ACCESSED_IN_PTE (PointerPte) == 0) {

        //
        // Even though the address is valid, the access bit is off so a
        // reference would cause a fault so return FALSE.  We may want to
        // rethink this later to instead update the PTE accessed bit if the
        // PFN lock and relevant working set mutex are not currently held.
        //

        return FALSE;
    }
#endif

    return TRUE;
}

VOID
MiInitializePfnForOtherProcess (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN PFN_NUMBER ContainingPageFrame
    )

/*++

Routine Description:

    This function initializes the specified PFN element to the
    active and valid state with the dirty bit on in the PTE and
    the PFN database marked as modified.

    As this PTE is not visible from the current process, the containing
    page frame must be supplied at the PTE contents field for the
    PFN database element are set to demand zero.

Arguments:

    PageFrameIndex - Supplies the page frame number of which to initialize.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.

    ContainingPageFrame - Supplies the page frame number of the page
                          table page which contains this PTE.
                          If the ContainingPageFrame is 0, then
                          the ShareCount for the
                          containing page is not incremented.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn1->PteAddress = PointerPte;
    Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    Pfn1->u3.e2.ReferenceCount += 1;

#if DBG
    if (Pfn1->u3.e2.ReferenceCount > 1) {
        DbgPrint("MM:incrementing ref count > 1 \n");
        MiFormatPfn(Pfn1);
        MiFormatPte(PointerPte);
    }
#endif

    Pfn1->u2.ShareCount += 1;
    Pfn1->u3.e1.PageLocation = ActiveAndValid;

    //
    // Set the page attribute to cached even though it isn't really mapped
    // into a TB entry yet - it will be when the I/O completes and in the
    // future, may get paged in and out multiple times and will be marked
    // as cached in those transactions also.  If in fact the driver stack
    // wants to map it some other way for the transfer, the correct mapping
    // will get used regardless.
    //

    Pfn1->u3.e1.CacheAttribute = MiCached;

    MI_SET_MODIFIED (Pfn1, 1, 0xD);

    Pfn1->u4.InPageError = 0;

    //
    // Increment the share count for the page table page containing
    // this PTE.
    //

    if (ContainingPageFrame != 0) {
        Pfn1->u4.PteFrame = ContainingPageFrame;
        Pfn2 = MI_PFN_ELEMENT (ContainingPageFrame);
        Pfn2->u2.ShareCount += 1;
    }
    return;
}

VOID
MiAddValidPageToWorkingSet (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG WsleMask
    )

/*++

Routine Description:

    This routine adds the specified virtual address into the
    appropriate working set list.

Arguments:

    VirtualAddress - Supplies the address to add to the working set list.

    PointerPte - Supplies a pointer to the PTE that is now valid.

    Pfn1 - Supplies the PFN database element for the physical page
           mapped by the virtual address.

    WsleMask - Supplies a mask (protection and flags) to OR into the
               working set list entry.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, working set lock.  PFN lock NOT held.

--*/

{
    WSLE_NUMBER WorkingSetIndex;
    PEPROCESS Process;
    PMMSUPPORT WsInfo;
    PMMWSLE Wsle;

#if !DBG
    UNREFERENCED_PARAMETER (PointerPte);
#endif

    ASSERT (MI_IS_PAGE_TABLE_ADDRESS(PointerPte));
    ASSERT (PointerPte->u.Hard.Valid == 1);

    if (MI_IS_SESSION_ADDRESS (VirtualAddress) || MI_IS_SESSION_PTE (VirtualAddress)) {
        //
        // Current process's session space working set.
        //

        WsInfo = &MmSessionSpace->Vm;
        Wsle = MmSessionSpace->Wsle;
    }
    else if (MI_IS_PROCESS_SPACE_ADDRESS(VirtualAddress)) {

        //
        // Per process working set.
        //

        Process = PsGetCurrentProcess();
        WsInfo = &Process->Vm;
        Wsle = MmWsle;

        PERFINFO_ADDTOWS(Pfn1, VirtualAddress, Process->UniqueProcessId)
    }
    else {

        //
        // System cache working set.
        //

        WsInfo = &MmSystemCacheWs;
        Wsle = MmSystemCacheWsle;

        PERFINFO_ADDTOWS(Pfn1, VirtualAddress, (HANDLE) -1);
    }

    WorkingSetIndex = MiLocateAndReserveWsle (WsInfo);
    MiUpdateWsle (&WorkingSetIndex,
                  VirtualAddress,
                  WsInfo->VmWorkingSetList,
                  Pfn1);
    Wsle[WorkingSetIndex].u1.Long |= WsleMask;

#if DBG
    if (MI_IS_SYSTEM_CACHE_ADDRESS(VirtualAddress)) {
        ASSERT (MmSystemCacheWsle[WorkingSetIndex].u1.e1.SameProtectAsProto);
    }
#endif //DBG

    MI_SET_PTE_IN_WORKING_SET (PointerPte, WorkingSetIndex);

    KeFillEntryTb ((PHARDWARE_PTE)PointerPte, VirtualAddress, FALSE);
    return;
}

PMMINPAGE_SUPPORT
MiGetInPageSupportBlock (
    IN LOGICAL PfnHeld,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine acquires an inpage support block.  If none are available,
    the PFN lock will be released and reacquired to add an entry to the list.
    NULL will then be returned.

Arguments:

    PfnHeld - Supplies TRUE if the caller holds the PFN lock, FALSE if not.

    Process - Supplies context if the working set mutex needs to be released
              and reacquired.

Return Value:

    A non-null pointer to an inpage block if one is already available.
    The PFN lock is not released in this path.

    NULL is returned if no inpage blocks were available.  In this path, the
    PFN lock is released and an entry is added - but NULL is still returned
    so the caller is aware that the state has changed due to the lock release
    and reacquisition.

Environment:

    Kernel mode, PFN lock may optionally be held.

--*/

{
    KIRQL OldIrql;
    KIRQL Ignore;
    ULONG Relock;
    LOGICAL WsHeldSafe;
    PMMINPAGE_SUPPORT Support;
    PSINGLE_LIST_ENTRY SingleListEntry;

#if DBG
    if (PfnHeld == TRUE) {
        MM_PFN_LOCK_ASSERT();
    }
    else {
        ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    }
#endif

    if (ExQueryDepthSList (&MmInPageSupportSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmInPageSupportSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         MMINPAGE_SUPPORT,
                                         ListEntry);

returnok:
            ASSERT (Support->WaitCount == 1);
            ASSERT (Support->u1.e1.PrefetchMdlHighBits == 0);
            ASSERT (Support->u1.LongFlags == 0);
            ASSERT (KeReadStateEvent (&Support->Event) == 0);
            ASSERT64 (Support->UsedPageTableEntries == 0);

            Support->Thread = PsGetCurrentThread();
#if DBG
            Support->ListEntry.Next = NULL;
#endif

            return Support;
        }
    }

    if (PfnHeld == TRUE) {
        UNLOCK_PFN (APC_LEVEL);
    }

    Support = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(MMINPAGE_SUPPORT),
                                     'nImM');

    if (Support != NULL) {

        KeInitializeEvent (&Support->Event, NotificationEvent, FALSE);

        Support->WaitCount = 1;
        Support->u1.LongFlags = 0;
        ASSERT (Support->u1.PrefetchMdl == NULL);
        ASSERT (KeReadStateEvent (&Support->Event) == 0);

#if defined (_WIN64)
        Support->UsedPageTableEntries = 0;
#endif
#if DBG
        Support->Thread = NULL;
#endif

        if (PfnHeld == FALSE) {
            goto returnok;
        }

        InterlockedPushEntrySList (&MmInPageSupportSListHead,
                                   (PSINGLE_LIST_ENTRY)&Support->ListEntry);

    }
    else {

        //
        // Initializing WsHeldSafe is not needed for
        // correctness but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        WsHeldSafe = FALSE;

        //
        // No pool is available - don't let a high priority thread consume
        // the machine in a continuous refault stream.  This delay allows
        // other system threads to run which will try to free up more pool.
        // Release the relevant working set mutex (if any) so the current
        // process can be trimmed for pages also.
        //

        Relock = FALSE;

        if (Process == HYDRA_PROCESS) {
            UNLOCK_SESSION_SPACE_WS (APC_LEVEL);
        }
        else if (Process == PREFETCH_PROCESS) {

            //
            // No mutex is held in this instance.
            //

            NOTHING;
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Handle both cases here and below.
            //

            UNLOCK_WS_REGARDLESS (Process, WsHeldSafe);
        }
        else {
            if (MmSystemLockOwner == PsGetCurrentThread()) {
                UNLOCK_SYSTEM_WS (APC_LEVEL);
                Relock = TRUE;
            }
            else {
            }
        }

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        if (Process == HYDRA_PROCESS) {
            LOCK_SESSION_SPACE_WS (Ignore, PsGetCurrentThread ());
        }
        else if (Process == PREFETCH_PROCESS) {
            NOTHING;
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Reacquire it in the same manner our caller did.
            //

            LOCK_WS_REGARDLESS (Process, WsHeldSafe);
        }
        else {
            if (Relock) {
                LOCK_SYSTEM_WS (Ignore, PsGetCurrentThread ());
            }
        }
    }

    if (PfnHeld == TRUE) {
        LOCK_PFN (OldIrql);
    }

    return NULL;

}

VOID
MiFreeInPageSupportBlock (
    IN PMMINPAGE_SUPPORT Support
    )

/*++

Routine Description:

    This routine returns the inpage support block to a list of freed blocks.

Arguments:

    Support - Supplies the inpage support block to put on the free list.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    ASSERT (Support->Thread != NULL);
    ASSERT (Support->WaitCount != 0);

    ASSERT ((Support->ListEntry.Next == NULL) ||
            (Support->u1.e1.PrefetchMdlHighBits != 0));

    //
    // An interlock is needed for the WaitCount decrement as an inpage support
    // block can be simultaneously freed by any number of threads.
    //
    // Careful synchronization is applied to the WaitCount field so
    // that freeing of the inpage block can occur lock-free.  Note
    // that the ReadInProgress bit on each PFN is set and cleared while
    // holding the PFN lock.  Inpage blocks are always (and must be)
    // freed _AFTER_ the ReadInProgress bit is cleared.
    //

    if (InterlockedDecrement (&Support->WaitCount) == 0) {

        if (Support->u1.e1.PrefetchMdlHighBits != 0) {
            PMDL Mdl;
            Mdl = MI_EXTRACT_PREFETCH_MDL (Support);
            if (Mdl != &Support->Mdl) {
                ExFreePool (Mdl);
            }
        }

        if (ExQueryDepthSList (&MmInPageSupportSListHead) < MmInPageSupportMinimum) {
            Support->WaitCount = 1;
            Support->u1.LongFlags = 0;
            KeClearEvent (&Support->Event);
#if defined (_WIN64)
            Support->UsedPageTableEntries = 0;
#endif
#if DBG
            Support->Thread = NULL;
#endif

            InterlockedPushEntrySList (&MmInPageSupportSListHead,
                                       (PSINGLE_LIST_ENTRY)&Support->ListEntry);
            return;
        }
        ExFreePool (Support);
    }

    return;
}

VOID
MiHandleBankedSection (
    IN PVOID VirtualAddress,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This routine invalidates a bank of video memory, calls out to the
    video driver and then enables the next bank of video memory.

Arguments:

    VirtualAddress - Supplies the address of the faulting page.

    Vad - Supplies the VAD which maps the range.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    PMMBANKED_SECTION Bank;
    PMMPTE PointerPte;
    ULONG BankNumber;
    ULONG size;

    Bank = ((PMMVAD_LONG) Vad)->u4.Banked;
    size = Bank->BankSize;

    RtlFillMemory (Bank->CurrentMappedPte,
                   size >> (PAGE_SHIFT - PTE_SHIFT),
                   (UCHAR)ZeroPte.u.Long);

    //
    // Flush the TB as we have invalidated all the PTEs in this range.
    //

    KeFlushEntireTb (TRUE, TRUE);

    //
    // Calculate new bank address and bank number.
    //

    PointerPte = MiGetPteAddress (
                        (PVOID)((ULONG_PTR)VirtualAddress & ~((LONG)size - 1)));
    Bank->CurrentMappedPte = PointerPte;

    BankNumber = (ULONG)(((PCHAR)PointerPte - (PCHAR)Bank->BasedPte) >> Bank->BankShift);

    (Bank->BankedRoutine) (BankNumber, BankNumber, Bank->Context);

    //
    // Set the new range valid.
    //

    RtlCopyMemory (PointerPte,
                   &Bank->BankTemplate[0],
                   size >> (PAGE_SHIFT - PTE_SHIFT));

    return;
}


NTSTATUS
MiSessionCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This function handles copy on write for image mapped session space.

Arguments:

    FaultingAddress - Supplies the address which caused the page fault.

    PointerPte - Supplies the pointer to the PTE which caused the page fault.

Return Value:

    STATUS_SUCCESS.

Environment:

    Kernel mode, APCs disabled, session WSL held.

--*/

{
    MMPTE TempPte;
    MMPTE PreviousPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NewPageIndex;
    PULONG CopyTo;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    WSLE_NUMBER WorkingSetIndex;
    PEPROCESS Process;

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    ASSERT (Pfn1->u3.e1.PrototypePte == 1);

    WorkingSetIndex = MiLocateWsle (FaultingAddress,
                                    MmSessionSpace->Vm.VmWorkingSetList,
                                    Pfn1->u1.WsIndex);

    LOCK_PFN (OldIrql);

    //
    // The page must be copied into a new page.
    //

    if (MiEnsureAvailablePageOrWait(HYDRA_PROCESS, NULL)) {

        //
        // A wait operation was performed to obtain an available
        // page and the working set mutex and PFN lock have
        // been released and various things may have changed for
        // the worse.  Rather than examine all the conditions again,
        // return and if things are still proper, the fault will
        // be taken again.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    TempPte = *PointerPte;

    ASSERT ((TempPte.u.Hard.Valid == 1) && (TempPte.u.Hard.Write == 0));

    //
    // Increment the number of private pages.
    //

    MmInfoCounters.CopyOnWriteCount += 1;

    MmSessionSpace->CopyOnWriteCount += 1;

    //
    // A page is being copied and made private, the global state of
    // the shared page does not need to be updated at this point because
    // it is guaranteed to be clean - no POSIX-style forking is allowed on
    // session addresses.
    //

#if 0

    //
    // This ASSERT is triggered if the session image came from removable media
    // (ie: a special CD install, etc) so it cannot be enabled.
    //

    ASSERT (Pfn1->u3.e1.Modified == 0);

#endif

    ASSERT (!MI_IS_PTE_DIRTY(TempPte));

    //
    // Get a new page to copy this one into.
    //

    NewPageIndex = MiRemoveAnyPage(MI_GET_PAGE_COLOR_FROM_SESSION(MmSessionSpace));

    MiInitializeCopyOnWritePfn (NewPageIndex,
                                PointerPte,
                                WorkingSetIndex,
                                MmSessionSpace);

    UNLOCK_PFN (OldIrql);

    //
    // Copy the accessed readonly page into the newly allocated writable page.
    //

    Process = PsGetCurrentProcess ();

    CopyTo = (PULONG)MiMapPageInHyperSpace (Process, NewPageIndex, &OldIrql);

    RtlCopyMemory (CopyTo, PAGE_ALIGN (FaultingAddress), PAGE_SIZE);

    MiUnmapPageInHyperSpace (Process, CopyTo, OldIrql);

    //
    // Since the page was a copy on write page, make it
    // accessed, dirty and writable.  Also clear the copy-on-write
    // bit in the PTE.
    //

    MI_SET_PTE_DIRTY (TempPte);
    TempPte.u.Hard.Write = 1;
    MI_SET_ACCESSED_IN_PTE (&TempPte, 1);
    TempPte.u.Hard.CopyOnWrite = 0;
    TempPte.u.Hard.PageFrameNumber = NewPageIndex;

    ASSERT (TempPte.u.Hard.Valid == 1);

    LOCK_PFN (OldIrql);

    //
    // Flush the TB entry for this page.
    //

    MI_FLUSH_SINGLE_SESSION_TB (FaultingAddress,
                                TRUE,
                                TRUE,
                                (PHARDWARE_PTE)PointerPte,
                                TempPte.u.Flush,
                                PreviousPte);

    ASSERT (Pfn1->u3.e1.PrototypePte == 1);

    //
    // Decrement the share count for the page which was copied
    // as this PTE no longer refers to it.
    //

    MiDecrementShareCount (PageFrameIndex);

    UNLOCK_PFN (OldIrql);
    return STATUS_SUCCESS;
}

#if DBG
VOID
MiCheckFileState (
    IN PMMPFN Pfn
    )

{
    PSUBSECTION Subsection;
    LARGE_INTEGER StartingOffset;

    if (Pfn->u3.e1.PrototypePte == 0) {
        return;
    }
    if (Pfn->OriginalPte.u.Soft.Prototype == 0) {
        return;
    }

    Subsection = MiGetSubsectionAddress (&(Pfn->OriginalPte));
    if (Subsection->ControlArea->u.Flags.NoModifiedWriting) {
        return;
    }
    StartingOffset.QuadPart = MiStartingOffset (Subsection,
                                                  Pfn->PteAddress);
    DbgPrint("file: %lx offset: %I64X\n",
            Subsection->ControlArea->FilePointer,
            StartingOffset.QuadPart);
    return;
}
#endif //DBG
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\pfndec.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   pfndec.c

Abstract:

    This module contains the routines to decrement the share count and
    the reference counts within the Page Frame Database.

Author:

    Lou Perazzoli (loup) 5-Apr-1989
    Landy Wang (landyw) 2-Jun-1997

Revision History:

--*/

#include "mi.h"

ULONG MmFrontOfList;
ULONG MiFlushForNonCached;


VOID
FASTCALL
MiDecrementShareCount (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine decrements the share count within the PFN element
    for the specified physical page.  If the share count becomes
    zero the corresponding PTE is converted to the transition state
    and the reference count is decremented and the ValidPte count
    of the PTEframe is decremented.

Arguments:

    PageFrameIndex - Supplies the physical page number of which to decrement
                     the share count.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    ULONG FreeBit;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    LOGICAL HyperMapped;
    PEPROCESS Process;

    ASSERT ((PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex > 0));

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (Pfn1->u3.e1.PageLocation != ActiveAndValid &&
        Pfn1->u3.e1.PageLocation != StandbyPageList) {
            KeBugCheckEx (PFN_LIST_CORRUPT,
                      0x99,
                      PageFrameIndex,
                      Pfn1->u3.e1.PageLocation,
                      0);
    }

    Pfn1->u2.ShareCount -= 1;

    PERFINFO_DECREFCNT(Pfn1, PERF_SOFT_TRIM, PERFINFO_LOG_TYPE_DECSHARCNT);

    ASSERT (Pfn1->u2.ShareCount < 0xF000000);

    if (Pfn1->u2.ShareCount == 0) {

        if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
            PERFINFO_PFN_INFORMATION PerfInfoPfn;

            PerfInfoPfn.PageFrameIndex = PageFrameIndex;
            PerfInfoLogBytes(PERFINFO_LOG_TYPE_ZEROSHARECOUNT, &PerfInfoPfn, sizeof(PerfInfoPfn));
        }

        //
        // The share count is now zero, decrement the reference count
        // for the PFN element and turn the referenced PTE into
        // the transition state if it refers to a prototype PTE.
        // PTEs which are not prototype PTEs do not need to be placed
        // into transition as they are placed in transition when
        // they are removed from the working set (working set free routine).
        //

        //
        // If the PTE referenced by this PFN element is actually
        // a prototype PTE, it must be mapped into hyperspace and
        // then operated on.
        //

        if (Pfn1->u3.e1.PrototypePte == 1) {

            if (MmIsAddressValid (Pfn1->PteAddress)) {
                Process = NULL;
                PointerPte = Pfn1->PteAddress;
                HyperMapped = FALSE;
            }
            else {

                //
                // The address is not valid in this process, map it into
                // hyperspace so it can be operated upon.
                //

                HyperMapped = TRUE;
                Process = PsGetCurrentProcess ();
                PointerPte = (PMMPTE)MiMapPageInHyperSpaceAtDpc(Process, Pfn1->u4.PteFrame);
                PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                        MiGetByteOffset(Pfn1->PteAddress));
            }

            TempPte = *PointerPte;
            MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                          Pfn1->OriginalPte.u.Soft.Protection);
            MI_WRITE_INVALID_PTE (PointerPte, TempPte);

            if (HyperMapped == TRUE) {
                MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
            }

            //
            // There is no need to flush the translation buffer at this
            // time as we only invalidated a prototype PTE.
            //

        }

        //
        // Change the page location to inactive (from active and valid).
        //

        Pfn1->u3.e1.PageLocation = TransitionPage;

        //
        // Decrement the reference count as the share count is now zero.
        //

        MM_PFN_LOCK_ASSERT();

        ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

        if (Pfn1->u3.e2.ReferenceCount == 1) {

            if (MI_IS_PFN_DELETED (Pfn1)) {

                Pfn1->u3.e2.ReferenceCount = 0;

                //
                // There is no referenced PTE for this page, delete the page
                // file space (if any), and place the page on the free list.
                //

                if ((Pfn1->u3.e1.CacheAttribute != MiCached) &&
                    (Pfn1->u3.e1.CacheAttribute != MiNotMapped)) {

                    //
                    // This page was mapped as noncached or writecombined, and
                    // is now being freed.  There may be a mapping for this
                    // page still in the TB because during system PTE unmap,
                    // the PTEs are zeroed but the TB is not flushed (in the
                    // interest of best performance).
                    //
                    // Flushing the TB on a per-page basis is admittedly
                    // expensive, especially in MP machines and if multiple
                    // pages are being done this way instead of batching them,
                    // but this should be a fairly rare occurrence.
                    //
                    // The TB must be flushed to ensure no stale mapping
                    // resides in it before this page can be given out with
                    // a conflicting mapping (ie: cached).  Since it's going
                    // on the freelist now, this must be completed before the
                    // PFN lock is released.
                    //
                    // A more elaborate scheme similar to the timestamping
                    // wrt to zeroing pages could be added if this becomes
                    // a hot path.
                    //

                    MiFlushForNonCached += 1;
                    KeFlushEntireTb (TRUE, TRUE);
                }

                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 0);

                FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                    MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                }

                //
                // Temporarily mark the frame as active and valid so that
                // MiIdentifyPfn knows it is safe to walk back through the
                // containing frames for a more accurate identification.
                // Note the page will be immediately re-marked as it is
                // inserted into the freelist.
                //

                Pfn1->u3.e1.PageLocation = ActiveAndValid;

                MiInsertPageInFreeList (PageFrameIndex);
            }
            else {
                MiDecrementReferenceCount (PageFrameIndex);
            }
        }
        else {
            Pfn1->u3.e2.ReferenceCount -= 1;
        }
    }

    return;
}

VOID
FASTCALL
MiDecrementReferenceCount (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This routine decrements the reference count for the specified page.
    If the reference count becomes zero, the page is placed on the
    appropriate list (free, modified, standby or bad).  If the page
    is placed on the free or standby list, the number of available
    pages is incremented and if it transitions from zero to one, the
    available page event is set.


Arguments:

    PageFrameIndex - Supplies the physical page number of which to
                     decrement the reference count.

Return Value:

    none.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PMMPFN Pfn1;

    MM_PFN_LOCK_ASSERT();

    ASSERT (PageFrameIndex <= MmHighestPhysicalPage);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
    Pfn1->u3.e2.ReferenceCount -= 1;

    if (Pfn1->u3.e2.ReferenceCount != 0) {

        //
        // The reference count is not zero, return.
        //

        return;
    }

    //
    // The reference count is now zero, put the page on some list.
    //

    if (Pfn1->u2.ShareCount != 0) {

        KeBugCheckEx (PFN_LIST_CORRUPT,
                      7,
                      PageFrameIndex,
                      Pfn1->u2.ShareCount,
                      0);
        return;
    }

    ASSERT (Pfn1->u3.e1.PageLocation != ActiveAndValid);

    if (MI_IS_PFN_DELETED (Pfn1)) {

        //
        // There is no referenced PTE for this page, delete the page file
        // space (if any), and place the page on the free list.
        //

        if ((Pfn1->u3.e1.CacheAttribute != MiCached) &&
            (Pfn1->u3.e1.CacheAttribute != MiNotMapped)) {

            //
            // This page was mapped as noncached or writecombined, and is now
            // being freed.  There may be a mapping for this page still in the
            // TB because system PTE unmap, the PTEs are zeroed but the TB
            // is not flushed (in the interest of best performance).
            //
            // Flushing the TB on a per-page basis is admittedly expensive,
            // especially in MP machines and if multiple pages are being done
            // this way instead of batching them, but this should be a
            // fairly rare occurrence.
            //
            // The TB must be flushed to ensure no stale mapping resides in it
            // before this page can be given out with a conflicting mapping
            // (ie: cached).  Since it's going on the freelist now, this must
            // be completed before the PFN lock is released.
            //
            // A more elaborate scheme similar to the timestamping wrt to
            // zeroing pages could be added if this becomes a hot path.
            //

            MiFlushForNonCached += 1;
            KeFlushEntireTb (TRUE, TRUE);
        }

        MiReleasePageFileSpace (Pfn1->OriginalPte);

        MiInsertPageInFreeList (PageFrameIndex);

        return;
    }

    ASSERT ((Pfn1->u3.e1.CacheAttribute != MiNonCached) &&
            (Pfn1->u3.e1.CacheAttribute != MiWriteCombined));

    //
    // Place the page on the modified or standby list depending
    // on the state of the modify bit in the PFN element.
    //

    if (Pfn1->u3.e1.Modified == 1) {
        MiInsertPageInList (&MmModifiedPageListHead, PageFrameIndex);
    }
    else {

        if (Pfn1->u3.e1.RemovalRequested == 1) {

            //
            // The page may still be marked as on the modified list if the
            // current thread is the modified writer completing the write.
            // Mark it as standby so restoration of the transition PTE
            // doesn't flag this as illegal.
            //

            Pfn1->u3.e1.PageLocation = StandbyPageList;

            MiRestoreTransitionPte (PageFrameIndex);
            MiInsertPageInList (&MmBadPageListHead, PageFrameIndex);
            return;
        }

        if (!MmFrontOfList) {
            MiInsertPageInList (&MmStandbyPageListHead, PageFrameIndex);
        }
        else {
            MiInsertStandbyListAtFront (PageFrameIndex);
        }
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\pfnlist.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   pfnlist.c

Abstract:

    This module contains the routines to manipulate pages within the
    Page Frame Database.

Author:

    Lou Perazzoli (loup) 4-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/
#include "mi.h"

//
// The following line will generate an error is the number of colored
// free page lists is not 2, ie ZeroedPageList and FreePageList.  If
// this number is changed, the size of the FreeCount array in the kernel
// node structure (KNODE) must be updated.
//

C_ASSERT(StandbyPageList == 2);

#define MM_LOW_LIMIT 2

KEVENT MmAvailablePagesEventHigh;
KEVENT MmAvailablePagesEventMedium;

PFN_NUMBER MmTransitionPrivatePages;
PFN_NUMBER MmTransitionSharedPages;

#define MI_TALLY_TRANSITION_PAGE_ADDITION(Pfn) \
    if (Pfn->u3.e1.PrototypePte) { \
        MmTransitionSharedPages += 1; \
    } \
    else { \
        MmTransitionPrivatePages += 1; \
    }
#if 0
    ASSERT (MmTransitionPrivatePages + MmTransitionSharedPages == MmStandbyPageListHead.Total + MmModifiedPageListHead.Total + MmModifiedNoWritePageListHead.Total);
#endif

#define MI_TALLY_TRANSITION_PAGE_REMOVAL(Pfn) \
    if (Pfn->u3.e1.PrototypePte) { \
        MmTransitionSharedPages -= 1; \
    } \
    else { \
        MmTransitionPrivatePages -= 1; \
    }
#if 0
    ASSERT (MmTransitionPrivatePages + MmTransitionSharedPages == MmStandbyPageListHead.Total + MmModifiedPageListHead.Total + MmModifiedNoWritePageListHead.Total);
#endif

//
// This counter is used to determine if standby pages are being cannibalized
// for use as free pages and therefore more aging should be attempted.
//

ULONG MiStandbyRemoved;

#if DBG
ULONG MiSaveStacks;
#endif

extern ULONG MmSystemShutdown;

PFN_NUMBER
FASTCALL
MiRemovePageByColor (
    IN PFN_NUMBER Page,
    IN ULONG PageColor
    );

VOID
FASTCALL
MiLogPfnInformation (
    IN PMMPFN Pfn1,
    IN USHORT Reason
    );

extern LOGICAL MiZeroingDisabled;

// #define _MI_DEBUG_PFN   1

#if defined (_MI_DEBUG_PFN)

#define MI_PFN_TRACE_MAX 0x8000

#define MI_PFN_BACKTRACE_LENGTH 6

typedef struct _MI_PFN_TRACES {

    PFN_NUMBER PageFrameIndex;
    MMLISTS Destination;
    MMPTE PteContents;
    PVOID Thread;

    MMPFN Pfn;

    PVOID StackTrace [MI_PFN_BACKTRACE_LENGTH];

} MI_PFN_TRACES, *PMI_PFN_TRACES;

LONG MiPfnIndex;

PMI_PFN_TRACES MiPfnTraces;

ULONG zpfn = 0x1;

VOID
FORCEINLINE
MiSnapPfn (
    IN PMMPFN Pfn1,
    IN MMLISTS Destination,
    IN ULONG CallerId
    )
{                                                           
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMI_PFN_TRACES Information;                                
    ULONG Hash;                                               
    ULONG Index;                                             
    PEPROCESS Process;
                                                            
    if (MiPfnTraces == NULL) {
        return;
    }

    if (zpfn & 0x1) {

        if (Pfn1->PteAddress < MiGetPteAddress (MmPagedPoolStart)) {
            return;
        }

        if (Pfn1->PteAddress > MiGetPteAddress (MmPagedPoolEnd)) {
            return;
        }
    }

    if (MmIsAddressValid (Pfn1->PteAddress)) {
        PointerPte = Pfn1->PteAddress;
        PointerPte = (PMMPTE)((ULONG_PTR)PointerPte & ~0x1);
        PteContents = *PointerPte;
    }
    else {

        //
        // The containing page table page is not valid,
        // map the page into hyperspace and reference it that way.
        //

        Process = PsGetCurrentProcess ();
        PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
        PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                MiGetByteOffset(Pfn1->PteAddress));
        PointerPte = (PMMPTE)((ULONG_PTR)PointerPte & ~0x1);
        PteContents = *PointerPte;
        MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
    }

    Index = InterlockedIncrement(&MiPfnIndex);       
    Index &= (MI_PFN_TRACE_MAX - 1);                
    Information = &MiPfnTraces[Index];             
    Information->PageFrameIndex = (Pfn1 - MmPfnDatabase); 
    Information->Destination = Destination;
    Information->PteContents = PteContents;
    Information->Thread = (PVOID)((ULONG_PTR)KeGetCurrentThread() | (CallerId));                                                            
    Information->Pfn = *Pfn1;                             
    RtlCaptureStackBackTrace (0, MI_PFN_BACKTRACE_LENGTH, Information->StackTrace, &Hash);                                                  
}

#define MI_SNAP_PFN(_Pfn, dest, callerid) MiSnapPfn(_Pfn, dest, callerid)

#else
#define MI_SNAP_PFN(_Pfn, dest, callerid)
#endif

VOID
MiInitializePfnTracing (
    VOID
    )
{
#if defined (_MI_DEBUG_PFN)
    MiPfnTraces = ExAllocatePoolWithTag (NonPagedPool,
                           MI_PFN_TRACE_MAX * sizeof (MI_PFN_TRACES),
                           'tCmM');
#endif
}


VOID
FASTCALL
MiInsertPageInFreeList (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the end of the free list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    PFN lock held.

--*/

{
    PFN_NUMBER last;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
    PMMPFNLIST ListHead;
    PMMCOLOR_TABLES ColorHead;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 8);

    MI_SNAP_PFN(Pfn1, FreePageList, 0x1);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_INSERTINFREELIST);
    }

    if (Pfn1->u3.e1.Rom == 1) {

        //
        // ROM pages do not go on free lists and are not counted towards
        // MmAvailablePages as they are not reusable.  Transform these pages
        // into their pre-Phase 0 state and keep them off all lists.
        //

        ASSERT (XIPConfigured == TRUE);

        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
        ASSERT (Pfn1->u3.e1.Modified == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u4.InPageError == 0);
        ASSERT (Pfn1->u3.e1.PrototypePte == 1);

        Pfn1->u1.Flink = 0;
        Pfn1->u3.e1.PageLocation = 0;

        return;
    }

    if (Pfn1->u3.e1.RemovalRequested == 1) {
        Pfn1->u3.e1.CacheAttribute = MiNotMapped;
        MiInsertPageInList (&MmBadPageListHead, PageFrameIndex);
        return;
    }

    ListHead = &MmFreePageListHead;

    ASSERT (Pfn1->u3.e1.LockCharged == 0);

    PERFINFO_INSERTINLIST(PageFrameIndex, ListHead);

    ListName = FreePageList;

#if DBG
#if defined (_X86_)
    if ((MiSaveStacks != 0) && (MmFirstReservedMappingPte != NULL)) {

        ULONG_PTR StackPointer;
        ULONG_PTR StackBytes;
        PULONG_PTR DataPage;
        PEPROCESS Process;

        _asm {
            mov StackPointer, esp
        }

        MiZeroingDisabled = TRUE;

        Process = PsGetCurrentProcess ();

        DataPage = MiMapPageInHyperSpaceAtDpc (Process, PageFrameIndex);

        DataPage[0] = StackPointer;
    
        //
        // For now, don't get fancy with copying more than what's in the current
        // stack page.  To do so would require checking the thread stack limits,
        // DPC stack limits, etc.
        //
    
        StackBytes = PAGE_SIZE - BYTE_OFFSET(StackPointer);
        DataPage[1] = StackBytes;
    
        if (StackBytes != 0) {
    
            if (StackBytes > MI_STACK_BYTES) {
                StackBytes = MI_STACK_BYTES;
            }
    
            RtlCopyMemory ((PVOID)&DataPage[2], (PVOID)StackPointer, StackBytes);
        }

        MiUnmapPageInHyperSpaceFromDpc (Process, DataPage);
    }
#endif
#endif

    ASSERT (Pfn1->u4.VerifierAllocation == 0);

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    ListHead->Total += 1;  // One more page on the list.

    last = ListHead->Blink;

    if (last != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT (last);
        Pfn2->u1.Flink = PageFrameIndex;
    }
    else {

        //
        // List is empty, add the page to the ListHead.
        //

        ListHead->Flink = PageFrameIndex;
    }

    ListHead->Blink = PageFrameIndex;
    Pfn1->u1.Flink = MM_EMPTY_LIST;
    Pfn1->u2.Blink = last;

    Pfn1->u3.e1.PageLocation = ListName;
    Pfn1->u3.e1.CacheAttribute = MiNotMapped;
    Pfn1->u4.InPageError = 0;

    //
    // Update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    MmAvailablePages += 1;

    //
    // A page has just become available, check to see if the
    // page wait events should be signaled.
    //

    if (MmAvailablePages <= MM_HIGH_LIMIT) {
        if (MmAvailablePages == MM_HIGH_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
        }
        else if (MmAvailablePages == MM_MEDIUM_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventMedium, 0, FALSE);
        }
        else if (MmAvailablePages == MM_LOW_LIMIT) {
            KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
        }
    }

    //
    // Signal applications if the freed page crosses a threshold.
    //

    if (MmAvailablePages == MmLowMemoryThreshold) {
        KeClearEvent (MiLowMemoryEvent);
    }
    else if (MmAvailablePages == MmHighMemoryThreshold) {
        KeSetEvent (MiHighMemoryEvent, 0, FALSE);
    }

#if defined(MI_MULTINODE)

    //
    // Increment the free page count for this node.
    //

    if (KeNumberNodes > 1) {
        KeNodeBlock[Pfn1->u3.e1.PageColor]->FreeCount[ListName]++;
    }
#endif

    //
    // We are adding a page to the free page list.
    // Add the page to the end of the correct colored page list.
    //

    Color = MI_GET_COLOR_FROM_LIST_ENTRY(PageFrameIndex, Pfn1);

    ColorHead = &MmFreePagesByColor[ListName][Color];

    if (ColorHead->Flink == MM_EMPTY_LIST) {

        //
        // This list is empty, add this as the first and last
        // entry.
        //

        ColorHead->Flink = PageFrameIndex;
        ColorHead->Blink = Pfn1;
    }
    else {
        Pfn2 = (PMMPFN)ColorHead->Blink;
        Pfn2->OriginalPte.u.Long = PageFrameIndex;
        ColorHead->Blink = Pfn1;
    }
    ColorHead->Count += 1;
    Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

    if ((ListHead->Total >= MmMinimumFreePagesToZero) &&
        (MmZeroingPageThreadActive == FALSE)) {

        //
        // There are enough pages on the free list, start
        // the zeroing page thread.
        //

        MmZeroingPageThreadActive = TRUE;
        KeSetEvent (&MmZeroingPageEvent, 0, FALSE);
    }

    return;
}

VOID
FASTCALL
MiInsertPageInList (
    IN PMMPFNLIST ListHead,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the end of the specified list (standby,
    bad, zeroed, modified).

Arguments:

    ListHead - Supplies the list of the list in which to insert the
               specified physical page.

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PFN_NUMBER last;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;
#if MI_BARRIER_SUPPORTED
    ULONG BarrierStamp;
#endif

    ASSERT (ListHead != &MmFreePageListHead);

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    ASSERT (Pfn1->u3.e1.LockCharged == 0);

    PERFINFO_INSERTINLIST(PageFrameIndex, ListHead);

    ListName = ListHead->ListName;

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 0x20 + ListName);

    MI_SNAP_PFN(Pfn1, ListName, 0x2);

#if DBG
    if (MmDebug & MM_DBG_PAGE_REF_COUNT) {

        if ((ListName == StandbyPageList) || (ListName == ModifiedPageList)) {

            PMMPTE PointerPte;
            PEPROCESS Process;

            if ((Pfn1->u3.e1.PrototypePte == 1)  &&
                    (MmIsAddressValid (Pfn1->PteAddress))) {
                Process = NULL;
                PointerPte = Pfn1->PteAddress;
            }
            else {

                //
                // The page containing the prototype PTE is not valid,
                // map the page into hyperspace and reference it that way.
                //

                Process = PsGetCurrentProcess ();
                PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
                PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                        MiGetByteOffset(Pfn1->PteAddress));
            }

            ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) ||
                    (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) == PageFrameIndex));
            ASSERT (PointerPte->u.Soft.Transition == 1);
            ASSERT (PointerPte->u.Soft.Prototype == 0);
            if (Process != NULL) {
                MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
            }
        }
    }

    if ((ListName == StandbyPageList) || (ListName == ModifiedPageList)) {
        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
           (Pfn1->OriginalPte.u.Soft.Transition == 1)) {
            KeBugCheckEx (MEMORY_MANAGEMENT, 0x8888, 0,0,0);
        }
    }
#endif

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    if (Pfn1->u3.e1.Rom == 1) {

        //
        // ROM pages do not go on transition lists and are not counted towards
        // MmAvailablePages as they are not reusable.  Migrate these pages
        // into a separate list but keep the PageLocation as standby.
        //

        ASSERT (XIPConfigured == TRUE);
        ASSERT (Pfn1->u3.e1.Modified == 0);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        ListHead = &MmRomPageListHead;
        ListHead->Total += 1;  // One more page on the list.
        last = ListHead->Blink;

        if (last != MM_EMPTY_LIST) {
            Pfn2 = MI_PFN_ELEMENT (last);
            Pfn2->u1.Flink = PageFrameIndex;
        }
        else {

            //
            // List is empty, add the page to the ListHead.
            //

            ListHead->Flink = PageFrameIndex;
        }

        ListHead->Blink = PageFrameIndex;
        Pfn1->u1.Flink = MM_EMPTY_LIST;
        Pfn1->u2.Blink = last;

        Pfn1->u3.e1.PageLocation = ListName;

        return;
    }

    ListHead->Total += 1;  // One more page on the list.

    if (ListHead == &MmModifiedPageListHead) {

        //
        // Leave the page as cached as it may need to be written out at some
        // point and presumably the driver stack will need to map it at that
        // time.
        //

        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        //
        // On MIPS R4000 modified pages destined for the paging file are
        // kept on separate lists which group pages of the same color
        // together.
        //

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            //
            // This page is destined for the paging file (not
            // a mapped file).  Change the list head to the
            // appropriate colored list head.
            //

#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
            ListHead = &MmModifiedPageListByColor [Pfn1->u3.e1.PageColor];
#else
            ListHead = &MmModifiedPageListByColor [0];
#endif
            ASSERT (ListHead->ListName == ListName);
            ListHead->Total += 1;
            MmTotalPagesForPagingFile += 1;
        }
        else {

            //
            // This page is destined for a mapped file (not
            // the paging file).  If there are no other pages currently
            // destined for the mapped file, start our timer so that we can
            // ensure that these pages make it to disk even if we don't pile
            // up enough of them to trigger the modified page writer or need
            // the memory.  If we don't do this here, then for this scenario,
            // only an orderly system shutdown will write them out (days,
            // weeks, months or years later) and any power out in between
            // means we'll have lost the data.
            //

            if (ListHead->Total - MmTotalPagesForPagingFile == 1) {

                //
                // Start the DPC timer because we're the first on the list.
                //

                if (MiTimerPending == FALSE) {
                    MiTimerPending = TRUE;

                    KeSetTimerEx (&MiModifiedPageWriterTimer,
                                  MiModifiedPageLife,
                                  0,
                                  &MiModifiedPageWriterTimerDpc);
                }
            }
        }
    }
    else if ((Pfn1->u3.e1.RemovalRequested == 1) &&
             (ListName <= StandbyPageList)) {

        ListHead->Total -= 1;  // Undo previous increment

        if (ListName == StandbyPageList) {
            Pfn1->u3.e1.PageLocation = StandbyPageList;
            MiRestoreTransitionPte (PageFrameIndex);
        }

        Pfn1->u3.e1.CacheAttribute = MiNotMapped;

        ListHead = &MmBadPageListHead;
        ASSERT (ListHead->ListName == BadPageList);
        ListHead->Total += 1;  // One more page on the list.
        ListName = BadPageList;
    }

    last = ListHead->Blink;

    if (last != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT (last);
        Pfn2->u1.Flink = PageFrameIndex;
    }
    else {

        //
        // List is empty, add the page to the ListHead.
        //

        ListHead->Flink = PageFrameIndex;
    }

    ListHead->Blink = PageFrameIndex;
    Pfn1->u1.Flink = MM_EMPTY_LIST;
    Pfn1->u2.Blink = last;

    Pfn1->u3.e1.PageLocation = ListName;

    //
    // If the page was placed on the standby or zeroed list,
    // update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    if (ListName <= StandbyPageList) {

        MmAvailablePages += 1;

        //
        // A page has just become available, check to see if the
        // page wait events should be signaled.
        //

        if (MmAvailablePages <= MM_HIGH_LIMIT) {
            if (MmAvailablePages == MM_HIGH_LIMIT) {
                KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
            }
            else if (MmAvailablePages == MM_MEDIUM_LIMIT) {
                KeSetEvent (&MmAvailablePagesEventMedium, 0, FALSE);
            }
            else if (MmAvailablePages == MM_LOW_LIMIT) {
                KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
            }
        }

        //
        // Signal applications if the freed page crosses a threshold.
        //

        if (MmAvailablePages == MmLowMemoryThreshold) {
            KeClearEvent (MiLowMemoryEvent);
        }
        else if (MmAvailablePages == MmHighMemoryThreshold) {
            KeSetEvent (MiHighMemoryEvent, 0, FALSE);
        }

        if (ListName <= FreePageList) {

            PMMCOLOR_TABLES ColorHead;

            ASSERT (ListName == ZeroedPageList);
            ASSERT (Pfn1->u4.InPageError == 0);

            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

#if defined(MI_MULTINODE)

            //
            // Increment the zero page count for this node.
            //

            if (KeNumberNodes > 1) {
                KeNodeBlock[Pfn1->u3.e1.PageColor]->FreeCount[ListName]++;
            }
#endif

            //
            // We are adding a page to the zeroed page list.
            // Add the page to the end of the correct colored page list.
            //

            Color = MI_GET_COLOR_FROM_LIST_ENTRY(PageFrameIndex, Pfn1);

            ColorHead = &MmFreePagesByColor[ListName][Color];

            if (ColorHead->Flink == MM_EMPTY_LIST) {

                //
                // This list is empty, add this as the first and last
                // entry.
                //

                ColorHead->Flink = PageFrameIndex;
                ColorHead->Blink = (PVOID)Pfn1;
            }
            else {
                Pfn2 = (PMMPFN)ColorHead->Blink;
                Pfn2->OriginalPte.u.Long = PageFrameIndex;
                ColorHead->Blink = (PVOID)Pfn1;
            }
            Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;

            ColorHead->Count += 1;

#if MI_BARRIER_SUPPORTED
            if (ListName == ZeroedPageList) {
                MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
                Pfn1->u4.PteFrame = BarrierStamp;
            }
#endif
        }
        else {

            //
            // Transition page list so tally it appropriately.
            //

            ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
            MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);
        }

        return;
    }

    //
    // Check to see if there are too many modified pages.
    //

    if (ListName == ModifiedPageList) {

        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        //
        // Transition page list so tally it appropriately.
        //

        MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {
            ASSERT (Pfn1->OriginalPte.u.Soft.PageFileHigh == 0);
        }

        PsGetCurrentProcess()->ModifiedPageCount += 1;

        if ((MmModifiedPageListHead.Total >= MmModifiedPageMaximum) &&
            (MmAvailablePages < MmMoreThanEnoughFreePages)) {

            //
            // Start the modified page writer.
            //

            KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
        }
    }
    else if (ListName == ModifiedNoWritePageList) {
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
        MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);
    }

    return;
}


VOID
FASTCALL
MiInsertStandbyListAtFront (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the front of the standby list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    PFN lock held.

--*/

{
    PFN_NUMBER first;
    PFN_NUMBER last;
    IN PMMPFNLIST ListHead;
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) && (PageFrameIndex <= MmHighestPhysicalPage) &&
        (PageFrameIndex >= MmLowestPhysicalPage));

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 9);

    PERFINFO_INSERT_FRONT_STANDBY(PageFrameIndex);

#if DBG
    if (MmDebug & MM_DBG_PAGE_REF_COUNT) {

        PMMPTE PointerPte;
        PEPROCESS Process;

        if ((Pfn1->u3.e1.PrototypePte == 1)  &&
                (MmIsAddressValid (Pfn1->PteAddress))) {
            PointerPte = Pfn1->PteAddress;
            Process = NULL;
        }
        else {

            //
            // The page containing the prototype PTE is not valid,
            // map the page into hyperspace and reference it that way.
            //

            Process = PsGetCurrentProcess ();
            PointerPte = MiMapPageInHyperSpaceAtDpc (Process, Pfn1->u4.PteFrame);
            PointerPte = (PMMPTE)((PCHAR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        ASSERT ((MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) == PageFrameIndex) ||
                (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) == PageFrameIndex));
        ASSERT (PointerPte->u.Soft.Transition == 1);
        ASSERT (PointerPte->u.Soft.Prototype == 0);
        if (Process != NULL) {
            MiUnmapPageInHyperSpaceFromDpc (Process, PointerPte);
        }
    }

    if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
       (Pfn1->OriginalPte.u.Soft.Transition == 1)) {
        KeBugCheckEx (MEMORY_MANAGEMENT, 0x8889, 0,0,0);
    }
#endif

    //
    // Check to ensure the reference count for the page is zero.
    //

    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u3.e1.PrototypePte == 1);
    ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

    if (Pfn1->u3.e1.Rom == 1) {

        //
        // ROM pages do not go on transition lists and are not counted towards
        // MmAvailablePages as they are not reusable.  Migrate these pages
        // into a separate list but keep the PageLocation as standby.  Note
        // it doesn't matter if the page is put at the head or the tail since
        // it's not reusable.
        //

        ASSERT (XIPConfigured == TRUE);
        ASSERT (Pfn1->u3.e1.Modified == 0);

        ListHead = &MmRomPageListHead;
        ListHead->Total += 1;  // One more page on the list.
        last = ListHead->Blink;

        if (last != MM_EMPTY_LIST) {
            Pfn2 = MI_PFN_ELEMENT (last);
            Pfn2->u1.Flink = PageFrameIndex;
        }
        else {

            //
            // List is empty, add the page to the ListHead.
            //

            ListHead->Flink = PageFrameIndex;
        }

        ListHead->Blink = PageFrameIndex;
        Pfn1->u1.Flink = MM_EMPTY_LIST;
        Pfn1->u2.Blink = last;

        Pfn1->u3.e1.PageLocation = StandbyPageList;

        return;
    }

    MmTransitionSharedPages += 1;

    MmStandbyPageListHead.Total += 1;  // One more page on the list.

    ASSERT (MmTransitionPrivatePages + MmTransitionSharedPages == MmStandbyPageListHead.Total + MmModifiedPageListHead.Total + MmModifiedNoWritePageListHead.Total);

    ListHead = &MmStandbyPageListHead;

    first = ListHead->Flink;
    if (first == MM_EMPTY_LIST) {

        //
        // List is empty add the page to the ListHead.
        //

        ListHead->Blink = PageFrameIndex;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT (first);
        Pfn2->u2.Blink = PageFrameIndex;
    }

    ListHead->Flink = PageFrameIndex;
    Pfn1->u2.Blink = MM_EMPTY_LIST;
    Pfn1->u1.Flink = first;
    Pfn1->u3.e1.PageLocation = StandbyPageList;

    //
    // If the page was placed on the free, standby or zeroed list,
    // update the count of usable pages in the system.  If the count
    // transitions from 0 to 1, the event associated with available
    // pages should become true.
    //

    MmAvailablePages += 1;

    //
    // A page has just become available, check to see if the
    // page wait events should be signalled.
    //

    if (MmAvailablePages <= MM_HIGH_LIMIT) {
        if (MmAvailablePages == MM_HIGH_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventHigh, 0, FALSE);
        }
        else if (MmAvailablePages == MM_MEDIUM_LIMIT) {
            KeSetEvent (&MmAvailablePagesEventMedium, 0, FALSE);
        }
        else if (MmAvailablePages == MM_LOW_LIMIT) {
            KeSetEvent (&MmAvailablePagesEvent, 0, FALSE);
        }
    }

    //
    // Signal applications if the freed page crosses a threshold.
    //

    if (MmAvailablePages == MmLowMemoryThreshold) {
        KeClearEvent (MiLowMemoryEvent);
    }
    else if (MmAvailablePages == MmHighMemoryThreshold) {
        KeSetEvent (MiHighMemoryEvent, 0, FALSE);
    }

    return;
}

PFN_NUMBER
FASTCALL
MiRemovePageFromList (
    IN PMMPFNLIST ListHead
    )

/*++

Routine Description:

    This procedure removes a page from the head of the specified list (free,
    standby or zeroed).

    This routine clears the flags word in the PFN database, hence the
    PFN information for this page must be initialized.

Arguments:

    ListHead - Supplies the list of the list in which to remove the
               specified physical page.

Return Value:

    The physical page number removed from the specified list.

Environment:

    PFN lock held.

--*/

{
    PMMCOLOR_TABLES ColorHead;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG Color;
    MMLISTS ListName;

    MM_PFN_LOCK_ASSERT();

    //
    // If the specified list is empty return MM_EMPTY_LIST.
    //

    if (ListHead->Total == 0) {
        KeBugCheckEx (PFN_LIST_CORRUPT, 1, (ULONG_PTR)ListHead, MmAvailablePages, 0);
    }

    ListName = ListHead->ListName;
    ASSERT (ListName != ModifiedPageList);

    //
    // Decrement the count of pages on the list and remove the first
    // page from the list.
    //

    ListHead->Total -= 1;
    PageFrameIndex = ListHead->Flink;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_REMOVEPAGEFROMLIST);
    }

    ListHead->Flink = Pfn1->u1.Flink;

    //
    // Zero the flink and blink in the PFN database element.
    //

    Pfn1->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Pfn1->u2.Blink = 0;

    //
    // If the last page was removed (the ListHead->Flink is now
    // MM_EMPTY_LIST) make the Listhead->Blink MM_EMPTY_LIST as well.
    //

    if (ListHead->Flink != MM_EMPTY_LIST) {

        //
        // Make the PFN element blink point to MM_EMPTY_LIST signifying this
        // is the first page in the list.
        //

        Pfn2 = MI_PFN_ELEMENT (ListHead->Flink);
        Pfn2->u2.Blink = MM_EMPTY_LIST;
    }
    else {
        ListHead->Blink = MM_EMPTY_LIST;
    }

    //
    // Check to see if we now have one less page available.
    //

    if (ListName <= StandbyPageList) {
        MmAvailablePages -= 1;

        if (ListName == StandbyPageList) {

            //
            // This page is currently in transition, restore the PTE to
            // its original contents so this page can be reused.
            //

            MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn1);
            MiRestoreTransitionPte (PageFrameIndex);
        }

        if (MmAvailablePages < MmMinimumFreePages) {

            //
            // Obtain free pages.
            //

            MiObtainFreePages();
        }
    }

    ASSERT ((PageFrameIndex != 0) &&
            (PageFrameIndex <= MmHighestPhysicalPage) &&
            (PageFrameIndex >= MmLowestPhysicalPage));

    //
    // Zero the PFN flags longword.
    //

    ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
    Color = Pfn1->u3.e1.PageColor;
    ASSERT (Pfn1->u3.e1.RemovalRequested == 0);
    ASSERT (Pfn1->u3.e1.Rom == 0);
    Pfn1->u3.e2.ShortFlags = 0;
    Pfn1->u3.e1.PageColor = Color;
    Pfn1->u3.e1.CacheAttribute = MiNotMapped;

    if (ListName <= FreePageList) {

        //
        // Update the color lists.
        //

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(PageFrameIndex, Pfn1);
        ColorHead = &MmFreePagesByColor[ListName][Color];
        ASSERT (ColorHead->Flink == PageFrameIndex);
        ColorHead->Flink = (PFN_NUMBER) Pfn1->OriginalPte.u.Long;
        ASSERT (ColorHead->Count >= 1);
        ColorHead->Count -= 1;
    }

    return PageFrameIndex;
}

VOID
FASTCALL
MiUnlinkPageFromList (
    IN PMMPFN Pfn
    )

/*++

Routine Description:

    This procedure removes a page from the middle of a list.  This is
    designed for the faulting of transition pages from the standby and
    modified list and making them active and valid again.

Arguments:

    Pfn - Supplies a pointer to the PFN database element for the physical
          page to remove from the list.

Return Value:

    none.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PMMPFNLIST ListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn2;
    MMLISTS ListName;

    MM_PFN_LOCK_ASSERT();

    PERFINFO_UNLINKPAGE((ULONG_PTR)(Pfn - MmPfnDatabase), Pfn->u3.e1.PageLocation);

    //
    // Page not on standby or modified list, check to see if the
    // page is currently being written by the modified page
    // writer, if so, just return this page.  The reference
    // count for the page will be incremented, so when the modified
    // page write completes, the page will not be put back on
    // the list, rather, it will remain active and valid.
    //

    if (Pfn->u3.e2.ReferenceCount > 0) {

        //
        // The page was not on any "transition lists", check to see
        // if this has I/O in progress.
        //

        if (Pfn->u2.ShareCount == 0) {
#if DBG
            if (MmDebug & MM_DBG_PAGE_IN_LIST) {
                DbgPrint("unlinking page not in list...\n");
                MiFormatPfn(Pfn);
            }
#endif
            return;
        }
        KdPrint(("MM:attempt to remove page from wrong page list\n"));
        KeBugCheckEx (PFN_LIST_CORRUPT,
                      2,
                      Pfn - MmPfnDatabase,
                      MmHighestPhysicalPage,
                      Pfn->u3.e2.ReferenceCount);
    }

    ListHead = MmPageLocationList[Pfn->u3.e1.PageLocation];

    //
    // Must not remove pages from free or zeroed without updating
    // the colored lists.
    //

    ListName = ListHead->ListName;
    ASSERT (ListName >= StandbyPageList);

    //
    // If memory mirroring is in progress, any additions or removals to the
    // free, zeroed, standby, modified or modified-no-write lists must
    // update the bitmap.
    //

    if (MiMirroringActive == TRUE) {
        RtlSetBit (MiMirrorBitMap2, (ULONG)(Pfn - MmPfnDatabase));
    }

    ASSERT (Pfn->u3.e1.CacheAttribute == MiCached);

    if (ListHead == &MmStandbyPageListHead) {
        if (Pfn->u3.e1.Rom == 1) {
            ASSERT (XIPConfigured == TRUE);
            ASSERT (Pfn->u3.e1.Modified == 0);

            ListHead = &MmRomPageListHead;

            //
            // Deliberately switch to an invalid page listname so the checks
            // below do not alter available pages, etc.
            //

            ListName = ActiveAndValid;
        }
    }
    else if ((ListHead == &MmModifiedPageListHead) &&
        (Pfn->OriginalPte.u.Soft.Prototype == 0)) {

        //
        // On MIPS R4000 modified pages destined for the paging file are
        // kept on separate lists which group pages of the same color
        // together.
        //

        //
        // This page is destined for the paging file (not
        // a mapped file).  Change the list head to the
        // appropriate colored list head.
        //

        ListHead->Total -= 1;
        MmTotalPagesForPagingFile -= 1;
#if MM_MAXIMUM_NUMBER_OF_COLORS > 1
        ListHead = &MmModifiedPageListByColor [Pfn->u3.e1.PageColor];
#else
        ListHead = &MmModifiedPageListByColor [0];
#endif
        ASSERT (ListHead->ListName == ListName);
    }

    ASSERT (Pfn->u3.e1.WriteInProgress == 0);
    ASSERT (Pfn->u3.e1.ReadInProgress == 0);
    ASSERT (ListHead->Total != 0);

    Next = Pfn->u1.Flink;
    Pfn->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Previous = Pfn->u2.Blink;
    Pfn->u2.Blink = 0;

    if (Next != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }
    else {
        ListHead->Blink = Previous;
    }

    if (Previous != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }
    else {
        ListHead->Flink = Next;
    }

    ListHead->Total -= 1;

    //
    // Check to see if we now have one less page available.
    //

    if (ListName <= StandbyPageList) {
        MmAvailablePages -= 1;

        if (ListName == StandbyPageList) {
            MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn);
        }

        if (MmAvailablePages < MmMinimumFreePages) {

            //
            // Obtain free pages.
            //

            MiObtainFreePages();

        }
    }
    else if (ListName == ModifiedPageList || ListName == ModifiedNoWritePageList) {
        MI_TALLY_TRANSITION_PAGE_REMOVAL (Pfn);
    }

    return;
}

VOID
MiUnlinkFreeOrZeroedPage (
    IN PFN_NUMBER Page
    )

/*++

Routine Description:

    This procedure removes a page from the middle of a list.  This is
    designed for the removing of free or zeroed pages from the middle of
    their lists.

Arguments:

    Page - Supplies a page frame index to remove from the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PMMPFNLIST ListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn2;
    PMMPFN Pfn;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    MMLISTS ListName;

    Pfn = MI_PFN_ELEMENT (Page);

    MM_PFN_LOCK_ASSERT();

    ListHead = MmPageLocationList[Pfn->u3.e1.PageLocation];
    ListName = ListHead->ListName;
    ASSERT (ListHead->Total != 0);
    ListHead->Total -= 1;

    ASSERT (ListName <= FreePageList);
    ASSERT (Pfn->u3.e1.WriteInProgress == 0);
    ASSERT (Pfn->u3.e1.ReadInProgress == 0);
    ASSERT (Pfn->u3.e1.CacheAttribute == MiNotMapped);

    //
    // If memory mirroring is in progress, any removals from the
    // free, zeroed, standby, modified or modified-no-write lists that
    // isn't immediately re-inserting into one of these 5 lists (WITHOUT
    // modifying the page contents) must update the bitmap.
    //

    if (MiMirroringActive == TRUE) {
        RtlSetBit (MiMirrorBitMap2, (ULONG)Page);
    }

    PERFINFO_UNLINKFREEPAGE (Page, Pfn->u3.e1.PageLocation);

    Next = Pfn->u1.Flink;
    Pfn->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Previous = Pfn->u2.Blink;
    Pfn->u2.Blink = 0;

    if (Next != MM_EMPTY_LIST) {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }
    else {
        ListHead->Blink = Previous;
    }

    if (Previous == MM_EMPTY_LIST) {
        ListHead->Flink = Next;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }

    //
    // We are removing a page from the middle of the free or zeroed page list.
    // The secondary color tables must be updated at this time.
    //

    Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, Pfn);

    //
    // Walk down the list and remove the page.
    //

    ColorHead = &MmFreePagesByColor[ListName][Color];
    Next = ColorHead->Flink;
    if (Next == Page) {
        ColorHead->Flink = (PFN_NUMBER) Pfn->OriginalPte.u.Long;
    }
    else {

        //
        // Walk the list to find the parent.
        //

        do {
            Pfn2 = MI_PFN_ELEMENT (Next);
            Next = (PFN_NUMBER) Pfn2->OriginalPte.u.Long;
            if (Page == Next) {
                Pfn2->OriginalPte.u.Long = Pfn->OriginalPte.u.Long;
                if ((PFN_NUMBER) Pfn->OriginalPte.u.Long == MM_EMPTY_LIST) {
                    ColorHead->Blink = Pfn2;
                }
                break;
            }
        } while (TRUE);
    }

    ASSERT (ColorHead->Count >= 1);
    ColorHead->Count -= 1;

    //
    // Decrement availability count.
    //

#if defined(MI_MULTINODE)

    if (KeNumberNodes > 1) {
        MI_NODE_FROM_COLOR(Color)->FreeCount[ListName]--;
    }

#endif

    MmAvailablePages -= 1;

    if (MmAvailablePages < MmMinimumFreePages) {

        //
        // Obtain free pages.
        //

        MiObtainFreePages();
    }

    return;
}


ULONG
FASTCALL
MiEnsureAvailablePageOrWait (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This procedure ensures that a physical page is available on
    the zeroed, free or standby list such that the next call the remove a
    page absolutely will not block.  This is necessary as blocking would
    require a wait which could cause a deadlock condition.

    If a page is available the function returns immediately with a value
    of FALSE indicating no wait operation was performed.  If no physical
    page is available, the thread enters a wait state and the function
    returns the value TRUE when the wait operation completes.

Arguments:

    Process - Supplies a pointer to the current process if, and only if,
              the working set mutex is held currently held and should
              be released if a wait operation is issued.  Supplies
              the value NULL otherwise.

    VirtualAddress - Supplies the virtual address for the faulting page.
                     If the value is NULL, the page is treated as a
                     user mode address.

Return Value:

    FALSE - if a page was immediately available.
    TRUE - if a wait operation occurred before a page became available.


Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PVOID Event;
    NTSTATUS Status;
    KIRQL OldIrql;
    KIRQL Ignore;
    ULONG Limit;
    ULONG Relock;
    PFN_NUMBER StrandedPages;
    LOGICAL WsHeldSafe;
    PMMPFN Pfn1;
    PMMPFN EndPfn;
    LARGE_INTEGER WaitBegin;
    LARGE_INTEGER WaitEnd;

    MM_PFN_LOCK_ASSERT();

    if (MmAvailablePages >= MM_HIGH_LIMIT) {

        //
        // Pages are available.
        //

        return FALSE;
    }

    //
    // If this fault is for paged pool (or pagable kernel space,
    // including page table pages), let it use the last page.
    //

#if defined(_IA64_)
    if (MI_IS_SYSTEM_ADDRESS(VirtualAddress) ||
        (MI_IS_HYPER_SPACE_ADDRESS(VirtualAddress))) {
#else
    if (((PMMPTE)VirtualAddress > MiGetPteAddress(HYPER_SPACE)) ||
        ((VirtualAddress > MM_HIGHEST_USER_ADDRESS) &&
         (VirtualAddress < (PVOID)PTE_BASE))) {
#endif

        //
        // This fault is in the system, use 1 page as the limit.
        //

        if (MmAvailablePages >= MM_LOW_LIMIT) {

            //
            // Pages are available.
            //

            return FALSE;
        }

        Limit = MM_LOW_LIMIT;
        Event = (PVOID)&MmAvailablePagesEvent;
    }
    else {

        //
        // If this thread has explicitly disabled APCs (FsRtlEnterFileSystem
        // does this), then it may be holding resources or mutexes that may in
        // turn be blocking memory making threads from making progress.
        //
        // Likewise give system threads a free pass as they may be worker
        // threads processing potentially blocking items drivers have queued.
        //

        if ((IS_SYSTEM_THREAD(PsGetCurrentThread())) ||
            (KeGetCurrentThread()->KernelApcDisable != 0)) {

            if (MmAvailablePages >= MM_MEDIUM_LIMIT) {

                //
                // Pages are available.
                //

                return FALSE;
            }

            Limit = MM_MEDIUM_LIMIT;
            Event = (PVOID) &MmAvailablePagesEventMedium;
        }
        else {
            Limit = MM_HIGH_LIMIT;
            Event = (PVOID) &MmAvailablePagesEventHigh;
        }
    }

    //
    // Initializing WsHeldSafe is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    WsHeldSafe = FALSE;

    while (MmAvailablePages < Limit) {

        KeClearEvent ((PKEVENT)Event);

        UNLOCK_PFN (APC_LEVEL);

        Relock = FALSE;

        if (Process == HYDRA_PROCESS) {
            UNLOCK_SESSION_SPACE_WS (APC_LEVEL);
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Handle both cases here and below.
            //

            UNLOCK_WS_REGARDLESS (Process, WsHeldSafe);
        }
        else {
            if (MmSystemLockOwner == PsGetCurrentThread()) {
                UNLOCK_SYSTEM_WS (APC_LEVEL);
                Relock = TRUE;
            }
        }

        KiQueryInterruptTime(&WaitBegin);

        //
        // Wait 7 minutes for pages to become available.
        //

        Status = KeWaitForSingleObject(Event,
                                       WrFreePage,
                                       KernelMode,
                                       FALSE,
                                       (PLARGE_INTEGER)&MmSevenMinutes);

        if (Status == STATUS_TIMEOUT) {

            KiQueryInterruptTime(&WaitEnd);

            if (MmSystemShutdown != 0) {

                //
                // Because applications are not terminated and drivers are
                // not unloaded, they can continue to access pages even after
                // the modified writer has terminated.  This can cause the
                // system to run out of pages since the pagefile(s) cannot be
                // used.
                //

                KeBugCheckEx (DISORDERLY_SHUTDOWN,
                              MmModifiedPageListHead.Total,
                              MmTotalPagesForPagingFile,
                              (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT) - MmAllocatedNonPagedPool,
                              MmSystemShutdown);
            }

            //
            // See how many transition pages have nonzero reference counts as
            // these indicate drivers that aren't unlocking the pages in their
            // MDLs.
            //

            Limit = 0;
            StrandedPages = 0;

            do {
        
                Pfn1 = MI_PFN_ELEMENT (MmPhysicalMemoryBlock->Run[Limit].BasePage);
                EndPfn = Pfn1 + MmPhysicalMemoryBlock->Run[Limit].PageCount;

                while (Pfn1 < EndPfn) {
                    if ((Pfn1->u3.e1.PageLocation == TransitionPage) &&
                        (Pfn1->u3.e2.ReferenceCount != 0)) {
                            StrandedPages += 1;
                    }
                    Pfn1 += 1;
                }
                Limit += 1;
        
            } while (Limit != MmPhysicalMemoryBlock->NumberOfRuns);

            //
            // This bugcheck can occur for the following reasons:
            //
            // A driver has blocked, deadlocking the modified or mapped
            // page writers.  Examples of this include mutex deadlocks or
            // accesses to paged out memory in filesystem drivers, filter
            // drivers, etc.  This indicates a driver bug.
            //
            // The storage driver(s) are not processing requests.  Examples
            // of this are stranded queues, non-responding drives, etc.  This
            // indicates a driver bug.
            //
            // Not enough pool is available for the storage stack to write out
            // modified pages.  This indicates a driver bug.
            //
            // A high priority realtime thread has starved the balance set
            // manager from trimming pages and/or starved the modified writer
            // from writing them out.  This indicates a bug in the component
            // that created this thread.
            //

            StrandedPages &= ~3;
            StrandedPages |= MmSystemShutdown;

            if (KdDebuggerNotPresent) {
                if (MmTotalPagesForPagingFile >= (MmModifiedPageListHead.Total >> 2)) {
                    KeBugCheckEx (NO_PAGES_AVAILABLE,
                                  MmModifiedPageListHead.Total,
                                  MmTotalPagesForPagingFile,
                                  (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT) - MmAllocatedNonPagedPool,
                                  StrandedPages);
                }
                KeBugCheckEx (DIRTY_MAPPED_PAGES_CONGESTION,
                              MmModifiedPageListHead.Total,
                              MmTotalPagesForPagingFile,
                              (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT) - MmAllocatedNonPagedPool,
                              StrandedPages);
            }

            DbgPrint ("MmEnsureAvailablePageOrWait: 7 min timeout %x %x %x %x\n", WaitEnd.HighPart, WaitEnd.LowPart, WaitBegin.HighPart, WaitBegin.LowPart);

            DbgPrint ("Without a debugger attached, the following bugcheck would have occurred.\n");
            if (MmTotalPagesForPagingFile >= (MmModifiedPageListHead.Total >> 2)) {
                DbgPrint ("%3lx ", NO_PAGES_AVAILABLE);
            }
            else {
                DbgPrint ("%3lxp ", DIRTY_MAPPED_PAGES_CONGESTION);
            }

            DbgPrint ("%p %p %p %p\n",
                      MmModifiedPageListHead.Total,
                      MmTotalPagesForPagingFile,
                      (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT) - MmAllocatedNonPagedPool,
                      StrandedPages);

            //
            // Pop into the debugger (even on free builds) to determine
            // the cause of the starvation and march on.
            //

            DbgBreakPoint ();
        }

        if (Process == HYDRA_PROCESS) {
            LOCK_SESSION_SPACE_WS (Ignore, PsGetCurrentThread ());
        }
        else if (Process != NULL) {

            //
            // The working set lock may have been acquired safely or unsafely
            // by our caller.  Reacquire it in the same manner our caller did.
            //

            LOCK_WS_REGARDLESS (Process, WsHeldSafe);
        }
        else {
            if (Relock) {
                LOCK_SYSTEM_WS (Ignore, PsGetCurrentThread ());
            }
        }

        LOCK_PFN (OldIrql);
    }

    return TRUE;
}


PFN_NUMBER
FASTCALL
MiRemoveZeroPage (
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a zero page from either the zeroed, free
    or standby lists (in that order).  If no pages exist on the zeroed
    or free list a transition page is removed from the standby list
    and the PTE (may be a prototype PTE) which refers to this page is
    changed from transition back to its original contents.

    If the page is not obtained from the zeroed list, it is zeroed.

Arguments:

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

            The above was true when we were concerned about caches
            which are virtually indexed (ie MIPS).  Today we
            are more concerned that we get a good usage spread across
            the L2 caches of most machines.  These caches are physically
            indexed.  By gathering pages that would have the same
            index to the same color, then maximizing the color spread,
            we maximize the effective use of the caches.

            This has been extended for NUMA machines.  The high part
            of the color gives the node color (basically node number).
            If we cannot allocate a page of the requested color, we
            try to allocate a page on the same node before taking a
            page from a different node.

Return Value:

    The physical page number removed from the specified list.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PFN_NUMBER Page;
    PMMPFN Pfn1;
    PMMCOLOR_TABLES FreePagesByColor;
#if MI_BARRIER_SUPPORTED
    ULONG BarrierStamp;
#endif
#if defined(MI_MULTINODE)
    PKNODE Node;
    ULONG NodeColor;
    ULONG OriginalColor;
#endif

    MM_PFN_LOCK_ASSERT();
    ASSERT(MmAvailablePages != 0);

    FreePagesByColor = MmFreePagesByColor[ZeroedPageList];

#if defined(MI_MULTINODE)

    //
    // Initializing Node is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    Node = NULL;

    NodeColor = Color & ~MmSecondaryColorMask;
    OriginalColor = Color;

    if (KeNumberNodes > 1) {
        Node = MI_NODE_FROM_COLOR(Color);
    }

    do {

#endif

        //
        // Attempt to remove a page from the zeroed page list. If a page
        // is available, then remove it and return its page frame index.
        // Otherwise, attempt to remove a page from the free page list or
        // the standby list.
        //
        // N.B. It is not necessary to change page colors even if the old
        //      color is not equal to the new color. The zero page thread
        //      ensures that all zeroed pages are removed from all caches.
        //

        ASSERT (Color < MmSecondaryColors);
        Page = FreePagesByColor[Color].Flink;

        if (Page != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the zeroed by color list.
            //

#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
#endif
            ASSERT ((Pfn1->u3.e1.PageLocation == ZeroedPageList) ||
                    ((Pfn1->u3.e1.PageLocation == FreePageList) &&
                     (FreePagesByColor == MmFreePagesByColor[FreePageList])));

            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

#if defined(MI_MULTINODE)

            if (FreePagesByColor != MmFreePagesByColor[ZeroedPageList]) {
                goto ZeroPage;
            }

#endif

            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            return Page;

        }

#if defined(MI_MULTINODE)

        //
        // If this is a multinode machine and there are zero
        // pages on this node, select another color on this 
        // node in preference to random selection.
        //

        if (KeNumberNodes > 1) {
            if (Node->FreeCount[ZeroedPageList] != 0) {
                Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
                ASSERT(Color != OriginalColor);
                continue;
            }

            //
            // No previously zeroed page with the specified secondary
            // color exists.  Since this is a multinode machine, zero
            // an available local free page now instead of allocating a
            // zeroed page from another node below.
            //

            if (Node->FreeCount[FreePageList] != 0) {
                if (FreePagesByColor != MmFreePagesByColor[FreePageList]) {
                    FreePagesByColor = MmFreePagesByColor[FreePageList];
                    Color = OriginalColor;
                }
                else {
                    Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
                    ASSERT(Color != OriginalColor);
                }
                continue;
            }
        }

        break;
    } while (TRUE);

#endif

    //
    // No previously zeroed page with the specified secondary color exists.
    // Try a zeroed page of any color.
    //

    Page = MmZeroedPageListHead.Flink;
    if (Page != MM_EMPTY_LIST) {
#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        ASSERT (Pfn1->u3.e1.PageLocation == ZeroedPageList);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

        return Page;
    }

    //
    // No zeroed page of the primary color exists, try a free page of the
    // secondary color.  Note in the multinode case this has already been done
    // above.
    //

#if defined(MI_MULTINODE)
    if (KeNumberNodes <= 1) {
#endif
        FreePagesByColor = MmFreePagesByColor[FreePageList];
    
        Page = FreePagesByColor[Color].Flink;
        if (Page != MM_EMPTY_LIST) {
    
            //
            // Remove the first entry on the free list by color.
            //
    
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
    
            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            goto ZeroPage;
        }
#if defined(MI_MULTINODE)
    }
#endif

    Page = MmFreePageListHead.Flink;
    if (Page != MM_EMPTY_LIST) {

        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));
#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
#endif
        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        goto ZeroPage;
    }

    ASSERT (MmZeroedPageListHead.Total == 0);
    ASSERT (MmFreePageListHead.Total == 0);

    //
    // Remove a page from the standby list and restore the original
    // contents of the PTE to free the last reference to the physical
    // page.
    //

    ASSERT (MmStandbyPageListHead.Total != 0);

    Page = MiRemovePageFromList (&MmStandbyPageListHead);
    ASSERT ((MI_PFN_ELEMENT(Page))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
    ASSERT (MI_PFN_ELEMENT(Page)->u3.e1.CacheAttribute == MiNotMapped);
    MiStandbyRemoved += 1;

    //
    // If memory mirroring is in progress, any removals from the
    // free, zeroed, standby, modified or modified-no-write lists that
    // isn't immediately re-inserting into one of these 5 lists (WITHOUT
    // modifying the page contents) must update the bitmap.
    //

    if (MiMirroringActive == TRUE) {
        RtlSetBit (MiMirrorBitMap2, (ULONG)Page);
    }

    //
    // Zero the page removed from the free or standby list.
    //

ZeroPage:

    Pfn1 = MI_PFN_ELEMENT(Page);

    MiZeroPhysicalPage (Page, 0);

#if MI_BARRIER_SUPPORTED

    //
    // Note the stamping must occur after the page is zeroed.
    //

    MI_BARRIER_STAMP_ZEROED_PAGE (&BarrierStamp);
    Pfn1->u4.PteFrame = BarrierStamp;

#endif

    ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u2.ShareCount == 0);
    ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

    return Page;
}

PFN_NUMBER
FASTCALL
MiRemoveAnyPage (
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a page from either the free, zeroed,
    or standby lists (in that order).  If no pages exist on the zeroed
    or free list a transition page is removed from the standby list
    and the PTE (may be a prototype PTE) which refers to this page is
    changed from transition back to its original contents.

    Note pages MUST exist to satisfy this request.  The caller ensures this
    by first calling MiEnsureAvailablePageOrWait.

Arguments:

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

            The above was true when we were concerned about caches
            which are virtually indexed.   (eg MIPS).   Today we
            are more concerned that we get a good usage spread across
            the L2 caches of most machines.  These caches are physically
            indexed.   By gathering pages that would have the same
            index to the same color, then maximizing the color spread,
            we maximize the effective use of the caches.

            This has been extended for NUMA machines.   The high part
            of the color gives the node color (basically node number).
            If we cannot allocate a page of the requested color, we
            try to allocate a page on the same node before taking a
            page from a different node.

Return Value:

    The physical page number removed from the specified list.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PFN_NUMBER Page;
#if DBG
    PMMPFN Pfn1;
#endif
#if defined(MI_MULTINODE)
    PKNODE Node;
    ULONG NodeColor;
    ULONG OriginalColor;
    PFN_NUMBER LocalNodePagesAvailable;
#endif

    MM_PFN_LOCK_ASSERT();
    ASSERT(MmAvailablePages != 0);

#if defined(MI_MULTINODE)

    //
    // Bias color to memory node.  The assumption is that if memory
    // of the correct color is not available on this node, it is
    // better to choose memory of a different color if you can stay
    // on this node.
    //

    LocalNodePagesAvailable = 0;
    NodeColor = Color & ~MmSecondaryColorMask;
    OriginalColor = Color;

    if (KeNumberNodes > 1) {
        Node = MI_NODE_FROM_COLOR(Color);
        LocalNodePagesAvailable = (Node->FreeCount[ZeroedPageList] | Node->FreeCount[ZeroedPageList]);
    }

    do {

#endif

        //
        // Check the free page list, and if a page is available
        // remove it and return its value.
        //

        ASSERT (Color < MmSecondaryColors);
        if (MmFreePagesByColor[FreePageList][Color].Flink != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the free by color list.
            //

            Page = MmFreePagesByColor[FreePageList][Color].Flink;
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
            ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                
            return Page;
        }

        //
        // Try the zero page list by primary color.
        //

        if (MmFreePagesByColor[ZeroedPageList][Color].Flink
                                                        != MM_EMPTY_LIST) {

            //
            // Remove the first entry on the zeroed by color list.
            //

            Page = MmFreePagesByColor[ZeroedPageList][Color].Flink;
#if DBG
            Pfn1 = MI_PFN_ELEMENT(Page);
#endif
            ASSERT (Pfn1->u3.e1.PageLocation == ZeroedPageList);
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

            Page = MiRemovePageByColor (Page, Color);

            ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
            ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
                
            return Page;
        }

        //
        // If this is a multinode machine and there are free
        // pages on this node, select another color on this 
        // node in preference to random selection.
        //

#if defined(MI_MULTINODE)

        if (LocalNodePagesAvailable != 0) {
            Color = ((Color + 1) & MmSecondaryColorMask) | NodeColor;
            ASSERT(Color != OriginalColor);
            continue;
        }

        break;
    } while (TRUE);

#endif

    //
    // Check the free page list, and if a page is available
    // remove it and return its value.
    //

    if (MmFreePageListHead.Flink != MM_EMPTY_LIST) {
        Page = MmFreePageListHead.Flink;
        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        ASSERT (Pfn1->u3.e1.PageLocation == FreePageList);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

        return Page;
    }
    ASSERT (MmFreePageListHead.Total == 0);

    //
    // Check the zeroed page list, and if a page is available
    // remove it and return its value.
    //

    if (MmZeroedPageListHead.Flink != MM_EMPTY_LIST) {
        Page = MmZeroedPageListHead.Flink;
        Color = MI_GET_COLOR_FROM_LIST_ENTRY(Page, MI_PFN_ELEMENT(Page));

#if DBG
        Pfn1 = MI_PFN_ELEMENT(Page);
#endif
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        Page = MiRemovePageByColor (Page, Color);

        ASSERT (Pfn1 == MI_PFN_ELEMENT(Page));
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

        return Page;
    }
    ASSERT (MmZeroedPageListHead.Total == 0);

    //
    // No pages exist on the free or zeroed list, use the standby list.
    //

    ASSERT(MmStandbyPageListHead.Total != 0);

    Page = MiRemovePageFromList (&MmStandbyPageListHead);
    ASSERT ((MI_PFN_ELEMENT(Page))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
    ASSERT ((MI_PFN_ELEMENT(Page))->u3.e1.CacheAttribute == MiNotMapped);
    MiStandbyRemoved += 1;

    //
    // If memory mirroring is in progress, any removals from the
    // free, zeroed, standby, modified or modified-no-write lists that
    // isn't immediately re-inserting into one of these 5 lists (WITHOUT
    // modifying the page contents) must update the bitmap.
    //

    if (MiMirroringActive == TRUE) {
        RtlSetBit (MiMirrorBitMap2, (ULONG)Page);
    }

    MI_CHECK_PAGE_ALIGNMENT(Page, Color & MM_COLOR_MASK);
#if DBG
    Pfn1 = MI_PFN_ELEMENT (Page);
#endif
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    ASSERT (Pfn1->u2.ShareCount == 0);

    return Page;
}


PFN_NUMBER
FASTCALL
MiRemovePageByColor (
    IN PFN_NUMBER Page,
    IN ULONG Color
    )

/*++

Routine Description:

    This procedure removes a page from the middle of the free or
    zeroed page list.

Arguments:

    Page - Supplies the physical page number to unlink from the list.

    Color - Supplies the page color for which this page is destined.
            This is used for checking virtual address alignments to
            determine if the D cache needs flushing before the page
            can be reused.

Return Value:

    The page frame number that was unlinked (always equal to the one
    passed in, but returned so the caller's fastcall sequences save
    extra register pushes and pops.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PMMPFNLIST ListHead;
    PMMPFNLIST PrimaryListHead;
    PFN_NUMBER Previous;
    PFN_NUMBER Next;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    ULONG NodeColor;
    MMLISTS ListName;
    PMMCOLOR_TABLES ColorHead;

    MM_PFN_LOCK_ASSERT();

    Pfn1 = MI_PFN_ELEMENT (Page);
    NodeColor = Pfn1->u3.e1.PageColor;

#if defined(MI_MULTINODE)

    ASSERT (NodeColor == (Color >> MmSecondaryColorNodeShift));

#endif

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        MiLogPfnInformation (Pfn1, PERFINFO_LOG_TYPE_REMOVEPAGEBYCOLOR);
    }

    //
    // If memory mirroring is in progress, any additions or removals to the
    // free, zeroed, standby, modified or modified-no-write lists must
    // update the bitmap.
    //

    if (MiMirroringActive == TRUE) {
        RtlSetBit (MiMirrorBitMap2, (ULONG)Page);
    }

    ListHead = MmPageLocationList[Pfn1->u3.e1.PageLocation];
    ListName = ListHead->ListName;

    ListHead->Total -= 1;

    PrimaryListHead = ListHead;

    Next = Pfn1->u1.Flink;
    Pfn1->u1.Flink = 0;         // Assumes Flink width is >= WsIndex width
    Previous = Pfn1->u2.Blink;
    Pfn1->u2.Blink = 0;

    if (Next == MM_EMPTY_LIST) {
        PrimaryListHead->Blink = Previous;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Next);
        Pfn2->u2.Blink = Previous;
    }

    if (Previous == MM_EMPTY_LIST) {
        PrimaryListHead->Flink = Next;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT(Previous);
        Pfn2->u1.Flink = Next;
    }

    ASSERT (Pfn1->u3.e1.RemovalRequested == 0);

    //
    // Zero the flags longword, but keep the color and cache information.
    //

    ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
    ASSERT (Pfn1->u3.e1.Rom == 0);
    Pfn1->u3.e2.ShortFlags = 0;
    Pfn1->u3.e1.PageColor = NodeColor;
    Pfn1->u3.e1.CacheAttribute = MiNotMapped;

    //
    // Update the color lists.
    //

    ASSERT (Color < MmSecondaryColors);
    ColorHead = &MmFreePagesByColor[ListName][Color];
    ColorHead->Flink = (PFN_NUMBER) Pfn1->OriginalPte.u.Long;
    ASSERT (ColorHead->Count >= 1);
    ColorHead->Count -= 1;

    //
    // Note that we now have one less page available.
    //

#if defined(MI_MULTINODE)
    if (KeNumberNodes > 1) {
        KeNodeBlock[NodeColor]->FreeCount[ListName]--;
    }
#endif

    MmAvailablePages -= 1;

    if (MmAvailablePages < MmMinimumFreePages) {

        //
        // Obtain free pages.
        //

        MiObtainFreePages();
    }

    return Page;
}


VOID
FASTCALL
MiInsertFrontModifiedNoWrite (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure inserts a page at the FRONT of the modified no write list.

Arguments:

    PageFrameIndex - Supplies the physical page number to insert in the list.

Return Value:

    None.

Environment:

    Must be holding the PFN database lock with APCs disabled.

--*/

{
    PFN_NUMBER first;
    PMMPFN Pfn1;
    PMMPFN Pfn2;

    MM_PFN_LOCK_ASSERT();
    ASSERT ((PageFrameIndex != 0) && (PageFrameIndex <= MmHighestPhysicalPage) &&
        (PageFrameIndex >= MmLowestPhysicalPage));

    //
    // Check to ensure the reference count for the page is zero.
    //

    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

    MI_SNAP_DATA (Pfn1, Pfn1->PteAddress, 0xA);

    ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    MmModifiedNoWritePageListHead.Total += 1;  // One more page on the list.

    MI_TALLY_TRANSITION_PAGE_ADDITION (Pfn1);

    first = MmModifiedNoWritePageListHead.Flink;
    if (first == MM_EMPTY_LIST) {

        //
        // List is empty add the page to the ListHead.
        //

        MmModifiedNoWritePageListHead.Blink = PageFrameIndex;
    }
    else {
        Pfn2 = MI_PFN_ELEMENT (first);
        Pfn2->u2.Blink = PageFrameIndex;
    }

    MmModifiedNoWritePageListHead.Flink = PageFrameIndex;
    Pfn1->u1.Flink = first;
    Pfn1->u2.Blink = MM_EMPTY_LIST;
    Pfn1->u3.e1.PageLocation = ModifiedNoWritePageList;
    return;
}

PFN_NUMBER
MiAllocatePfn (
    IN PMMPTE PointerPte,
    IN ULONG Protection
    )

/*++

Routine Description:

    This procedure allocates and initializes a page of memory.

Arguments:

    PointerPte - Supplies the PTE to initialize.

Return Value:

    The page frame index allocated.

Environment:

    Kernel mode.

--*/
{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;

    LOCK_PFN (OldIrql);

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    PointerPte->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

    PointerPte->u.Soft.Protection |= Protection;

    MiInitializePfn (PageFrameIndex, PointerPte, 1);

    UNLOCK_PFN (OldIrql);

    return PageFrameIndex;
}

VOID
FASTCALL
MiLogPfnInformation (
    IN PMMPFN Pfn1,
    IN USHORT Reason
    )
{
    MMPFN_IDENTITY PfnIdentity;

    RtlZeroMemory (&PfnIdentity, sizeof(PfnIdentity));

    if (Reason == PERFINFO_LOG_TYPE_INSERTINFREELIST) {
        MI_MARK_PFN_UNDELETED (Pfn1);
    }

    MiIdentifyPfn(Pfn1, &PfnIdentity);

    PerfInfoLogBytes (Reason, &PfnIdentity, sizeof(PfnIdentity));

    if (Reason == PERFINFO_LOG_TYPE_INSERTINFREELIST) {
        MI_SET_PFN_DELETED (Pfn1);
    }
}

VOID
MiPurgeTransitionList (
    VOID
    )
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;

    //
    // Run the transition list and free all the entries so transition
    // faults are not satisfied for any of the non modified pages that were
    // freed.
    //

    LOCK_PFN (OldIrql);

    while (MmStandbyPageListHead.Total != 0) {

        PageFrameIndex = MiRemovePageFromList (&MmStandbyPageListHead);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

        Pfn1->u3.e2.ReferenceCount += 1;
        Pfn1->OriginalPte = ZeroPte;

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementReferenceCount (PageFrameIndex);

        //
        // If memory mirroring is in progress, any removal from
        // the standby, modified or modified-no-write lists that isn't
        // immediately re-inserting in one of these 3 lists must
        // update the bitmap.
        //

        if (MiMirroringActive == TRUE) {
            RtlSetBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
        }
    }

    UNLOCK_PFN (OldIrql);
    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\pfsup.c ===
/*++

Copyright (c) 1999 Microsoft Corporation

Module Name:

    pfsup.c

Abstract:

    This module contains the Mm support routines for prefetching groups of pages
    from secondary storage.

    The caller builds a list of various file objects and logical block offsets,
    passing them to MmPrefetchPages.  The code here then examines the
    internal pages, reading in those that are not already valid or in
    transition.  These pages are read with a single read, using a dummy page
    to bridge small gaps.  If the gap is "large", then separate reads are
    issued.

    Upon conclusion of all the I/Os, control is returned to the calling
    thread, and any pages that needed to be read are placed in transition
    within the prototype PTE-managed segments.  Thus any future references
    to these pages should result in soft faults only, provided these pages
    do not themselves get trimmed under memory pressure.

Author:

    Landy Wang (landyw) 09-Jul-1999

Revision History:

--*/

#include "mi.h"

#if DBG

ULONG MiPfDebug;

#define MI_PF_FORCE_PREFETCH    0x1     // Trim all user pages to force prefetch
#define MI_PF_DELAY             0x2     // Delay hoping to trigger collisions
#define MI_PF_VERBOSE           0x4     // Verbose printing
#define MI_PF_PRINT_ERRORS      0x8     // Print to debugger on errors

#endif

//
// If an MDL contains DUMMY_RATIO times as many dummy pages as real pages
// then don't bother with the read.
//

#define DUMMY_RATIO 16

//
// If two consecutive read-list entries are more than "seek threshold"
// distance apart, the read-list is split between these entries.  Otherwise
// the dummy page is used for the gap and only one MDL is used.
//

#define SEEK_THRESHOLD ((128 * 1024) / PAGE_SIZE)

//
// Minimum number of pages to prefetch per section.
//

#define MINIMUM_READ_LIST_PAGES 1

//
// If at least this many available physical pages, then attempt prefetch.
//

#define MINIMUM_AVAILABLE_PAGES MM_HIGH_LIMIT

//
// Read-list structures.
//

typedef struct _RLETYPE {
    ULONG_PTR Partial : 1;          // This entry is a partial page.
    ULONG_PTR NewSubsection : 1;    // This entry starts in the next subsection.
    ULONG_PTR DontUse : 30;
} RLETYPE;

typedef struct _MI_READ_LIST_ENTRY {

    union {
        PMMPTE PrototypePte;
        RLETYPE e1;
    } u1;

} MI_READ_LIST_ENTRY, *PMI_READ_LIST_ENTRY;

#define MI_RLEPROTO_BITS        3

#define MI_RLEPROTO_TO_PROTO(ProtoPte) ((PMMPTE)((ULONG_PTR)ProtoPte & ~MI_RLEPROTO_BITS))

typedef struct _MI_READ_LIST {

    PCONTROL_AREA ControlArea;
    PFILE_OBJECT FileObject;
    ULONG LastPteOffsetReferenced;

    //
    // Note that entries are chained through the inpage support blocks from
    // this listhead.  This list is not protected by interlocks because it is
    // only accessed by the owning thread.  Inpage blocks _ARE_ accessed with
    // interlocks when they are inserted or removed from the memory management
    // freelists, but by the time they get to this module they are decoupled.
    //

    SINGLE_LIST_ENTRY InPageSupportHead;

    MI_READ_LIST_ENTRY List[ANYSIZE_ARRAY];

} MI_READ_LIST, *PMI_READ_LIST;

VOID
MiPfReleaseSubsectionReferences (
    IN PMI_READ_LIST MiReadList
    );

VOID
MiPfFreeDummyPage (
    IN PMMPFN DummyPagePfn
    );

NTSTATUS
MiPfPrepareReadList (
    IN PREAD_LIST ReadList,
    OUT PMI_READ_LIST *OutMiReadList
    );

NTSTATUS
MiPfPutPagesInTransition (
    IN PMI_READ_LIST ReadList,
    IN OUT PMMPFN *DummyPagePfn
    );

VOID
MiPfExecuteReadList (
    IN PMI_READ_LIST ReadList
    );

VOID
MiPfCompletePrefetchIos (
    PMI_READ_LIST ReadList
    );

#if DBG
VOID
MiPfDbgDumpReadList (
    IN PMI_READ_LIST ReadList
    );

VOID
MiRemoveUserPages (
    VOID
    );
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text (PAGE, MmPrefetchPages)
#pragma alloc_text (PAGE, MiPfPrepareReadList)
#pragma alloc_text (PAGE, MiPfExecuteReadList)
#pragma alloc_text (PAGE, MiPfReleaseSubsectionReferences)
#endif


NTSTATUS
MmPrefetchPages (
    IN ULONG NumberOfLists,
    IN PREAD_LIST *ReadLists
    )

/*++

Routine Description:

    This routine reads pages described in the read-lists in the optimal fashion.

    This is the only externally callable prefetch routine. No component
    should use this interface except the cache manager.

Arguments:

    NumberOfLists - Supplies the number of read-lists.

    ReadLists - Supplies an array of read-lists.
    
Return Value:

    NTSTATUS codes.

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    PMI_READ_LIST *MiReadLists;
    PMMPFN DummyPagePfn;
    NTSTATUS status;
    ULONG i;
    KIRQL OldIrql;
    LOGICAL ReadBuilt;
    LOGICAL ApcNeeded;
    PETHREAD CurrentThread;
    NTSTATUS CauseOfReadBuildFailures;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // Allocate memory for internal Mi read-lists.
    //

    MiReadLists = (PMI_READ_LIST *) ExAllocatePoolWithTag (
        NonPagedPool,
        sizeof (PMI_READ_LIST) * NumberOfLists,
        'lRmM'
        );

    if (MiReadLists == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    ReadBuilt = FALSE;
    CauseOfReadBuildFailures = STATUS_SUCCESS;

    //
    // Prepare read-lists (determine runs and allocate MDLs).
    //

    for (i = 0; i < NumberOfLists; i += 1) {

        //
        // Note any non-null list is referenced by this call so this routine
        // must dereference it when done to re-enable dynamic prototype PTEs.
        //

        status = MiPfPrepareReadList (ReadLists[i], &MiReadLists[i]);

        //
        // MiPfPrepareReadList never returns half-formed inpage support
        // blocks and MDLs.  Either nothing is returned, partial lists are
        // returned or a complete list is returned.  Any non-null list
        // can therefore be processed.
        //

        if (NT_SUCCESS (status)) {
            if (MiReadLists[i] != NULL) {
                ASSERT (MiReadLists[i]->InPageSupportHead.Next != NULL);
                ReadBuilt = TRUE;
            }
        }
        else {
            CauseOfReadBuildFailures = status;
        }
    }

    if (ReadBuilt == FALSE) {

        //
        // No lists were created so nothing further needs to be done.
        // CauseOfReadBuildFailures tells us whether this was due to all
        // the desired pages already being resident or that resources to
        // build the request could not be allocated.
        //

        ExFreePool (MiReadLists);

        if (CauseOfReadBuildFailures != STATUS_SUCCESS) {
            return CauseOfReadBuildFailures;
        }

        //
        // All the pages the caller asked for are already resident.
        //

        return STATUS_SUCCESS;
    }

    //
    // APCs must be disabled once we put a page in transition.  Otherwise
    // a thread suspend will stop us from issuing the I/O - this will hang
    // any other threads that need the same page.
    //

    CurrentThread = PsGetCurrentThread();
    ApcNeeded = FALSE;

    ASSERT ((PKTHREAD)CurrentThread == KeGetCurrentThread ());
    KeEnterCriticalRegionThread ((PKTHREAD)CurrentThread);

    //
    // The nested fault count protects this thread from deadlocks where a
    // special kernel APC fires and references the same user page(s) we are
    // putting in transition.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    CurrentThread->NestedFaultCount += 1;
    KeLowerIrql (OldIrql);

    //
    // Allocate physical memory.
    //

    DummyPagePfn = NULL;
    ReadBuilt = FALSE;
    CauseOfReadBuildFailures = STATUS_SUCCESS;

#if DBG
    status = 0xC0033333;
#endif

    for (i = 0; i < NumberOfLists; i += 1) {

        if ((MiReadLists[i] != NULL) &&
            (MiReadLists[i]->InPageSupportHead.Next != NULL)) {

            status = MiPfPutPagesInTransition (MiReadLists[i], &DummyPagePfn);

            if (NT_SUCCESS (status)) {
                if (MiReadLists[i]->InPageSupportHead.Next != NULL) {
                    ReadBuilt = TRUE;

                    //
                    // Issue I/Os.
                    //
    
                    MiPfExecuteReadList (MiReadLists[i]);
                }
                else {
                    MiPfReleaseSubsectionReferences (MiReadLists[i]);
                    ExFreePool (MiReadLists[i]);
                    MiReadLists[i] = NULL;
                }
            }
            else {

                CauseOfReadBuildFailures = status;

                //
                // If not even a single page is available then don't bother
                // trying to prefetch anything else.
                //

                for (; i < NumberOfLists; i += 1) {
                    if (MiReadLists[i] != NULL) {
                        MiPfReleaseSubsectionReferences (MiReadLists[i]);
                        ExFreePool (MiReadLists[i]);
                        MiReadLists[i] = NULL;
                    }
                }

                break;
            }
        }
    }

    //
    // At least one call to MiPfPutPagesInTransition was made, which
    // sets status properly.
    //

    ASSERT (status != 0xC0033333);

    if (ReadBuilt == TRUE) {

        status = STATUS_SUCCESS;

        //
        // Wait for I/Os to complete. Note APCs must remain disabled.
        //

        for (i = 0; i < NumberOfLists; i += 1) {
    
            if (MiReadLists[i] != NULL) {
    
                ASSERT (MiReadLists[i]->InPageSupportHead.Next != NULL);
    
                MiPfCompletePrefetchIos (MiReadLists[i]);

                MiPfReleaseSubsectionReferences (MiReadLists[i]);
            }
        }
    }
    else {

        //
        // No reads were issued.
        //
        // CauseOfReadBuildFailures tells us whether this was due to all
        // the desired pages already being resident or that resources to
        // build the request could not be allocated.
        //

        status = CauseOfReadBuildFailures;
    }

    //
    // Put DummyPage back on the free list.
    //

    if (DummyPagePfn != NULL) {
        MiPfFreeDummyPage (DummyPagePfn);
    }

    //
    // Only when all the I/Os have been completed (not just issued) can
    // APCs be re-enabled.  This prevents a user-issued suspend APC from
    // keeping a shared page in transition forever.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);

    ASSERT (CurrentThread->NestedFaultCount == 1);

    CurrentThread->NestedFaultCount -= 1;

    if (CurrentThread->ApcNeeded == 1) {
        ApcNeeded = TRUE;
        CurrentThread->ApcNeeded = 0;
    }

    KeLowerIrql (OldIrql);

    KeLeaveCriticalRegionThread ((PKTHREAD)CurrentThread);

    for (i = 0; i < NumberOfLists; i += 1) {
        if (MiReadLists[i] != NULL) {
            ExFreePool (MiReadLists[i]);
        }
    }

    ExFreePool (MiReadLists);

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    ASSERT (CurrentThread->ApcNeeded == 0);

    if (ApcNeeded == TRUE) {
        KeRaiseIrql (APC_LEVEL, &OldIrql);
        IoRetryIrpCompletions ();
        KeLowerIrql (OldIrql);
    }

    return status;
}

VOID
MiPfFreeDummyPage (
    IN PMMPFN DummyPagePfn
    )

/*++

Routine Description:

    This nonpaged wrapper routine frees the dummy page PFN.

Arguments:

    DummyPagePfn - Supplies the dummy page PFN.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;

    PageFrameIndex = DummyPagePfn - MmPfnDatabase;

    LOCK_PFN (OldIrql);

    ASSERT (DummyPagePfn->u2.ShareCount == 1);
    ASSERT (DummyPagePfn->u3.e1.PrototypePte == 0);
    ASSERT (DummyPagePfn->OriginalPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE);

    ASSERT (DummyPagePfn->u3.e2.ReferenceCount == 2);
    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(DummyPagePfn, 17);

    //
    // Clear the read in progress bit as this page may never have used for an
    // I/O after all.  The inpage error bit must also be cleared as any number
    // of errors may have occurred during reads of pages (that were immaterial
    // anyway).
    //

    DummyPagePfn->u3.e1.ReadInProgress = 0;
    DummyPagePfn->u4.InPageError = 0;

    MI_SET_PFN_DELETED (DummyPagePfn);

    MiDecrementShareCount (PageFrameIndex);

    UNLOCK_PFN (OldIrql);
}

VOID
MiMovePageToEndOfStandbyList(
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This nonpaged routine obtains the PFN lock and moves a page to the end of
    the standby list (if the page is still in transition).

Arguments:

    PointerPte - Supplies the prototype PTE to examine.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock not held.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;

    LOCK_PFN (OldIrql);

    if (!MmIsAddressValid (PointerPte)) {

        //
        // If the paged pool containing the prototype PTE is not resident
        // then the actual page itself may still be transition or not.  This
        // should be so rare it's not worth making the pool resident so the
        // proper checks can be applied.  Just bail.
        //

        UNLOCK_PFN (OldIrql);
        return;
    }

    PteContents = *PointerPte;

    if ((PteContents.u.Hard.Valid == 0) &&
        (PteContents.u.Soft.Prototype == 0) &&
        (PteContents.u.Soft.Transition == 1)) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // The page is still in transition, move it to the end to protect it
        // from possible cannibalization.  Note that if the page is currently
        // being written to disk it will be on the modified list and when the
        // write completes it will automatically go to the end of the standby
        // list anyway so skip those.
        //

        if (Pfn1->u3.e1.PageLocation == StandbyPageList) {
            MiUnlinkPageFromList (Pfn1);
            MiInsertPageInList (&MmStandbyPageListHead, PageFrameIndex);
        }
    }

    UNLOCK_PFN (OldIrql);
}

VOID
MiPfReleaseSubsectionReferences (
    IN PMI_READ_LIST MiReadList
    )

/*++

Routine Description:

    This routine releases reference counts on subsections examined by the
    prefetch scanner.

Arguments:

    MiReadList - Supplies a read-list entry.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PMSUBSECTION MappedSubsection;
    PCONTROL_AREA ControlArea;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    ControlArea = MiReadList->ControlArea;

    ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);
    ASSERT (ControlArea->FilePointer != NULL);

    //
    // Image files don't have dynamic prototype PTEs.
    //

    if (ControlArea->u.Flags.Image == 1) {
        return;
    }

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    MappedSubsection = (PMSUBSECTION)(ControlArea + 1);

    MiRemoveViewsFromSectionWithPfn (MappedSubsection,
                                     MiReadList->LastPteOffsetReferenced);
}


NTSTATUS
MiPfPrepareReadList (
    IN PREAD_LIST ReadList,
    OUT PMI_READ_LIST *OutMiReadList
    )

/*++

Routine Description:

    This routine constructs MDLs that describe the pages in the argument
    read-list. The caller will then issue the I/Os on return.

Arguments:

    ReadList - Supplies the read-list.

    OutMiReadList - Supplies a pointer to receive the Mi readlist.

Return Value:

    Various NTSTATUS codes.

    If STATUS_SUCCESS is returned, OutMiReadList is set to a pointer to an Mi
    readlist to be used for prefetching or NULL if no prefetching is needed.

    If OutMireadList is non-NULL (on success only) then the caller must call
    MiRemoveViewsFromSectionWithPfn (VeryFirstSubsection, LastPteOffsetReferenced) for data files.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    ULONG LastPteOffset;
    NTSTATUS Status;
    MMPTE PteContents;
    PMMPTE LocalPrototypePte;
    PMMPTE LastPrototypePte;
    PMMPTE StartPrototypePte;
    PMMPTE EndPrototypePte;
    PMI_READ_LIST MiReadList;
    PMI_READ_LIST_ENTRY Rle;
    PMI_READ_LIST_ENTRY StartRleRun;
    PMI_READ_LIST_ENTRY EndRleRun;
    PMI_READ_LIST_ENTRY RleMax;
    PMI_READ_LIST_ENTRY FirstRleInRun;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PSUBSECTION PreviousSubsection;
    PMSUBSECTION VeryFirstSubsection;
    PMSUBSECTION VeryLastSubsection;
    UINT64 StartOffset;
    LARGE_INTEGER EndQuad;
    UINT64 EndOffset;
    UINT64 FileOffset;
    PMMINPAGE_SUPPORT InPageSupport;
    PMDL Mdl;
    ULONG i;
    PFN_NUMBER NumberOfPages;
    UINT64 StartingOffset;
    UINT64 TempOffset;
    ULONG ReadSize;
    ULONG NumberOfEntries;
#if DBG
    PPFN_NUMBER Page;
#endif

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    *OutMiReadList = NULL;

    //
    // Create an Mi readlist from the argument Cc readlist.
    //

    NumberOfEntries = ReadList->NumberOfEntries;

    MiReadList = (PMI_READ_LIST) ExAllocatePoolWithTag (
        NonPagedPool,
        sizeof (MI_READ_LIST) + NumberOfEntries * sizeof (MI_READ_LIST_ENTRY),
        'lRmM');

    if (MiReadList == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Translate the section object into the relevant control area.
    //

    if (ReadList->IsImage) {
        ControlArea = (PCONTROL_AREA)ReadList->FileObject->SectionObjectPointer->ImageSectionObject;
        ASSERT (ControlArea != NULL );
        ASSERT (ControlArea->u.Flags.Image == 1);
    }
    else {
        ControlArea = (PCONTROL_AREA)ReadList->FileObject->SectionObjectPointer->DataSectionObject;
    }

    //
    // If the section is backed by a ROM, then there's no need to prefetch
    // anything as it would waste RAM.
    //

    if (ControlArea->u.Flags.Rom == 1) {
        ExFreePool (MiReadList);
        return STATUS_SUCCESS;
    }

    //
    // Make sure the section is really prefetchable - physical and
    // pagefile-backed sections are not.
    //

    if ((ControlArea->u.Flags.PhysicalMemory) ||
         (ControlArea->FilePointer == NULL)) {
        ExFreePool (MiReadList);
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Initialize the internal Mi readlist.
    //

    MiReadList->ControlArea = ControlArea;
    MiReadList->FileObject = ReadList->FileObject;
    MiReadList->InPageSupportHead.Next = NULL;

    RtlZeroMemory (MiReadList->List,
                   sizeof (MI_READ_LIST_ENTRY) * NumberOfEntries);

    //
    // Copy pages from the Cc readlists to the internal Mi readlists.
    //

    NumberOfPages = 0;
    FirstRleInRun = NULL;
    VeryFirstSubsection = NULL;
    VeryLastSubsection = NULL;
    LastPteOffset = 0;

    if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);

        //
        // Ensure all prototype PTE bases are valid for all subsections of the
        // requested file so the traversal code doesn't have to check
        // everywhere.  As long as the files are not too large this should
        // be a cheap operation.
        //

        if (ControlArea->u.Flags.Image == 0) {
            ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);
            ASSERT (ControlArea->FilePointer != NULL);

            VeryFirstSubsection = (PMSUBSECTION) Subsection;
            VeryLastSubsection = (PMSUBSECTION) Subsection;

            do {

                //
                // A memory barrier is needed to read the subsection chains
                // in order to ensure the writes to the actual individual
                // subsection data structure fields are visible in correct
                // order.  This avoids the need to acquire any stronger
                // synchronization (ie: PFN lock), thus yielding better
                // performance and pagability.
                //

                KeMemoryBarrier ();

                LastPteOffset += VeryLastSubsection->PtesInSubsection;
                if (VeryLastSubsection->NextSubsection == NULL) {
                    break;
                }
                VeryLastSubsection = (PMSUBSECTION) VeryLastSubsection->NextSubsection;
            } while (TRUE);

            MiReadList->LastPteOffsetReferenced = LastPteOffset;

            Status = MiAddViewsForSectionWithPfn (VeryFirstSubsection,
                                                  LastPteOffset);

            if (!NT_SUCCESS (Status)) {
                ExFreePool (MiReadList);
                return Status;
            }
        }
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    StartOffset = (UINT64)MiStartingOffset (Subsection, Subsection->SubsectionBase);
    EndQuad = MiEndingOffset(Subsection);
    EndOffset = (UINT64)EndQuad.QuadPart;

    //
    // If the file is bigger than the subsection, truncate the subsection range
    // checks.
    //

    if ((StartOffset & ~(PAGE_SIZE - 1)) + (Subsection->PtesInSubsection << PAGE_SHIFT) < EndOffset) {
        EndOffset = (StartOffset & ~(PAGE_SIZE - 1)) + (Subsection->PtesInSubsection << PAGE_SHIFT);
    }

    TempOffset = EndOffset;

    PreviousSubsection = NULL;
    LastPrototypePte = NULL;

    Rle = MiReadList->List;

#if DBG
    if (MiPfDebug & MI_PF_FORCE_PREFETCH) {
        MiRemoveUserPages ();
    }
#endif

    //
    // Initializing FileOffset is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    FileOffset = 0;

    for (i = 0; i < NumberOfEntries; i += 1, Rle += 1) {

        ASSERT ((i == 0) || (ReadList->List[i].Alignment > FileOffset));

        FileOffset = ReadList->List[i].Alignment;

        ASSERT (Rle->u1.PrototypePte == NULL);

        //
        // Calculate which PTE maps the given logical block offset.
        //
        // Since our caller always passes ordered lists of logical block offsets
        // within a given file, always look forwards (as an optimization) in the
        // subsection chain.
        //
        // A quick check is made first to avoid recalculations and loops where
        // possible.
        //
    
        if ((StartOffset <= FileOffset) && (FileOffset < EndOffset)) {
            ASSERT (Subsection->SubsectionBase != NULL);
            LocalPrototypePte = Subsection->SubsectionBase +
                ((FileOffset - StartOffset) >> PAGE_SHIFT);
            ASSERT (TempOffset != 0);
            ASSERT (EndOffset != 0);
        }
        else {
            LocalPrototypePte = NULL;
            do {
    
                ASSERT (Subsection->SubsectionBase != NULL);

                if ((Subsection->StartingSector == 0) &&
                    (ControlArea->u.Flags.Image == 1) &&
                    (Subsection->SubsectionBase != ControlArea->Segment->PrototypePte)) {

                    //
                    // This is an image that was built with a linker pre-1995
                    // (version 2.39 is one example) that put bss into a
                    // separate subsection with zero as a starting file offset
                    // field in the on-disk image.  Ignore any prefetch as it
                    // would read from the wrong offset trying to satisfy these
                    // ranges (which are actually demand zero when the fault
                    // occurs).
                    //
                    // We could be clever here and just ignore this particular
                    // file offset, but for now just don't prefetch this file
                    // at all.  Note that this offset would only be present in
                    // a prefetch database that was constructed without the
                    // accompanying fix just before the call to
                    // CcPfLogPageFault.
                    //

                    Subsection = NULL;
                    break;
                }

                StartOffset = (UINT64)MiStartingOffset (Subsection, Subsection->SubsectionBase);

                EndQuad = MiEndingOffset(Subsection);
                EndOffset = (UINT64)EndQuad.QuadPart;

                //
                // If the file is bigger than the subsection, truncate the
                // subsection range checks.
                //

                if ((StartOffset & ~(PAGE_SIZE - 1)) + (Subsection->PtesInSubsection << PAGE_SHIFT) < EndOffset) {
                    EndOffset = (StartOffset & ~(PAGE_SIZE - 1)) + (Subsection->PtesInSubsection << PAGE_SHIFT);
                }

                if ((StartOffset <= FileOffset) && (FileOffset < EndOffset)) {
    
                    LocalPrototypePte = Subsection->SubsectionBase +
                        ((FileOffset - StartOffset) >> PAGE_SHIFT);
    
                    TempOffset = EndOffset;
    
                    break;
                }
    
                if ((VeryLastSubsection != NULL) &&
                    ((PMSUBSECTION)Subsection == VeryLastSubsection)) {

                    //
                    // The requested block is beyond the size the section
                    // was on entry.  Reject it as this subsection is not
                    // referenced.
                    //

                    Subsection = NULL;
                    break;
                }

                Subsection = Subsection->NextSubsection;

            } while (Subsection != NULL);
        }

        if ((Subsection == NULL) || (LocalPrototypePte == LastPrototypePte)) {

            //
            // Illegal offsets are not prefetched.  Either the file has
            // been replaced since the scenario was logged or Cc is passing
            // trash.  Either way, this prefetch is over.
            //
    
#if DBG
            if (MiPfDebug & MI_PF_PRINT_ERRORS) {
                DbgPrint ("MiPfPrepareReadList: Illegal readlist passed %p, %p, %p\n", ReadList, LocalPrototypePte, LastPrototypePte);
            }
#endif

            if (VeryFirstSubsection != NULL) {
                MiRemoveViewsFromSectionWithPfn (VeryFirstSubsection,
                                                 LastPteOffset);
            }
            ExFreePool (MiReadList);
            return STATUS_INVALID_PARAMETER_1;
        }

        PteContents = *LocalPrototypePte;

        //
        // See if this page needs to be read in.  Note that these reads
        // are done without the PFN or system cache working set locks.
        // This is ok because later before we make the final decision on
        // whether to read each page, we'll look again.
        // If the page is in tranisition, make the call to (possibly) move 
        // it to the end of the standby list to prevent cannibalization.
        //

        if (PteContents.u.Hard.Valid == 1) {
            continue;
        }

        if (PteContents.u.Soft.Prototype == 0) {
            if (PteContents.u.Soft.Transition == 1) {
                MiMovePageToEndOfStandbyList (LocalPrototypePte);
            }
            else {

                //
                // Demand zero or pagefile-backed, don't prefetch from the
                // file or we'd lose the contents.  Note this can happen for
                // session-space images as we back modified (ie: for relocation
                // fixups or IAT updated) portions from the pagefile.
                //

                NOTHING;
            }
            continue;
        }

        Rle->u1.PrototypePte = LocalPrototypePte;
        LastPrototypePte = LocalPrototypePte;

        //
        // Check for partial pages as they require further processing later.
        //
    
        StartingOffset = (UINT64) MiStartingOffset (Subsection, LocalPrototypePte);

        ASSERT (StartingOffset < TempOffset);

        if ((StartingOffset + PAGE_SIZE) > TempOffset) {
            Rle->u1.e1.Partial = 1;
        }

        //
        // The NewSubsection marker is used to delimit the beginning of a new
        // subsection because RLE chunks must be split to accomodate inpage
        // completion so that proper zeroing (based on subsection alignment)
        // is done in MiWaitForInPageComplete.
        //

        if (FirstRleInRun == NULL) {
            FirstRleInRun = Rle;
            Rle->u1.e1.NewSubsection = 1;
            PreviousSubsection = Subsection;
        }
        else {
            if (Subsection != PreviousSubsection) {
                Rle->u1.e1.NewSubsection = 1;
                PreviousSubsection = Subsection;
            }
        }

        NumberOfPages += 1;
    }

    //
    // If the number of pages to read in is extremely small, don't bother.
    //

    if (NumberOfPages < MINIMUM_READ_LIST_PAGES) {
        if (VeryFirstSubsection != NULL) {
            MiRemoveViewsFromSectionWithPfn (VeryFirstSubsection,
                                             LastPteOffset);
        }
        ExFreePool (MiReadList);
        return STATUS_SUCCESS;
    }

    RleMax = MiReadList->List + NumberOfEntries;
    ASSERT (FirstRleInRun != RleMax);

    Status = STATUS_SUCCESS;

    //
    // Walk the readlists to determine runs.  Cross-subsection runs are split
    // here so the completion code can zero the proper amount for any
// non-aligned files.
    //

    EndRleRun = NULL;
    Rle = FirstRleInRun;

    //
    // Initializing StartRleRun & EndPrototypePte is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    StartRleRun = NULL;
    EndPrototypePte = NULL;

    while (Rle < RleMax) {

        if (Rle->u1.PrototypePte != NULL) {

            if (EndRleRun != NULL) {

                StartPrototypePte = MI_RLEPROTO_TO_PROTO(Rle->u1.PrototypePte);

                if (StartPrototypePte - EndPrototypePte > SEEK_THRESHOLD) {
                    Rle -= 1;
                    goto BuildMdl;
                }
            }

            if (Rle->u1.e1.NewSubsection == 1) {
                if (EndRleRun != NULL) {
                    Rle -= 1;
                    goto BuildMdl;
                }
            }

            if (EndRleRun == NULL) {
                StartRleRun = Rle;
            }

            EndRleRun = Rle;
            EndPrototypePte = MI_RLEPROTO_TO_PROTO(Rle->u1.PrototypePte);

            if (Rle->u1.e1.Partial == 1) {

                //
                // This must be the last RLE in this subsection as it is a
                // partial page.  Split this run now.
                //

                goto BuildMdl;
            }
        }

        Rle += 1;

        //
        // Handle any straggling last run as well.
        //

        if (Rle == RleMax) {
            if (EndRleRun != NULL) {
                Rle -= 1;
                goto BuildMdl;
            }
        }

        continue;

BuildMdl:

        //
        // Note no preceding or trailing dummy pages are possible as they are
        // trimmed immediately each time when the first real page of a run
        // is discovered above.
        //

        ASSERT (Rle >= StartRleRun);
        ASSERT (StartRleRun->u1.PrototypePte != NULL);
        ASSERT (EndRleRun->u1.PrototypePte != NULL);

        StartPrototypePte = MI_RLEPROTO_TO_PROTO(StartRleRun->u1.PrototypePte);
        EndPrototypePte = MI_RLEPROTO_TO_PROTO(EndRleRun->u1.PrototypePte);

        NumberOfPages = (EndPrototypePte - StartPrototypePte) + 1;

        //
        // Allocate and initialize an inpage support block for this run.
        //

        InPageSupport = MiGetInPageSupportBlock (FALSE, PREFETCH_PROCESS);
    
        if (InPageSupport == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            break;
        }
    
        //
        // Use the MDL embedded in the inpage support block if it's big enough.
        // Otherwise allocate and initialize an MDL for this run.
        //

        if (NumberOfPages <= MM_MAXIMUM_READ_CLUSTER_SIZE + 1) {
            Mdl = &InPageSupport->Mdl;
            MmInitializeMdl (Mdl, NULL, NumberOfPages << PAGE_SHIFT);
        }
        else {
            Mdl = MmCreateMdl (NULL, NULL, NumberOfPages << PAGE_SHIFT);
            if (Mdl == NULL) {
                ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
            
#if DBG
                InPageSupport->ListEntry.Next = NULL;
#endif
            
                MiFreeInPageSupportBlock (InPageSupport);
                Status = STATUS_INSUFFICIENT_RESOURCES;
                break;
            }
        }

#if DBG
        if (MiPfDebug & MI_PF_VERBOSE) {
            DbgPrint ("MiPfPrepareReadList: Creating INPAGE/MDL %p %p for %x pages\n", InPageSupport, Mdl, NumberOfPages);
        }

        Page = (PPFN_NUMBER)(Mdl + 1);
        *Page = MM_EMPTY_LIST;
#endif
        //
        // Find the subsection for the start RLE.  From this the file offset
        // can be derived.
        //

        ASSERT (StartPrototypePte != NULL);

        if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
            Subsection = (PSUBSECTION)(ControlArea + 1);
        }
        else {
            Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }

        do {
            ASSERT (Subsection->SubsectionBase != NULL);

            if ((StartPrototypePte >= Subsection->SubsectionBase) &&
                (StartPrototypePte < Subsection->SubsectionBase + Subsection->PtesInSubsection)) {
                    break;
            }
            Subsection = Subsection->NextSubsection;

        } while (Subsection != NULL);

        //
        // Start the read at the proper file offset.
        //

        StartingOffset = (UINT64) MiStartingOffset (Subsection,
                                                    StartPrototypePte);

        InPageSupport->ReadOffset = *((PLARGE_INTEGER)(&StartingOffset));

        //
        // Since the RLE is not always valid here, only walk the remaining
        // subsections for valid partial RLEs as only they need truncation.
        //
        // Note only image file reads need truncation as the filesystem cannot
        // blindly zero the rest of the page for these reads as they are packed
        // by memory management on a 512-byte sector basis.  Data reads use
        // the whole page and the filesystems zero fill any remainder beyond
        // valid data length.  It is important to specify the entire page where
        // possible so the filesystem won't post this which will hurt perf.
        //

        if ((EndRleRun->u1.e1.Partial == 1) && (ReadList->IsImage)) {

            ASSERT ((EndPrototypePte >= Subsection->SubsectionBase) &&
                    (EndPrototypePte < Subsection->SubsectionBase + Subsection->PtesInSubsection));

            //
            // The read length for a partial RLE must be truncated correctly.
            //

            EndQuad = MiEndingOffset(Subsection);
            TempOffset = (UINT64)EndQuad.QuadPart;

            if ((ULONG)(TempOffset - StartingOffset) <= Mdl->ByteCount) {
                ReadSize = (ULONG)(TempOffset - StartingOffset);

                //
                // Round the offset to a 512-byte offset as this will help
                // filesystems optimize the transfer.  Note that filesystems
                // will always zero fill the remainder between VDL and the
                // next 512-byte multiple and we have already zeroed the
                // whole page.
                //

                ReadSize = ((ReadSize + MMSECTOR_MASK) & ~MMSECTOR_MASK);

                Mdl->ByteCount = ReadSize;
            }
            else {
                ASSERT ((StartingOffset & ~(PAGE_SIZE - 1)) + (Subsection->PtesInSubsection << PAGE_SHIFT) < TempOffset);
            }
        }

        //
        // Stash these in the inpage block so we can walk it quickly later
        // in pass 2.
        //

        InPageSupport->BasePte = (PMMPTE)StartRleRun;
        InPageSupport->FilePointer = (PFILE_OBJECT)EndRleRun;

        ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
        InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);

        PushEntryList (&MiReadList->InPageSupportHead,
                       &InPageSupport->ListEntry);

        Rle += 1;
        EndRleRun = NULL;
    }

    //
    // Check for the entire list being full (or empty).
    //
    // Status is STATUS_INSUFFICIENT_RESOURCES if an MDL or inpage block
    // allocation failed.  If any allocations succeeded, then set STATUS_SUCCESS
    // as pass2 must occur.
    //

    if (MiReadList->InPageSupportHead.Next != NULL) {

        Status = STATUS_SUCCESS;
    }
    else {
        if (VeryFirstSubsection != NULL) {
            MiRemoveViewsFromSectionWithPfn (VeryFirstSubsection, LastPteOffset);
        }
        ExFreePool (MiReadList);
        MiReadList = NULL;
    }

    //
    // Note that a nonzero *OutMiReadList return value means that the caller
    // needs to remove the views for the section.
    //

    *OutMiReadList = MiReadList;

    return Status;
}

NTSTATUS
MiPfPutPagesInTransition (
    IN PMI_READ_LIST ReadList,
    IN OUT PMMPFN *DummyPagePfn
    )

/*++

Routine Description:

    This routine allocates physical memory for the specified read-list and
    puts all the pages in transition.  On return the caller must issue I/Os
    for the list not only because of this thread, but also to satisfy
    collided faults from other threads for these same pages.

Arguments:

    ReadList - Supplies a pointer to the read-list.

    DummyPagePfn - If this points at a NULL pointer, then a dummy page is
                   allocated and placed in this pointer.  Otherwise this points
                   at a PFN to use as a dummy page.

Return Value:

    STATUS_SUCCESS
    STATUS_INSUFFICIENT_RESOURCES

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    LOGICAL Waited;
    PVOID StartingVa;
    PFN_NUMBER MdlPages;
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPTE RlePrototypePte;
    PMMPTE FirstRlePrototypeInRun;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER Page;
    PPFN_NUMBER DestinationPage;
    ULONG PageColor;
    PMI_READ_LIST_ENTRY Rle;
    PMI_READ_LIST_ENTRY RleMax;
    PMI_READ_LIST_ENTRY FirstRleInRun;
    PFN_NUMBER DummyPage;
    PMDL Mdl;
    PMDL FreeMdl;
    PMMPFN PfnProto;
    PMMPFN Pfn1;
    PMMPFN DummyPfn1;
    ULONG i;
    PFN_NUMBER DummyTrim;
    PFN_NUMBER DummyReferences;
    ULONG NumberOfPages;
    MMPTE TempPte;
    PMMPTE PointerPde;
    PEPROCESS CurrentProcess;
    PSINGLE_LIST_ENTRY PrevEntry;
    PSINGLE_LIST_ENTRY NextEntry;
    PMMINPAGE_SUPPORT InPageSupport;
    SINGLE_LIST_ENTRY ReversedInPageSupportHead;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // Reverse the singly linked list of inpage support blocks so the
    // blocks are read in the same order requested for better performance
    // (ie: keep the disk heads seeking in the same direction).
    //

    ReversedInPageSupportHead.Next = NULL;

    do {

        NextEntry = PopEntryList (&ReadList->InPageSupportHead);

        if (NextEntry == NULL) {
            break;
        }

        PushEntryList (&ReversedInPageSupportHead, NextEntry);

    } while (TRUE);

    ASSERT (ReversedInPageSupportHead.Next != NULL);
    ReadList->InPageSupportHead.Next = ReversedInPageSupportHead.Next;

    DummyReferences = 0;
    FreeMdl = NULL;
    CurrentProcess = PsGetCurrentProcess();

    PfnProto = NULL;
    PointerPde = NULL;

    LOCK_PFN (OldIrql);

    //
    // Do a quick sanity check to avoid doing unnecessary work.
    //

    if ((MmAvailablePages < MINIMUM_AVAILABLE_PAGES) ||
        (MI_NONPAGABLE_MEMORY_AVAILABLE() < MINIMUM_AVAILABLE_PAGES)) {

        UNLOCK_PFN (OldIrql);

        do {

            NextEntry = PopEntryList(&ReadList->InPageSupportHead);
            if (NextEntry == NULL) {
                break;
            }
    
            InPageSupport = CONTAINING_RECORD(NextEntry,
                                              MMINPAGE_SUPPORT,
                                              ListEntry);
    
#if DBG
            InPageSupport->ListEntry.Next = NULL;
#endif

            MiFreeInPageSupportBlock (InPageSupport);
        } while (TRUE);

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Allocate a dummy page that will map discarded pages that aren't skipped.
    // Do it only if it's not already allocated.
    //

    if (*DummyPagePfn == NULL) {

        MiEnsureAvailablePageOrWait (NULL, NULL);

        DummyPage = MiRemoveAnyPage (0);
        Pfn1 = MI_PFN_ELEMENT (DummyPage);

        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

        MiInitializePfnForOtherProcess (DummyPage, MI_PF_DUMMY_PAGE_PTE, 0);

        //
        // Give the page a containing frame so MiIdentifyPfn won't crash.
        //

        Pfn1->u4.PteFrame = PsInitialSystemProcess->Pcb.DirectoryTableBase[0] >> PAGE_SHIFT;

        //
        // Always bias the reference count by 1 and charge for this locked page
        // up front so the myriad increments and decrements don't get slowed
        // down with needless checking.
        //

        Pfn1->u3.e1.PrototypePte = 0;
        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 11);
        Pfn1->u3.e2.ReferenceCount += 1;

        Pfn1->u3.e1.ReadInProgress = 1;

        *DummyPagePfn = Pfn1;
    }
    else {
        Pfn1 = *DummyPagePfn;
        DummyPage = Pfn1 - MmPfnDatabase;
    }

    DummyPfn1 = Pfn1;

    PrevEntry = NULL;
    NextEntry = ReadList->InPageSupportHead.Next;
    while (NextEntry != NULL) {

        InPageSupport = CONTAINING_RECORD(NextEntry,
                                          MMINPAGE_SUPPORT,
                                          ListEntry);

        Rle = (PMI_READ_LIST_ENTRY)InPageSupport->BasePte;
        RleMax = (PMI_READ_LIST_ENTRY)InPageSupport->FilePointer;

        ASSERT (Rle->u1.PrototypePte != NULL);
        ASSERT (RleMax->u1.PrototypePte != NULL);

        //
        // Properly initialize the inpage support block fields we overloaded.
        //

        InPageSupport->BasePte = MI_RLEPROTO_TO_PROTO (Rle->u1.PrototypePte);
        InPageSupport->FilePointer = ReadList->FileObject;

        FirstRleInRun = Rle;
        FirstRlePrototypeInRun = MI_RLEPROTO_TO_PROTO (Rle->u1.PrototypePte);
        RleMax += 1;

        Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);

        Page = (PPFN_NUMBER)(Mdl + 1);

        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        MdlPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                  Mdl->ByteCount);

        //
        // Default the MDL entry to the dummy page as the RLE PTEs may
        // be noncontiguous and we have no way to distinguish the jumps.
        //

        for (i = 0; i < MdlPages; i += 1) {
            *Page = DummyPage;
            Page += 1;
        }

        DummyReferences += MdlPages;

        if (DummyPfn1->u3.e2.ReferenceCount + MdlPages >= MAXUSHORT) {

            //
            // The USHORT ReferenceCount wrapped.
            //
            // Dequeue all remaining inpage blocks.
            //

            UNLOCK_PFN (OldIrql);

            if (PrevEntry != NULL) {
                PrevEntry->Next = NULL;
            }
            else {
                ReadList->InPageSupportHead.Next = NULL;
            }

            do {

                InPageSupport = CONTAINING_RECORD(NextEntry,
                                                  MMINPAGE_SUPPORT,
                                                  ListEntry);

#if DBG
                InPageSupport->ListEntry.Next = NULL;
#endif

                NextEntry = NextEntry->Next;

                MiFreeInPageSupportBlock (InPageSupport);

            } while (NextEntry != NULL);

            LOCK_PFN (OldIrql);

            break;
        }

        DummyPfn1->u3.e2.ReferenceCount =
            (USHORT)(DummyPfn1->u3.e2.ReferenceCount + MdlPages);

        NumberOfPages = 0;
        Waited = FALSE;

        //
        // Build the proper InPageSupport and MDL to describe this run.
        //

        for (; Rle < RleMax; Rle += 1) {
    
            //
            // Fill the MDL entry for this RLE.
            //
    
            RlePrototypePte = MI_RLEPROTO_TO_PROTO (Rle->u1.PrototypePte);

            if (RlePrototypePte == NULL) {
                continue;
            }

            //
            // The RlePrototypePte better be inside a prototype PTE allocation
            // so that subsequent page trims update the correct PTEs.
            //

            ASSERT (((RlePrototypePte >= (PMMPTE)MmPagedPoolStart) &&
                    (RlePrototypePte <= (PMMPTE)MmPagedPoolEnd)) ||
                    ((RlePrototypePte >= (PMMPTE)MmSpecialPoolStart) && (RlePrototypePte <= (PMMPTE)MmSpecialPoolEnd)));

            //
            // This is a page that our first pass which ran lock-free decided
            // needed to be read.  Here this must be rechecked as the page
            // state could have changed.  Note this check is final as the
            // PFN lock is held.  The PTE must be put in transition with
            // read in progress before the PFN lock is released.
            //

            //
            // Lock page containing prototype PTEs in memory by
            // incrementing the reference count for the page.
            // Unlock any page locked earlier containing prototype PTEs if
            // the containing page is not the same for both.
            //

            if (PfnProto != NULL) {

                if (PointerPde != MiGetPteAddress (RlePrototypePte)) {

                    ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 5);
                    PfnProto = NULL;
                }
            }

            if (PfnProto == NULL) {

                ASSERT (!MI_IS_PHYSICAL_ADDRESS (RlePrototypePte));
    
                PointerPde = MiGetPteAddress (RlePrototypePte);
    
                if (PointerPde->u.Hard.Valid == 0) {

                    //
                    // Set Waited to TRUE if we ever release the PFN lock as
                    // that means a release path below must factor this in.
                    //

                    if (MiMakeSystemAddressValidPfn (RlePrototypePte) == TRUE) {
                        Waited = TRUE;
                    }

                    MiMakeSystemAddressValidPfn (RlePrototypePte);
                }

                PfnProto = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
                MI_ADD_LOCKED_PAGE_CHARGE(PfnProto, 4);
                PfnProto->u3.e2.ReferenceCount += 1;
                ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
            }

            PteContents = *(RlePrototypePte);

            if (PteContents.u.Hard.Valid == 1) {

                //
                // The page has become resident since the last pass.  Don't
                // include it.
                //

                NOTHING;
            }
            else if (PteContents.u.Soft.Prototype == 0) {

                //
                // The page is either in transition (so don't prefetch it).
                //
                //      - OR -
                //
                // it is now pagefile (or demand zero) backed - in which case
                // prefetching it from the file here would cause us to lose
                // the contents.  Note this can happen for session-space images
                // as we back modified (ie: for relocation fixups or IAT
                // updated) portions from the pagefile.
                //

                NOTHING;
            }
            else if ((MmAvailablePages >= MINIMUM_AVAILABLE_PAGES) &&
                (MI_NONPAGABLE_MEMORY_AVAILABLE() >= MINIMUM_AVAILABLE_PAGES)) {


                NumberOfPages += 1;

                //
                // Allocate a physical page.
                //

                PageColor = MI_PAGE_COLOR_VA_PROCESS (
                    MiGetVirtualAddressMappedByPte (RlePrototypePte),
                    &CurrentProcess->NextPageColor
                    );

                if (Rle->u1.e1.Partial == 1) {

                    //
                    // This read crosses the end of a subsection, get a zeroed
                    // page and correct the read size.
                    //

                    PageFrameIndex = MiRemoveZeroPage (PageColor);
                }
                else {
                    PageFrameIndex = MiRemoveAnyPage (PageColor);
                }

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
                ASSERT (Pfn1->u2.ShareCount == 0);
                ASSERT (RlePrototypePte->u.Hard.Valid == 0);

                //
                // Initialize read-in-progress PFN.
                //
            
                MiInitializePfn (PageFrameIndex, RlePrototypePte, 0);

                //
                // These pieces of MiInitializePfn initialization are overridden
                // here as these pages are only going into prototype
                // transition and not into any page tables.
                //

                Pfn1->u3.e1.PrototypePte = 1;
                MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, 38);
                Pfn1->u2.ShareCount -= 1;
                Pfn1->u3.e1.PageLocation = ZeroedPageList;

                //
                // Initialize the I/O specific fields.
                //
            
                ASSERT (FirstRleInRun->u1.PrototypePte != NULL);
                Pfn1->u1.Event = &InPageSupport->Event;
                Pfn1->u3.e1.ReadInProgress = 1;
                ASSERT (Pfn1->u4.InPageError == 0);

                //
                // Increment the PFN reference count in the control area for
                // the subsection.
                //

                ReadList->ControlArea->NumberOfPfnReferences += 1;
            
                //
                // Put the PTE into the transition state.
                // No TB flush needed as the PTE is still not valid.
                //

                MI_MAKE_TRANSITION_PTE (TempPte,
                                        PageFrameIndex,
                                        RlePrototypePte->u.Soft.Protection,
                                        RlePrototypePte);
                MI_WRITE_INVALID_PTE (RlePrototypePte, TempPte);

                Page = (PPFN_NUMBER)(Mdl + 1);

                ASSERT ((ULONG)(RlePrototypePte - FirstRlePrototypeInRun) < MdlPages);

                *(Page + (RlePrototypePte - FirstRlePrototypeInRun)) = PageFrameIndex;
            }
            else {

                //
                // Failed allocation - this concludes prefetching for this run.
                //

                break;
            }
        }
    
        //
        // If all the pages were resident, dereference the dummy page references
        // now and notify our caller that I/Os are not necessary.  Note that
        // STATUS_SUCCESS must still be returned so our caller knows to continue
        // on to the next readlist.
        //
    
        if (NumberOfPages == 0) {
            ASSERT (DummyPfn1->u3.e2.ReferenceCount > MdlPages);
            DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - MdlPages);

            UNLOCK_PFN (OldIrql);

            if (PrevEntry != NULL) {
                PrevEntry->Next = NextEntry->Next;
            }
            else {
                ReadList->InPageSupportHead.Next = NextEntry->Next;
            }

            NextEntry = NextEntry->Next;

#if DBG
            InPageSupport->ListEntry.Next = NULL;
#endif
            MiFreeInPageSupportBlock (InPageSupport);

            LOCK_PFN (OldIrql);
            continue;
        }

        //
        // Carefully trim leading dummy pages.
        //

        Page = (PPFN_NUMBER)(Mdl + 1);

        DummyTrim = 0;
        for (i = 0; i < MdlPages - 1; i += 1) {
            if (*Page == DummyPage) {
                DummyTrim += 1;
                Page += 1;
            }
            else {
                break;
            }
        }

        if (DummyTrim != 0) {

            Mdl->Size =
                (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
            Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
            ASSERT (Mdl->ByteCount != 0);
            InPageSupport->ReadOffset.QuadPart += (DummyTrim * PAGE_SIZE);
            DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);

            //
            // Shuffle down the PFNs in the MDL.
            // Recalculate BasePte to adjust for the shuffle.
            //

            Pfn1 = MI_PFN_ELEMENT (*Page);
    
            ASSERT (Pfn1->PteAddress->u.Hard.Valid == 0);
            ASSERT ((Pfn1->PteAddress->u.Soft.Prototype == 0) &&
                     (Pfn1->PteAddress->u.Soft.Transition == 1));
    
            InPageSupport->BasePte = Pfn1->PteAddress;

            DestinationPage = (PPFN_NUMBER)(Mdl + 1);

            do {
                *DestinationPage = *Page;
                DestinationPage += 1;
                Page += 1;
                i += 1;
            } while (i < MdlPages);

            MdlPages -= DummyTrim;
        }

        //
        // Carefully trim trailing dummy pages.
        //

        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        MdlPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                  Mdl->ByteCount);

        ASSERT (MdlPages != 0);

        Page = (PPFN_NUMBER)(Mdl + 1) + MdlPages - 1;

        if (*Page == DummyPage) {

            ASSERT (MdlPages >= 2);

            //
            // Trim the last page specially as it may be a partial page.
            //

            Mdl->Size -= sizeof(PFN_NUMBER);
            if (BYTE_OFFSET(Mdl->ByteCount) != 0) {
                Mdl->ByteCount &= ~(PAGE_SIZE - 1);
            }
            else {
                Mdl->ByteCount -= PAGE_SIZE;
            }
            ASSERT (Mdl->ByteCount != 0);
            DummyPfn1->u3.e2.ReferenceCount -= 1;

            //
            // Now trim any other trailing pages.
            //

            Page -= 1;
            DummyTrim = 0;
            while (Page != ((PPFN_NUMBER)(Mdl + 1))) {
                if (*Page != DummyPage) {
                    break;
                }
                DummyTrim += 1;
                Page -= 1;
            }
            if (DummyTrim != 0) {
                ASSERT (Mdl->Size > (USHORT)(DummyTrim * sizeof(PFN_NUMBER)));
                Mdl->Size = 
                    (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
                Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
                DummyPfn1->u3.e2.ReferenceCount =
                    (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);
            }

            ASSERT (MdlPages > DummyTrim + 1);
            MdlPages -= (DummyTrim + 1);

#if DBG
            StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
        
            ASSERT (MdlPages == ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                               Mdl->ByteCount));
#endif
        }

        //
        // If the MDL is not already embedded in the inpage block, see if its
        // final size qualifies it - if so, embed it now.
        //

        if ((Mdl != &InPageSupport->Mdl) &&
            (Mdl->ByteCount <= (MM_MAXIMUM_READ_CLUSTER_SIZE + 1) * PAGE_SIZE)){

#if DBG
            RtlFillMemoryUlong (&InPageSupport->Page[0],
                                (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof (PFN_NUMBER),
                                0xf1f1f1f1);
#endif

            RtlCopyMemory (&InPageSupport->Mdl, Mdl, Mdl->Size);

            Mdl->Next = FreeMdl;
            FreeMdl = Mdl;

            Mdl = &InPageSupport->Mdl;

            ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
            InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);
        }

        //
        // If the MDL contains a large number of dummy pages to real pages
        // then just discard it.  Only check large MDLs as embedded ones are
        // always worth the I/O.
        //
        // The PFN lock may have been released above during the
        // MiMakeSystemAddressValidPfn call.  If so, other threads may
        // have collided on the pages in the prefetch MDL and if so,
        // this I/O must be issued regardless of the inefficiency of
        // dummy pages within it.  Otherwise the other threads will
        // hang in limbo forever.
        //

        ASSERT (MdlPages != 0);

#if DBG
        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        ASSERT (MdlPages == ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                           Mdl->ByteCount));
#endif

        if ((Mdl != &InPageSupport->Mdl) &&
            (Waited == FALSE) &&
            ((MdlPages - NumberOfPages) / DUMMY_RATIO >= NumberOfPages)) {

            if (PrevEntry != NULL) {
                PrevEntry->Next = NextEntry->Next;
            }
            else {
                ReadList->InPageSupportHead.Next = NextEntry->Next;
            }

            NextEntry = NextEntry->Next;

            ASSERT (MI_EXTRACT_PREFETCH_MDL(InPageSupport) == Mdl);

            //
            // Note the pages are individually freed here (rather than just
            // "completing" the I/O with an error) as the PFN lock has
            // never been released since the pages were put in transition.
            // So no collisions on these pages are possible.
            //

            ASSERT (InPageSupport->WaitCount == 1);

            Page = (PPFN_NUMBER)(Mdl + 1) + MdlPages - 1;

            do {
                if (*Page != DummyPage) {
                    Pfn1 = MI_PFN_ELEMENT (*Page);
            
                    ASSERT (Pfn1->PteAddress->u.Hard.Valid == 0);
                    ASSERT ((Pfn1->PteAddress->u.Soft.Prototype == 0) &&
                             (Pfn1->PteAddress->u.Soft.Transition == 1));
                    ASSERT (Pfn1->u3.e1.ReadInProgress == 1);
                    ASSERT (Pfn1->u3.e1.PrototypePte == 1);
                    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
                    ASSERT (Pfn1->u2.ShareCount == 0);
            
                    Pfn1->u3.e1.PageLocation = StandbyPageList;
                    Pfn1->u3.e1.ReadInProgress = 0;
                    MiRestoreTransitionPte (*Page);

                    MI_SET_PFN_DELETED (Pfn1);
                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 39);
                }

                Page -= 1;
            } while (Page >= (PPFN_NUMBER)(Mdl + 1));

            ASSERT (InPageSupport->WaitCount == 1);

            ASSERT (DummyPfn1->u3.e2.ReferenceCount > MdlPages);
            DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - MdlPages);

            UNLOCK_PFN (OldIrql);

#if DBG
            InPageSupport->ListEntry.Next = NULL;
#endif
            MiFreeInPageSupportBlock (InPageSupport);
            LOCK_PFN (OldIrql);

            continue;
        }

#if DBG
        MiPfDbgDumpReadList (ReadList);
#endif

        ASSERT ((USHORT)Mdl->Size - sizeof(MDL) == BYTES_TO_PAGES(Mdl->ByteCount) * sizeof(PFN_NUMBER));

        DummyPfn1->u3.e2.ReferenceCount =
            (USHORT)(DummyPfn1->u3.e2.ReferenceCount - NumberOfPages);
    
        MmInfoCounters.PageReadIoCount += 1;
        MmInfoCounters.PageReadCount += NumberOfPages;

        //
        // March on to the next run and its InPageSupport and MDL.
        //

        PrevEntry = NextEntry;
        NextEntry = NextEntry->Next;
    }

    //
    // Unlock page containing prototype PTEs.
    //

    if (PfnProto != NULL) {
        ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 5);
    }

    UNLOCK_PFN (OldIrql);

#if DBG

    if (MiPfDebug & MI_PF_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

    //
    // Free any collapsed MDLs that are no longer needed.
    //

    while (FreeMdl != NULL) {
        Mdl = FreeMdl->Next;
        ExFreePool (FreeMdl);
        FreeMdl = Mdl;
    }

    return STATUS_SUCCESS;
}

VOID
MiPfExecuteReadList (
    IN PMI_READ_LIST ReadList
    )

/*++

Routine Description:

    This routine executes the read list by issuing paging I/Os for all
    runs described in the read-list.

Arguments:

    ReadList - Pointer to the read-list.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PMDL Mdl;
    NTSTATUS status;
    PMMPFN Pfn1;
    PMMPTE LocalPrototypePte;
    PFN_NUMBER PageFrameIndex;
    PSINGLE_LIST_ENTRY NextEntry;
    PMMINPAGE_SUPPORT InPageSupport;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    NextEntry = ReadList->InPageSupportHead.Next;
    while (NextEntry != NULL) {

        InPageSupport = CONTAINING_RECORD(NextEntry,
                                          MMINPAGE_SUPPORT,
                                          ListEntry);

        //
        // Initialize the prefetch MDL.
        //
    
        Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);

        ASSERT ((Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
        Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

        ASSERT (InPageSupport->u1.e1.Completed == 0);
        ASSERT (InPageSupport->Thread == PsGetCurrentThread());
        ASSERT64 (InPageSupport->UsedPageTableEntries == 0);
        ASSERT (InPageSupport->WaitCount >= 1);
        ASSERT (InPageSupport->u1.e1.PrefetchMdlHighBits != 0);

        //
        // Initialize the inpage support block fields we overloaded.
        //

        ASSERT (InPageSupport->FilePointer == ReadList->FileObject);
        LocalPrototypePte = InPageSupport->BasePte;

        ASSERT (LocalPrototypePte->u.Hard.Valid == 0);
        ASSERT ((LocalPrototypePte->u.Soft.Prototype == 0) &&
                 (LocalPrototypePte->u.Soft.Transition == 1));

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(LocalPrototypePte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        InPageSupport->Pfn = Pfn1;

        status = IoAsynchronousPageRead (InPageSupport->FilePointer,
                                         Mdl,
                                         &InPageSupport->ReadOffset,
                                         &InPageSupport->Event,
                                         &InPageSupport->IoStatus);

        if (!NT_SUCCESS (status)) {

            //
            // Set the event as the I/O system doesn't set it on errors.
            //

            InPageSupport->IoStatus.Status = status;
            InPageSupport->IoStatus.Information = 0;
            KeSetEvent (&InPageSupport->Event, 0, FALSE);
        }

        NextEntry = NextEntry->Next;
    }

#if DBG

    if (MiPfDebug & MI_PF_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

}

VOID
MiPfCompletePrefetchIos (
    IN PMI_READ_LIST ReadList
    )

/*++

Routine Description:

    This routine waits for a series of page reads to complete
    and completes the requests.

Arguments:

    ReadList - Pointer to the read-list.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PMDL Mdl;
    PMMPFN Pfn1;
    PMMPFN PfnClusterPage;
    PPFN_NUMBER Page;
    NTSTATUS status;
    LONG NumberOfBytes;
    PMMINPAGE_SUPPORT InPageSupport;
    PSINGLE_LIST_ENTRY NextEntry;
    extern ULONG MmFrontOfList;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    do {

        NextEntry = PopEntryList(&ReadList->InPageSupportHead);
        if (NextEntry == NULL) {
            break;
        }

        InPageSupport = CONTAINING_RECORD(NextEntry,
                                          MMINPAGE_SUPPORT,
                                          ListEntry);

        ASSERT (InPageSupport->Pfn != 0);

        Pfn1 = InPageSupport->Pfn;
        Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
        Page = (PPFN_NUMBER)(Mdl + 1);

        status = MiWaitForInPageComplete (InPageSupport->Pfn,
                                          InPageSupport->BasePte,
                                          NULL,
                                          InPageSupport->BasePte,
                                          InPageSupport,
                                          PREFETCH_PROCESS);

        //
        // MiWaitForInPageComplete RETURNS WITH THE PFN LOCK HELD!!!
        //

        //
        // If we are prefetching for boot, insert prefetched pages to the front
        // of the list. Otherwise the pages prefetched first end up susceptible 
        // at the front of the list as we prefetch more. We prefetch pages in 
        // the order they will be used. When there is a spike in memory usage 
        // and there is no free memory, we lose these pages before we can 
        // get cache-hits on them. Thus boot gets ahead and starts discarding 
        // prefetched pages that it could use just a little later.
        //

        if (CCPF_IS_PREFETCHING_FOR_BOOT()) {
            MmFrontOfList = TRUE;
        }

        NumberOfBytes = (LONG)Mdl->ByteCount;

        while (NumberOfBytes > 0) {

            //
            // Decrement all reference counts.
            //

            PfnClusterPage = MI_PFN_ELEMENT (*Page);

#if DBG
            if (PfnClusterPage->u4.InPageError) {

                //
                // If the page is marked with an error, then the whole transfer
                // must be marked as not successful as well.  The only exception
                // is the prefetch dummy page which is used in multiple
                // transfers concurrently and thus may have the inpage error
                // bit set at any time (due to another transaction besides
                // the current one).
                //

                ASSERT ((status != STATUS_SUCCESS) ||
                        (PfnClusterPage->PteAddress == MI_PF_DUMMY_PAGE_PTE));
            }
#endif
            if (PfnClusterPage->u3.e1.ReadInProgress != 0) {

                ASSERT (PfnClusterPage->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                PfnClusterPage->u3.e1.ReadInProgress = 0;

                if (PfnClusterPage->u4.InPageError == 0) {
                    PfnClusterPage->u1.Event = NULL;
                }
            }

            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnClusterPage, 39);

            Page += 1;
            NumberOfBytes -= PAGE_SIZE;
        }

        //
        // If we were inserting prefetched pages to front of standby list
        // for boot prefetching, stop it before we release the pfn lock.
        //

        MmFrontOfList = FALSE;

        if (status != STATUS_SUCCESS) {

            //
            // An I/O error occurred during the page read
            // operation.  All the pages which were just
            // put into transition must be put onto the
            // free list if InPageError is set, and their
            // PTEs restored to the proper contents.
            //

            Page = (PPFN_NUMBER)(Mdl + 1);
            NumberOfBytes = (LONG)Mdl->ByteCount;

            while (NumberOfBytes > 0) {

                PfnClusterPage = MI_PFN_ELEMENT (*Page);

                if (PfnClusterPage->u4.InPageError == 1) {

                    if (PfnClusterPage->u3.e2.ReferenceCount == 0) {

                        ASSERT (PfnClusterPage->u3.e1.PageLocation ==
                                                        StandbyPageList);

                        MiUnlinkPageFromList (PfnClusterPage);
                        MiRestoreTransitionPte (*Page);
                        MiInsertPageInFreeList (*Page);
                    }
                }
                Page += 1;
                NumberOfBytes -= PAGE_SIZE;
            }
        }

        //
        // All the relevant prototype PTEs should be in transition state.
        //

        //
        // We took out an extra reference on the inpage block to prevent
        // MiWaitForInPageComplete from freeing it (and the MDL), since we
        // needed to process the MDL above.  Now let it go for good.
        //

        ASSERT (InPageSupport->WaitCount >= 1);
        UNLOCK_PFN (PASSIVE_LEVEL);

#if DBG
        InPageSupport->ListEntry.Next = NULL;
#endif

        MiFreeInPageSupportBlock (InPageSupport);

    } while (TRUE);
}

#if DBG
VOID
MiPfDbgDumpReadList (
    IN PMI_READ_LIST ReadList
    )

/*++

Routine Description:

    This routine dumps the given read-list range to the debugger.

Arguments:

    ReadList - Pointer to the read-list.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    PMDL Mdl;
    PMMPFN Pfn1;
    PMMPTE LocalPrototypePte;
    PFN_NUMBER PageFrameIndex;
    PMMINPAGE_SUPPORT InPageSupport;
    PSINGLE_LIST_ENTRY NextEntry;
    PPFN_NUMBER Page;
    PVOID StartingVa;
    PFN_NUMBER MdlPages;
    LARGE_INTEGER ReadOffset;

    if ((MiPfDebug & MI_PF_VERBOSE) == 0) {
        return;
    }

    DbgPrint ("\nPF: Dumping read-list %x (FileObject %x ControlArea %x)\n\n",
              ReadList, ReadList->FileObject, ReadList->ControlArea);

    DbgPrint ("\tFileOffset | Pte           | Pfn      \n"
              "\t-----------+---------------+----------\n");

    NextEntry = ReadList->InPageSupportHead.Next;
    while (NextEntry != NULL) {

        InPageSupport = CONTAINING_RECORD(NextEntry,
                                          MMINPAGE_SUPPORT,
                                          ListEntry);

        ReadOffset = InPageSupport->ReadOffset;
        Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);

        Page = (PPFN_NUMBER)(Mdl + 1);
#if DBG
        //
        // MDL isn't filled in yet, skip it.
        //

        if (*Page == MM_EMPTY_LIST) {
            NextEntry = NextEntry->Next;
            continue;
        }
#endif

        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        MdlPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                  Mdl->ByteCount);

        //
        // Default the MDL entry to the dummy page as the RLE PTEs may
        // be noncontiguous and we have no way to distinguish the jumps.
        //

        for (i = 0; i < MdlPages; i += 1) {
            PageFrameIndex = *Page;
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            LocalPrototypePte = Pfn1->PteAddress;

            if (LocalPrototypePte != MI_PF_DUMMY_PAGE_PTE) {
                ASSERT (LocalPrototypePte->u.Hard.Valid == 0);
                ASSERT ((LocalPrototypePte->u.Soft.Prototype == 0) &&
                         (LocalPrototypePte->u.Soft.Transition == 1));
            }

            DbgPrint ("\t  %8x | %8x      | %8x\n",
                ReadOffset.LowPart,
                LocalPrototypePte,
                PageFrameIndex);

            Page += 1;
            ReadOffset.LowPart += PAGE_SIZE;
        }

        NextEntry = NextEntry->Next;
    }

    DbgPrint ("\t\n");
}

VOID
MiRemoveUserPages (
    VOID
    )

/*++

Routine Description:

    This routine removes user space pages.

Arguments:

    None.

Return Value:

    Number of pages removed.

Environment:

    Kernel mode.

--*/

{
    InterlockedIncrement (&MiDelayPageFaults);
    MmEmptyAllWorkingSets ();
    MiFlushAllPages ();
    InterlockedDecrement (&MiDelayPageFaults);

    //
    // Run the transition list and free all the entries so transition
    // faults are not satisfied for any of the non modified pages that were
    // freed.
    //

    MiPurgeTransitionList ();
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\protect.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   protect.c

Abstract:

    This module contains the routines which implement the
    NtProtectVirtualMemory service.

Author:

    Lou Perazzoli (loup) 18-Aug-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if DBG
PEPROCESS MmWatchProcess;
#endif // DBG

HARDWARE_PTE
MiFlushTbAndCapture(
    IN PMMPTE PtePointer,
    IN HARDWARE_PTE TempPte,
    IN PMMPFN Pfn1
    );

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

MMPTE
MiCaptureSystemPte (
    IN PMMPTE PointerProtoPte,
    IN PEPROCESS Process
    );

extern CCHAR MmReadWrite[32];

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtProtectVirtualMemory)
#pragma alloc_text(PAGE,MiProtectVirtualMemory)
#pragma alloc_text(PAGE,MiSetProtectionOnSection)
#pragma alloc_text(PAGE,MiGetPageProtection)
#pragma alloc_text(PAGE,MiChangeNoAccessForkPte)
#pragma alloc_text(PAGE,MiCheckSecuredVad)
#endif


NTSTATUS
NtProtectVirtualMemory(
     IN HANDLE ProcessHandle,
     IN OUT PVOID *BaseAddress,
     IN OUT PSIZE_T RegionSize,
     IN ULONG NewProtect,
     OUT PULONG OldProtect
     )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

Arguments:

     ProcessHandle - An open handle to a process object.

     BaseAddress - The base address of the region of pages
          whose protection is to be changed. This value is
          rounded down to the next host page address
          boundary.

     RegionSize - A pointer to a variable that will receive
          the actual size in bytes of the protected region
          of pages. The initial value of this argument is
          rounded up to the next host page size boundary.

     NewProtect - The new protection desired for the
          specified region of pages.

     Protect Values

          PAGE_NOACCESS - No access to the specified region
               of pages is allowed. An attempt to read,
               write, or execute the specified region
               results in an access violation (i.e. a GP
               fault).

          PAGE_EXECUTE - Execute access to the specified
               region of pages is allowed. An attempt to
               read or write the specified region results in
               an access violation.

          PAGE_READONLY - Read only and execute access to the
               specified region of pages is allowed. An
               attempt to write the specified region results
               in an access violation.

          PAGE_READWRITE - Read, write, and execute access to
               the specified region of pages is allowed. If
               write access to the underlying section is
               allowed, then a single copy of the pages are
               shared. Otherwise the pages are shared read
               only/copy on write.

          PAGE_GUARD - Read, write, and execute access to the
               specified region of pages is allowed,
               however, access to the region causes a "guard
               region entered" condition to be raised in the
               subject process. If write access to the
               underlying section is allowed, then a single
               copy of the pages are shared. Otherwise the
               pages are shared read only/copy on write.

          PAGE_NOCACHE - The page should be treated as uncached.
               This is only valid for non-shared pages.


     OldProtect - A pointer to a variable that will receive
          the old protection of the first page within the
          specified region of pages.

Return Value:

    Returns the status

    TBS


Environment:

    Kernel mode.

--*/


{
    //
    // note - special treatment for the following cases...
    //
    // if a page is locked in the working set (memory?) and the
    // protection is changed to no access, the page should be
    // removed from the working set... valid pages can't be no access.
    //
    // if page is going to be read only or no access? and is demand
    // zero, make sure it is changed to a page of zeroes.
    //
    // update the vm spec to explain locked pages are unlocked when
    // freed or protection is changed to no-access (this may be a nasty
    // problem if we don't want to do this!!
    //

    KAPC_STATE ApcState;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    ULONG Attached = FALSE;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    ULONG ProtectionMask;
    ULONG LastProtect;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (NewProtect);

    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        //
        // Capture the region size and base address under an exception handler.
        //

        try {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
            ProbeForWriteUlong (OldProtect);

            //
            // Capture the region size and base address.
            //

            CapturedBase = *BaseAddress;
            CapturedRegionSize = *RegionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }

    }
    else {

        //
        // Capture the region size and base address.
        //

        CapturedRegionSize = *RegionSize;
        CapturedBase = *BaseAddress;
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                  CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER_3;
    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Status = MiProtectVirtualMemory (Process,
                                     &CapturedBase,
                                     &CapturedRegionSize,
                                     NewProtect,
                                     &LastProtect);


    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    ObDereferenceObject (Process);

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        //
        // Reprobe the addresses as certain architectures (intel 386 for one)
        // do not trap kernel writes.  This is the one service which allows
        // the protection of the page to change between the initial probe
        // and the final argument update.
        //

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
            ProbeForWriteUlong (OldProtect);
        }

        *RegionSize = CapturedRegionSize;
        *BaseAddress = CapturedBase;
        *OldProtect = LastProtect;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return Status;
}


NTSTATUS
MiProtectVirtualMemory (
    IN PEPROCESS Process,
    IN PVOID *BaseAddress,
    IN PSIZE_T RegionSize,
    IN ULONG NewProtect,
    IN PULONG LastProtect
    )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

Arguments:

    Process - Supplies a pointer to the current process.

    BaseAddress - Supplies the starting address to protect.

    RegionsSize - Supplies the size of the region to protect.

    NewProtect - Supplies the new protection to set.

    LastProtect - Supplies the address of a kernel owned pointer to
                  store (without probing) the old protection into.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    LOGICAL WsHeld;
    PMMVAD FoundVad;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    NTSTATUS Status;
    ULONG Attached;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerProtoPte;
    PMMPTE LastProtoPte;
    PMMPFN Pfn1;
    ULONG CapturedOldProtect;
    ULONG ProtectionMask;
    MMPTE TempPte;
    MMPTE PteContents;
    MMPTE PreviousPte;
    ULONG Locked;
    PVOID Va;
    ULONG DoAgain;
    ULONG Waited;
    PVOID UsedPageTableHandle;
    ULONG WorkingSetIndex;
    ULONG OriginalProtect;
#if defined(_MIALT4K_)
    PVOID OriginalBase;
    SIZE_T OriginalRegionSize;
    ULONG OriginalProtectionMask;
    PVOID StartingAddressFor4k;
    PVOID EndingAddressFor4k;
    SIZE_T CapturedRegionSizeFor4k;
    ULONG CapturedOldProtectFor4k;
    LOGICAL EmulationFor4kPage;

#endif

    Attached = FALSE;
    Locked = FALSE;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time.
    // Get the working set mutex so PTEs can be modified.
    // Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    CapturedBase = *BaseAddress;
    CapturedRegionSize = *RegionSize;
    OriginalProtect = NewProtect;

#if defined(_MIALT4K_)
    EmulationFor4kPage = FALSE; 
    OriginalBase = CapturedBase;
    OriginalRegionSize = CapturedRegionSize;
    CapturedOldProtectFor4k = 0;
    OriginalProtectionMask = 0;

    if (Process->Wow64Process != NULL) {

        StartingAddressFor4k = (PVOID)PAGE_4K_ALIGN(OriginalBase);

        EndingAddressFor4k = (PVOID)(((ULONG_PTR)OriginalBase +
                                      OriginalRegionSize - 1) | (PAGE_4K - 1));
            
        CapturedRegionSizeFor4k = (ULONG_PTR)EndingAddressFor4k - 
                                  (ULONG_PTR)StartingAddressFor4k + 1L;

        OriginalProtectionMask = MiMakeProtectionMask(NewProtect);
        if (OriginalProtectionMask == MM_INVALID_PROTECTION) {
            return STATUS_INVALID_PAGE_PROTECTION;
        }

        EmulationFor4kPage = TRUE;
    }
    else {
        //
        // Initializing these is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        StartingAddressFor4k = 0;
        EndingAddressFor4k = 0;
        CapturedRegionSizeFor4k = 0;
    }
#endif

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    EndingAddress = (PVOID)(((ULONG_PTR)CapturedBase +
                                CapturedRegionSize - 1L) | (PAGE_SIZE - 1L));

    StartingAddress = (PVOID)PAGE_ALIGN(CapturedBase);

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorFound;
    }

    FoundVad = MiCheckForConflictingVad (Process, StartingAddress, EndingAddress);

    if (FoundVad == NULL) {

        //
        // No virtual address is reserved at the specified base address,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    //
    // Ensure that the starting and ending addresses are all within
    // the same virtual address descriptor.
    //

    if ((MI_VA_TO_VPN (StartingAddress) < FoundVad->StartingVpn) ||
        (MI_VA_TO_VPN (EndingAddress) > FoundVad->EndingVpn)) {

        //
        // Not within the section virtual address descriptor,
        // return an error.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.UserPhysicalPages == 1) {

        //
        // These regions are always readwrite (but no execute).
        //

        if (ProtectionMask == MM_READWRITE) {

            UNLOCK_ADDRESS_SPACE (Process);

            *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
            *BaseAddress = StartingAddress;
            *LastProtect = PAGE_READWRITE;

            return STATUS_SUCCESS;
        }

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.PhysicalMapping == 1) {

        //
        // Setting the protection of a physically mapped section is
        // not allowed as there is no corresponding PFN database element.
        //

        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorFound;
    }

    if (FoundVad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is made at changing the protection
        // of a secured VAD, check to see if the address range
        // to change allows the change.
        //

        Status = MiCheckSecuredVad (FoundVad,
                                    CapturedBase,
                                    CapturedRegionSize,
                                    ProtectionMask);

        if (!NT_SUCCESS (Status)) {
            goto ErrorFound;
        }
    }
#if defined(_MIALT4K_)
    else if (EmulationFor4kPage == TRUE) {

        if (StartingAddressFor4k >= (PVOID)MM_MAX_WOW64_ADDRESS) {
            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorFound;
        }

        //
        // If not secured, relax the protection.
        //

        NewProtect = MiMakeProtectForNativePage (StartingAddressFor4k, 
                                                 NewProtect, 
                                                 Process);

        ProtectionMask = MiMakeProtectionMask(NewProtect);

        if (ProtectionMask == MM_INVALID_PROTECTION) {
            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorFound;
        }
    }
#endif

    if (FoundVad->u.VadFlags.PrivateMemory == 0) {

        //
        // For mapped sections, the NO_CACHE attribute is not allowed.
        //

        if (NewProtect & PAGE_NOCACHE) {

            //
            // Not allowed.
            //

            Status = STATUS_INVALID_PARAMETER_4;
            goto ErrorFound;
        }

        //
        // Make sure the section page protection is compatible with
        // the specified page protection.
        //

        if ((FoundVad->ControlArea->u.Flags.Image == 0) &&
            (!MiIsPteProtectionCompatible ((ULONG)FoundVad->u.VadFlags.Protection,
                                           OriginalProtect))) {
            Status = STATUS_SECTION_PROTECTION;
            goto ErrorFound;
        }

        //
        // If this is a file mapping, then all pages must be
        // committed as there can be no sparse file maps. Images
        // can have non-committed pages if the alignment is greater
        // than the page size.
        //

        if ((FoundVad->ControlArea->u.Flags.File == 0) ||
            (FoundVad->ControlArea->u.Flags.Image == 1)) {

            PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                        MI_VA_TO_VPN (StartingAddress));
            LastProtoPte = MiGetProtoPteAddress (FoundVad,
                                        MI_VA_TO_VPN (EndingAddress));

            //
            // Release the working set mutex and acquire the section
            // commit mutex.  Check all the prototype PTEs described by
            // the virtual address range to ensure they are committed.
            //

            ExAcquireFastMutexUnsafe (&MmSectionCommitMutex);

            while (PointerProtoPte <= LastProtoPte) {

                //
                // Check to see if the prototype PTE is committed, if
                // not return an error.
                //

                if (PointerProtoPte->u.Long == 0) {

                    //
                    // Error, this prototype PTE is not committed.
                    //

                    ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);
                    Status = STATUS_NOT_COMMITTED;
                    goto ErrorFound;
                }
                PointerProtoPte += 1;
            }

            //
            // The range is committed, release the section commitment
            // mutex, acquire the working set mutex and update the local PTEs.
            //

            ExReleaseFastMutexUnsafe (&MmSectionCommitMutex);
        }

#if defined(_MIALT4K_)

        //
        // The alternate permission table must be updated before PTEs
        // are created for the protection change.
        //

        if (EmulationFor4kPage == TRUE) {

            //
            // Capture the old protection.
            //

            CapturedOldProtectFor4k = 
                MiQueryProtectionFor4kPage (StartingAddressFor4k, Process);
            
            if (CapturedOldProtectFor4k != 0) {
 
                CapturedOldProtectFor4k = 
                    MI_CONVERT_FROM_PTE_PROTECTION(CapturedOldProtectFor4k);

            }

            //
            // Update the alternate permission table.
            //

            if ((FoundVad->u.VadFlags.ImageMap == 1) ||
                (FoundVad->u2.VadFlags2.CopyOnWrite == 1)) {

                //
                // Only set the MM_PROTECTION_COPY_MASK if the new protection
                // includes MM_PROTECTION_WRITE_MASK, otherwise, it will be
                // considered as MM_READ inside MiProtectFor4kPage ().
                //

                if ((OriginalProtectionMask & MM_PROTECTION_WRITE_MASK) == MM_PROTECTION_WRITE_MASK) {
                    OriginalProtectionMask |= MM_PROTECTION_COPY_MASK;
                }

            }

            MiProtectFor4kPage (StartingAddressFor4k, 
                                CapturedRegionSizeFor4k, 
                                OriginalProtectionMask, 
                                ALT_CHANGE,
                                Process);
        }
#endif

        //
        // Set the protection on the section pages.
        //

        Status = MiSetProtectionOnSection (Process,
                                           FoundVad,
                                           StartingAddress,
                                           EndingAddress,
                                           NewProtect,
                                           &CapturedOldProtect,
                                           FALSE,
                                           &Locked);

        //
        //      ***  WARNING ***
        //
        // The alternate PTE support routines called by MiSetProtectionOnSection
        // may have deleted the old (small) VAD and replaced it with a different
        // (large) VAD - if so, the old VAD is freed and cannot be referenced.
        //

        if (!NT_SUCCESS (Status)) {
            goto ErrorFound;
        }
    }
    else {

        //
        // Not a section, private.
        // For private pages, the WRITECOPY attribute is not allowed.
        //

        if ((NewProtect & PAGE_WRITECOPY) ||
            (NewProtect & PAGE_EXECUTE_WRITECOPY)) {

            //
            // Not allowed.
            //

            Status = STATUS_INVALID_PARAMETER_4;
            goto ErrorFound;
        }

        LOCK_WS_UNSAFE (Process);

        //
        // Ensure all of the pages are already committed as described
        // in the virtual address descriptor.
        //

        if ( !MiIsEntireRangeCommitted (StartingAddress,
                                        EndingAddress,
                                        FoundVad,
                                        Process)) {

            //
            // Previously reserved pages have been decommitted, or an error
            // occurred, release mutex and return status.
            //

            UNLOCK_WS_UNSAFE (Process);
            Status = STATUS_NOT_COMMITTED;
            goto ErrorFound;
        }

#if defined(_MIALT4K_)

        //
        // The alternate permission table must be updated before PTEs
        // are created for the protection change.
        //

        if (EmulationFor4kPage == TRUE) {

            //
            // Before accessing Alternate Table, unlock the working set mutex.
            //

            UNLOCK_WS_UNSAFE (Process);

            //
            // Get the old protection
            //

            CapturedOldProtectFor4k = 
                MiQueryProtectionFor4kPage(StartingAddressFor4k, Process);
            
            if (CapturedOldProtectFor4k != 0) {
 
                CapturedOldProtectFor4k = 
                    MI_CONVERT_FROM_PTE_PROTECTION(CapturedOldProtectFor4k);

            }

            //
            // Update the alternate permission table.
            //

            MiProtectFor4kPage (StartingAddressFor4k, 
                                CapturedRegionSizeFor4k, 
                                OriginalProtectionMask, 
                                ALT_CHANGE,
                                Process);

            LOCK_WS_UNSAFE (Process);
        }
#endif

        //
        // The address range is committed, change the protection.
        //

        PointerPde = MiGetPdeAddress (StartingAddress);
        PointerPte = MiGetPteAddress (StartingAddress);
        LastPte = MiGetPteAddress (EndingAddress);

        MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);

        //
        // Capture the protection for the first page.
        //

        if (PointerPte->u.Long != 0) {

            CapturedOldProtect = MiGetPageProtection (PointerPte, Process, FALSE);

            //
            // Make sure the page directory & table pages are still resident.
            //

            MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);

        }
        else {

            //
            // Get the protection from the VAD.
            //

            CapturedOldProtect =
               MI_CONVERT_FROM_PTE_PROTECTION(FoundVad->u.VadFlags.Protection);
        }

        //
        // For all the PTEs in the specified address range, set the
        // protection depending on the state of the PTE.
        //

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress(PointerPte);

                MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);
            }

            PteContents = *PointerPte;

            if (PteContents.u.Long == 0) {

                //
                // Increment the count of non-zero page table entries
                // for this page table and the number of private pages
                // for the process.  The protection will be set as
                // if the PTE was demand zero.
                //

                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));

                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }

            if (PteContents.u.Hard.Valid == 1) {

                //
                // Set the protection into both the PTE and the original PTE
                // in the PFN database.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                if (Pfn1->u3.e1.PrototypePte == 1) {

                    //
                    // This PTE refers to a fork prototype PTE, make it
                    // private.
                    //

                    MiCopyOnWrite (MiGetVirtualAddressMappedByPte (PointerPte),
                                   PointerPte);

                    //
                    // This may have released the working set mutex and
                    // the page directory and table pages may no longer be
                    // in memory.
                    //

                    do {

                        MiDoesPxeExistAndMakeValid (MiGetPdeAddress (PointerPde),
                                                    Process,
                                                    FALSE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS >= 4)
                        Waited = 0;
#endif

                        MiDoesPpeExistAndMakeValid (MiGetPteAddress (PointerPde),
                                                    Process,
                                                    FALSE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS < 4)
                        Waited = 0;
#endif

                        MiDoesPdeExistAndMakeValid (PointerPde,
                                                    Process,
                                                    FALSE,
                                                    &Waited);

                    } while (Waited != 0);

                    //
                    // Do the loop again for the same PTE.
                    //

                    continue;
                }

                //
                // The PTE is a private page which is valid, if the
                // specified protection is no-access or guard page
                // remove the PTE from the working set.
                //

                if ((NewProtect & PAGE_NOACCESS) || (NewProtect & PAGE_GUARD)) {

                    //
                    // Remove the page from the working set.
                    //

                    Locked = MiRemovePageFromWorkingSet (PointerPte,
                                                         Pfn1,
                                                         &Process->Vm);

                    continue;
                }

                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

                MI_MAKE_VALID_PTE (TempPte,
                                   PointerPte->u.Hard.PageFrameNumber,
                                   ProtectionMask,
                                   PointerPte);

                WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);
                MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);

                //
                // Flush the TB as we have changed the protection
                // of a valid PTE.
                //

                PreviousPte.u.Flush = MiFlushTbAndCapture (PointerPte,
                                                           TempPte.u.Flush,
                                                           Pfn1);
            }
            else {

                if (PteContents.u.Soft.Prototype == 1) {

                    //
                    // This PTE refers to a fork prototype PTE, make the
                    // page private.  This is accomplished by releasing
                    // the working set mutex, reading the page thereby
                    // causing a fault, and re-executing the loop, hopefully,
                    // this time, we'll find the page present and will
                    // turn it into a private page.
                    //
                    // Note, that a TRY is used to catch guard
                    // page exceptions and no-access exceptions.
                    //

                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    DoAgain = TRUE;

                    while (PteContents.u.Hard.Valid == 0) {
    
                        UNLOCK_WS_UNSAFE (Process);
                        WsHeld = FALSE;
    
                        try {
    
                            *(volatile ULONG *)Va;
    
                        } except (EXCEPTION_EXECUTE_HANDLER) {
    
                            if (GetExceptionCode() == STATUS_ACCESS_VIOLATION) {
    
                                //
                                // The prototype PTE must be noaccess.
                                //
    
                                WsHeld = TRUE;
                                LOCK_WS_UNSAFE (Process);
                                MiMakePdeExistAndMakeValid (PointerPde,
                                                            Process,
                                                            FALSE);
    
                                if (MiChangeNoAccessForkPte (PointerPte, ProtectionMask) == TRUE) {
                                    DoAgain = FALSE;
                                }
                            }
                            else if (GetExceptionCode() == STATUS_IN_PAGE_ERROR) {
    
                                //
                                // Ignore this page and go on to the next one.
                                //
    
                                PointerPte += 1;
                                DoAgain = TRUE;
    
                                WsHeld = TRUE;
                                LOCK_WS_UNSAFE (Process);
                                break;
                            }
                        }
    
                        if (WsHeld == FALSE) {
                            LOCK_WS_UNSAFE (Process);
                        }
    
                        MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);
    
                        PteContents = *PointerPte;
                    }

                    if (DoAgain) {
                        continue;
                    }

                }
                else {

                    if (PteContents.u.Soft.Transition == 1) {

                        if (MiSetProtectionOnTransitionPte (
                                                    PointerPte,
                                                    ProtectionMask)) {
                            continue;
                        }
                    }
                    else {

                        //
                        // Must be page file space or demand zero.
                        //

                        PointerPte->u.Soft.Protection = ProtectionMask;
                        ASSERT (PointerPte->u.Long != 0);
                    }
                }
            }

            PointerPte += 1;

        } //end while

        UNLOCK_WS_UNSAFE (Process);
    }

    UNLOCK_ADDRESS_SPACE (Process);

    //
    // Common completion code.
    //

#if defined(_MIALT4K_)

    if (EmulationFor4kPage == TRUE) {

        StartingAddress = StartingAddressFor4k;

        EndingAddress = EndingAddressFor4k;
            
        if (CapturedOldProtectFor4k != 0) {

            //
            // change CapturedOldProtect when CapturedOldProtectFor4k
            // contains the true protection for the 4k page
            //

            CapturedOldProtect = CapturedOldProtectFor4k;

        }
    }
#endif

    *RegionSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    *BaseAddress = StartingAddress;
    *LastProtect = CapturedOldProtect;

    if (Locked) {
        return STATUS_WAS_UNLOCKED;
    }

    return STATUS_SUCCESS;

ErrorFound:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}


NTSTATUS
MiSetProtectionOnSection (
    IN PEPROCESS Process,
    IN PMMVAD FoundVad,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN ULONG NewProtect,
    OUT PULONG CapturedOldProtect,
    IN ULONG DontCharge,
    OUT PULONG Locked
    )

/*++

Routine Description:

    This routine changes the protection on a region of committed pages
    within the virtual address space of the subject process.  Setting
    the protection on a range of pages causes the old protection to be
    replaced by the specified protection value.

Arguments:

    Process - Supplies a pointer to the current process.

    FoundVad - Supplies a pointer to the VAD containing the range to protect.

    StartingAddress - Supplies the starting address to protect.

    EndingAddress - Supplies the ending address to protect.

    NewProtect - Supplies the new protection to set.

    CapturedOldProtect - Supplies the address of a kernel owned pointer to
                store (without probing) the old protection into.

    DontCharge - Supplies TRUE if no quota or commitment should be charged.

    Locked - Receives TRUE if a locked page was removed from the working
             set (protection was guard page or no-access), FALSE otherwise.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, address creation mutex held, APCs disabled.

--*/

{
    LOGICAL WsHeld;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE PointerProtoPte;
    PMMPFN Pfn1;
    MMPTE TempPte;
    MMPTE PreviousPte;
    ULONG ProtectionMask;
    ULONG ProtectionMaskNotCopy;
    ULONG NewProtectionMask;
    MMPTE PteContents;
    WSLE_NUMBER Index;
    PULONG Va;
    ULONG WriteCopy;
    ULONG DoAgain;
    ULONG Waited;
    SIZE_T QuotaCharge;
    PVOID UsedPageTableHandle;
    ULONG WorkingSetIndex;
    NTSTATUS Status;

#if DBG

#define PTES_TRACKED 0x10

    ULONG PteIndex = 0;
    MMPTE PteTracker[PTES_TRACKED];
    MMPFN PfnTracker[PTES_TRACKED];
    SIZE_T PteQuotaCharge;
#endif

    PAGED_CODE();

    *Locked = FALSE;
    WriteCopy = FALSE;
    QuotaCharge = 0;

    //
    // Make the protection field.
    //

    ASSERT (FoundVad->u.VadFlags.PrivateMemory == 0);

    if ((FoundVad->u.VadFlags.ImageMap == 1) ||
        (FoundVad->u2.VadFlags2.CopyOnWrite == 1)) {

        if (NewProtect & PAGE_READWRITE) {
            NewProtect &= ~PAGE_READWRITE;
            NewProtect |= PAGE_WRITECOPY;
        }

        if (NewProtect & PAGE_EXECUTE_READWRITE) {
            NewProtect &= ~PAGE_EXECUTE_READWRITE;
            NewProtect |= PAGE_EXECUTE_WRITECOPY;
        }
    }

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {

        //
        // Return the error.
        //

        return STATUS_INVALID_PAGE_PROTECTION;
    }

    //
    // Determine if copy on write is being set.
    //

    ProtectionMaskNotCopy = ProtectionMask;
    if ((ProtectionMask & MM_COPY_ON_WRITE_MASK) == MM_COPY_ON_WRITE_MASK) {
        WriteCopy = TRUE;
        ProtectionMaskNotCopy &= ~MM_PROTECTION_COPY_MASK;
    }

#if defined(_MIALT4K_)

    if ((Process->Wow64Process != NULL) && 
        (FoundVad->u.VadFlags.ImageMap == 0) &&
        (FoundVad->u2.VadFlags2.CopyOnWrite == 0) && 
        (WriteCopy)) {
        
        PMMVAD NewVad;

        Status = MiSetCopyPagesFor4kPage (Process,
                                          FoundVad,
                                          StartingAddress,
                                          EndingAddress,
                                          ProtectionMask,
                                          &NewVad);
        if (!NT_SUCCESS (Status)) {
            return Status;
        }

        //
        //  *** WARNING ***
        //
        // The alternate PTE support routines may need to expand the entry
        // VAD - if so, the old VAD is freed and cannot be referenced.
        //

        ASSERT (NewVad != NULL);

        FoundVad = NewVad;
    }
        
#endif

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    LOCK_WS_UNSAFE (Process);

    MiMakePdeExistAndMakeValid(PointerPde, Process, FALSE);

    //
    // Capture the protection for the first page.
    //

    if (PointerPte->u.Long != 0) {

        *CapturedOldProtect = MiGetPageProtection (PointerPte, Process, FALSE);

        //
        // Make sure the Page table page is still resident.
        //

        PointerPpe = MiGetPteAddress (PointerPde);
        PointerPxe = MiGetPdeAddress (PointerPde);

        do {

            MiDoesPxeExistAndMakeValid (PointerPxe, Process, FALSE, &Waited);
#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            MiDoesPpeExistAndMakeValid (PointerPpe, Process, FALSE, &Waited);
#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            MiDoesPdeExistAndMakeValid (PointerPde, Process, FALSE, &Waited);
        } while (Waited != 0);

    }
    else {

        //
        // Get the protection from the VAD, unless image file.
        //

        if (FoundVad->u.VadFlags.ImageMap == 0) {

            //
            // This is not an image file, the protection is in the VAD.
            //

            *CapturedOldProtect =
                MI_CONVERT_FROM_PTE_PROTECTION(FoundVad->u.VadFlags.Protection);
        }
        else {

            //
            // This is an image file, the protection is in the
            // prototype PTE.
            //

            PointerProtoPte = MiGetProtoPteAddress (FoundVad,
                                    MI_VA_TO_VPN (
                                    MiGetVirtualAddressMappedByPte (PointerPte)));

            TempPte = MiCaptureSystemPte (PointerProtoPte, Process);

            *CapturedOldProtect = MiGetPageProtection (&TempPte,
                                                       Process,
                                                       TRUE);

            //
            // Make sure the Page directory and table pages are still resident.
            //

            PointerPpe = MiGetPteAddress (PointerPde);
            PointerPxe = MiGetPdeAddress (PointerPde);

            do {

                MiDoesPxeExistAndMakeValid(PointerPxe, Process, FALSE, &Waited);
#if (_MI_PAGING_LEVELS >= 4)
                Waited = 0;
#endif

                MiDoesPpeExistAndMakeValid(PointerPpe, Process, FALSE, &Waited);
#if (_MI_PAGING_LEVELS < 4)
                Waited = 0;
#endif

                MiDoesPdeExistAndMakeValid(PointerPde, Process, FALSE, &Waited);
            } while (Waited != 0);
        }
    }

    //
    // If the page protection is being change to be copy-on-write, the
    // commitment and page file quota for the potentially dirty private pages
    // must be calculated and charged.  This must be done before any
    // protections are changed as the changes cannot be undone.
    //

    if (WriteCopy) {

        //
        // Calculate the charges.  If the page is shared and not write copy
        // it is counted as a charged page.
        //

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif
                do {

                    while (!MiDoesPxeExistAndMakeValid(PointerPxe, Process, FALSE, &Waited)) {

                        //
                        // No PXE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PXE.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerProtoPte = PointerPte;
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }
                        QuotaCharge += PointerPte - PointerProtoPte;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif

                    while (!MiDoesPpeExistAndMakeValid(PointerPpe, Process, FALSE, &Waited)) {

                        //
                        // No PPE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PPE.
                        //

                        PointerPpe += 1;
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerProtoPte = PointerPte;
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }

#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPdeBoundary (PointerPpe)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            goto retry;
                        }
#endif
                        QuotaCharge += PointerPte - PointerProtoPte;
                    }

#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif

                    while (!MiDoesPdeExistAndMakeValid(PointerPde, Process, FALSE, &Waited)) {

                        //
                        // No PDE exists for this address.  Therefore
                        // all the PTEs are shared and not copy on write.
                        // go to the next PDE.
                        //

                        PointerPde += 1;
                        PointerProtoPte = PointerPte;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            QuotaCharge += 1 + LastPte - PointerProtoPte;
                            goto Done;
                        }
                        QuotaCharge += PointerPte - PointerProtoPte;
#if (_MI_PAGING_LEVELS >= 3)
                        if (MiIsPteOnPdeBoundary (PointerPde)) {
                            Waited = 1;
                            break;
                        }
#endif
                    }
                } while (Waited != 0);
            }

            PteContents = *PointerPte;

            if (PteContents.u.Long == 0) {

                //
                // The PTE has not been evaluated, assume copy on write.
                //

                QuotaCharge += 1;

            }
            else if (PteContents.u.Hard.Valid == 1) {
                if (PteContents.u.Hard.CopyOnWrite == 0) {

                    //
                    // See if this is a prototype PTE, if so charge it.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                    if (Pfn1->u3.e1.PrototypePte == 1) {
                        QuotaCharge += 1;
                    }
                }
            }
            else {

                if (PteContents.u.Soft.Prototype == 1) {

                    //
                    // This is a prototype PTE.  Charge if it is not
                    // in copy on write format.
                    //

                    if (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED) {
                        //
                        // Page protection is within the PTE.
                        //

                        if (!MI_IS_PTE_PROTECTION_COPY_WRITE(PteContents.u.Soft.Protection)) {
                            QuotaCharge += 1;
                        }
                    }
                    else {

                        //
                        // The PTE references the prototype directly, therefore
                        // it can't be copy on write.  Charge.
                        //

                        QuotaCharge += 1;
                    }
                }
            }
            PointerPte += 1;
        }

Done:

        //
        // If any quota is required, charge for it now.
        //

        if ((!DontCharge) && (QuotaCharge != 0)) {

            Status = PsChargeProcessPageFileQuota (Process, QuotaCharge);
            if (!NT_SUCCESS (Status)) {
                UNLOCK_WS_UNSAFE (Process);
                return STATUS_PAGEFILE_QUOTA_EXCEEDED;
            }

            if (Process->CommitChargeLimit) {
                if (Process->CommitCharge + QuotaCharge > Process->CommitChargeLimit) {
                    PsReturnProcessPageFileQuota (Process, QuotaCharge);
                    if (Process->Job) {
                        PsReportProcessMemoryLimitViolation ();
                    }
                    UNLOCK_WS_UNSAFE (Process);
                    return STATUS_COMMITMENT_LIMIT;
                }
            }
            if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                if (PsChangeJobMemoryUsage(QuotaCharge) == FALSE) {
                    PsReturnProcessPageFileQuota (Process, QuotaCharge);
                    UNLOCK_WS_UNSAFE (Process);
                    return STATUS_COMMITMENT_LIMIT;
                }
            }

            if (MiChargeCommitment (QuotaCharge, Process) == FALSE) {
                if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                    PsChangeJobMemoryUsage(-(SSIZE_T)QuotaCharge);
                }
                PsReturnProcessPageFileQuota (Process, QuotaCharge);
                UNLOCK_WS_UNSAFE (Process);
                return STATUS_COMMITMENT_LIMIT;
            }

            //
            // Add the quota into the charge to the VAD.
            //

            MM_TRACK_COMMIT (MM_DBG_COMMIT_SET_PROTECTION, QuotaCharge);
            FoundVad->u.VadFlags.CommitCharge += QuotaCharge;
            Process->CommitCharge += QuotaCharge;
            if (Process->CommitCharge > Process->CommitChargePeak) {
                Process->CommitChargePeak = Process->CommitCharge;
            }
            MI_INCREMENT_TOTAL_PROCESS_COMMIT (QuotaCharge);
        }
    }

#if DBG
    PteQuotaCharge = QuotaCharge;
#endif

    //
    // For all the PTEs in the specified address range, set the
    // protection depending on the state of the PTE.
    //

    //
    // If the PTE was copy on write (but not written) and the
    // new protection is NOT copy-on-write, return page file quota
    // and commitment.
    //

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);

    do {

        MiDoesPxeExistAndMakeValid (PointerPxe, Process, FALSE, &Waited);
#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        MiDoesPpeExistAndMakeValid (PointerPpe, Process, FALSE, &Waited);

#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

        MiDoesPdeExistAndMakeValid (PointerPde, Process, FALSE, &Waited);

    } while (Waited != 0);

    QuotaCharge = 0;

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {
            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPdeAddress (PointerPte);
            PointerPxe = MiGetPpeAddress (PointerPte);

            MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);
        }

        PteContents = *PointerPte;

        if (PteContents.u.Long == 0) {

            //
            // Increment the count of non-zero page table entries
            // for this page table and the number of private pages
            // for the process.
            //

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            //
            // The PTE is zero, set it into prototype PTE format
            // with the protection in the prototype PTE.
            //

            TempPte = PrototypePte;
            TempPte.u.Soft.Protection = ProtectionMask;
            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
        }
        else if (PteContents.u.Hard.Valid == 1) {

            //
            // Set the protection into both the PTE and the original PTE
            // in the PFN database for private pages only.
            //

            NewProtectionMask = ProtectionMask;

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

#if DBG
            if (PteIndex < PTES_TRACKED) {
                PteTracker[PteIndex] = PteContents;
                PfnTracker[PteIndex] = *Pfn1;
                PteIndex += 1;
            }
#endif

            if ((NewProtect & PAGE_NOACCESS) ||
                (NewProtect & PAGE_GUARD)) {

                *Locked = MiRemovePageFromWorkingSet (PointerPte,
                                                      Pfn1,
                                                      &Process->Vm);
                continue;
            }

            if (Pfn1->u3.e1.PrototypePte == 1) {

                //
                // The true protection may be in the WSLE, locate
                // the WSLE.
                //

                Va = (PULONG)MiGetVirtualAddressMappedByPte (PointerPte);

                //
                // Check to see if this is a prototype PTE.  This
                // is done by comparing the PTE address in the
                // PFN database to the PTE address indicated by the VAD.
                // If they are not equal, this is a fork prototype PTE.
                //

                if (Pfn1->PteAddress !=
                              MiGetProtoPteAddress (FoundVad,
                                          MI_VA_TO_VPN ((PVOID)Va))) {

                    //
                    // This PTE refers to a fork prototype PTE, make it
                    // private.
                    //

                    if (MiCopyOnWrite ((PVOID)Va, PointerPte) == TRUE) {
                        if (WriteCopy) {
                            QuotaCharge += 1;
                        }
                    }

                    //
                    // This may have released the working set mutex and
                    // the page table page may no longer be in memory.
                    //

                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);

                    do {

                        MiDoesPxeExistAndMakeValid (PointerPxe,
                                                    Process,
                                                    FALSE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS >= 4)
                        Waited = 0;
#endif

                        MiDoesPpeExistAndMakeValid (PointerPpe,
                                                    Process,
                                                    FALSE,
                                                    &Waited);

#if (_MI_PAGING_LEVELS < 4)
                        Waited = 0;
#endif

                        MiDoesPdeExistAndMakeValid (PointerPde,
                                                    Process,
                                                    FALSE,
                                                    &Waited);

                    } while (Waited != 0);

                    //
                    // Do the loop again.
                    //

                    continue;

                }

                //
                // Update the protection field in the WSLE and
                // the PTE.
                //
                //
                // If the PTE is copy on write uncharge the
                // previously charged quota.
                //

                if ((!WriteCopy) && (PteContents.u.Hard.CopyOnWrite == 1)) {
                    QuotaCharge += 1;
                }

                //
                // The true protection may be in the WSLE, locate it.
                //

                Index = MiLocateWsle ((PVOID)Va, 
                                      MmWorkingSetList,
                                      Pfn1->u1.WsIndex);

                MmWsle[Index].u1.e1.Protection = ProtectionMask;
                MmWsle[Index].u1.e1.SameProtectAsProto = 0;
            }
            else {

                //
                // Page is private (copy on written), protection mask
                // is stored in the original PTE field.
                //

                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMaskNotCopy;

                NewProtectionMask = ProtectionMaskNotCopy;
            }

            MI_SNAP_DATA (Pfn1, PointerPte, 7);

            MI_MAKE_VALID_PTE (TempPte,
                               PteContents.u.Hard.PageFrameNumber,
                               NewProtectionMask,
                               PointerPte);

            WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);

            MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);

            //
            // Flush the TB as we have changed the protection
            // of a valid PTE.
            //

            PreviousPte.u.Flush = MiFlushTbAndCapture (PointerPte,
                                                       TempPte.u.Flush,
                                                       Pfn1);
        }
        else {

            if (PteContents.u.Soft.Prototype == 1) {

#if DBG
                if (PteIndex < PTES_TRACKED) {
                    PteTracker[PteIndex] = PteContents;
                    *(PULONG)(&PfnTracker[PteIndex]) = 0x88;
                    PteIndex += 1;
                }
#endif

                //
                // The PTE is in prototype PTE format.
                //

                //
                // Is it a fork prototype PTE?
                //

                Va = (PULONG)MiGetVirtualAddressMappedByPte (PointerPte);

                if ((PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) &&
                   (MiPteToProto (PointerPte) !=
                                     MiGetProtoPteAddress (FoundVad,
                                         MI_VA_TO_VPN ((PVOID)Va)))) {

                    //
                    // This PTE refers to a fork prototype PTE, make the
                    // page private.  This is accomplished by releasing
                    // the working set mutex, reading the page thereby
                    // causing a fault, and re-executing the loop, hopefully,
                    // this time, we'll find the page present and will
                    // turn it into a private page.
                    //
                    // Note, that page with prototype = 1 cannot be
                    // no-access.
                    //

                    DoAgain = TRUE;

                    while (PteContents.u.Hard.Valid == 0) {
    
                        UNLOCK_WS_UNSAFE (Process);
    
                        WsHeld = FALSE;
    
                        try {
    
                            *(volatile ULONG *)Va;
                        } except (EXCEPTION_EXECUTE_HANDLER) {
    
                            if (GetExceptionCode() != STATUS_GUARD_PAGE_VIOLATION) {
    
                                //
                                // The prototype PTE must be noaccess.
                                //
    
                                WsHeld = TRUE;
                                LOCK_WS_UNSAFE (Process);
                                MiMakePdeExistAndMakeValid (PointerPde,
                                                            Process,
                                                            FALSE);
                                if (MiChangeNoAccessForkPte (PointerPte, ProtectionMask) == TRUE) {
                                    DoAgain = FALSE;
                                }
                            }
                        }
    
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPdeAddress (PointerPde);
    
                        if (WsHeld == FALSE) {
                            LOCK_WS_UNSAFE (Process);
                        }
    
                        MiMakePdeExistAndMakeValid (PointerPde, Process, FALSE);
    
                        PteContents = *PointerPte;
                    }

                    if (DoAgain) {
                        continue;
                    }

                }
                else {

                    //
                    // If the new protection is not write-copy, the PTE
                    // protection is not in the prototype PTE (can't be
                    // write copy for sections), and the protection in
                    // the PTE is write-copy, release the page file
                    // quota and commitment for this page.
                    //

                    if ((!WriteCopy) &&
                        (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED)) {
                        if (MI_IS_PTE_PROTECTION_COPY_WRITE(PteContents.u.Soft.Protection)) {
                            QuotaCharge += 1;
                        }

                    }

                    //
                    // The PTE is a prototype PTE.  Make the high part
                    // of the PTE indicate that the protection field
                    // is in the PTE itself.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, PrototypePte);
                    PointerPte->u.Soft.Protection = ProtectionMask;
                }

            }
            else {

                if (PteContents.u.Soft.Transition == 1) {

#if DBG
                    if (PteIndex < PTES_TRACKED) {
                        PteTracker[PteIndex] = PteContents;
                        Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
                        PfnTracker[PteIndex] = *Pfn1;
                        PteIndex += 1;
                    }
#endif

                    //
                    // This is a transition PTE. (Page is private)
                    //

                    if (MiSetProtectionOnTransitionPte (
                                                PointerPte,
                                                ProtectionMaskNotCopy)) {
                        continue;
                    }

                }
                else {

#if DBG
                    if (PteIndex < PTES_TRACKED) {
                        PteTracker[PteIndex] = PteContents;
                        *(PULONG)(&PfnTracker[PteIndex]) = 0x99;
                        PteIndex += 1;
                    }
#endif

                    //
                    // Must be page file space or demand zero.
                    //

                    PointerPte->u.Soft.Protection = ProtectionMaskNotCopy;
                }
            }
        }

        PointerPte += 1;
    }

    //
    // Return the quota charge and the commitment, if any.
    //

    if ((QuotaCharge > 0) && (!DontCharge)) {

        MiReturnCommitment (QuotaCharge);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROTECTION, QuotaCharge);
        PsReturnProcessPageFileQuota (Process, QuotaCharge);

#if DBG
        if (QuotaCharge > FoundVad->u.VadFlags.CommitCharge) {
            DbgPrint ("MMPROTECT QUOTA FAILURE: %p %p %x %p\n",
                PteTracker, PfnTracker, PteIndex, PteQuotaCharge);
            DbgBreakPoint ();
        }
#endif

        ASSERT (QuotaCharge <= FoundVad->u.VadFlags.CommitCharge);

        FoundVad->u.VadFlags.CommitCharge -= QuotaCharge;
        if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
            PsChangeJobMemoryUsage(-(SSIZE_T)QuotaCharge);
        }
        Process->CommitCharge -= QuotaCharge;

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - QuotaCharge);
    }

    UNLOCK_WS_UNSAFE (Process);

    return STATUS_SUCCESS;
}

ULONG
MiGetPageProtection (
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN LOGICAL PteCapturedToLocalStack
    )

/*++

Routine Description:

    This routine returns the page protection of a non-zero PTE.
    It may release and reacquire the working set mutex.

Arguments:

    PointerPte - Supplies a pointer to a non-zero PTE.

    Process - Supplies the relevant process if its working set mutex is held.

    PteCapturedToLocalStack - Supplies TRUE if PointerPte points at a
                              captured local stack location.

Return Value:

    Returns the protection code.

Environment:

    Kernel mode, working set and address creation mutex held.
    Note, that the address creation mutex does not need to be held
    if the working set mutex does not need to be released in the
    case of a prototype PTE.

--*/

{
    MMPTE PteContents;
    MMPTE ProtoPteContents;
    PMMPFN Pfn1;
    PMMPTE ProtoPteAddress;
    PVOID Va;
    WSLE_NUMBER Index;

    PAGED_CODE();

    //
    // Initializing ProtoPteContents is not needed for correctness,
    // but without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    ProtoPteContents = ZeroKernelPte;

    PteContents = *PointerPte;

    if ((PteContents.u.Soft.Valid == 0) && (PteContents.u.Soft.Prototype == 1)) {

        //
        // This PTE is in prototype format, the protection is
        // stored in the prototype PTE.
        //

        if ((MI_IS_PTE_PROTOTYPE(PointerPte)) ||
            (PteCapturedToLocalStack == TRUE) ||
            (PteContents.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED)) {

            //
            // The protection is within this PTE.
            //

            return MI_CONVERT_FROM_PTE_PROTECTION (
                                            PteContents.u.Soft.Protection);
        }

        ProtoPteAddress = MiPteToProto (PointerPte);

        //
        // Capture protopte PTE contents.
        //

        ProtoPteContents = MiCaptureSystemPte (ProtoPteAddress, Process);

        //
        // The working set mutex may have been released and the
        // page may no longer be in prototype format, get the
        // new contents of the PTE and obtain the protection mask.
        //

        PteContents = MiCaptureSystemPte (PointerPte, Process);
    }

    if ((PteContents.u.Soft.Valid == 0) && (PteContents.u.Soft.Prototype == 1)) {

        //
        // Pte is still prototype, return the protection captured
        // from the prototype PTE.
        //

        if (ProtoPteContents.u.Hard.Valid == 1) {

            //
            // The prototype PTE is valid, get the protection from
            // the PFN database.
            //

            Pfn1 = MI_PFN_ELEMENT (ProtoPteContents.u.Hard.PageFrameNumber);
            return MI_CONVERT_FROM_PTE_PROTECTION(
                                      Pfn1->OriginalPte.u.Soft.Protection);

        }
        else {

            //
            // The prototype PTE is not valid, return the protection from the
            // PTE.
            //

            return MI_CONVERT_FROM_PTE_PROTECTION (
                                     ProtoPteContents.u.Soft.Protection);
        }
    }

    if (PteContents.u.Hard.Valid == 1) {

        //
        // The page is valid, the protection field is either in the
        // PFN database original PTE element or the WSLE.  If
        // the page is private, get it from the PFN original PTE
        // element, else use the WSLE.
        //

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

        if ((Pfn1->u3.e1.PrototypePte == 0) ||
            (PteCapturedToLocalStack == TRUE) ||
            (MI_IS_PTE_PROTOTYPE(PointerPte))) {

            //
            // This is a private PTE or the PTE address is that of a
            // prototype PTE, hence the protection is in
            // the original PTE.
            //

            return MI_CONVERT_FROM_PTE_PROTECTION(
                                      Pfn1->OriginalPte.u.Soft.Protection);
        }

        //
        // The PTE was a hardware PTE, get the protection
        // from the WSLE.

        Va = (PULONG)MiGetVirtualAddressMappedByPte (PointerPte);

        Index = MiLocateWsle ((PVOID)Va,
                              MmWorkingSetList,
                              Pfn1->u1.WsIndex);

        return MI_CONVERT_FROM_PTE_PROTECTION (MmWsle[Index].u1.e1.Protection);
    }

    //
    // PTE is either demand zero or transition, in either
    // case protection is in PTE.
    //

    return MI_CONVERT_FROM_PTE_PROTECTION (PteContents.u.Soft.Protection);

}

ULONG
MiChangeNoAccessForkPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:


Arguments:

    PointerPte - Supplies a pointer to the current PTE.

    ProtectionMask - Supplies the protection mask to set.

Return Value:

    TRUE if the loop does NOT need to be repeated for this PTE, FALSE
    if it does need retrying.

Environment:

    Kernel mode, address creation mutex held, APCs disabled.

--*/

{
    PAGED_CODE();

    if (ProtectionMask == MM_NOACCESS) {

        //
        // No need to change the page protection.
        //

        return TRUE;
    }

    PointerPte->u.Proto.ReadOnly = 1;

    return FALSE;
}


HARDWARE_PTE
MiFlushTbAndCapture(
    IN PMMPTE PointerPte,
    IN HARDWARE_PTE TempPte,
    IN PMMPFN Pfn1
    )

// non pagable helper routine.

{
    MMPTE PreviousPte;
    KIRQL OldIrql;
    PEPROCESS Process;
    PVOID VirtualAddress;

    //
    // Flush the TB as we have changed the protection
    // of a valid PTE.
    //

    LOCK_PFN (OldIrql);

    PreviousPte.u.Flush = KeFlushSingleTb (
                            MiGetVirtualAddressMappedByPte (PointerPte),
                            FALSE,
                            FALSE,
                            (PHARDWARE_PTE)PointerPte,
                            TempPte);

    ASSERT (PreviousPte.u.Hard.Valid == 1);

    //
    // A page protection is being changed, on certain
    // hardware the dirty bit should be ORed into the
    // modify bit in the PFN element.
    //

    MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);

    //
    // If the PTE indicates the page has been modified (this is different
    // from the PFN indicating this), then ripple it back to the write watch
    // bitmap now since we are still in the correct process context.
    //

    if (MiActiveWriteWatch != 0) {
        if ((Pfn1->u3.e1.PrototypePte == 0) &&
            (MI_IS_PTE_DIRTY(PreviousPte))) {

            Process = PsGetCurrentProcess();

            if (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) {

                //
                // This process has (or had) write watch VADs.  Search now
                // for a write watch region encapsulating the PTE being
                // invalidated.
                //

                VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                MiCaptureWriteWatchDirtyBit (Process, VirtualAddress);
            }
        }
    }
#if DBG
    else {
        Process = PsGetCurrentProcess();
        ASSERT ((Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH) == 0);
    }
#endif

    UNLOCK_PFN (OldIrql);
    return PreviousPte.u.Flush;
}

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    )

    // nonpaged helper routine.

{
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPFN Pfn1;

    //
    // This is a transition PTE. (Page is private)
    //

    //
    // Need the PFN lock to ensure page doesn't become
    // non-transition.
    //

    LOCK_PFN (OldIrql);

    //
    // Make sure the page is still a transition page.
    //

    PteContents = *(volatile MMPTE *)PointerPte;

    if ((PteContents.u.Soft.Prototype == 0) &&
                     (PointerPte->u.Soft.Transition == 1)) {

        Pfn1 = MI_PFN_ELEMENT (
                      PteContents.u.Trans.PageFrameNumber);

        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
        PointerPte->u.Soft.Protection = ProtectionMask;
        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

    //
    // Do this loop again for the same PTE.
    //

    UNLOCK_PFN (OldIrql);
    return TRUE;
}

MMPTE
MiCaptureSystemPte (
    IN PMMPTE PointerProtoPte,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    Nonpagable helper routine to capture the contents of a pagable PTE.

Arguments:

    PointerProtoPte - Supplies a pointer to the prototype PTE.

    Process - Supplies the relevant process.

Return Value:

    PTE contents.

Environment:

    Kernel mode.  Caller holds address space and working set mutexes if
    Process is set.  Working set mutex was acquired unsafe.

--*/

{
    MMPTE TempPte;
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    MiMakeSystemAddressValidPfnWs (PointerProtoPte, Process);
    TempPte = *PointerProtoPte;
    UNLOCK_PFN (OldIrql);
    return TempPte;
}

NTSTATUS
MiCheckSecuredVad (
    IN PMMVAD Vad,
    IN PVOID Base,
    IN SIZE_T Size,
    IN ULONG ProtectionMask
    )

/*++

Routine Description:

    This routine checks to see if the specified VAD is secured in such
    a way as to conflict with the address range and protection mask
    specified.

Arguments:

    Vad - Supplies a pointer to the VAD containing the address range.

    Base - Supplies the base of the range the protection starts at.

    Size - Supplies the size of the range.

    ProtectionMask - Supplies the protection mask being set.

Return Value:

    Status value.

Environment:

    Kernel mode.

--*/

{
    PVOID End;
    PLIST_ENTRY Next;
    PMMSECURE_ENTRY Entry;
    NTSTATUS Status = STATUS_SUCCESS;

    End = (PVOID)((PCHAR)Base + Size);

    if (ProtectionMask < MM_SECURE_DELETE_CHECK) {
        if ((Vad->u.VadFlags.NoChange == 1) &&
            (Vad->u2.VadFlags2.SecNoChange == 1) &&
            (Vad->u.VadFlags.Protection != ProtectionMask)) {

            //
            // An attempt is made at changing the protection
            // of a SEC_NO_CHANGE section - return an error.
            //

            Status = STATUS_INVALID_PAGE_PROTECTION;
            goto done;
        }
    }
    else {

        //
        // Deletion - set to no-access for check.  SEC_NOCHANGE allows
        // deletion, but does not allow page protection changes.
        //

        ProtectionMask = 0;
    }

    if (Vad->u2.VadFlags2.OneSecured) {

        if (((ULONG_PTR)Base <= ((PMMVAD_LONG)Vad)->u3.Secured.EndVpn) &&
             ((ULONG_PTR)End >= ((PMMVAD_LONG)Vad)->u3.Secured.StartVpn)) {

            //
            // This region conflicts, check the protections.
            //

            if (ProtectionMask & MM_GUARD_PAGE) {
                Status = STATUS_INVALID_PAGE_PROTECTION;
                goto done;
            }

            if (Vad->u2.VadFlags2.ReadOnly) {
                if (MmReadWrite[ProtectionMask] < 10) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
            }
            else {
                if (MmReadWrite[ProtectionMask] < 11) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
            }
        }

    }
    else if (Vad->u2.VadFlags2.MultipleSecured) {

        Next = ((PMMVAD_LONG)Vad)->u3.List.Flink;
        do {
            Entry = CONTAINING_RECORD( Next,
                                       MMSECURE_ENTRY,
                                       List);

            if (((ULONG_PTR)Base <= Entry->EndVpn) &&
                ((ULONG_PTR)End >= Entry->StartVpn)) {

                //
                // This region conflicts, check the protections.
                //

                if (ProtectionMask & MM_GUARD_PAGE) {
                    Status = STATUS_INVALID_PAGE_PROTECTION;
                    goto done;
                }
    
                if (Entry->u2.VadFlags2.ReadOnly) {
                    if (MmReadWrite[ProtectionMask] < 10) {
                        Status = STATUS_INVALID_PAGE_PROTECTION;
                        goto done;
                    }
                }
                else {
                    if (MmReadWrite[ProtectionMask] < 11) {
                        Status = STATUS_INVALID_PAGE_PROTECTION;
                        goto done;
                    }
                }
            }
            Next = Entry->List.Flink;
        } while (Entry->List.Flink != &((PMMVAD_LONG)Vad)->u3.List);
    }

done:
    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\querysec.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   querysec.c

Abstract:

    This module contains the routines which implement the
    NtQuerySection service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/


#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtQuerySection)
#endif


NTSTATUS
NtQuerySection(
    IN HANDLE SectionHandle,
    IN SECTION_INFORMATION_CLASS SectionInformationClass,
    OUT PVOID SectionInformation,
    IN SIZE_T SectionInformationLength,
    OUT PSIZE_T ReturnLength OPTIONAL
    )

/*++

Routine Description:

   This function provides the capability to determine the base address,
   size, granted access, and allocation of an opened section object.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    SectionInformationClass - The section information class about
                              which to retrieve information.

    SectionInformation - A pointer to a buffer that receives the
                         specified information.  The format and content of the
                         buffer depend on the specified section class.

       SectionInformation Format by Information Class:

       SectionBasicInformation - Data type is PSECTION_BASIC_INFORMATION.

           SECTION_BASIC_INFORMATION Structure

           PVOID BaseAddress - The base virtual address of the
                               section if the section is based.

           LARGE_INTEGER MaximumSize - The maximum size of the section in
                                       bytes.

           ULONG AllocationAttributes - The allocation attributes flags.

               AllocationAttributes Flags

               SEC_BASED - The section is a based section.

               SEC_FILE - The section is backed by a data file.

               SEC_RESERVE - All pages of the section were initially
                             set to the reserved state.

               SEC_COMMIT - All pages of the section were initially
                            to the committed state.

               SEC_IMAGE - The section was mapped as an executable image file.

        SECTION_IMAGE_INFORMATION

    SectionInformationLength - Specifies the length in bytes of the
                               section information buffer.

    ReturnLength - An optional pointer which, if specified, receives the
                   number of bytes placed in the section information buffer.


Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    PSECTION Section;
    KPROCESSOR_MODE PreviousMode;

    PAGED_CODE();

    //
    // Get previous processor mode and probe output argument if necessary.
    //

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {

        //
        // Check arguments.
        //

        try {

            ProbeForWrite(SectionInformation,
                          SectionInformationLength,
                          sizeof(ULONG));

            if (ARGUMENT_PRESENT (ReturnLength)) {
                ProbeForWriteUlong_ptr (ReturnLength);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }

    //
    // Check argument validity.
    //

    if ((SectionInformationClass != SectionBasicInformation) &&
        (SectionInformationClass != SectionImageInformation)) {
        return STATUS_INVALID_INFO_CLASS;
    }

    if (SectionInformationClass == SectionBasicInformation) {
        if (SectionInformationLength < (ULONG)sizeof(SECTION_BASIC_INFORMATION)) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }
    }
    else {
        if (SectionInformationLength < (ULONG)sizeof(SECTION_IMAGE_INFORMATION)) {
            return STATUS_INFO_LENGTH_MISMATCH;
        }
    }

    //
    // Reference section object by handle for READ access, get the information
    // from the section object, dereference the section
    // object, fill in information structure, optionally return the length of
    // the information structure, and return service status.
    //

    Status = ObReferenceObjectByHandle (SectionHandle,
                                        SECTION_QUERY,
                                        MmSectionObjectType,
                                        PreviousMode,
                                        (PVOID *)&Section,
                                        NULL);

    if (NT_SUCCESS(Status)) {

        try {

            if (SectionInformationClass == SectionBasicInformation) {
                ((PSECTION_BASIC_INFORMATION)SectionInformation)->BaseAddress =
                                           (PVOID)Section->Address.StartingVpn;

                ((PSECTION_BASIC_INFORMATION)SectionInformation)->MaximumSize =
                                                 Section->SizeOfSection;

                ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes =
                                                        0;

                if (Section->u.Flags.Image) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes =
                                                        SEC_IMAGE;
                }
                if (Section->u.Flags.Based) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_BASED;
                }
                if (Section->u.Flags.File) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_FILE;
                }
                if (Section->u.Flags.NoCache) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_NOCACHE;
                }
                if (Section->u.Flags.Reserve) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_RESERVE;
                }
                if (Section->u.Flags.Commit) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_COMMIT;
                }
                if (Section->Segment->ControlArea->u.Flags.GlobalMemory) {
                    ((PSECTION_BASIC_INFORMATION)SectionInformation)->AllocationAttributes |=
                                                        SEC_GLOBAL;
                }

                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(SECTION_BASIC_INFORMATION);
                }
            }
            else {

                if (Section->u.Flags.Image == 0) {
                    Status = STATUS_SECTION_NOT_IMAGE;
                }
                else {
                    *((PSECTION_IMAGE_INFORMATION)SectionInformation) =
                        *Section->Segment->u2.ImageInformation;
    
                    if (ARGUMENT_PRESENT(ReturnLength)) {
                        *ReturnLength = sizeof(SECTION_IMAGE_INFORMATION);
                    }
                }
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode ();
        }

        ObDereferenceObject ((PVOID)Section);
    }
    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\physical.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   physical.c

Abstract:

    This module contains the routines to manipulate physical memory from
    user space.

    There are restrictions on how user controlled physical memory can be used.
    Realize that all this memory is nonpaged and hence applications should
    allocate this with care as it represents a very real system resource.

    Virtual memory which maps user controlled physical memory pages must be :

    1.  Private memory only (ie: cannot be shared between processes).

    2.  The same physical page cannot be mapped at 2 different virtual
        addresses.

    3.  Callers must have LOCK_VM privilege to create these VADs.

    4.  Device drivers cannot call MmSecureVirtualMemory on it - this means
        that applications should not expect to use this memory for win32k.sys
        calls.

    5.  NtProtectVirtualMemory only allows read-write protection on this
        memory.  No other protection (no access, guard pages, readonly, etc)
        are allowed.

    6.  NtFreeVirtualMemory allows only MEM_RELEASE and NOT MEM_DECOMMIT on
        these VADs.  Even MEM_RELEASE is only allowed on entire VAD ranges -
        that is, splitting of these VADs is not allowed.

    7.  fork() style child processes don't inherit physical VADs.

    8.  The physical pages in these VADs are not subject to job limits.

Author:

    Landy Wang (landyw) 25-Jan-1999

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtMapUserPhysicalPages)
#pragma alloc_text(PAGE,NtMapUserPhysicalPagesScatter)
#pragma alloc_text(PAGE,MiRemoveUserPhysicalPagesVad)
#pragma alloc_text(PAGE,MiAllocateAweInfo)
#pragma alloc_text(PAGE,MiCleanPhysicalProcessPages)
#pragma alloc_text(PAGE,NtAllocateUserPhysicalPages)
#pragma alloc_text(PAGE,NtFreeUserPhysicalPages)
#pragma alloc_text(PAGE,MiAweViewInserter)
#pragma alloc_text(PAGE,MiAweViewRemover)
#endif

//
// This local stack size definition is deliberately large as ISVs have told
// us they expect to typically do up to this amount.
//

#define COPY_STACK_SIZE         1024
#define SMALL_COPY_STACK_SIZE    512

#define BITS_IN_ULONG ((sizeof (ULONG)) * 8)
    
#define LOWEST_USABLE_PHYSICAL_ADDRESS    (16 * 1024 * 1024)
#define LOWEST_USABLE_PHYSICAL_PAGE       (LOWEST_USABLE_PHYSICAL_ADDRESS >> PAGE_SHIFT)

#define LOWEST_BITMAP_PHYSICAL_PAGE       0
#define MI_FRAME_TO_BITMAP_INDEX(x)       ((ULONG)(x))
#define MI_BITMAP_INDEX_TO_FRAME(x)       ((ULONG)(x))

PFN_NUMBER MmVadPhysicalPages;

#if DBG
LOGICAL MiUsingLowPagesForAwe = FALSE;
#endif


NTSTATUS
NtMapUserPhysicalPages (
    IN PVOID VirtualAddress,
    IN ULONG_PTR NumberOfPages,
    IN PULONG_PTR UserPfnArray OPTIONAL
    )

/*++

Routine Description:

    This function maps the specified nonpaged physical pages into the specified
    user address range.

    Note no WSLEs are maintained for this range as it is all nonpaged.

Arguments:

    VirtualAddress - Supplies a user virtual address within a UserPhysicalPages
                     Vad.
        
    NumberOfPages - Supplies the number of pages to map.
        
    UserPfnArray - Supplies a pointer to the page frame numbers to map in.
                   If this is zero, then the virtual addresses are set to
                   NO_ACCESS.

Return Value:

    Various NTSTATUS codes.

--*/

{
    KIRQL OldIrql;
    ULONG_PTR OldValue;
    ULONG_PTR NewValue;
    PAWEINFO AweInfo;
    PULONG BitBuffer;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PVOID EndAddress;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS Status;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID PoolArea;
    PVOID PoolAreaEnd;
    PPFN_NUMBER FrameList;
    ULONG BitMapIndex;
    ULONG_PTR StackArray[COPY_STACK_SIZE];
    MMPTE OldPteContents;
    MMPTE OriginalPteContents;
    MMPTE NewPteContents;
    MMPTE JunkPte;
    ULONG_PTR NumberOfBytes;
    ULONG SizeOfBitMap;
    PRTL_BITMAP BitMap;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    PEX_PUSH_LOCK PushLock;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (NumberOfPages > (MAXULONG_PTR / PAGE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    VirtualAddress = PAGE_ALIGN(VirtualAddress);
    EndAddress = (PVOID)((PCHAR)VirtualAddress + (NumberOfPages << PAGE_SHIFT) -1);

    if (EndAddress <= VirtualAddress) {
        return STATUS_INVALID_PARAMETER_2;
    }

    //
    // Carefully probe and capture all user parameters.
    //

    FrameList = NULL;
    PoolArea = (PVOID)&StackArray[0];

    if (ARGUMENT_PRESENT(UserPfnArray)) {

        //
        // Check for zero pages here so the loops further down can be optimized
        // taking into account this can never happen.
        //

        if (NumberOfPages == 0) {
            return STATUS_SUCCESS;
        }

        NumberOfBytes = NumberOfPages * sizeof(ULONG_PTR);

        if (NumberOfPages > COPY_STACK_SIZE) {
            PoolArea = ExAllocatePoolWithTag (NonPagedPool,
                                              NumberOfBytes,
                                              'wRmM');
    
            if (PoolArea == NULL) {
                return STATUS_INSUFFICIENT_RESOURCES;
            }
        }
    
        //
        // Capture the specified page frame numbers.
        //

        try {
            ProbeForRead (UserPfnArray,
                          NumberOfBytes,
                          sizeof(ULONG_PTR));

            RtlCopyMemory (PoolArea, UserPfnArray, NumberOfBytes);

        } except(EXCEPTION_EXECUTE_HANDLER) {
            if (PoolArea != (PVOID)&StackArray[0]) {
                ExFreePool (PoolArea);
            }
            return GetExceptionCode();
        }

        FrameList = (PPFN_NUMBER)PoolArea;
    }

    PoolAreaEnd = (PVOID)((PULONG_PTR)PoolArea + NumberOfPages);

    PointerPte = MiGetPteAddress (VirtualAddress);
    LastPte = PointerPte + NumberOfPages;

    Process = PsGetCurrentProcess ();

    PageFrameIndex = 0;

    //
    // Initialize as much as possible before acquiring any locks.
    //

    MI_MAKE_VALID_PTE (NewPteContents,
                       PageFrameIndex,
                       MM_READWRITE,
                       PointerPte);

    MI_SET_PTE_DIRTY (NewPteContents);

    PteFlushList.Count = 0;

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        if (PoolArea != (PVOID)&StackArray[0]) {
            ExFreePool (PoolArea);
        }
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Block APCs to prevent recursive pushlock scenarios as this is not
    // supported.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);

    //
    // Pushlock protection protects insertion/removal of Vads into each process'
    // AweVadList.  It also protects creation/deletion and adds/removes
    // of the VadPhysicalPagesBitMap.  Finally, it protects the PFN
    // modifications for pages in the bitmap.
    //

    PushLock = ExAcquireCacheAwarePushLockShared (AweInfo->PushLock);

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    //
    // Note that the push lock is sufficient to traverse this list.
    //

    NextEntry = AweInfo->AweVadList.Flink;

    //
    // Note the compiler generates much better code with the syntax below
    // than with "while (NextEntry != &AweInfo->AweVadList) {"
    //

    do {
            
        if (NextEntry == &AweInfo->AweVadList) {

            //
            // No virtual address is reserved at the specified base address,
            // return an error.
            //

            Status = STATUS_INVALID_PARAMETER_1;
            goto ErrorReturn;
        }

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        ASSERT (PhysicalView->u.LongFlags == MI_PHYSICAL_VIEW_AWE);
        ASSERT (PhysicalView->Vad->u.VadFlags.UserPhysicalPages == 1);

        if ((VirtualAddress >= (PVOID)PhysicalView->StartVa) &&
            (EndAddress <= (PVOID)PhysicalView->EndVa)) {

            break;
        }

        NextEntry = NextEntry->Flink;

    } while (TRUE);

    //
    // Ensure the PFN element corresponding to each specified page is owned
    // by the specified VAD.
    //
    // Since this ownership can only be changed while holding this process'
    // working set lock, the PFN can be scanned here without holding the PFN
    // lock.
    //
    // Note the PFN lock is not needed because any race with MmProbeAndLockPages
    // can only result in the I/O going to the old page or the new page.
    // If the user breaks the rules, the PFN database (and any pages being
    // windowed here) are still protected because of the reference counts
    // on the pages with inprogress I/O.  This is possible because NO pages
    // are actually freed here - they are just windowed.
    //

    if (ARGUMENT_PRESENT(UserPfnArray)) {

        //
        // By keeping the PFN bitmap in the VAD (instead of in the PFN
        // database itself), a few benefits are realized:
        //
        // 1. No need to acquire the PFN lock here.
        // 2. Faster handling of PFN databases with holes.
        // 3. Transparent support for dynamic PFN database growth.
        // 4. Less nonpaged memory is used (for the bitmap vs adding a
        //    field to the PFN) on systems with no unused pack space in
        //    the PFN database, presuming not many of these VADs get
        //    allocated.
        //

        //
        // The first pass here ensures all the frames are secure.
        //

        //
        // N.B.  This implies that PFN_NUMBER is always ULONG_PTR in width
        //       as PFN_NUMBER is not exposed to application code today.
        //

        SizeOfBitMap = BitMap->SizeOfBitMap;

        BitBuffer = BitMap->Buffer;

        do {
            
            PageFrameIndex = *FrameList;

            //
            // Frames past the end of the bitmap are not allowed.
            //

            BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(PageFrameIndex);

#if defined (_WIN64)
            //
            // Ensure the frame is a 32-bit number.
            //

            if (BitMapIndex != PageFrameIndex) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }
#endif
            
            if (BitMapIndex >= SizeOfBitMap) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // Frames not in the bitmap are not allowed.
            //

            if (MI_CHECK_BIT (BitBuffer, BitMapIndex) == 0) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // The frame must not be already mapped anywhere.
            // Or be passed in twice in different spots in the array.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            ASSERT (MI_PFN_IS_AWE (Pfn1));

            OldValue = Pfn1->u2.ShareCount;

            if (OldValue != 1) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            NewValue = OldValue + 2;

            //
            // Mark the frame as "about to be mapped".
            //

#if defined (_WIN64)
            OldValue = InterlockedCompareExchange64 ((PLONGLONG)&Pfn1->u2.ShareCount,
                                                     (LONGLONG)NewValue,
                                                     (LONGLONG)OldValue);
#else
            OldValue = InterlockedCompareExchange ((PLONG)&Pfn1->u2.ShareCount,
                                                   NewValue,
                                                   OldValue);
#endif
                                                             
            if (OldValue != 1) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            ASSERT (MI_PFN_IS_AWE (Pfn1));

            ASSERT (Pfn1->u2.ShareCount == 3);

            ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                    (MiUsingLowPagesForAwe == TRUE));

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        //
        // This pass actually inserts them all into the page table pages and
        // the TBs now that we know the frames are good.  Check the PTEs and
        // PFNs carefully as a malicious user may issue more than one remap
        // request for all or portions of the same region simultaneously.
        //

        FrameList = (PPFN_NUMBER)PoolArea;

        do {
            
            PageFrameIndex = *FrameList;
            NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;

            do {

                OldPteContents = *PointerPte;

                OriginalPteContents.u.Long = InterlockedCompareExchangePte (
                                                    PointerPte,
                                                    NewPteContents.u.Long,
                                                    OldPteContents.u.Long);

            } while (OriginalPteContents.u.Long != OldPteContents.u.Long);

            //
            // The PTE is now pointing at the new frame.  Note that another
            // thread can immediately access the page contents via this PTE
            // even though they're not supposed to until this API returns.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't really still or yet own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);
                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 2);

                //
                // Carefully clear the PteAddress before decrementing the share
                // count.
                //

                Pfn1->PteAddress = NULL;

                InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
                    PteFlushList.Count += 1;
                }
            }

            //
            // Update counters for the new frame we just put in the PTE and
            // TB.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->PteAddress == NULL);
            ASSERT (Pfn1->u2.ShareCount == 3);
            Pfn1->PteAddress = PointerPte;
            InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);

            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            PointerPte += 1;
            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);
    }
    else {

        //
        // Set the specified virtual address range to no access.
        //

        while (PointerPte < LastPte) {

            do {

                OldPteContents = *PointerPte;

                OriginalPteContents.u.Long = InterlockedCompareExchangePte (
                                                PointerPte,
                                                ZeroPte.u.Long,
                                                OldPteContents.u.Long);

            } while (OriginalPteContents.u.Long != OldPteContents.u.Long);

            //
            // The PTE has been cleared.  Note that another thread can still
            // be accessing the page contents via the stale PTE until the TB
            // entry is flushed even though they're not supposed to.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't still own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);
                ASSERT (MI_PFN_IS_AWE (Pfn1));
                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 2);
                Pfn1->PteAddress = NULL;
                InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
                    PteFlushList.Count += 1;
                }
            }

            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
            PointerPte += 1;
        }
    }

    ExReleaseCacheAwarePushLockShared (PushLock);

    KeLowerIrql (OldIrql);

    //
    // Flush the TB entries for any relevant pages.  Note this can be done
    // without holding the AWE push lock because the PTEs have already been
    // filled so any concurrent (bogus) map/unmap call will see the right
    // entries.  AND any free of the physical pages will also see the right
    // entries (although the free must do a TB flush while holding the AWE
    // push lock exclusive to ensure no thread gets to continue using a
    // stale mapping to the page being freed prior to the flush below).
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, FALSE, ZeroPte);
    }

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return STATUS_SUCCESS;

ErrorReturn0:

    while (FrameList > (PPFN_NUMBER)PoolArea) {
        FrameList -= 1;
        PageFrameIndex = *FrameList;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u2.ShareCount == 3);
        Pfn1->u2.ShareCount = 1;
    }

ErrorReturn:

    ExReleaseCacheAwarePushLockShared (PushLock);

    KeLowerIrql (OldIrql);

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return Status;
}


NTSTATUS
NtMapUserPhysicalPagesScatter (
    IN PVOID *VirtualAddresses,
    IN ULONG_PTR NumberOfPages,
    IN PULONG_PTR UserPfnArray OPTIONAL
    )

/*++

Routine Description:

    This function maps the specified nonpaged physical pages into the specified
    user address range.

    Note no WSLEs are maintained for this range as it is all nonpaged.

Arguments:

    VirtualAddresses - Supplies a pointer to an array of user virtual addresses
                       within UserPhysicalPages Vads.  Each array entry is
                       presumed to map a single page.
        
    NumberOfPages - Supplies the number of pages to map.
        
    UserPfnArray - Supplies a pointer to the page frame numbers to map in.
                   If this is zero, then the virtual addresses are set to
                   NO_ACCESS.  If the array entry is zero then just the
                   corresponding virtual address is set to NO_ACCESS.

Return Value:

    Various NTSTATUS codes.

--*/

{
    KIRQL OldIrql;
    ULONG_PTR OldValue;
    ULONG_PTR NewValue;
    PULONG BitBuffer;
    PAWEINFO AweInfo;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    NTSTATUS Status;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID PoolArea;
    PVOID PoolAreaEnd;
    PVOID *PoolVirtualArea;
    PVOID *PoolVirtualAreaBase;
    PVOID *PoolVirtualAreaEnd;
    PPFN_NUMBER FrameList;
    ULONG BitMapIndex;
    PVOID StackVirtualArray[SMALL_COPY_STACK_SIZE];
    ULONG_PTR StackArray[SMALL_COPY_STACK_SIZE];
    MMPTE OriginalPteContents;
    MMPTE OldPteContents;
    MMPTE NewPteContents0;
    MMPTE NewPteContents;
    MMPTE JunkPte;
    ULONG_PTR NumberOfBytes;
    PRTL_BITMAP BitMap;
    PLIST_ENTRY NextEntry;
    PLIST_ENTRY FirstEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    PVOID VirtualAddress;
    ULONG SizeOfBitMap;
    PEX_PUSH_LOCK PushLock;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (NumberOfPages > (MAXULONG_PTR / PAGE_SIZE)) {
        return STATUS_INVALID_PARAMETER_2;
    }

    //
    // Carefully probe and capture the user virtual address array.
    //

    PoolArea = (PVOID)&StackArray[0];
    PoolVirtualAreaBase = (PVOID)&StackVirtualArray[0];

    NumberOfBytes = NumberOfPages * sizeof(PVOID);

    if (NumberOfPages > SMALL_COPY_STACK_SIZE) {
        PoolVirtualAreaBase = ExAllocatePoolWithTag (NonPagedPool,
                                                 NumberOfBytes,
                                                 'wRmM');

        if (PoolVirtualAreaBase == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    PoolVirtualArea = PoolVirtualAreaBase;

    try {
        ProbeForRead (VirtualAddresses,
                      NumberOfBytes,
                      sizeof(PVOID));

        RtlCopyMemory (PoolVirtualArea, VirtualAddresses, NumberOfBytes);

    } except(EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
        goto ErrorReturn;
    }

    //
    // Check for zero pages here so the loops further down can be optimized
    // taking into account this can never happen.
    //

    if (NumberOfPages == 0) {
        return STATUS_SUCCESS;
    }

    //
    // Carefully probe and capture the user PFN array.
    //

    if (ARGUMENT_PRESENT(UserPfnArray)) {

        NumberOfBytes = NumberOfPages * sizeof(ULONG_PTR);

        if (NumberOfPages > SMALL_COPY_STACK_SIZE) {
            PoolArea = ExAllocatePoolWithTag (NonPagedPool,
                                              NumberOfBytes,
                                              'wRmM');
    
            if (PoolArea == NULL) {
                PoolArea = (PVOID)&StackArray[0];
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn;
            }
        }
    
        //
        // Capture the specified page frame numbers.
        //

        try {
            ProbeForRead (UserPfnArray,
                          NumberOfBytes,
                          sizeof(ULONG_PTR));

            RtlCopyMemory (PoolArea, UserPfnArray, NumberOfBytes);

        } except(EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode();
            goto ErrorReturn;
        }
    }

    PoolAreaEnd = (PVOID)((PULONG_PTR)PoolArea + NumberOfPages);
    Process = PsGetCurrentProcess();

    //
    // Initialize as much as possible before acquiring any locks.
    //

    PageFrameIndex = 0;

    PhysicalView = NULL;

    PteFlushList.Count = 0;

    FrameList = (PPFN_NUMBER)PoolArea;

    ASSERT (NumberOfPages != 0);

    PoolVirtualAreaEnd = PoolVirtualAreaBase + NumberOfPages;

    MI_MAKE_VALID_PTE (NewPteContents0,
                       PageFrameIndex,
                       MM_READWRITE,
                       MiGetPteAddress(PoolVirtualArea[0]));

    MI_SET_PTE_DIRTY (NewPteContents0);

    Status = STATUS_SUCCESS;

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    //
    // Block APCs to prevent recursive pushlock scenarios as this is not
    // supported.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);

    //
    // Pushlock protection protects insertion/removal of Vads into each process'
    // AweVadList.  It also protects creation/deletion and adds/removes
    // of the VadPhysicalPagesBitMap.  Finally, it protects the PFN
    // modifications for pages in the bitmap.
    //

    PushLock = ExAcquireCacheAwarePushLockShared (AweInfo->PushLock);

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    //
    // Note that the PFN lock is not needed to traverse this list (even though
    // MmProbeAndLockPages uses it), because the pushlock has been acquired.
    //
    // The AweVadList should typically have just one entry - the view
    // we're looking for, so this traverse should be quick.
    //

    //
    // Snap the first entry now so compares in the loop save an indirect
    // reference as we know it can't change.  Check it for being empty now
    // so that also doesn't need to be checked in the loop.
    //

    FirstEntry = AweInfo->AweVadList.Flink;

    if (FirstEntry == &AweInfo->AweVadList) {

        //
        // No AWE Vads exist - return an error.
        //

        ExReleaseCacheAwarePushLockShared (PushLock);
        KeLowerIrql (OldIrql);
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    PhysicalView = CONTAINING_RECORD (FirstEntry, MI_PHYSICAL_VIEW, ListEntry);

    do {

        VirtualAddress = *PoolVirtualArea;

        //
        // Check the last physical view interrogated (hint) first.
        //

        ASSERT (PhysicalView->u.LongFlags == MI_PHYSICAL_VIEW_AWE);
        ASSERT (PhysicalView->Vad->u.VadFlags.UserPhysicalPages == 1);

        if ((VirtualAddress >= (PVOID)PhysicalView->StartVa) &&
            (VirtualAddress <= (PVOID)PhysicalView->EndVa)) {

            //
            // The virtual address is within the hint so it's good.
            //

            PoolVirtualArea += 1;
            continue;
        }

        NextEntry = FirstEntry;

        //
        // Note the compiler generates much better code with the syntax below
        // than with "while (NextEntry != &AweInfo->AweVadList) {"
        //

        do {
            
            if (NextEntry == &AweInfo->AweVadList) {

                //
                // No virtual address is reserved at the specified base address,
                // return an error.
                //

                ExReleaseCacheAwarePushLockShared (PushLock);
                KeLowerIrql (OldIrql);
                Status = STATUS_INVALID_PARAMETER_1;
                goto ErrorReturn;
            }

            PhysicalView = CONTAINING_RECORD (NextEntry,
                                              MI_PHYSICAL_VIEW,
                                              ListEntry);

            ASSERT (PhysicalView->Vad->u.VadFlags.UserPhysicalPages == 1);
            ASSERT (PhysicalView->u.LongFlags == MI_PHYSICAL_VIEW_AWE);

            if ((VirtualAddress >= (PVOID)PhysicalView->StartVa) &&
                (VirtualAddress <= (PVOID)PhysicalView->EndVa)) {

                break;
            }

            NextEntry = NextEntry->Flink;

        } while (TRUE);

        PoolVirtualArea += 1;

    } while (PoolVirtualArea < PoolVirtualAreaEnd);

    //
    // Ensure the PFN element corresponding to each specified page is owned
    // by the specified VAD.
    //
    // Since this ownership can only be changed while holding this process'
    // working set lock, the PFN can be scanned here without holding the PFN
    // lock.
    //
    // Note the PFN lock is not needed because any race with MmProbeAndLockPages
    // can only result in the I/O going to the old page or the new page.
    // If the user breaks the rules, the PFN database (and any pages being
    // windowed here) are still protected because of the reference counts
    // on the pages with inprogress I/O.  This is possible because NO pages
    // are actually freed here - they are just windowed.
    //

    PoolVirtualArea = PoolVirtualAreaBase;

    if (ARGUMENT_PRESENT(UserPfnArray)) {

        //
        // By keeping the PFN bitmap in the process (instead of in the PFN
        // database itself), a few benefits are realized:
        //
        // 1. No need to acquire the PFN lock here.
        // 2. Faster handling of PFN databases with holes.
        // 3. Transparent support for dynamic PFN database growth.
        // 4. Less nonpaged memory is used (for the bitmap vs adding a
        //    field to the PFN) on systems with no unused pack space in
        //    the PFN database.
        //

        //
        // The first pass here ensures all the frames are secure.
        //

        //
        // N.B.  This implies that PFN_NUMBER is always ULONG_PTR in width
        //       as PFN_NUMBER is not exposed to application code today.
        //

        SizeOfBitMap = BitMap->SizeOfBitMap;
        BitBuffer = BitMap->Buffer;

        do {

            PageFrameIndex = *FrameList;

            //
            // Zero entries are treated as a command to unmap.
            //

            if (PageFrameIndex == 0) {
                FrameList += 1;
                continue;
            }

            //
            // Frames past the end of the bitmap are not allowed.
            //

            BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(PageFrameIndex);

#if defined (_WIN64)
            //
            // Ensure the frame is a 32-bit number.
            //

            if (BitMapIndex != PageFrameIndex) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }
#endif
            
            if (BitMapIndex >= SizeOfBitMap) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // Frames not in the bitmap are not allowed.
            //

            if (MI_CHECK_BIT (BitBuffer, BitMapIndex) == 0) {
                Status = STATUS_CONFLICTING_ADDRESSES;
                goto ErrorReturn0;
            }

            //
            // The frame must not be already mapped anywhere.
            // Or be passed in twice in different spots in the array.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (MI_PFN_IS_AWE (Pfn1));

            OldValue = Pfn1->u2.ShareCount;

            if (OldValue != 1) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            NewValue = OldValue + 2;

            //
            // Mark the frame as "about to be mapped".
            //

#if defined (_WIN64)
            OldValue = InterlockedCompareExchange64 ((PLONGLONG)&Pfn1->u2.ShareCount,
                                                     (LONGLONG)NewValue,
                                                     (LONGLONG)OldValue);
#else
            OldValue = InterlockedCompareExchange ((PLONG)&Pfn1->u2.ShareCount,
                                                   NewValue,
                                                   OldValue);
#endif
                                                             
            if (OldValue != 1) {
                Status = STATUS_INVALID_PARAMETER_3;
                goto ErrorReturn0;
            }

            ASSERT (MI_PFN_IS_AWE (Pfn1));

            ASSERT (Pfn1->u2.ShareCount == 3);

            ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                    (MiUsingLowPagesForAwe == TRUE));

            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);

        //
        // This pass actually inserts them all into the page table pages and
        // the TBs now that we know the frames are good.  Check the PTEs and
        // PFNs carefully as a malicious user may issue more than one remap
        // request for all or portions of the same region simultaneously.
        //

        FrameList = (PPFN_NUMBER)PoolArea;

        do {

            PageFrameIndex = *FrameList;

            if (PageFrameIndex != 0) {
                NewPteContents = NewPteContents0;
                NewPteContents.u.Hard.PageFrameNumber = PageFrameIndex;
            }
            else {
                NewPteContents.u.Long = ZeroPte.u.Long;
            }

            VirtualAddress = *PoolVirtualArea;
            PoolVirtualArea += 1;

            PointerPte = MiGetPteAddress (VirtualAddress);

            do {

                OldPteContents = *PointerPte;

                OriginalPteContents.u.Long = InterlockedCompareExchangePte (
                                                    PointerPte,
                                                    NewPteContents.u.Long,
                                                    OldPteContents.u.Long);

            } while (OriginalPteContents.u.Long != OldPteContents.u.Long);

            //
            // The PTE is now pointing at the new frame.  Note that another
            // thread can immediately access the page contents via this PTE
            // even though they're not supposed to until this API returns.
            // Thus, the page frames are handled carefully so that malicious
            // apps cannot corrupt frames they don't really still or yet own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                // Note the app could maliciously dirty data in the old frame
                // until the TB flush completes, so don't allow frame reuse
                // till then (although allowing remapping within this process
                // is ok).
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);
                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 2);
                ASSERT (MI_PFN_IS_AWE (Pfn1));

                Pfn1->PteAddress = NULL;
                InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
                    PteFlushList.Count += 1;
                }
            }

            if (PageFrameIndex != 0) {
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->PteAddress == NULL);
                ASSERT (Pfn1->u2.ShareCount == 3);
                Pfn1->PteAddress = PointerPte;
                InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);
            }
    
            FrameList += 1;

        } while (FrameList < (PPFN_NUMBER) PoolAreaEnd);
    }
    else {

        //
        // Set the specified virtual address range to no access.
        //

        do {

            VirtualAddress = *PoolVirtualArea;
            PointerPte = MiGetPteAddress (VirtualAddress);
    
            do {

                OldPteContents = *PointerPte;

                OriginalPteContents.u.Long = InterlockedCompareExchangePte (
                                                    PointerPte,
                                                    ZeroPte.u.Long,
                                                    OldPteContents.u.Long);

            } while (OriginalPteContents.u.Long != OldPteContents.u.Long);

            //
            // The PTE is now zeroed.  Note that another thread can still
            // Note the app could maliciously dirty data in the old frame
            // until the TB flush completes, so don't allow frame reuse
            // till then (although allowing remapping within this process
            // is ok) to prevent the app from corrupting frames it doesn't
            // really still own.
            //
        
            if (OldPteContents.u.Hard.Valid == 1) {

                //
                // The old frame was mapped so the TB entry must be flushed.
                //

                Pfn1 = MI_PFN_ELEMENT (OldPteContents.u.Hard.PageFrameNumber);
                ASSERT (Pfn1->PteAddress != NULL);
                ASSERT (Pfn1->u2.ShareCount == 2);
                ASSERT (MI_PFN_IS_AWE (Pfn1));

                Pfn1->PteAddress = NULL;
                InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -1);

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                    PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
                    PteFlushList.Count += 1;
                }
            }

            PoolVirtualArea += 1;

        } while (PoolVirtualArea < PoolVirtualAreaEnd);
    }

    ExReleaseCacheAwarePushLockShared (PushLock);
    KeLowerIrql (OldIrql);

    //
    // Flush the TB entries for any relevant pages.  Note this can be done
    // without holding the AWE push lock because the PTEs have already been
    // filled so any concurrent (bogus) map/unmap call will see the right
    // entries.  AND any free of the physical pages will also see the right
    // entries (although the free must do a TB flush while holding the AWE
    // push lock exclusive to ensure no thread gets to continue using a
    // stale mapping to the page being freed prior to the flush below).
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, FALSE, ZeroPte);
    }

ErrorReturn:

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    if (PoolVirtualAreaBase != (PVOID)&StackVirtualArray[0]) {
        ExFreePool (PoolVirtualAreaBase);
    }

    return Status;

ErrorReturn0:

    while (FrameList > (PPFN_NUMBER)PoolArea) {
        FrameList -= 1;
        PageFrameIndex = *FrameList;
        if (PageFrameIndex != 0) {
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u2.ShareCount == 3);
            ASSERT (MI_PFN_IS_AWE (Pfn1));
            InterlockedExchangeAddSizeT (&Pfn1->u2.ShareCount, -2);
        }
    }

    ExReleaseCacheAwarePushLockShared (PushLock);
    KeLowerIrql (OldIrql);

    goto ErrorReturn;
}

PVOID
MiAllocateAweInfo (
    VOID
    )

/*++

Routine Description:

    This function allocates an AWE structure for the current process.  Note
    this structure is never destroyed while the process is alive in order to
    allow various checks to occur lock free.

Arguments:

    None.

Return Value:

    A non-NULL AweInfo pointer on success, NULL on failure.

Environment:

    Kernel mode, PASSIVE_LEVEL, no locks held.

--*/

{
    PAWEINFO AweInfo;
    PEPROCESS Process;

    AweInfo = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof (AWEINFO),
                                     'wAmM');

    if (AweInfo != NULL) {

        AweInfo->VadPhysicalPagesBitMap = NULL;
        AweInfo->VadPhysicalPages = 0;

        InitializeListHead (&AweInfo->AweVadList);

        AweInfo->PushLock = ExAllocateCacheAwarePushLock ();
        if (AweInfo->PushLock == NULL) {
            ExFreePool (AweInfo);
            return NULL;
        }

        Process = PsGetCurrentProcess();

        //
        // A memory barrier is needed to ensure the writes initializing the
        // AweInfo fields are visible prior to setting the EPROCESS AweInfo
        // pointer.  This is because the reads from these fields are done
        // lock free for improved performance.
        //

        KeMemoryBarrier ();

        if (InterlockedCompareExchangePointer (&Process->AweInfo,
                                               AweInfo,
                                               NULL) != NULL) {
            
            ExFreeCacheAwarePushLock (AweInfo->PushLock);

            ExFreePool (AweInfo);
            AweInfo = Process->AweInfo;
            ASSERT (AweInfo != NULL);
        }
    }

    return (PVOID) AweInfo;
}


NTSTATUS
NtAllocateUserPhysicalPages (
    IN HANDLE ProcessHandle,
    IN OUT PULONG_PTR NumberOfPages,
    OUT PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function allocates nonpaged physical pages for the specified
    subject process.

    No WSLEs are maintained for this range.

    The caller must check the NumberOfPages returned to determine how many
    pages were actually allocated (this number may be less than the requested
    amount).

    On success, the user array is filled with the allocated physical page
    frame numbers (only up to the returned NumberOfPages is filled in).

    No PTEs are filled here - this gives the application the flexibility
    to order the address space with no metadata structure imposed by the Mm.
    Applications do this via NtMapUserPhysicalPages - ie:

        - Each physical page allocated is set in the process's bitmap.
          This provides remap, free and unmap a way to validate and rundown
          these frames.

          Unmaps may result in a walk of the entire bitmap, but that's ok as
          unmaps should be less frequent.  The win is it saves us from
          using up system virtual address space to manage these frames.

        - Note that the same physical frame may NOT be mapped at two different
          virtual addresses in the process.  This makes frees and unmaps
          substantially faster as no checks for aliasing need be performed.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    NumberOfPages - Supplies a pointer to a variable that supplies the
                    desired size in pages of the allocation.  This is filled
                    with the actual number of pages allocated.
        
    UserPfnArray - Supplies a pointer to user memory to store the allocated
                   frame numbers into.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PAWEINFO AweInfo;
    ULONG i;
    KAPC_STATE ApcState;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    LOGICAL Attached;
    LOGICAL WsHeld;
    ULONG_PTR CapturedNumberOfPages;
    ULONG_PTR AllocatedPages;
    ULONG_PTR MdlRequestInPages;
    ULONG_PTR TotalAllocatedPages;
    PMDL MemoryDescriptorList;
    PMDL MemoryDescriptorList2;
    PMDL MemoryDescriptorHead;
    PPFN_NUMBER MdlPage;
    PRTL_BITMAP BitMap;
    ULONG BitMapSize;
    ULONG BitMapIndex;
    PMMPFN Pfn1;
    PHYSICAL_ADDRESS LowAddress;
    PHYSICAL_ADDRESS HighAddress;
    PHYSICAL_ADDRESS SkipBytes;
    ULONG SizeOfBitMap;
    PFN_NUMBER HighestPossiblePhysicalPage;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    Attached = FALSE;
    WsHeld = FALSE;

    //
    // Check the allocation type field.
    //

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        //
        // Capture the number of pages.
        //

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (NumberOfPages);

            CapturedNumberOfPages = *NumberOfPages;

            if (CapturedNumberOfPages == 0) {
                return STATUS_SUCCESS;
            }

            if (CapturedNumberOfPages > (MAXULONG_PTR / sizeof(ULONG_PTR))) {
                return STATUS_INVALID_PARAMETER_2;
            }

            ProbeForWrite (UserPfnArray,
                           CapturedNumberOfPages * sizeof (ULONG_PTR),
                           sizeof(PULONG_PTR));

        }
        else {
            CapturedNumberOfPages = *NumberOfPages;
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    if (!SeSinglePrivilegeCheck (SeLockMemoryPrivilege, PreviousMode)) {
        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        return STATUS_PRIVILEGE_NOT_HELD;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    BitMapSize = 0;

    //
    // Get the working set mutex to synchronize.  This also blocks APCs so
    // an APC which takes a page fault does not corrupt various structures.
    //

    WsHeld = TRUE;

    LOCK_WS (Process);

    //
    // Make sure the address space was not deleted, If so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    AweInfo = Process->AweInfo;

    if (AweInfo == NULL) {

        AweInfo = (PAWEINFO) MiAllocateAweInfo ();

        if (AweInfo == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn;
        }
        ASSERT (AweInfo == Process->AweInfo);
    }

    //
    // Create the physical pages bitmap if it does not already exist.
    // LockMemory privilege is required.
    //

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    if (BitMap == NULL) {

        HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;

#if defined (_WIN64)
        //
        // Force a 32-bit maximum on any page allocation because the bitmap
        // package is currently 32-bit.
        //

        if (HighestPossiblePhysicalPage + 1 >= _4gb) {
            HighestPossiblePhysicalPage = _4gb - 2;
        }
#endif

        BitMapSize = sizeof(RTL_BITMAP) + (ULONG)((((HighestPossiblePhysicalPage + 1) + 31) / 32) * 4);

        BitMap = ExAllocatePoolWithTag (NonPagedPool, BitMapSize, 'LdaV');

        if (BitMap == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn;
        }

        RtlInitializeBitMap (BitMap,
                             (PULONG)(BitMap + 1),
                             (ULONG)(HighestPossiblePhysicalPage + 1));

        RtlClearAllBits (BitMap);

        //
        // Charge quota for the nonpaged pool for the bitmap.  This is
        // done here rather than by using ExAllocatePoolWithQuota
        // so the process object is not referenced by the quota charge.
        //

        Status = PsChargeProcessNonPagedPoolQuota (Process, BitMapSize);

        if (!NT_SUCCESS(Status)) {
            ExFreePool (BitMap);
            goto ErrorReturn;
        }

        SizeOfBitMap = BitMap->SizeOfBitMap;
    }
    else {

        //
        // It's ok to snap this without a lock.
        //

        SizeOfBitMap = AweInfo->VadPhysicalPagesBitMap->SizeOfBitMap;
    }

    AllocatedPages = 0;
    TotalAllocatedPages = 0;
    MemoryDescriptorHead = NULL;

    SkipBytes.QuadPart = 0;

    //
    // Don't use the low 16mb of memory so that at least some low pages are left
    // for 32/24-bit device drivers.  Just under 4gb is the maximum allocation
    // per MDL so the ByteCount field does not overflow.
    //

    HighAddress.QuadPart = ((ULONGLONG)(SizeOfBitMap - 1)) << PAGE_SHIFT;

    LowAddress.QuadPart = LOWEST_USABLE_PHYSICAL_ADDRESS;

    if (LowAddress.QuadPart >= HighAddress.QuadPart) {

        //
        // If there's less than 16mb of RAM, just take pages from anywhere.
        //

#if DBG
        MiUsingLowPagesForAwe = TRUE;
#endif
        LowAddress.QuadPart = 0;
    }

    do {

        MdlRequestInPages = CapturedNumberOfPages - TotalAllocatedPages;

        if (MdlRequestInPages > (ULONG_PTR)((MAXULONG - PAGE_SIZE) >> PAGE_SHIFT)) {
            MdlRequestInPages = (ULONG_PTR)((MAXULONG - PAGE_SIZE) >> PAGE_SHIFT);
        }

        //
        // Note this allocation returns zeroed pages.
        //

        MemoryDescriptorList = MmAllocatePagesForMdl (LowAddress,
                                                      HighAddress,
                                                      SkipBytes,
                                                      MdlRequestInPages << PAGE_SHIFT);

        if (MemoryDescriptorList == NULL) {

            //
            // No (more) pages available.  If this becomes a common situation,
            // all the working sets could be flushed here.
            //

            if (TotalAllocatedPages == 0) {
                if (BitMapSize) {
                    ExFreePool (BitMap);
                    PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
                }
                Status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn;
            }

            //
            // Make do with what we've gotten so far.
            //

            break;
        }

        MemoryDescriptorList->Next = MemoryDescriptorHead;
        MemoryDescriptorHead = MemoryDescriptorList;

        MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        AllocatedPages = MemoryDescriptorList->ByteCount >> PAGE_SHIFT;
        TotalAllocatedPages += AllocatedPages;

        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, AllocatedPages);

        //
        // The per-process WS lock guards updates to Process->VadPhysicalPages.
        //

        AweInfo->VadPhysicalPages += AllocatedPages;

        //
        // Update the allocation bitmap for each allocated frame.
        // Note the PFN lock is not needed to modify the PteAddress below.
        // In fact, even the AWE push lock is not needed as these pages
        // are brand new.
        //

        for (i = 0; i < AllocatedPages; i += 1) {

            ASSERT ((*MdlPage >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                    (MiUsingLowPagesForAwe == TRUE));

            BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(*MdlPage);

            ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
            ASSERT (MI_CHECK_BIT (BitMap->Buffer, BitMapIndex) == 0);

            ASSERT64 (*MdlPage < _4gb);

            Pfn1 = MI_PFN_ELEMENT (*MdlPage);
            ASSERT (MI_PFN_IS_AWE (Pfn1));
            Pfn1->PteAddress = NULL;
            ASSERT (Pfn1->u2.ShareCount == 1);

            //
            // Once this bit is set (and the mutex released below), a rogue
            // thread that is passing random frame numbers to
            // NtFreeUserPhysicalPages can free this frame.  This means NO
            // references can be made to it by this routine after this point
            // without first re-checking the bitmap.
            //

            MI_SET_BIT (BitMap->Buffer, BitMapIndex);

            MdlPage += 1;
        }

        ASSERT (TotalAllocatedPages <= CapturedNumberOfPages);

        if (TotalAllocatedPages == CapturedNumberOfPages) {
            break;
        }

        //
        // Try the same memory range again - there might be more pages
        // left in it that can be claimed as a truncated MDL had to be
        // used for the last request.
        //

    } while (TRUE);

    ASSERT (TotalAllocatedPages != 0);

    if (BitMapSize != 0) {

        //
        // If this API resulted in the creation of the bitmap, then set it
        // in the process structure now.  No need for locking around this.
        //

        AweInfo->VadPhysicalPagesBitMap = BitMap;
    }

    UNLOCK_WS (Process);
    WsHeld = FALSE;

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    //
    // Establish an exception handler and carefully write out the
    // number of pages and the frame numbers.
    //

    Status = STATUS_SUCCESS;

    try {

        ASSERT (TotalAllocatedPages <= CapturedNumberOfPages);

        *NumberOfPages = TotalAllocatedPages;

        MemoryDescriptorList = MemoryDescriptorHead;

        while (MemoryDescriptorList != NULL) {

            MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            AllocatedPages = MemoryDescriptorList->ByteCount >> PAGE_SHIFT;

            for (i = 0; i < AllocatedPages; i += 1) {
                *UserPfnArray = *(PULONG_PTR)MdlPage;
#if 0
                //
                // The bitmap entry for this page was set above, so a rogue
                // thread that is passing random frame numbers to
                // NtFreeUserPhysicalPages may have already freed this frame.
                // This means the ASSERT below cannot be made without first
                // re-checking the bitmap to see if the page is still in it.
                // It's not worth reacquiring the mutex just for this, so turn
                // the assert off for now.
                //

                ASSERT (MI_PFN_ELEMENT(*MdlPage)->u2.ShareCount == 1);
#endif
                UserPfnArray += 1;
                MdlPage += 1;
            }
            MemoryDescriptorList = MemoryDescriptorList->Next;
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If anything went wrong communicating the pages back to the user
        // then the user has really hurt himself because these addresses
        // passed the probe tests at the beginning of the service.  Rather
        // than carrying around extensive recovery code, just return back
        // success as this scenario is the same as if the user scribbled
        // over the output parameters after the service returned anyway.
        // You can't stop someone who's determined to lose their values !
        //
        // Fall through...
        //
    }

    //
    // Free the space consumed by the MDLs now that the page frame numbers
    // have been saved in the bitmap and copied to the user.
    //

    MemoryDescriptorList = MemoryDescriptorHead;
    while (MemoryDescriptorList != NULL) {
        MemoryDescriptorList2 = MemoryDescriptorList->Next;
        ExFreePool (MemoryDescriptorList);
        MemoryDescriptorList = MemoryDescriptorList2;
    }

ErrorReturn:

    if (WsHeld == TRUE) {
        UNLOCK_WS (Process);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    return Status;
}


NTSTATUS
NtFreeUserPhysicalPages (
    IN HANDLE ProcessHandle,
    IN OUT PULONG_PTR NumberOfPages,
    IN PULONG_PTR UserPfnArray
    )

/*++

Routine Description:

    This function frees the nonpaged physical pages for the specified
    subject process.  Any PTEs referencing these pages are also invalidated.

    Note there is no need to walk the entire VAD tree to clear the PTEs that
    match each page as each physical page can only be mapped at a single
    virtual address (alias addresses within the VAD are not allowed).

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    NumberOfPages - Supplies the size in pages of the allocation to delete.
                    Returns the actual number of pages deleted.
        
    UserPfnArray - Supplies a pointer to memory to retrieve the page frame
                   numbers from.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PAWEINFO AweInfo;
    PULONG BitBuffer;
    KAPC_STATE ApcState;
    ULONG_PTR CapturedNumberOfPages;
    PMDL MemoryDescriptorList;
    PPFN_NUMBER MdlPage;
    PPFN_NUMBER LastMdlPage;
    PFN_NUMBER PagesInMdl;
    PFN_NUMBER PageFrameIndex;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    ULONG_PTR PagesProcessed;
    PFN_NUMBER MdlHack[(sizeof(MDL) / sizeof(PFN_NUMBER)) + COPY_STACK_SIZE];
    ULONG_PTR MdlPages;
    ULONG_PTR NumberOfBytes;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    LOGICAL Attached;
    PMMPFN Pfn1;
    LOGICAL WsHeld;
    LOGICAL OnePassComplete;
    LOGICAL ProcessReferenced;
    MMPTE_FLUSH_LIST PteFlushList;
    PMMPTE PointerPte;
    MMPTE OldPteContents;
    MMPTE JunkPte;
    PETHREAD CurrentThread;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // Establish an exception handler, probe the specified addresses
    // for read access and capture the page frame numbers.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread (&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        try {

            ProbeForWritePointer (NumberOfPages);

            CapturedNumberOfPages = *NumberOfPages;

            //
            // Initialize the NumberOfPages freed to zero so the user can be
            // reasonably informed about errors that occur midway through
            // the transaction.
            //

            *NumberOfPages = 0;

        } except (ExSystemExceptionFilter()) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //
    
            return GetExceptionCode();
        }
    }
    else {
        CapturedNumberOfPages = *NumberOfPages;
    }

    if (CapturedNumberOfPages == 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

    OnePassComplete = FALSE;
    PagesProcessed = 0;

    //
    // Initializing MdlPages is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    MdlPages = 0;

    MemoryDescriptorList = NULL;

    if (CapturedNumberOfPages > COPY_STACK_SIZE) {

        //
        // Ensure the number of pages can fit into an MDL's ByteCount.
        //

        if (CapturedNumberOfPages > ((ULONG)MAXULONG >> PAGE_SHIFT)) {
            MdlPages = (ULONG_PTR)((ULONG)MAXULONG >> PAGE_SHIFT);
        }
        else {
            MdlPages = CapturedNumberOfPages;
        }

        while (MdlPages > COPY_STACK_SIZE) {
            MemoryDescriptorList = MmCreateMdl (NULL,
                                                0,
                                                MdlPages << PAGE_SHIFT);
    
            if (MemoryDescriptorList != NULL) {
                break;
            }

            MdlPages >>= 1;
        }
    }

    if (MemoryDescriptorList == NULL) {
        MdlPages = COPY_STACK_SIZE;
        MemoryDescriptorList = (PMDL)&MdlHack[0];
    }

    WsHeld = FALSE;
    ProcessReferenced = FALSE;

    Process = PsGetCurrentProcessByThread (CurrentThread);

repeat:

    if (CapturedNumberOfPages < MdlPages) {
        MdlPages = CapturedNumberOfPages;
    }

    MmInitializeMdl (MemoryDescriptorList, 0, MdlPages << PAGE_SHIFT);

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    NumberOfBytes = MdlPages * sizeof(ULONG_PTR);

    Attached = FALSE;

    //
    // Establish an exception handler, probe the specified addresses
    // for read access and capture the page frame numbers.
    //

    if (PreviousMode != KernelMode) {

        try {

            //
            // Update the user's count so if anything goes wrong, the user can
            // be reasonably informed about how far into the transaction it
            // occurred.
            //

            *NumberOfPages = PagesProcessed;

            ProbeForRead (UserPfnArray,
                          NumberOfBytes,
                          sizeof(PULONG_PTR));

            RtlCopyMemory ((PVOID)MdlPage,
                           UserPfnArray,
                           NumberOfBytes);

        } except (ExSystemExceptionFilter()) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            Status = GetExceptionCode();
            goto ErrorReturn;
        }
    }
    else {
        RtlCopyMemory ((PVOID)MdlPage,
                       UserPfnArray,
                       NumberOfBytes);
    }

    if (OnePassComplete == FALSE) {

        //
        // Reference the specified process handle for VM_OPERATION access.
        //
    
        if (ProcessHandle == NtCurrentProcess()) {
            Process = PsGetCurrentProcessByThread(CurrentThread);
        }
        else {
            Status = ObReferenceObjectByHandle ( ProcessHandle,
                                                 PROCESS_VM_OPERATION,
                                                 PsProcessType,
                                                 PreviousMode,
                                                 (PVOID *)&Process,
                                                 NULL );
    
            if (!NT_SUCCESS(Status)) {
                goto ErrorReturn;
            }
            ProcessReferenced = TRUE;
        }
    }
    
    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (PsGetCurrentProcessByThread(CurrentThread) != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // A memory barrier is needed to read the EPROCESS AweInfo field
    // in order to ensure the writes to the AweInfo structure fields are
    // visible in correct order.  This avoids the need to acquire any
    // stronger synchronization (ie: spinlock/pushlock, etc) in the interest
    // of best performance.
    //

    KeMemoryBarrier ();

    AweInfo = (PAWEINFO) Process->AweInfo;

    //
    // The physical pages bitmap must exist.
    //

    if ((AweInfo == NULL) || (AweInfo->VadPhysicalPagesBitMap == NULL)) {
        Status = STATUS_INVALID_PARAMETER_1;
        goto ErrorReturn;
    }

    PteFlushList.Count = 0;
    Status = STATUS_SUCCESS;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    WsHeld = TRUE;
    LOCK_WS (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    BitMap = AweInfo->VadPhysicalPagesBitMap;

    ASSERT (BitMap != NULL);

    BitBuffer = BitMap->Buffer;

    LastMdlPage = MdlPage + MdlPages;

    //
    // Flush the entire TB for this process while holding its AWE push lock
    // exclusive so that if this free is occurring prior to any pending
    // flushes at the end of an in-progress map/unmap, the app is not left
    // with a stale TB entry that would allow him to corrupt pages that no
    // longer belong to him.
    //

    //
    // Block APCs to prevent recursive pushlock scenarios as this is not
    // supported.
    //

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    KeFlushEntireTb (TRUE, FALSE);

    while (MdlPage < LastMdlPage) {

        PageFrameIndex = *MdlPage;
        BitMapIndex = MI_FRAME_TO_BITMAP_INDEX(PageFrameIndex);

#if defined (_WIN64)
        //
        // Ensure the frame is a 32-bit number.
        //

        if (BitMapIndex != PageFrameIndex) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }
#endif
            
        //
        // Frames past the end of the bitmap are not allowed.
        //

        if (BitMapIndex >= BitMap->SizeOfBitMap) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }

        //
        // Frames not in the bitmap are not allowed.
        //

        if (MI_CHECK_BIT (BitBuffer, BitMapIndex) == 0) {
            Status = STATUS_CONFLICTING_ADDRESSES;
            break;
        }

        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));

        PagesProcessed += 1;

        ASSERT64 (PageFrameIndex < _4gb);

        MI_CLEAR_BIT (BitBuffer, BitMapIndex);

        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

        ASSERT (MI_PFN_IS_AWE (Pfn1));

#if DBG
        if (Pfn1->u2.ShareCount == 1) {
            ASSERT (Pfn1->PteAddress == NULL);
        }
        else if (Pfn1->u2.ShareCount == 2) {
            ASSERT (Pfn1->PteAddress != NULL);
        }
        else {
            ASSERT (FALSE);
        }
#endif

        //
        // If the frame is currently mapped in the Vad then the PTE must
        // be cleared and the TB entry flushed.
        //

        if (Pfn1->u2.ShareCount != 1) {

            //
            // Note the exclusive hold of the AWE push lock prevents
            // any other concurrent threads from mapping or unmapping
            // right now.  This also eliminates the need to update the PFN
            // sharecount with an interlocked sequence as well.
            //

            Pfn1->u2.ShareCount -= 1;

            PointerPte = Pfn1->PteAddress;
            Pfn1->PteAddress = NULL;

            OldPteContents = *PointerPte;
    
            ASSERT (OldPteContents.u.Hard.Valid == 1);

            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] =
                    MiGetVirtualAddressMappedByPte (PointerPte);
                PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
                PteFlushList.Count += 1;
            }

            MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
        }

        MI_SET_PFN_DELETED (Pfn1);

        MdlPage += 1;
    }

    //
    // Flush the TB entries for any relevant pages.
    //

    MiFlushPteList (&PteFlushList, FALSE, ZeroPte);

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);

    //
    // Free the actual pages (this may be a partially filled MDL).
    //

    PagesInMdl = MdlPage - (PPFN_NUMBER)(MemoryDescriptorList + 1);

    //
    // Set the ByteCount to the actual number of validated pages - the caller
    // may have lied and we have to sync up here to account for any bogus
    // frames.
    //

    MemoryDescriptorList->ByteCount = (ULONG)(PagesInMdl << PAGE_SHIFT);

    if (PagesInMdl != 0) {
        AweInfo->VadPhysicalPages -= PagesInMdl;

        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, 0 - PagesInMdl);

        MmFreePagesFromMdl (MemoryDescriptorList);
    }

    CapturedNumberOfPages -= PagesInMdl;

    if ((Status == STATUS_SUCCESS) && (CapturedNumberOfPages != 0)) {

        UNLOCK_WS (Process);
        WsHeld = FALSE;

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            Attached = FALSE;
        }

        OnePassComplete = TRUE;
        ASSERT (MdlPages == PagesInMdl);
        UserPfnArray += MdlPages;

        //
        // Do it all again until all the pages are freed or an error occurs.
        //

        goto repeat;
    }

    //
    // Fall through.
    //

ErrorReturn:

    if (WsHeld == TRUE) {
        UNLOCK_WS (Process);
    }

    //
    // Free any pool acquired for holding MDLs.
    //

    if (MemoryDescriptorList != (PMDL)&MdlHack[0]) {
        ExFreePool (MemoryDescriptorList);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    //
    // Establish an exception handler and carefully write out the
    // number of pages actually processed.
    //

    try {

        *NumberOfPages = PagesProcessed;

    } except (EXCEPTION_EXECUTE_HANDLER) {

        //
        // Return success at this point even if the results
        // cannot be written.
        //

        NOTHING;
    }

    if (ProcessReferenced == TRUE) {
        ObDereferenceObject (Process);
    }

    return Status;
}


VOID
MiRemoveUserPhysicalPagesVad (
    IN PMMVAD_SHORT Vad
    )

/*++

Routine Description:

    This function removes the user-physical-pages mapped region from the
    current process's address space.  This mapped region is private memory.

    The physical pages of this Vad are unmapped here, but not freed.

    Pagetable pages are freed and their use/commitment counts/quotas are
    managed by our caller.

Arguments:

    Vad - Supplies the VAD which manages the address space.

Return Value:

    None.

Environment:

    APC level, working set mutex and address creation mutex held.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PEPROCESS Process;
    PFN_NUMBER PageFrameIndex;
    MMPTE_FLUSH_LIST PteFlushList;
    PMMPTE PointerPte;
    MMPTE PteContents;
    MMPTE JunkPte;
    PMMPTE EndingPte;
    PAWEINFO AweInfo;
#if DBG
    ULONG_PTR ActualPages;
    ULONG_PTR ExpectedPages;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
#endif

    ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    ASSERT (Vad->u.VadFlags.UserPhysicalPages == 1);

    Process = PsGetCurrentProcess();

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo != NULL);

    //
    // If the physical pages count is zero, nothing needs to be done.
    // On checked systems, verify the list anyway.
    //

#if DBG
    ActualPages = 0;
    ExpectedPages = AweInfo->VadPhysicalPages;
#else
    if (AweInfo->VadPhysicalPages == 0) {
        return;
    }
#endif

    PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
    EndingPte = MiGetPteAddress (MI_VPN_TO_VA_ENDING (Vad->EndingVpn));

    PteFlushList.Count = 0;
    
    //
    // The caller must have removed this Vad from the physical view list,
    // otherwise another thread could immediately remap pages back into this
    // same Vad.
    //

    KeRaiseIrql (APC_LEVEL, &OldIrql);
    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

#if DBG
    NextEntry = AweInfo->AweVadList.Flink;
    while (NextEntry != &AweInfo->AweVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        ASSERT (PhysicalView->Vad != (PMMVAD)Vad);

        NextEntry = NextEntry->Flink;
    }
#endif

    while (PointerPte <= EndingPte) {
        PteContents = *PointerPte;
        if (PteContents.u.Hard.Valid == 0) {
            PointerPte += 1;
            continue;
        }

        //
        // The frame is currently mapped in this Vad so the PTE must
        // be cleared and the TB entry flushed.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));
        ASSERT (ExpectedPages != 0);

        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);

        ASSERT (MI_PFN_IS_AWE (Pfn1));
        ASSERT (Pfn1->u2.ShareCount == 2);
        ASSERT (Pfn1->PteAddress == PointerPte);

        //
        // Note the AWE/PFN locks are not needed here because we have acquired
        // the pushlock exclusive so no one can be mapping or unmapping
        // right now.  In fact, the PFN sharecount doesn't even have to be
        // updated with an interlocked sequence because the pushlock is held
        // exclusive.
        //

        Pfn1->u2.ShareCount -= 1;

        Pfn1->PteAddress = NULL;

        if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushVa[PteFlushList.Count] =
                MiGetVirtualAddressMappedByPte (PointerPte);
            PteFlushList.FlushPte[PteFlushList.Count] = &JunkPte;
            PteFlushList.Count += 1;
        }

        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

        PointerPte += 1;
#if DBG
        ActualPages += 1;
#endif
        ASSERT (ActualPages <= ExpectedPages);
    }

    //
    // Flush the TB entries for any relevant pages.
    //

    MiFlushPteList (&PteFlushList, FALSE, ZeroPte);

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);
    KeLowerIrql (OldIrql);

    return;
}

VOID
MiCleanPhysicalProcessPages (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine frees the VadPhysicalBitMap, any remaining physical pages (as
    they may not have been currently mapped into any Vads) and returns the
    bitmap quota.

Arguments:

    Process - Supplies the process to clean.

Return Value:

    None.

Environment:

    Kernel mode, APC level, working set mutex held.  Called only on process
    exit, so the AWE push lock is not needed here.

--*/

{
    PMMPFN Pfn1;
    PAWEINFO AweInfo;
    ULONG BitMapSize;
    ULONG BitMapIndex;
    ULONG BitMapHint;
    PRTL_BITMAP BitMap;
    PPFN_NUMBER MdlPage;
    PFN_NUMBER MdlHack[(sizeof(MDL) / sizeof(PFN_NUMBER)) + COPY_STACK_SIZE];
    ULONG_PTR MdlPages;
    ULONG_PTR NumberOfPages;
    ULONG_PTR TotalFreedPages;
    PMDL MemoryDescriptorList;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER HighestPossiblePhysicalPage;
#if DBG
    ULONG_PTR ActualPages = 0;
    ULONG_PTR ExpectedPages = 0;
#endif

    ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    AweInfo = (PAWEINFO) Process->AweInfo;

    if (AweInfo == NULL) {
        return;
    }

    TotalFreedPages = 0;
    BitMap = AweInfo->VadPhysicalPagesBitMap;

    if (BitMap == NULL) {
        goto Finish;
    }

#if DBG
    ExpectedPages = AweInfo->VadPhysicalPages;
#else
    if (AweInfo->VadPhysicalPages == 0) {
        goto Finish;
    }
#endif

    MdlPages = COPY_STACK_SIZE;
    MemoryDescriptorList = (PMDL)&MdlHack[0];

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = 0;
    
    BitMapHint = 0;

    while (TRUE) {

        BitMapIndex = RtlFindSetBits (BitMap, 1, BitMapHint);

        if (BitMapIndex < BitMapHint) {
            break;
        }

        if (BitMapIndex == NO_BITS_FOUND) {
            break;
        }

        PageFrameIndex = MI_BITMAP_INDEX_TO_FRAME(BitMapIndex);

        ASSERT64 (PageFrameIndex < _4gb);

        //
        // The bitmap search wraps, so handle it here.
        // Note PFN 0 is illegal.
        //

        ASSERT (PageFrameIndex != 0);
        ASSERT ((PageFrameIndex >= LOWEST_USABLE_PHYSICAL_PAGE) ||
                (MiUsingLowPagesForAwe == TRUE));

        ASSERT (ExpectedPages != 0);
        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->PteAddress == NULL);

        ASSERT (MI_PFN_IS_AWE (Pfn1));

        MI_SET_PFN_DELETED(Pfn1);

        *MdlPage = PageFrameIndex;
        MdlPage += 1;
        NumberOfPages += 1;
#if DBG
        ActualPages += 1;
#endif

        if (NumberOfPages == COPY_STACK_SIZE) {

            //
            // Free the pages in the full MDL.
            //

            MmInitializeMdl (MemoryDescriptorList,
                             0,
                             NumberOfPages << PAGE_SHIFT);

            MmFreePagesFromMdl (MemoryDescriptorList);

            MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
            AweInfo->VadPhysicalPages -= NumberOfPages;
            TotalFreedPages += NumberOfPages;
            NumberOfPages = 0;
        }

        BitMapHint = BitMapIndex + 1;
        if (BitMapHint >= BitMap->SizeOfBitMap) {
            break;
        }
    }

    //
    // Free any straggling MDL pages here.
    //

    if (NumberOfPages != 0) {
        MmInitializeMdl (MemoryDescriptorList,
                         0,
                         NumberOfPages << PAGE_SHIFT);

        MmFreePagesFromMdl (MemoryDescriptorList);
        AweInfo->VadPhysicalPages -= NumberOfPages;
        TotalFreedPages += NumberOfPages;
    }

Finish:

    ASSERT (ExpectedPages == ActualPages);

    HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;

#if defined (_WIN64)
    //
    // Force a 32-bit maximum on any page allocation because the bitmap
    // package is currently 32-bit.
    //

    if (HighestPossiblePhysicalPage + 1 >= _4gb) {
        HighestPossiblePhysicalPage = _4gb - 2;
    }
#endif

    ASSERT (AweInfo->VadPhysicalPages == 0);

    if (BitMap != NULL) {
        BitMapSize = sizeof(RTL_BITMAP) + (ULONG)((((HighestPossiblePhysicalPage + 1) + 31) / 32) * 4);

        ExFreePool (BitMap);
        PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
    }

    ExFreeCacheAwarePushLock (AweInfo->PushLock);
    ExFreePool (AweInfo);

    Process->AweInfo = NULL;

    ASSERT (ExpectedPages == ActualPages);

    if (TotalFreedPages != 0) {
        InterlockedExchangeAddSizeT (&MmVadPhysicalPages, 0 - TotalFreedPages);
    }

    return;
}

VOID
MiAweViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This function inserts a new AWE view into the specified process' AWE chain.

Arguments:

    Process - Supplies the process to add the AWE VAD to.

    PhysicalView - Supplies the physical view data to link in.

Return Value:

    TRUE if the view was inserted, FALSE if not.

Environment:

    Kernel mode.  APC_LEVEL, working set and address space mutexes held.

--*/

{
    PAWEINFO AweInfo;

    AweInfo = (PAWEINFO) Process->AweInfo;

    ASSERT (AweInfo != NULL);

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    InsertTailList (&AweInfo->AweVadList, &PhysicalView->ListEntry);

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);
}

VOID
MiAweViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function removes an AWE Vad from the specified process' AWE chain.

Arguments:

    Process - Supplies the process to remove the AWE VAD from.

    Vad - Supplies the Vad to remove.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, working set and address space mutexes held.

--*/

{
    PAWEINFO AweInfo;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW AweView;

    AweInfo = (PAWEINFO) Process->AweInfo;
    ASSERT (AweInfo != NULL);

    ExAcquireCacheAwarePushLockExclusive (AweInfo->PushLock);

    NextEntry = AweInfo->AweVadList.Flink;
    while (NextEntry != &AweInfo->AweVadList) {

        AweView = CONTAINING_RECORD (NextEntry,
                                     MI_PHYSICAL_VIEW,
                                     ListEntry);

        if (AweView->Vad == Vad) {
            RemoveEntryList (NextEntry);
            ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);
            ExFreePool (AweView);
            return;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (FALSE);

    ExReleaseCacheAwarePushLockExclusive (AweInfo->PushLock);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\procsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   procsup.c

Abstract:

    This module contains routines which support the process structure.

Author:

    Lou Perazzoli (loup) 25-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/


#include "mi.h"

#if (_MI_PAGING_LEVELS >= 3)

#include "wow64t.h"

#define MI_LARGE_STACK_SIZE     KERNEL_LARGE_STACK_SIZE

#if defined(_AMD64_)

#define MM_PROCESS_COMMIT_CHARGE 6
#define MM_PROCESS_CREATE_CHARGE 8

#elif defined(_IA64_)

#define MM_PROCESS_COMMIT_CHARGE 5
#define MM_PROCESS_CREATE_CHARGE 8

#endif

#else

//
// Registry settable but must always be a page multiple and less than
// or equal to KERNEL_LARGE_STACK_SIZE.
//

ULONG MmLargeStackSize = KERNEL_LARGE_STACK_SIZE;

#define MI_LARGE_STACK_SIZE     MmLargeStackSize

#if !defined (_X86PAE_)
#define MM_PROCESS_COMMIT_CHARGE 4
#define MM_PROCESS_CREATE_CHARGE 6
#else
#define MM_PROCESS_COMMIT_CHARGE 8
#define MM_PROCESS_CREATE_CHARGE 10
#endif

#endif

#define DONTASSERT(x)

extern ULONG MmProductType;

extern MM_SYSTEMSIZE MmSystemSize;

extern PVOID BBTBuffer;

SIZE_T MmProcessCommit;

ULONG MmKernelStackPages;
PFN_NUMBER MmKernelStackResident;
ULONG MmLargeStacks;
ULONG MmSmallStacks;

MMPTE KernelDemandZeroPte = {MM_KERNEL_DEMAND_ZERO_PTE};

CCHAR MmRotatingUniprocessorNumber;

//
// Enforced minimal commit for user mode stacks
//

ULONG MmMinimumStackCommitInBytes;

PFN_NUMBER
MiMakeOutswappedPageResident (
    IN PMMPTE ActualPteAddress,
    IN PMMPTE PointerTempPte,
    IN ULONG Global,
    IN PFN_NUMBER ContainingPage
    );

NTSTATUS
MiCreatePebOrTeb (
    IN PEPROCESS TargetProcess,
    IN ULONG Size,
    OUT PVOID *Base
    );

VOID
MiDeleteAddressesInWorkingSet (
    IN PEPROCESS Process
    );

VOID
MiDeleteValidAddress (
    IN PVOID Va,
    IN PEPROCESS CurrentProcess
    );

VOID
MiDeleteFreeVm (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

VOID
VadTreeWalk (
    VOID
    );

PMMVAD
MiAllocateVad(
    IN ULONG_PTR StartingVirtualAddress,
    IN ULONG_PTR EndingVirtualAddress,
    IN LOGICAL Deletable
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmCreateTeb)
#pragma alloc_text(PAGE,MmCreatePeb)
#pragma alloc_text(PAGE,MiCreatePebOrTeb)
#pragma alloc_text(PAGE,MmDeleteTeb)
#pragma alloc_text(PAGE,MiAllocateVad)
#pragma alloc_text(PAGE,MiDeleteAddressesInWorkingSet)
#pragma alloc_text(PAGE,MmSetMemoryPriorityProcess)
#pragma alloc_text(PAGE,MmInitializeHandBuiltProcess)
#pragma alloc_text(PAGE,MmInitializeHandBuiltProcess2)
#pragma alloc_text(PAGE,MmGetDirectoryFrameFromProcess)
#endif


BOOLEAN
MmCreateProcessAddressSpace (
    IN ULONG MinimumWorkingSetSize,
    IN PEPROCESS NewProcess,
    OUT PULONG_PTR DirectoryTableBase
    )

/*++

Routine Description:

    This routine creates an address space which maps the system
    portion and contains a hyper space entry.

Arguments:

    MinimumWorkingSetSize - Supplies the minimum working set size for
                            this address space.  This value is only used
                            to ensure that ample physical pages exist
                            to create this process.

    NewProcess - Supplies a pointer to the process object being created.

    DirectoryTableBase - Returns the value of the newly created
                         address space's Page Directory (PD) page and
                         hyper space page.

Return Value:

    Returns TRUE if an address space was successfully created, FALSE
    if ample physical pages do not exist.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PFN_NUMBER PageDirectoryIndex;
    PFN_NUMBER HyperSpaceIndex;
    PFN_NUMBER PageContainingWorkingSet;
    PFN_NUMBER VadBitMapPage;
    MMPTE TempPte;
    PEPROCESS CurrentProcess;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    ULONG Color;
    PMMPTE PointerPte;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PFN_NUMBER PageDirectoryParentIndex;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PFN_NUMBER HyperDirectoryIndex;
#endif
#if defined (_X86PAE_)
    ULONG MaximumStart;
    ULONG TopQuad;
    MMPTE TopPte;
    PPAE_ENTRY PaeVa;
    ULONG i;
    ULONG NumberOfPdes;
    PFN_NUMBER HyperSpaceIndex2;
    PFN_NUMBER PageDirectories[PD_PER_SYSTEM];
#endif
#if !defined (_IA64_)
    PMMPTE PointerFillPte;
    PMMPTE CurrentAddressSpacePde;
#endif

    CurrentProcess = PsGetCurrentProcess ();

    //
    // Charge commitment for the page directory pages, working set page table
    // page, and working set list.  If Vad bitmap lookups are enabled, then
    // charge for a page or two for that as well.
    //

    if (MiChargeCommitment (MM_PROCESS_COMMIT_CHARGE, NULL) == FALSE) {
        return FALSE;
    }

    NewProcess->NextPageColor = (USHORT)(RtlRandom(&MmProcessColorSeed));
    KeInitializeSpinLock (&NewProcess->HyperSpaceLock);

#if defined (_X86PAE_)
    TopQuad = MiPaeAllocate (&PaeVa);
    if (TopQuad == 0) {
        MiReturnCommitment (MM_PROCESS_COMMIT_CHARGE);
        return FALSE;
    }
#endif

    LOCK_WS (CurrentProcess);

    //
    // Get the PFN lock to prevent another thread in this
    // process from using hyper space and to get physical pages.
    //

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)MinimumWorkingSetSize){

        UNLOCK_PFN (OldIrql);
        UNLOCK_WS (CurrentProcess);
        MiReturnCommitment (MM_PROCESS_COMMIT_CHARGE);

#if defined (_X86PAE_)
        MiPaeFree (PaeVa);
#endif

        //
        // Indicate no directory base was allocated.
        //

        return FALSE;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_PROCESS_CREATE, MM_PROCESS_COMMIT_CHARGE);

    MmResidentAvailablePages -= MinimumWorkingSetSize;
    MM_BUMP_COUNTER(6, MinimumWorkingSetSize);
    MmProcessCommit += MM_PROCESS_COMMIT_CHARGE;

    ASSERT (NewProcess->AddressSpaceInitialized == 0);
    PS_SET_BITS (&NewProcess->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
    ASSERT (NewProcess->AddressSpaceInitialized == 1);

    NewProcess->Vm.MinimumWorkingSetSize = MinimumWorkingSetSize;

    //
    // Allocate a page directory (parent for 64-bit systems) page.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color =  MI_PAGE_COLOR_PTE_PROCESS (PDE_BASE,
                                        &CurrentProcess->NextPageColor);

    PageDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

#if defined (_X86PAE_)

    //
    // Allocate the additional page directory pages.
    //

    for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {

        MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

        Color =  MI_PAGE_COLOR_PTE_PROCESS (PDE_BASE,
                                            &CurrentProcess->NextPageColor);

        PageDirectories[i] = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);
    }

    PageDirectories[i] = PageDirectoryIndex;

    //
    // Recursively map each page directory page so it points to itself.
    //

    TempPte = ValidPdePde;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    PointerPte = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess, PageDirectoryIndex);
    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        TempPte.u.Hard.PageFrameNumber = PageDirectories[i];
        PointerPte[i] = TempPte;
    }
    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPte);

    //
    // Initialize the parent page directory entries.
    //

    TopPte.u.Long = TempPte.u.Long & ~MM_PAE_PDPTE_MASK;
    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        TopPte.u.Hard.PageFrameNumber = PageDirectories[i];
        PaeVa->PteEntry[i].u.Long = TopPte.u.Long;
    }

    NewProcess->PaeTop = (PVOID)PaeVa;
    DirectoryTableBase[0] = TopQuad;
#else
    INITIALIZE_DIRECTORY_TABLE_BASE(&DirectoryTableBase[0], PageDirectoryIndex);
#endif

#if (_MI_PAGING_LEVELS >= 3)

    PointerPpe = KSEG_ADDRESS (PageDirectoryIndex);
    TempPte = ValidPdePde;

    //
    // Map the top level page directory parent page recursively onto itself.
    //

    TempPte.u.Hard.PageFrameNumber = PageDirectoryIndex;

    //
    // Set the PTE address in the PFN for the top level page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryIndex);

#if (_MI_PAGING_LEVELS >= 4)

    PageDirectoryParentIndex = PageDirectoryIndex;

    PointerPxe = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                     PageDirectoryIndex);

    Pfn1->PteAddress = MiGetPteAddress(PXE_BASE);

    PointerPxe[MiGetPxeOffset(PXE_BASE)] = TempPte;

    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPxe);

    //
    // Now that the top level extended page parent page is initialized,
    // allocate a page parent page.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color =  MI_PAGE_COLOR_PTE_PROCESS (PDE_BASE,
                                        &CurrentProcess->NextPageColor);

    PageDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    //
    //
    // Map this directory parent page into the top level
    // extended page directory parent page.
    //

    TempPte.u.Hard.PageFrameNumber = PageDirectoryIndex;

    PointerPxe = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                     PageDirectoryParentIndex);
    PointerPxe[MiGetPxeOffset(HYPER_SPACE)] = TempPte;

    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPxe);

#else
    Pfn1->PteAddress = MiGetPteAddress((PVOID)PDE_TBASE);
    PointerPpe[MiGetPpeOffset(PDE_TBASE)] = TempPte;
#endif

    //
    // Allocate the page directory for hyper space and map this directory
    // page into the page directory parent page.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color = MI_PAGE_COLOR_PTE_PROCESS (MiGetPpeAddress(HYPER_SPACE),
                                       &CurrentProcess->NextPageColor);

    HyperDirectoryIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    TempPte.u.Hard.PageFrameNumber = HyperDirectoryIndex;

#if (_MI_PAGING_LEVELS >= 4)
    PointerPpe = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                     PageDirectoryIndex);
#endif

    PointerPpe[MiGetPpeOffset(HYPER_SPACE)] = TempPte;

#if (_MI_PAGING_LEVELS >= 4)
    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPpe);
#endif

#if defined (_IA64_)

    //
    // Initialize the page directory parent for the session (or win32k) space.
    // Any new process shares the session (or win32k) address space (and TB)
    // of its parent.
    //

    NewProcess->Pcb.SessionParentBase = CurrentProcess->Pcb.SessionParentBase;
    NewProcess->Pcb.SessionMapInfo = CurrentProcess->Pcb.SessionMapInfo;

#endif

#endif

    //
    // Allocate the hyper space page table page.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color = MI_PAGE_COLOR_PTE_PROCESS (MiGetPdeAddress(HYPER_SPACE),
                                       &CurrentProcess->NextPageColor);

    HyperSpaceIndex = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

#if (_MI_PAGING_LEVELS >= 3)
#if defined (_IA64_)
    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex;
    PointerPde = KSEG_ADDRESS (HyperDirectoryIndex);
    PointerPde[MiGetPdeOffset(HYPER_SPACE)] = TempPte;
#endif
#if (_AMD64_)
    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex;
    PointerPde = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                     HyperDirectoryIndex);

    PointerPde[MiGetPdeOffset(HYPER_SPACE)] = TempPte;
    MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPde);
#endif

#endif

#if defined (_X86PAE_)

    //
    // Allocate the second hyper space page table page.
    // Save it in the first PTE used by the first hyperspace PDE.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color = MI_PAGE_COLOR_PTE_PROCESS (MiGetPdeAddress(HYPER_SPACE2),
                                       &CurrentProcess->NextPageColor);

    HyperSpaceIndex2 = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    //
    // Unlike DirectoryTableBase[0], the HyperSpaceIndex is stored as an
    // absolute PFN and does not need to be below 4GB.
    //

    DirectoryTableBase[1] = HyperSpaceIndex;
#else
    INITIALIZE_DIRECTORY_TABLE_BASE(&DirectoryTableBase[1], HyperSpaceIndex);
#endif

    //
    // Remove page(s) for the VAD bitmap.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color = MI_PAGE_COLOR_VA_PROCESS (MmWorkingSetList,
                                      &CurrentProcess->NextPageColor);

    VadBitMapPage = MiRemoveZeroPageMayReleaseLocks (Color, OldIrql);

    //
    // Remove page for the working set list.
    //

    MiEnsureAvailablePageOrWait (CurrentProcess, NULL);

    Color = MI_PAGE_COLOR_VA_PROCESS (MmWorkingSetList,
                                      &CurrentProcess->NextPageColor);

    PageContainingWorkingSet = MiRemoveZeroPageIfAny (Color);
    if (PageContainingWorkingSet == 0) {
        PageContainingWorkingSet = MiRemoveAnyPage (Color);
        UNLOCK_PFN (OldIrql);
        MiZeroPhysicalPage (PageContainingWorkingSet, Color);
    }
    else {

        //
        // Release the PFN lock as the needed pages have been allocated.
        //

        UNLOCK_PFN (OldIrql);
    }

    NewProcess->WorkingSetPage = PageContainingWorkingSet;

    //
    // Initialize the page reserved for hyper space.
    //

    MI_INITIALIZE_HYPERSPACE_MAP (HyperSpaceIndex);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Set the PTE address in the PFN for the hyper space page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (HyperDirectoryIndex);

    Pfn1->PteAddress = MiGetPpeAddress(HYPER_SPACE);

#if defined (_AMD64_)

    //
    // Copy the system mappings including the shared user page & session space.
    //

    CurrentAddressSpacePde = MiGetPxeAddress(KI_USER_SHARED_DATA);
    PointerPxe = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess,
                                                PageDirectoryParentIndex,
                                                &OldIrql);

    PointerFillPte = &PointerPxe[MiGetPxeOffset(KI_USER_SHARED_DATA)];
    RtlCopyMemory (PointerFillPte,
                   CurrentAddressSpacePde,
                   ((1 + (MiGetPxeAddress(MM_SYSTEM_SPACE_END) -
                      CurrentAddressSpacePde)) * sizeof(MMPTE)));

    MiUnmapPageInHyperSpace (CurrentProcess, PointerPxe, OldIrql);

#endif

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = VadBitMapPage;
    MI_SET_GLOBAL_STATE (TempPte, 0);

#if defined (_AMD64_)
    PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess,
                                                HyperSpaceIndex,
                                                &OldIrql);

    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;

    MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);
#else
    PointerPte = KSEG_ADDRESS (HyperSpaceIndex);
    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;
#endif

#else // the following is for (_MI_PAGING_LEVELS < 3) only

#if defined (_X86PAE_)

    //
    // Stash the second hyperspace PDE in the first PTE for the initial
    // hyperspace entry.
    //

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex2;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess, HyperSpaceIndex, &OldIrql);

    PointerPte[0] = TempPte;

    TempPte.u.Hard.PageFrameNumber = VadBitMapPage;
    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;

    MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);

#else

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = VadBitMapPage;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess, HyperSpaceIndex, &OldIrql);

    PointerPte[MiGetPteOffset(VAD_BITMAP_SPACE)] = TempPte;

    TempPte.u.Hard.PageFrameNumber = PageContainingWorkingSet;
    PointerPte[MiGetPteOffset(MmWorkingSetList)] = TempPte;

    MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);

#endif

    //
    // Set the PTE address in the PFN for the page directory page.
    //

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryIndex);

    Pfn1->PteAddress = (PMMPTE)PDE_BASE;

    TempPte = ValidPdePde;
    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex;
    MI_SET_GLOBAL_STATE (TempPte, 0);

    //
    // Map the page directory page in hyperspace.
    // Note for PAE, this is the high 1GB virtual only.
    //

    PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess, PageDirectoryIndex, &OldIrql);
    PointerPte[MiGetPdeOffset(HYPER_SPACE)] = TempPte;

#if defined (_X86PAE_)

    //
    // Map in the second hyperspace page directory.
    // The page directory page is already recursively mapped.
    //

    TempPte.u.Hard.PageFrameNumber = HyperSpaceIndex2;
    PointerPte[MiGetPdeOffset(HYPER_SPACE2)] = TempPte;

#else

    //
    // Recursively map the page directory page so it points to itself.
    //

    TempPte.u.Hard.PageFrameNumber = PageDirectoryIndex;
    PointerPte[MiGetPdeOffset(PTE_BASE)] = TempPte;

#endif

    //
    // Map in the non paged portion of the system.
    //

    //
    // For the PAE case, only the last page directory is currently mapped, so
    // only copy the system PDEs for the last 1GB - any that need copying in
    // the 2gb->3gb range will be done a little later.
    //

    if (MmVirtualBias != 0) {
        PointerFillPte = &PointerPte[MiGetPdeOffset(CODE_START + MmVirtualBias)];
        CurrentAddressSpacePde = MiGetPdeAddress(CODE_START + MmVirtualBias);

        RtlCopyMemory (PointerFillPte,
                       CurrentAddressSpacePde,
                       (((1 + CODE_END) - CODE_START) / MM_VA_MAPPED_BY_PDE) * sizeof(MMPTE));
    }

    PointerFillPte = &PointerPte[MiGetPdeOffset(MmNonPagedSystemStart)];
    CurrentAddressSpacePde = MiGetPdeAddress(MmNonPagedSystemStart);

    RtlCopyMemory (PointerFillPte,
                   CurrentAddressSpacePde,
                   ((1 + (MiGetPdeAddress(NON_PAGED_SYSTEM_END) -
                      CurrentAddressSpacePde))) * sizeof(MMPTE));

    //
    // Map in the system cache page table pages.
    //

    PointerFillPte = &PointerPte[MiGetPdeOffset(MM_SYSTEM_CACHE_WORKING_SET)];
    CurrentAddressSpacePde = MiGetPdeAddress(MM_SYSTEM_CACHE_WORKING_SET);

    RtlCopyMemory (PointerFillPte,
                   CurrentAddressSpacePde,
                   ((1 + (MiGetPdeAddress(MmSystemCacheEnd) -
                      CurrentAddressSpacePde))) * sizeof(MMPTE));

#if !defined (_X86PAE_)

    //
    // Map all the virtual space in the 2GB->3GB range when it's not user space.
    // This includes kernel/HAL code & data, the PFN database, initial nonpaged
    // pool, any extra system PTE or system cache areas, system views and
    // session space.
    //

    if (MmVirtualBias == 0) {

        PointerFillPte = &PointerPte[MiGetPdeOffset(CODE_START)];
        CurrentAddressSpacePde = MiGetPdeAddress(CODE_START);

        RtlCopyMemory (PointerFillPte,
                       CurrentAddressSpacePde,
                       ((MM_SESSION_SPACE_DEFAULT_END - CODE_START) / MM_VA_MAPPED_BY_PDE) * sizeof(MMPTE));
    }
    else {

        //
        // Booted /3GB, so copy the bootstrap entry for session space as it's
        // not included in 2GB->3GB copy above.
        //

        PointerFillPte = &PointerPte[MiGetPdeOffset(MmSessionSpace)];
        CurrentAddressSpacePde = MiGetPdeAddress(MmSessionSpace);
        if (CurrentAddressSpacePde->u.Hard.Valid == 1) {
            MI_WRITE_VALID_PTE (PointerFillPte, *CurrentAddressSpacePde);
        }
        else {
            MI_WRITE_INVALID_PTE (PointerFillPte, *CurrentAddressSpacePde);
        }
    }

    if (MiMaximumSystemExtraSystemPdes) {

        PointerFillPte = &PointerPte[MiGetPdeOffset(MiUseMaximumSystemSpace)];
        CurrentAddressSpacePde = MiGetPdeAddress(MiUseMaximumSystemSpace);

        RtlCopyMemory (PointerFillPte,
                       CurrentAddressSpacePde,
                       MiMaximumSystemExtraSystemPdes * sizeof(MMPTE));
    }
#endif

    MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);

#if defined (_X86PAE_)

    //
    // Map all the virtual space in the 2GB->3GB range when it's not user space.
    // This includes kernel/HAL code & data, the PFN database, initial nonpaged
    // pool, any extra system PTE or system cache areas, system views and
    // session space.
    //

    if (MmVirtualBias == 0) {

        PageDirectoryIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PaeVa->PteEntry[PD_PER_SYSTEM - 2]);

        PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess, PageDirectoryIndex, &OldIrql);

        PointerFillPte = &PointerPte[MiGetPdeOffset(CODE_START)];
        CurrentAddressSpacePde = MiGetPdeAddress(CODE_START);

        RtlCopyMemory (PointerFillPte,
                       CurrentAddressSpacePde,
                       ((MM_SESSION_SPACE_DEFAULT_END - CODE_START) / MM_VA_MAPPED_BY_PDE) * sizeof(MMPTE));

	MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);
    }


    //
    // If portions of the range between 1GB and 2GB are being used for
    // additional system PTEs, then copy those too.
    //

    if (MiMaximumSystemExtraSystemPdes != 0) {

	MaximumStart = MiUseMaximumSystemSpace;

	while (MaximumStart < MiUseMaximumSystemSpaceEnd) {
            i = MiGetPdPteOffset (MiUseMaximumSystemSpace);
            PageDirectoryIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PaeVa->PteEntry[i]);

	    PointerPte = (PMMPTE)MiMapPageInHyperSpace (CurrentProcess,
							PageDirectoryIndex,
							&OldIrql);

	    PointerFillPte = &PointerPte[MiGetPdeOffset(MaximumStart)];
	    CurrentAddressSpacePde = MiGetPdeAddress(MaximumStart);

	    NumberOfPdes = PDE_PER_PAGE - MiGetPdeOffset(MaximumStart);

	    RtlCopyMemory (PointerFillPte,
			   CurrentAddressSpacePde,
			   NumberOfPdes * sizeof(MMPTE));

	    MiUnmapPageInHyperSpace (CurrentProcess, PointerPte, OldIrql);

	    MaximumStart = (ULONG) MiGetVirtualAddressMappedByPde (CurrentAddressSpacePde + NumberOfPdes);
	}
    }
#endif

#endif  // end of (_MI_PAGING_LEVELS < 3) specific else

    //
    // Up the session space reference count.
    //

    MiSessionAddProcess (NewProcess);

    //
    // Release working set mutex and lower IRQL.
    //

    UNLOCK_WS (CurrentProcess);

    return TRUE;
}

NTSTATUS
MmInitializeProcessAddressSpace (
    IN PEPROCESS ProcessToInitialize,
    IN PEPROCESS ProcessToClone OPTIONAL,
    IN PVOID SectionToMap OPTIONAL,
    OUT POBJECT_NAME_INFORMATION *AuditName OPTIONAL
    )

/*++

Routine Description:

    This routine initializes the working set and mutexes within a
    newly created address space to support paging.

    No page faults may occur in a new process until this routine has
    completed.

Arguments:

    ProcessToInitialize - Supplies a pointer to the process to initialize.

    ProcessToClone - Optionally supplies a pointer to the process whose
                     address space should be copied into the
                     ProcessToInitialize address space.

    SectionToMap - Optionally supplies a section to map into the newly
                   initialized address space.

    Only one of ProcessToClone and SectionToMap may be specified.

    AuditName - Supplies an opaque object name information pointer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  APCs disabled.

--*/

{
    KIRQL OldIrql;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PVOID BaseAddress;
    SIZE_T ViewSize;
    NTSTATUS Status;
    PFILE_OBJECT FilePointer;
    PFN_NUMBER PageContainingWorkingSet;
    LARGE_INTEGER SectionOffset;
    PSECTION_IMAGE_INFORMATION ImageInfo;
    PMMVAD VadShare;
    PMMVAD VadReserve;
    PLOCK_HEADER LockedPagesHeader;
    PFN_NUMBER PdePhysicalPage;
    PFN_NUMBER VadBitMapPage;
    ULONG i;
    ULONG NumberOfPages;

#if defined (_X86PAE_)
    PFN_NUMBER PdePhysicalPage2;
#endif

#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER PpePhysicalPage;
#if DBG
    ULONG j;
    PUCHAR p;
#endif
#endif

#if (_MI_PAGING_LEVELS >= 4)
    PFN_NUMBER PxePhysicalPage;
#endif

#if defined(_WIN64)
    PWOW64_PROCESS Wow64Process;
#endif

    //
    // Initialize Working Set Mutex in process header.
    //

    KeAttachProcess (&ProcessToInitialize->Pcb);

    ASSERT (ProcessToInitialize->AddressSpaceInitialized <= 1);
    PS_CLEAR_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
    ASSERT (ProcessToInitialize->AddressSpaceInitialized == 0);

    PS_SET_BITS (&ProcessToInitialize->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE2);
    ASSERT (ProcessToInitialize->AddressSpaceInitialized == 2);


    ExInitializeFastMutex (&ProcessToInitialize->AddressCreationLock);
    ExInitializeFastMutex (&ProcessToInitialize->WorkingSetLock);

    //
    // NOTE:  The process block has been zeroed when allocated, so
    // there is no need to zero fields and set pointers to NULL.
    //

    ASSERT (ProcessToInitialize->VadRoot == NULL);

    KeQuerySystemTime(&ProcessToInitialize->Vm.LastTrimTime);
    ProcessToInitialize->Vm.VmWorkingSetList = MmWorkingSetList;

    //
    // Obtain a page to map the working set and initialize the
    // working set.  Get the PFN lock to allocate physical pages.
    //

    LOCK_PFN (OldIrql);

    //
    // Initialize the PFN database for the Page Directory and the
    // PDE which maps hyper space.
    //

#if (_MI_PAGING_LEVELS >= 3)

#if (_MI_PAGING_LEVELS >= 4)
    PointerPte = MiGetPteAddress (PXE_BASE);
    PxePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    MiInitializePfn (PxePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPxeAddress (HYPER_SPACE);
#else
    PointerPte = MiGetPteAddress ((PVOID)PDE_TBASE);
#endif

    PpePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    MiInitializePfn (PpePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPpeAddress (HYPER_SPACE);

#elif defined (_X86PAE_)
    PointerPte = MiGetPdeAddress (PDE_BASE);
#else
    PointerPte = MiGetPteAddress (PDE_BASE);
#endif

    PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    MiInitializePfn (PdePhysicalPage, PointerPte, 1);

    PointerPte = MiGetPdeAddress (HYPER_SPACE);
    MiInitializePfn (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte), PointerPte, 1);

#if defined (_X86PAE_)

    for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
        PointerPte = MiGetPteAddress (PDE_BASE + (i << PAGE_SHIFT));
        PdePhysicalPage2 = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        MiInitializePfn (PdePhysicalPage2, PointerPte, 1);
    }

    PointerPte = MiGetPdeAddress (HYPER_SPACE2);
    MiInitializePfn (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte), PointerPte, 1);
#endif

    //
    // The VAD bitmap spans one page when booted 2GB and the working set
    // page follows it.  If booted 3GB, the VAD bitmap spans 1.5 pages and
    // the working set list uses the last half of the second page.
    //

    NumberOfPages = 2;

    PointerPte = MiGetPteAddress (VAD_BITMAP_SPACE);

    for (i = 0; i < NumberOfPages; i += 1) {

        ASSERT (PointerPte->u.Long != 0);
        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        PointerPte->u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        MiInitializePfn (VadBitMapPage, PointerPte, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           VadBitMapPage,
                           MM_READWRITE,
                           PointerPte);

        MI_SET_PTE_DIRTY (TempPte);
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);

    PageContainingWorkingSet = ProcessToInitialize->WorkingSetPage;

    ASSERT (ProcessToInitialize->LockedPagesList == NULL);

    if (MmTrackLockedPages == TRUE) {
        LockedPagesHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                   sizeof(LOCK_HEADER),
                                                   'xTmM');

        if (LockedPagesHeader != NULL) {
            LockedPagesHeader->Count = 0;
            InitializeListHead (&LockedPagesHeader->ListHead);
            ProcessToInitialize->LockedPagesList = (PVOID)LockedPagesHeader;
        }
    }

    MiInitializeWorkingSetList (ProcessToInitialize);

    InitializeListHead (&ProcessToInitialize->PhysicalVadList);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Allocate the commitment tracking bitmaps for page directory and page
    // table pages.  This must be done before any VAD creations occur.
    //

    ASSERT (MmWorkingSetList->CommittedPageTables == NULL);
    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectories == 0);

    ASSERT ((ULONG_PTR)MM_SYSTEM_RANGE_START % (PTE_PER_PAGE * PAGE_SIZE) == 0);

    MmWorkingSetList->CommittedPageTables = (PULONG)
        ExAllocatePoolWithTag (MmPagedPoolEnd != NULL ? PagedPool : NonPagedPool,
                               (MM_USER_PAGE_TABLE_PAGES + 7) / 8,
                               'dPmM');

    if (MmWorkingSetList->CommittedPageTables == NULL) {
        KeDetachProcess ();
        return STATUS_NO_MEMORY;
    }

#if (_MI_PAGING_LEVELS >= 4)

#if DBG
    p = (PUCHAR) MmWorkingSetList->CommittedPageDirectoryParents;

    for (j = 0; j < ((MM_USER_PAGE_DIRECTORY_PARENT_PAGES + 7) / 8); j += 1) {
        ASSERT (*p == 0);
        p += 1;
    }
#endif

    ASSERT (MmWorkingSetList->CommittedPageDirectories == NULL);
    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectoryParents == 0);

    MmWorkingSetList->CommittedPageDirectories = (PULONG)
        ExAllocatePoolWithTag (MmPagedPoolEnd != NULL ? PagedPool : NonPagedPool,
                               (MM_USER_PAGE_DIRECTORY_PAGES + 7) / 8,
                               'dPmM');

    if (MmWorkingSetList->CommittedPageDirectories == NULL) {
        ExFreePool (MmWorkingSetList->CommittedPageTables);
        MmWorkingSetList->CommittedPageTables = NULL;
        KeDetachProcess ();
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (MmWorkingSetList->CommittedPageDirectories,
                   (MM_USER_PAGE_DIRECTORY_PAGES + 7) / 8);
#endif

    RtlZeroMemory (MmWorkingSetList->CommittedPageTables,
                   (MM_USER_PAGE_TABLE_PAGES + 7) / 8);

#if DBG
    p = (PUCHAR) MmWorkingSetList->CommittedPageDirectories;

    for (j = 0; j < ((MM_USER_PAGE_DIRECTORY_PAGES + 7) / 8); j += 1) {
        ASSERT (*p == 0);
        p += 1;
    }
#endif

#endif

    //
    // Page faults may be taken now.
    //
    // If the system has been biased to an alternate base address to allow
    // 3gb of user address space and a process is not being cloned, then
    // create a VAD for the shared memory page.
    //

#if defined(_X86_) && defined(MM_SHARED_USER_DATA_VA)

    if ((MmVirtualBias != 0) && (ProcessToClone == NULL)) {

        VadShare = NULL;

        //
        // Allocate a VAD to map the shared memory page. If a VAD cannot be
        // allocated, then detach from the target process and return a failure
        // status.  This VAD is marked as not deletable.
        //

        if (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA) {
            VadShare = MiAllocateVad (MM_SHARED_USER_DATA_VA,
                                      MM_SHARED_USER_DATA_VA,
                                      FALSE);

            if (VadShare == NULL) {
                KeDetachProcess ();
                return STATUS_NO_MEMORY;
            }
        }

        //
        // If a section is being mapped and the executable is not large
        // address space aware, then create a VAD that reserves the address
        // space between 2gb and the highest user address.
        //

        if (SectionToMap != NULL) {

            if (!((PSECTION)SectionToMap)->u.Flags.Image) {
                KeDetachProcess ();
                if (VadShare != NULL) {
                    ExFreePool (VadShare);
                }
                return STATUS_SECTION_NOT_IMAGE;
            }

            ImageInfo = ((PSECTION)SectionToMap)->Segment->u2.ImageInformation;

            if ((ImageInfo->ImageCharacteristics & IMAGE_FILE_LARGE_ADDRESS_AWARE) == 0) {
                //
                // Allocate a VAD to map the address space between 2gb and
                // the highest user address. If a VAD cannot be allocated,
                // then deallocate the shared address space VAD, detach from
                // the target process, and return a failure status.
                // This VAD is marked as not deletable.
                //

                VadReserve = MiAllocateVad (_2gb,
                                            (ULONG_PTR)MM_HIGHEST_VAD_ADDRESS,
                                            FALSE);

                if (VadReserve == NULL) {
                    KeDetachProcess ();
                    if (VadShare != NULL) {
                        ExFreePool (VadShare);
                    }
                    return STATUS_NO_MEMORY;
                }

                //
                // Insert the VAD.
                //
                // N.B. No failure can occur since there is no commit charge.
                //

                Status = MiInsertVad (VadReserve);
                ASSERT (NT_SUCCESS(Status));
            }
        }

        //
        // Insert the VAD.
        //
        // N.B. No failure can occur since there is no commit charge.
        //

        if (VadShare != NULL) {
            Status = MiInsertVad (VadShare);
            ASSERT (NT_SUCCESS(Status));
        }
    }

#endif

#if defined(_WIN64)

    if (ProcessToClone == NULL) {

        //
        // Reserve the address space just below KUSER_SHARED_DATA as the
        // compatibility area.  This range (and pieces of it) can be
        // unreserved by user mode code such as WOW64 or csrss.  Hence
        // commit must be charged for the page directory and table pages.
        //

        ASSERT(MiCheckForConflictingVad(ProcessToInitialize, WOW64_COMPATIBILITY_AREA_ADDRESS, MM_SHARED_USER_DATA_VA) == NULL);

        VadShare = MiAllocateVad (WOW64_COMPATIBILITY_AREA_ADDRESS,
                                  MM_SHARED_USER_DATA_VA,
                                  TRUE);

    	if (VadShare == NULL) {
           KeDetachProcess ();
           return STATUS_NO_MEMORY;
    	}

        //
        // Zero the commit charge so inserting the VAD will result in the
        // proper charges being applied.  This way when it is split later,
        // the correct commitment will be returned.
        //
        // N.B.  The system process is not allocated with commit because
        //       paged pool and quotas don't exist at the point in Phase0
        //       where this is called.
        //

        if (MmPagedPoolEnd != NULL) {
            VadShare->u.VadFlags.CommitCharge = 0;
        }

        //
        // Reserve the memory above 2GB to prevent 32 bit (WOW64) process
        // access.
        //

    	if (SectionToMap != NULL) {
            if (!((PSECTION)SectionToMap)->u.Flags.Image) {
                KeDetachProcess ();
                ExFreePool (VadShare);
                return STATUS_SECTION_NOT_IMAGE;
            }
            ImageInfo = ((PSECTION)SectionToMap)->Segment->u2.ImageInformation;

            if (((ProcessToInitialize->Flags & PS_PROCESS_FLAGS_OVERRIDE_ADDRESS_SPACE) == 0) &&
                ((ImageInfo->ImageCharacteristics & IMAGE_FILE_LARGE_ADDRESS_AWARE) == 0 ||
                 ImageInfo->Machine == IMAGE_FILE_MACHINE_I386)) {

                //
            	// Allocate a VAD to reserve the address space between 2gb and
            	// the highest user address.  If a VAD cannot be allocated,
            	// then deallocate the compatibility VAD, detach from the target
                // process and return a failure status.
            	//

                VadReserve = MiAllocateVad (_2gb,
                                            (ULONG_PTR)MM_HIGHEST_USER_ADDRESS,
                                            TRUE);

            	if (VadReserve == NULL) {
                    KeDetachProcess ();
                    ExFreePool (VadShare);
                    return STATUS_NO_MEMORY;
            	}

            	//
            	// Insert the VAD.
                //
                // N.B. No failure can occur since there is no commit charge.
            	//

                Status = MiInsertVad (VadReserve);
                ASSERT (NT_SUCCESS(Status));

                if (ImageInfo->Machine == IMAGE_FILE_MACHINE_I386) {

                    //
                    // Initialize the Wow64 process structure.
                    //

                    Wow64Process = (PWOW64_PROCESS) ExAllocatePoolWithTag (
                                                        NonPagedPool,
                                                        sizeof(WOW64_PROCESS),
                                                        'WowM');

                    if (Wow64Process == NULL) {
                        KeDetachProcess ();
                        ExFreePool (VadShare);
                        return STATUS_NO_MEMORY;
                    }

                    RtlZeroMemory (Wow64Process, sizeof(WOW64_PROCESS));

                    ProcessToInitialize->Wow64Process = Wow64Process;

#if defined(_MIALT4K_)

                    //
                    // Initialize the alternate page table for the 4k
                    // page functionality.
                    //

                    Status = MiInitializeAlternateTable (ProcessToInitialize);
                    if (Status != STATUS_SUCCESS) {
                        KeDetachProcess ();
                        ExFreePool (VadShare);
                        return Status;
                    }
#endif
                }
            }
        }

    	//
        // Insert the VAD.  Since this VAD has a commit charge, the working set
        // mutex must be held (as calls inside MiInsertVad to support routines
        // to charge commit require it), failures can occur and must be handled.
    	//

        LOCK_WS (ProcessToInitialize);

        Status = MiInsertVad (VadShare);

        UNLOCK_WS (ProcessToInitialize);

        if (!NT_SUCCESS(Status)) {

            //
            // Note that the VadReserve and Wow64 allocations are automatically
            // released on process destruction so there is no need to tear
            // them down here.
            //

            ExFreePool (VadShare);
            KeDetachProcess ();
            return Status;
        }
    }

#endif

    if (SectionToMap != NULL) {

        //
        // Map the specified section into the address space of the
        // process but only if it is an image section.
        //

        if (!((PSECTION)SectionToMap)->u.Flags.Image) {
            Status = STATUS_SECTION_NOT_IMAGE;
        }
        else {
            UNICODE_STRING UnicodeString;
            ULONG n;
            PWSTR Src;
            PCHAR Dst;
            PSECTION_IMAGE_INFORMATION ImageInformation;

            FilePointer = ((PSECTION)SectionToMap)->Segment->ControlArea->FilePointer;
            ImageInformation = ((PSECTION)SectionToMap)->Segment->u2.ImageInformation;
            UnicodeString = FilePointer->FileName;
            Src = (PWSTR)((PCHAR)UnicodeString.Buffer + UnicodeString.Length);
            n = 0;
            if (UnicodeString.Buffer != NULL) {
                while (Src > UnicodeString.Buffer) {
                    if (*--Src == OBJ_NAME_PATH_SEPARATOR) {
                        Src += 1;
                        break;
                    }
                    else {
                        n += 1;
                    }
                }
            }
            Dst = (PCHAR)ProcessToInitialize->ImageFileName;
            if (n >= sizeof (ProcessToInitialize->ImageFileName)) {
                n = sizeof (ProcessToInitialize->ImageFileName) - 1;
            }

            while (n--) {
                *Dst++ = (UCHAR)*Src++;
            }
            *Dst = '\0';

            if (AuditName != NULL) {
                Status = SeInitializeProcessAuditName (FilePointer, FALSE, AuditName);

                if (!NT_SUCCESS(Status)) {
                    KeDetachProcess ();
                    return Status;
                }
            }

            ProcessToInitialize->SubSystemMajorVersion =
                (UCHAR)ImageInformation->SubSystemMajorVersion;
            ProcessToInitialize->SubSystemMinorVersion =
                (UCHAR)ImageInformation->SubSystemMinorVersion;

            BaseAddress = NULL;
            ViewSize = 0;
            ZERO_LARGE (SectionOffset);

            Status = MmMapViewOfSection ((PSECTION)SectionToMap,
                                         ProcessToInitialize,
                                         &BaseAddress,
                                         0,
                                         0,
                                         &SectionOffset,
                                         &ViewSize,
                                         ViewShare,
                                         0,
                                         PAGE_READWRITE);

            ProcessToInitialize->SectionBaseAddress = BaseAddress;
        }

        KeDetachProcess ();
        return Status;
    }

    if (ProcessToClone != NULL) {

        strcpy ((PCHAR)ProcessToInitialize->ImageFileName,
                (PCHAR)ProcessToClone->ImageFileName);

        //
        // Clone the address space of the specified process.
        //
        // As the page directory and page tables are private to each
        // process, the physical pages which map the directory page
        // and the page table usage must be mapped into system space
        // so they can be updated while in the context of the process
        // we are cloning.
        //

#if defined(_WIN64)

        if (ProcessToClone->Wow64Process != NULL) {

            //
            // Initialize the Wow64 process structure.
            //

            Wow64Process = (PWOW64_PROCESS) ExAllocatePoolWithTag (
                                                NonPagedPool,
                                                sizeof(WOW64_PROCESS),
                                                'WowM');

            if (Wow64Process == NULL) {
                KeDetachProcess ();
                return STATUS_NO_MEMORY;
            }

            RtlZeroMemory (Wow64Process, sizeof(WOW64_PROCESS));

            ProcessToInitialize->Wow64Process = Wow64Process;

#if defined(_MIALT4K_)

            //
            // Initialize the alternate page table for the 4k
            // page functionality.
            //

            Status = MiInitializeAlternateTable (ProcessToInitialize);
            if (Status != STATUS_SUCCESS) {
                KeDetachProcess ();
                return Status;
            }
#endif
        }

#endif

        KeDetachProcess ();

        return MiCloneProcessAddressSpace (ProcessToClone,
                                           ProcessToInitialize,
#if (_MI_PAGING_LEVELS >= 4)
                                           PxePhysicalPage,
#elif (_MI_PAGING_LEVELS >= 3)
                                           PpePhysicalPage,
#else
                                           PdePhysicalPage,
#endif
                                           PageContainingWorkingSet);

    }

    //
    // System Process.
    //

    KeDetachProcess ();
    return STATUS_SUCCESS;
}


VOID
MmInitializeHandBuiltProcess (
    IN PEPROCESS ProcessToInitialize,
    OUT PULONG_PTR DirectoryTableBase
    )

/*++

Routine Description:

    This routine initializes the working set mutex and
    address creation mutex for this "hand built" process.
    Normally the call to MmInitializeAddressSpace initializes the
    working set mutex.  However, in this case, we have already initialized
    the address space and we are now creating a second process using
    the address space of the idle thread.

Arguments:

    ProcessToInitialize - Supplies a pointer to the process to initialize.

    DirectoryTableBase - Receives the pair of directory table base pointers.

Return Value:

    None.

Environment:

    Kernel mode.  APCs disabled, idle process context.

--*/

{
    PEPROCESS CurrentProcess;
#if defined (_X86PAE_)
    ULONG i;
    PMMPTE PdeBase;
    PFN_NUMBER PageFrameIndex;
#endif

    CurrentProcess = PsGetCurrentProcess();

    DirectoryTableBase[0] = CurrentProcess->Pcb.DirectoryTableBase[0];
    DirectoryTableBase[1] = CurrentProcess->Pcb.DirectoryTableBase[1];

#if defined(_IA64_)
    ProcessToInitialize->Pcb.SessionMapInfo = CurrentProcess->Pcb.SessionMapInfo;
    ProcessToInitialize->Pcb.SessionParentBase = CurrentProcess->Pcb.SessionParentBase;
#endif

    ExInitializeFastMutex(&ProcessToInitialize->WorkingSetLock);

    ExInitializeFastMutex(&ProcessToInitialize->AddressCreationLock);

    KeInitializeSpinLock (&ProcessToInitialize->HyperSpaceLock);

    ASSERT (ProcessToInitialize->VadRoot == NULL);

    ProcessToInitialize->Vm.WorkingSetSize = CurrentProcess->Vm.WorkingSetSize;
    ProcessToInitialize->Vm.VmWorkingSetList = MmWorkingSetList;

    KeQuerySystemTime(&ProcessToInitialize->Vm.LastTrimTime);

#if defined (_X86PAE_)
    if (MiSystemPaeVa.PteEntry[0].u.Long == 0) {

        PageFrameIndex = (((PKPROCESS)CurrentProcess)->DirectoryTableBase[0] >> PAGE_SHIFT);

        PdeBase = (PMMPTE)MiMapSinglePage (NULL,
                                           PageFrameIndex,
                                           MmCached,
                                           HighPagePriority);

        if (PdeBase == NULL) {
              KeBugCheckEx (MEMORY_MANAGEMENT,
                            0x3452,
                            (ULONG_PTR)CurrentProcess,
                            (ULONG_PTR)ProcessToInitialize,
                            1);
        }

        for (i = 0; i < PD_PER_SYSTEM; i += 1) {
            MiSystemPaeVa.PteEntry[i] = *PdeBase;
            PdeBase += 1;
        }

        PdeBase -= PD_PER_SYSTEM;
        MiUnmapSinglePage (PdeBase);
    }
#endif

}

NTSTATUS
MmInitializeHandBuiltProcess2 (
    IN PEPROCESS ProcessToInitialize
    )

/*++

Routine Description:

    This routine initializes the shared user VAD.  This only needs to be done
    for x86 when booted /3GB because on all other systems, the shared user
    address is located above the highest user address.  For x86 /3GB, this
    VAD must be allocated so that other random VAD allocations do not overlap
    this area which would cause the mapping to receive the wrong data.

Arguments:

    ProcessToInitialize - Supplies the process that needs initialization.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  APCs Disabled.

--*/
{
#if defined(_X86_) && defined(MM_SHARED_USER_DATA_VA)

    PMMVAD VadShare;
    NTSTATUS Status;

    Status = STATUS_SUCCESS;

    //
    // Allocate a VAD to map the shared memory page. If a VAD cannot be
    // allocated, then detach from the target process and return a failure
    // status.  This VAD is marked as not deletable.
    //

    if ((MmVirtualBias != 0) &&
        (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA)) {

        KeAttachProcess (&ProcessToInitialize->Pcb);

        VadShare = MiAllocateVad (MM_SHARED_USER_DATA_VA,
                                  MM_SHARED_USER_DATA_VA,
                                  FALSE);

        //
        // Insert the VAD.
        //
        // N.B. No failure can occur since there is no commit charge.
        //

        if (VadShare != NULL) {
            Status = MiInsertVad (VadShare);
            ASSERT (NT_SUCCESS(Status));
        }
        else {
            Status = STATUS_NO_MEMORY;
        }

        KeDetachProcess ();
    }

    return Status;
#else

    UNREFERENCED_PARAMETER (ProcessToInitialize);

    return STATUS_SUCCESS;

#endif
}


VOID
MmDeleteProcessAddressSpace (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine deletes a process's Page Directory and working set page.

Arguments:

    Process - Supplies a pointer to the deleted process.

Return Value:

    None.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PEPROCESS CurrentProcess;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER VadBitMapPage;
    PFN_NUMBER PageFrameIndex2;
#if (_MI_PAGING_LEVELS >= 4)
    PFN_NUMBER PageFrameIndex3;
    PMMPTE ExtendedPageDirectoryParent;
    PMMPTE PointerPxe;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PageDirectoryParent;
    PMMPTE PointerPpe;
#endif
#if defined (_X86PAE_)
    ULONG i;
    PFN_NUMBER HyperPage2;
    PPAE_ENTRY PaeVa;

    PaeVa = (PPAE_ENTRY) Process->PaeTop;
#endif

    CurrentProcess = PsGetCurrentProcess ();

    //
    // Return commitment.
    //

    MiReturnCommitment (MM_PROCESS_COMMIT_CHARGE);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROCESS_DELETE, MM_PROCESS_COMMIT_CHARGE);
    ASSERT (Process->CommitCharge == 0);

    //
    // Remove the working set list page from the deleted process.
    //

    Pfn1 = MI_PFN_ELEMENT (Process->WorkingSetPage);

    LOCK_PFN (OldIrql);
    MmProcessCommit -= MM_PROCESS_COMMIT_CHARGE;

    if (Process->AddressSpaceInitialized == 2) {

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (Process->WorkingSetPage);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

        //
        // Map the hyper space page table page from the deleted process
        // so the vad bit map (and second hyperspace page) can be captured.
        //

        PageFrameIndex = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (Process);

        PointerPte = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                         PageFrameIndex);

#if defined (_X86PAE_)
        PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPte);
#endif

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + MiGetPteOffset(VAD_BITMAP_SPACE));

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPte);

        //
        // Remove the VAD bitmap page.
        //

        Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (VadBitMapPage);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

        //
        // Remove the first hyper space page table page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (PageFrameIndex);
        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

#if defined (_X86PAE_)

        //
        // Remove the second hyper space page table page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex2);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (PageFrameIndex2);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

        //
        // Remove the page directory pages.
        //

        PointerPte = (PMMPTE) PaeVa;
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            MI_SET_PFN_DELETED (Pfn1);

            MiDecrementShareAndValidCount (PageFrameIndex);
            MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);

            ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));
            PointerPte += 1;
        }
#endif

        //
        // Remove the top level page directory page.
        //

        PageFrameIndex = MI_GET_DIRECTORY_FRAME_FROM_PROCESS(Process);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Get a pointer to the top-level page directory parent page via
        // its KSEG0 address.
        //

#if (_MI_PAGING_LEVELS >= 4)

        ExtendedPageDirectoryParent = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                             CurrentProcess,
                                                             PageFrameIndex);

        //
        // Remove the hyper space page directory parent page
        // from the deleted process.
        //

        PointerPxe = &ExtendedPageDirectoryParent[MiGetPxeOffset(HYPER_SPACE)];
        PageFrameIndex3 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPxe);
        ASSERT (MI_PFN_ELEMENT(PageFrameIndex3)->u4.PteFrame == PageFrameIndex);

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, ExtendedPageDirectoryParent);

        PageDirectoryParent = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                             CurrentProcess,
                                                             PageFrameIndex3);

#else
        PageDirectoryParent = KSEG_ADDRESS (PageFrameIndex);
#endif

        //
        // Remove the hyper space page directory page from the deleted process.
        //

        PointerPpe = &PageDirectoryParent[MiGetPpeOffset(HYPER_SPACE)];
        PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPpe);

#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);
#endif

        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex2);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (PageFrameIndex2);
        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

#if (_MI_PAGING_LEVELS >= 4)
        Pfn1 = MI_PFN_ELEMENT(PageFrameIndex3);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MiDecrementShareCountOnly (PageFrameIndex3);
        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));
#endif
#endif

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        MiDecrementShareAndValidCount (PageFrameIndex);

        MiDecrementShareCountOnly (PageFrameIndex);

        ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) || (Pfn1->u3.e1.WriteInProgress));

    }
    else {

        //
        // Process initialization never completed, just return the pages
        // to the free list.
        //

        MiInsertPageInFreeList (Process->WorkingSetPage);

        PageFrameIndex = MI_GET_DIRECTORY_FRAME_FROM_PROCESS (Process);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Get a pointer to the top-level page directory parent page via
        // its KSEG0 address.
        //

        PageDirectoryParent = KSEG_ADDRESS (PageFrameIndex);

#if (_MI_PAGING_LEVELS >= 4)
        PageDirectoryParent = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                       CurrentProcess,
                                                       PageFrameIndex);

        PageFrameIndex3 = MI_GET_PAGE_FRAME_FROM_PTE (&PageDirectoryParent[MiGetPxeOffset(HYPER_SPACE)]);

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);

        PageDirectoryParent = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                       CurrentProcess,
                                                       PageFrameIndex3);
#endif

        PointerPpe = &PageDirectoryParent[MiGetPpeOffset(HYPER_SPACE)];
        PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE(PointerPpe);

#if (_MI_PAGING_LEVELS >= 4)
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParent);
#endif

        MiInsertPageInFreeList (PageFrameIndex2);

#if (_MI_PAGING_LEVELS >= 4)
        MiInsertPageInFreeList (PageFrameIndex3);
#endif
#endif

        PageFrameIndex2 = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (Process);

        PointerPte = (PMMPTE)MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                         PageFrameIndex2);

#if defined (_X86PAE_)
        HyperPage2 = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
#endif

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte + MiGetPteOffset(VAD_BITMAP_SPACE));

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PointerPte);

        //
        // Free the VAD bitmap page.
        //

        MiInsertPageInFreeList (VadBitMapPage);

        //
        // Free the first hyper space page table page.
        //

        MiInsertPageInFreeList (PageFrameIndex2);

#if defined (_X86PAE_)
        MiInsertPageInFreeList (HyperPage2);

        PointerPte = (PMMPTE) PaeVa;
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            MiInsertPageInFreeList (PageFrameIndex2);
            PointerPte += 1;
        }
#endif

        //
        // Free the topmost page directory page.
        //

        MiInsertPageInFreeList (PageFrameIndex);
    }

    MmResidentAvailablePages += MM_PROCESS_CREATE_CHARGE;
    MM_BUMP_COUNTER(7, MM_PROCESS_CREATE_CHARGE);

    UNLOCK_PFN (OldIrql);

#if defined (_X86PAE_)

    //
    // Free the page directory page pointers.
    //

    MiPaeFree (PaeVa);

#endif

    if (Process->Session != NULL) {

        //
        // The Terminal Server session space data page and mapping PTE can only
        // be freed when the last process in the session is deleted.  This is
        // because IA64 maps session space into region 1 and exited processes
        // maintain their session space mapping as attaches may occur even
        // after process exit that reference win32k, etc.  Since the region 1
        // mapping is being inserted into region registers during swap context,
        // these mappings cannot be torn down until the very last deletion
        // occurs.
        //

        MiReleaseProcessReferenceToSessionDataPage (Process->Session);
    }

    //
    // Check to see if the paging files should be contracted.
    //

    MiContractPagingFiles ();

    return;
}


VOID
MiDeletePteRange (
    IN PEPROCESS Process,
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL AddressSpaceDeletion
    )

/*++

Routine Description:

    This routine deletes a range of PTEs and when possible, the PDEs, PPEs and
    PXEs as well.  Commit is returned here for the hierarchies here.

Arguments:

    Process - Supplies the process whose PTEs are being deleted.

    PointerPte - Supplies the PTE to begin deleting at.

    LastPte - Supplies the PTE to stop deleting at (don't delete this one).
              -1 signifies keep going until a nonvalid PTE is found.

    AddressSpaceDeletion - Supplies TRUE if the address space is in the final
                           stages of deletion, FALSE otherwise.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PVOID TempVa;
    KIRQL OldIrql;
    MMPTE_FLUSH_LIST PteFlushList;
    PFN_NUMBER CommittedPages;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    LOGICAL Boundary;
    LOGICAL FinalPte;
    PMMPFN Pfn1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    if (PointerPte >= LastPte) {
        return;
    }

    CommittedPages = 0;
    PteFlushList.Count = 0;

    //
    // Initializing OldIrql is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;
    //
    // Final address space deletion is done with the PFN lock already held.
    //

    if (AddressSpaceDeletion == FALSE) {
        LOCK_PFN (OldIrql);
    }

#if (_MI_PAGING_LEVELS >= 3)
    PointerPpe = MiGetPdeAddress (PointerPte);
    PointerPde = MiGetPteAddress (PointerPte);

#if (_MI_PAGING_LEVELS >= 4)
    PointerPxe = MiGetPpeAddress (PointerPte);
    if ((PointerPxe->u.Hard.Valid == 1) &&
        (PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1) &&
        (PointerPte->u.Hard.Valid == 1))
#else
    if ((PointerPpe->u.Hard.Valid == 1) &&
        (PointerPde->u.Hard.Valid == 1) &&
        (PointerPte->u.Hard.Valid == 1))
#endif
    {

        do {

            ASSERT (PointerPte->u.Hard.Valid == 1);

            TempVa = MiGetVirtualAddressMappedByPte(PointerPte);
            MiDeletePte (PointerPte,
                         TempVa,
                         AddressSpaceDeletion,
                         Process,
                         NULL,
                         &PteFlushList);

            CommittedPages += 1;
            PointerPte += 1;
            Process->NumberOfPrivatePages += 1;

            //
            // If all the entries have been removed from the previous page
            // table page, delete the page table page itself.  Likewise with
            // the page directory page.
            //

            if (MiIsPteOnPdeBoundary(PointerPte)) {
                Boundary = TRUE;
            }
            else {
                Boundary = FALSE;
            }

            if ((PointerPte >= LastPte) ||
#if (_MI_PAGING_LEVELS >= 4)
                ((MiGetPpeAddress(PointerPte))->u.Hard.Valid == 0) ||
#endif
                ((MiGetPdeAddress(PointerPte))->u.Hard.Valid == 0) ||
                ((MiGetPteAddress(PointerPte))->u.Hard.Valid == 0) ||
                (PointerPte->u.Hard.Valid == 0)) {
                FinalPte = TRUE;
            }
            else {
                FinalPte = FALSE;
            }

            if ((Boundary == TRUE) || (FinalPte == TRUE)) {

                MiFlushPteList (&PteFlushList, FALSE, ZeroPte);

                PointerPde = MiGetPteAddress (PointerPte - 1);

                ASSERT (PointerPde->u.Hard.Valid == 1);

                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPde));

                if (Pfn1->u2.ShareCount == 1 && Pfn1->u3.e2.ReferenceCount == 1)
                {
                    MiDeletePte (PointerPde,
                                 PointerPte - 1,
                                 AddressSpaceDeletion,
                                 Process,
                                 NULL,
                                 NULL);

                    CommittedPages += 1;
                    Process->NumberOfPrivatePages += 1;

                    if ((FinalPte == TRUE) || (MiIsPteOnPpeBoundary(PointerPte))) {

                        PointerPpe = MiGetPteAddress (PointerPde);

                        ASSERT (PointerPpe->u.Hard.Valid == 1);

                        Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPpe));

                        if (Pfn1->u2.ShareCount == 1 && Pfn1->u3.e2.ReferenceCount == 1)
                        {
                            MiDeletePte (PointerPpe,
                                         PointerPde,
                                         AddressSpaceDeletion,
                                         Process,
                                         NULL,
                                         NULL);

                            CommittedPages += 1;
                            Process->NumberOfPrivatePages += 1;
#if (_MI_PAGING_LEVELS >= 4)
                            if ((FinalPte == TRUE) || (MiIsPteOnPxeBoundary(PointerPte))) {

                                PointerPxe = MiGetPdeAddress (PointerPde);

                                ASSERT (PointerPxe->u.Hard.Valid == 1);

                                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPxe));

                                if (Pfn1->u2.ShareCount == 1 && Pfn1->u3.e2.ReferenceCount == 1)
                                {
                                    MiDeletePte (PointerPxe,
                                                 PointerPpe,
                                                 AddressSpaceDeletion,
                                                 Process,
                                                 NULL,
                                                 NULL);
                                    CommittedPages += 1;
                                    Process->NumberOfPrivatePages += 1;
                                }
                            }
#endif
                        }
                    }
                }
                if (FinalPte == TRUE) {
                    break;
                }
            }
            ASSERT (PointerPte->u.Hard.Valid == 1);
        } while (TRUE);
    }
#else
    while (PointerPte->u.Hard.Valid) {
        TempVa = MiGetVirtualAddressMappedByPte(PointerPte);
        MiDeletePte (PointerPte,
                     TempVa,
                     AddressSpaceDeletion,
                     Process,
                     NULL,
                     &PteFlushList);

        CommittedPages += 1;
        Process->NumberOfPrivatePages += 1;
        PointerPte += 1;
        if (PointerPte >= LastPte) {
            break;
        }
    }
#endif

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, FALSE, ZeroPte);
    }

    if (AddressSpaceDeletion == FALSE) {
        UNLOCK_PFN (OldIrql);
    }

    if (CommittedPages != 0) {
        MiReturnCommitment (CommittedPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PTE_RANGE, CommittedPages);
    }

    return;
}


VOID
MmCleanProcessAddressSpace (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine cleans an address space by deleting all the user and
    pagable portions of the address space.  At the completion of this
    routine, no page faults may occur within the process.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    PKTHREAD CurrentThread;
    PMMVAD Vad;
    KEVENT Event;
    KIRQL OldIrql;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    LONG AboveWsMin;
    ULONG NumberOfCommittedPageTables;
#if defined (_WIN64)
    PWOW64_PROCESS TempWow64;
#endif

    if ((Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) ||
        (Process->AddressSpaceInitialized == 0)) {

        //
        // This process's address space has already been deleted.  However,
        // this process can still have a session space.  Get rid of it now.
        //

        MiSessionRemoveProcess ();

        return;
    }

    if (Process->AddressSpaceInitialized == 1) {

        //
        // The process has been created but not fully initialized.
        // Return partial resources now.
        //

        LOCK_PFN (OldIrql);
        MmResidentAvailablePages += (Process->Vm.MinimumWorkingSetSize -
                                                    MM_PROCESS_CREATE_CHARGE);

        MM_BUMP_COUNTER(41, Process->Vm.MinimumWorkingSetSize -
                                                    MM_PROCESS_CREATE_CHARGE);
        UNLOCK_PFN (OldIrql);

        //
        // Clear the AddressSpaceInitialized flag so we don't over-return
        // resident available as this routine can be called more than once
        // for the same process.
        //

        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_ADDRESS_SPACE1);
        ASSERT (Process->AddressSpaceInitialized == 0);

        //
        // This process's address space has already been deleted.  However,
        // this process can still have a session space.  Get rid of it now.
        //

        MiSessionRemoveProcess ();

        return;
    }

    //
    // If working set expansion for this process is allowed, disable
    // it and remove the process from expanded process list if it
    // is on it.
    //

    LOCK_EXPANSION (OldIrql);

    if (Process->Vm.Flags.BeingTrimmed) {

        //
        // Initialize an event and put the event address
        // in the blink field.  When the trimming is complete,
        // this event will be set.
        //

        KeInitializeEvent(&Event, NotificationEvent, FALSE);

        Process->Vm.WorkingSetExpansionLinks.Blink = (PLIST_ENTRY)&Event;

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_EXPANSION_AND_THEN_WAIT (OldIrql);

        KeWaitForSingleObject(&Event,
                              WrVirtualMemory,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);

        KeLeaveCriticalRegionThread (CurrentThread);
    }
    else if (Process->Vm.WorkingSetExpansionLinks.Flink == MM_NO_WS_EXPANSION) {

        //
        // No trimming is in progress and no expansion allowed, so this cannot
        // be on any lists.
        //

        ASSERT (Process->Vm.WorkingSetExpansionLinks.Blink != MM_WS_EXPANSION_IN_PROGRESS);

        UNLOCK_EXPANSION (OldIrql);
    }
    else {

        RemoveEntryList (&Process->Vm.WorkingSetExpansionLinks);

        //
        // Disable expansion.
        //

        Process->Vm.WorkingSetExpansionLinks.Flink = MM_NO_WS_EXPANSION;

        UNLOCK_EXPANSION (OldIrql);
    }

    MiSessionRemoveProcess ();

    PointerPte = MiGetPteAddress (&MmWsle[MM_MAXIMUM_WORKING_SET]) + 1;

    //
    // Delete all the user owned pagable virtual addresses in the process.
    //

    //
    // Both mutexes must be owned to synchronize with the bit setting and
    // clearing of VM_DELETED.   This is because various callers acquire
    // only one of them (either one) before checking.
    //

    LOCK_WS_AND_ADDRESS_SPACE (Process);

    PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_VM_DELETED);

    //
    // Delete all the valid user mode addresses from the working set
    // list.  At this point NO page faults are allowed on user space
    // addresses.  Faults are allowed on page tables for user space, which
    // requires that we keep the working set structure consistent until we
    // finally take it all down.
    //

    MiDeleteAddressesInWorkingSet (Process);

    //
    // Remove hash table pages, if any.  This is the first time we do this
    // during the deletion path, but we need to do it again before we finish
    // because we may fault in some page tables during the VAD clearing.  We
    // could have maintained the hash table validity during the WorkingSet
    // deletion above in order to avoid freeing the hash table twice, but since
    // we're just deleting it all anyway, it's faster to do it this way.  Note
    // that if we don't do this or maintain the validity, we can trap later
    // in MiGrowWsleHash.
    //

    LastPte = MiGetPteAddress (MmWorkingSetList->HighestPermittedHashAddress);

    MiDeletePteRange (Process, PointerPte, LastPte, FALSE);

    //
    // Clear the hash fields as a fault may occur below on the page table
    // pages during VAD clearing and resolution of the fault may result in
    // adding a hash table.  Thus these fields must be consistent with the
    // clearing just done above.
    //

    MmWorkingSetList->HashTableSize = 0;
    MmWorkingSetList->HashTable = NULL;

    //
    // Delete the virtual address descriptors and dereference any
    // section objects.
    //

    Vad = Process->VadRoot;

    while (Vad != (PMMVAD)NULL) {

        MiRemoveVad (Vad);

        //
        // If the system has been biased to an alternate base address to
        // allow 3gb of user address space, then check if the current VAD
        // describes the shared memory page.
        //

#if defined(_X86_) && defined(MM_SHARED_USER_DATA_VA)

        if (MmVirtualBias != 0) {

            //
            // If the VAD describes the shared memory page, then free the
            // VAD and continue with the next entry.
            //

            if (Vad->StartingVpn == MI_VA_TO_VPN (MM_SHARED_USER_DATA_VA)) {
                ASSERT (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA);
                goto LoopEnd;
            }
        }
#endif

        if (((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) ||
            (Vad->u.VadFlags.PhysicalMapping == 1)) {

            //
            // This VAD represents a mapped view or a driver-mapped physical
            // view - delete the view and perform any section related cleanup
            // operations.
            //

            MiRemoveMappedView (Process, Vad);

        }
        else {

            if (Vad->u.VadFlags.UserPhysicalPages == 1) {

                //
                // Free all the physical pages that this VAD might be mapping.
                // Since only the AWE lock synchronizes the remap API, carefully
                // remove this VAD from the list first.
                //

                MiAweViewRemover (Process, Vad);

                MiRemoveUserPhysicalPagesVad ((PMMVAD_SHORT)Vad);

                MiDeletePageTablesForPhysicalRange (
                        MI_VPN_TO_VA (Vad->StartingVpn),
                        MI_VPN_TO_VA_ENDING (Vad->EndingVpn));
            }
            else {

                if (Vad->u.VadFlags.WriteWatch == 1) {
                    MiPhysicalViewRemover (Process, Vad);
                }

                LOCK_PFN (OldIrql);

                //
                // Don't specify address space deletion as TRUE as
                // the working set must be consistent as page faults may
                // be taken during clone removal, protoPTE lookup, etc.
                //

                MiDeleteVirtualAddresses (MI_VPN_TO_VA (Vad->StartingVpn),
                                          MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                          FALSE,
                                          Vad);

                UNLOCK_PFN (OldIrql);
            }
        }

#if defined(_X86_) && defined(MM_SHARED_USER_DATA_VA)
LoopEnd:
#endif

        ExFreePool (Vad);
        Vad = Process->VadRoot;
    }

    ASSERT (Process->NumberOfVads == 0);
    ASSERT (IsListEmpty (&Process->PhysicalVadList) != 0);

    MiCleanPhysicalProcessPages (Process);

    //
    // Delete the shared data page, if any.
    //

    LOCK_PFN (OldIrql);

#if defined(MM_SHARED_USER_DATA_VA)
#if defined (_X86_)
    if (MmHighestUserAddress > (PVOID) MM_SHARED_USER_DATA_VA) {
#endif
        MiDeleteVirtualAddresses ((PVOID) MM_SHARED_USER_DATA_VA,
                                  (PVOID) MM_SHARED_USER_DATA_VA,
                                  FALSE,
                                  NULL);
#if defined (_X86_)
    }
#endif
#endif

    //
    // Delete the system portion of the address space.
    // Only now is it safe to specify TRUE to MiDelete because now that the
    // VADs have been deleted we can no longer fault on user space pages.
    //

    Process->Vm.Flags.AddressSpaceBeingDeleted = 1;

    //
    // Adjust the count of pages above working set maximum.  This
    // must be done here because the working set list is not
    // updated during this deletion.
    //

    AboveWsMin = (LONG)Process->Vm.WorkingSetSize - (LONG)Process->Vm.MinimumWorkingSetSize;
    if (AboveWsMin > 0) {
        MmPagesAboveWsMinimum -= AboveWsMin;
    }

    UNLOCK_PFN (OldIrql);

    //
    // Return commitment for page table pages.
    //

    NumberOfCommittedPageTables = MmWorkingSetList->NumberOfCommittedPageTables;

#if (_MI_PAGING_LEVELS >= 3)
    NumberOfCommittedPageTables += MmWorkingSetList->NumberOfCommittedPageDirectories;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    NumberOfCommittedPageTables += MmWorkingSetList->NumberOfCommittedPageDirectoryParents;
#endif

    MiReturnCommitment (NumberOfCommittedPageTables);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PROCESS_CLEAN_PAGETABLES,
                     NumberOfCommittedPageTables);

    if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
        PsChangeJobMemoryUsage(-(SSIZE_T)NumberOfCommittedPageTables);
    }
    Process->CommitCharge -= NumberOfCommittedPageTables;
    PsReturnProcessPageFileQuota (Process, NumberOfCommittedPageTables);


    MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - NumberOfCommittedPageTables);

#if (_MI_PAGING_LEVELS >= 3)
    if (MmWorkingSetList->CommittedPageTables != NULL) {
        ExFreePool (MmWorkingSetList->CommittedPageTables);
        MmWorkingSetList->CommittedPageTables = NULL;
    }
#endif

#if (_MI_PAGING_LEVELS >= 4)
    if (MmWorkingSetList->CommittedPageDirectories != NULL) {
        ExFreePool (MmWorkingSetList->CommittedPageDirectories);
        MmWorkingSetList->CommittedPageDirectories = NULL;
    }
#endif

    //
    // Check to make sure all the clone descriptors went away.
    //

    ASSERT (Process->CloneRoot == (PMMCLONE_DESCRIPTOR)NULL);

    if (Process->NumberOfLockedPages != 0) {
        if (Process->LockedPagesList) {

            PLIST_ENTRY NextEntry;
            PLOCK_TRACKER Tracker;
            PLOCK_HEADER LockedPagesHeader;

            LockedPagesHeader = (PLOCK_HEADER)Process->LockedPagesList;
            if ((LockedPagesHeader->Count != 0) && (MiTrackingAborted == FALSE)) {
                ASSERT (IsListEmpty (&LockedPagesHeader->ListHead) == 0);
                NextEntry = LockedPagesHeader->ListHead.Flink;

                Tracker = CONTAINING_RECORD (NextEntry,
                                             LOCK_TRACKER,
                                             ListEntry);

                KeBugCheckEx (DRIVER_LEFT_LOCKED_PAGES_IN_PROCESS,
                              (ULONG_PTR)Tracker->CallingAddress,
                              (ULONG_PTR)Tracker->CallersCaller,
                              (ULONG_PTR)Tracker->Mdl,
                              Process->NumberOfLockedPages);
            }
        }

        KeBugCheckEx (PROCESS_HAS_LOCKED_PAGES,
                      0,
                      (ULONG_PTR)Process,
                      Process->NumberOfLockedPages,
                      (ULONG_PTR)Process->LockedPagesList);
        return;
    }

    if (Process->LockedPagesList) {
        ASSERT (MmTrackLockedPages == TRUE);
        ExFreePool (Process->LockedPagesList);
        Process->LockedPagesList = NULL;
    }

#if DBG
    if ((Process->NumberOfPrivatePages != 0) && (MmDebug & MM_DBG_PRIVATE_PAGES)) {
        DbgPrint("MM: Process contains private pages %ld\n",
               Process->NumberOfPrivatePages);
        DbgBreakPoint();
    }
#endif


#if defined(_WIN64)

    //
    // Delete the WowProcess structure.
    //

    if (Process->Wow64Process != NULL) {
#if defined(_MIALT4K_)
        MiDeleteAlternateTable(Process);
#endif
        TempWow64 = Process->Wow64Process;
        Process->Wow64Process = NULL;
        ExFreePool (TempWow64);
    }
#endif

    //
    // Remove the working set list pages (except for the first one).
    // These pages are not removed because DPCs could still occur within
    // the address space.  In a DPC, nonpagedpool could be allocated
    // which could require removing a page from the standby list, requiring
    // hyperspace to map the previous PTE.
    //

    PointerPte = MiGetPteAddress (MmWorkingSetList) + 1;

    LOCK_PFN (OldIrql);

    MiDeletePteRange (Process, PointerPte, (PMMPTE)-1, TRUE);

    //
    // Remove hash table pages, if any.  Yes, we've already done this once
    // during the deletion path, but we need to do it again because we may
    // have faulted in some page tables during the VAD clearing.
    //

    PointerPte = MiGetPteAddress (&MmWsle[MM_MAXIMUM_WORKING_SET]) + 1;

    ASSERT (PointerPte < LastPte);

    MiDeletePteRange (Process, PointerPte, LastPte, TRUE);

    //
    // Update the count of available resident pages.
    //

    ASSERT (Process->Vm.MinimumWorkingSetSize >= MM_PROCESS_CREATE_CHARGE);
    MmResidentAvailablePages += Process->Vm.MinimumWorkingSetSize -
                                                    MM_PROCESS_CREATE_CHARGE;
    MM_BUMP_COUNTER(8, Process->Vm.MinimumWorkingSetSize -
                                                    MM_PROCESS_CREATE_CHARGE);
    ASSERT (Process->Vm.WorkingSetExpansionLinks.Flink == MM_NO_WS_EXPANSION);
    UNLOCK_PFN (OldIrql);

    UNLOCK_WS_AND_ADDRESS_SPACE (Process);
    return;
}

#if !defined(_IA64_)
#define KERNEL_BSTORE_SIZE          0
#define KERNEL_LARGE_BSTORE_SIZE    0
#define KERNEL_LARGE_BSTORE_COMMIT  0
#define KERNEL_STACK_GUARD_PAGES    1
#else
#define KERNEL_STACK_GUARD_PAGES    2       // One for stack, one for RSE.
#endif


PVOID
MmCreateKernelStack (
    IN BOOLEAN LargeStack,
    IN UCHAR PreferredNode
    )

/*++

Routine Description:

    This routine allocates a kernel stack and a no-access page within
    the non-pagable portion of the system address space.

Arguments:

    LargeStack - Supplies the value TRUE if a large stack should be
                 created.  FALSE if a small stack is to be created.

    PreferredNode - Supplies the preferred node to use for the physical
                    page allocations.  MP/NUMA systems only.

Return Value:

    Returns a pointer to the base of the kernel stack.  Note, that the
    base address points to the guard page, so space must be allocated
    on the stack before accessing the stack.

    If a kernel stack cannot be created, the value NULL is returned.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PMMPTE BasePte;
    MMPTE TempPte;
    PFN_NUMBER NumberOfPages;
    ULONG NumberOfPtes;
    ULONG ChargedPtes;
    ULONG RequestedPtes;
    ULONG NumberOfBackingStorePtes;
    PFN_NUMBER PageFrameIndex;
    ULONG i;
    PVOID StackVa;
    KIRQL OldIrql;
    PSLIST_HEADER DeadStackList;

    if (!LargeStack) {

        //
        // Check to see if any unused stacks are available.
        //

#if defined(MI_MULTINODE)
        DeadStackList = &KeNodeBlock[PreferredNode]->DeadStackList;
#else
        UNREFERENCED_PARAMETER (PreferredNode);
        DeadStackList = &MmDeadStackSListHead;
#endif

        if (ExQueryDepthSList (DeadStackList) != 0) {

            Pfn1 = (PMMPFN) InterlockedPopEntrySList (DeadStackList);

            if (Pfn1 != NULL) {
                PointerPte = Pfn1->PteAddress;
                PointerPte += 1;
                StackVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);
                return StackVa;
            }
        }
        NumberOfPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE);
        NumberOfBackingStorePtes = BYTES_TO_PAGES (KERNEL_BSTORE_SIZE);
        NumberOfPages = NumberOfPtes + NumberOfBackingStorePtes;
    }
    else {
        NumberOfPtes = BYTES_TO_PAGES (MI_LARGE_STACK_SIZE);
        NumberOfBackingStorePtes = BYTES_TO_PAGES (KERNEL_LARGE_BSTORE_SIZE);
        NumberOfPages = BYTES_TO_PAGES (KERNEL_LARGE_STACK_COMMIT
                                        + KERNEL_LARGE_BSTORE_COMMIT);
    }

    ChargedPtes = NumberOfPtes + NumberOfBackingStorePtes;

    //
    // Charge commitment for the page file space for the kernel stack.
    //

    if (MiChargeCommitment (ChargedPtes, NULL) == FALSE) {

        //
        // Commitment exceeded, return NULL, indicating no kernel
        // stacks are available.
        //

        return NULL;
    }

    //
    // Obtain enough pages to contain the stack plus a guard page from
    // the system PTE pool.  The system PTE pool contains nonpaged PTEs
    // which are currently empty.
    //
    // Note for IA64, the PTE allocation is divided between kernel stack
    // and RSE space.  The stack grows downward and the RSE grows upward.
    //

    RequestedPtes = ChargedPtes + KERNEL_STACK_GUARD_PAGES;

    BasePte = MiReserveSystemPtes (RequestedPtes, SystemPteSpace);

    if (BasePte == NULL) {
        MiReturnCommitment (ChargedPtes);
        return NULL;
    }

    PointerPte = BasePte;

    StackVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte + NumberOfPtes + 1);

    if (LargeStack) {
        PointerPte += BYTES_TO_PAGES (MI_LARGE_STACK_SIZE - KERNEL_LARGE_STACK_COMMIT);
    }

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        MiReleaseSystemPtes (BasePte, RequestedPtes, SystemPteSpace);
        MiReturnCommitment (ChargedPtes);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_KERNEL_STACK_CREATE, ChargedPtes);

    MmResidentAvailablePages -= NumberOfPages;
    MM_BUMP_COUNTER(9, NumberOfPages);

    for (i = 0; i < NumberOfPages; i += 1) {
        PointerPte += 1;
        ASSERT (PointerPte->u.Hard.Valid == 0);
        MiEnsureAvailablePageOrWait (NULL, NULL);
        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_NODE (PreferredNode));

        PointerPte->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

        PointerPte->u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MiInitializePfn (PageFrameIndex, PointerPte, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           PointerPte);
        MI_SET_PTE_DIRTY (TempPte);

        MI_WRITE_VALID_PTE (PointerPte, TempPte);
    }
    MmProcessCommit += ChargedPtes;
    MmKernelStackResident += NumberOfPages;
    MmLargeStacks += LargeStack;
    MmSmallStacks += !LargeStack;
    MmKernelStackPages += RequestedPtes;

    UNLOCK_PFN (OldIrql);

    return StackVa;
}

VOID
MmDeleteKernelStack (
    IN PVOID PointerKernelStack,
    IN BOOLEAN LargeStack
    )

/*++

Routine Description:

    This routine deletes a kernel stack and the no-access page within
    the non-pagable portion of the system address space.

Arguments:

    PointerKernelStack - Supplies a pointer to the base of the kernel stack.

    LargeStack - Supplies the value TRUE if a large stack is being deleted.
                 FALSE if a small stack is to be deleted.

Return Value:

    None.

Environment:

    Kernel mode.  APCs Disabled.

--*/

{
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER NumberOfPages;
    ULONG NumberOfPtes;
    ULONG NumberOfStackPtes;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    ULONG i;
    KIRQL OldIrql;
    MMPTE PteContents;
    PSLIST_HEADER DeadStackList;

    PointerPte = MiGetPteAddress (PointerKernelStack);

    //
    // PointerPte points to the guard page, point to the previous
    // page before removing physical pages.
    //

    PointerPte -= 1;

    //
    // Check to see if the stack page should be placed on the dead
    // kernel stack page list.  The dead kernel stack list is a
    // singly linked list of kernel stacks from terminated threads.
    // The stacks are saved on a linked list up to a maximum number
    // to avoid the overhead of flushing the entire TB on all processors
    // everytime a thread terminates.  The TB on all processors must
    // be flushed as kernel stacks reside in the non paged system part
    // of the address space.
    //

    if (!LargeStack) {

#if defined(MI_MULTINODE)

        //
        // Scan the physical page frames and only place this stack on the
        // dead stack list if all the pages are on the same node.  Realize
        // if this push goes cross node it may make the interlocked instruction
        // slightly more expensive, but worth it all things considered.
        //

        ULONG NodeNumber;

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        NodeNumber = Pfn1->u3.e1.PageColor;

        DeadStackList = &KeNodeBlock[NodeNumber]->DeadStackList;

#else

        DeadStackList = &MmDeadStackSListHead;

#endif

        NumberOfPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE + KERNEL_BSTORE_SIZE);

        if (ExQueryDepthSList (DeadStackList) < MmMaximumDeadKernelStacks) {

#if defined(MI_MULTINODE)

            //
            // The node could use some more dead stacks - but first make sure
            // all the physical pages are from the same node in a multinode
            // system.
            //

            if (KeNumberNodes > 1) {

                ULONG CheckPtes;

                //
                // Note IA64 RSE space is not included for checking purposes
                // since it's never trimmed for small stacks.
                //

                CheckPtes = BYTES_TO_PAGES (KERNEL_STACK_SIZE);

                PointerPte -= 1;
                for (i = 1; i < CheckPtes; i += 1) {

                    PteContents = *PointerPte;

                    if (PteContents.u.Hard.Valid == 0) {
                        break;
                    }

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                    if (NodeNumber != Pfn1->u3.e1.PageColor) {
                        PointerPte += i;
                        goto FreeStack;
                    }
                    PointerPte -= 1;
                }
                PointerPte += CheckPtes;
            }
#endif

            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

            InterlockedPushEntrySList (DeadStackList, &Pfn1->u1.NextStackPfn);

            PERFINFO_DELETE_STACK(PointerPte, NumberOfPtes);
            return;
        }
    }
    else {
        NumberOfPtes = BYTES_TO_PAGES (MI_LARGE_STACK_SIZE + KERNEL_LARGE_BSTORE_SIZE);
    }


#if defined(MI_MULTINODE)
FreeStack:
#endif

#if defined(_IA64_)

    //
    // Note on IA64, PointerKernelStack points to the center of the stack space,
    // the size of kernel backing store needs to be added to get the
    // top of the stack space.
    //

    if (LargeStack) {
        PointerPte = MiGetPteAddress (
                  (PCHAR)PointerKernelStack + KERNEL_LARGE_BSTORE_SIZE);
    }
    else {
        PointerPte = MiGetPteAddress (
                  (PCHAR)PointerKernelStack + KERNEL_BSTORE_SIZE);
    }

    //
    // PointerPte points to the guard page, point to the previous
    // page before removing physical pages.
    //

    PointerPte -= 1;

#endif

    //
    // We have exceeded the limit of dead kernel stacks or this is a large
    // stack, delete this kernel stack.
    //

    NumberOfPages = 0;

    NumberOfStackPtes = NumberOfPtes + KERNEL_STACK_GUARD_PAGES;

    LOCK_PFN (OldIrql);

    for (i = 0; i < NumberOfPtes; i += 1) {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            PageTableFrameIndex = Pfn1->u4.PteFrame;
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Mark the page as deleted so it will be freed when the
            // reference count goes to zero.
            //

            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCountOnly (PageFrameIndex);
            NumberOfPages += 1;
        }
        PointerPte -= 1;
    }

    //
    // Now at the stack guard page, ensure it is still a guard page.
    //

    ASSERT (PointerPte->u.Hard.Valid == 0);

    //
    // Update the count of resident available pages.
    //

    MmResidentAvailablePages += NumberOfPages;
    MM_BUMP_COUNTER(10, NumberOfPages);

    MmKernelStackPages  -= NumberOfStackPtes;
    MmKernelStackResident -= NumberOfPages;
    MmProcessCommit -= NumberOfPtes;
    MmLargeStacks -= LargeStack;
    MmSmallStacks -= !LargeStack;

    UNLOCK_PFN (OldIrql);

    //
    // Return PTEs and commitment.
    //

    MiReleaseSystemPtes (PointerPte, NumberOfStackPtes, SystemPteSpace);

    MiReturnCommitment (NumberOfPtes);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_KERNEL_STACK_DELETE, NumberOfPtes);

    return;
}

#if defined(_IA64_)
ULONG MiStackGrowthFailures[2];
#else
ULONG MiStackGrowthFailures[1];
#endif


NTSTATUS
MmGrowKernelStack (
    IN PVOID CurrentStack
    )

/*++

Routine Description:

    This function attempts to grows the current thread's kernel stack
    such that there is always KERNEL_LARGE_STACK_COMMIT bytes below
    the current stack pointer.

Arguments:

    CurrentStack - Supplies a pointer to the current stack pointer.

Return Value:

    STATUS_SUCCESS is returned if the stack was grown.

    STATUS_STACK_OVERFLOW is returned if there was not enough space reserved
    for the commitment.

    STATUS_NO_MEMORY is returned if there was not enough physical memory
    in the system.

--*/

{
    PMMPTE NewLimit;
    PMMPTE StackLimit;
    PMMPTE EndStack;
    PETHREAD Thread;
    PFN_NUMBER NumberOfPages;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    MMPTE TempPte;

    Thread = PsGetCurrentThread ();
    ASSERT (((PCHAR)Thread->Tcb.StackBase - (PCHAR)Thread->Tcb.StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    StackLimit = MiGetPteAddress (Thread->Tcb.StackLimit);

    ASSERT (StackLimit->u.Hard.Valid == 1);

    NewLimit = MiGetPteAddress ((PVOID)((PUCHAR)CurrentStack -
                                                    KERNEL_LARGE_STACK_COMMIT));

    if (NewLimit == StackLimit) {
        return STATUS_SUCCESS;
    }

    //
    // If the new stack limit exceeds the reserved region for the kernel
    // stack, then return an error.
    //

    EndStack = MiGetPteAddress ((PVOID)((PUCHAR)Thread->Tcb.StackBase -
                                                    MI_LARGE_STACK_SIZE));

    if (NewLimit < EndStack) {

        //
        // Don't go into guard page.
        //

        MiStackGrowthFailures[0] += 1;

#if DBG
        DbgPrint ("MmGrowKernelStack failed: Thread %p %p %p\n",
                        Thread, NewLimit, EndStack);
#endif

        return STATUS_STACK_OVERFLOW;

    }

    //
    // Lock the PFN database and attempt to expand the kernel stack.
    //

    StackLimit -= 1;

    NumberOfPages = (PFN_NUMBER) (StackLimit - NewLimit + 1);

    LOCK_PFN (OldIrql);

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        return STATUS_NO_MEMORY;
    }

    //
    // Note MmResidentAvailablePages must be charged before calling
    // MiEnsureAvailablePageOrWait as it may release the PFN lock.
    //

    MmResidentAvailablePages -= NumberOfPages;
    MM_BUMP_COUNTER(11, NumberOfPages);

    while (StackLimit >= NewLimit) {

        ASSERT (StackLimit->u.Hard.Valid == 0);

        MiEnsureAvailablePageOrWait (NULL, NULL);
        PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (StackLimit));
        StackLimit->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

        StackLimit->u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MiInitializePfn (PageFrameIndex, StackLimit, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           StackLimit);

        MI_SET_PTE_DIRTY (TempPte);
        *StackLimit = TempPte;
        StackLimit -= 1;
    }

    MmKernelStackResident += NumberOfPages;
    UNLOCK_PFN (OldIrql);

#if DBG
    ASSERT (NewLimit->u.Hard.Valid == 1);
    if (NewLimit != EndStack) {
        ASSERT ((NewLimit - 1)->u.Hard.Valid == 0);
    }
#endif

    Thread->Tcb.StackLimit = MiGetVirtualAddressMappedByPte (NewLimit);

    PERFINFO_GROW_STACK(Thread);

    return STATUS_SUCCESS;
}

#if defined(_IA64_)


NTSTATUS
MmGrowKernelBackingStore (
    IN PVOID CurrentBackingStorePointer
    )

/*++

Routine Description:

    This function attempts to grows the backing store for the current thread's
    kernel stack such that there is always KERNEL_LARGE_STACK_COMMIT bytes
    above the current backing store pointer.

Arguments:

    CurrentBackingStorePointer - Supplies a pointer to the current backing
                                 store pointer for the active kernel stack.

Return Value:

    NTSTATUS.

--*/

{
    PMMPTE NewLimit;
    PMMPTE BstoreLimit;
    PMMPTE EndStack;
    PETHREAD Thread;
    PFN_NUMBER NumberOfPages;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    MMPTE TempPte;

    Thread = PsGetCurrentThread ();

    ASSERT (((PCHAR)Thread->Tcb.BStoreLimit - (PCHAR)Thread->Tcb.StackBase) <=
            (KERNEL_LARGE_BSTORE_SIZE + PAGE_SIZE));

    BstoreLimit = MiGetPteAddress ((PVOID)((PCHAR)Thread->Tcb.BStoreLimit - 1));

    ASSERT (BstoreLimit->u.Hard.Valid == 1);

    NewLimit = MiGetPteAddress ((PVOID)((PUCHAR)CurrentBackingStorePointer +
                                                 KERNEL_LARGE_BSTORE_COMMIT-1));

    if (NewLimit == BstoreLimit) {
        return STATUS_SUCCESS;
    }

    //
    // If the new stack limit exceeds the reserved region for the kernel
    // stack, then return an error.
    //

    EndStack = MiGetPteAddress ((PVOID)((PUCHAR)Thread->Tcb.StackBase +
                                                 KERNEL_LARGE_BSTORE_SIZE-1));

    if (NewLimit > EndStack) {

        //
        // Don't go into guard page.
        //

        MiStackGrowthFailures[1] += 1;

#if DBG
        DbgPrint ("MmGrowKernelBackingStore failed: Thread %p %p %p\n",
                        Thread, NewLimit, EndStack);
#endif

        return STATUS_STACK_OVERFLOW;

    }

    //
    // Lock the PFN database and attempt to expand the backing store.
    //

    BstoreLimit += 1;

    NumberOfPages = (PFN_NUMBER)(NewLimit - BstoreLimit + 1);

    LOCK_PFN (OldIrql);

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        return STATUS_NO_MEMORY;
    }

    //
    // Note MmResidentAvailablePages must be charged before calling
    // MiEnsureAvailablePageOrWait as it may release the PFN lock.
    //

    MmResidentAvailablePages -= NumberOfPages;
    MM_BUMP_COUNTER(2, NumberOfPages);

    while (BstoreLimit <= NewLimit) {

        ASSERT (BstoreLimit->u.Hard.Valid == 0);

        MiEnsureAvailablePageOrWait (NULL, NULL);
        PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (BstoreLimit));
        BstoreLimit->u.Long = MM_KERNEL_DEMAND_ZERO_PTE;

        BstoreLimit->u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MiInitializePfn (PageFrameIndex, BstoreLimit, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           BstoreLimit);

        MI_SET_PTE_DIRTY (TempPte);
        *BstoreLimit = TempPte;
        BstoreLimit += 1;
    }

    MmKernelStackResident += NumberOfPages;
    UNLOCK_PFN (OldIrql);

#if DBG
    ASSERT (NewLimit->u.Hard.Valid == 1);
    if (NewLimit != EndStack) {
        ASSERT ((NewLimit + 1)->u.Hard.Valid == 0);
    }
#endif

    Thread->Tcb.BStoreLimit = MiGetVirtualAddressMappedByPte (BstoreLimit);

    return STATUS_SUCCESS;
}
#endif // defined(_IA64_)


VOID
MmOutPageKernelStack (
    IN PKTHREAD Thread
    )

/*++

Routine Description:

    This routine makes the specified kernel stack non-resident and
    puts the pages on the transition list.  Note that pages below
    the CurrentStackPointer are not useful and these pages are freed here.

Arguments:

    Thread - Supplies a pointer to the thread whose stack should be removed.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

#define MAX_STACK_PAGES ((KERNEL_LARGE_STACK_SIZE + KERNEL_LARGE_BSTORE_SIZE) / PAGE_SIZE)

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE EndOfStackPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    KIRQL OldIrql;
    MMPTE TempPte;
    PVOID BaseOfKernelStack;
    PVOID FlushVa[MAX_STACK_PAGES];
    ULONG StackSize;
    ULONG Count;
    PMMPTE LimitPte;
    PMMPTE LowestLivePte;

    ASSERT (KERNEL_LARGE_STACK_SIZE >= MI_LARGE_STACK_SIZE);

    ASSERT (((PCHAR)Thread->StackBase - (PCHAR)Thread->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    if (NtGlobalFlag & FLG_DISABLE_PAGE_KERNEL_STACKS) {
        return;
    }

    //
    // The first page of the stack is the page before the base
    // of the stack.
    //

    BaseOfKernelStack = ((PCHAR)Thread->StackBase - PAGE_SIZE);
    PointerPte = MiGetPteAddress (BaseOfKernelStack);
    LastPte = MiGetPteAddress ((PULONG)Thread->KernelStack - 1);
    if (Thread->LargeStack) {
        StackSize = MI_LARGE_STACK_SIZE >> PAGE_SHIFT;

        //
        // The stack pagein won't necessarily bring back all the pages.
        // Make sure that we account now for the ones that will disappear.
        //

        LimitPte = MiGetPteAddress (Thread->StackLimit);

        LowestLivePte = MiGetPteAddress ((PVOID)((PUCHAR)Thread->InitialStack -
                                            KERNEL_LARGE_STACK_COMMIT));

        if (LowestLivePte < LimitPte) {
            LowestLivePte = LimitPte;
        }
    }
    else {
        StackSize = KERNEL_STACK_SIZE >> PAGE_SHIFT;
        LowestLivePte = MiGetPteAddress (Thread->StackLimit);
    }
    EndOfStackPte = PointerPte - StackSize;

    ASSERT (LowestLivePte <= LastPte);

    //
    // Put a signature at the current stack location - sizeof(ULONG_PTR).
    //

    *((PULONG_PTR)Thread->KernelStack - 1) = (ULONG_PTR)Thread;

    Count = 0;

    LOCK_PFN (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        TempPte = *PointerPte;
        MI_MAKE_VALID_PTE_TRANSITION (TempPte, 0);

        TempPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn2->OriginalPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        FlushVa[Count] = BaseOfKernelStack;

        MiDecrementShareCount (PageFrameIndex);
        PointerPte -= 1;
        Count += 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack - PAGE_SIZE);
    } while (PointerPte >= LastPte);

    //
    // Just toss the pages that won't ever come back in.
    //

    while (PointerPte != EndOfStackPte) {
        if (PointerPte->u.Hard.Valid == 0) {
            break;
        }

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte));

        TempPte = KernelDemandZeroPte;

        TempPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        FlushVa[Count] = BaseOfKernelStack;
        Count += 1;

        //
        // Return resident available for pages beyond the guaranteed portion
        // as an explicit call to grow the kernel stack will be needed to get
        // these pages back.
        //

        if (PointerPte < LowestLivePte) {
            ASSERT (Thread->LargeStack);
            MmResidentAvailablePages += 1;
            MM_BUMP_COUNTER(12, 1);
        }

        PointerPte -= 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack - PAGE_SIZE);
    }

#if defined(_IA64_)

    //
    // Transition or free RSE stack pages as appropriate.
    //

    BaseOfKernelStack = Thread->StackBase;
    PointerPte = MiGetPteAddress (BaseOfKernelStack);
    LastPte = MiGetPteAddress ((PULONG)Thread->KernelBStore);

    if (Thread->LargeStack) {
        StackSize = KERNEL_LARGE_BSTORE_SIZE >> PAGE_SHIFT;
        LowestLivePte = MiGetPteAddress ((PVOID) ((PUCHAR) Thread->InitialBStore + KERNEL_LARGE_BSTORE_COMMIT - 1));
    }
    else {
        StackSize = KERNEL_BSTORE_SIZE >> PAGE_SHIFT;
        LowestLivePte = PointerPte + StackSize;
    }
    EndOfStackPte = PointerPte + StackSize;

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        TempPte = *PointerPte;
        MI_MAKE_VALID_PTE_TRANSITION (TempPte, 0);

        TempPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;
        Pfn2 = MI_PFN_ELEMENT(PageFrameIndex);
        Pfn2->OriginalPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        FlushVa[Count] = BaseOfKernelStack;

        MiDecrementShareCount (PageFrameIndex);
        PointerPte += 1;
        Count += 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack + PAGE_SIZE);
    } while (PointerPte <= LastPte);

    while (PointerPte != EndOfStackPte) {
        if (PointerPte->u.Hard.Valid == 0) {
            break;
        }

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte));

        TempPte = KernelDemandZeroPte;

        TempPte.u.Soft.Protection = MM_KSTACK_OUTSWAPPED;

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        FlushVa[Count] = BaseOfKernelStack;
        Count += 1;

        //
        // Return resident available for pages beyond the guaranteed portion
        // as an explicit call to grow the kernel stack will be needed to get
        // these pages back.
        //

        if (PointerPte > LowestLivePte) {
            ASSERT (Thread->LargeStack);
            MmResidentAvailablePages += 1;
            MM_BUMP_COUNTER(4, 1);
        }

        PointerPte += 1;
        BaseOfKernelStack = ((PCHAR)BaseOfKernelStack + PAGE_SIZE);
    }

#endif // _IA64_

    //
    // Increase the available pages by the number of pages that were
    // deleted and turned into demand zero.
    //

    MmKernelStackResident -= Count;

    UNLOCK_PFN (OldIrql);

    ASSERT (Count <= MAX_STACK_PAGES);

    if (Count < MM_MAXIMUM_FLUSH_COUNT) {
        KeFlushMultipleTb (Count,
                           &FlushVa[0],
                           TRUE,
                           TRUE,
                           NULL,
                           *(PHARDWARE_PTE)&ZeroPte.u.Flush);
    }
    else {
        KeFlushEntireTb (TRUE, TRUE);
    }

    return;
}

VOID
MmInPageKernelStack (
    IN PKTHREAD Thread
    )

/*++

Routine Description:

    This routine makes the specified kernel stack resident.

Arguments:

    Supplies a pointer to the base of the kernel stack.

Return Value:

    Thread - Supplies a pointer to the thread whose stack should be
             made resident.

Environment:

    Kernel mode.

--*/

{
    PVOID BaseOfKernelStack;
    PMMPTE PointerPte;
    PMMPTE EndOfStackPte;
    PMMPTE SignaturePte;
    ULONG DiskRead;
    PFN_NUMBER ContainingPage;
    KIRQL OldIrql;

    ASSERT (((PCHAR)Thread->StackBase - (PCHAR)Thread->StackLimit) <=
            ((LONG)MI_LARGE_STACK_SIZE + PAGE_SIZE));

    if (NtGlobalFlag & FLG_DISABLE_PAGE_KERNEL_STACKS) {
        return;
    }

    //
    // The first page of the stack is the page before the base
    // of the stack.
    //

    if (Thread->LargeStack) {
        PointerPte = MiGetPteAddress ((PVOID)((PUCHAR)Thread->StackLimit));

        EndOfStackPte = MiGetPteAddress ((PVOID)((PUCHAR)Thread->InitialStack -
                                            KERNEL_LARGE_STACK_COMMIT));
        //
        // Trim back the stack.  Make sure that the stack does not grow, i.e.
        // StackLimit remains the limit.
        //

        if (EndOfStackPte < PointerPte) {
            EndOfStackPte = PointerPte;
        }
        Thread->StackLimit = MiGetVirtualAddressMappedByPte (EndOfStackPte);
    }
    else {
        EndOfStackPte = MiGetPteAddress (Thread->StackLimit);
    }

#if defined(_IA64_)

    if (Thread->LargeStack) {

        PVOID TempAddress = (PVOID)((PUCHAR)Thread->BStoreLimit);

        BaseOfKernelStack = (PVOID)(((ULONG_PTR)Thread->InitialBStore +
                               KERNEL_LARGE_BSTORE_COMMIT) &
                               ~(ULONG_PTR)(PAGE_SIZE - 1));

        //
        // Make sure the guard page is not set to valid.
        //

        if (BaseOfKernelStack > TempAddress) {
            BaseOfKernelStack = TempAddress;
        }
        Thread->BStoreLimit = BaseOfKernelStack;
    }
    BaseOfKernelStack = ((PCHAR)Thread->BStoreLimit - PAGE_SIZE);
#else
    BaseOfKernelStack = ((PCHAR)Thread->StackBase - PAGE_SIZE);
#endif // _IA64_

    PointerPte = MiGetPteAddress (BaseOfKernelStack);

    DiskRead = 0;
    SignaturePte = MiGetPteAddress ((PULONG_PTR)Thread->KernelStack - 1);
    ASSERT (SignaturePte->u.Hard.Valid == 0);
    if ((SignaturePte->u.Long != MM_KERNEL_DEMAND_ZERO_PTE) &&
        (SignaturePte->u.Soft.Transition == 0)) {
            DiskRead = 1;
    }

    LOCK_PFN (OldIrql);

    while (PointerPte >= EndOfStackPte) {

        if (!((PointerPte->u.Long == KernelDemandZeroPte.u.Long) ||
                (PointerPte->u.Soft.Protection == MM_KSTACK_OUTSWAPPED))) {
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x3451,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)Thread,
                          0);
        }
        ASSERT (PointerPte->u.Hard.Valid == 0);
        if (PointerPte->u.Soft.Protection == MM_KSTACK_OUTSWAPPED) {
            PointerPte->u.Soft.Protection = PAGE_READWRITE;
        }

        ContainingPage = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress (PointerPte));
        MiMakeOutswappedPageResident (PointerPte,
                                      PointerPte,
                                      1,
                                      ContainingPage);

        PointerPte -= 1;
        MmKernelStackResident += 1;
    }

    //
    // Check the signature at the current stack location - 4.
    //

    if (*((PULONG_PTR)Thread->KernelStack - 1) != (ULONG_PTR)Thread) {
        KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                      DiskRead,
                      *((PULONG_PTR)Thread->KernelStack - 1),
                      0,
                      (ULONG_PTR)Thread->KernelStack);
    }

    UNLOCK_PFN (OldIrql);
    return;
}


VOID
MmOutSwapProcess (
    IN PKPROCESS Process
    )

/*++

Routine Description:

    This routine out swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process that is swapped out of memory.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PEPROCESS OutProcess;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PFN_NUMBER HyperSpacePageTable;
    PMMPTE HyperSpacePageTableMap;
    PFN_NUMBER PdePage;
    PFN_NUMBER ProcessPage;
    MMPTE TempPte;
    PMMPTE PageDirectoryMap;
    PFN_NUMBER VadBitMapPage;
    MMPTE TempPte2;
    PEPROCESS CurrentProcess;
#if defined (_X86PAE_)
    ULONG i;
    PFN_NUMBER PdePage2;
    PFN_NUMBER HyperPage2;
    PPAE_ENTRY PaeVa;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER PpePage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PFN_NUMBER PxePage;
#endif

    OutProcess = CONTAINING_RECORD (Process, EPROCESS, Pcb);

    PS_SET_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAP_ENABLED);

#if DBG
    if ((MmDebug & MM_DBG_SWAP_PROCESS) != 0) {
        return;
    }
#endif

    if (OutProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        MiSessionOutSwapProcess (OutProcess);
    }

    CurrentProcess = PsGetCurrentProcess ();

    if ((OutProcess->Vm.WorkingSetSize == MM_PROCESS_COMMIT_CHARGE) &&
        (OutProcess->Vm.Flags.AllowWorkingSetAdjustment)) {

        LOCK_EXPANSION (OldIrql);

        ASSERT (OutProcess->Outswapped == 0);

        if (OutProcess->Vm.Flags.BeingTrimmed == TRUE) {

            //
            // An outswap is not allowed at this point because the process
            // has been attached to and is being trimmed.
            //

            UNLOCK_EXPANSION (OldIrql);
            return;
        }

        //
        // Swap the process working set info and page parent/directory/table
        // pages from memory.
        //

        PS_SET_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAPPED);

        UNLOCK_EXPANSION (OldIrql);

        LOCK_PFN (OldIrql);

        //
        // Remove the working set list page from the process.
        //

        HyperSpacePageTable = MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS (OutProcess);

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess,
                                                           HyperSpacePageTable);

        TempPte = HyperSpacePageTableMap[MiGetPteOffset(MmWorkingSetList)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        HyperSpacePageTableMap[MiGetPteOffset(MmWorkingSetList)] = TempPte;

        PointerPte = &HyperSpacePageTableMap[MiGetPteOffset (VAD_BITMAP_SPACE)];
        TempPte2 = *PointerPte;

        VadBitMapPage = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte2);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte2, MM_READWRITE);

        *PointerPte = TempPte2;

#if defined (_X86PAE_)
        TempPte2 = HyperSpacePageTableMap[0];

        HyperPage2 = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte2);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte2, MM_READWRITE);

        HyperSpacePageTableMap[0] = TempPte2;
#endif

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        //
        // Remove the VAD bitmap page from the process.
        //

        ASSERT ((MI_PFN_ELEMENT (VadBitMapPage))->u3.e1.Modified == 1);

        MiDecrementShareCount (VadBitMapPage);

        //
        // Remove the hyper space page from the process.
        //

        ASSERT ((MI_PFN_ELEMENT (OutProcess->WorkingSetPage))->u3.e1.Modified == 1);
        MiDecrementShareCount (OutProcess->WorkingSetPage);

        //
        // Remove the hyper space page table from the process.
        //

        Pfn1 = MI_PFN_ELEMENT (HyperSpacePageTable);
        PdePage = Pfn1->u4.PteFrame;
        ASSERT (PdePage);

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)];

        ASSERT (TempPte.u.Hard.Valid == 1);
        ASSERT (TempPte.u.Hard.PageFrameNumber == HyperSpacePageTable);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)] = TempPte;

        ASSERT (Pfn1->u3.e1.Modified == 1);

        MiDecrementShareCount (HyperSpacePageTable);

#if defined (_X86PAE_)

        //
        // Remove the second hyper space page from the process.
        //

        Pfn1 = MI_PFN_ELEMENT (HyperPage2);

        ASSERT (Pfn1->u3.e1.Modified == 1);

        PdePage = Pfn1->u4.PteFrame;
        ASSERT (PdePage);

        PageDirectoryMap[MiGetPdeOffset(HYPER_SPACE2)] = TempPte2;

        MiDecrementShareCount (HyperPage2);

        //
        // Remove the additional page directory pages.
        //

        PaeVa = (PPAE_ENTRY)OutProcess->PaeTop;
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {

            TempPte = PageDirectoryMap[i];
            PdePage2 = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte);

            MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

            PageDirectoryMap[i] = TempPte;
            Pfn1 = MI_PFN_ELEMENT (PdePage2);
            ASSERT (Pfn1->u3.e1.Modified == 1);

            MiDecrementShareCount (PdePage2);
            PaeVa->PteEntry[i].u.Long = TempPte.u.Long;
        }

#if DBG
        TempPte = PageDirectoryMap[i];
        PdePage2 = MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)&TempPte);
        Pfn1 = MI_PFN_ELEMENT (PdePage2);
        ASSERT (Pfn1->u3.e1.Modified == 1);
#endif

#endif

#if (_MI_PAGING_LEVELS >= 3)

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        //
        // Remove the page directory page.
        //

        Pfn1 = MI_PFN_ELEMENT (PdePage);
        PpePage = Pfn1->u4.PteFrame;
        ASSERT (PpePage);

#if (_MI_PAGING_LEVELS==3)
        ASSERT (PpePage == MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&(OutProcess->Pcb.DirectoryTableBase[0]))));
#endif

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PpePage);

        TempPte = PageDirectoryMap[MiGetPpeOffset(MmWorkingSetList)];

        ASSERT (TempPte.u.Hard.Valid == 1);
        ASSERT (TempPte.u.Hard.PageFrameNumber == PdePage);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPpeOffset(MmWorkingSetList)] = TempPte;

        ASSERT (Pfn1->u3.e1.Modified == 1);

        MiDecrementShareCount (PdePage);

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Remove the page directory parent page.  Then remove
        // the top level extended page directory parent page.
        //

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        PxePage = Pfn1->u4.PteFrame;
        ASSERT (PxePage);
        ASSERT (PxePage == MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&(OutProcess->Pcb.DirectoryTableBase[0]))));

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PxePage);

        TempPte = PageDirectoryMap[MiGetPxeOffset(MmWorkingSetList)];

        ASSERT (TempPte.u.Hard.Valid == 1);
        ASSERT (TempPte.u.Hard.PageFrameNumber == PpePage);

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPxeOffset(MmWorkingSetList)] = TempPte;

        ASSERT (MI_PFN_ELEMENT(PpePage)->u3.e1.Modified == 1);

        MiDecrementShareCount (PpePage);

        TempPte = PageDirectoryMap[MiGetPxeOffset(PXE_BASE)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPxeOffset(PXE_BASE)] = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PxePage);
#else

        //
        // Remove the top level page directory parent page.
        //

        TempPte = PageDirectoryMap[MiGetPpeOffset(PDE_TBASE)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                      MM_READWRITE);

        PageDirectoryMap[MiGetPpeOffset(PDE_TBASE)] = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PpePage);
#endif

#else

        //
        // Remove the top level page directory page.
        //

        TempPte = PageDirectoryMap[MiGetPdeOffset(PDE_BASE)];

        MI_MAKE_VALID_PTE_TRANSITION (TempPte, MM_READWRITE);

        PageDirectoryMap[MiGetPdeOffset(PDE_BASE)] = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PdePage);

#endif

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        //
        // Decrement share count so the top level page directory page gets
        // removed.  This can cause the PteCount to equal the sharecount as the
        // page directory page no longer contains itself, yet can have
        // itself as a transition page.
        //

        Pfn1->u2.ShareCount -= 2;
        Pfn1->PteAddress = (PMMPTE)&OutProcess->PageDirectoryPte;

        OutProcess->PageDirectoryPte = TempPte.u.Flush;

#if defined (_X86PAE_)
        PaeVa->PteEntry[i].u.Long = TempPte.u.Long;
#endif

        if (MI_IS_PHYSICAL_ADDRESS(OutProcess)) {
            ProcessPage = MI_CONVERT_PHYSICAL_TO_PFN (OutProcess);
        }
        else {
            PointerPte = MiGetPteAddress (OutProcess);
            ProcessPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        }

        Pfn1->u4.PteFrame = ProcessPage;
        Pfn1 = MI_PFN_ELEMENT (ProcessPage);

        //
        // Increment the share count for the process page.
        //

        Pfn1->u2.ShareCount += 1;

        UNLOCK_PFN (OldIrql);

        LOCK_EXPANSION (OldIrql);
        if (OutProcess->Vm.WorkingSetExpansionLinks.Flink >
                                                       MM_IO_IN_PROGRESS) {

            //
            // The entry must be on the list.
            //
            RemoveEntryList (&OutProcess->Vm.WorkingSetExpansionLinks);
            OutProcess->Vm.WorkingSetExpansionLinks.Flink = MM_WS_SWAPPED_OUT;
        }
        UNLOCK_EXPANSION (OldIrql);

        OutProcess->WorkingSetPage = 0;
        OutProcess->Vm.WorkingSetSize = 0;
#if defined(_IA64_)

        //
        // Force assignment of new PID as we have removed
        // the page directory page.
        // Note that a TB flush would not work here as we
        // are in the wrong process context.
        //

        Process->ProcessRegion.SequenceNumber = 0;
#endif _IA64_

    }

    return;
}

VOID
MmInSwapProcess (
    IN PKPROCESS Process
    )

/*++

Routine Description:

    This routine in swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process that is to be swapped
              into memory.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PEPROCESS OutProcess;
    PEPROCESS CurrentProcess;
    PFN_NUMBER PdePage;
    PMMPTE PageDirectoryMap;
    MMPTE VadBitMapPteContents;
    PFN_NUMBER VadBitMapPage;
    ULONG WorkingSetListPteOffset;
    ULONG VadBitMapPteOffset;
    PMMPTE WorkingSetListPte;
    PMMPTE VadBitMapPte;
    MMPTE TempPte;
    PFN_NUMBER HyperSpacePageTable;
    PMMPTE HyperSpacePageTableMap;
    PFN_NUMBER WorkingSetPage;
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    PFN_NUMBER ProcessPage;
#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER TopPage;
    PFN_NUMBER PageDirectoryPage;
    PMMPTE PageDirectoryParentMap;
#endif
#if defined (_X86PAE_)
    ULONG i;
    PPAE_ENTRY PaeVa;
    MMPTE TempPte2;
    MMPTE PageDirectoryPtes[PD_PER_SYSTEM];
#endif

    CurrentProcess = PsGetCurrentProcess ();

    OutProcess = CONTAINING_RECORD (Process, EPROCESS, Pcb);

    if (OutProcess->Flags & PS_PROCESS_FLAGS_OUTSWAPPED) {

        //
        // The process is out of memory, rebuild the initialized page
        // structure.
        //

        if (MI_IS_PHYSICAL_ADDRESS(OutProcess)) {
            ProcessPage = MI_CONVERT_PHYSICAL_TO_PFN (OutProcess);
        }
        else {
            PointerPte = MiGetPteAddress (OutProcess);
            ProcessPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        }

        WorkingSetListPteOffset = MiGetPteOffset (MmWorkingSetList);
        VadBitMapPteOffset = MiGetPteOffset (VAD_BITMAP_SPACE);

        WorkingSetListPte = MiGetPteAddress (MmWorkingSetList);
        VadBitMapPte = MiGetPteAddress (VAD_BITMAP_SPACE);

        LOCK_PFN (OldIrql);

        PdePage = MiMakeOutswappedPageResident (
#if (_MI_PAGING_LEVELS >= 4)
                                        MiGetPteAddress (PXE_BASE),
#elif (_MI_PAGING_LEVELS >= 3)
                                        MiGetPteAddress ((PVOID)PDE_TBASE),
#else
                                        MiGetPteAddress (PDE_BASE),
#endif
                                        (PMMPTE)&OutProcess->PageDirectoryPte,
                                        0,
                                        ProcessPage);

        //
        // Adjust the counts for the process page.
        //

        Pfn1 = MI_PFN_ELEMENT (ProcessPage);
        Pfn1->u2.ShareCount -= 1;

        ASSERT ((LONG)Pfn1->u2.ShareCount >= 1);

#if (_MI_PAGING_LEVELS >= 3)
        TopPage = PdePage;
#endif

        //
        // Adjust the counts properly for the page directory page.
        //

        Pfn1 = MI_PFN_ELEMENT (PdePage);
        Pfn1->u2.ShareCount += 1;
        Pfn1->u1.Event = (PVOID)OutProcess;
        Pfn1->u4.PteFrame = PdePage;

#if (_MI_PAGING_LEVELS >= 4)
        Pfn1->PteAddress = MiGetPteAddress (PXE_BASE);
#elif (_MI_PAGING_LEVELS >= 3)
        Pfn1->PteAddress = MiGetPteAddress ((PVOID)PDE_TBASE);
#else
        Pfn1->PteAddress = MiGetPteAddress (PDE_BASE);
#endif

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Only the extended page directory parent page has really been
        // read in above.  Read in the page directory parent page now.
        //

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryParentMap[MiGetPxeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PageDirectoryPage = MiMakeOutswappedPageResident (
                                 MiGetPxeAddress (MmWorkingSetList),
                                 &TempPte,
                                 0,
                                 PdePage);

        ASSERT (PageDirectoryPage == TempPte.u.Hard.PageFrameNumber);
        ASSERT (Pfn1->u2.ShareCount >= 3);

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        PageDirectoryParentMap[MiGetPxeOffset(PXE_BASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
        PageDirectoryParentMap[MiGetPxeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PdePage = PageDirectoryPage;

#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Only the page directory parent page has really been read in above
        // (and the extended page directory parent for 4-level architectures).
        // Read in the page directory page now.
        //

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryParentMap[MiGetPpeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PageDirectoryPage = MiMakeOutswappedPageResident (
                                 MiGetPpeAddress (MmWorkingSetList),
                                 &TempPte,
                                 0,
                                 PdePage);

        ASSERT (PageDirectoryPage == TempPte.u.Hard.PageFrameNumber);

        PageDirectoryParentMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

#if (_MI_PAGING_LEVELS==3)
        ASSERT (Pfn1->u2.ShareCount >= 3);
        PageDirectoryParentMap[MiGetPpeOffset(PDE_TBASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
#endif

        PageDirectoryParentMap[MiGetPpeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryParentMap);

        PdePage = PageDirectoryPage;

#endif

#if defined (_X86PAE_)

        //
        // Locate the additional page directory pages and make them resident.
        //

        PaeVa = (PPAE_ENTRY)OutProcess->PaeTop;

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageDirectoryPtes[i] = PageDirectoryMap[i];
        }
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            MiMakeOutswappedPageResident (
                                 MiGetPteAddress (PDE_BASE + (i << PAGE_SHIFT)),
                                 &PageDirectoryPtes[i],
                                 0,
                                 PdePage);
            PaeVa->PteEntry[i].u.Long = (PageDirectoryPtes[i].u.Long & ~MM_PAE_PDPTE_MASK);
        }

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);
        for (i = 0; i < PD_PER_SYSTEM - 1; i += 1) {
            PageDirectoryMap[i] = PageDirectoryPtes[i];
        }
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        TempPte.u.Flush = OutProcess->PageDirectoryPte;
        TempPte.u.Long &= ~MM_PAE_PDPTE_MASK;
        PaeVa->PteEntry[i].u.Flush = TempPte.u.Flush;

        //
        // Locate the second page table page for hyperspace & make it resident.
        //

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryMap[MiGetPdeOffset(HYPER_SPACE2)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        HyperSpacePageTable = MiMakeOutswappedPageResident (
                                 MiGetPdeAddress (HYPER_SPACE2),
                                 &TempPte,
                                 0,
                                 PdePage);

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);
        PageDirectoryMap[MiGetPdeOffset(HYPER_SPACE2)] = TempPte;
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        TempPte2 = TempPte;
#endif

        //
        // Locate the page table page for hyperspace and make it resident.
        //

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

        TempPte = PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        HyperSpacePageTable = MiMakeOutswappedPageResident (
                                 MiGetPdeAddress (HYPER_SPACE),
                                 &TempPte,
                                 0,
                                 PdePage);

        ASSERT (Pfn1->u2.ShareCount >= 3);

        PageDirectoryMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, PdePage);

#if (_MI_PAGING_LEVELS==2)
        PageDirectoryMap[MiGetPdeOffset(PDE_BASE)].u.Flush =
                                              OutProcess->PageDirectoryPte;
#endif

        PageDirectoryMap[MiGetPdeOffset(MmWorkingSetList)] = TempPte;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, PageDirectoryMap);

        //
        // Map in the hyper space page table page and retrieve the
        // PTEs that map the working set list and VAD bitmap.  Note that
        // although both PTEs lie in the same page table page, they must
        // be retrieved separately because: the Vad PTE may indicate its page
        // is in a paging file and the WSL PTE may indicate its PTE is in
        // transition.  The VAD page inswap may take the WSL page from
        // the transition list - CHANGING the WSL PTE !  So the WSL PTE cannot
        // be captured until after the VAD inswap completes.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        VadBitMapPteContents = HyperSpacePageTableMap[VadBitMapPteOffset];

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        Pfn1 = MI_PFN_ELEMENT (HyperSpacePageTable);
        Pfn1->u1.WsIndex = 1;

        //
        // Read in the VAD bitmap page.
        //

        VadBitMapPage = MiMakeOutswappedPageResident (VadBitMapPte,
                                                      &VadBitMapPteContents,
                                                      0,
                                                      HyperSpacePageTable);

        //
        // Read in the working set list page.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        TempPte = HyperSpacePageTableMap[WorkingSetListPteOffset];
        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        WorkingSetPage = MiMakeOutswappedPageResident (WorkingSetListPte,
                                                       &TempPte,
                                                       0,
                                                       HyperSpacePageTable);

        //
        // Update the PTEs, this can be done together for PTEs that lie within
        // the same page table page.
        //

        HyperSpacePageTableMap = MiMapPageInHyperSpaceAtDpc (CurrentProcess, HyperSpacePageTable);
        HyperSpacePageTableMap[WorkingSetListPteOffset] = TempPte;
#if defined (_X86PAE_)
        HyperSpacePageTableMap[0] = TempPte2;
#endif

        HyperSpacePageTableMap[VadBitMapPteOffset] = VadBitMapPteContents;

        MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, HyperSpacePageTableMap);

        Pfn1 = MI_PFN_ELEMENT (WorkingSetPage);
        Pfn1->u1.WsIndex = 3;

        Pfn1 = MI_PFN_ELEMENT (VadBitMapPage);
        Pfn1->u1.WsIndex = 2;

        UNLOCK_PFN (OldIrql);

        LOCK_EXPANSION (OldIrql);

        //
        // Allow working set trimming on this process.
        //

        OutProcess->Vm.Flags.AllowWorkingSetAdjustment = TRUE;
        if (OutProcess->Vm.WorkingSetExpansionLinks.Flink == MM_WS_SWAPPED_OUT) {
            InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                            &OutProcess->Vm.WorkingSetExpansionLinks);
        }
        UNLOCK_EXPANSION (OldIrql);

        //
        // Set up process structures.
        //

#if (_MI_PAGING_LEVELS >= 3)
        PdePage = TopPage;
#endif

        OutProcess->WorkingSetPage = WorkingSetPage;

        OutProcess->Vm.WorkingSetSize = MM_PROCESS_COMMIT_CHARGE;

#if !defined (_X86PAE_)

        INITIALIZE_DIRECTORY_TABLE_BASE (&Process->DirectoryTableBase[0],
                                         PdePage);
        INITIALIZE_DIRECTORY_TABLE_BASE (&Process->DirectoryTableBase[1],
                                         HyperSpacePageTable);
#else
        //
        // The DirectoryTableBase[0] never changes for PAE processes.
        //

        Process->DirectoryTableBase[1] = HyperSpacePageTable;
#endif

        PS_CLEAR_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAPPED);
    }

    if (OutProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        MiSessionInSwapProcess (OutProcess);
    }

    PS_CLEAR_BITS (&OutProcess->Flags, PS_PROCESS_FLAGS_OUTSWAP_ENABLED);

    if (PERFINFO_IS_GROUP_ON(PERF_MEMORY)) {
        PERFINFO_SWAPPROCESS_INFORMATION PerfInfoSwapProcess;
        PerfInfoSwapProcess.ProcessId = HandleToUlong((OutProcess)->UniqueProcessId);
        PerfInfoSwapProcess.PageDirectoryBase = MmGetDirectoryFrameFromProcess(OutProcess);
        PerfInfoLogBytes (PERFINFO_LOG_TYPE_INSWAPPROCESS,
                          &PerfInfoSwapProcess,
                          sizeof(PerfInfoSwapProcess));
    }
    return;
}

NTSTATUS
MiCreatePebOrTeb (
    IN PEPROCESS TargetProcess,
    IN ULONG Size,
    OUT PVOID *Base
    )

/*++

Routine Description:

    This routine creates a TEB or PEB page within the target process.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    the structure.

    Size - Supplies the size of the structure to create a VAD for.

    Base - Supplies a pointer to place the PEB/TEB virtual address on success.
           This has no meaning if success is not returned.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, attached to the specified process.

--*/

{
    PMMVAD_LONG Vad;
    NTSTATUS Status;

    //
    // Allocate and initialize the Vad before acquiring the address space
    // and working set mutexes so as to minimize mutex hold duration.
    //

    Vad = (PMMVAD_LONG) ExAllocatePoolWithTag (NonPagedPool,
                                               sizeof(MMVAD_LONG),
                                               'ldaV');

    if (Vad == NULL) {
        return STATUS_NO_MEMORY;
    }

    Vad->u.LongFlags = 0;

    Vad->u.VadFlags.CommitCharge = BYTES_TO_PAGES (Size);
    Vad->u.VadFlags.MemCommit = 1;
    Vad->u.VadFlags.PrivateMemory = 1;
    Vad->u.VadFlags.Protection = MM_EXECUTE_READWRITE;

    //
    // Mark VAD as not deletable, no protection change.
    //

    Vad->u.VadFlags.NoChange = 1;
    Vad->u2.LongFlags2 = 0;
    Vad->u2.VadFlags2.OneSecured = 1;
    Vad->u2.VadFlags2.LongVad = 1;
    Vad->u2.VadFlags2.ReadOnly = 0;

#if defined(_MIALT4K_)
    Vad->AliasInformation = NULL;
#endif

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Find a VA for the PEB on a page-size boundary.
    //

    Status = MiFindEmptyAddressRangeDown (TargetProcess->VadRoot,
                                          ROUND_TO_PAGES (Size),
                                          ((PCHAR)MM_HIGHEST_VAD_ADDRESS + 1),
                                          PAGE_SIZE,
                                          Base);

    if (!NT_SUCCESS(Status)) {

        //
        // No range was available, deallocate the Vad and return the status.
        //

        UNLOCK_ADDRESS_SPACE (TargetProcess);
        ExFreePool (Vad);
        return Status;
    }

    //
    // An unoccupied address range has been found, finish initializing the
    // virtual address descriptor to describe this range.
    //

    Vad->StartingVpn = MI_VA_TO_VPN (*Base);
    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)*Base + Size - 1);

    Vad->u3.Secured.StartVpn = (ULONG_PTR)*Base;
    Vad->u3.Secured.EndVpn = (ULONG_PTR)MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

    LOCK_WS_UNSAFE (TargetProcess);

    Status = MiInsertVad ((PMMVAD) Vad);

    UNLOCK_WS_UNSAFE (TargetProcess);

#if defined (_IA64_)
    if ((NT_SUCCESS(Status)) && (TargetProcess->Wow64Process != NULL)) {
        MiProtectFor4kPage (*Base,
                            ROUND_TO_PAGES (Size),
                            MM_READWRITE ,
                            ALT_COMMIT,
                            TargetProcess);
    }
#endif

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    if (!NT_SUCCESS(Status)) {

        //
        // A failure has occurred.  Deallocate the Vad and return the status.
        //

        ExFreePool (Vad);
    }

    return Status;
}

NTSTATUS
MmCreateTeb (
    IN PEPROCESS TargetProcess,
    IN PINITIAL_TEB InitialTeb,
    IN PCLIENT_ID ClientId,
    OUT PTEB *Base
    )

/*++

Routine Description:

    This routine creates a TEB page within the target process
    and copies the initial TEB values into it.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the TEB.

    InitialTeb - Supplies a pointer to the initial TEB to copy into the
                 newly created TEB.

    ClientId - Supplies a client ID.

    Base - Supplies a location to return the base of the newly created
           TEB on success.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    PTEB TebBase;
    NTSTATUS Status;
    ULONG TebSize;
#if defined(_WIN64)
    PWOW64_PROCESS Wow64Process;
    PTEB32 Teb32Base = NULL;
#endif

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    TebSize = sizeof (TEB);

#if defined(_WIN64)
    Wow64Process = TargetProcess->Wow64Process;
    if (Wow64Process != NULL) {
        TebSize = ROUND_TO_PAGES (sizeof (TEB)) + sizeof (TEB32);
    } 
#endif

    Status = MiCreatePebOrTeb (TargetProcess, TebSize, (PVOID) &TebBase);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess();
        return Status;
    }

    //
    // Initialize the TEB.  Note accesses to the TEB can raise exceptions
    // if no address space is available for the TEB or the user has exceeded
    // quota (non-paged, pagefile, commit) or the TEB is paged out and an
    // inpage error occurs when fetching it.
    //

    //
    // Note that since the TEB is populated with demand zero pages, only
    // nonzero fields need to be initialized here.
    //

    try {

#if !defined(_WIN64)
        TebBase->NtTib.ExceptionList = EXCEPTION_CHAIN_END;
#endif

        //
        // Although various fields must be zero for the process to launch
        // properly, don't assert them as an ordinary user could provoke these
        // by maliciously writing over random addresses in another thread,
        // hoping to nail a just-being-created TEB.
        //

        DONTASSERT (TebBase->NtTib.SubSystemTib == NULL);
        TebBase->NtTib.Version = OS2_VERSION;
        DONTASSERT (TebBase->NtTib.ArbitraryUserPointer == NULL);
        TebBase->NtTib.Self = (PNT_TIB)TebBase;
        DONTASSERT (TebBase->EnvironmentPointer == NULL);
        TebBase->ProcessEnvironmentBlock = TargetProcess->Peb;
        TebBase->ClientId = *ClientId;
        TebBase->RealClientId = *ClientId;
        DONTASSERT (TebBase->ActivationContextStack.Flags == 0);
        DONTASSERT (TebBase->ActivationContextStack.ActiveFrame == NULL);
        InitializeListHead(&TebBase->ActivationContextStack.FrameListCache);
        TebBase->ActivationContextStack.NextCookieSequenceNumber = 1;

        if ((InitialTeb->OldInitialTeb.OldStackBase == NULL) &&
            (InitialTeb->OldInitialTeb.OldStackLimit == NULL)) {

            TebBase->NtTib.StackBase = InitialTeb->StackBase;
            TebBase->NtTib.StackLimit = InitialTeb->StackLimit;
            TebBase->DeallocationStack = InitialTeb->StackAllocationBase;

#if defined(_IA64_)
            TebBase->BStoreLimit = InitialTeb->BStoreLimit;
            TebBase->DeallocationBStore = (PCHAR)InitialTeb->StackBase
                 + ((ULONG_PTR)InitialTeb->StackBase - (ULONG_PTR)InitialTeb->StackAllocationBase);
#endif

        }
        else {
            TebBase->NtTib.StackBase = InitialTeb->OldInitialTeb.OldStackBase;
            TebBase->NtTib.StackLimit = InitialTeb->OldInitialTeb.OldStackLimit;
        }

        TebBase->StaticUnicodeString.Buffer = TebBase->StaticUnicodeBuffer;
        TebBase->StaticUnicodeString.MaximumLength = (USHORT) sizeof (TebBase->StaticUnicodeBuffer);
        DONTASSERT (TebBase->StaticUnicodeString.Length == 0);

        //
        // Used for BBT of ntdll and kernel32.dll.
        //

        TebBase->ReservedForPerf = BBTBuffer;

#if defined(_WIN64)
        if (Wow64Process != NULL) {

            Teb32Base = (PTEB32)((PCHAR)TebBase + ROUND_TO_PAGES (sizeof(TEB)));

            Teb32Base->NtTib.ExceptionList = PtrToUlong (EXCEPTION_CHAIN_END);
            Teb32Base->NtTib.Version = TebBase->NtTib.Version;
            Teb32Base->NtTib.Self = PtrToUlong (Teb32Base);
            Teb32Base->ProcessEnvironmentBlock = PtrToUlong (Wow64Process->Wow64);
            Teb32Base->ClientId.UniqueProcess = PtrToUlong (TebBase->ClientId.UniqueProcess);
            Teb32Base->ClientId.UniqueThread = PtrToUlong (TebBase->ClientId.UniqueThread);
            Teb32Base->RealClientId.UniqueProcess = PtrToUlong (TebBase->RealClientId.UniqueProcess);
            Teb32Base->RealClientId.UniqueThread = PtrToUlong (TebBase->RealClientId.UniqueThread);
            Teb32Base->StaticUnicodeString.Buffer = PtrToUlong (Teb32Base->StaticUnicodeBuffer);
            Teb32Base->StaticUnicodeString.MaximumLength = (USHORT)sizeof (Teb32Base->StaticUnicodeBuffer);
            ASSERT (Teb32Base->StaticUnicodeString.Length == 0);
            Teb32Base->GdiBatchCount = PtrToUlong (TebBase);
            Teb32Base->Vdm = PtrToUlong (TebBase->Vdm);
            ASSERT (Teb32Base->ActivationContextStack.Flags == 0);
            Teb32Base->ActivationContextStack.ActiveFrame = PtrToUlong(TebBase->ActivationContextStack.ActiveFrame);
            InitializeListHead32 (&Teb32Base->ActivationContextStack.FrameListCache);
            Teb32Base->ActivationContextStack.NextCookieSequenceNumber = TebBase->ActivationContextStack.NextCookieSequenceNumber;
        }
        
        TebBase->NtTib.ExceptionList = (PVOID)Teb32Base;
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {

        //
        // An exception has occurred, inform our caller.
        //

        Status = GetExceptionCode ();
    }

    KeDetachProcess();
    *Base = TebBase;

    return Status;
}

//
// This code is built twice on the Win64 build - once for PE32+
// and once for PE32 images.
//

#define MI_INIT_PEB_FROM_IMAGE(Hdrs, ImgConfig) {                           \
    PebBase->ImageSubsystem = (Hdrs)->OptionalHeader.Subsystem;             \
    PebBase->ImageSubsystemMajorVersion =                                   \
        (Hdrs)->OptionalHeader.MajorSubsystemVersion;                       \
    PebBase->ImageSubsystemMinorVersion =                                   \
        (Hdrs)->OptionalHeader.MinorSubsystemVersion;                       \
                                                                            \
    /*                                                                   */ \
    /* See if this image wants GetVersion to lie about who the system is */ \
    /* If so, capture the lie into the PEB for the process.              */ \
    /*                                                                   */ \
                                                                            \
    if ((Hdrs)->OptionalHeader.Win32VersionValue != 0) {                    \
        PebBase->OSMajorVersion =                                           \
            (Hdrs)->OptionalHeader.Win32VersionValue & 0xFF;                \
        PebBase->OSMinorVersion =                                           \
            ((Hdrs)->OptionalHeader.Win32VersionValue >> 8) & 0xFF;         \
        PebBase->OSBuildNumber  =                                           \
            (USHORT)(((Hdrs)->OptionalHeader.Win32VersionValue >> 16) & 0x3FFF); \
        if ((ImgConfig) != NULL && (ImgConfig)->CSDVersion != 0) {          \
            PebBase->OSCSDVersion = (ImgConfig)->CSDVersion;                \
            }                                                               \
                                                                            \
        /* Win32 API GetVersion returns the following bogus bit definitions */ \
        /* in the high two bits:                                            */ \
        /*                                                                  */ \
        /*      00 - Windows NT                                             */ \
        /*      01 - reserved                                               */ \
        /*      10 - Win32s running on Windows 3.x                          */ \
        /*      11 - Windows 95                                             */ \
        /*                                                                  */ \
        /*                                                                  */ \
        /* Win32 API GetVersionEx returns a dwPlatformId with the following */ \
        /* values defined in winbase.h                                      */ \
        /*                                                                  */ \
        /*      00 - VER_PLATFORM_WIN32s                                    */ \
        /*      01 - VER_PLATFORM_WIN32_WINDOWS                             */ \
        /*      10 - VER_PLATFORM_WIN32_NT                                  */ \
        /*      11 - reserved                                               */ \
        /*                                                                  */ \
        /*                                                                  */ \
        /* So convert the former from the Win32VersionValue field into the  */ \
        /* OSPlatformId field.  This is done by XORing with 0x2.  The       */ \
        /* translation is symmetric so there is the same code to do the     */ \
        /* reverse in windows\base\client\module.c (GetVersion)             */ \
        /*                                                                  */ \
        PebBase->OSPlatformId   =                                           \
            ((Hdrs)->OptionalHeader.Win32VersionValue >> 30) ^ 0x2;         \
        }                                                                   \
    }


#if defined(_WIN64)
NTSTATUS
MiInitializeWowPeb (
    IN PIMAGE_NT_HEADERS NtHeaders,
    IN PPEB PebBase,
    IN PEPROCESS TargetProcess
    )

/*++

Routine Description:

    This routine creates a PEB32 page within the target process
    and copies the initial PEB32 values into it.

Arguments:

    NtHeaders - Supplies a pointer to the NT headers for the image.

    PebBase - Supplies a pointer to the initial PEB to derive the PEB32 values
              from.

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the PEB32.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    NTSTATUS Status;
    ULONG ReturnedSize;
    PPEB32 PebBase32;
    ULONG ProcessAffinityMask;
    PIMAGE_LOAD_CONFIG_DIRECTORY32 ImageConfigData32;

    ProcessAffinityMask = 0;
    ImageConfigData32 = NULL;

    //
    // All references to the Peb and NtHeaders must be wrapped in try-except
    // in case the user has exceeded quota (non-paged, pagefile, commit)
    // or any inpage errors happen for the user addresses, etc.
    //

    //
    // Image is 32-bit.
    //

    try {
        ImageConfigData32 = RtlImageDirectoryEntryToData (
                                PebBase->ImageBaseAddress,
                                TRUE,
                                IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG,
                                &ReturnedSize);

        ProbeForReadSmallStructure ((PVOID)ImageConfigData32,
                                    sizeof (*ImageConfigData32),
                                    sizeof (ULONG));

        MI_INIT_PEB_FROM_IMAGE ((PIMAGE_NT_HEADERS32)NtHeaders,
                                ImageConfigData32);

        if ((ImageConfigData32 != NULL) && (ImageConfigData32->ProcessAffinityMask != 0)) {
            ProcessAffinityMask = ImageConfigData32->ProcessAffinityMask;
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    //
    // Create a PEB32 for the process.
    //

    Status = MiCreatePebOrTeb (TargetProcess,
                               (ULONG)sizeof (PEB32),
                               (PVOID)&PebBase32);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // Mark the process as WOW64 by storing the 32-bit PEB pointer
    // in the Wow64 field.
    //

    TargetProcess->Wow64Process->Wow64 = PebBase32;

    //
    // Clone the PEB into the PEB32.
    //

    try {
        PebBase32->InheritedAddressSpace = PebBase->InheritedAddressSpace;
        PebBase32->Mutant = PtrToUlong(PebBase->Mutant);
        PebBase32->ImageBaseAddress = PtrToUlong(PebBase->ImageBaseAddress);
        PebBase32->AnsiCodePageData = PtrToUlong(PebBase->AnsiCodePageData);
        PebBase32->OemCodePageData = PtrToUlong(PebBase->OemCodePageData);
        PebBase32->UnicodeCaseTableData = PtrToUlong(PebBase->UnicodeCaseTableData);
        PebBase32->NumberOfProcessors = PebBase->NumberOfProcessors;
        PebBase32->BeingDebugged = PebBase->BeingDebugged;
        PebBase32->NtGlobalFlag = PebBase->NtGlobalFlag;
        PebBase32->CriticalSectionTimeout = PebBase->CriticalSectionTimeout;

        if (PebBase->HeapSegmentReserve > 1024*1024*1024) { // 1GB
            PebBase32->HeapSegmentReserve = 1024*1024;      // 1MB
        }
        else {
            PebBase32->HeapSegmentReserve = (ULONG)PebBase->HeapSegmentReserve;
        }

        if (PebBase->HeapSegmentCommit > PebBase32->HeapSegmentReserve) {
            PebBase32->HeapSegmentCommit = 2*PAGE_SIZE;
        }
        else {
            PebBase32->HeapSegmentCommit = (ULONG)PebBase->HeapSegmentCommit;
        }

        PebBase32->HeapDeCommitTotalFreeThreshold = (ULONG)PebBase->HeapDeCommitTotalFreeThreshold;
        PebBase32->HeapDeCommitFreeBlockThreshold = (ULONG)PebBase->HeapDeCommitFreeBlockThreshold;
        PebBase32->NumberOfHeaps = PebBase->NumberOfHeaps;
        PebBase32->MaximumNumberOfHeaps = (PAGE_SIZE - sizeof(PEB32)) / sizeof(ULONG);
        PebBase32->ProcessHeaps = PtrToUlong(PebBase32+1);
        PebBase32->OSMajorVersion = PebBase->OSMajorVersion;
        PebBase32->OSMinorVersion = PebBase->OSMinorVersion;
        PebBase32->OSBuildNumber = PebBase->OSBuildNumber;
        PebBase32->OSPlatformId = PebBase->OSPlatformId;
        PebBase32->OSCSDVersion = PebBase->OSCSDVersion;
        PebBase32->ImageSubsystem = PebBase->ImageSubsystem;
        PebBase32->ImageSubsystemMajorVersion = PebBase->ImageSubsystemMajorVersion;
        PebBase32->ImageSubsystemMinorVersion = PebBase->ImageSubsystemMinorVersion;
        PebBase32->SessionId = MmGetSessionId (TargetProcess);
        DONTASSERT (PebBase32->pShimData == 0);
        DONTASSERT (PebBase32->AppCompatFlags.QuadPart == 0);

        //
        // Leave the AffinityMask in the 32bit PEB as zero and let the
        // 64bit NTDLL set the initial mask.  This is to allow the
        // round robin scheduling of non MP safe imaging in the
        // caller to work correctly.
        //
        // Later code will set the affinity mask in the PEB32 if the
        // image actually specifies one.
        //
        // Note that the AffinityMask in the PEB is simply a mechanism
        // to pass affinity information from the image to the loader.
        //
        // Pass the affinity mask up to the 32 bit NTDLL via
        // the PEB32.  The 32 bit NTDLL will determine that the
        // affinity is not zero and try to set the affinity
        // mask from user-mode.  This call will be intercepted
        // by the wow64 thunks which will convert it
        // into a 64bit affinity mask and call the kernel.
        //

        PebBase32->ImageProcessAffinityMask = ProcessAffinityMask;

        DONTASSERT (PebBase32->ActivationContextData == 0);
        DONTASSERT (PebBase32->SystemDefaultActivationContextData == 0);

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode ();
    }
    return Status;
}
#endif


NTSTATUS
MmCreatePeb (
    IN PEPROCESS TargetProcess,
    IN PINITIAL_PEB InitialPeb,
    OUT PPEB *Base
    )

/*++

Routine Description:

    This routine creates a PEB page within the target process
    and copies the initial PEB values into it.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to create
                    and initialize the PEB.

    InitialPeb - Supplies a pointer to the initial PEB to copy into the
                 newly created PEB.

    Base - Supplies a location to return the base of the newly created
           PEB on success.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    PPEB PebBase;
    USHORT Magic;
    USHORT Characteristics;
    NTSTATUS Status;
    PVOID ViewBase;
    LARGE_INTEGER SectionOffset;
    PIMAGE_NT_HEADERS NtHeaders;
    SIZE_T ViewSize;
    ULONG ReturnedSize;
    PIMAGE_LOAD_CONFIG_DIRECTORY ImageConfigData;
    ULONG_PTR ProcessAffinityMask;

    ViewBase = NULL;
    SectionOffset.LowPart = 0;
    SectionOffset.HighPart = 0;
    ViewSize = 0;

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    //
    // Map the NLS tables into the application's address space.
    //

    Status = MmMapViewOfSection (InitNlsSectionPointer,
                                 TargetProcess,
                                 &ViewBase,
                                 0L,
                                 0L,
                                 &SectionOffset,
                                 &ViewSize,
                                 ViewShare,
                                 MEM_TOP_DOWN | SEC_NO_CHANGE,
                                 PAGE_READONLY);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess ();
        return Status;
    }

    Status = MiCreatePebOrTeb (TargetProcess, sizeof(PEB), (PVOID)&PebBase);

    if (!NT_SUCCESS(Status)) {
        KeDetachProcess ();
        return Status;
    }

    //
    // Initialize the Peb.  Every reference to the Peb
    // must be wrapped in try-except in case the inpage fails.  The inpage
    // can fail for any reason including network failures, disk errors,
    // low resources, etc.
    //

    try {
        PebBase->InheritedAddressSpace = InitialPeb->InheritedAddressSpace;
        PebBase->Mutant = InitialPeb->Mutant;
        PebBase->ImageBaseAddress = TargetProcess->SectionBaseAddress;

        PebBase->AnsiCodePageData = (PVOID)((PUCHAR)ViewBase+InitAnsiCodePageDataOffset);
        PebBase->OemCodePageData = (PVOID)((PUCHAR)ViewBase+InitOemCodePageDataOffset);
        PebBase->UnicodeCaseTableData = (PVOID)((PUCHAR)ViewBase+InitUnicodeCaseTableDataOffset);

        PebBase->NumberOfProcessors = KeNumberProcessors;
        PebBase->BeingDebugged = (BOOLEAN)(TargetProcess->DebugPort != NULL ? TRUE : FALSE);
        PebBase->NtGlobalFlag = NtGlobalFlag;
        PebBase->CriticalSectionTimeout = MmCriticalSectionTimeout;
        PebBase->HeapSegmentReserve = MmHeapSegmentReserve;
        PebBase->HeapSegmentCommit = MmHeapSegmentCommit;
        PebBase->HeapDeCommitTotalFreeThreshold = MmHeapDeCommitTotalFreeThreshold;
        PebBase->HeapDeCommitFreeBlockThreshold = MmHeapDeCommitFreeBlockThreshold;
        DONTASSERT (PebBase->NumberOfHeaps == 0);
        PebBase->MaximumNumberOfHeaps = (PAGE_SIZE - sizeof (PEB)) / sizeof( PVOID);
        PebBase->ProcessHeaps = (PVOID *)(PebBase+1);

        PebBase->OSMajorVersion = NtMajorVersion;
        PebBase->OSMinorVersion = NtMinorVersion;
        PebBase->OSBuildNumber = (USHORT)(NtBuildNumber & 0x3FFF);
        PebBase->OSPlatformId = 2;      // VER_PLATFORM_WIN32_NT from winbase.h
        PebBase->OSCSDVersion = (USHORT)CmNtCSDVersion;
        DONTASSERT (PebBase->pShimData == 0);
        DONTASSERT (PebBase->AppCompatFlags.QuadPart == 0);
        DONTASSERT (PebBase->ActivationContextData == NULL);
        DONTASSERT (PebBase->SystemDefaultActivationContextData == NULL);

        if (TargetProcess->Session != NULL) {
            PebBase->SessionId = MmGetSessionId (TargetProcess);
        }

        PebBase->MinimumStackCommit = (SIZE_T)MmMinimumStackCommitInBytes;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        KeDetachProcess();
        return GetExceptionCode ();
    }

    //
    // Every reference to NtHeaders (including the call to RtlImageNtHeader)
    // must be wrapped in try-except in case the inpage fails.  The inpage
    // can fail for any reason including network failures, disk errors,
    // low resources, etc.
    //

    try {
        NtHeaders = RtlImageNtHeader (PebBase->ImageBaseAddress);
        Magic = NtHeaders->OptionalHeader.Magic;
        Characteristics = NtHeaders->FileHeader.Characteristics;
    } except (EXCEPTION_EXECUTE_HANDLER) {
        KeDetachProcess();
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if (NtHeaders != NULL) {

        ProcessAffinityMask = 0;

#if defined(_WIN64)

        if (TargetProcess->Wow64Process) {

            Status = MiInitializeWowPeb (NtHeaders, PebBase, TargetProcess);

            if (!NT_SUCCESS(Status)) {
                KeDetachProcess ();
                return Status;
            }
        }
        else      // a PE32+ image
#endif
        {
            try {
                ImageConfigData = RtlImageDirectoryEntryToData (
                                        PebBase->ImageBaseAddress,
                                        TRUE,
                                        IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG,
                                        &ReturnedSize);

                ProbeForReadSmallStructure ((PVOID)ImageConfigData,
                                            sizeof (*ImageConfigData),
                                            PROBE_ALIGNMENT (IMAGE_LOAD_CONFIG_DIRECTORY));

                MI_INIT_PEB_FROM_IMAGE(NtHeaders, ImageConfigData);

                if (ImageConfigData != NULL && ImageConfigData->ProcessAffinityMask != 0) {
                    ProcessAffinityMask = ImageConfigData->ProcessAffinityMask;
                }

            } except (EXCEPTION_EXECUTE_HANDLER) {
                KeDetachProcess();
                return STATUS_INVALID_IMAGE_PROTECT;
            }

        }

        //
        // Note NT4 examined the NtHeaders->FileHeader.Characteristics
        // for the IMAGE_FILE_AGGRESIVE_WS_TRIM bit, but this is not needed
        // or used for NT5 and above.
        //

        //
        // See if image wants to override the default processor affinity mask.
        //

        try {

            if (Characteristics & IMAGE_FILE_UP_SYSTEM_ONLY) {

                //
                // Image is NOT MP safe.  Assign it a processor on a rotating
                // basis to spread these processes around on MP systems.
                //

                do {
                    PebBase->ImageProcessAffinityMask = (KAFFINITY)(0x1 << MmRotatingUniprocessorNumber);
                    if (++MmRotatingUniprocessorNumber >= KeNumberProcessors) {
                        MmRotatingUniprocessorNumber = 0;
                    }
                } while ((PebBase->ImageProcessAffinityMask & KeActiveProcessors) == 0);
            }
            else {

                if (ProcessAffinityMask != 0) {

                    //
                    // Pass the affinity mask from the image header
                    // to LdrpInitializeProcess via the PEB.
                    //

                    PebBase->ImageProcessAffinityMask = ProcessAffinityMask;
                }
            }
        } except (EXCEPTION_EXECUTE_HANDLER) {
            KeDetachProcess();
            return STATUS_INVALID_IMAGE_PROTECT;
        }
    }

    KeDetachProcess();

    *Base = PebBase;

    return STATUS_SUCCESS;
}

VOID
MmDeleteTeb (
    IN PEPROCESS TargetProcess,
    IN PVOID TebBase
    )

/*++

Routine Description:

    This routine deletes a TEB page within the target process.

Arguments:

    TargetProcess - Supplies a pointer to the process in which to delete
                    the TEB.

    TebBase - Supplies the base address of the TEB to delete.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID EndingAddress;
    PMMVAD_LONG Vad;
    NTSTATUS Status;
    PMMSECURE_ENTRY Secure;
    PMMVAD PreviousVad;
    PMMVAD NextVad;

    EndingAddress = ((PCHAR)TebBase +
                                ROUND_TO_PAGES (sizeof(TEB)) - 1);

#if defined(_WIN64)
    if (TargetProcess->Wow64Process) {
        EndingAddress = ((PCHAR)EndingAddress + ROUND_TO_PAGES (sizeof(TEB32)));
    }
#endif

    //
    // Attach to the specified process.
    //

    KeAttachProcess (&TargetProcess->Pcb);

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    Vad = (PMMVAD_LONG) MiLocateAddress (TebBase);

    ASSERT (Vad != NULL);

    ASSERT ((Vad->StartingVpn == MI_VA_TO_VPN (TebBase)) &&
            (Vad->EndingVpn == MI_VA_TO_VPN (EndingAddress)));

#if defined(_MIALT4K_)
    ASSERT (Vad->AliasInformation == NULL);
#endif
    //
    // If someone has secured the TEB (in addition to the standard securing
    // that was done by memory management on creation, then don't delete it
    // now - just leave it around until the entire process is deleted.
    //

    ASSERT (Vad->u.VadFlags.NoChange == 1);
    if (Vad->u2.VadFlags2.OneSecured) {
        Status = STATUS_SUCCESS;
    }
    else {
        ASSERT (Vad->u2.VadFlags2.MultipleSecured);
        ASSERT (IsListEmpty (&Vad->u3.List) == 0);

        //
        // If there's only one entry, then that's the one we defined when we
        // initially created the TEB.  So TEB deletion can take place right
        // now.  If there's more than one entry, let the TEB sit around until
        // the process goes away.
        //

        Secure = CONTAINING_RECORD (Vad->u3.List.Flink,
                                    MMSECURE_ENTRY,
                                    List);

        if (Secure->List.Flink == &Vad->u3.List) {
            Status = STATUS_SUCCESS;
        }
        else {
            Status = STATUS_NOT_FOUND;
        }
    }

    if (NT_SUCCESS(Status)) {

        PreviousVad = MiGetPreviousVad (Vad);
        NextVad = MiGetNextVad (Vad);

        LOCK_WS_UNSAFE (TargetProcess);
        MiRemoveVad ((PMMVAD)Vad);

        //
        // Return commitment for page table pages and clear VAD bitmaps
        // if possible.
        //

        MiReturnPageTablePageCommitment (TebBase,
                                         EndingAddress,
                                         TargetProcess,
                                         PreviousVad,
                                         NextVad);

        MiDeleteFreeVm (TebBase, EndingAddress);
        UNLOCK_WS_AND_ADDRESS_SPACE (TargetProcess);
        ExFreePool (Vad);
    }
    else {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
    }

    KeDetachProcess();
}

VOID
MmAllowWorkingSetExpansion (
    VOID
    )

/*++

Routine Description:

    This routine updates the working set list head FLINK field to
    indicate that working set adjustment is allowed.

    NOTE: This routine may be called more than once per process.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{

    PEPROCESS CurrentProcess;
    KIRQL OldIrql;

    //
    // Check the current state of the working set adjustment flag
    // in the process header.
    //

    CurrentProcess = PsGetCurrentProcess();

    LOCK_EXPANSION (OldIrql);

    if (!CurrentProcess->Vm.Flags.AllowWorkingSetAdjustment) {
        CurrentProcess->Vm.Flags.AllowWorkingSetAdjustment = TRUE;

        InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                        &CurrentProcess->Vm.WorkingSetExpansionLinks);
    }

    UNLOCK_EXPANSION (OldIrql);
    return;
}

#if DBG
ULONG MiDeleteLocked;
#endif


VOID
MiDeleteAddressesInWorkingSet (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine deletes all user mode addresses from the working set
    list.

Arguments:

    Process = Pointer to the current process.

Return Value:

    None.

Environment:

    Kernel mode, Working Set Lock held.

--*/

{
    PMMWSLE Wsle;
    WSLE_NUMBER index;
    WSLE_NUMBER Entry;
    PVOID Va;
#if DBG
    PVOID SwapVa;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMWSLE LastWsle;
#endif

    //
    // Go through the working set and for any user-accessible page which is
    // in it, rip it out of the working set and free the page.
    //

    index = 2;
    Wsle = &MmWsle[index];

    MmWorkingSetList->HashTable = NULL;

    //
    // Go through the working set list and remove all pages for user
    // space addresses.
    //

    while (index <= MmWorkingSetList->LastEntry) {
        if (Wsle->u1.e1.Valid == 1) {

#if (_MI_PAGING_LEVELS >= 4)
            ASSERT(MiGetPxeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
#if (_MI_PAGING_LEVELS >= 3)
            ASSERT(MiGetPpeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
            ASSERT(MiGetPdeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
            ASSERT(MiGetPteAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);

            if (Wsle->u1.VirtualAddress < (PVOID)MM_HIGHEST_USER_ADDRESS) {

                //
                // This is a user mode address, for each one we remove we must
                // maintain the NonDirectCount.  This is because we may fault
                // later for page tables and need to grow the hash table when
                // updating the working set.  NonDirectCount needs to be correct
                // at that point.
                //

                if (Wsle->u1.e1.Direct == 0) {
                    Process->Vm.VmWorkingSetList->NonDirectCount -= 1;
                }

                //
                // This entry is in the working set list.
                //

                Va = Wsle->u1.VirtualAddress;

                MiReleaseWsle (index, &Process->Vm);

                MiDeleteValidAddress (Va, Process);

                if (index < MmWorkingSetList->FirstDynamic) {

                    //
                    // This entry is locked.
                    //

                    MmWorkingSetList->FirstDynamic -= 1;

                    if (index != MmWorkingSetList->FirstDynamic) {

                        Entry = MmWorkingSetList->FirstDynamic;
#if DBG
                        MiDeleteLocked += 1;
                        SwapVa = MmWsle[MmWorkingSetList->FirstDynamic].u1.VirtualAddress;
                        SwapVa = PAGE_ALIGN (SwapVa);

                        PointerPte = MiGetPteAddress (SwapVa);
                        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

                        ASSERT (Entry == MiLocateWsle (SwapVa, MmWorkingSetList, Pfn1->u1.WsIndex));
#endif
                        MiSwapWslEntries (Entry, index, &Process->Vm);
                    }
                }
            }
        }
        index += 1;
        Wsle += 1;
    }

#if DBG
    Wsle = &MmWsle[2];
    LastWsle = &MmWsle[MmWorkingSetList->LastInitializedWsle];
    while (Wsle <= LastWsle) {
        if (Wsle->u1.e1.Valid == 1) {
#if (_MI_PAGING_LEVELS >= 4)
            ASSERT(MiGetPxeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
#if (_MI_PAGING_LEVELS >= 3)
            ASSERT(MiGetPpeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
#endif
            ASSERT(MiGetPdeAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
            ASSERT(MiGetPteAddress(Wsle->u1.VirtualAddress)->u.Hard.Valid == 1);
        }
        Wsle += 1;
    }
#endif

}


VOID
MiDeleteValidAddress (
    IN PVOID Va,
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine deletes the specified virtual address.

Arguments:

    Va - Supplies the virtual address to delete.

    CurrentProcess - Supplies the current process.

Return Value:

    None.

Environment:

    Kernel mode.  PFN LOCK HELD.

    Note since this is only called during process teardown, the write watch
    bits are not updated.  If this ever called from other places, code
    will need to be added here to update those bits.

--*/

{
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;

    CloneDescriptor = NULL;

    //
    // Initializing CloneBlock is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    CloneBlock = NULL;

    PointerPte = MiGetPteAddress (Va);

    LOCK_PFN (OldIrql);
#if (_MI_PAGING_LEVELS >= 4)
    ASSERT(MiGetPxeAddress(Va)->u.Hard.Valid == 1);
#endif
#if (_MI_PAGING_LEVELS >= 3)
    ASSERT(MiGetPpeAddress(Va)->u.Hard.Valid == 1);
#endif
    ASSERT(MiGetPdeAddress(Va)->u.Hard.Valid == 1);
    ASSERT (PointerPte->u.Hard.Valid == 1);

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (Pfn1->u3.e1.PrototypePte == 1) {

        CloneBlock = (PMMCLONE_BLOCK)Pfn1->PteAddress;

        //
        // Capture the state of the modified bit for this PTE.
        //

        MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

        //
        // Decrement the share and valid counts of the page table
        // page which maps this PTE.
        //

        PointerPde = MiGetPteAddress (PointerPte);

        PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        //
        // Decrement the share count for the physical page.
        //

        MiDecrementShareCount (PageFrameIndex);

        //
        // Check to see if this is a fork prototype PTE and if so
        // update the clone descriptor address.
        //

        if (Va <= MM_HIGHEST_USER_ADDRESS) {

            //
            // Locate the clone descriptor within the clone tree.
            //

            CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);
        }
    }
    else {

        //
        // This PTE is a NOT a prototype PTE, delete the physical page.
        //

        //
        // Decrement the share and valid counts of the page table
        // page which maps this PTE.
        //

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        //
        // Decrement the share count for the physical page.  As the page
        // is private it will be put on the free list.
        //

        MiDecrementShareCountOnly (PageFrameIndex);

        //
        // Decrement the count for the number of private pages.
        //

        CurrentProcess->NumberOfPrivatePages -= 1;
    }

    //
    // Set the pointer to PTE to be a demand zero PTE.  This allows
    // the page usage count to be kept properly and handles the case
    // when a page table page has only valid PTEs and needs to be
    // deleted later when the VADs are removed.
    //

    PointerPte->u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    if (CloneDescriptor != NULL) {

        //
        // Decrement the reference count for the clone block,
        // note that this could release and reacquire
        // the mutexes hence cannot be done until after the
        // working set index has been removed.
        //

        if (MiDecrementCloneBlockReference (CloneDescriptor,
                                            CloneBlock,
                                            CurrentProcess)) {

        }
    }

    UNLOCK_PFN (OldIrql);
}

PFN_NUMBER
MiMakeOutswappedPageResident (
    IN PMMPTE ActualPteAddress,
    IN OUT PMMPTE PointerTempPte,
    IN ULONG Global,
    IN PFN_NUMBER ContainingPage
    )

/*++

Routine Description:

    This routine makes the specified PTE valid.

Arguments:

    ActualPteAddress - Supplies the actual address that the PTE will
                       reside at.  This is used for page coloring.

    PointerTempPte - Supplies the PTE to operate on, returns a valid
                     PTE.

    Global - Supplies 1 if the resulting PTE is global.

    ContainingPage - Supplies the physical page number of the page which
                     contains the resulting PTE.  If this value is 0, no
                     operations on the containing page are performed.

Return Value:

    Returns the physical page number that was allocated for the PTE.

Environment:

    Kernel mode, PFN LOCK HELD!

--*/

{
    MMPTE TempPte;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PMDL Mdl;
    LARGE_INTEGER StartingOffset;
    KEVENT Event;
    IO_STATUS_BLOCK IoStatus;
    PFN_NUMBER PageFileNumber;
    NTSTATUS Status;
    PPFN_NUMBER Page;
    ULONG RefaultCount;
#if DBG
    PVOID HyperVa;
    PEPROCESS CurrentProcess;
#endif

    MM_PFN_LOCK_ASSERT();

#if defined (_IA64_)
    UNREFERENCED_PARAMETER (Global);
#endif

restart:

    OldIrql = APC_LEVEL;

    ASSERT (PointerTempPte->u.Hard.Valid == 0);

    if (PointerTempPte->u.Long == MM_KERNEL_DEMAND_ZERO_PTE) {

        //
        // Any page will do.
        //

        MiEnsureAvailablePageOrWait (NULL, NULL);
        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_FROM_PTE (ActualPteAddress));

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           ActualPteAddress);
        MI_SET_PTE_DIRTY (TempPte);
        MI_SET_GLOBAL_STATE (TempPte, Global);

        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);
        MiInitializePfnForOtherProcess (PageFrameIndex,
                                        ActualPteAddress,
                                        ContainingPage);

    }
    else if (PointerTempPte->u.Soft.Transition == 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerTempPte);
        PointerTempPte->u.Trans.Protection = MM_READWRITE;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        if ((MmAvailablePages == 0) ||
            ((Pfn1->u4.InPageError == 1) && (Pfn1->u3.e1.ReadInProgress == 1))) {

            //
            // This can only happen if the system is utilizing a hardware
            // compression cache.  This ensures that only a safe amount
            // of the compressed virtual cache is directly mapped so that
            // if the hardware gets into trouble, we can bail it out.
            //

            UNLOCK_PFN (OldIrql);

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmHalfSecond);
            LOCK_PFN (OldIrql);
            goto restart;
        }

        //
        // PTE refers to a transition PTE.
        //

        if (Pfn1->u3.e1.PageLocation != ActiveAndValid) {
            MiUnlinkPageFromList (Pfn1);

            //
            // Even though this routine is only used to bring in special
            // system pages that are separately charged, a modified write
            // may be in progress and if so, will have applied a systemwide
            // charge against the locked pages count.  This all works out nicely
            // (with no code needed here) as the write completion will see
            // the nonzero ShareCount and remove the charge.
            //

            ASSERT ((Pfn1->u3.e2.ReferenceCount == 0) ||
                    (Pfn1->u3.e1.LockCharged == 1));

            Pfn1->u3.e2.ReferenceCount += 1;
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
        }

        //
        // Update the PFN database, the share count is now 1 and
        // the reference count is incremented as the share count
        // just went from zero to 1.
        //

        Pfn1->u2.ShareCount += 1;

        MI_SET_MODIFIED (Pfn1, 1, 0x12);

        if (Pfn1->u3.e1.WriteInProgress == 0) {

            //
            // Release the page file space for this page.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
        }

        MI_MAKE_TRANSITION_PTE_VALID (TempPte, PointerTempPte);

        MI_SET_PTE_DIRTY (TempPte);
        MI_SET_GLOBAL_STATE (TempPte, Global);
        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);

    }
    else {

        //
        // Page resides in a paging file.
        // Any page will do.
        //

        MiEnsureAvailablePageOrWait (NULL, NULL);
        PageFrameIndex = MiRemoveAnyPage (
                            MI_GET_PAGE_COLOR_FROM_PTE (ActualPteAddress));

        //
        // Initialize the PFN database element, but don't
        // set read in progress as collided page faults cannot
        // occur here.
        //

        MiInitializePfnForOtherProcess (PageFrameIndex,
                                        ActualPteAddress,
                                        ContainingPage);

        UNLOCK_PFN (OldIrql);

        PointerTempPte->u.Soft.Protection = MM_READWRITE;

        KeInitializeEvent (&Event, NotificationEvent, FALSE);

        //
        // Calculate the VPN for the in-page operation.
        //

        TempPte = *PointerTempPte;
        PageFileNumber = GET_PAGING_FILE_NUMBER (TempPte);

        StartingOffset.QuadPart = (LONGLONG)(GET_PAGING_FILE_OFFSET (TempPte)) <<
                                    PAGE_SHIFT;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Build MDL for request.
        //

        Mdl = (PMDL)&MdlHack[0];
        MmInitializeMdl (Mdl,
                         MiGetVirtualAddressMappedByPte (ActualPteAddress),
                         PAGE_SIZE);
        Mdl->MdlFlags |= MDL_PAGES_LOCKED;

        Page = (PPFN_NUMBER)(Mdl + 1);
        *Page = PageFrameIndex;

#if DBG
        CurrentProcess = PsGetCurrentProcess ();

        HyperVa = MiMapPageInHyperSpace (CurrentProcess, PageFrameIndex, &OldIrql);
        RtlFillMemoryUlong (HyperVa,
                            PAGE_SIZE,
                            0x34785690);
        MiUnmapPageInHyperSpace (CurrentProcess, HyperVa, OldIrql);
#endif

        //
        // Issue the read request.
        //

        RefaultCount = 0;

Refault:
        Status = IoPageRead (MmPagingFile[PageFileNumber]->File,
                             Mdl,
                             &StartingOffset,
                             &Event,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject (&Event,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)NULL);
            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (NT_SUCCESS(Status)) {
            if (IoStatus.Information != PAGE_SIZE) {
                KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                              2,
                              IoStatus.Status,
                              PageFileNumber,
                              StartingOffset.LowPart);
            }
        }

        if ((!NT_SUCCESS(Status)) || (!NT_SUCCESS(IoStatus.Status))) {

            if ((MmIsRetryIoStatus (Status)) ||
                (MmIsRetryIoStatus (IoStatus.Status))) {
                    
                RefaultCount -= 1;

                if (RefaultCount & MiFaultRetryMask) {

                    //
                    // Insufficient resources, delay and reissue
                    // the in page operation.
                    //

                    KeDelayExecutionThread (KernelMode,
                                            FALSE,
                                            (PLARGE_INTEGER)&MmHalfSecond);
                    KeClearEvent (&Event);
                    RefaultCount -= 1;
                    goto Refault;
                }
            }
            KeBugCheckEx (KERNEL_STACK_INPAGE_ERROR,
                          Status,
                          IoStatus.Status,
                          PageFileNumber,
                          StartingOffset.LowPart);
        }

        LOCK_PFN (OldIrql);

        //
        // Release the page file space.
        //

        MiReleasePageFileSpace (TempPte);
        Pfn1->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           ActualPteAddress);
        MI_SET_PTE_DIRTY (TempPte);

        MI_SET_MODIFIED (Pfn1, 1, 0x13);

        MI_SET_GLOBAL_STATE (TempPte, Global);

        MI_WRITE_VALID_PTE (PointerTempPte, TempPte);
    }
    return PageFrameIndex;
}


UCHAR
MiSetMemoryPriorityProcess (
    IN PEPROCESS Process,
    IN UCHAR MemoryPriority
    )

/*++

Routine Description:

    Nonpaged wrapper to set the memory priority of a process.

Arguments:

    Process - Supplies the process to update.

    MemoryPriority - Supplies the new memory priority of the process.

Return Value:

    Old priority.

--*/

{
    KIRQL OldIrql;
    UCHAR OldPriority;

    LOCK_EXPANSION (OldIrql);

    OldPriority = (UCHAR) Process->Vm.Flags.MemoryPriority;
    Process->Vm.Flags.MemoryPriority = MemoryPriority;

    UNLOCK_EXPANSION (OldIrql);

    return OldPriority;
}

VOID
MmSetMemoryPriorityProcess (
    IN PEPROCESS Process,
    IN UCHAR MemoryPriority
    )

/*++

Routine Description:

    Sets the memory priority of a process.

Arguments:

    Process - Supplies the process to update

    MemoryPriority - Supplies the new memory priority of the process

Return Value:

    None.

--*/

{
    if (MmSystemSize == MmSmallSystem && MmNumberOfPhysicalPages < ((15*1024*1024)/PAGE_SIZE)) {

        //
        // If this is a small system, make every process BACKGROUND.
        //

        MemoryPriority = MEMORY_PRIORITY_BACKGROUND;
    }

    MiSetMemoryPriorityProcess (Process, MemoryPriority);

    return;
}


PMMVAD
MiAllocateVad (
    IN ULONG_PTR StartingVirtualAddress,
    IN ULONG_PTR EndingVirtualAddress,
    IN LOGICAL Deletable
    )

/*++

Routine Description:

    Reserve the specified range of address space.

Arguments:

    StartingVirtualAddress - Supplies the starting virtual address.

    EndingVirtualAddress - Supplies the ending virtual address.

    Deletable - Supplies TRUE if the VAD is to be marked as deletable, FALSE
                if deletions of this VAD should be disallowed.

Return Value:

    A VAD pointer on success, NULL on failure.

--*/

{
    PMMVAD_LONG Vad;

    ASSERT (StartingVirtualAddress <= EndingVirtualAddress);

    if (Deletable == TRUE) {
        Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_SHORT), 'SdaV');
    }
    else {
        Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');
    }

    if (Vad == NULL) {
       return NULL;
    }

    //
    // Set the starting and ending virtual page numbers of the VAD.
    //

    Vad->StartingVpn = MI_VA_TO_VPN (StartingVirtualAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingVirtualAddress);

    //
    // Mark VAD as no commitment, private, and readonly.
    //

    Vad->u.LongFlags = 0;
    Vad->u.VadFlags.CommitCharge = MM_MAX_COMMIT;
    Vad->u.VadFlags.Protection = MM_READONLY;
    Vad->u.VadFlags.PrivateMemory = 1;

    if (Deletable == TRUE) {
        ASSERT (Vad->u.VadFlags.NoChange == 0);
    }
    else {
        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.LongFlags2 = 0;
        Vad->u2.VadFlags2.OneSecured = 1;
        Vad->u2.VadFlags2.LongVad = 1;
        Vad->u2.VadFlags2.ReadOnly = 1;
        Vad->u3.Secured.StartVpn = StartingVirtualAddress;
        Vad->u3.Secured.EndVpn = EndingVirtualAddress;
#if defined(_MIALT4K_)
        Vad->AliasInformation = NULL;
#endif
    }

    return (PMMVAD) Vad;
}

#if 0
VOID
MiVerifyReferenceCounts (
    IN ULONG PdePage
    )

    //
    // Verify the share and valid PTE counts for page directory page.
    //

{
    PMMPFN Pfn1;
    PMMPFN Pfn3;
    PMMPTE Pte1;
    ULONG Share = 0;
    ULONG Valid = 0;
    ULONG i, ix, iy;
    PMMPTE PageDirectoryMap;
    KIRQL OldIrql;
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();
    PageDirectoryMap = (PMMPTE)MiMapPageInHyperSpace (Process, PdePage, &OldIrql);
    Pfn1 = MI_PFN_ELEMENT (PdePage);
    Pte1 = (PMMPTE)PageDirectoryMap;

    //
    // Map in the non paged portion of the system.
    //

    ix = MiGetPdeOffset(CODE_START);

    for (i = 0;i < ix; i += 1) {
        if (Pte1->u.Hard.Valid == 1) {
            Valid += 1;
        }
        else if ((Pte1->u.Soft.Prototype == 0) &&
                   (Pte1->u.Soft.Transition == 1)) {
            Pfn3 = MI_PFN_ELEMENT (Pte1->u.Trans.PageFrameNumber);
            if (Pfn3->u3.e1.PageLocation == ActiveAndValid) {
                ASSERT (Pfn1->u2.ShareCount > 1);
                Valid += 1;
            }
            else {
                Share += 1;
            }
        }
        Pte1 += 1;
    }

    iy = MiGetPdeOffset(PTE_BASE);
    Pte1 = &PageDirectoryMap[iy];
    ix  = MiGetPdeOffset(HYPER_SPACE_END) + 1;

    for (i = iy; i < ix; i += 1) {
        if (Pte1->u.Hard.Valid == 1) {
            Valid += 1;
        }
        else if ((Pte1->u.Soft.Prototype == 0) &&
                   (Pte1->u.Soft.Transition == 1)) {
            Pfn3 = MI_PFN_ELEMENT (Pte1->u.Trans.PageFrameNumber);
            if (Pfn3->u3.e1.PageLocation == ActiveAndValid) {
                ASSERT (Pfn1->u2.ShareCount > 1);
                Valid += 1;
            }
            else {
                Share += 1;
            }
        }
        Pte1 += 1;
    }

    if (Pfn1->u2.ShareCount != (Share+Valid+1)) {
        DbgPrint ("MMPROCSUP - PDE page %lx ShareCount %lx found %lx\n",
                PdePage, Pfn1->u2.ShareCount, Valid+Share+1);
    }

    MiUnmapPageInHyperSpace (Process, PageDirectoryMap, OldIrql);
    ASSERT (Pfn1->u2.ShareCount == (Share+Valid+1));
    return;
}
#endif //0

PFN_NUMBER
MmGetDirectoryFrameFromProcess(
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine retrieves the PFN of the process's top pagetable page.  It can
    be used to map physical pages back to a process.

Arguments:

    Process - Supplies the process to query.

Return Value:

    Page frame number of the top level page table page.

Environment:

    Kernel mode.  No locks held.

--*/

{
    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    return MI_GET_DIRECTORY_FRAME_FROM_PROCESS(Process);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\session.c ===
/*++

Copyright (c) 1997  Microsoft Corporation

Module Name:

   session.c

Abstract:

    This module contains the routines which implement the creation and
    deletion of session spaces along with associated support routines.

Author:

    Landy Wang (landyw) 05-Dec-1997

Revision History:

--*/

#include "mi.h"

ULONG MiSessionCount;

LONG MmSessionDataPages;

#define MM_MAXIMUM_CONCURRENT_SESSIONS  16384

FAST_MUTEX  MiSessionIdMutex;

PRTL_BITMAP MiSessionIdBitmap;

#if (_MI_PAGING_LEVELS >= 3)

#if defined(_AMD64_)
#define MI_SESSION_COMMIT_CHARGE 5

#elif defined(_IA64_)
#define MI_SESSION_COMMIT_CHARGE 5

extern REGION_MAP_INFO MmSessionMapInfo;
extern PFN_NUMBER MmSessionParentTablePage;

#else
#define MI_SESSION_COMMIT_CHARGE 4
#endif

#else
#define MI_SESSION_COMMIT_CHARGE 3
#endif

VOID
MiSessionAddProcess (
    PEPROCESS NewProcess
    );

VOID
MiSessionRemoveProcess (
    VOID
    );

VOID
MiInitializeSessionIds (
    VOID
    );

NTSTATUS
MiSessionCreateInternal (
    OUT PULONG SessionId
    );

NTSTATUS
MiSessionCommitPageTables (
    IN PVOID StartVa,
    IN PVOID EndVa
    );

VOID
MiDereferenceSession (
    VOID
    );

VOID
MiSessionDeletePde (
    IN PMMPTE Pde,
    IN LOGICAL WorkingSetInitialized,
    IN PMMPTE SelfMapPde
    );

VOID
MiDereferenceSessionFinal (
    VOID
    );

#if DBG
VOID
MiCheckSessionVirtualSpace (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    );
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSessionIds)

#pragma alloc_text(PAGE, MmSessionSetUnloadAddress)
#pragma alloc_text(PAGE, MmSessionCreate)
#pragma alloc_text(PAGE, MmSessionDelete)
#pragma alloc_text(PAGE, MmGetSessionLocaleId)
#pragma alloc_text(PAGE, MmSetSessionLocaleId)
#pragma alloc_text(PAGE, MmQuitNextSession)
#pragma alloc_text(PAGE, MiDereferenceSession)
#pragma alloc_text(PAGE, MiDetachFromSecureProcessInSession)
#if DBG
#pragma alloc_text(PAGE, MiCheckSessionVirtualSpace)
#endif

#pragma alloc_text(PAGELK, MiSessionCreateInternal)
#pragma alloc_text(PAGELK, MiDereferenceSessionFinal)

#endif

VOID
MiSessionLeader (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    Mark the argument process as having the ability to create or delete session
    spaces.  This is only granted to the session manager process.

Arguments:

    Process - Supplies a pointer to the privileged process.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;

    LOCK_EXPANSION (OldIrql);

    Process->Vm.Flags.SessionLeader = 1;

    UNLOCK_EXPANSION (OldIrql);
}


VOID
MmSessionSetUnloadAddress (
    IN PDRIVER_OBJECT pWin32KDevice
    )

/*++

Routine Description:

    Copy the win32k.sys driver object to the session structure for use during
    unload.

Arguments:

    NewProcess - Supplies a pointer to the win32k driver object.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PDRIVER_OBJECT Destination;

    ASSERT (PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION);
    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

    Destination = &MmSessionSpace->Win32KDriverObject;

    RtlCopyMemory (Destination, pWin32KDevice, sizeof(DRIVER_OBJECT));
}

VOID
MiSessionAddProcess (
    PEPROCESS NewProcess
    )

/*++

Routine Description:

    Add the new process to the current session space.

Arguments:

    NewProcess - Supplies a pointer to the process being created.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    KIRQL OldIrql;
    PMM_SESSION_SPACE SessionGlobal;

    //
    // If the calling process has no session, then the new process won't get
    // one either.
    //

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION)== 0) {
        return;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    InterlockedIncrement ((PLONG)&SessionGlobal->ReferenceCount);

    InterlockedIncrement (&SessionGlobal->ProcessReferenceToSession);

    //
    // Once the Session pointer in the EPROCESS is set it can never
    // be cleared because it is accessed lock-free.
    //

    ASSERT (NewProcess->Session == NULL);
    NewProcess->Session = (PVOID) SessionGlobal;

#if defined(_IA64_)
    KeAddSessionSpace(&NewProcess->Pcb,
                      &SessionGlobal->SessionMapInfo,
                      SessionGlobal->PageDirectoryParentPage);
#endif

    //
    // Link the process entry into the session space and WSL structures.
    //

    LOCK_EXPANSION (OldIrql);

    if (IsListEmpty(&SessionGlobal->ProcessList)) {

        if (MmSessionSpace->Vm.Flags.AllowWorkingSetAdjustment == FALSE) {

            ASSERT (MmSessionSpace->u.Flags.WorkingSetInserted == 0);

            MmSessionSpace->Vm.Flags.AllowWorkingSetAdjustment = TRUE;

            InsertTailList (&MmWorkingSetExpansionHead.ListHead,
                            &SessionGlobal->Vm.WorkingSetExpansionLinks);

            MmSessionSpace->u.Flags.WorkingSetInserted = 1;
        }
    }

    InsertTailList (&SessionGlobal->ProcessList, &NewProcess->SessionProcessLinks);
    UNLOCK_EXPANSION (OldIrql);

    PS_SET_BITS (&NewProcess->Flags, PS_PROCESS_FLAGS_IN_SESSION);
}


VOID
MiSessionRemoveProcess (
    VOID
    )

/*++

Routine Description:

    This routine removes the current process from the current session space.
    This may trigger a substantial round of dereferencing and resource freeing
    if it is also the last process in the session, (holding the last image
    in the group, etc).

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL and below, but queueing of APCs to this thread has
    been permanently disabled.  This is the last thread in the process
    being deleted.  The caller has ensured that this process is not
    on the expansion list and therefore there can be no races in regards to
    trimming.

--*/

{
    KIRQL OldIrql;
    PEPROCESS CurrentProcess;
#if DBG
    ULONG Found;
    PEPROCESS Process;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE SessionGlobal;
#endif

    CurrentProcess = PsGetCurrentProcess();

    if (((CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) ||
        (CurrentProcess->Vm.Flags.SessionLeader == 1)) {
        return;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    //
    // Remove this process from the list of processes in the current session.
    //

    LOCK_EXPANSION (OldIrql);

#if DBG

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    Found = 0;
    NextEntry = SessionGlobal->ProcessList.Flink;

    while (NextEntry != &SessionGlobal->ProcessList) {
        Process = CONTAINING_RECORD (NextEntry, EPROCESS, SessionProcessLinks);

        if (Process == CurrentProcess) {
            Found = 1;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (Found == 1);

#endif

    RemoveEntryList (&CurrentProcess->SessionProcessLinks);

    UNLOCK_EXPANSION (OldIrql);

    //
    // Decrement this process' reference count to the session.  If this
    // is the last reference, then the entire session will be destroyed
    // upon return.  This includes unloading drivers, unmapping pools,
    // freeing page tables, etc.
    //

    MiDereferenceSession ();
}

LCID
MmGetSessionLocaleId (
    VOID
    )

/*++

Routine Description:

    This routine gets the locale ID for the current session.

Arguments:

    None.

Return Value:

    The locale ID for the current session.

Environment:

    PASSIVE_LEVEL, the caller must supply any desired synchronization.

--*/

{
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    PAGED_CODE ();

    Process = PsGetCurrentProcess ();

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        return PsDefaultThreadLocaleId;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        return PsDefaultThreadLocaleId;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    return SessionGlobal->LocaleId;
}

VOID
MmSetSessionLocaleId (
    IN LCID LocaleId
    )

/*++

Routine Description:

    This routine sets the locale ID for the current session.

Arguments:

    LocaleId - Supplies the desired locale ID.

Return Value:

    None.

Environment:

    PASSIVE_LEVEL, the caller must supply any desired synchronization.

--*/

{
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    PAGED_CODE ();

    Process = PsGetCurrentProcess ();

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        PsDefaultThreadLocaleId = LocaleId;
        return;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        PsDefaultThreadLocaleId = LocaleId;
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    SessionGlobal->LocaleId = LocaleId;
}

VOID
MiInitializeSessionIds (
    VOID
    )

/*++

Routine Description:

    This routine creates and initializes session ID allocation/deallocation.

Arguments:

    None.

Return Value:

    None.

--*/

{
    //
    // If this ever grows beyond 2x the page size, both the allocation and
    // deletion code will need to be updated.
    //

    ASSERT (sizeof(MM_SESSION_SPACE) <= 2 * PAGE_SIZE);

    ExInitializeFastMutex (&MiSessionIdMutex);

    MiCreateBitMap (&MiSessionIdBitmap,
                    MM_MAXIMUM_CONCURRENT_SESSIONS,
                    PagedPool);

    if (MiSessionIdBitmap == NULL) {
        MiCreateBitMap (&MiSessionIdBitmap,
                        MM_MAXIMUM_CONCURRENT_SESSIONS,
                        NonPagedPoolMustSucceed);
    }

    RtlClearAllBits (MiSessionIdBitmap);
}

NTSTATUS
MiSessionCreateInternal (
    OUT PULONG SessionId
    )

/*++

Routine Description:

    This routine creates the data structure that describes and maintains
    the session space.  It resides at the beginning of the session space.
    Carefully construct the first page mapping to bootstrap the fault
    handler which relies on the session space data structure being
    present and valid.

    In NT32, this initial mapping for the portion of session space
    mapped by the first PDE will automatically be inherited by all child
    processes when the system copies the system portion of the page
    directory for new address spaces.  Additional entries are faulted
    in by the session space fault handler, which references this structure.

    For NT64, everything is automatically inherited.

    This routine commits virtual memory within the current session space with
    backing pages.  The virtual addresses within session space are
    allocated with a separate facility in the image management facility.
    This is because images must be at a unique system wide virtual address.

Arguments:

    SessionId - Supplies a pointer to place the new session ID into.

Return Value:

    STATUS_SUCCESS if all went well, various failure status codes
    if the session was not created.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    KIRQL  OldIrql;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE GlobalMappingPte;
    NTSTATUS Status;
    PMM_SESSION_SPACE SessionSpace;
    PMM_SESSION_SPACE SessionGlobal;
    PFN_NUMBER ResidentPages;
    LOGICAL GotCommit;
    LOGICAL GotPages;
    LOGICAL GotUnmappedDataPage;
    LOGICAL PoolInitialized;
    PMMPFN Pfn1;
    MMPTE TempPte;
    MMPTE TempPte2;
    ULONG_PTR Va;
    PFN_NUMBER DataPage;
    PFN_NUMBER DataPage2;
    PFN_NUMBER PageTablePage;
    ULONG PageColor;
    ULONG ProcessFlags;
    ULONG NewProcessFlags;
    PEPROCESS Process;
#if (_MI_PAGING_LEVELS < 3)
    SIZE_T PageTableBytes;
    PMMPTE PageTables;
#else
    PMMPTE PointerPpe;
    PFN_NUMBER PageDirectoryPage;
    PFN_NUMBER PageDirectoryParentPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    GotCommit = FALSE;
    GotPages = FALSE;
    GotUnmappedDataPage = FALSE;
    GlobalMappingPte = NULL;
    PoolInitialized = FALSE;

    //
    // Initializing these are not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    DataPage = 0;
    DataPage2 = 0;
    PageTablePage = 0;
    PointerPte = NULL;

#if (_MI_PAGING_LEVELS >= 3)
    PageDirectoryPage = 0;
    PageDirectoryParentPage = 0;
#endif

    Process = PsGetCurrentProcess();

#if defined(_IA64_)
    ASSERT (MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&Process->Pcb.SessionParentBase)) == MmSessionParentTablePage);
#else
    ASSERT (MmIsAddressValid(MmSessionSpace) == FALSE);
#endif



    //
    // Check for concurrent session creation attempts.
    //


    ProcessFlags = Process->Flags;

    while (TRUE) {

        if (ProcessFlags & PS_PROCESS_FLAGS_CREATING_SESSION) {
            return STATUS_ALREADY_COMMITTED;
        }

        NewProcessFlags = (ProcessFlags | PS_PROCESS_FLAGS_CREATING_SESSION);

        NewProcessFlags = InterlockedCompareExchange ((PLONG)&Process->Flags,
                                                      (LONG)NewProcessFlags,
                                                      (LONG)ProcessFlags);
                                                             
        if (NewProcessFlags == ProcessFlags) {
            break;
        }

        //
        // The structure changed beneath us.  Use the return value from the
        // exchange and try it all again.
        //

        ProcessFlags = NewProcessFlags;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);

#if (_MI_PAGING_LEVELS < 3)

    PageTableBytes = MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES * sizeof (MMPTE);

    PageTables = (PMMPTE) ExAllocatePoolWithTag (NonPagedPool,
                                                 PageTableBytes,
                                                 'tHmM');

    if (PageTables == NULL) {

        ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (PageTables, PageTableBytes);

#endif

    //
    // Select a free session ID.
    //



    ExAcquireFastMutex (&MiSessionIdMutex);

    *SessionId = RtlFindClearBitsAndSet (MiSessionIdBitmap, 1, 0);

    ExReleaseFastMutex (&MiSessionIdMutex);

    if (*SessionId == NO_BITS_FOUND) {

        ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

#if (_MI_PAGING_LEVELS < 3)
        ExFreePool (PageTables);
#endif

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_IDS);
        return STATUS_NO_MEMORY;
    }


    //
    // Lock down this routine in preparation for the PFN lock acquisition.
    // Note this is done prior to the commitment charges just to simplify
    // error handling.
    //

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Charge commitment.
    //


    ResidentPages = MI_SESSION_COMMIT_CHARGE;

    if (MiChargeCommitment (ResidentPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        goto Failure;
    }

    GotCommit = TRUE;

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_CREATE, ResidentPages);



    //
    // Reserve global system PTEs to map the data pages with.
    //



    GlobalMappingPte = MiReserveSystemPtes (2, SystemPteSpace);

    if (GlobalMappingPte == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_SYSPTES);
        goto Failure;
    }




    //
    // Ensure the resident physical pages are available.
    //



    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)(ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM) > MI_NONPAGABLE_MEMORY_AVAILABLE()) {

        UNLOCK_PFN (OldIrql);

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        goto Failure;
    }

    GotPages = TRUE;

    MmResidentAvailablePages -= (ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM);

    MM_BUMP_COUNTER(40, ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM);



    //
    // Allocate both session space data pages first as on some architectures
    // a region ID will be used immediately for the TB references as the
    // PTE mappings are initialized.
    //



    TempPte.u.Long = ValidKernelPte.u.Long;

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    DataPage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    TempPte.u.Hard.PageFrameNumber = DataPage;



    TempPte2.u.Long = ValidKernelPte.u.Long;

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    DataPage2 = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    TempPte2.u.Hard.PageFrameNumber = DataPage2;

    GotUnmappedDataPage = TRUE;



    //
    // Map the data pages immediately in global space.  Some architectures
    // use a region ID which is used immediately for the TB references after
    // the PTE mappings are initialized.
    //
    //
    // The global bit can be left on for the global mappings (unlike the
    // session space mapping which must be have the global bit off since
    // we need to make sure the TB entry is flushed when we switch to
    // a process in a different session space).
    //

    MI_WRITE_VALID_PTE (GlobalMappingPte, TempPte);
    MI_WRITE_VALID_PTE (GlobalMappingPte + 1, TempPte2);

    SessionGlobal = (PMM_SESSION_SPACE) MiGetVirtualAddressMappedByPte (GlobalMappingPte);




#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initialize the page directory parent page.
    //

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageDirectoryParentPage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryParentPage;

#if defined(_IA64_)

    ASSERT (MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&Process->Pcb.SessionParentBase)) == MmSessionParentTablePage);


    //
    // In order to prevent races with threads on other processors context
    // switching into this process, initialize the region registers and
    // translation registers stored in the KPROCESS now.  Otherwise
    // access to the top level parent can go away which would be fatal.
    //
    // Note this could not be done until both the top level page and the
    // session data page were acquired.
    //
    // The top level session entry is mapped with a translation register.
    // Replacing a current TR entry requires a purge first if the virtual
    // address and RID are the same in the new and old entries.
    //

    KeEnableSessionSharing (&SessionGlobal->SessionMapInfo,
                            PageDirectoryParentPage);

    //
    // Install the selfmap entry for this session space.
    //

    PointerPpe = KSEG_ADDRESS (PageDirectoryParentPage);

    PointerPpe[MiGetPpeOffset(PDE_STBASE)] = TempPte;

    PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);

    MiInitializePfnForOtherProcess (PageDirectoryParentPage, PointerPpe, 0);

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryParentPage);
    Pfn1->u4.PteFrame = PageDirectoryParentPage;

#else

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryParentPage;

    PointerPxe = MiGetPxeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPxe->u.Long == 0);

    *PointerPxe = TempPte;

    //
    // Do not reference the top level parent page as it belongs to the
    // current process (SMSS).
    //

    MiInitializePfnForOtherProcess (PageDirectoryParentPage, PointerPxe, 0);

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryParentPage);
    Pfn1->u4.PteFrame = 0;

    KeFillEntryTb ((PHARDWARE_PTE) PointerPxe,
                   MiGetVirtualAddressMappedByPte (PointerPxe),
                   FALSE);
#endif

    ASSERT (MI_PFN_ELEMENT(PageDirectoryParentPage)->u1.WsIndex == 0);

    //
    // Initialize the page directory page.
    //

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageDirectoryPage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageDirectoryPage;

    PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPpe->u.Long == 0);

    *PointerPpe = TempPte;

#if defined(_IA64_)
    //
    // IA64 can reference the top level parent page here because a unique
    // one is allocated per process.
    //
    MiInitializePfnForOtherProcess (PageDirectoryPage, PointerPpe, PageDirectoryParentPage);
#else
    //
    // Do not reference the top level parent page as it belongs to the
    // current process (SMSS).
    //
    MiInitializePfnForOtherProcess (PageDirectoryPage, PointerPpe, 0);
    Pfn1 = MI_PFN_ELEMENT (PageDirectoryPage);
    Pfn1->u4.PteFrame = 0;
#endif

    ASSERT (MI_PFN_ELEMENT(PageDirectoryPage)->u1.WsIndex == 0);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPpe,
                   MiGetVirtualAddressMappedByPte (PointerPpe),
                   FALSE);
#endif

    //
    // Initialize the page table page.
    //

    MiEnsureAvailablePageOrWait (NULL, NULL);

    PageColor = MI_GET_PAGE_COLOR_FROM_VA (NULL);

    PageTablePage = MiRemoveZeroPageMayReleaseLocks (PageColor, OldIrql);

    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPdeLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = PageTablePage;

    PointerPde = MiGetPdeAddress ((PVOID)MmSessionSpace);

    ASSERT (PointerPde->u.Long == 0);

    MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
    MiInitializePfnForOtherProcess (PageTablePage, PointerPde, PageDirectoryPage);
#else
    //
    // This page frame references itself instead of the current (SMSS.EXE)
    // page directory as its PteFrame.  This allows the current process to
    // appear more normal (at least on 32-bit NT).  It just means we have
    // to treat this page specially during teardown.
    //

    MiInitializePfnForOtherProcess (PageTablePage, PointerPde, PageTablePage);
#endif

    //
    // This page is never paged, ensure that its WsIndex stays clear so the
    // release of the page is handled correctly.
    //

    ASSERT (MI_PFN_ELEMENT(PageTablePage)->u1.WsIndex == 0);

    Va = (ULONG_PTR)MiGetPteAddress (MmSessionSpace);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPde, (PMMPTE)Va, FALSE);






    //
    // The global bit is masked off since we need to make sure the TB entry
    // is flushed when we switch to a process in a different session space.
    //

    TempPte.u.Long = ValidKernelPteLocal.u.Long;
    TempPte.u.Hard.PageFrameNumber = DataPage;

    PointerPte = MiGetPteAddress (MmSessionSpace);

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    MiInitializePfn (DataPage, PointerPte, 1);

    //
    // Now do the second data page.
    //

    TempPte.u.Hard.PageFrameNumber = DataPage2;

    MI_WRITE_VALID_PTE (PointerPte + 1, TempPte);

    MiInitializePfn (DataPage2, PointerPte + 1, 1);

    //
    // Now that the data page is mapped, it can be freed as full hierarchy
    // teardown in the event of any future failures encountered by this routine.
    //

    GotUnmappedDataPage = FALSE;

    ASSERT (MI_PFN_ELEMENT(DataPage)->u1.WsIndex == 0);
    ASSERT (MI_PFN_ELEMENT(DataPage2)->u1.WsIndex == 0);

    UNLOCK_PFN (OldIrql);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPte,
                   (PMMPTE)MmSessionSpace,
                   FALSE);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPte + 1,
                   (PMMPTE)((PCHAR)MmSessionSpace + PAGE_SIZE),
                   FALSE);

    //
    // Initialize the new session space data structure.
    //

    SessionSpace = MmSessionSpace;

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGETABLE_ALLOC, 1);
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_ALLOC, 1);

    SessionSpace->GlobalPteEntry = GlobalMappingPte;
    SessionSpace->GlobalVirtualAddress = SessionGlobal;

#if defined(_IA64_)
    SessionSpace->PageDirectoryParentPage = PageDirectoryParentPage;
#endif
    SessionSpace->ReferenceCount = 1;
    SessionSpace->u.LongFlags = 0;
    SessionSpace->SessionId = *SessionId;
    SessionSpace->LocaleId = PsDefaultSystemLocaleId;
    SessionSpace->SessionPageDirectoryIndex = PageTablePage;

    SessionSpace->Color = PageColor;

    //
    // Track the page table page and the data page.
    //

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_CREATE, (ULONG)ResidentPages);
    SessionSpace->NonPagablePages = ResidentPages;
    SessionSpace->CommittedPages = ResidentPages;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initialize the session data page directory entry so trimmers can attach.
    //

#if defined(_AMD64_)
    PointerPpe = MiGetPxeAddress ((PVOID)MmSessionSpace);
#else
    PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);
#endif
    SessionSpace->PageDirectory = *PointerPpe;

#else

    SessionSpace->PageTables = PageTables;

    //
    // Load the session data page table entry so that other processes
    // can fault in the mapping.
    //

    SessionSpace->PageTables[PointerPde - MiGetPdeAddress (MmSessionBase)] = *PointerPde;

#endif

    //
    // This list entry is only referenced while within the
    // session space and has session space (not global) addresses.
    //

    InitializeListHead (&SessionSpace->ImageList);

    //
    // Initialize the session space pool.
    //

    Status = MiInitializeSessionPool ();

    if (!NT_SUCCESS(Status)) {
        goto Failure;
    }

    PoolInitialized = TRUE;

    //
    // Initialize the view mapping support - note this must happen after
    // initializing session pool.
    //

    if (MiInitializeSystemSpaceMap (&SessionGlobal->Session) == FALSE) {
        goto Failure;
    }

    MmUnlockPagableImageSection (ExPageLockHandle);

#if defined (_WIN64)
    MiInitializeSpecialPool (PagedPoolSession);
#endif

    //
    // Use the global virtual address rather than the session space virtual
    // address to set up fields that need to be globally accessible.
    //

    InitializeListHead (&SessionGlobal->WsListEntry);

    InitializeListHead (&SessionGlobal->ProcessList);

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

    ASSERT (Process->Session == NULL);

    ASSERT (SessionGlobal->ProcessReferenceToSession == 0);
    SessionGlobal->ProcessReferenceToSession = 1;

    InterlockedIncrement (&MmSessionDataPages);

    return STATUS_SUCCESS;

Failure:

#if (_MI_PAGING_LEVELS < 3)
    ExFreePool (PageTables);
#endif

    if (GotCommit == TRUE) {
        MiReturnCommitment (ResidentPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_CREATE_FAILURE,
                         ResidentPages);
    }

    if (GotPages == TRUE) {

#if (_MI_PAGING_LEVELS >= 4)
        PointerPxe = MiGetPxeAddress ((PVOID)MmSessionSpace);
        ASSERT (PointerPxe->u.Hard.Valid != 0);
#endif

#if (_MI_PAGING_LEVELS >= 3)
        PointerPpe = MiGetPpeAddress ((PVOID)MmSessionSpace);
        ASSERT (PointerPpe->u.Hard.Valid != 0);
#endif

        PointerPde = MiGetPdeAddress (MmSessionSpace);
        ASSERT (PointerPde->u.Hard.Valid != 0);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE_FAIL1, 1);
        MM_BUMP_COUNTER (49, ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_FAIL1, 1);


        //
        // Do not call MiFreeSessionSpaceMap () as the maps cannot have been
        // initialized if we are in this path.
        //

        //
        // Free the initial page table page that was allocated for the
        // paged pool range (if it has been allocated at this point).
        //

        MiFreeSessionPoolBitMaps ();

        //
        // Capture all needed session information now as after sharing
        // is disabled below, no references to session space can be made.
        //

#if defined(_IA64_)
        KeDetachSessionSpace (&MmSessionMapInfo, MmSessionParentTablePage);
#else
        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);
        MI_WRITE_INVALID_PTE (PointerPte + 1, ZeroKernelPte);
        MI_WRITE_INVALID_PTE (PointerPde, ZeroKernelPte);
#if defined(_AMD64_)
        MI_WRITE_INVALID_PTE (PointerPpe, ZeroKernelPte);
        MI_WRITE_INVALID_PTE (PointerPxe, ZeroKernelPte);
#endif

#endif

        MI_FLUSH_SESSION_TB ();

        LOCK_PFN (OldIrql);

        //
        // Free the session data structure pages.
        //

        Pfn1 = MI_PFN_ELEMENT (DataPage);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (DataPage);

        Pfn1 = MI_PFN_ELEMENT (DataPage2);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (DataPage2);

        //
        // Free the page table page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageTablePage);

#if (_MI_PAGING_LEVELS >= 3)

        if (PoolInitialized == TRUE) {
            ASSERT (Pfn1->u2.ShareCount == 2);
            Pfn1->u2.ShareCount -= 1;
        }
        ASSERT (Pfn1->u2.ShareCount == 1);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);

#else

        ASSERT (PageTablePage == Pfn1->u4.PteFrame);

        if (PoolInitialized == TRUE) {
            ASSERT (Pfn1->u2.ShareCount == 3);
            Pfn1->u2.ShareCount -= 2;
        }
        else {
            ASSERT (Pfn1->u2.ShareCount == 2);
            Pfn1->u2.ShareCount -= 1;
        }

#endif

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (PageTablePage);

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Free the page directory page.
        //

        Pfn1 = MI_PFN_ELEMENT (PageDirectoryPage);
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (PageDirectoryPage);

        //
        // Free the page directory parent page.
        //

        ASSERT (Pfn1->u4.PteFrame == PageDirectoryParentPage);

        Pfn1 = MI_PFN_ELEMENT (PageDirectoryParentPage);

        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCountOnly (PageDirectoryParentPage);

#endif

        MmResidentAvailablePages += (ResidentPages + MI_SESSION_SPACE_WORKING_SET_MINIMUM);

        UNLOCK_PFN (OldIrql);
    }

    if (GlobalMappingPte != NULL) {
        MiReleaseSystemPtes (GlobalMappingPte, 2, SystemPteSpace);
    }

    if (GotUnmappedDataPage == TRUE) {
        LOCK_PFN (OldIrql);

        Pfn1 = MI_PFN_ELEMENT (DataPage);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        Pfn1->u3.e2.ReferenceCount = 1;
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
        MI_SET_PFN_DELETED (Pfn1);
#if DBG
        Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif
        MiDecrementReferenceCount (DataPage);

        Pfn1 = MI_PFN_ELEMENT (DataPage2);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        Pfn1->u3.e2.ReferenceCount = 1;
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
        MI_SET_PFN_DELETED (Pfn1);
#if DBG
        Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif

        MiDecrementReferenceCount (DataPage2);
        UNLOCK_PFN (OldIrql);
    }

    MmUnlockPagableImageSection (ExPageLockHandle);

    ExAcquireFastMutex (&MiSessionIdMutex);

    ASSERT (RtlCheckBit (MiSessionIdBitmap, *SessionId));
    RtlClearBit (MiSessionIdBitmap, *SessionId);

    ExReleaseFastMutex (&MiSessionIdMutex);

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_CREATING_SESSION);
    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_CREATING_SESSION);

    return STATUS_NO_MEMORY;
}


VOID
MiIncrementSessionCount (
    VOID
    )

/*++

Routine Description:

    Nonpaged wrapper to increment the session count.  A spinlock is used to
    provide serialization with win32k callouts.

Arguments:

    None.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_EXPANSION (OldIrql);

    MiSessionCount += 1;

    UNLOCK_EXPANSION (OldIrql);
}

LONG MiSessionLeaderExists;


NTSTATUS
MmSessionCreate (
    OUT PULONG SessionId
    )

/*++

Routine Description:

    Called from NtSetSystemInformation() to create a session space
    in the calling process with the specified SessionId.  An error is returned
    if the calling process already has a session space.

Arguments:

    SessionId - Supplies a pointer to place the resulting session id in.

Return Value:

    Various NTSTATUS error codes.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    ULONG SessionLeaderExists;
    PKTHREAD CurrentThread;
    NTSTATUS Status;
    PEPROCESS CurrentProcess;
#if DBG && (_MI_PAGING_LEVELS < 3)
    PMMPTE StartPde;
    PMMPTE EndPde;
#endif

    CurrentThread = KeGetCurrentThread ();
    ASSERT ((PETHREAD)CurrentThread == PsGetCurrentThread ());
    CurrentProcess = PsGetCurrentProcessByThread ((PETHREAD)CurrentThread);

    //
    // A simple check to see if the calling process already has a session space.
    // No need to go through all this if it does.  Creation races are caught
    // below and recovered from regardless.
    //

    if (CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) {
        return STATUS_ALREADY_COMMITTED;
    }

    if (CurrentProcess->Vm.Flags.SessionLeader == 0) {

        //
        // Only the session manager can create a session.  Make the current
        // process the session leader if this is the first session creation
        // ever.
        //
        // Make sure the add is only done once as this is called multiple times.
        //
    
        SessionLeaderExists = InterlockedCompareExchange (&MiSessionLeaderExists, 1, 0);
    
        if (SessionLeaderExists != 0) {
            return STATUS_INVALID_SYSTEM_SERVICE;
        }

        MiSessionLeader (CurrentProcess);
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == FALSE);

#if defined (_AMD64_)
    ASSERT ((MiGetPxeAddress(MmSessionBase))->u.Long == ZeroKernelPte.u.Long);
#endif

#if (_MI_PAGING_LEVELS < 3)

#if DBG
    StartPde = MiGetPdeAddress (MmSessionBase);
    EndPde = MiGetPdeAddress (MiSessionSpaceEnd);

    while (StartPde < EndPde) {
        ASSERT (StartPde->u.Long == ZeroKernelPte.u.Long);
        StartPde += 1;
    }
#endif

#endif

    KeEnterCriticalRegionThread (CurrentThread);

    Status = MiSessionCreateInternal (SessionId);

    if (!NT_SUCCESS(Status)) {
        KeLeaveCriticalRegionThread (CurrentThread);
        return Status;
    }

    MiIncrementSessionCount ();

    //
    // Add the session space to the working set list.
    //

    Status = MiSessionInitializeWorkingSetList ();

    if (!NT_SUCCESS(Status)) {
        MiDereferenceSession ();
        KeLeaveCriticalRegionThread (CurrentThread);
        return Status;
    }

    KeLeaveCriticalRegionThread (CurrentThread);

    MmSessionSpace->u.Flags.Initialized = 1;

    PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_IN_SESSION);

    if (MiSessionLeaderExists == 1) {
        InterlockedCompareExchange (&MiSessionLeaderExists, 2, 1);
    }

    return Status;
}


NTSTATUS
MmSessionDelete (
    ULONG SessionId
    )

/*++

Routine Description:

    Called from NtSetSystemInformation() to detach from an existing
    session space in the calling process.  An error is returned
    if the calling process has no session space.

Arguments:

    SessionId - Supplies the session id to delete.

Return Value:

    STATUS_SUCCESS on success, STATUS_UNABLE_TO_FREE_VM on failure.

    This process will not be able to access session space anymore upon
    a successful return.  If this is the last process in the session then
    the entire session is torn down.

Environment:

    Kernel mode, no mutexes held.

--*/

{
    PKTHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    CurrentThread = KeGetCurrentThread ();
    ASSERT ((PETHREAD)CurrentThread == PsGetCurrentThread ());
    CurrentProcess = PsGetCurrentProcessByThread ((PETHREAD)CurrentThread);

    //
    // See if the calling process has a session space - this must be
    // checked since we can be called via a system service.
    //

    if ((CurrentProcess->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
#if DBG
        DbgPrint ("MmSessionDelete: Process %p not in a session\n",
            CurrentProcess);
        DbgBreakPoint();
#endif
        return STATUS_UNABLE_TO_FREE_VM;
    }

    if (CurrentProcess->Vm.Flags.SessionLeader == 0) {

        //
        // Only the session manager can delete a session.  This is because
        // it affects the virtual mappings for all threads within the process
        // when this address space is deleted.  This is different from normal
        // VAD clearing because win32k and other drivers rely on this space.
        //

        return STATUS_UNABLE_TO_FREE_VM;
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

    if (MmSessionSpace->SessionId != SessionId) {
#if DBG
        DbgPrint("MmSessionDelete: Wrong SessionId! Own %d, Ask %d\n",
            MmSessionSpace->SessionId,
            SessionId);
        DbgBreakPoint();
#endif
        return STATUS_UNABLE_TO_FREE_VM;
    }

    KeEnterCriticalRegionThread (CurrentThread);

    MiDereferenceSession ();

    KeLeaveCriticalRegionThread (CurrentThread);

    return STATUS_SUCCESS;
}


VOID
MiAttachSession (
    IN PMM_SESSION_SPACE SessionGlobal
    )

/*++

Routine Description:

    Attaches to the specified session space.

Arguments:

    SessionGlobal - Supplies a pointer to the session to attach to.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  Current process must not have a session
    space - ie: the caller should be the system process or smss.exe.

--*/

{
    PMMPTE PointerPde;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0);

#if defined (_AMD64_)

    PointerPde = MiGetPxeAddress (MmSessionBase);
    ASSERT (PointerPde->u.Long == ZeroKernelPte.u.Long);
    MI_WRITE_VALID_PTE (PointerPde, SessionGlobal->PageDirectory);

#elif defined(_IA64_)

    PointerPde = (PMMPTE) (&PsGetCurrentProcess()->Pcb.SessionParentBase);

    ASSERT (MI_GET_PAGE_FRAME_FROM_PTE(PointerPde) == MmSessionParentTablePage);

    KeAttachSessionSpace (&SessionGlobal->SessionMapInfo,
                          SessionGlobal->PageDirectoryParentPage);

#else

    PointerPde = MiGetPdeAddress (MmSessionBase);

    ASSERT (RtlCompareMemoryUlong (PointerPde, 
                                   MiSessionSpacePageTables * sizeof (MMPTE),
                                   0) == MiSessionSpacePageTables * sizeof (MMPTE));

    RtlCopyMemory (PointerPde,
                   &SessionGlobal->PageTables[0],
                   MiSessionSpacePageTables * sizeof (MMPTE));

#endif
}


VOID
MiDetachSession (
    VOID
    )

/*++

Routine Description:

    Detaches from the specified session space.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  No locks held.  Current process must not have a session
    space to return to - ie: this should be the system process.

--*/

{
    PMMPTE PointerPde;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0);
    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

#if defined (_AMD64_)

    PointerPde = MiGetPxeAddress (MmSessionBase);
    PointerPde->u.Long = ZeroKernelPte.u.Long;

#elif defined(_IA64_)

    PointerPde = (PMMPTE) (&PsGetCurrentProcess()->Pcb.SessionParentBase);

    ASSERT (MI_GET_PAGE_FRAME_FROM_PTE(PointerPde) == MmSessionSpace->PageDirectoryParentPage);

    KeDetachSessionSpace (&MmSessionMapInfo, MmSessionParentTablePage);

#else

    PointerPde = MiGetPdeAddress (MmSessionBase);

    RtlZeroMemory (PointerPde, MiSessionSpacePageTables * sizeof (MMPTE));

#endif

    MI_FLUSH_SESSION_TB ();
}

#if DBG

VOID
MiCheckSessionVirtualSpace (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Used to verify that no drivers fail to clean up their session allocations.

Arguments:

    VirtualAddress - Supplies the starting virtual address to check.

    NumberOfBytes - Supplies the number of bytes to check.

Return Value:

    TRUE if all the PTEs have been freed, FALSE if not.

Environment:

    Kernel mode.  APCs disabled.

--*/

{
    PMMPTE StartPde;
    PMMPTE EndPde;
    PMMPTE StartPte;
    PMMPTE EndPte;
    ULONG Index;

    //
    // Check the specified region.  Everything should have been cleaned up
    // already.
    //

#if defined (_AMD64_)
    ASSERT64 (MiGetPxeAddress (VirtualAddress)->u.Hard.Valid == 1);
#endif

    ASSERT64 (MiGetPpeAddress (VirtualAddress)->u.Hard.Valid == 1);

    StartPde = MiGetPdeAddress (VirtualAddress);
    EndPde = MiGetPdeAddress ((PVOID)((PCHAR)VirtualAddress + NumberOfBytes - 1));

    StartPte = MiGetPteAddress (VirtualAddress);
    EndPte = MiGetPteAddress ((PVOID)((PCHAR)VirtualAddress + NumberOfBytes - 1));

    Index = (ULONG)(StartPde - MiGetPdeAddress ((PVOID)MmSessionBase));

#if (_MI_PAGING_LEVELS >= 3)
    while (StartPde <= EndPde && StartPde->u.Long == 0)
#else
    while (StartPde <= EndPde && MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
    {
        StartPde += 1;
        Index += 1;
        StartPte = MiGetVirtualAddressMappedByPte (StartPde);
    }

    while (StartPte <= EndPte) {

        if (MiIsPteOnPdeBoundary(StartPte)) {

            StartPde = MiGetPteAddress (StartPte);
            Index = (ULONG)(StartPde - MiGetPdeAddress ((PVOID)MmSessionBase));

#if (_MI_PAGING_LEVELS >= 3)
            while (StartPde <= EndPde && StartPde->u.Long == 0)
#else
            while (StartPde <= EndPde && MmSessionSpace->PageTables[Index].u.Long == 0)
#endif
            {
                Index += 1;
                StartPde += 1;
                StartPte = MiGetVirtualAddressMappedByPte (StartPde);
            }
            if (StartPde > EndPde) {
                break;
            }
        }

        if (StartPte->u.Long != 0 && StartPte->u.Long != MM_KERNEL_NOACCESS_PTE) {
            DbgPrint("MiCheckSessionVirtualSpace: StartPte 0x%p is still valid! 0x%p, VA 0x%p\n",
                StartPte,
                StartPte->u.Long,
                MiGetVirtualAddressMappedByPte(StartPte));

            DbgBreakPoint();
        }
        StartPte += 1;
    }
}
#endif


VOID
MiSessionDeletePde (
    IN PMMPTE Pde,
    IN LOGICAL WorkingSetInitialized,
    IN PMMPTE SelfMapPde
    )

/*++

Routine Description:

    Used to delete a page directory entry from a session space.

Arguments:

    Pde - Supplies the page directory entry to delete.

    WorkingSetInitialized - Supplies TRUE if the working set has been
                            initialized.

    SelfMapPde - Supplies the page directory entry that contains the self map
                 session page.


Return Value:

    None.

Environment:

    Kernel mode.  PFN lock held.

--*/

{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    LOGICAL SelfMapPage;

#if DBG
    PMMPFN Pfn2;
#else
    UNREFERENCED_PARAMETER (WorkingSetInitialized);
#endif

    if (Pde->u.Long == ZeroKernelPte.u.Long) {
        return;
    }

    SelfMapPage = (Pde == SelfMapPde ? TRUE : FALSE);

    ASSERT (Pde->u.Hard.Valid == 1);

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (Pde);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

#if DBG

    ASSERT (PageFrameIndex <= MmHighestPhysicalPage);

    if (WorkingSetInitialized == TRUE) {
        ASSERT (Pfn1->u1.WsIndex);
    }

    ASSERT (Pfn1->u3.e1.PrototypePte == 0);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
    ASSERT (Pfn1->u4.PteFrame <= MmHighestPhysicalPage);

    Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

    //
    // Verify the containing page table is still the page
    // table page mapping the session data structure.
    //

    if (SelfMapPage == FALSE) {

        //
        // Note these ASSERTs will fail if win32k leaks pool.
        //

        ASSERT (Pfn1->u2.ShareCount == 1);

        //
        // NT32 points the additional page tables at the master.
        // NT64 doesn't need to use this trick as there is always
        // an additional hierarchy level.
        //

        ASSERT32 (Pfn1->u4.PteFrame == MI_GET_PAGE_FRAME_FROM_PTE (SelfMapPde));
        ASSERT32 (Pfn2->u2.ShareCount >= 2);
    }
    else {
        ASSERT32 (Pfn1 == Pfn2);
        ASSERT32 (Pfn1->u2.ShareCount == 2);

        ASSERT64 (Pfn1->u2.ShareCount == 1);
    }

#endif // DBG

    if (SelfMapPage == FALSE) {
        MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
    }

    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCountOnly (PageFrameIndex);
}


VOID
MiReleaseProcessReferenceToSessionDataPage (
    PMM_SESSION_SPACE SessionGlobal
    )

/*++

Routine Description:

    Decrement this process' session reference.  The session itself may have
    already been deleted.  If this is the last reference to the session,
    then the session data page and its mapping PTE (if any) will be destroyed
    upon return.

Arguments:

    SessionGlobal - Supplies the global session space pointer being
                    dereferenced.  The caller has already verified that this
                    process is a member of the target session.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
    ULONG SessionId;
    PMMPTE GlobalPteEntrySave;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageFrameIndex2;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;

    if (InterlockedDecrement (&SessionGlobal->ProcessReferenceToSession) != 0) {
        return;
    }

    SessionId = SessionGlobal->SessionId;

    //
    // Free the datapages & self-map PTE now since this is the last
    // process reference and KeDetach has returned.
    //

#if (_MI_PAGING_LEVELS < 3)
    ExFreePool (SessionGlobal->PageTables);
#endif

    GlobalPteEntrySave = SessionGlobal->GlobalPteEntry;

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(SessionGlobal));
    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (GlobalPteEntrySave);
    PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE (GlobalPteEntrySave + 1);
    MiReleaseSystemPtes (GlobalPteEntrySave, 2, SystemPteSpace);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn2 = MI_PFN_ELEMENT (PageFrameIndex2);

    LOCK_PFN (OldIrql);

    ASSERT (Pfn1->u2.ShareCount == 1);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);

    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCountOnly (PageFrameIndex);

    //
    // Free the second datapage.
    //

    ASSERT (Pfn2->u2.ShareCount == 1);
    ASSERT (Pfn2->u3.e2.ReferenceCount == 1);

    MI_SET_PFN_DELETED (Pfn2);
    MiDecrementShareCountOnly (PageFrameIndex2);

    MmResidentAvailablePages += 2;
    MM_BUMP_COUNTER(52, 2);

    UNLOCK_PFN (OldIrql);

    InterlockedDecrement (&MmSessionDataPages);

    //
    // Return commitment for the datapages.
    //

    MiReturnCommitment (2);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_DATAPAGE, 2);

    //
    // Release the session ID so it can be recycled.
    //

    ExAcquireFastMutex (&MiSessionIdMutex);

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));
    RtlClearBit (MiSessionIdBitmap, SessionId);

    ExReleaseFastMutex (&MiSessionIdMutex);
}


VOID
MiDereferenceSession (
    VOID
    )

/*++

Routine Description:

    Decrement this process' reference count to the session, unmapping access
    to the session for the current process.  If this is the last process
    reference to this session, then the entire session will be destroyed upon
    return.  This includes unloading drivers, unmapping pools, freeing
    page tables, etc.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
#if !defined(_IA64_)
    PMMPTE StartPde;
#endif
    ULONG SessionId;
    PEPROCESS Process;
    PMM_SESSION_SPACE SessionGlobal;

    ASSERT ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) ||
            ((MmSessionSpace->u.Flags.Initialized == 0) && (PsGetCurrentProcess()->Vm.Flags.SessionLeader == 1) && (MmSessionSpace->ReferenceCount == 1)));

    SessionId = MmSessionSpace->SessionId;

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));

    if (InterlockedDecrement ((PLONG)&MmSessionSpace->ReferenceCount) != 0) {

        Process = PsGetCurrentProcess ();

        PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_IN_SESSION);

        //
        // Don't delete any non-smss session space mappings here.  Let them
        // live on through process death.  This handles the case where
        // MmDispatchWin32Callout picks csrss - csrss has exited as it's not
        // the last process (smss is).  smss is simultaneously detaching from
        // the session and since it is the last process, it's waiting on
        // the AttachCount below.  The dispatch callout ends up in csrss but
        // has no way to synchronize against csrss exiting through this path
        // as the object reference count doesn't stop it.  So leave the
        // session space mappings alive so the callout can execute through
        // the remains of csrss.
        //
        // Note that when smss detaches, the address space must get cleared
        // here so that subsequent session creations by smss will succeed.
        //

        if (Process->Vm.Flags.SessionLeader == 1) {

            SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

#if defined(_IA64_)

            //
            // Revert back to the pre-session dummy top level page.
            //

            KeDetachSessionSpace (&MmSessionMapInfo, MmSessionParentTablePage);

#elif defined (_AMD64_)

            StartPde = MiGetPxeAddress (MmSessionBase);
            StartPde->u.Long = ZeroKernelPte.u.Long;

#else

            StartPde = MiGetPdeAddress (MmSessionBase);
            RtlZeroMemory (StartPde, MiSessionSpacePageTables * sizeof(MMPTE));

#endif

            MI_FLUSH_SESSION_TB ();
    
            //
            // This process' reference to the session must be NULL as the
            // KeDetach has completed so no swap context referencing the
            // earlier session page can occur from here on.  This is also
            // needed because during clean shutdowns, the final dereference
            // of this process (smss) object will trigger an
            // MmDeleteProcessAddressSpace - this routine will dereference
            // the (no-longer existing) session space if this
            // bit is not cleared properly.
            //

            ASSERT (Process->Session == NULL);

            //
            // Another process may have won the race and exited the session
            // as this process is executing here.  Hence the reference count
            // is carefully checked here to ensure no leaks occur.
            //

            MiReleaseProcessReferenceToSessionDataPage (SessionGlobal);
        }
        return;
    }

    //
    // This is the final process in the session so the entire session must
    // be dereferenced now.
    //

    MiDereferenceSessionFinal ();
}


VOID
MiDereferenceSessionFinal (
    VOID
    )

/*++

Routine Description:

    Decrement this process' reference count to the session, unmapping access
    to the session for the current process.  If this is the last process
    reference to this session, then the entire session will be destroyed upon
    return.  This includes unloading drivers, unmapping pools, freeing
    page tables, etc.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no mutexes held, APCs disabled.

--*/

{
    KIRQL OldIrql;
    ULONG Index;
    ULONG_PTR CountReleased;
    ULONG_PTR CountReleased2;
    ULONG SessionId;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageFrameIndex2;
    ULONG SessionDataPdeIndex;
    KEVENT Event;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE PointerPte;
    PMMPTE EndPte;
    PMMPTE GlobalPteEntrySave;
    PMMPTE StartPde;
    PMM_SESSION_SPACE SessionGlobal;
    LOGICAL WorkingSetWasInitialized;
    ULONG AttachCount;
    PEPROCESS Process;
    PKTHREAD CurrentThread;
#if (_MI_PAGING_LEVELS >= 3)
    PFN_NUMBER PageDirectoryFrame;
    PFN_NUMBER PageParentFrame;
    MMPTE SavePageTables[MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES];
#endif

    Process = PsGetCurrentProcess();

    ASSERT ((Process->Flags & PS_PROCESS_FLAGS_IN_SESSION) ||
            ((MmSessionSpace->u.Flags.Initialized == 0) && (Process->Vm.Flags.SessionLeader == 1) && (MmSessionSpace->ReferenceCount == 0)));

    SessionId = MmSessionSpace->SessionId;

    ASSERT (RtlCheckBit (MiSessionIdBitmap, SessionId));

    //
    // This is the final dereference.  We could be any process
    // including SMSS when a session space load fails.  Note also that
    // processes can terminate in any order as well.
    //

    SessionGlobal = SESSION_GLOBAL (MmSessionSpace);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_EXPANSION (OldIrql);

    //
    // Wait for any cross-session process attaches to detach.  Refuse
    // subsequent attempts to cross-session attach so the address invalidation
    // code doesn't surprise an ongoing or subsequent attachee.
    //

    ASSERT (MmSessionSpace->u.Flags.DeletePending == 0);

    MmSessionSpace->u.Flags.DeletePending = 1;

    AttachCount = MmSessionSpace->AttachCount;

    if (AttachCount) {

        KeInitializeEvent (&SessionGlobal->AttachEvent,
                           NotificationEvent,
                           FALSE);

        UNLOCK_EXPANSION (OldIrql);

        KeWaitForSingleObject( &SessionGlobal->AttachEvent,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        LOCK_EXPANSION (OldIrql);

        ASSERT (MmSessionSpace->u.Flags.DeletePending == 1);
        ASSERT (MmSessionSpace->AttachCount == 0);
    }

    if (MmSessionSpace->Vm.Flags.BeingTrimmed) {

        //
        // Initialize an event and put the event address
        // in the VmSupport.  When the trimming is complete,
        // this event will be set.
        //

        KeInitializeEvent(&Event, NotificationEvent, FALSE);

        MmSessionSpace->Vm.WorkingSetExpansionLinks.Blink = (PLIST_ENTRY)&Event;

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_EXPANSION_AND_THEN_WAIT (OldIrql);

        KeWaitForSingleObject(&Event,
                              WrVirtualMemory,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);
        KeLeaveCriticalRegionThread (CurrentThread);

        LOCK_EXPANSION (OldIrql);
    }
    else if (MmSessionSpace->u.Flags.WorkingSetInserted == 1) {

        //
        // Remove this session from the session list and the working
        // set list.
        //

        RemoveEntryList (&SessionGlobal->Vm.WorkingSetExpansionLinks);

        MmSessionSpace->u.Flags.WorkingSetInserted = 0;
    }

    if (MmSessionSpace->u.Flags.SessionListInserted == 1) {

        RemoveEntryList (&SessionGlobal->WsListEntry);

        MmSessionSpace->u.Flags.SessionListInserted = 0;
    }

    MiSessionCount -= 1;

    UNLOCK_EXPANSION (OldIrql);

#if DBG
    if (Process->Vm.Flags.SessionLeader == 0) {
        ASSERT (MmSessionSpace->ProcessOutSwapCount == 0);
        ASSERT (MmSessionSpace->ReferenceCount == 0);
    }
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(0);

    //
    // If an unload function has been registered for WIN32K.SYS,
    // call it now before we force an unload on any modules.  WIN32K.SYS
    // is responsible for calling any other loaded modules that have
    // unload routines to be run.  Another option is to have the other
    // session drivers register a DLL initialize/uninitialize pair on load.
    //

    if (MmSessionSpace->Win32KDriverObject.DriverUnload) {
        MmSessionSpace->Win32KDriverObject.DriverUnload (&MmSessionSpace->Win32KDriverObject);
    }

    //
    // Complete all deferred pool block deallocations.
    //

    ExDeferredFreePool (&MmSessionSpace->PagedPool);

    //
    // Now that all modules have had their unload routine(s)
    // called, check for pool leaks before unloading the images.
    //

    MiCheckSessionPoolAllocations ();

    ASSERT (MmSessionSpace->ReferenceCount == 0);

#if defined (_WIN64)
    if (MmSessionSpecialPoolStart != 0) {
        MiDeleteSessionSpecialPool ();
    }
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(1);

    //
    // Destroy the view mapping structures.
    //

    MiFreeSessionSpaceMap ();

    MM_SNAP_SESS_MEMORY_COUNTERS(2);

    //
    // Walk down the list of modules we have loaded dereferencing them.
    //
    // This allows us to force an unload of any kernel images loaded by
    // the session so we do not have any virtual space and paging
    // file leaks.
    //

    MiSessionUnloadAllImages ();

    MM_SNAP_SESS_MEMORY_COUNTERS(3);

    //
    // Destroy the session space bitmap structure
    //

    MiFreeSessionPoolBitMaps ();

    MM_SNAP_SESS_MEMORY_COUNTERS(4);

    //
    // Reference the session space structure using its global
    // kernel PTE based address.  This is to avoid deleting it out
    // from underneath ourselves.
    //

    GlobalPteEntrySave = MmSessionSpace->GlobalPteEntry;
    ASSERT (GlobalPteEntrySave != NULL);

    //
    // Sweep the individual regions in their proper order.
    //

#if DBG

    //
    // Check the executable image region. All images
    // should have been unloaded by the image handler.
    //

    MiCheckSessionVirtualSpace ((PVOID) MiSessionImageStart,
                                MiSessionImageEnd - MiSessionImageStart);
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(5);

#if DBG

    //
    // Check the view region. All views should have been cleaned up already.
    //

    MiCheckSessionVirtualSpace ((PVOID) MiSessionViewStart, MmSessionViewSize);
#endif

#if (_MI_PAGING_LEVELS >= 3)
    RtlCopyMemory (SavePageTables,
                   MiGetPdeAddress ((PVOID)MmSessionBase),
                   MiSessionSpacePageTables * sizeof (MMPTE));
#endif

    MM_SNAP_SESS_MEMORY_COUNTERS(6);

#if DBG
    //
    // Check everything possible before the remaining virtual address space
    // is torn down.  In this way if anything is amiss, the data can be
    // more easily examined.
    //

    Pfn1 = MI_PFN_ELEMENT (MmSessionSpace->SessionPageDirectoryIndex);

    //
    // This should be greater than 1 because working set page tables are
    // using this as their parent as well.
    //

    ASSERT (Pfn1->u2.ShareCount > 1);
#endif

    CountReleased = 0;

    if (MmSessionSpace->u.Flags.HasWsLock == 1) {

        PointerPte = MiGetPteAddress ((PVOID)MiSessionSpaceWs);
        EndPte = MiGetPteAddress (MmSessionSpace->Vm.VmWorkingSetList->HighestPermittedHashAddress);

        for ( ; PointerPte < EndPte; PointerPte += 1) {

            if (PointerPte->u.Long) {

                ASSERT (PointerPte->u.Hard.Valid == 1);

                CountReleased += 1;
            }
        }
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_PAGE_FREE, (ULONG) CountReleased);

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CountReleased);

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_WS_PAGE_FREE, (ULONG) CountReleased);
        MmSessionSpace->NonPagablePages -= CountReleased;

        WorkingSetWasInitialized = TRUE;
        MmSessionSpace->u.Flags.HasWsLock = 0;
    }
    else {
        WorkingSetWasInitialized = FALSE;
    }

    //
    // Account for the session data structure data page.  For NT64, the page
    // directory page is also accounted for here.
    //
    // Note CountReleased is deliberately incremented by one less than the
    // CommittedPages/NonPagablePages is incremented by.
    // This is because the data page (and its commitment) can only be returned
    // after the last process has been reaped (not just exited).
    //

#if (_MI_PAGING_LEVELS >= 3)
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE, 4);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, -4);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_DESTROY, 4);
    MmSessionSpace->NonPagablePages -= 4;

    CountReleased += 3;
#else
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_INITIAL_PAGE_FREE, 2);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, -2);
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_DESTROY, 2);
    MmSessionSpace->NonPagablePages -= 2;
#endif

    //
    // Account for any needed session space page tables.  Note the common
    // structure (not the local PDEs) must be examined as any page tables
    // that were dynamically materialized in the context of a different
    // process may not be in the current process' page directory (ie: the
    // current process has never accessed the materialized VAs) !
    //

#if (_MI_PAGING_LEVELS >= 3)
    StartPde = MiGetPdeAddress ((PVOID)MmSessionBase);
#else
    StartPde = &MmSessionSpace->PageTables[0];
#endif

    CountReleased2 = 0;
    for (Index = 0; Index < MiSessionSpacePageTables; Index += 1) {

        if (StartPde->u.Long != ZeroKernelPte.u.Long) {
            CountReleased2 += 1;
        }

        StartPde += 1;
    }

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_PAGETABLE_FREE, (ULONG) CountReleased2);
    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                 0 - CountReleased2);
    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_SESSION_PTDESTROY, (ULONG) CountReleased2);
    MmSessionSpace->NonPagablePages -= CountReleased2;
    CountReleased += CountReleased2;

    ASSERT (MmSessionSpace->NonPagablePages == 0);

    //
    // Note that whenever win32k or drivers loaded by it leak pool, the
    // ASSERT below will be triggered.
    //

    ASSERT (MmSessionSpace->CommittedPages == 0);

    MiReturnCommitment (CountReleased);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_DEREFERENCE, CountReleased);

    //
    // Sweep the working set entries.
    // No more accesses to the working set or its lock are allowed.
    //

    if (WorkingSetWasInitialized == TRUE) {
        ExDeleteResourceLite (&SessionGlobal->WsLock);

        PointerPte = MiGetPteAddress ((PVOID)MiSessionSpaceWs);
        EndPte = MiGetPteAddress (MmSessionSpace->Vm.VmWorkingSetList->HighestPermittedHashAddress);

        for ( ; PointerPte < EndPte; PointerPte += 1) {

            if (PointerPte->u.Long) {

                ASSERT (PointerPte->u.Hard.Valid == 1);

                //
                // Delete the page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                //
                // Each page should still be locked in the session working set.
                //

                LOCK_PFN (OldIrql);

                ASSERT (Pfn1->u3.e2.ReferenceCount == 1);

                MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
                MI_SET_PFN_DELETED (Pfn1);
                MiDecrementShareCountOnly (PageFrameIndex);
                MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

                //
                // Don't return the resident available pages charge here
                // as it's going to be returned in one chunk below as part of
                // CountReleased.
                //

                UNLOCK_PFN (OldIrql);
            }
        }
    }

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(SessionGlobal));
    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (GlobalPteEntrySave);
    PageFrameIndex2 = MI_GET_PAGE_FRAME_FROM_PTE (GlobalPteEntrySave + 1);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    Pfn2 = MI_PFN_ELEMENT (PageFrameIndex2);

    ASSERT (Pfn1->u4.PteFrame == MmSessionSpace->SessionPageDirectoryIndex);
    ASSERT (Pfn2->u4.PteFrame == MmSessionSpace->SessionPageDirectoryIndex);

    //
    // Make sure the data pages are still locked.
    //

    ASSERT (Pfn1->u1.WsIndex == 0);
    ASSERT (Pfn2->u1.WsIndex == 1);

#if defined(_IA64_)
    KeDetachSessionSpace (&MmSessionMapInfo, MmSessionParentTablePage);
#elif defined (_AMD64_)
    StartPde = MiGetPxeAddress (MmSessionBase);
    StartPde->u.Long = ZeroKernelPte.u.Long;
#else
    StartPde = MiGetPdeAddress (MmSessionBase);
    RtlZeroMemory (StartPde, MiSessionSpacePageTables * sizeof(MMPTE));
#endif

    //
    // Delete the VA space - no more accesses to MmSessionSpace at this point.
    //
    // Cut off the pagetable reference as the local page table is going to be
    // freed now.  Any needed references must go through the global PTE
    // space or superpages but never through the local session VA.  The
    // actual session data pages are not freed until the very last process
    // of the session receives its very last object dereference.
    //

    LOCK_PFN (OldIrql);

    ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
    ASSERT (Pfn2->u3.e2.ReferenceCount == 1);

#if (_MI_PAGING_LEVELS >= 3)
    PageDirectoryFrame = MI_PFN_ELEMENT(Pfn1->u4.PteFrame)->u4.PteFrame;
    PageParentFrame = MI_PFN_ELEMENT(PageDirectoryFrame)->u4.PteFrame;

    ASSERT (PageDirectoryFrame == MI_PFN_ELEMENT(Pfn2->u4.PteFrame)->u4.PteFrame);
#endif

    MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
    MiDecrementShareAndValidCount (Pfn2->u4.PteFrame);

    //
    // N.B.  Pfn1/2 and PageFrameIndex/2 cannot be referenced from here on out
    // as the interlocked decrement has been done and another process
    // may be racing through MmDeleteProcessAddressSpace.
    //

    MmResidentAvailablePages += CountReleased;
    MM_BUMP_COUNTER(53, CountReleased);

    MmResidentAvailablePages += MI_SESSION_SPACE_WORKING_SET_MINIMUM;
    MM_BUMP_COUNTER(56, MI_SESSION_SPACE_WORKING_SET_MINIMUM);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Delete the session page directory and top level pages.
    //

    Pfn1 = MI_PFN_ELEMENT (PageDirectoryFrame);
    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCountOnly (PageDirectoryFrame);
    MiDecrementShareCountOnly (PageDirectoryFrame);

    Pfn1 = MI_PFN_ELEMENT (PageParentFrame);
    MI_SET_PFN_DELETED (Pfn1);
    MiDecrementShareCountOnly (PageParentFrame);
    MiDecrementShareCountOnly (PageParentFrame);

#endif

    //
    // At this point everything has been deleted except the data pages.
    //
    // Delete page table pages.  Note that the page table page mapping the
    // session space data structure is done last so that we can apply
    // various ASSERTs in the DeletePde routine.
    //

    SessionDataPdeIndex = MiGetPdeSessionIndex (MmSessionSpace);

    for (Index = 0; Index < MiSessionSpacePageTables; Index += 1) {

        if (Index == SessionDataPdeIndex) {

            //
            // The self map entry must be done last.
            //

            continue;
        }

#if (_MI_PAGING_LEVELS >= 3)
        MiSessionDeletePde (&SavePageTables[Index],
                            WorkingSetWasInitialized,
                            &SavePageTables[SessionDataPdeIndex]);
#else
        MiSessionDeletePde (&SessionGlobal->PageTables[Index],
                            WorkingSetWasInitialized,
                            &SessionGlobal->PageTables[SessionDataPdeIndex]);
#endif
    }

#if (_MI_PAGING_LEVELS >= 3)
    MiSessionDeletePde (&SavePageTables[SessionDataPdeIndex],
                        WorkingSetWasInitialized,
                        &SavePageTables[SessionDataPdeIndex]);
#else
    MiSessionDeletePde (&SessionGlobal->PageTables[SessionDataPdeIndex],
                        WorkingSetWasInitialized,
                        &SessionGlobal->PageTables[SessionDataPdeIndex]);
#endif

    UNLOCK_PFN (OldIrql);

    //
    // Flush the session space TB entries.
    //

    MI_FLUSH_SESSION_TB ();

    PS_CLEAR_BITS (&Process->Flags, PS_PROCESS_FLAGS_IN_SESSION);

    //
    // The session space has been deleted and all TB flushing is complete.
    //

    MmUnlockPagableImageSection (ExPageLockHandle);

    return;
}

NTSTATUS
MiSessionCommitImagePages (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This routine commits virtual memory within the current session space with
    backing pages.  The virtual addresses within session space are
    allocated with a separate facility in the image management facility.
    This is because images must be at a unique systemwide virtual address.

Arguments:

    VirtualAddress - Supplies the first virtual address to commit.

    NumberOfBytes - Supplies the number of bytes to commit.

Return Value:

    STATUS_SUCCESS if all went well, STATUS_NO_MEMORY if the current process
    has no session.

Environment:

    Kernel mode, MmSystemLoadLock held.
    
    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    KIRQL WsIrql;
    KIRQL OldIrql;
    ULONG Color;
    PFN_NUMBER SizeInPages;
    PMMPFN Pfn1;
    ULONG_PTR AllocationStart;
    PFN_NUMBER PageFrameIndex;
    NTSTATUS Status;
    PMMPTE StartPte, EndPte;
    MMPTE TempPte;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    if (NumberOfBytes == 0) {
        return STATUS_SUCCESS;
    }

    if (MmIsAddressValid(MmSessionSpace) == FALSE) {
#if DBG
        DbgPrint ("MiSessionCommitImagePages: No session space!\n");
#endif
        return STATUS_NO_MEMORY;
    }

    ASSERT (((ULONG_PTR)VirtualAddress % PAGE_SIZE) == 0);
    ASSERT ((NumberOfBytes % PAGE_SIZE) == 0);

    SizeInPages = (PFN_NUMBER)(NumberOfBytes >> PAGE_SHIFT);

    if (MiChargeCommitment (SizeInPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        return STATUS_NO_MEMORY;
    }

    //
    // Calculate pages needed.
    //

    AllocationStart = (ULONG_PTR)VirtualAddress;

    StartPte = MiGetPteAddress ((PVOID)AllocationStart);
    EndPte = MiGetPteAddress ((PVOID)(AllocationStart + NumberOfBytes));

    TempPte = ValidKernelPteLocal;

    TempPte.u.Long |= MM_PTE_EXECUTE;

    //
    // Lock the session space working set.
    //

    LOCK_SESSION_SPACE_WS(WsIrql, PsGetCurrentThread ());

    //
    // Make sure we have page tables for the PTE
    // entries we must fill in the session space structure.
    //

    Status = MiSessionCommitPageTables ((PVOID)AllocationStart,
                                        (PVOID)(AllocationStart + NumberOfBytes));

    if (!NT_SUCCESS(Status)) {
        UNLOCK_SESSION_SPACE_WS(WsIrql);
        MiReturnCommitment (SizeInPages);
        return STATUS_NO_MEMORY;
    }

    //
    // Loop allocating them and placing them into the page tables.
    //

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if ((SPFN_NUMBER)SizeInPages > MI_NONPAGABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        UNLOCK_SESSION_SPACE_WS(WsIrql);
        MiReturnCommitment (SizeInPages);
        return STATUS_NO_MEMORY;
    }

    //
    // Check to make sure the actual pages are currently available.  Normally
    // it would be fine to skip this check and use MiEnsureAvailablePageOrWait
    // calls in the loop below which could release the session space mutex
    // and wait - but this code has not been modified to handle rechecking
    // all the state, so it's easier to just check once up front.
    // The hardcoded 50 better always be higher than MM_HIGH_LIMIT.
    //

    if ((PFN_COUNT)SizeInPages + MM_HIGH_LIMIT + 50 > MmAvailablePages) {
        UNLOCK_PFN (OldIrql);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_AVAILABLE);
        UNLOCK_SESSION_SPACE_WS(WsIrql);
        MiReturnCommitment (SizeInPages);
        return STATUS_NO_MEMORY;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_IMAGE_PAGES, SizeInPages);

    MmResidentAvailablePages -= SizeInPages;

    MM_BUMP_COUNTER(45, SizeInPages);

    while (StartPte < EndPte) {

        ASSERT (StartPte->u.Long == ZeroKernelPte.u.Long);

#if 0
        //
        // If MiEnsureAvailablePageOrWait released the session space mutex
        // while waiting for a page all this code would need to be modified
        // to handle rechecking all the state.  Instead the check was made
        // once up front for enough currently available pages and the code
        // here removed.
        //

        Waited = MiEnsureAvailablePageOrWait (HYDRA_PROCESS, NULL);

        ASSERT (Waited == FALSE);
#endif

        Color = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);

        PageFrameIndex = MiRemoveZeroPage (Color);

        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (StartPte, TempPte);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u1.WsIndex == 0);

        MiInitializePfn (PageFrameIndex, StartPte, 1);

        KeFillEntryTb ((PHARDWARE_PTE) StartPte, (PMMPTE)AllocationStart, FALSE);

        StartPte += 1;
        AllocationStart += PAGE_SIZE;
    }

    UNLOCK_PFN (OldIrql);

    MmSessionSpace->NonPagablePages += SizeInPages;

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_DRIVER_PAGES_LOCKED, (ULONG)SizeInPages);

    UNLOCK_SESSION_SPACE_WS(WsIrql);

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, SizeInPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_COMMIT_IMAGE, (ULONG)SizeInPages);

    return STATUS_SUCCESS;
}

NTSTATUS
MiSessionCommitPageTables (
    IN PVOID StartVa,
    IN PVOID EndVa
    )

/*++

Routine Description:

    Fill in page tables covering the specified virtual address range.

Arguments:

    StartVa - Supplies a starting virtual address.

    EndVa - Supplies an ending virtual address.

Return Value:

    STATUS_SUCCESS on success, STATUS_NO_MEMORY on failure.

Environment:

    Kernel mode.  Session space working set mutex held.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    KIRQL OldIrql;
    ULONG Color;
    ULONG Index;
    PMMPTE StartPde;
    PMMPTE EndPde;
    MMPTE TempPte;
    PMMPFN Pfn1;
    WSLE_NUMBER Entry;
    WSLE_NUMBER SwapEntry;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER PageTablePage;
    PVOID SessionPte;
    PMMWSL WorkingSetList;
    ULONG AllocatedPageTables[(MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES + sizeof(ULONG)*8-1)/(sizeof(ULONG)*8)];

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

    MM_SESSION_SPACE_WS_LOCK_ASSERT();

    ASSERT (StartVa >= (PVOID)MmSessionBase);
    ASSERT (EndVa < (PVOID)MiSessionSpaceEnd);

    //
    // Allocate the page table pages, loading them
    // into the current process's page directory.
    //

    StartPde = MiGetPdeAddress (StartVa);
    EndPde = MiGetPdeAddress (EndVa);
    Index = MiGetPdeSessionIndex (StartVa);

    SizeInPages = 0;

    while (StartPde <= EndPde) {
#if (_MI_PAGING_LEVELS >= 3)
        if (StartPde->u.Long == ZeroKernelPte.u.Long)
#else
        if (MmSessionSpace->PageTables[Index].u.Long == ZeroKernelPte.u.Long)
#endif
        {
            SizeInPages += 1;
        }
        StartPde += 1;
        Index += 1;
    }

    if (SizeInPages == 0) {
        return STATUS_SUCCESS;
    }

    if (MiChargeCommitment (SizeInPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (AllocatedPageTables, sizeof (AllocatedPageTables));

    WorkingSetList = MmSessionSpace->Vm.VmWorkingSetList;

    StartPde = MiGetPdeAddress (StartVa);
    Index = MiGetPdeSessionIndex (StartVa);

    TempPte = ValidKernelPdeLocal;

    LOCK_PFN (OldIrql);

    //
    // Check to make sure the physical pages are available.
    //

    if ((SPFN_NUMBER)SizeInPages > MI_NONPAGABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (SizeInPages);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_RESIDENT);
        return STATUS_NO_MEMORY;
    }

    //
    // Check to make sure the actual pages are currently available.  Normally
    // it would be fine to skip this check and use MiEnsureAvailablePageOrWait
    // calls in the loop below which could release the session space mutex
    // and wait - but this code has not been modified to handle rechecking
    // all the state, so it's easier to just check once up front.
    // The hardcoded 50 better always be higher than MM_HIGH_LIMIT.
    //

    if ((PFN_COUNT)SizeInPages + 50 > MmAvailablePages) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (SizeInPages);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_AVAILABLE);
        return STATUS_NO_MEMORY;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_PAGETABLE_PAGES, SizeInPages);

    MmResidentAvailablePages -= SizeInPages;

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_PAGETABLE_ALLOC, (ULONG)SizeInPages);

    MM_BUMP_COUNTER(44, SizeInPages);

    while (StartPde <= EndPde) {

#if (_MI_PAGING_LEVELS >= 3)
        if (StartPde->u.Long == ZeroKernelPte.u.Long)
#else
        if (MmSessionSpace->PageTables[Index].u.Long == ZeroKernelPte.u.Long)
#endif
        {

            ASSERT (StartPde->u.Hard.Valid == 0);

            MI_SET_BIT (AllocatedPageTables, Index);

#if 0
            //
            // If MiEnsureAvailablePageOrWait released the session space mutex
            // while waiting for a page all this code would need to be modified
            // to handle rechecking all the state.  Instead the check was made
            // once up front for enough currently available pages and the code
            // here removed.
            //

            Waited = MiEnsureAvailablePageOrWait (HYDRA_PROCESS, NULL);

            ASSERT (Waited == FALSE);
#endif

            Color = MI_GET_PAGE_COLOR_FROM_SESSION (MmSessionSpace);

            PageTablePage = MiRemoveZeroPage (Color);

            TempPte.u.Hard.PageFrameNumber = PageTablePage;
            MI_WRITE_VALID_PTE (StartPde, TempPte);

#if (_MI_PAGING_LEVELS < 3)
            MmSessionSpace->PageTables[Index] = TempPte;
#endif
            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_COMMIT_IMAGE_PT, 1);
            MmSessionSpace->NonPagablePages += 1;
            InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 1);

            MiInitializePfnForOtherProcess (PageTablePage,
                                            StartPde,
                                            MmSessionSpace->SessionPageDirectoryIndex);
        }

        StartPde += 1;
        Index += 1;
    }

    UNLOCK_PFN (OldIrql);

    StartPde = MiGetPdeAddress (StartVa);
    Index = MiGetPdeSessionIndex (StartVa);

    while (StartPde <= EndPde) {

        if (MI_CHECK_BIT(AllocatedPageTables, Index)) {

            ASSERT (StartPde->u.Hard.Valid == 1);

            PageTablePage = MI_GET_PAGE_FRAME_FROM_PTE (StartPde);

            Pfn1 = MI_PFN_ELEMENT (PageTablePage);

            ASSERT (Pfn1->u1.Event == NULL);
            Pfn1->u1.Event = (PVOID)PsGetCurrentThread ();

            SessionPte = MiGetVirtualAddressMappedByPte (StartPde);

            MiAddValidPageToWorkingSet (SessionPte,
                                        StartPde,
                                        Pfn1,
                                        0);

            Entry = MiLocateWsle (SessionPte,
                                  MmSessionSpace->Vm.VmWorkingSetList,
                                  Pfn1->u1.WsIndex);

            if (Entry >= WorkingSetList->FirstDynamic) {

                SwapEntry = WorkingSetList->FirstDynamic;

                if (Entry != WorkingSetList->FirstDynamic) {

                    //
                    // Swap this entry with the one at first dynamic.
                    //

                    MiSwapWslEntries (Entry, SwapEntry, &MmSessionSpace->Vm);
                }

                WorkingSetList->FirstDynamic += 1;
            }
            else {
                SwapEntry = Entry;
            }

            //
            // Indicate that the page is locked.
            //

            MmSessionSpace->Wsle[SwapEntry].u1.e1.LockedInWs = 1;
        }

        StartPde += 1;
        Index += 1;
    }

    return STATUS_SUCCESS;
}

#if DBG
typedef struct _MISWAP {
    ULONG Flag;
    ULONG OutSwapCount;
    PEPROCESS Process;
    PMM_SESSION_SPACE Session;
} MISWAP, *PMISWAP;

ULONG MiSessionInfo[4];
MISWAP MiSessionSwap[0x100];
ULONG  MiSwapIndex;
#endif


VOID
MiSessionOutSwapProcess (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine notifies the containing session that the specified process is
    being outswapped.  When all the processes within a session have been
    outswapped, the containing session undergoes a heavy trim.

Arguments:

    Process - Supplies a pointer to the process that is swapped out of memory.

Return Value:

    None.

Environment:

    Kernel mode.  This routine must not enter a wait state for memory resources
    or the system will deadlock.

--*/

{
    KIRQL OldIrql;
    PMM_SESSION_SPACE SessionGlobal;
#if DBG
    ULONG InCount;
    ULONG OutCount;
    PLIST_ENTRY NextEntry;
#endif

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_IN_SESSION);

    //
    // smss doesn't count when we swap it before it has detached from the
    // session it is currently creating.
    //

    if (Process->Vm.Flags.SessionLeader == 1) {
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;
    ASSERT (SessionGlobal != NULL);

    LOCK_EXPANSION (OldIrql);

    SessionGlobal->ProcessOutSwapCount += 1;

#if DBG
    ASSERT ((LONG)SessionGlobal->ProcessOutSwapCount > 0);

    InCount = 0;
    OutCount = 0;
    NextEntry = SessionGlobal->ProcessList.Flink;

    while (NextEntry != &SessionGlobal->ProcessList) {
        Process = CONTAINING_RECORD (NextEntry, EPROCESS, SessionProcessLinks);

        if (Process->Flags & PS_PROCESS_FLAGS_OUTSWAP_ENABLED) {
            OutCount += 1;
        }
        else {
            InCount += 1;
        }

        NextEntry = NextEntry->Flink;
    }

    if (InCount + OutCount > SessionGlobal->ReferenceCount) {
        DbgPrint ("MiSessionOutSwapProcess : process count mismatch %p %x %x %x\n",
            SessionGlobal,
            SessionGlobal->ReferenceCount,
            InCount,
            OutCount);
        DbgBreakPoint ();
    }

    if (SessionGlobal->ProcessOutSwapCount != OutCount) {
        DbgPrint ("MiSessionOutSwapProcess : out count mismatch %p %x %x %x %x\n",
            SessionGlobal,
            SessionGlobal->ReferenceCount,
            SessionGlobal->ProcessOutSwapCount,
            InCount,
            OutCount);
        DbgBreakPoint ();
    }

    ASSERT (SessionGlobal->ProcessOutSwapCount <= SessionGlobal->ReferenceCount);

    MiSessionSwap[MiSwapIndex].Flag = 1;
    MiSessionSwap[MiSwapIndex].Process = Process;
    MiSessionSwap[MiSwapIndex].Session = SessionGlobal;
    MiSessionSwap[MiSwapIndex].OutSwapCount = SessionGlobal->ProcessOutSwapCount;
    MiSwapIndex += 1;
    if (MiSwapIndex == 0x100) {
        MiSwapIndex = 0;
    }
#endif

    if (SessionGlobal->ProcessOutSwapCount == SessionGlobal->ReferenceCount) {
        SessionGlobal->Vm.Flags.TrimHard = 1;
#if DBG
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrint ("Mm: Last process (%d total) just swapped out for session %d, %d pages\n",
                SessionGlobal->ProcessOutSwapCount,
                SessionGlobal->SessionId,
                SessionGlobal->Vm.WorkingSetSize);
        }
        MiSessionInfo[0] += 1;
#endif
        KeQuerySystemTime (&SessionGlobal->LastProcessSwappedOutTime);
    }
#if DBG
    else {
        MiSessionInfo[1] += 1;
    }
#endif

    UNLOCK_EXPANSION (OldIrql);
}


VOID
MiSessionInSwapProcess (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine in swaps the specified process.

Arguments:

    Process - Supplies a pointer to the process that is to be swapped
        into memory.

Return Value:

    None.

Environment:

    Kernel mode.  This routine must not enter a wait state for memory resources
    or the system will deadlock.

--*/

{
    KIRQL OldIrql;
    PMM_SESSION_SPACE SessionGlobal;
#if DBG
    ULONG InCount;
    ULONG OutCount;
    PLIST_ENTRY NextEntry;
#endif

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_IN_SESSION);

    //
    // smss doesn't count when we swap it before it has detached from the
    // session it is currently creating.
    //

    if (Process->Vm.Flags.SessionLeader == 1) {
        return;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;
    ASSERT (SessionGlobal != NULL);

    LOCK_EXPANSION (OldIrql);

#if DBG
    ASSERT ((LONG)SessionGlobal->ProcessOutSwapCount > 0);

    InCount = 0;
    OutCount = 0;
    NextEntry = SessionGlobal->ProcessList.Flink;

    while (NextEntry != &SessionGlobal->ProcessList) {
        Process = CONTAINING_RECORD (NextEntry, EPROCESS, SessionProcessLinks);

        if (Process->Flags & PS_PROCESS_FLAGS_OUTSWAP_ENABLED) {
            OutCount += 1;
        }
        else {
            InCount += 1;
        }

        NextEntry = NextEntry->Flink;
    }

    if (InCount + OutCount > SessionGlobal->ReferenceCount) {
        DbgPrint ("MiSessionInSwapProcess : count mismatch %p %x %x %x\n",
            SessionGlobal,
            SessionGlobal->ReferenceCount,
            InCount,
            OutCount);
        DbgBreakPoint ();
    }

    if (SessionGlobal->ProcessOutSwapCount != OutCount) {
        DbgPrint ("MiSessionInSwapProcess : out count mismatch %p %x %x %x %x\n",
            SessionGlobal,
            SessionGlobal->ReferenceCount,
            SessionGlobal->ProcessOutSwapCount,
            InCount,
            OutCount);
        DbgBreakPoint ();
    }

    ASSERT (SessionGlobal->ProcessOutSwapCount <= SessionGlobal->ReferenceCount);

    MiSessionSwap[MiSwapIndex].Flag = 2;
    MiSessionSwap[MiSwapIndex].Process = Process;
    MiSessionSwap[MiSwapIndex].Session = SessionGlobal;
    MiSessionSwap[MiSwapIndex].OutSwapCount = SessionGlobal->ProcessOutSwapCount;
    MiSwapIndex += 1;
    if (MiSwapIndex == 0x100) {
        MiSwapIndex = 0;
    }
#endif

    if (SessionGlobal->ProcessOutSwapCount == SessionGlobal->ReferenceCount) {
#if DBG
        MiSessionInfo[2] += 1;
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrint ("Mm: First process (%d total) just swapped back in for session %d, %d pages\n",
                SessionGlobal->ProcessOutSwapCount,
                SessionGlobal->SessionId,
                SessionGlobal->Vm.WorkingSetSize);
        }
#endif
        SessionGlobal->Vm.Flags.TrimHard = 0;
    }
#if DBG
    else {
        MiSessionInfo[3] += 1;
    }
#endif

    SessionGlobal->ProcessOutSwapCount -= 1;

    ASSERT ((LONG)SessionGlobal->ProcessOutSwapCount >= 0);

    UNLOCK_EXPANSION (OldIrql);
}


ULONG
MmGetSessionId (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine returns the session ID of the specified process.

Arguments:

    Process - Supplies a pointer to the process whose session ID is desired.

Return Value:

    The session ID.  Note these are recycled when sessions exit, hence the
    caller must use proper object referencing on the specified process.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    PMM_SESSION_SPACE SessionGlobal;

    if (Process->Vm.Flags.SessionLeader == 1) {

        //
        // smss may transiently have a session space but that's of no interest
        // to our caller.
        //

        return 0;
    }

    //
    // The Session field of the EPROCESS is never cleared once set so these
    // checks can be done lock free.
    //

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    if (SessionGlobal == NULL) {

        //
        // The system process has no session space.
        //

        return 0;
    }

    SessionGlobal = (PMM_SESSION_SPACE) Process->Session;

    return SessionGlobal->SessionId;
}

PVOID
MmGetNextSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function allows code to enumerate all the sessions in the system.
    The first session (if OpaqueSession is NULL) or subsequent session
    (if session is not NULL) is returned on each call.

    If OpaqueSession is not NULL then this session must have previously
    been obtained by a call to MmGetNextSession.

    Enumeration may be terminated early by calling MmQuitNextSession on
    the last non-NULL session returned by MmGetNextSession.

    Sessions may be referenced in this manner and used later safely.

    For example, to enumerate all sessions in a loop use this code fragment:

    for (OpaqueSession = MmGetNextSession (NULL);
         OpaqueSession != NULL;
         OpaqueSession = MmGetNextSession (OpaqueSession)) {

         ...
         ...

         //
         // Checking for a specific session (if needed) is handled like this:
         //

         if (MmGetSessionId (OpaqueSession) == DesiredId) {

             //
             // Attach to session now to perform operations...
             //

             KAPC_STATE ApcState;

             if (NT_SUCCESS (MmAttachSession (OpaqueSession, &ApcState))) {

                //
                // Session hasn't exited yet, so call interesting work
                // functions that need session context ...
                //

                ...

                //
                // Detach from session.
                //

                MmDetachSession (OpaqueSession, &ApcState);
             }

             //
             // If the interesting work functions failed and error recovery
             // (ie: walk back through all the sessions already operated on
             // and try to undo the actions), then do this.  Note you must add
             // similar checks to the above if the operations were only done
             // to specifically requested session IDs.
             //

             if (ErrorRecoveryNeeded) {

                 for (OpaqueSession = MmGetPreviousSession (OpaqueSession);
                      OpaqueSession != NULL;
                      OpaqueSession = MmGetPreviousSession (OpaqueSession)) {

                      //
                      // MmAttachSession/DetachSession as needed to obtain
                      // context, etc.
                      //
                 }

                 break;
             }

             //
             // Bail if only this session was of interest.
             //

             MmQuitNextSession (OpaqueSession);
             break;
         }

         //
         // Early terminating conditions are handled like this:
         //

         if (NeedToBreakOutEarly) {
             MmQuitNextSession (OpaqueSession);
             break;
         }
    }
    

Arguments:

    OpaqueSession - Supplies the session to get the next session from
                    or NULL for the first session.

Return Value:

    Next session or NULL if no more sessions exist.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PMM_SESSION_SPACE EntrySession;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueNextSession;
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueNextSession = NULL;

    EntryProcess = (PEPROCESS) OpaqueSession;

    if (EntryProcess == NULL) {
        EntrySession = NULL;
    }
    else {
        ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

        //
        // The Session field of the EPROCESS is never cleared once set so this
        // field can be used lock free.
        //

        EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

        ASSERT (EntrySession != NULL);
    }

    LOCK_EXPANSION (OldIrql);

    if (EntrySession == NULL) {
        NextEntry = MiSessionWsList.Flink;
    }
    else {
        NextEntry = EntrySession->WsListEntry.Flink;
    }

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if ((Session->u.Flags.DeletePending == 0) &&
            (NextProcessEntry != &Session->ProcessList)) {

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            if (Process->Vm.Flags.SessionLeader == 1) {

                //
                // If session manager is still the first process (ie: smss
                // hasn't detached yet), then don't bother delivering to this
                // session this early in its lifetime.  And since smss is
                // serialized, it can't be creating another session yet so
                // just bail now as we must be at the end of the list.
                //

                break;
            }

            //
            // If the process has finished rudimentary initialization, then
            // select it as an attach can be performed safely.  If this first
            // process has not finished initializing there can be no others
            // in this session, so just march on to the next session.
            //
            // Note the VmWorkingSetList is used instead of the
            // AddressSpaceInitialized field because the VmWorkingSetList is
            // never cleared so we can never see an exiting process (whose
            // AddressSpaceInitialized field gets zeroed) and incorrectly
            // decide the list must be empty.
            //

            if (Process->Vm.VmWorkingSetList != NULL) {

                //
                // Reference any process in the session so that the session
                // cannot be completely deleted once the expansion lock is
                // released (note this does NOT prevent the session from being
                // cleaned).
                //

                ObReferenceObject (Process);
                OpaqueNextSession = (PVOID) Process;
                break;
            }
        }
        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION (OldIrql);

    //
    // Regardless of whether a next session is returned, if a starting one
    // was passed in, it must be dereferenced now.
    //

    if (EntryProcess != NULL) {
        ObDereferenceObject (EntryProcess);
    }

    return OpaqueNextSession;
}

PVOID
MmGetPreviousSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function allows code to reverse-enumerate all the sessions in
    the system.  This is typically used for error recovery - ie: to walk
    backwards undoing work done by MmGetNextSession semantics.

    The first session (if OpaqueSession is NULL) or subsequent session
    (if session is not NULL) is returned on each call.

    If OpaqueSession is not NULL then this session must have previously
    been obtained by a call to MmGetNextSession.

Arguments:

    OpaqueSession - Supplies the session to get the next session from
                    or NULL for the first session.

Return Value:

    Next session or NULL if no more sessions exist.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PMM_SESSION_SPACE EntrySession;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueNextSession;
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueNextSession = NULL;

    EntryProcess = (PEPROCESS) OpaqueSession;

    if (EntryProcess == NULL) {
        EntrySession = NULL;
    }
    else {
        ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

        //
        // The Session field of the EPROCESS is never cleared once set so this
        // field can be used lock free.
        //

        EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

        ASSERT (EntrySession != NULL);
    }

    LOCK_EXPANSION (OldIrql);

    if (EntrySession == NULL) {
        NextEntry = MiSessionWsList.Blink;
    }
    else {
        NextEntry = EntrySession->WsListEntry.Blink;
    }

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if ((Session->u.Flags.DeletePending == 0) &&
            (NextProcessEntry != &Session->ProcessList)) {

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            ASSERT (Process->Vm.Flags.SessionLeader == 0);

            //
            // Reference any process in the session so that the session
            // cannot be completely deleted once the expansion lock is
            // released (note this does NOT prevent the session from being
            // cleaned).
            //

            ObReferenceObject (Process);
            OpaqueNextSession = (PVOID) Process;
            break;
        }
        NextEntry = NextEntry->Blink;
    }

    UNLOCK_EXPANSION (OldIrql);

    //
    // Regardless of whether a next session is returned, if a starting one
    // was passed in, it must be dereferenced now.
    //

    if (EntryProcess != NULL) {
        ObDereferenceObject (EntryProcess);
    }

    return OpaqueNextSession;
}

NTSTATUS
MmQuitNextSession (
    IN PVOID OpaqueSession
    )

/*++

Routine Description:

    This function is used to prematurely terminate a session enumeration
    that began using MmGetNextSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

Return Value:

    NTSTATUS.

--*/

{
    PEPROCESS EntryProcess;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    ASSERT (EntryProcess->Session != NULL);

    ObDereferenceObject (EntryProcess);

    return STATUS_SUCCESS;
}

NTSTATUS
MmAttachSession (
    IN PVOID OpaqueSession,
    OUT PRKAPC_STATE ApcState
    )

/*++

Routine Description:

    This function attaches the calling thread to a referenced session
    previously obtained via MmGetNextSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

    ApcState - Supplies APC state storage for the subsequent detach.

Return Value:

    NTSTATUS.  If successful then we are attached on return.  The caller is
               responsible for calling MmDetachSession when done.

--*/

{
    KIRQL OldIrql;
    PEPROCESS EntryProcess;
    PMM_SESSION_SPACE EntrySession;
    PEPROCESS CurrentProcess;
    PMM_SESSION_SPACE CurrentSession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

    ASSERT (EntrySession != NULL);

    CurrentProcess = PsGetCurrentProcess ();

    CurrentSession = (PMM_SESSION_SPACE) CurrentProcess->Session;

    LOCK_EXPANSION (OldIrql);

    if (EntrySession->u.Flags.DeletePending == 1) {
        UNLOCK_EXPANSION (OldIrql);
        return STATUS_PROCESS_IS_TERMINATING;
    }

    EntrySession->AttachCount += 1;

    UNLOCK_EXPANSION (OldIrql);

    if ((CurrentProcess->Vm.Flags.SessionLeader == 0) &&
        (CurrentSession != NULL)) {

        //
        // smss may transiently have a session space but that's of
        // no interest to our caller.
        //

        if (CurrentSession == EntrySession) {

            ASSERT (CurrentSession->SessionId == EntrySession->SessionId);

            //
            // The current and target sessions match so an attach is not needed.
            // Call KeStackAttach anyway (this has the overhead of an extra
            // dispatcher lock acquire and release) so that callers can always
            // use MmDetachSession to detach.  This is a very infrequent path so
            // the extra lock acquire and release is not significant.
            //
            // Note that by resetting EntryProcess below, an attach will not
            // actually occur.
            //

            EntryProcess = CurrentProcess;
        }
        else {
            ASSERT (CurrentSession->SessionId != EntrySession->SessionId);
        }
    }

    KeStackAttachProcess (&EntryProcess->Pcb, ApcState);

    return STATUS_SUCCESS;
}

NTSTATUS
MmDetachSession (
    IN PVOID OpaqueSession,
    IN PRKAPC_STATE ApcState
    )
/*++

Routine Description:

    This function detaches the calling thread from the referenced session
    previously attached to via MmAttachSession.

Arguments:

    OpaqueSession - Supplies a non-NULL session previously obtained by
                    a call to MmGetNextSession.

    ApcState - Supplies APC state storage information for the detach.

Return Value:

    NTSTATUS.  If successful then we are detached on return.  The caller is
               responsible for eventually calling MmQuitNextSession on return.

--*/

{
    KIRQL OldIrql;
    PEPROCESS EntryProcess;
    PMM_SESSION_SPACE EntrySession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    EntryProcess = (PEPROCESS) OpaqueSession;

    ASSERT (EntryProcess->Vm.Flags.SessionLeader == 0);

    //
    // The Session field of the EPROCESS is never cleared once set so this
    // field can be used lock free.
    //

    EntrySession = (PMM_SESSION_SPACE) EntryProcess->Session;

    ASSERT (EntrySession != NULL);

    LOCK_EXPANSION (OldIrql);

    ASSERT (EntrySession->AttachCount >= 1);

    EntrySession->AttachCount -= 1;

    if ((EntrySession->u.Flags.DeletePending == 0) ||
        (EntrySession->AttachCount != 0)) {

        EntrySession = NULL;
    }

    UNLOCK_EXPANSION (OldIrql);

    KeUnstackDetachProcess (ApcState);

    if (EntrySession != NULL) {
        KeSetEvent (&EntrySession->AttachEvent, 0, FALSE);
    }

    return STATUS_SUCCESS;
}

PVOID
MmGetSessionById (
    IN ULONG SessionId
    )

/*++

Routine Description:

    This function allows callers to obtain a reference to a specific session.
    The caller can then MmAttachSession, MmDetachSession & MmQuitNextSession
    to complete the proper sequence so reference counting and address context
    operate properly.

Arguments:

    SessionId - Supplies the session ID of the desired session.

Return Value:

    An opaque session token or NULL if the session cannot be found.

Environment:

    Kernel mode, the caller must guarantee the session cannot exit or the ID
    becomes meaningless as it can be reused.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMM_SESSION_SPACE Session;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS Process;
    PVOID OpaqueSession;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    OpaqueSession = NULL;

    LOCK_EXPANSION (OldIrql);

    NextEntry = MiSessionWsList.Flink;

    while (NextEntry != &MiSessionWsList) {

        Session = CONTAINING_RECORD (NextEntry, MM_SESSION_SPACE, WsListEntry);

        NextProcessEntry = Session->ProcessList.Flink;

        if (Session->SessionId == SessionId) {

            if ((Session->u.Flags.DeletePending != 0) ||
                (NextProcessEntry == &Session->ProcessList)) {

                //
                // Session is empty or exiting so return failure to the caller.
                //

                break;
            }

            Process = CONTAINING_RECORD (NextProcessEntry,
                                         EPROCESS,
                                         SessionProcessLinks);

            if (Process->Vm.Flags.SessionLeader == 1) {

                //
                // Session manager is still the first process (ie: smss
                // hasn't detached yet), don't bother delivering to this
                // session this early in its lifetime.  And since smss is
                // serialized, it can't be creating another session yet so
                // just bail now as we must be at the end of the list.
                //

                break;
            }

            //
            // Reference any process in the session so that the session
            // cannot be completely deleted once the expansion lock is
            // released (note this does NOT prevent the session from being
            // cleaned).
            //

            ObReferenceObject (Process);
            OpaqueSession = (PVOID) Process;
            break;
        }
        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION (OldIrql);

    return OpaqueSession;
}

PVOID
MiAttachToSecureProcessInSession (
    IN PRKAPC_STATE ApcState
    )

/*++

Routine Description:

    This function allows callers to attach to a secure process
    in the current session.  This is the first process created in it.

Arguments:

    ApcState - Supplies APC state storage information for the attach.

Return Value:

    An opaque session token or NULL if the attach could not be performed.
    If non-null, then the process attach been performed on return.

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextProcessEntry;
    PEPROCESS TargetProcess;
    PMM_SESSION_SPACE SessionGlobal;

    SessionGlobal = SESSION_GLOBAL(MmSessionSpace);

    LOCK_EXPANSION (OldIrql);

    NextProcessEntry = SessionGlobal->ProcessList.Flink;

    if ((SessionGlobal->u.Flags.DeletePending == 0) &&
        (NextProcessEntry != &SessionGlobal->ProcessList)) {

        TargetProcess = CONTAINING_RECORD (NextProcessEntry,
                                           EPROCESS,
                                           SessionProcessLinks);

        ObReferenceObject (TargetProcess);
    }
    else {
        TargetProcess = NULL;
    }

    UNLOCK_EXPANSION (OldIrql);

    if (TargetProcess != NULL) {
        KeStackAttachProcess (&TargetProcess->Pcb, ApcState);
    }

    return TargetProcess;
}

VOID
MiDetachFromSecureProcessInSession (
    IN PVOID OpaqueSession,
    IN PRKAPC_STATE ApcState
    )

/*++

Routine Description:

    This function allows callers to detach from the currently secure process
    in the current session (and dereference the secure process object).

Arguments:

    OpaqueSession - Supplies the opaque session value returned by
                    MiAttachToSecureProcessInSession.

    ApcState - Supplies APC state storage information for the detach.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PEPROCESS TargetProcess;

    TargetProcess = (PEPROCESS) OpaqueSession;

    KeUnstackDetachProcess (ApcState);

    ObDereferenceObject (TargetProcess);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\queryvm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   queryvm.c

Abstract:

    This module contains the routines which implement the
    NtQueryVirtualMemory service.

Author:

    Lou Perazzoli (loup) 21-Aug-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

extern POBJECT_TYPE IoFileObjectType;

NTSTATUS
MiGetWorkingSetInfo (
    IN PMEMORY_WORKING_SET_INFORMATION WorkingSetInfo,
    IN SIZE_T Length,
    IN PEPROCESS Process
    );

MMPTE
MiCaptureSystemPte (
    IN PMMPTE PointerProtoPte,
    IN PEPROCESS Process
    );

#if DBG
PEPROCESS MmWatchProcess;
#endif // DBG

ULONG
MiQueryAddressState (
    IN PVOID Va,
    IN PMMVAD Vad,
    IN PEPROCESS TargetProcess,
    OUT PULONG ReturnedProtect,
    OUT PVOID *NextVaToQuery
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtQueryVirtualMemory)
#pragma alloc_text(PAGE,MiQueryAddressState)
#pragma alloc_text(PAGE,MiGetWorkingSetInfo)
#endif


NTSTATUS
NtQueryVirtualMemory (
    IN HANDLE ProcessHandle,
    IN PVOID BaseAddress,
    IN MEMORY_INFORMATION_CLASS MemoryInformationClass,
    OUT PVOID MemoryInformation,
    IN SIZE_T MemoryInformationLength,
    OUT PSIZE_T ReturnLength OPTIONAL
     )

/*++

Routine Description:

    This function provides the capability to determine the state,
    protection, and type of a region of pages within the virtual address
    space of the subject process.

    The state of the first page within the region is determined and then
    subsequent entries in the process address map are scanned from the
    base address upward until either the entire range of pages has been
    scanned or until a page with a nonmatching set of attributes is
    encountered. The region attributes, the length of the region of pages
    with matching attributes, and an appropriate status value are
    returned.

    If the entire region of pages does not have a matching set of
    attributes, then the returned length parameter value can be used to
    calculate the address and length of the region of pages that was not
    scanned.

Arguments:


    ProcessHandle - An open handle to a process object.

    BaseAddress - The base address of the region of pages to be
                  queried. This value is rounded down to the next host-page-
                  address boundary.

    MemoryInformationClass - The memory information class about which
                             to retrieve information.

    MemoryInformation - A pointer to a buffer that receives the specified
                        information.  The format and content of the buffer
                        depend on the specified information class.


        MemoryBasicInformation - Data type is PMEMORY_BASIC_INFORMATION.

            MEMORY_BASIC_INFORMATION Structure


            ULONG RegionSize - The size of the region in bytes beginning at
                               the base address in which all pages have
                               identical attributes.

            ULONG State - The state of the pages within the region.

                State Values

                MEM_COMMIT - The state of the pages within the region
                             is committed.

                MEM_FREE - The state of the pages within the region
                           is free.

                MEM_RESERVE - The state of the pages within the
                              region is reserved.

            ULONG Protect - The protection of the pages within the region.

                Protect Values

                PAGE_NOACCESS - No access to the region of pages is allowed.
                                An attempt to read, write, or execute within
                                the region results in an access violation.

                PAGE_EXECUTE - Execute access to the region of pages
                               is allowed. An attempt to read or write within
                               the region results in an access violation.

                PAGE_READONLY - Read-only and execute access to the region
                                of pages is allowed. An attempt to write within
                                the region results in an access violation.

                PAGE_READWRITE - Read, write, and execute access to the region
                                 of pages is allowed. If write access to the
                                 underlying section is allowed, then a single
                                 copy of the pages are shared. Otherwise,
                                 the pages are shared read-only/copy-on-write.

                PAGE_GUARD - Read, write, and execute access to the
                             region of pages is allowed; however, access to
                             the region causes a "guard region entered"
                             condition to be raised in the subject process.

                PAGE_NOCACHE - Disable the placement of committed
                               pages into the data cache.

            ULONG Type - The type of pages within the region.

                Type Values

                MEM_PRIVATE - The pages within the region are private.

                MEM_MAPPED - The pages within the region are mapped
                             into the view of a section.

                MEM_IMAGE - The pages within the region are mapped
                            into the view of an image section.

    MemoryInformationLength - Specifies the length in bytes of
                              the memory information buffer.

    ReturnLength - An optional pointer which, if specified, receives the
                   number of bytes placed in the process information buffer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.

--*/

{
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS TargetProcess;
    NTSTATUS Status;
    PMMVAD Vad;
    PVOID Va;
    PVOID NextVaToQuery;
    LOGICAL Found;
    SIZE_T TheRegionSize;
    ULONG NewProtect;
    ULONG NewState;
    PVOID FilePointer;
    ULONG_PTR BaseVpn;
    MEMORY_BASIC_INFORMATION Info;
    PMEMORY_BASIC_INFORMATION BasicInfo;
    LOGICAL Attached;
    LOGICAL Leaped;
    ULONG MemoryInformationLengthUlong;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;

    Found = FALSE;
    Leaped = TRUE;
    FilePointer = NULL;

    //
    // Make sure the user's buffer is large enough for the requested operation.
    //
    // Check argument validity.
    //

    switch (MemoryInformationClass) {
        case MemoryBasicInformation:
            if (MemoryInformationLength < sizeof(MEMORY_BASIC_INFORMATION)) {
                return STATUS_INFO_LENGTH_MISMATCH;
            }
            break;

        case MemoryWorkingSetInformation:
            if (MemoryInformationLength < sizeof(ULONG_PTR)) {
                return STATUS_INFO_LENGTH_MISMATCH;
            }
            break;

        case MemoryMappedFilenameInformation:
            break;

        default:
            return STATUS_INVALID_INFO_CLASS;
    }

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    if (PreviousMode != KernelMode) {

        //
        // Check arguments.
        //

        try {

            ProbeForWrite(MemoryInformation,
                          MemoryInformationLength,
                          sizeof(ULONG_PTR));

            if (ARGUMENT_PRESENT(ReturnLength)) {
                ProbeForWriteUlong_ptr(ReturnLength);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }
    }

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER;
    }

    if ((BaseAddress >= MM_HIGHEST_VAD_ADDRESS)
#if defined(MM_SHARED_USER_DATA_VA)
            ||
         (PAGE_ALIGN(BaseAddress) == (PVOID)MM_SHARED_USER_DATA_VA)
#endif
             ) {

        //
        // Indicate a reserved area from this point on.
        //

        if (MemoryInformationClass == MemoryBasicInformation) {

            try {
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationBase =
                                      (PCHAR) MM_HIGHEST_VAD_ADDRESS + 1;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationProtect =
                                                                      PAGE_READONLY;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->BaseAddress =
                                                       PAGE_ALIGN(BaseAddress);
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->RegionSize =
                                    ((PCHAR)MM_HIGHEST_USER_ADDRESS + 1) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->State = MEM_RESERVE;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Protect = PAGE_NOACCESS;
                ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Type = MEM_PRIVATE;

                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
                }

#if defined(MM_SHARED_USER_DATA_VA)
                if (PAGE_ALIGN(BaseAddress) == (PVOID)MM_SHARED_USER_DATA_VA) {

                    //
                    // This is the page that is double mapped between
                    // user mode and kernel mode.
                    //

                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->AllocationBase =
                                (PVOID)MM_SHARED_USER_DATA_VA;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->Protect =
                                                                 PAGE_READONLY;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->RegionSize =
                                                                 PAGE_SIZE;
                    ((PMEMORY_BASIC_INFORMATION)MemoryInformation)->State =
                                                                 MEM_COMMIT;
                }
#endif

            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Just return success.
                //
            }

            return STATUS_SUCCESS;
        }
        else {
            return STATUS_INVALID_ADDRESS;
        }
    }

    if (ProcessHandle == NtCurrentProcess()) {
        TargetProcess = PsGetCurrentProcessByThread(CurrentThread);
    }
    else {
        Status = ObReferenceObjectByHandle (ProcessHandle,
                                            PROCESS_QUERY_INFORMATION,
                                            PsProcessType,
                                            PreviousMode,
                                            (PVOID *)&TargetProcess,
                                            NULL);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    if (MemoryInformationClass == MemoryWorkingSetInformation) {

        Status = MiGetWorkingSetInfo (
                            (PMEMORY_WORKING_SET_INFORMATION) MemoryInformation,
                            MemoryInformationLength,
                            TargetProcess);

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (TargetProcess);
        }

        //
        // If MiGetWorkingSetInfo failed then inform the caller.
        //

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        try {

            if (ARGUMENT_PRESENT(ReturnLength)) {
                *ReturnLength = ((((PMEMORY_WORKING_SET_INFORMATION)
                                    MemoryInformation)->NumberOfEntries - 1) *
                                        sizeof(ULONG)) +
                                        sizeof(MEMORY_WORKING_SET_INFORMATION);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
        }

        return STATUS_SUCCESS;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    //
    // Get working set mutex and block APCs.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject (TargetProcess);
        }
        return STATUS_PROCESS_IS_TERMINATING;
    }

    //
    // Locate the VAD that contains the base address or the VAD
    // which follows the base address.
    //

    Vad = TargetProcess->VadRoot;
    BaseVpn = MI_VA_TO_VPN (BaseAddress);

    for (;;) {

        if (Vad == NULL) {
            break;
        }

        if ((BaseVpn >= Vad->StartingVpn) &&
            (BaseVpn <= Vad->EndingVpn)) {
            Found = TRUE;
            break;
        }

        if (BaseVpn < Vad->StartingVpn) {
            if (Vad->LeftChild == NULL) {
                break;
            }
            Vad = Vad->LeftChild;

        }
        else {
            if (BaseVpn < Vad->EndingVpn) {
                break;
            }
            if (Vad->RightChild == NULL) {
                break;
            }
            Vad = Vad->RightChild;
        }
    }

    if (!Found) {

        //
        // There is no virtual address allocated at the base
        // address.  Return the size of the hole starting at
        // the base address.
        //

        if (Vad == NULL) {
            TheRegionSize = ((PCHAR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
        }
        else {
            if (Vad->StartingVpn < BaseVpn) {

                //
                // We are looking at the Vad which occupies the range
                // just before the desired range.  Get the next Vad.
                //

                Vad = MiGetNextVad (Vad);
                if (Vad == NULL) {
                    TheRegionSize = ((PCHAR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
                }
                else {
                    TheRegionSize = (PCHAR)MI_VPN_TO_VA (Vad->StartingVpn) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
                }
            }
            else {
                TheRegionSize = (PCHAR)MI_VPN_TO_VA (Vad->StartingVpn) -
                                                (PCHAR)PAGE_ALIGN(BaseAddress);
            }
        }

        UNLOCK_ADDRESS_SPACE (TargetProcess);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject (TargetProcess);
        }

        //
        // Establish an exception handler and write the information and
        // returned length.
        //

        if (MemoryInformationClass == MemoryBasicInformation) {
            BasicInfo = (PMEMORY_BASIC_INFORMATION) MemoryInformation;
            Found = FALSE;
            try {

                BasicInfo->AllocationBase = NULL;
                BasicInfo->AllocationProtect = 0;
                BasicInfo->BaseAddress = PAGE_ALIGN(BaseAddress);
                BasicInfo->RegionSize = TheRegionSize;
                BasicInfo->State = MEM_FREE;
                BasicInfo->Protect = PAGE_NOACCESS;
                BasicInfo->Type = 0;

                Found = TRUE;
                if (ARGUMENT_PRESENT(ReturnLength)) {
                    *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
                }

            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Just return success if the BasicInfo was successfully
                // filled in.
                //
                
                if (Found == FALSE) {
                    return GetExceptionCode ();
                }
            }

            return STATUS_SUCCESS;
        }
        return STATUS_INVALID_ADDRESS;
    }

    //
    // Found a VAD.
    //

    Va = PAGE_ALIGN(BaseAddress);
    Info.BaseAddress = Va;

    //
    // There is a page mapped at the base address.
    //

    if (Vad->u.VadFlags.PrivateMemory) {
        Info.Type = MEM_PRIVATE;
    }
    else {
        if (Vad->u.VadFlags.ImageMap == 1) {
            Info.Type = MEM_IMAGE;
        }
        else {
            Info.Type = MEM_MAPPED;
        }

        if (MemoryInformationClass == MemoryMappedFilenameInformation) {

            if (Vad->ControlArea) {
                FilePointer = Vad->ControlArea->FilePointer;
            }
            if (FilePointer == NULL) {
                FilePointer = (PVOID)1;
            }
            else {
                ObReferenceObject(FilePointer);
            }
        }
    }

    LOCK_WS_UNSAFE (TargetProcess);

    Info.State = MiQueryAddressState (Va,
                                      Vad,
                                      TargetProcess,
                                      &Info.Protect,
                                      &NextVaToQuery);

    Va = NextVaToQuery;

    while (MI_VA_TO_VPN (Va) <= Vad->EndingVpn) {

        NewState = MiQueryAddressState (Va,
                                        Vad,
                                        TargetProcess,
                                        &NewProtect,
                                        &NextVaToQuery);

        if ((NewState != Info.State) || (NewProtect != Info.Protect)) {

            //
            // The state for this address does not match, calculate
            // size and return.
            //

            Leaped = FALSE;
            break;
        }
        Va = NextVaToQuery;
    }

    UNLOCK_WS_UNSAFE (TargetProcess);

    //
    // We may have aggressively leaped past the end of the VAD.  Shorten the
    // Va here if we did.
    //

    if (Leaped == TRUE) {
        Va = MI_VPN_TO_VA (Vad->EndingVpn + 1);
    }

    Info.RegionSize = ((PCHAR)Va - (PCHAR)Info.BaseAddress);
    Info.AllocationBase = MI_VPN_TO_VA (Vad->StartingVpn);
    Info.AllocationProtect = MI_CONVERT_FROM_PTE_PROTECTION (
                                             Vad->u.VadFlags.Protection);

    //
    // A range has been found, release the mutexes, detach from the
    // target process and return the information.
    //

#if defined(_MIALT4K_)

    if (TargetProcess->Wow64Process != NULL) {
        
        Info.BaseAddress = PAGE_4K_ALIGN(BaseAddress);

        MiQueryRegionFor4kPage (Info.BaseAddress,
                                MI_VPN_TO_VA_ENDING(Vad->EndingVpn),
                                &Info.RegionSize,
                                &Info.State,
                                &Info.Protect,
                                TargetProcess);
    }

#endif

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        ObDereferenceObject (TargetProcess);
    }

    if (MemoryInformationClass == MemoryBasicInformation) {
        Found = FALSE;
        try {

            *(PMEMORY_BASIC_INFORMATION)MemoryInformation = Info;

            Found = TRUE;
            if (ARGUMENT_PRESENT(ReturnLength)) {
                *ReturnLength = sizeof(MEMORY_BASIC_INFORMATION);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // Just return success if the BasicInfo was successfully
            // filled in.
            //
                
            if (Found == FALSE) {
                return GetExceptionCode ();
            }
        }
        return STATUS_SUCCESS;
    }

    //
    // Try to return the name of the file that is mapped.
    //

    if (FilePointer == NULL) {
        return STATUS_INVALID_ADDRESS;
    }

    if (FilePointer == (PVOID)1) {
        return STATUS_FILE_INVALID;
    }

    MemoryInformationLengthUlong = (ULONG)MemoryInformationLength;

    if ((SIZE_T)MemoryInformationLengthUlong < MemoryInformationLength) {
        return STATUS_INVALID_PARAMETER_5;
    }
    
    //
    // We have a referenced pointer to the file.  Call ObQueryNameString
    // and get the file name.
    //

    Status = ObQueryNameString (FilePointer,
                                (POBJECT_NAME_INFORMATION) MemoryInformation,
                                 MemoryInformationLengthUlong,
                                 (PULONG)ReturnLength);

    ObDereferenceObject (FilePointer);

    return Status;
}


ULONG
MiQueryAddressState (
    IN PVOID Va,
    IN PMMVAD Vad,
    IN PEPROCESS TargetProcess,
    OUT PULONG ReturnedProtect,
    OUT PVOID *NextVaToQuery
    )

/*++

Routine Description:


Arguments:

Return Value:

    Returns the state (MEM_COMMIT, MEM_RESERVE, MEM_PRIVATE).

Environment:

    Kernel mode.  Working set lock and address creation lock held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    MMPTE CapturedProtoPte;
    PMMPTE ProtoPte;
    LOGICAL PteIsZero;
    ULONG State;
    ULONG Protect;
    ULONG Waited;
    LOGICAL PteDetected;
    PVOID NextVa;

    State = MEM_RESERVE;
    Protect = 0;

#ifdef LARGE_PAGES
    if (Vad->u.VadFlags.LargePages) {
        *ReturnedProtect = MI_CONVERT_FROM_PTE_PROTECTION (
                                             Vad->u.VadFlags.Protection);
        return MEM_COMMIT;
    }
#endif //LARGE_PAGES

    PointerPxe = MiGetPxeAddress (Va);
    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);

    ASSERT ((Vad->StartingVpn <= MI_VA_TO_VPN (Va)) &&
            (Vad->EndingVpn >= MI_VA_TO_VPN (Va)));

    PteIsZero = TRUE;
    PteDetected = FALSE;

    *NextVaToQuery = (PVOID)((PCHAR)Va + PAGE_SIZE);

    do {

        if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                         TargetProcess,
                                         FALSE,
                                         &Waited)) {

#if (_MI_PAGING_LEVELS >= 4)
            NextVa = MiGetVirtualAddressMappedByPte (PointerPxe + 1);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
#endif
            break;
        }
    
#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                         TargetProcess,
                                         FALSE,
                                         &Waited)) {
#if (_MI_PAGING_LEVELS >= 3)
            NextVa = MiGetVirtualAddressMappedByPte (PointerPpe + 1);
            NextVa = MiGetVirtualAddressMappedByPte (NextVa);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
#endif
            break;
        }
    
#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

        if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                         TargetProcess,
                                         FALSE,
                                         &Waited)) {
            NextVa = MiGetVirtualAddressMappedByPte (PointerPde + 1);
            *NextVaToQuery = MiGetVirtualAddressMappedByPte (NextVa);
            break;
        }

        if (Waited == 0) {
            PteDetected = TRUE;
        }

    } while (Waited != 0);

    if (PteDetected == TRUE) {

        //
        // A PTE exists at this address, see if it is zero.
        //

        if (PointerPte->u.Long != 0) {

            PteIsZero = FALSE;

            //
            // There is a non-zero PTE at this address, use
            // it to build the information block.
            //

            if (MiIsPteDecommittedPage (PointerPte)) {
                ASSERT (Protect == 0);
                ASSERT (State == MEM_RESERVE);
            }
            else {
                State = MEM_COMMIT;
                if (Vad->u.VadFlags.PhysicalMapping == 1) {

                    //
                    // Physical mapping, there is no corresponding
                    // PFN element to get the page protection from.
                    //

                    Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                             Vad->u.VadFlags.Protection);
                }
                else {
                    Protect = MiGetPageProtection (PointerPte,
                                                   TargetProcess,
                                                   FALSE);

                    if ((PointerPte->u.Soft.Valid == 0) &&
                        (PointerPte->u.Soft.Prototype == 1) &&
                        (Vad->u.VadFlags.PrivateMemory == 0) &&
                        (Vad->ControlArea != (PCONTROL_AREA)NULL)) {

                        //
                        // Make sure the protoPTE is committed.
                        //

                        ProtoPte = MiGetProtoPteAddress(Vad,
                                                    MI_VA_TO_VPN (Va));
                        CapturedProtoPte.u.Long = 0;
                        if (ProtoPte) {
                            CapturedProtoPte = MiCaptureSystemPte (ProtoPte,
                                                               TargetProcess);
                        }
                        if (CapturedProtoPte.u.Long == 0) {
                            State = MEM_RESERVE;
                            Protect = 0;
                        }
                    }
                }
            }
        }
    }

    if (PteIsZero) {

        //
        // There is no PDE at this address, the template from
        // the VAD supplies the information unless the VAD is
        // for an image file.  For image files the individual
        // protection is on the prototype PTE.
        //

        //
        // Get the default protection information.
        //

        State = MEM_RESERVE;
        Protect = 0;

        if (Vad->u.VadFlags.PhysicalMapping == 1) {

            //
            // Must be banked memory, just return reserved.
            //

            NOTHING;

        } else if ((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != (PCONTROL_AREA)NULL)) {

            //
            // This VAD refers to a section.  Even though the PTE is
            // zero, the actual page may be committed in the section.
            //

            *NextVaToQuery = (PVOID)((PCHAR)Va + PAGE_SIZE);

            ProtoPte = MiGetProtoPteAddress(Vad, MI_VA_TO_VPN (Va));

            CapturedProtoPte.u.Long = 0;
            if (ProtoPte) {
                CapturedProtoPte = MiCaptureSystemPte (ProtoPte,
                                                       TargetProcess);
            }

            if (CapturedProtoPte.u.Long != 0) {
                State = MEM_COMMIT;

                if (Vad->u.VadFlags.ImageMap == 0) {
                    Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                              Vad->u.VadFlags.Protection);
                }
                else {

                    //
                    // This is an image file, the protection is in the
                    // prototype PTE.
                    //

                    Protect = MiGetPageProtection (&CapturedProtoPte,
                                                   TargetProcess,
                                                   TRUE);
                }
            }

        }
        else {

            //
            // Get the protection from the corresponding VAD.
            //

            if (Vad->u.VadFlags.MemCommit) {
                State = MEM_COMMIT;
                Protect = MI_CONVERT_FROM_PTE_PROTECTION (
                                            Vad->u.VadFlags.Protection);
            }
        }
    }

    *ReturnedProtect = Protect;
    return State;
}



NTSTATUS
MiGetWorkingSetInfo (
    IN PMEMORY_WORKING_SET_INFORMATION WorkingSetInfo,
    IN SIZE_T Length,
    IN PEPROCESS Process
    )

{
    PMDL Mdl;
    PMEMORY_WORKING_SET_INFORMATION Info;
    PMEMORY_WORKING_SET_BLOCK Entry;
#if DBG
    PMEMORY_WORKING_SET_BLOCK LastEntry;
#endif
    PMMWSLE Wsle;
    PMMWSLE LastWsle;
    WSLE_NUMBER WsSize;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    NTSTATUS status;
    LOGICAL Attached;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;

    //
    // Allocate an MDL to map the request.
    //

    Mdl = ExAllocatePoolWithTag (NonPagedPool,
                                 sizeof(MDL) + sizeof(PFN_NUMBER) +
                                     BYTES_TO_PAGES (Length) * sizeof(PFN_NUMBER),
                                 '  mM');

    if (Mdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initialize the MDL for the request.
    //

    MmInitializeMdl(Mdl, WorkingSetInfo, Length);

    CurrentThread = PsGetCurrentThread ();

    try {
        MmProbeAndLockPages (Mdl,
                             KeGetPreviousModeByThread (&CurrentThread->Tcb),
                             IoWriteAccess);

    } except (EXCEPTION_EXECUTE_HANDLER) {

        ExFreePool (Mdl);
        return GetExceptionCode();
    }

    Info = MmGetSystemAddressForMdlSafe (Mdl, NormalPagePriority);

    if (Info == NULL) {
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (PsGetCurrentProcessByThread (CurrentThread) != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    status = STATUS_SUCCESS;

    LOCK_WS (Process);

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
    }
    else {
        WsSize = Process->Vm.WorkingSetSize;
        ASSERT (WsSize != 0);
        Info->NumberOfEntries = WsSize;
        if (sizeof(MEMORY_WORKING_SET_INFORMATION) + (WsSize-1) * sizeof(ULONG_PTR) > Length) {
            status = STATUS_INFO_LENGTH_MISMATCH;
        }
    }

    if (!NT_SUCCESS(status)) {

        UNLOCK_WS (Process);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        MmUnlockPages (Mdl);
        ExFreePool (Mdl);
        return status;
    }

    Wsle = MmWsle;
    LastWsle = &MmWsle[MmWorkingSetList->LastEntry];
    Entry = &Info->WorkingSetInfo[0];

#if DBG
    LastEntry = (PMEMORY_WORKING_SET_BLOCK)(
                            (PCHAR)Info + (Length & (~(sizeof(ULONG_PTR) - 1))));
#endif

    do {
        if (Wsle->u1.e1.Valid == 1) {
            Entry->VirtualPage = Wsle->u1.e1.VirtualPageNumber;
            PointerPte = MiGetPteAddress (Wsle->u1.VirtualAddress);
            ASSERT (PointerPte->u.Hard.Valid == 1);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

#if defined(MI_MULTINODE)
            Entry->Node = Pfn1->u3.e1.PageColor;
#else
            Entry->Node = 0;
#endif
            Entry->Shared = Pfn1->u3.e1.PrototypePte;
            if (Pfn1->u3.e1.PrototypePte == 0) {
                Entry->ShareCount = 0;
                Entry->Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn1->OriginalPte);
            }
            else {
                if (Pfn1->u2.ShareCount <= 7) {
                    Entry->ShareCount = Pfn1->u2.ShareCount;
                }
                else {
                    Entry->ShareCount = 7;
                }
                if (Wsle->u1.e1.SameProtectAsProto == 1) {
                    Entry->Protection = MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn1->OriginalPte);
                }
                else {
                    Entry->Protection = Wsle->u1.e1.Protection;
                }
            }
            Entry += 1;
        }
        Wsle += 1;
#if DBG
        ASSERT ((Entry < LastEntry) || (Wsle > LastWsle));
#endif
    } while (Wsle <= LastWsle);

    UNLOCK_WS (Process);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    MmUnlockPages (Mdl);
    ExFreePool (Mdl);
    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\readwrt.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   readwrt.c

Abstract:

    This module contains the routines which implement the capability
    to read and write the virtual memory of a target process.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

//
// The maximum amount to try to Probe and Lock is 14 pages, this
// way it always fits in a 16 page allocation.
//

#define MAX_LOCK_SIZE ((ULONG)(14 * PAGE_SIZE))

//
// The maximum to move in a single block is 64k bytes.
//

#define MAX_MOVE_SIZE (LONG)0x10000

//
// The minimum to move is a single block is 128 bytes.
//

#define MINIMUM_ALLOCATION (LONG)128

//
// Define the pool move threshold value.
//

#define POOL_MOVE_THRESHOLD 511

//
// Define forward referenced procedure prototypes.
//

ULONG
MiGetExceptionInfo (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PLOGICAL ExceptionAddressConfirmed,
    IN PULONG_PTR BadVa
    );

NTSTATUS
MiDoMappedCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     );

NTSTATUS
MiDoPoolCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiGetExceptionInfo)
#pragma alloc_text(PAGE,NtReadVirtualMemory)
#pragma alloc_text(PAGE,NtWriteVirtualMemory)
#pragma alloc_text(PAGE,MiDoMappedCopy)
#pragma alloc_text(PAGE,MiDoPoolCopy)
#pragma alloc_text(PAGE,MmCopyVirtualMemory)
#endif

#define COPY_STACK_SIZE 64

NTSTATUS
NtReadVirtualMemory (
     IN HANDLE ProcessHandle,
     IN PVOID BaseAddress,
     OUT PVOID Buffer,
     IN SIZE_T BufferSize,
     OUT PSIZE_T NumberOfBytesRead OPTIONAL
     )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address in the specified process
                   to be read.

     Buffer - Supplies the address of a buffer which receives the
              contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    SIZE_T BytesCopied;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    PETHREAD CurrentThread;

    PAGED_CODE();

    //
    // Get the previous mode and probe output argument if necessary.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);
    if (PreviousMode != KernelMode) {

        if (((PCHAR)BaseAddress + BufferSize < (PCHAR)BaseAddress) ||
            ((PCHAR)Buffer + BufferSize < (PCHAR)Buffer) ||
            ((PVOID)((PCHAR)BaseAddress + BufferSize) > MM_HIGHEST_USER_ADDRESS) ||
            ((PVOID)((PCHAR)Buffer + BufferSize) > MM_HIGHEST_USER_ADDRESS)) {

            return STATUS_ACCESS_VIOLATION;
        }

        if (ARGUMENT_PRESENT(NumberOfBytesRead)) {
            try {
                ProbeForWriteUlong_ptr (NumberOfBytesRead);

            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }
    }

    //
    // If the buffer size is not zero, then attempt to read data from the
    // specified process address space into the current process address
    // space.
    //

    BytesCopied = 0;
    Status = STATUS_SUCCESS;
    if (BufferSize != 0) {

        //
        // Reference the target process.
        //

        Status = ObReferenceObjectByHandle(ProcessHandle,
                                           PROCESS_VM_READ,
                                           PsProcessType,
                                           PreviousMode,
                                           (PVOID *)&Process,
                                           NULL);

        //
        // If the process was successfully referenced, then attempt to
        // read the specified memory either by direct mapping or copying
        // through nonpaged pool.
        //

        if (Status == STATUS_SUCCESS) {

            Status = MmCopyVirtualMemory (Process,
                                          BaseAddress,
                                          PsGetCurrentProcessByThread(CurrentThread),
                                          Buffer,
                                          BufferSize,
                                          PreviousMode,
                                          &BytesCopied);

            //
            // Dereference the target process.
            //

            ObDereferenceObject(Process);
        }
    }

    //
    // If requested, return the number of bytes read.
    //

    if (ARGUMENT_PRESENT(NumberOfBytesRead)) {
        try {
            *NumberOfBytesRead = BytesCopied;

        } except(EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }
    }

    return Status;
}
NTSTATUS
NtWriteVirtualMemory(
     IN HANDLE ProcessHandle,
     OUT PVOID BaseAddress,
     IN CONST VOID *Buffer,
     IN SIZE_T BufferSize,
     OUT PSIZE_T NumberOfBytesWritten OPTIONAL
     )

/*++

Routine Description:

    This function copies the specified address range from the current
    process into the specified address range of the specified process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address to be written to in the
                   specified process.

     Buffer - Supplies the address of a buffer which contains the
              contents to be written into the specified process
              address space.

     BufferSize - Supplies the requested number of bytes to write
                  into the specified process.

     NumberOfBytesWritten - Receives the actual number of bytes
                            transferred into the specified address space.

Return Value:

    NTSTATUS.

--*/

{
    SIZE_T BytesCopied;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    PETHREAD CurrentThread;

    PAGED_CODE();

    //
    // Get the previous mode and probe output argument if necessary.
    //

    CurrentThread = PsGetCurrentThread ();
    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);
    if (PreviousMode != KernelMode) {

        if (((PCHAR)BaseAddress + BufferSize < (PCHAR)BaseAddress) ||
            ((PCHAR)Buffer + BufferSize < (PCHAR)Buffer) ||
            ((PVOID)((PCHAR)BaseAddress + BufferSize) > MM_HIGHEST_USER_ADDRESS) ||
            ((PVOID)((PCHAR)Buffer + BufferSize) > MM_HIGHEST_USER_ADDRESS)) {

            return STATUS_ACCESS_VIOLATION;
        }

        if (ARGUMENT_PRESENT(NumberOfBytesWritten)) {
            try {
                ProbeForWriteUlong_ptr(NumberOfBytesWritten);

            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }
    }

    //
    // If the buffer size is not zero, then attempt to write data from the
    // current process address space into the target process address space.
    //

    BytesCopied = 0;
    Status = STATUS_SUCCESS;
    if (BufferSize != 0) {

        //
        // Reference the target process.
        //

        Status = ObReferenceObjectByHandle(ProcessHandle,
                                           PROCESS_VM_WRITE,
                                           PsProcessType,
                                           PreviousMode,
                                           (PVOID *)&Process,
                                           NULL);

        //
        // If the process was successfully referenced, then attempt to
        // write the specified memory either by direct mapping or copying
        // through nonpaged pool.
        //

        if (Status == STATUS_SUCCESS) {

            Status = MmCopyVirtualMemory (PsGetCurrentProcessByThread(CurrentThread),
                                          Buffer,
                                          Process,
                                          BaseAddress,
                                          BufferSize,
                                          PreviousMode,
                                          &BytesCopied);

            //
            // Dereference the target process.
            //

            ObDereferenceObject(Process);
        }
    }

    //
    // If requested, return the number of bytes read.
    //

    if (ARGUMENT_PRESENT(NumberOfBytesWritten)) {
        try {
            *NumberOfBytesWritten = BytesCopied;

        } except(EXCEPTION_EXECUTE_HANDLER) {
            NOTHING;
        }
    }

    return Status;
}


NTSTATUS
MmCopyVirtualMemory(
    IN PEPROCESS FromProcess,
    IN CONST VOID *FromAddress,
    IN PEPROCESS ToProcess,
    OUT PVOID ToAddress,
    IN SIZE_T BufferSize,
    IN KPROCESSOR_MODE PreviousMode,
    OUT PSIZE_T NumberOfBytesCopied
    )
{
    NTSTATUS Status;
    PEPROCESS ProcessToLock;

    if (BufferSize == 0) {
        ASSERT (FALSE);         // No one should call with a zero size.
        return STATUS_SUCCESS;
    }

    ProcessToLock = FromProcess;
    if (FromProcess == PsGetCurrentProcess()) {
        ProcessToLock = ToProcess;
    }

    //
    // Make sure the process still has an address space.
    //

    if (ExAcquireRundownProtection (&ProcessToLock->RundownProtect) == FALSE) {
        return STATUS_PROCESS_IS_TERMINATING;
    }

    //
    // If the buffer size is greater than the pool move threshold,
    // then attempt to write the memory via direct mapping.
    //

    if (BufferSize > POOL_MOVE_THRESHOLD) {
        Status = MiDoMappedCopy(FromProcess,
                                FromAddress,
                                ToProcess,
                                ToAddress,
                                BufferSize,
                                PreviousMode,
                                NumberOfBytesCopied);

        //
        // If the completion status is not a working quota problem,
        // then finish the service. Otherwise, attempt to write the
        // memory through nonpaged pool.
        //

        if (Status != STATUS_WORKING_SET_QUOTA) {
            goto CompleteService;
        }

        *NumberOfBytesCopied = 0;
    }

    //
    // There was not enough working set quota to write the memory via
    // direct mapping or the size of the write was below the pool move
    // threshold. Attempt to write the specified memory through nonpaged
    // pool.
    //

    Status = MiDoPoolCopy(FromProcess,
                          FromAddress,
                          ToProcess,
                          ToAddress,
                          BufferSize,
                          PreviousMode,
                          NumberOfBytesCopied);

    //
    // Dereference the target process.
    //

CompleteService:

    //
    // Indicate that the vm operation is complete.
    //

    ExReleaseRundownProtection (&ProcessToLock->RundownProtect);

    return Status;
}


ULONG
MiGetExceptionInfo (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN OUT PLOGICAL ExceptionAddressConfirmed,
    IN OUT PULONG_PTR BadVa
    )

/*++

Routine Description:

    This routine examines a exception record and extracts the virtual
    address of an access violation, guard page violation, or in-page error.

Arguments:

    ExceptionPointers - Supplies a pointer to the exception record.

    ExceptionAddressConfirmed - Receives TRUE if the exception address was
                                reliably detected, FALSE if not.

    BadVa - Receives the virtual address which caused the access violation.

Return Value:

    EXECUTE_EXCEPTION_HANDLER

--*/

{
    PEXCEPTION_RECORD ExceptionRecord;

    PAGED_CODE();

    //
    // If the exception code is an access violation, guard page violation,
    // or an in-page read error, then return the faulting address. Otherwise.
    // return a special address value.
    //

    *ExceptionAddressConfirmed = FALSE;

    ExceptionRecord = ExceptionPointers->ExceptionRecord;

    if ((ExceptionRecord->ExceptionCode == STATUS_ACCESS_VIOLATION) ||
        (ExceptionRecord->ExceptionCode == STATUS_GUARD_PAGE_VIOLATION) ||
        (ExceptionRecord->ExceptionCode == STATUS_IN_PAGE_ERROR)) {

        //
        // The virtual address which caused the exception is the 2nd
        // parameter in the exception information array.
        //
        // The number of parameters will be zero if an exception handler
        // above us (like the one in MmProbeAndLockPages) caught the
        // original exception and subsequently just raised status.
        // This means the number of bytes copied is zero.
        //

        if (ExceptionRecord->NumberParameters > 1) {
            *ExceptionAddressConfirmed = TRUE;
            *BadVa = ExceptionRecord->ExceptionInformation[1];
        }
    }

    return EXCEPTION_EXECUTE_HANDLER;
}

NTSTATUS
MiDoMappedCopy (
    IN PEPROCESS FromProcess,
    IN CONST VOID *FromAddress,
    IN PEPROCESS ToProcess,
    OUT PVOID ToAddress,
    IN SIZE_T BufferSize,
    IN KPROCESSOR_MODE PreviousMode,
    OUT PSIZE_T NumberOfBytesRead
    )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     FromProcess - Supplies an open handle to a process object.

     FromAddress - Supplies the base address in the specified process
                   to be read.

     ToProcess - Supplies an open handle to a process object.

     ToAddress - Supplies the address of a buffer which receives the
                 contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     PreviousMode - Supplies the previous processor mode.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    SIZE_T AmountToMove;
    ULONG_PTR BadVa;
    LOGICAL Moving;
    LOGICAL Probing;
    LOGICAL LockedMdlPages;
    CONST VOID *InVa;
    SIZE_T LeftToMove;
    PSIZE_T MappedAddress;
    SIZE_T MaximumMoved;
    PMDL Mdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + (MAX_LOCK_SIZE >> PAGE_SHIFT) + 1];
    PVOID OutVa;
    LOGICAL MappingFailed;
    LOGICAL ExceptionAddressConfirmed;

    PAGED_CODE();

    MappingFailed = FALSE;

    InVa = FromAddress;
    OutVa = ToAddress;

    MaximumMoved = MAX_LOCK_SIZE;
    if (BufferSize <= MAX_LOCK_SIZE) {
        MaximumMoved = BufferSize;
    }

    Mdl = (PMDL)&MdlHack[0];

    //
    // Map the data into the system part of the address space, then copy it.
    //

    LeftToMove = BufferSize;
    AmountToMove = MaximumMoved;

    Probing = FALSE;

    //
    // Initializing BadVa & ExceptionAddressConfirmed is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BadVa = 0;
    ExceptionAddressConfirmed = FALSE;

#if 0

    //
    // It is unfortunate that Windows 2000 and all the releases of NT always
    // inadvertently returned from this routine detached, as we must maintain
    // this behavior even now.
    //

    KeDetachProcess();

#endif

    while (LeftToMove > 0) {

        if (LeftToMove < AmountToMove) {

            //
            // Set to move the remaining bytes.
            //

            AmountToMove = LeftToMove;
        }

        KeStackAttachProcess (&FromProcess->Pcb, &ApcState);

        MappedAddress = NULL;
        LockedMdlPages = FALSE;
        Moving = FALSE;
        ASSERT (Probing == FALSE);

        //
        // We may be touching a user's memory which could be invalid,
        // declare an exception handler.
        //

        try {

            //
            // Probe to make sure that the specified buffer is accessible in
            // the target process.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForRead (FromAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            //
            // Initialize MDL for request.
            //

            MmInitializeMdl (Mdl, (PVOID)InVa, AmountToMove);

            MmProbeAndLockPages (Mdl, PreviousMode, IoReadAccess);

            LockedMdlPages = TRUE;

            MappedAddress = MmMapLockedPagesSpecifyCache (Mdl,
                                                          KernelMode,
                                                          MmCached,
                                                          NULL,
                                                          FALSE,
                                                          HighPagePriority);

            if (MappedAddress == NULL) {
                MappingFailed = TRUE;
                ExRaiseStatus(STATUS_INSUFFICIENT_RESOURCES);
            }

            //
            // Deattach from the FromProcess and attach to the ToProcess.
            //

            KeUnstackDetachProcess (&ApcState);
            KeStackAttachProcess (&ToProcess->Pcb, &ApcState);

            //
            // Now operating in the context of the ToProcess.
            //
            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForWrite (ToAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            Moving = TRUE;
            RtlCopyMemory (OutVa, MappedAddress, AmountToMove);

        } except (MiGetExceptionInfo (GetExceptionInformation(),
                                      &ExceptionAddressConfirmed,
                                      &BadVa)) {


            //
            // If an exception occurs during the move operation or probe,
            // return the exception code as the status value.
            //

            KeUnstackDetachProcess (&ApcState);

            if (MappedAddress != NULL) {
                MmUnmapLockedPages (MappedAddress, Mdl);
            }
            if (LockedMdlPages == TRUE) {
                MmUnlockPages (Mdl);
            }

            if (GetExceptionCode() == STATUS_WORKING_SET_QUOTA) {
                return STATUS_WORKING_SET_QUOTA;
            }

            if ((Probing == TRUE) || (MappingFailed == TRUE)) {
                return GetExceptionCode();

            }

            //
            // If the failure occurred during the move operation, determine
            // which move failed, and calculate the number of bytes
            // actually moved.
            //

            *NumberOfBytesRead = BufferSize - LeftToMove;

            if (Moving == TRUE) {
                if (ExceptionAddressConfirmed == TRUE) {
                    *NumberOfBytesRead = (SIZE_T)((ULONG_PTR)BadVa - (ULONG_PTR)FromAddress);
                }
            }

            return STATUS_PARTIAL_COPY;
        }

        KeUnstackDetachProcess (&ApcState);

        MmUnmapLockedPages (MappedAddress, Mdl);
        MmUnlockPages (Mdl);

        LeftToMove -= AmountToMove;
        InVa = (PVOID)((ULONG_PTR)InVa + AmountToMove);
        OutVa = (PVOID)((ULONG_PTR)OutVa + AmountToMove);
    }

    //
    // Set number of bytes moved.
    //

    *NumberOfBytesRead = BufferSize;
    return STATUS_SUCCESS;
}

NTSTATUS
MiDoPoolCopy (
     IN PEPROCESS FromProcess,
     IN CONST VOID *FromAddress,
     IN PEPROCESS ToProcess,
     OUT PVOID ToAddress,
     IN SIZE_T BufferSize,
     IN KPROCESSOR_MODE PreviousMode,
     OUT PSIZE_T NumberOfBytesRead
     )

/*++

Routine Description:

    This function copies the specified address range from the specified
    process into the specified address range of the current process.

Arguments:

     ProcessHandle - Supplies an open handle to a process object.

     BaseAddress - Supplies the base address in the specified process
                   to be read.

     Buffer - Supplies the address of a buffer which receives the
              contents from the specified process address space.

     BufferSize - Supplies the requested number of bytes to read from
                  the specified process.

     PreviousMode - Supplies the previous processor mode.

     NumberOfBytesRead - Receives the actual number of bytes
                         transferred into the specified buffer.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    SIZE_T AmountToMove;
    LOGICAL ExceptionAddressConfirmed;
    ULONG_PTR BadVa;
    PEPROCESS CurrentProcess;
    LOGICAL Moving;
    LOGICAL Probing;
    CONST VOID *InVa;
    SIZE_T LeftToMove;
    SIZE_T MaximumMoved;
    PVOID OutVa;
    PVOID PoolArea;
    LONGLONG StackArray[COPY_STACK_SIZE];
    ULONG FreePool;

    PAGED_CODE();

    ASSERT (BufferSize != 0);

    //
    // Get the address of the current process object and initialize copy
    // parameters.
    //

    CurrentProcess = PsGetCurrentProcess();

    InVa = FromAddress;
    OutVa = ToAddress;

    //
    // Allocate non-paged memory to copy in and out of.
    //

    MaximumMoved = MAX_MOVE_SIZE;
    if (BufferSize <= MAX_MOVE_SIZE) {
        MaximumMoved = BufferSize;
    }

    FreePool = FALSE;
    if (BufferSize <= sizeof(StackArray)) {
        PoolArea = (PVOID)&StackArray[0];
    } else {
        do {
            PoolArea = ExAllocatePoolWithTag (NonPagedPool, MaximumMoved, 'wRmM');
            if (PoolArea != NULL) {
                FreePool = TRUE;
                break;
            }

            MaximumMoved = MaximumMoved >> 1;
            if (MaximumMoved <= sizeof(StackArray)) {
                PoolArea = (PVOID)&StackArray[0];
                break;
            }
        } while (TRUE);
    }

    //
    // Initializing BadVa & ExceptionAddressConfirmed is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    BadVa = 0;
    ExceptionAddressConfirmed = FALSE;

    //
    // Copy the data into pool, then copy back into the ToProcess.
    //

    LeftToMove = BufferSize;
    AmountToMove = MaximumMoved;
    Probing = FALSE;

#if 0

    //
    // It is unfortunate that Windows 2000 and all the releases of NT always
    // inadvertently returned from this routine detached, as we must maintain
    // this behavior even now.
    //

    KeDetachProcess();

#endif

    while (LeftToMove > 0) {

        if (LeftToMove < AmountToMove) {

            //
            // Set to move the remaining bytes.
            //

            AmountToMove = LeftToMove;
        }

        KeStackAttachProcess (&FromProcess->Pcb, &ApcState);

        Moving = FALSE;
        ASSERT (Probing == FALSE);

        //
        // We may be touching a user's memory which could be invalid,
        // declare an exception handler.
        //

        try {

            //
            // Probe to make sure that the specified buffer is accessible in
            // the target process.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForRead (FromAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            RtlCopyMemory (PoolArea, InVa, AmountToMove);

            KeUnstackDetachProcess (&ApcState);

            KeStackAttachProcess (&ToProcess->Pcb, &ApcState);

            //
            // Now operating in the context of the ToProcess.
            //

            if ((InVa == FromAddress) && (PreviousMode != KernelMode)){
                Probing = TRUE;
                ProbeForWrite (ToAddress, BufferSize, sizeof(CHAR));
                Probing = FALSE;
            }

            Moving = TRUE;

            RtlCopyMemory (OutVa, PoolArea, AmountToMove);

        } except (MiGetExceptionInfo (GetExceptionInformation(),
                                      &ExceptionAddressConfirmed,
                                      &BadVa)) {

            //
            // If an exception occurs during the move operation or probe,
            // return the exception code as the status value.
            //

            KeUnstackDetachProcess (&ApcState);

            if (FreePool) {
                ExFreePool (PoolArea);
            }
            if (Probing == TRUE) {
                return GetExceptionCode();

            }

            //
            // If the failure occurred during the move operation, determine
            // which move failed, and calculate the number of bytes
            // actually moved.
            //

            *NumberOfBytesRead = BufferSize - LeftToMove;

            if (Moving == TRUE) {

                //
                // The failure occurred writing the data.
                //

                if (ExceptionAddressConfirmed == TRUE) {
                    *NumberOfBytesRead = (SIZE_T)((ULONG_PTR)(BadVa - (ULONG_PTR)FromAddress));
                }

            }

            return STATUS_PARTIAL_COPY;
        }

        KeUnstackDetachProcess (&ApcState);

        LeftToMove -= AmountToMove;
        InVa = (PVOID)((ULONG_PTR)InVa + AmountToMove);
        OutVa = (PVOID)((ULONG_PTR)OutVa + AmountToMove);
    }

    if (FreePool) {
        ExFreePool (PoolArea);
    }

    //
    // Set number of bytes moved.
    //

    *NumberOfBytesRead = BufferSize;
    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\sectsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   sectsup.c

Abstract:

    This module contains the routines which implement the
    section object.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/


#include "mi.h"

VOID
FASTCALL
MiRemoveBasedSection (
    IN PSECTION Section
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiSectionInitialization)
#pragma alloc_text(PAGE,MiRemoveBasedSection)
#pragma alloc_text(PAGE,MmGetFileNameForSection)
#pragma alloc_text(PAGE,MmGetFileNameForAddress)
#pragma alloc_text(PAGE,MiSectionDelete)
#pragma alloc_text(PAGE,MiInsertBasedSection)
#pragma alloc_text(PAGE,MiGetEventCounter)
#pragma alloc_text(PAGE,MiFreeEventCounter)
#pragma alloc_text(PAGE,MmGetFileObjectForSection)
#endif

ULONG   MmUnusedSegmentForceFree;

ULONG   MiSubsectionsProcessed;
ULONG   MiSubsectionActions;

SIZE_T MmSharedCommit = 0;
extern const ULONG MMCONTROL;

//
// Define segment dereference thread wait object types.
//

typedef enum _SEGMENT_DEREFERENCE_OBJECT {
    SegmentDereference,
    UsedSegmentCleanup,
    SegMaximumObject
    } BALANCE_OBJECT;

extern POBJECT_TYPE IoFileObjectType;

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("INITCONST")
#endif
const GENERIC_MAPPING MiSectionMapping = {
    STANDARD_RIGHTS_READ |
        SECTION_QUERY | SECTION_MAP_READ,
    STANDARD_RIGHTS_WRITE |
        SECTION_MAP_WRITE,
    STANDARD_RIGHTS_EXECUTE |
        SECTION_MAP_EXECUTE,
    SECTION_ALL_ACCESS
};
#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg()
#endif

VOID
MiRemoveUnusedSegments(
    VOID
    );


VOID
FASTCALL
MiInsertBasedSection (
    IN PSECTION Section
    )

/*++

Routine Description:

    This function inserts a virtual address descriptor into the tree and
    reorders the splay tree as appropriate.

Arguments:

    Section - Supplies a pointer to a based section.

Return Value:

    None.

Environment:

    Must be holding the section based mutex.

--*/

{
    PMMADDRESS_NODE *Root;

    ASSERT (Section->Address.EndingVpn >= Section->Address.StartingVpn);

    Root = &MmSectionBasedRoot;

    MiInsertNode (&Section->Address, Root);
    return;
}


VOID
FASTCALL
MiRemoveBasedSection (
    IN PSECTION Section
    )

/*++

Routine Description:

    This function removes a based section from the tree.

Arguments:

    Section - pointer to the based section object to remove.

Return Value:

    None.

Environment:

    Must be holding the section based mutex.

--*/

{
    PMMADDRESS_NODE *Root;

    Root = &MmSectionBasedRoot;

    MiRemoveNode (&Section->Address, Root);

    return;
}


VOID
MiSegmentDelete (
    PSEGMENT Segment
    )

/*++

Routine Description:

    This routine is called whenever the last reference to a segment object
    has been removed.  This routine releases the pool allocated for the
    prototype PTEs and performs consistency checks on those PTEs.

    For segments which map files, the file object is dereferenced.

    Note, that for a segment which maps a file, no PTEs may be valid
    or transition, while a segment which is backed by a paging file
    may have transition pages, but no valid pages (there can be no
    PTEs which refer to the segment).

Arguments:

    Segment - a pointer to the segment structure.

Return Value:

    None.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    volatile PCONTROL_AREA ControlArea;
    PEVENT_COUNTER Event;
    MMPTE PteContents;
    PSUBSECTION Subsection;
    PSUBSECTION NextSubsection;
    PMSUBSECTION MappedSubsection;
    PFN_NUMBER PageTableFrameIndex;
    SIZE_T NumberOfCommittedPages;

    ControlArea = Segment->ControlArea;

    ASSERT (ControlArea->u.Flags.BeingDeleted == 1);

    ASSERT (ControlArea->Segment->WritableUserReferences == 0);

    LOCK_PFN (OldIrql);
    if (ControlArea->DereferenceList.Flink != NULL) {

        //
        // Remove this from the list of unused segments.  The dereference
        // segment thread cannot be processing any subsections from this
        // control area right now because it bumps the NumberOfMappedViews
        // for the control area prior to releasing the PFN lock and it checks
        // for BeingDeleted.
        //

        ExAcquireSpinLockAtDpcLevel (&MmDereferenceSegmentHeader.Lock);
        RemoveEntryList (&ControlArea->DereferenceList);

        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

        ExReleaseSpinLockFromDpcLevel (&MmDereferenceSegmentHeader.Lock);
    }
    UNLOCK_PFN (OldIrql);

    if ((ControlArea->u.Flags.Image) || (ControlArea->u.Flags.File)) {

        //
        // Unload kernel debugger symbols if any were loaded.
        //

        if (ControlArea->u.Flags.DebugSymbolsLoaded != 0) {

            //
            //  TEMP TEMP TEMP rip out when debugger converted
            //

            ANSI_STRING AnsiName;
            NTSTATUS Status;

            Status = RtlUnicodeStringToAnsiString( &AnsiName,
                                                   (PUNICODE_STRING)&Segment->ControlArea->FilePointer->FileName,
                                                   TRUE );

            if (NT_SUCCESS( Status)) {
                DbgUnLoadImageSymbols( &AnsiName,
                                       Segment->BasedAddress,
                                       (ULONG_PTR)PsGetCurrentProcess());
                RtlFreeAnsiString( &AnsiName );
            }
            LOCK_PFN (OldIrql);
            ControlArea->u.Flags.DebugSymbolsLoaded = 0;
        }
        else {
            LOCK_PFN (OldIrql);
        }

        //
        // Signal any threads waiting on the deletion event.
        //

        Event = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;

        UNLOCK_PFN (OldIrql);

        if (Event != NULL) {
            KeSetEvent (&Event->Event, 0, FALSE);
        }

        //
        // Clear the segment context and dereference the file object
        // for this Segment.
        //
        // If the segment was deleted due to a name collision at insertion
        // we don't want to dereference the file pointer.
        //

        if (ControlArea->u.Flags.BeingCreated == FALSE) {

#if DBG
            if (ControlArea->u.Flags.Image == 1) {
                ASSERT (ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject != (PVOID)ControlArea);
            }
            else {
                ASSERT (ControlArea->FilePointer->SectionObjectPointer->DataSectionObject != (PVOID)ControlArea);
            }
#endif

            PERFINFO_SEGMENT_DELETE(ControlArea->FilePointer);
            ObDereferenceObject (ControlArea->FilePointer);
        }

        //
        // If there have been committed pages in this segment, adjust
        // the total commit count.
        //

        if (ControlArea->u.Flags.Image == 0) {

            //
            // This is a mapped data file.  None of the prototype
            // PTEs may be referencing a physical page (valid or transition).
            //

            if (ControlArea->u.Flags.Rom == 0) {
                Subsection = (PSUBSECTION)(ControlArea + 1);
            }
            else {
                Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
            }

#if DBG
            if (Subsection->SubsectionBase != NULL) {
                PointerPte = Subsection->SubsectionBase;
                LastPte = PointerPte + Segment->NonExtendedPtes;

                while (PointerPte < LastPte) {

                    //
                    // Prototype PTEs for segments backed by paging file are
                    // either in demand zero, page file format, or transition.
                    //

                    ASSERT (PointerPte->u.Hard.Valid == 0);
                    ASSERT ((PointerPte->u.Soft.Prototype == 1) ||
                            (PointerPte->u.Long == 0));
                    PointerPte += 1;
                }
            }
#endif

            //
            // Deallocate the control area and subsections.
            //

            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

            if (ControlArea->FilePointer != NULL) {

                MappedSubsection = (PMSUBSECTION) Subsection;

                LOCK_PFN (OldIrql);

                while (MappedSubsection != NULL) {

                    if (MappedSubsection->DereferenceList.Flink != NULL) {

                        //
                        // Remove this from the list of unused subsections.
                        //

                        RemoveEntryList (&MappedSubsection->DereferenceList);

                        MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);
                    }
                    MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;
                }
                UNLOCK_PFN (OldIrql);

                if (Subsection->SubsectionBase != NULL) {
                    ExFreePool (Subsection->SubsectionBase);
                }
            }

            Subsection = Subsection->NextSubsection;

            while (Subsection != NULL) {
                if (Subsection->SubsectionBase != NULL) {
                    ExFreePool (Subsection->SubsectionBase);
                }
                NextSubsection = Subsection->NextSubsection;
                ExFreePool (Subsection);
                Subsection = NextSubsection;
            }

            NumberOfCommittedPages = Segment->NumberOfCommittedPages;

            if (NumberOfCommittedPages != 0) {
                MiReturnCommitment (NumberOfCommittedPages);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE1,
                                 NumberOfCommittedPages);

                InterlockedExchangeAddSizeT (&MmSharedCommit, 0-NumberOfCommittedPages);
            }

            ExFreePool (ControlArea);
            ExFreePool (Segment);

            //
            // The file mapped Segment object is now deleted.
            //

            return;
        }
    }

    //
    // This is a page file backed or image Segment.  The Segment is being
    // deleted, remove all references to the paging file and physical memory.
    //
    // The PFN lock is required for deallocating pages from a paging
    // file and for deleting transition PTEs.
    //

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PointerPte = Subsection->SubsectionBase;
    LastPte = PointerPte + Segment->NonExtendedPtes;

    LOCK_PFN (OldIrql);

    MiMakeSystemAddressValidPfn (PointerPte);

    while (PointerPte < LastPte) {

        if (MiIsPteOnPdeBoundary(PointerPte)) {

            //
            // We are on a page boundary, make sure this PTE is resident.
            //

            if (MmIsAddressValid (PointerPte) == FALSE) {

                MiMakeSystemAddressValidPfn (PointerPte);
            }
        }

        PteContents = *PointerPte;

        //
        // Prototype PTEs for Segments backed by paging file
        // are either in demand zero, page file format, or transition.
        //

        ASSERT (PteContents.u.Hard.Valid == 0);

        if (PteContents.u.Soft.Prototype == 0) {

            if (PteContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, put the page on the free list.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // Check the reference count for the page, if the reference
                // count is zero and the page is not on the freelist,
                // move the page to the free list, if the reference
                // count is not zero, ignore this page.
                // When the reference count goes to zero, it will be placed
                // on the free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                }

            }
            else {

                //
                // This is not a prototype PTE, if any paging file
                // space has been allocated, release it.
                //

                if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {
                    MiReleasePageFileSpace (PteContents);
                }
            }
        }
#if DBG
        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
#endif
        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);

    //
    // If there have been committed pages in this segment, adjust
    // the total commit count.
    //

    NumberOfCommittedPages = Segment->NumberOfCommittedPages;

    if (NumberOfCommittedPages != 0) {
        MiReturnCommitment (NumberOfCommittedPages);

        if (ControlArea->u.Flags.Image) {
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE2,
                             NumberOfCommittedPages);
        }
        else {
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SEGMENT_DELETE3,
                             NumberOfCommittedPages);
        }

        InterlockedExchangeAddSizeT (&MmSharedCommit, 0-NumberOfCommittedPages);
    }

    ExFreePool (ControlArea);
    ExFreePool (Segment);

    return;
}

ULONG
MmDoesFileHaveUserWritableReferences (
    IN PSECTION_OBJECT_POINTERS SectionPointer
    )

/*++

Routine Description:

    This routine is called by the transaction filesystem to determine if
    the given transaction is referencing a file which has user writable sections
    or user writable views into it.  If so, the transaction must be aborted
    as it cannot be guaranteed atomicity.

    The transaction filesystem is responsible for checking and intercepting
    file object creates that specify write access prior to using this
    interface.  Specifically, prior to starting a transaction, the transaction
    filesystem must ensure that there are no writable file objects that
    currently exist for the given file in the transaction.  While the
    transaction is ongoing, requests to create file objects with write access
    for the transaction files must be refused.

    This Mm routine exists to catch the case where the user has closed the
    file handles and the section handles, but still has open writable views.

    For this reason, no locks are needed to read the value below.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

Return Value:

    Number of user writable references.

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{
    KIRQL OldIrql;
    ULONG WritableUserReferences;
    PCONTROL_AREA ControlArea;

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);

    if (ControlArea == NULL) {
        UNLOCK_PFN (OldIrql);
        return 0;
    }

    //
    // Up the map view count so the control area cannot be deleted
    // out from under the call.
    //

    ControlArea->NumberOfMappedViews += 1;
    MiMakeSystemAddressValidPfn (&ControlArea->Segment->WritableUserReferences);
    WritableUserReferences = ControlArea->Segment->WritableUserReferences;
    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    //
    // This routine will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);

    return WritableUserReferences;
}

VOID
MiDereferenceControlAreaBySection (
    IN PCONTROL_AREA ControlArea,
    IN ULONG UserRef
    )

/*++

Routine Description:

    This is a nonpaged helper routine to dereference the specified control area.

Arguments:

    ControlArea - Supplies a pointer to the control area.

    UserRef - Supplies the number of user dereferences to apply.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ControlArea->NumberOfSectionReferences -= 1;
    ControlArea->NumberOfUserReferences -= UserRef;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}

VOID
MiSectionDelete (
    IN PVOID Object
    )

/*++

Routine Description:


    This routine is called by the object management procedures whenever
    the last reference to a section object has been removed.  This routine
    dereferences the associated segment object and checks to see if
    the segment object should be deleted by queueing the segment to the
    segment deletion thread.

Arguments:

    Object - a pointer to the body of the section object.

Return Value:

    None.

--*/

{
    PSECTION Section;
    volatile PCONTROL_AREA ControlArea;
    ULONG UserRef;

    Section = (PSECTION)Object;

    if (Section->Segment == (PSEGMENT)NULL) {

        //
        // The section was never initialized, no need to remove
        // any structures.
        //
        return;
    }

    UserRef = Section->u.Flags.UserReference;
    ControlArea = Section->Segment->ControlArea;

    if (Section->Address.StartingVpn != 0) {

        //
        // This section is based, remove the base address from the
        // tree.
        //

        //
        // Get the allocation base mutex.
        //

        ExAcquireFastMutex (&MmSectionBasedMutex);

        MiRemoveBasedSection (Section);

        ExReleaseFastMutex (&MmSectionBasedMutex);

    }

    //
    // Adjust the count of writable user sections for transaction support.
    //

    if ((Section->u.Flags.UserWritable == 1) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL)) {

        ASSERT (Section->InitialPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE));

        InterlockedDecrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
    }

    //
    // Decrement the number of section references to the segment for this
    // section.  This requires APCs to be blocked and the PFN lock to
    // synchronize upon.
    //

    MiDereferenceControlAreaBySection (ControlArea, UserRef);

    return;
}


VOID
MiDereferenceSegmentThread (
    IN PVOID StartContext
    )

/*++

Routine Description:

    This routine is the thread for dereferencing segments which have
    no references from any sections or mapped views AND there are
    no prototype PTEs within the segment which are in the transition
    state (i.e., no PFN database references to the segment).

    It also does double duty and is used for expansion of paging files.

Arguments:

    StartContext - Not used.

Return Value:

    None.

--*/

{
    PCONTROL_AREA ControlArea;
    PETHREAD CurrentThread;
    PMMPAGE_FILE_EXPANSION PageExpand;
    PLIST_ENTRY NextEntry;
    KIRQL OldIrql;
    static KWAIT_BLOCK WaitBlockArray[SegMaximumObject];
    PVOID WaitObjects[SegMaximumObject];
    NTSTATUS Status;

    UNREFERENCED_PARAMETER (StartContext);

    //
    // Make this a real time thread.
    //

    CurrentThread = PsGetCurrentThread();
    KeSetPriorityThread (&CurrentThread->Tcb, LOW_REALTIME_PRIORITY + 2);

    CurrentThread->MemoryMaker = 1;

    WaitObjects[SegmentDereference] = (PVOID)&MmDereferenceSegmentHeader.Semaphore;
    WaitObjects[UsedSegmentCleanup] = (PVOID)&MmUnusedSegmentCleanup;

    for (;;) {

        Status = KeWaitForMultipleObjects(SegMaximumObject,
                                          &WaitObjects[0],
                                          WaitAny,
                                          WrVirtualMemory,
                                          UserMode,
                                          FALSE,
                                          NULL,
                                          &WaitBlockArray[0]);

        //
        // Switch on the wait status.
        //

        switch (Status) {

        case SegmentDereference:

            //
            // An entry is available to dereference, acquire the spinlock
            // and remove the entry.
            //

            ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

            if (IsListEmpty (&MmDereferenceSegmentHeader.ListHead)) {

                //
                // There is nothing in the list, rewait.
                //

                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);
                break;
            }

            NextEntry = RemoveHeadList (&MmDereferenceSegmentHeader.ListHead);

            ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

            ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

            ControlArea = CONTAINING_RECORD (NextEntry,
                                             CONTROL_AREA,
                                             DereferenceList);

            if (ControlArea->Segment != NULL) {

                //
                // This is a control area, delete it after indicating
                // this entry is not on any list.
                //

                ControlArea->DereferenceList.Flink = NULL;

                ASSERT (ControlArea->u.Flags.FilePointerNull == 1);
                MiSegmentDelete (ControlArea->Segment);

            }
            else {

                //
                // This is a request to expand or reduce the paging files.
                //

                PageExpand = (PMMPAGE_FILE_EXPANSION)ControlArea;

                if (PageExpand->RequestedExpansionSize == MI_CONTRACT_PAGEFILES) {

                    //
                    // Attempt to reduce the size of the paging files.
                    //

                    ExFreePool (PageExpand);

                    MiAttemptPageFileReduction ();
                }
                else {

                    //
                    // Attempt to expand the size of the paging files.
                    //

                    MiExtendPagingFiles (PageExpand);
                    KeSetEvent (&PageExpand->Event, 0, FALSE);
                    MiRemoveUnusedSegments();
                }
            }
            break;

        case UsedSegmentCleanup:

            MiRemoveUnusedSegments();

            KeClearEvent (&MmUnusedSegmentCleanup);

            break;

        default:

            KdPrint(("MMSegmentderef: Illegal wait status, %lx =\n", Status));
            break;
        } // end switch

    } //end for

    return;
}


ULONG
MiSectionInitialization (
    )

/*++

Routine Description:

    This function creates the section object type descriptor at system
    initialization and stores the address of the object type descriptor
    in global storage.

Arguments:

    None.

Return Value:

    TRUE - Initialization was successful.

    FALSE - Initialization Failed.



--*/

{
    OBJECT_TYPE_INITIALIZER ObjectTypeInitializer;
    UNICODE_STRING TypeName;
    HANDLE ThreadHandle;
    OBJECT_ATTRIBUTES ObjectAttributes;
    UNICODE_STRING SectionName;
    PSECTION Section;
    HANDLE Handle;
    PSEGMENT Segment;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;

    MmSectionBasedRoot = (PMMADDRESS_NODE)NULL;

    //
    // Initialize the common fields of the Object Type Initializer record
    //

    RtlZeroMemory( &ObjectTypeInitializer, sizeof( ObjectTypeInitializer ) );
    ObjectTypeInitializer.Length = sizeof( ObjectTypeInitializer );
    ObjectTypeInitializer.InvalidAttributes = OBJ_OPENLINK;
    ObjectTypeInitializer.GenericMapping = MiSectionMapping;
    ObjectTypeInitializer.PoolType = PagedPool;
    ObjectTypeInitializer.DefaultPagedPoolCharge = sizeof(SECTION);

    //
    // Initialize string descriptor.
    //

    RtlInitUnicodeString (&TypeName, (const PUSHORT)L"Section");

    //
    // Create the section object type descriptor
    //

    ObjectTypeInitializer.ValidAccessMask = SECTION_ALL_ACCESS;
    ObjectTypeInitializer.DeleteProcedure = MiSectionDelete;
    ObjectTypeInitializer.GenericMapping = MiSectionMapping;
    ObjectTypeInitializer.UseDefaultObject = TRUE;
    if ( !NT_SUCCESS(ObCreateObjectType(&TypeName,
                                     &ObjectTypeInitializer,
                                     (PSECURITY_DESCRIPTOR) NULL,
                                     &MmSectionObjectType
                                     )) ) {
        return FALSE;
    }

    //
    // Create the Segment dereferencing thread.
    //

    InitializeObjectAttributes( &ObjectAttributes,
                                NULL,
                                0,
                                NULL,
                                NULL );

    if ( !NT_SUCCESS(PsCreateSystemThread(
                    &ThreadHandle,
                    THREAD_ALL_ACCESS,
                    &ObjectAttributes,
                    0,
                    NULL,
                    MiDereferenceSegmentThread,
                    NULL
                    )) ) {
        return FALSE;
    }
    ZwClose (ThreadHandle);

    //
    // Create the permanent section which maps physical memory.
    //

    Segment = (PSEGMENT)ExAllocatePoolWithTag (PagedPool,
                                               sizeof(SEGMENT),
                                               'gSmM');
    if (Segment == NULL) {
        return FALSE;
    }

    ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                         (ULONG)sizeof(CONTROL_AREA),
                                         MMCONTROL);
    if (ControlArea == NULL) {
        ExFreePool (Segment);
        return FALSE;
    }

    RtlZeroMemory (Segment, sizeof(SEGMENT));
    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA));

    ControlArea->Segment = Segment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->u.Flags.PhysicalMemory = 1;

    Segment->ControlArea = ControlArea;
    Segment->SegmentPteTemplate = ZeroPte;

    //
    // Now that the segment object is created, create a section object
    // which refers to the segment object.
    //

    RtlInitUnicodeString (&SectionName, (const PUSHORT)L"\\Device\\PhysicalMemory");

    InitializeObjectAttributes( &ObjectAttributes,
                                &SectionName,
                                OBJ_PERMANENT,
                                NULL,
                                NULL
                              );

    Status = ObCreateObject (KernelMode,
                             MmSectionObjectType,
                             &ObjectAttributes,
                             KernelMode,
                             NULL,
                             sizeof(SECTION),
                             sizeof(SECTION),
                             0,
                             (PVOID *)&Section);
    if (!NT_SUCCESS(Status)) {
        ExFreePool (ControlArea);
        ExFreePool (Segment);
        return FALSE;
    }

    Section->Segment = Segment;
    Section->SizeOfSection.QuadPart = ((LONGLONG)1 << PHYSICAL_ADDRESS_BITS) - 1;
    Section->u.LongFlags = 0;
    Section->InitialPageProtection = PAGE_READWRITE;

    Status = ObInsertObject ((PVOID)Section,
                                    NULL,
                                    SECTION_MAP_READ,
                                    0,
                                    (PVOID *)NULL,
                                    &Handle);

    if (!NT_SUCCESS( Status )) {
        return FALSE;
    }

    if ( !NT_SUCCESS (NtClose ( Handle))) {
        return FALSE;
    }

    return TRUE;
}

BOOLEAN
MmForceSectionClosed (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer,
    IN BOOLEAN DelayClose
    )

/*++

Routine Description:

    This function examines the Section object pointers.  If they are NULL,
    no further action is taken and the value TRUE is returned.

    If the Section object pointer is not NULL, the section reference count
    and the map view count are checked. If both counts are zero, the
    segment associated with the file is deleted and the file closed.
    If one of the counts is non-zero, no action is taken and the
    value FALSE is returned.

Arguments:

    SectionObjectPointer - Supplies a pointer to a section object.

    DelayClose - Supplies the value TRUE if the close operation should
                 occur as soon as possible in the event this section
                 cannot be closed now due to outstanding references.

Return Value:

    TRUE - The segment was deleted and the file closed or no segment was
           located.

    FALSE - The segment was not deleted and no action was performed OR
            an I/O error occurred trying to write the pages.

--*/

{
    PCONTROL_AREA ControlArea;
    KIRQL OldIrql;
    LOGICAL state;

    //
    // Check the status of the control area, if the control area is in use
    // or the control area is being deleted, this operation cannot continue.
    //

    state = MiCheckControlAreaStatus (CheckBothSection,
                                      SectionObjectPointer,
                                      DelayClose,
                                      &ControlArea,
                                      &OldIrql);

    if (ControlArea == NULL) {
        return (BOOLEAN) state;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    //
    // Repeat until there are no more control areas - multiple control areas
    // for the same image section occur to support user global DLLs - these DLLs
    // require data that is shared within a session but not across sessions.
    // Note this can only happen for Hydra.
    //

    do {

        //
        // Set the being deleted flag and up the number of mapped views
        // for the segment.  Upping the number of mapped views prevents
        // the segment from being deleted and passed to the deletion thread
        // while we are forcing a delete.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ASSERT (ControlArea->NumberOfMappedViews == 0);
        ControlArea->NumberOfMappedViews = 1;

        //
        // This is a page file backed or image Segment.  The Segment is being
        // deleted, remove all references to the paging file and physical memory.
        //

        UNLOCK_PFN (OldIrql);

        //
        // Delete the section by flushing all modified pages back to the section
        // if it is a file and freeing up the pages such that the
        // PfnReferenceCount goes to zero.
        //

        MiCleanSection (ControlArea, TRUE);

        //
        // Get the next Hydra control area.
        //

        state = MiCheckControlAreaStatus (CheckBothSection,
                                          SectionObjectPointer,
                                          DelayClose,
                                          &ControlArea,
                                          &OldIrql);

    } while (ControlArea);

    return (BOOLEAN) state;
}


VOID
MiCleanSection (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL DirtyDataPagesOk
    )

/*++

Routine Description:

    This function examines each prototype PTE in the section and
    takes the appropriate action to "delete" the prototype PTE.

    If the PTE is dirty and is backed by a file (not a paging file),
    the corresponding page is written to the file.

    At the completion of this service, the section which was
    operated upon is no longer usable.

    NOTE - ALL I/O ERRORS ARE IGNORED.  IF ANY WRITES FAIL, THE
           DIRTY PAGES ARE MARKED CLEAN AND THE SECTION IS DELETED.

Arguments:

    ControlArea - Supplies a pointer to the control area for the section.

    DirtyDataPagesOk - Supplies TRUE if dirty data pages are ok.  If FALSE
                       is specified then no dirty data pages are expected (as
                       this is a dereference operation) so any encountered
                       must be due to pool corruption so bugcheck.

                       Note that dirty image pages are always discarded.
                       This should only happen for images that were either
                       read in from floppies or images with shared global
                       subsections.

Return Value:

    None.

--*/

{
    LOGICAL DroppedPfnLock;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastWritten;
    PMMPTE FirstWritten;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPFN Pfn3;
    PMMPTE WrittenPte;
    MMPTE WrittenContents;
    KIRQL OldIrql;
    PMDL Mdl;
    PSUBSECTION Subsection;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    LARGE_INTEGER StartingOffset;
    LARGE_INTEGER TempOffset;
    NTSTATUS Status;
    IO_STATUS_BLOCK IoStatus;
    ULONG WriteNow;
    ULONG ImageSection;
    ULONG DelayCount;
    ULONG First;
    KEVENT IoEvent;
    PFN_NUMBER PageTableFrameIndex;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_WRITE_CLUSTER];
    ULONG ReflushCount;
    ULONG MaxClusterSize;

    WriteNow = FALSE;
    ImageSection = FALSE;
    DelayCount = 0;
    MaxClusterSize = MmModifiedWriteClusterSize;
    FirstWritten = NULL;

    ASSERT (ControlArea->FilePointer);

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (ControlArea->u.Flags.Image) {
        ImageSection = TRUE;
        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + ControlArea->Segment->NonExtendedPtes;
    }
    else {

        //
        // Initializing these are not needed for correctness as they are
        // overwritten below, but without it the compiler cannot compile
        // this code W4 to check for use of uninitialized variables.
        //

        PointerPte = NULL;
        LastPte = NULL;
    }

    Mdl = (PMDL)&MdlHack;

    KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

    LastWritten = NULL;
    ASSERT (MmModifiedWriteClusterSize == MM_MAXIMUM_WRITE_CLUSTER);
    LastPage = NULL;

    //
    // Initializing StartingOffset is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    StartingOffset.QuadPart = 0;

    //
    // The PFN lock is required for deallocating pages from a paging
    // file and for deleting transition PTEs.
    //

    LOCK_PFN (OldIrql);

    //
    // Stop the modified page writer from writing pages to this
    // file, and if any paging I/O is in progress, wait for it
    // to complete.
    //

    ControlArea->u.Flags.NoModifiedWriting = 1;

    while (ControlArea->ModifiedWriteCount != 0) {

        //
        // There is modified page writing in progess.  Set the
        // flag in the control area indicating the modified page
        // writer should signal when a write to this control area
        // is complete.  Release the PFN LOCK and wait in an
        // atomic operation.  Once the wait is satisfied, recheck
        // to make sure it was this file's I/O that was written.
        //

        ControlArea->u.Flags.SetMappedFileIoComplete = 1;

        //
        // Keep APCs blocked so no special APCs can be delivered in KeWait
        // which would cause the dispatcher lock to be released opening a
        // window where this thread could miss a pulse.
        //

        UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

        KeWaitForSingleObject (&MmMappedFileIoComplete,
                               WrPageOut,
                               KernelMode,
                               FALSE,
                               NULL);
        KeLowerIrql (OldIrql);

        LOCK_PFN (OldIrql);
    }

    if (ImageSection == FALSE) {
        while (Subsection->SubsectionBase == NULL) {
            Subsection = Subsection->NextSubsection;
            if (Subsection == NULL) {
                goto alldone;
            }
        }

        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + Subsection->PtesInSubsection;
    }

    for (;;) {

restartchunk:

        First = TRUE;

        while (PointerPte < LastPte) {

            if ((MiIsPteOnPdeBoundary(PointerPte)) || (First)) {

                First = FALSE;

                if ((ImageSection) ||
                    (MiCheckProtoPtePageState(PointerPte, FALSE, &DroppedPfnLock))) {
                    MiMakeSystemAddressValidPfn (PointerPte);
                }
                else {

                    //
                    // Paged pool page is not resident, hence no transition or
                    // valid prototype PTEs can be present in it.  Skip it.
                    //

                    PointerPte = (PMMPTE)((((ULONG_PTR)PointerPte | PAGE_SIZE - 1)) + 1);
                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                    goto WriteItOut;
                }
            }

            PteContents = *PointerPte;

            //
            // Prototype PTEs for Segments backed by paging file
            // are either in demand zero, page file format, or transition.
            //

            if (PteContents.u.Hard.Valid == 1) {
                KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                              0x0,
                              (ULONG_PTR)ControlArea,
                              (ULONG_PTR)PointerPte,
                              (ULONG_PTR)PteContents.u.Long);
            }

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // This is a normal prototype PTE in mapped file format.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }
            else if (PteContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, there are 3 possible cases:
                //  1. The page is part of an image which is sharable and
                //     refers to the paging file - dereference page file
                //     space and free the physical page.
                //  2. The page refers to the segment but is not modified -
                //     free the physical page.
                //  3. The page refers to the segment and is modified -
                //     write the page to the file and free the physical page.
                //

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                if (Pfn1->u3.e2.ReferenceCount != 0) {
                    if (DelayCount < 20) {

                        //
                        // There must be an I/O in progress on this
                        // page.  Wait for the I/O operation to complete.
                        //

                        UNLOCK_PFN (OldIrql);

                        //
                        // Drain the deferred lists as these pages may be
                        // sitting in there right now.
                        //

                        MiDeferredUnlockPages (0);

                        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

                        DelayCount += 1;

                        //
                        // Redo the loop, if the delay count is greater than
                        // 20, assume that this thread is deadlocked and
                        // don't purge this page.  The file system can deal
                        // with the write operation in progress.
                        //

                        LOCK_PFN (OldIrql);
                        MiMakeSystemAddressValidPfn (PointerPte);
                        continue;
                    }
#if DBG
                    //
                    // The I/O still has not completed, just ignore
                    // the fact that the I/O is in progress and
                    // delete the page.
                    //

                    KdPrint(("MM:CLEAN - page number %lx has i/o outstanding\n",
                          PteContents.u.Trans.PageFrameNumber));
#endif
                }

                if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

                    //
                    // Paging file reference (case 1).
                    //

                    MI_SET_PFN_DELETED (Pfn1);

                    if (!ImageSection) {

                        //
                        // This is not an image section, it must be a
                        // page file backed section, therefore decrement
                        // the PFN reference count for the control area.
                        //

                        ControlArea->NumberOfPfnReferences -= 1;
                        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
                    }
#if DBG
                    else {
                        //
                        // This should only happen for images with shared
                        // global subsections.
                        //
                    }
#endif

                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    //
                    // Check the reference count for the page, if the                               // reference count is zero and the page is not on the
                    // freelist, move the page to the free list, if the
                    // reference count is not zero, ignore this page.  When
                    // the reference count goes to zero, it will be placed
                    // on the free list.
                    //

                    if ((Pfn1->u3.e2.ReferenceCount == 0) &&
                         (Pfn1->u3.e1.PageLocation != FreePageList)) {

                        MiUnlinkPageFromList (Pfn1);
                        MiReleasePageFileSpace (Pfn1->OriginalPte);
                        MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));

                    }
                    PointerPte->u.Long = 0;

                    //
                    // If a cluster of pages to write has been completed,
                    // set the WriteNow flag.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }

                }
                else {

                    if ((Pfn1->u3.e1.Modified == 0) || (ImageSection)) {

                        //
                        // Non modified or image file page (case 2).
                        //

                        MI_SET_PFN_DELETED (Pfn1);
                        ControlArea->NumberOfPfnReferences -= 1;
                        ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // Check the reference count for the page, if the
                        // reference count is zero and the page is not on
                        // the freelist, move the page to the free list,
                        // if the reference count is not zero, ignore this
                        // page. When the reference count goes to zero, it
                        // will be placed on the free list.
                        //

                        if ((Pfn1->u3.e2.ReferenceCount == 0) &&
                             (Pfn1->u3.e1.PageLocation != FreePageList)) {

                            MiUnlinkPageFromList (Pfn1);
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                        }

                        PointerPte->u.Long = 0;

                        //
                        // If a cluster of pages to write has been
                        // completed, set the WriteNow flag.
                        //

                        if (LastWritten != NULL) {
                            WriteNow = TRUE;
                        }

                    }
                    else {

                        //
                        // Modified page backed by the file (case 3).
                        // Check to see if this is the first page of a
                        // cluster.
                        //

                        if (LastWritten == NULL) {
                            LastPage = (PPFN_NUMBER)(Mdl + 1);
                            ASSERT (MiGetSubsectionAddress(&Pfn1->OriginalPte) ==
                                                                Subsection);

                            //
                            // Calculate the offset to read into the file.
                            //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
                            //

                            ASSERT (Subsection->ControlArea->u.Flags.Image == 0);
                            StartingOffset.QuadPart = MiStartingOffset(
                                                         Subsection,
                                                         Pfn1->PteAddress);

                            MI_INITIALIZE_ZERO_MDL (Mdl);
                            Mdl->MdlFlags |= MDL_PAGES_LOCKED;

                            Mdl->StartVa = NULL;
                            Mdl->Size = (CSHORT)(sizeof(MDL) +
                                       (sizeof(PFN_NUMBER) * MaxClusterSize));
                            FirstWritten = PointerPte;
                        }

                        LastWritten = PointerPte;
                        Mdl->ByteCount += PAGE_SIZE;

                        //
                        // If the cluster is now full,
                        // set the write now flag.
                        //

                        if (Mdl->ByteCount == (PAGE_SIZE * MaxClusterSize)) {
                            WriteNow = TRUE;
                        }

                        MiUnlinkPageFromList (Pfn1);

                        MI_SET_MODIFIED (Pfn1, 0, 0x27);

                        //
                        // Up the reference count for the physical page as
                        // there is I/O in progress.
                        //

                        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE(Pfn1, 22);
                        Pfn1->u3.e2.ReferenceCount += 1;

                        //
                        // Clear the modified bit for the page and set the
                        // write in progress bit.
                        //

                        *LastPage = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

                        LastPage += 1;
                    }
                }
            }
            else {

                if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {
                    MiReleasePageFileSpace (PteContents);
                }
                PointerPte->u.Long = 0;

                //
                // If a cluster of pages to write has been completed,
                // set the WriteNow flag.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }

            //
            // Write the current cluster if it is complete,
            // full, or the loop is now complete.
            //

            PointerPte += 1;
WriteItOut:
            DelayCount = 0;

            if ((WriteNow) ||
                ((PointerPte == LastPte) && (LastWritten != NULL))) {

                //
                // Issue the write request.
                //

                UNLOCK_PFN (OldIrql);

                if (DirtyDataPagesOk == FALSE) {
                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x1,
                                  (ULONG_PTR)ControlArea,
                                  (ULONG_PTR)Mdl,
                                  ControlArea->u.LongFlags);
                }

                WriteNow = FALSE;

                //
                // Make sure the write does not go past the
                // end of file. (segment size).
                //

                ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

                TempOffset = MiEndingOffset(Subsection);

                if (((UINT64)StartingOffset.QuadPart + Mdl->ByteCount) >
                             (UINT64)TempOffset.QuadPart) {

                    ASSERT ((ULONG)(TempOffset.QuadPart -
                                        StartingOffset.QuadPart) >
                             (Mdl->ByteCount - PAGE_SIZE));

                    Mdl->ByteCount = (ULONG)(TempOffset.QuadPart -
                                            StartingOffset.QuadPart);
                }

                ReflushCount = 0;

                while (TRUE) {

                    KeClearEvent (&IoEvent);

                    Status = IoSynchronousPageWrite (ControlArea->FilePointer,
                                                     Mdl,
                                                     &StartingOffset,
                                                     &IoEvent,
                                                     &IoStatus);

                    if (NT_SUCCESS(Status)) {

                        KeWaitForSingleObject (&IoEvent,
                                               WrPageOut,
                                               KernelMode,
                                               FALSE,
                                               NULL);
                    }
                    else {
                        IoStatus.Status = Status;
                    }

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (MmIsRetryIoStatus(IoStatus.Status)) {

                        ReflushCount -= 1;
                        if (ReflushCount & MiIoRetryMask) {
                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
                            continue;
                        }
                    }
                    break;
                }

                Page = (PPFN_NUMBER)(Mdl + 1);

                LOCK_PFN (OldIrql);

                if (MiIsPteOnPdeBoundary(PointerPte) == 0) {

                    //
                    // The next PTE is not in a different page, make
                    // sure this page did not leave memory when the
                    // I/O was in progress.
                    //

                    MiMakeSystemAddressValidPfn (PointerPte);
                }

                if (!NT_SUCCESS(IoStatus.Status)) {

                    if ((MmIsRetryIoStatus(IoStatus.Status)) &&
                        (MaxClusterSize != 1) &&
                        (Mdl->ByteCount > PAGE_SIZE)) {

                        //
                        // Retried I/O of a cluster have failed, reissue
                        // the cluster one page at a time as the
                        // storage stack should always be able to
                        // make forward progress this way.
                        //

                        ASSERT (FirstWritten != NULL);
                        ASSERT (LastWritten != NULL);
                        ASSERT (FirstWritten != LastWritten);

                        IoStatus.Information = 0;

                        while (Page < LastPage) {

                            Pfn2 = MI_PFN_ELEMENT (*Page);

                            //
                            // Mark the page dirty again so it can be rewritten.
                            //

                            MI_SET_MODIFIED (Pfn2, 1, 0xE);

                            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn2, 21);
                            Page += 1;
                        }

                        PointerPte = FirstWritten;
                        LastWritten = NULL;

                        MaxClusterSize = 1;
                        goto restartchunk;
                    }
                }

                //
                // I/O complete unlock pages.
                //
                // NOTE that the error status is ignored.
                //

                while (Page < LastPage) {

                    Pfn2 = MI_PFN_ELEMENT (*Page);

                    //
                    // Make sure the page is still transition.
                    //

                    WrittenPte = Pfn2->PteAddress;

                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn2, 23);

                    if (!MI_IS_PFN_DELETED (Pfn2)) {

                        //
                        // Make sure the prototype PTE is
                        // still in the working set.
                        //

                        MiMakeSystemAddressValidPfn (WrittenPte);

                        if (Pfn2->PteAddress != WrittenPte) {

                            //
                            // The PFN lock was released to make the
                            // page table page valid, and while it
                            // was released, the physical page
                            // was reused.  Go onto the next one.
                            //

                            Page += 1;
                            continue;
                        }

                        WrittenContents = *WrittenPte;

                        if ((WrittenContents.u.Soft.Prototype == 0) &&
                             (WrittenContents.u.Soft.Transition == 1)) {

                            MI_SET_PFN_DELETED (Pfn2);
                            ControlArea->NumberOfPfnReferences -= 1;
                            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                            PageTableFrameIndex = Pfn2->u4.PteFrame;
                            Pfn3 = MI_PFN_ELEMENT (PageTableFrameIndex);
                            MiDecrementShareCountInline (Pfn3, PageTableFrameIndex);

                            //
                            // Check the reference count for the page,
                            // if the reference count is zero and the
                            // page is not on the freelist, move the page
                            // to the free list, if the reference
                            // count is not zero, ignore this page.
                            // When the reference count goes to zero,
                            // it will be placed on the free list.
                            //

                            if ((Pfn2->u3.e2.ReferenceCount == 0) &&
                               (Pfn2->u3.e1.PageLocation != FreePageList)) {

                                MiUnlinkPageFromList (Pfn2);
                                MiReleasePageFileSpace (Pfn2->OriginalPte);
                                MiInsertPageInFreeList (*Page);
                            }
                        }
                        WrittenPte->u.Long = 0;
                    }
                    Page += 1;
                }

                //
                // Indicate that there is no current cluster being built.
                //

                LastWritten = NULL;
            }

        } // end while

        //
        // Get the next subsection if any.
        //

        if (Subsection->NextSubsection == NULL) {
            break;
        }

        Subsection = Subsection->NextSubsection;

        if (ImageSection == FALSE) {
            while (Subsection->SubsectionBase == NULL) {
                Subsection = Subsection->NextSubsection;
                if (Subsection == NULL) {
                    goto alldone;
                }
            }
        }

        PointerPte = Subsection->SubsectionBase;
        LastPte = PointerPte + Subsection->PtesInSubsection;

    } // end for

alldone:

    ControlArea->NumberOfMappedViews = 0;

    ASSERT (ControlArea->NumberOfPfnReferences == 0);

    if (ControlArea->u.Flags.FilePointerNull == 0) {
        ControlArea->u.Flags.FilePointerNull = 1;

        if (ControlArea->u.Flags.Image) {

            MiRemoveImageSectionObject (ControlArea->FilePointer, ControlArea);
        }
        else {

            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
            ControlArea->FilePointer->SectionObjectPointer->DataSectionObject = NULL;

        }
    }
    UNLOCK_PFN (OldIrql);

    //
    // Delete the segment structure.
    //

    MiSegmentDelete (ControlArea->Segment);

    return;
}

NTSTATUS
MmGetFileNameForSection (
    IN PSECTION SectionObject,
    OUT PSTRING FileName
    )

/*++

Routine Description:

    This function returns the file name for the corresponding section.

Arguments:

    SectionObject - Supplies the section to get the name of.

    FileName - Returns the name of the corresponding section.

Return Value:

    TBS

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/

{

    POBJECT_NAME_INFORMATION FileNameInfo;
    ULONG whocares;
    NTSTATUS Status;

#define xMAX_NAME 1024

    if (SectionObject->u.Flags.Image == 0) {
        return STATUS_SECTION_NOT_IMAGE;
    }

    FileNameInfo = ExAllocatePoolWithTag (PagedPool, xMAX_NAME, '  mM');

    if ( !FileNameInfo ) {
        return STATUS_NO_MEMORY;
    }

    Status = ObQueryNameString(
                SectionObject->Segment->ControlArea->FilePointer,
                FileNameInfo,
                xMAX_NAME,
                &whocares
                );

    if ( !NT_SUCCESS(Status) ) {
        ExFreePool(FileNameInfo);
        return Status;
    }

    FileName->Length = 0;
    FileName->MaximumLength = (USHORT)((FileNameInfo->Name.Length/sizeof(WCHAR)) + 1);
    FileName->Buffer = ExAllocatePoolWithTag (PagedPool,
                                              FileName->MaximumLength,
                                              '  mM');
    if (!FileName->Buffer) {
        ExFreePool(FileNameInfo);
        return STATUS_NO_MEMORY;
    }

    RtlUnicodeStringToAnsiString ((PANSI_STRING)FileName,
                                  &FileNameInfo->Name,FALSE);

    FileName->Buffer[FileName->Length] = '\0';
    ExFreePool(FileNameInfo);

    return STATUS_SUCCESS;
}


NTSTATUS
MmGetFileNameForAddress (
    IN PVOID ProcessVa,
    OUT PUNICODE_STRING FileName
    )

/*++

Routine Description:

    This function returns the file name for the corresponding process address if it corresponds to an image section.

Arguments:

    ProcessVa - Process virtual address

    FileName - Returns the name of the corresponding section.

Return Value:

    NTSTATUS - Status of operation

Environment:

    Kernel mode, APC_LEVEL or below, no mutexes held.

--*/
{
    PMMVAD Vad;
    PFILE_OBJECT FileObject;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    ULONG RetLen;
    ULONG BufLen;
    PEPROCESS Process;
    POBJECT_NAME_INFORMATION FileNameInfo;

    PAGED_CODE ();

    Process = PsGetCurrentProcess();

    LOCK_ADDRESS_SPACE (Process);

    Vad = MiLocateAddress (ProcessVa);

    if (Vad == NULL) {

        //
        // No virtual address is allocated at the specified base address,
        // return an error.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    //
    // Reject private memory.
    //

    if (Vad->u.VadFlags.PrivateMemory == 1) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    ControlArea = Vad->ControlArea;

    if (ControlArea == NULL) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    //
    // Reject non-image sections.
    //

    if (ControlArea->u.Flags.Image == 0) {
        Status = STATUS_SECTION_NOT_IMAGE;
        goto ErrorReturn;
    }

    FileObject = ControlArea->FilePointer;

    ASSERT (FileObject != NULL);

    ObReferenceObject (FileObject);

    UNLOCK_ADDRESS_SPACE (Process);

    //
    // Pick an initial size big enough for most reasonable files.
    //

    BufLen = sizeof (*FileNameInfo) + 1024;

    do {

        FileNameInfo = ExAllocatePoolWithTag (PagedPool, BufLen, '  mM');

        if (FileNameInfo == NULL) {
            Status = STATUS_NO_MEMORY;
            break;
        }

        RetLen = 0;

        Status = ObQueryNameString (FileObject, FileNameInfo, BufLen, &RetLen);

        if (NT_SUCCESS (Status)) {
            FileName->Length = FileName->MaximumLength = FileNameInfo->Name.Length;
            FileName->Buffer = (PWCHAR) FileNameInfo;
            RtlMoveMemory (FileName->Buffer, FileNameInfo->Name.Buffer, FileName->Length);
        }
        else {
            ExFreePool (FileNameInfo);
            if (RetLen > BufLen) {
                BufLen = RetLen;
                continue;
            }
        }
        break;

    } while (TRUE);

    ObDereferenceObject (FileObject);
    return Status;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}

PFILE_OBJECT
MmGetFileObjectForSection (
    IN PVOID Section
    )

/*++

Routine Description:

    This routine returns a pointer to the file object backing a section object.

Arguments:

    Section - Supplies the section to query.

Return Value:

    A pointer to the file object backing the argument section.

Environment:

    Kernel mode, PASSIVE_LEVEL.

    The caller must ensure that the section is valid for the
    duration of the call.

--*/

{
    PFILE_OBJECT FileObject;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (Section != NULL);

    FileObject = ((PSECTION)Section)->Segment->ControlArea->FilePointer;

    return FileObject;
}

VOID
MiCheckControlArea (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS CurrentProcess,
    IN KIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine checks the reference counts for the specified
    control area, and if the counts are all zero, it marks the
    control area for deletion and queues it to the deletion thread.


    *********************** NOTE ********************************
    This routine returns with the PFN LOCK RELEASED!!!!!

Arguments:

    ControlArea - Supplies a pointer to the control area to check.

    CurrentProcess - Supplies a pointer to the current process if and ONLY
                     IF the working set lock is held.

    PreviousIrql - Supplies the previous IRQL.

Return Value:

    NONE.

Environment:

    Kernel mode, PFN lock held, PFN lock released upon return!!!

--*/

{
    PEVENT_COUNTER PurgeEvent;
    ULONG DeleteOnClose;
    ULONG DereferenceSegment;
    ULONG PagedPoolPercentInUse;
    ULONG NonPagedPoolPercentInUse;

    PurgeEvent = NULL;
    DeleteOnClose = FALSE;
    DereferenceSegment = FALSE;

    MM_PFN_LOCK_ASSERT();
    if ((ControlArea->NumberOfMappedViews == 0) &&
         (ControlArea->NumberOfSectionReferences == 0)) {

        ASSERT (ControlArea->NumberOfUserReferences == 0);

        if (ControlArea->FilePointer != (PFILE_OBJECT)NULL) {

            if (ControlArea->NumberOfPfnReferences == 0) {

                //
                // There are no views and no physical pages referenced
                // by the Segment, dereference the Segment object.
                //

                ControlArea->u.Flags.BeingDeleted = 1;
                DereferenceSegment = TRUE;

                ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
                ControlArea->u.Flags.FilePointerNull = 1;

                if (ControlArea->u.Flags.Image) {

                    MiRemoveImageSectionObject (ControlArea->FilePointer, ControlArea);
                }
                else {

                    ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
                    ControlArea->FilePointer->SectionObjectPointer->DataSectionObject = NULL;

                }
            }
            else {

                //
                // Insert this segment into the unused segment list (unless
                // it is already on the list).
                //

                if (ControlArea->DereferenceList.Flink == NULL) {
                    MI_INSERT_UNUSED_SEGMENT (ControlArea);
                }

                //
                // Indicate if this section should be deleted now that
                // the reference counts are zero.
                //

                DeleteOnClose = ControlArea->u.Flags.DeleteOnClose;

                //
                // The number of mapped views are zero, the number of
                // section references are zero, but there are some
                // pages of the file still resident.  If this is
                // an image with Global Memory, "purge" the subsections
                // which contain the global memory and reset them to
                // point back to the file.
                //

                if (ControlArea->u.Flags.GlobalMemory == 1) {
                    ASSERT (ControlArea->u.Flags.Image == 1);

                    ControlArea->u.Flags.BeingPurged = 1;
                    ControlArea->NumberOfMappedViews = 1;

                    MiPurgeImageSection (ControlArea, CurrentProcess);

                    ControlArea->u.Flags.BeingPurged = 0;
                    ControlArea->NumberOfMappedViews -= 1;
                    if ((ControlArea->NumberOfMappedViews == 0) &&
                        (ControlArea->NumberOfSectionReferences == 0) &&
                        (ControlArea->NumberOfPfnReferences == 0)) {

                        ControlArea->u.Flags.BeingDeleted = 1;
                        DereferenceSegment = TRUE;
                        ControlArea->u.Flags.FilePointerNull = 1;

                        MiRemoveImageSectionObject (ControlArea->FilePointer,
                                                    ControlArea);

                    }
                    else {

                        PurgeEvent = ControlArea->WaitingForDeletion;
                        ControlArea->WaitingForDeletion = NULL;
                    }
                }

                //
                // If delete on close is set and the segment was
                // not deleted, up the count of mapped views so the
                // control area will not be deleted when the PFN lock
                // is released.
                //

                if (DeleteOnClose && !DereferenceSegment) {
                    ControlArea->NumberOfMappedViews = 1;
                    ControlArea->u.Flags.BeingDeleted = 1;
                }
            }

        }
        else {

            //
            // This Segment is backed by a paging file, dereference the
            // Segment object when the number of views goes from 1 to 0
            // without regard to the number of PFN references.
            //

            ControlArea->u.Flags.BeingDeleted = 1;
            DereferenceSegment = TRUE;
        }
    }
    else if (ControlArea->WaitingForDeletion != NULL) {
        PurgeEvent = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;
    }

    UNLOCK_PFN (PreviousIrql);

    if (DereferenceSegment || DeleteOnClose) {

        //
        // Release the working set mutex, if it is held as the object
        // management routines may page fault, etc..
        //

        if (CurrentProcess) {
            UNLOCK_WS_UNSAFE (CurrentProcess);
        }

        ASSERT (ControlArea->Segment->WritableUserReferences == 0);

        if (DereferenceSegment) {

            //
            // Delete the segment.
            //

            MiSegmentDelete (ControlArea->Segment);

        }
        else {

            //
            // The segment should be forced closed now.
            //

            MiCleanSection (ControlArea, TRUE);
        }

        ASSERT (PurgeEvent == NULL);

        //
        // Reacquire the working set lock, if a process was specified.
        //

        if (CurrentProcess) {
            LOCK_WS_UNSAFE (CurrentProcess);
        }

    }
    else {

        //
        // If any threads are waiting for the segment, indicate the
        // the purge operation has completed.
        //

        if (PurgeEvent != NULL) {
            KeSetEvent (&PurgeEvent->Event, 0, FALSE);
        }

        PagedPoolPercentInUse = (ULONG)((MmPagedPoolInfo.AllocatedPagedPool * 100) / (MmSizeOfPagedPoolInBytes >> PAGE_SHIFT));

        NonPagedPoolPercentInUse = (ULONG)((MmAllocatedNonPagedPool * 100) / (MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT));

        if ((PagedPoolPercentInUse > MmConsumedPoolPercentage) ||
            (NonPagedPoolPercentInUse > MmConsumedPoolPercentage)) {

            KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
        }
    }

    return;
}


VOID
MiCheckForControlAreaDeletion (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine checks the reference counts for the specified
    control area, and if the counts are all zero, it marks the
    control area for deletion and queues it to the deletion thread.

Arguments:

    ControlArea - Supplies a pointer to the control area to check.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    KIRQL OldIrql;

    MM_PFN_LOCK_ASSERT();
    if ((ControlArea->NumberOfPfnReferences == 0) &&
        (ControlArea->NumberOfMappedViews == 0) &&
        (ControlArea->NumberOfSectionReferences == 0 )) {

        //
        // This segment is no longer mapped in any address space
        // nor are there any prototype PTEs within the segment
        // which are valid or in a transition state.  Queue
        // the segment to the segment-dereferencer thread
        // which will dereference the segment object, potentially
        // causing the segment to be deleted.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
        ControlArea->u.Flags.FilePointerNull = 1;

        if (ControlArea->u.Flags.Image) {

            MiRemoveImageSectionObject (ControlArea->FilePointer,
                                        ControlArea);
        }
        else {
            ControlArea->FilePointer->SectionObjectPointer->DataSectionObject =
                                                            NULL;
        }

        ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

        if (ControlArea->DereferenceList.Flink != NULL) {

            //
            // Remove the entry from the unused segment list and put it
            // on the dereference list.
            //

            RemoveEntryList (&ControlArea->DereferenceList);

            MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);
        }

        //
        // Image sections still have useful header information in their segment
        // even if no pages are valid or transition so put these at the tail.
        // Data sections have nothing of use if all the data pages are gone so
        // we used to put those at the front.  Now both types go to the rear
        // so that commit extensions go to the front for earlier processing.
        //

        InsertTailList (&MmDereferenceSegmentHeader.ListHead,
                        &ControlArea->DereferenceList);

        ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

        KeReleaseSemaphore (&MmDereferenceSegmentHeader.Semaphore,
                            0L,
                            1L,
                            FALSE);
    }
    return;
}


LOGICAL
MiCheckControlAreaStatus (
    IN SECTION_CHECK_TYPE SectionCheckType,
    IN PSECTION_OBJECT_POINTERS SectionObjectPointers,
    IN ULONG DelayClose,
    OUT PCONTROL_AREA *ControlAreaOut,
    OUT PKIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine checks the status of the control area for the specified
    SectionObjectPointers.  If the control area is in use, that is, the
    number of section references and the number of mapped views are not
    both zero, no action is taken and the function returns FALSE.

    If there is no control area associated with the specified
    SectionObjectPointers or the control area is in the process of being
    created or deleted, no action is taken and the value TRUE is returned.

    If, there are no section objects and the control area is not being
    created or deleted, the address of the control area is returned
    in the ControlArea argument, the address of a pool block to free
    is returned in the SegmentEventOut argument and the PFN_LOCK is
    still held at the return.

Arguments:

    *SegmentEventOut - Returns a pointer to NonPaged Pool which much be
                       freed by the caller when the PFN_LOCK is released.
                       This value is NULL if no pool is allocated and the
                       PFN_LOCK is not held.

    SectionCheckType - Supplies the type of section to check on, one of
                      CheckImageSection, CheckDataSection, CheckBothSection.

    SectionObjectPointers - Supplies the section object pointers through
                            which the control area can be located.

    DelayClose - Supplies a boolean which if TRUE and the control area
                 is being used, the delay on close field should be set
                 in the control area.

    *ControlAreaOut - Returns the address of the control area.

    PreviousIrql - Returns, in the case the PFN_LOCK is held, the previous
                   IRQL so the lock can be released properly.

Return Value:

    FALSE if the control area is in use, TRUE if the control area is gone or
    in the process or being created or deleted.

Environment:

    Kernel mode, PFN lock NOT held.

--*/


{
    PKTHREAD CurrentThread;
    PEVENT_COUNTER IoEvent;
    PEVENT_COUNTER SegmentEvent;
    LOGICAL DeallocateSegmentEvent;
    PCONTROL_AREA ControlArea;
    ULONG SectRef;
    KIRQL OldIrql;

    //
    // Allocate an event to wait on in case the segment is in the
    // process of being deleted.  This event cannot be allocated
    // with the PFN database locked as pool expansion would deadlock.
    //

    *ControlAreaOut = NULL;

    do {

        SegmentEvent = MiGetEventCounter ();

        if (SegmentEvent != NULL) {
            break;
        }

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&MmShortTime);

    } while (TRUE);

    //
    // Acquire the PFN lock and examine the section object pointer
    // value within the file object.
    //
    // File control blocks live in non-paged pool.
    //

    LOCK_PFN (OldIrql);

    if (SectionCheckType != CheckImageSection) {
        ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->DataSectionObject));
    }
    else {
        ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->ImageSectionObject));
    }

    if (ControlArea == NULL) {

        if (SectionCheckType != CheckBothSection) {

            //
            // This file no longer has an associated segment.
            //

            UNLOCK_PFN (OldIrql);
            MiFreeEventCounter (SegmentEvent);
            return TRUE;
        }
        else {
            ControlArea = ((PCONTROL_AREA)(SectionObjectPointers->ImageSectionObject));
            if (ControlArea == NULL) {

                //
                // This file no longer has an associated segment.
                //

                UNLOCK_PFN (OldIrql);
                MiFreeEventCounter (SegmentEvent);
                return TRUE;
            }
        }
    }

    //
    //  Depending on the type of section, check for the pertinent
    //  reference count being non-zero.
    //

    if (SectionCheckType != CheckUserDataSection) {
        SectRef = ControlArea->NumberOfSectionReferences;
    }
    else {
        SectRef = ControlArea->NumberOfUserReferences;
    }

    if ((SectRef != 0) ||
        (ControlArea->NumberOfMappedViews != 0) ||
        (ControlArea->u.Flags.BeingCreated)) {


        //
        // The segment is currently in use or being created.
        //

        if (DelayClose) {

            //
            // The section should be deleted when the reference
            // counts are zero, set the delete on close flag.
            //

            ControlArea->u.Flags.DeleteOnClose = 1;
        }

        UNLOCK_PFN (OldIrql);
        MiFreeEventCounter (SegmentEvent);
        return FALSE;
    }

    //
    // The segment has no references, delete it.  If the segment
    // is already being deleted, set the event field in the control
    // area and wait on the event.
    //

    if (ControlArea->u.Flags.BeingDeleted) {

        //
        // The segment object is in the process of being deleted.
        // Check to see if another thread is waiting for the deletion,
        // otherwise create and event object to wait upon.
        //

        if (ControlArea->WaitingForDeletion == NULL) {

            //
            // Create an event and put its address in the control area.
            //

            DeallocateSegmentEvent = FALSE;
            ControlArea->WaitingForDeletion = SegmentEvent;
            IoEvent = SegmentEvent;
        }
        else {
            DeallocateSegmentEvent = TRUE;
            IoEvent = ControlArea->WaitingForDeletion;

            //
            // No interlock is needed for the RefCount increment as
            // no thread can be decrementing it since it is still
            // pointed to by the control area.
            //

            IoEvent->RefCount += 1;
        }

        //
        // Release the mutex and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_PFN_AND_THEN_WAIT(OldIrql);

        KeWaitForSingleObject(&IoEvent->Event,
                              WrPageOut,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);

        //
        // Before this event can be set, the control area
        // WaitingForDeletion field must be cleared (and may be
        // reinitialized to something else), but cannot be reset
        // to our local event.  This allows us to dereference the
        // event count lock free.
        //

#if 0
        //
        // Note that the control area cannot be referenced at this
        // point because it may have been freed.
        //

        ASSERT (IoEvent != ControlArea->WaitingForDeletion);
#endif

        KeLeaveCriticalRegionThread (CurrentThread);

        MiFreeEventCounter (IoEvent);
        if (DeallocateSegmentEvent == TRUE) {
            MiFreeEventCounter (SegmentEvent);
        }
        return TRUE;
    }

    //
    // Return with the PFN database locked.
    //

    ASSERT (SegmentEvent->RefCount == 1);
    ASSERT (SegmentEvent->ListEntry.Next == NULL);

    //
    // NO interlock is needed for the RefCount clearing as the event counter
    // was never pointed to by a control area.
    //

#if DBG
    SegmentEvent->RefCount = 0;
#endif

    InterlockedPushEntrySList (&MmEventCountSListHead,
                               (PSINGLE_LIST_ENTRY)&SegmentEvent->ListEntry);

    *ControlAreaOut = ControlArea;
    *PreviousIrql = OldIrql;
    return FALSE;
}


PEVENT_COUNTER
MiGetEventCounter (
    VOID
    )

/*++

Routine Description:

    This function maintains a list of "events" to allow waiting
    on segment operations (deletion, creation, purging).

Arguments:

    None.

Return Value:

    Event to be used for waiting (stored into the control area) or NULL if
    no event could be allocated.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSINGLE_LIST_ENTRY SingleListEntry;
    PEVENT_COUNTER Support;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    if (ExQueryDepthSList (&MmEventCountSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmEventCountSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         EVENT_COUNTER,
                                         ListEntry);

            ASSERT (Support->RefCount == 0);
            KeClearEvent (&Support->Event);
            Support->RefCount = 1;
#if DBG
            Support->ListEntry.Next = NULL;
#endif
            return Support;
        }
    }

    Support = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(EVENT_COUNTER),
                                     'xEmM');
    if (Support == NULL) {
        return NULL;
    }

    KeInitializeEvent (&Support->Event, NotificationEvent, FALSE);

    Support->RefCount = 1;
#if DBG
    Support->ListEntry.Next = NULL;
#endif

    return Support;
}


VOID
MiFreeEventCounter (
    IN PEVENT_COUNTER Support
    )

/*++

Routine Description:

    This routine frees an event counter back to the free list.

Arguments:

    Support - Supplies a pointer to the event counter.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSINGLE_LIST_ENTRY SingleListEntry;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    ASSERT (Support->RefCount != 0);
    ASSERT (Support->ListEntry.Next == NULL);

    //
    // An interlock is needed for the RefCount decrement as the event counter
    // is no longer pointed to by a control area and thus, any number of
    // threads can be running this code without any other serialization.
    //

    if (InterlockedDecrement ((PLONG)&Support->RefCount) == 0) {

        if (ExQueryDepthSList (&MmEventCountSListHead) < 4) {
            InterlockedPushEntrySList (&MmEventCountSListHead,
                                       (PSINGLE_LIST_ENTRY)&Support->ListEntry);
            return;
        }
        ExFreePool (Support);
    }

    //
    // If excess event blocks are stashed then free them now.
    //

    while (ExQueryDepthSList (&MmEventCountSListHead) > 4) {

        SingleListEntry = InterlockedPopEntrySList (&MmEventCountSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         EVENT_COUNTER,
                                         ListEntry);

            ExFreePool (Support);
        }
    }

    return;
}


BOOLEAN
MmCanFileBeTruncated (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN PLARGE_INTEGER NewFileSize
    )

/*++

Routine Description:

    This routine does the following:

        1.  Checks to see if a image section is in use for the file,
            if so it returns FALSE.

        2.  Checks to see if a user section exists for the file, if
            it does, it checks to make sure the new file size is greater
            than the size of the file, if not it returns FALSE.

        3.  If no image section exists, and no user created data section
            exists or the file's size is greater, then TRUE is returned.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

    NewFileSize - Supplies a pointer to the size the file is getting set to.

Return Value:

    TRUE if the file can be truncated, FALSE if it cannot be.

Environment:

    Kernel mode.

--*/

{
    LARGE_INTEGER LocalOffset;
    KIRQL OldIrql;

    //
    //  Capture caller's file size, since we may modify it.
    //

    if (ARGUMENT_PRESENT(NewFileSize)) {

        LocalOffset = *NewFileSize;
        NewFileSize = &LocalOffset;
    }

    if (MiCanFileBeTruncatedInternal( SectionPointer, NewFileSize, FALSE, &OldIrql )) {

        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    return FALSE;
}

ULONG
MiCanFileBeTruncatedInternal (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN PLARGE_INTEGER NewFileSize OPTIONAL,
    IN LOGICAL BlockNewViews,
    OUT PKIRQL PreviousIrql
    )

/*++

Routine Description:

    This routine does the following:

        1.  Checks to see if a image section is in use for the file,
            if so it returns FALSE.

        2.  Checks to see if a user section exists for the file, if
            it does, it checks to make sure the new file size is greater
            than the size of the file, if not it returns FALSE.

        3.  If no image section exists, and no user created data section
            exists or the files size is greater, then TRUE is returned.

Arguments:

    SectionPointer - Supplies a pointer to the section object pointers
                     from the file object.

    NewFileSize - Supplies a pointer to the size the file is getting set to.

    BlockNewViews - Supplies TRUE if the caller will block new views while
                    the operation (usually a purge) proceeds.  This allows
                    this routine to return TRUE even if the user has section
                    references, provided the user currently has no mapped views.

    PreviousIrql - If returning TRUE, returns Irql to use when unlocking
                   Pfn database.

Return Value:

    TRUE if the file can be truncated (PFN locked).
    FALSE if it cannot be truncated (PFN not locked).

Environment:

    Kernel mode.

--*/

{
    KIRQL OldIrql;
    LARGE_INTEGER SegmentSize;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMAPPED_FILE_SEGMENT Segment;

    if (!MmFlushImageSection (SectionPointer, MmFlushForWrite)) {
        return FALSE;
    }

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);

    if (ControlArea != NULL) {

        if ((ControlArea->u.Flags.BeingCreated) ||
            (ControlArea->u.Flags.BeingDeleted) ||
            (ControlArea->u.Flags.Rom)) {
            goto UnlockAndReturn;
        }

        //
        // If there are user references and the size is less than the
        // size of the user view, don't allow the truncation.
        //

        if ((ControlArea->NumberOfUserReferences != 0) &&
            ((BlockNewViews == FALSE) || (ControlArea->NumberOfMappedViews != 0))) {

            //
            // You cannot truncate the entire section if there is a user
            // reference.
            //

            if (!ARGUMENT_PRESENT(NewFileSize)) {
                goto UnlockAndReturn;
            }

            //
            // Locate last subsection and get total size.
            //

            ASSERT (ControlArea->u.Flags.Image == 0);
            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

            Subsection = (PSUBSECTION)(ControlArea + 1);

            if (ControlArea->FilePointer != NULL) {
                Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

                if (MmIsAddressValid (Segment)) {
                    if (Segment->LastSubsectionHint != NULL) {
                        Subsection = (PSUBSECTION) Segment->LastSubsectionHint;
                    }
                }
            }

            while (Subsection->NextSubsection != NULL) {
                Subsection = Subsection->NextSubsection;
            }

            ASSERT (Subsection->ControlArea == ControlArea);

            SegmentSize = MiEndingOffset(Subsection);

            if ((UINT64)NewFileSize->QuadPart < (UINT64)SegmentSize.QuadPart) {
                goto UnlockAndReturn;
            }

            //
            // If there are mapped views, we will skip the last page
            // of the section if the size passed in falls in that page.
            // The caller (like Cc) may want to clear this fractional page.
            //

            SegmentSize.QuadPart += PAGE_SIZE - 1;
            SegmentSize.LowPart &= ~(PAGE_SIZE - 1);
            if ((UINT64)NewFileSize->QuadPart < (UINT64)SegmentSize.QuadPart) {
                *NewFileSize = SegmentSize;
            }
        }
    }

    *PreviousIrql = OldIrql;
    return TRUE;

UnlockAndReturn:
    UNLOCK_PFN (OldIrql);
    return FALSE;
}

PFILE_OBJECT *
MmPerfUnusedSegmentsEnumerate (
    VOID
    )

/*++

Routine Description:

    This routine walks the MmUnusedSegmentList and returns 
    a pointer to a pool allocation containing the 
    referenced file object pointers.

Arguments:

    None.

Return Value:
    
    Returns a pointer to a NULL terminated pool allocation containing the
    file object pointers from the unused segment list, NULL if the memory
    could not be allocated.

    It is also the responsibility of the caller to dereference each
    file object in the list and then free the returned pool.

Environment:

    PASSIVE_LEVEL, arbitrary thread context.

--*/
{
    KIRQL OldIrql;
    ULONG SegmentCount;
    PFILE_OBJECT *FileObjects;
    PFILE_OBJECT *File;
    PLIST_ENTRY NextEntry;
    PCONTROL_AREA ControlArea;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

ReAllocate:

    SegmentCount = MmUnusedSegmentCount + 10;

    FileObjects = (PFILE_OBJECT *) ExAllocatePoolWithTag (
                                            NonPagedPool,
                                            SegmentCount * sizeof(PFILE_OBJECT),
                                            '01pM');

    if (FileObjects == NULL) {
        return NULL;
    }

    File = FileObjects;

    LOCK_PFN (OldIrql);

    //
    // Leave space for NULL terminator.
    //

    if (SegmentCount - 1 < MmUnusedSegmentCount) {
        UNLOCK_PFN (OldIrql);
        ExFreePool (FileObjects);
        goto ReAllocate;
    }

    NextEntry = MmUnusedSegmentList.Flink; 

    while (NextEntry != &MmUnusedSegmentList) {

        ControlArea = CONTAINING_RECORD (NextEntry,
                                         CONTROL_AREA,
                                         DereferenceList);

        *File = ControlArea->FilePointer;
        ObReferenceObject(*File);
        File += 1;

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_PFN (OldIrql);

    *File = NULL;

    return FileObjects;
}

#if DBG
PMSUBSECTION MiActiveSubsection;
LOGICAL MiRemoveSubsectionsFirst;
#endif


VOID
MiRemoveUnusedSegments (
    VOID
    )

/*++

Routine Description:

    This routine removes unused segments (no section references,
    no mapped views only PFN references that are in transition state).

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    LOGICAL DroppedPfnLock;
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    ULONG ConsecutiveFileLockFailures;
    ULONG ConsecutivePagingIOs;
    PSUBSECTION Subsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    PMSUBSECTION MappedSubsection;
    ULONG NumberOfPtes;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE ProtoPtes;
    PMMPTE ProtoPtes2;
    PMMPTE LastProtoPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    IO_STATUS_BLOCK IoStatus;
    LOGICAL DirtyPagesOk;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    ULONG ForceFree;
    ULONG LoopCount;
    PMMPAGE_FILE_EXPANSION PageExpand;

    LoopCount = 0;
    ConsecutivePagingIOs = 0;
    ConsecutiveFileLockFailures = 0;

    //
    // If overall system pool usage is acceptable, then don't discard
    // any cache.
    //

    while ((MI_UNUSED_SEGMENTS_SURPLUS()) || (MmUnusedSegmentForceFree != 0)) {

        LoopCount += 1;
        if ((LoopCount & (64 - 1)) == 0) {

            //
            // Periodically delay so the mapped and modified writers get
            // a shot at writing out the pages this (higher priority) thread
            // is releasing.
            //

            ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);

            while (!IsListEmpty (&MmDereferenceSegmentHeader.ListHead)) {

                MiSubsectionActions |= 0x8000000;

                //
                // The list is not empty, see if the first request is for
                // a commit extension and if so, process it now.
                //

                NextEntry = MmDereferenceSegmentHeader.ListHead.Flink;

                ControlArea = CONTAINING_RECORD (NextEntry,
                                                 CONTROL_AREA,
                                                 DereferenceList);

                if (ControlArea->Segment != NULL) {
                    break;
                }

                PageExpand = (PMMPAGE_FILE_EXPANSION) ControlArea;

                if (PageExpand->RequestedExpansionSize == MI_CONTRACT_PAGEFILES) {
                    break;
                }

                //
                // This is a request to expand the paging files.
                //

                MiSubsectionActions |= 0x10000000;
                RemoveEntryList (NextEntry);
                ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

                MiExtendPagingFiles (PageExpand);
                KeSetEvent (&PageExpand->Event, 0, FALSE);

                ExAcquireSpinLock (&MmDereferenceSegmentHeader.Lock, &OldIrql);
            }

            ExReleaseSpinLock (&MmDereferenceSegmentHeader.Lock, OldIrql);

            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
        }

        //
        // Eliminate some of the unused segments which are only
        // kept in memory because they contain transition pages.
        //

        Status = STATUS_SUCCESS;

        LOCK_PFN (OldIrql);

        if ((IsListEmpty(&MmUnusedSegmentList)) &&
            (IsListEmpty(&MmUnusedSubsectionList))) {

            //
            // There is nothing in the list, rewait.
            //

            ForceFree = MmUnusedSegmentForceFree;
            MmUnusedSegmentForceFree = 0;
            ASSERT (MmUnusedSegmentCount == 0);
            UNLOCK_PFN (OldIrql);

            //
            // We weren't able to get as many segments or subsections as we
            // wanted.  So signal the cache manager to start unmapping
            // system cache views in an attempt to get back the paged
            // pool containing its prototype PTEs.  If Cc was able to free
            // any at all, then restart our loop.
            //

            if (CcUnmapInactiveViews (50) == TRUE) {
                LOCK_PFN (OldIrql);
                if (ForceFree > MmUnusedSegmentForceFree) {
                    MmUnusedSegmentForceFree = ForceFree;
                }
                UNLOCK_PFN (OldIrql);
                continue;
            }

            break;
        }

        if (MmUnusedSegmentForceFree != 0) {
            MmUnusedSegmentForceFree -= 1;
        }

#if DBG
        if (MiRemoveSubsectionsFirst == TRUE) {
            if (!IsListEmpty(&MmUnusedSubsectionList)) {
                goto ProcessSubsectionsFirst;
            }
        }
#endif

        if (IsListEmpty(&MmUnusedSegmentList)) {

#if DBG
ProcessSubsectionsFirst:
#endif

            //
            // The unused segment list was empty, go for the unused subsection
            // list instead.
            //

            ASSERT (!IsListEmpty(&MmUnusedSubsectionList));

            MiSubsectionsProcessed += 1;
            NextEntry = RemoveHeadList(&MmUnusedSubsectionList);

            MappedSubsection = CONTAINING_RECORD (NextEntry,
                                                  MSUBSECTION,
                                                  DereferenceList);

            ControlArea = MappedSubsection->ControlArea;

            ASSERT (ControlArea->u.Flags.Image == 0);
            ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);
            ASSERT (ControlArea->FilePointer != NULL);
            ASSERT (MappedSubsection->NumberOfMappedViews == 0);
            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            //
            // Set the flink to NULL indicating this subsection
            // is not on any lists.
            //

            MappedSubsection->DereferenceList.Flink = NULL;

            if (ControlArea->u.Flags.BeingDeleted == 1) {
                MiSubsectionActions |= 0x1;
                UNLOCK_PFN (OldIrql);
                ConsecutivePagingIOs = 0;
                continue;
            }

            if (ControlArea->u.Flags.NoModifiedWriting == 1) {
                MiSubsectionActions |= 0x2;
                InsertTailList (&MmUnusedSubsectionList,
                                &MappedSubsection->DereferenceList);
                MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
                UNLOCK_PFN (OldIrql);
                ConsecutivePagingIOs = 0;
                continue;
            }

            //
            // Up the number of mapped views to prevent other threads
            // from freeing this.  Clear the accessed bit so we'll know
            // if another thread opens the subsection while we're flushing
            // and closes it before we finish the flush - the other thread
            // may have modified some pages which can then cause our
            // MiCleanSection call (which expects no modified pages in this
            // case) to deadlock with the filesystem.
            //

            MappedSubsection->NumberOfMappedViews = 1;
            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 0;

#if DBG
            MiActiveSubsection = MappedSubsection;
#endif

            //
            // Increment the number of mapped views on the control area to
            // prevent threads that are purging the section from deleting it
            // from underneath us while we process one of its subsections.
            //

            ControlArea->NumberOfMappedViews += 1;

            UNLOCK_PFN (OldIrql);

            ASSERT (MappedSubsection->SubsectionBase != NULL);

            PointerPte = &MappedSubsection->SubsectionBase[0];
            LastPte = &MappedSubsection->SubsectionBase
                            [MappedSubsection->PtesInSubsection - 1];

            //
            // Preacquire the file to prevent deadlocks with other flushers
            // Also mark ourself as a top level IRP so the filesystem knows
            // we are holding no other resources and that it can unroll if
            // it needs to in order to avoid deadlock.  Don't hold this
            // protection any longer than we need to.
            //

            Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            if (NT_SUCCESS(Status)) {

                IoSetTopLevelIrp((PIRP)FSRTL_FSP_TOP_LEVEL_IRP);

                Status = MiFlushSectionInternal (PointerPte,
                                                 LastPte,
                                                 (PSUBSECTION) MappedSubsection,
                                                 (PSUBSECTION) MappedSubsection,
                                                 FALSE,
                                                 FALSE,
                                                 &IoStatus);

                IoSetTopLevelIrp((PIRP)NULL);

                //
                //  Now release the file.
                //

                FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);
            }

            LOCK_PFN (OldIrql);

#if DBG
            MiActiveSubsection = NULL;
#endif

            //
            // Before checking for any failure codes, see if any other
            // threads accessed the subsection while the flush was ongoing.
            //
            // Note that beyond the case of another thread currently using
            // the subsection, the more subtle one is where another
            // thread accessed the subsection and modified some pages.
            // The flush needs to redone (so the clean is guaranteed to work)
            // before another clean can be issued.
            //
            // If any of these cases have occurred, grant this subsection
            // a reprieve.
            //

            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
            if ((MappedSubsection->NumberOfMappedViews != 1) ||
                (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1) ||
                (ControlArea->u.Flags.BeingDeleted == 1)) {

Requeue:
                MappedSubsection->NumberOfMappedViews -= 1;

                MiSubsectionActions |= 0x4;

                //
                // If the other thread(s) are done with this subsection,
                // it MUST be requeued here - otherwise if there are any
                // pages in the subsection, when they are reclaimed,
                // MiCheckForControlAreaDeletion checks for and expects
                // the control area to be queued on the unused segment list.
                //
                // Note this must be done very carefully because if the other
                // threads are not done with the subsection, it had better
                // not get put on the unused subsection list.
                //

                if ((MappedSubsection->NumberOfMappedViews == 0) &&
                    (ControlArea->u.Flags.BeingDeleted == 0)) {

                    MiSubsectionActions |= 0x8;
                    ASSERT (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1);
                    ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

                    InsertTailList (&MmUnusedSubsectionList,
                                    &MappedSubsection->DereferenceList);

                    MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
                }

                ControlArea->NumberOfMappedViews -= 1;
                UNLOCK_PFN (OldIrql);
                continue;
            }

            ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

            if (!NT_SUCCESS(Status)) {

                MiSubsectionActions |= 0x10;

                //
                // If the filesystem told us it had to unroll to avoid
                // deadlock OR we hit a mapped writer collision OR
                // the error occurred on a local file:
                //
                // Then requeue this at the end so we can try again later.
                //
                // Any other errors for networked files are assumed to be
                // permanent (ie: the link may have gone down for an indefinite
                // period), so these sections are cleaned regardless.
                //

                MappedSubsection->NumberOfMappedViews -= 1;

                InsertTailList (&MmUnusedSubsectionList,
                                &MappedSubsection->DereferenceList);

                MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);

                ControlArea->NumberOfMappedViews -= 1;

                UNLOCK_PFN (OldIrql);

                if (Status == STATUS_FILE_LOCK_CONFLICT) {
                    ConsecutiveFileLockFailures += 1;
                }
                else {
                    ConsecutiveFileLockFailures = 0;
                }

                //
                // 10 consecutive file locking failures means we need to
                // yield the processor to allow the filesystem to unjam.
                // Nothing magic about 10, just a number so it
                // gives the worker threads a chance to run.
                //

                if (ConsecutiveFileLockFailures >= 10) {
                    KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                    ConsecutiveFileLockFailures = 0;
                }
                continue;
            }

            //
            // The final check that must be made is whether any faults are
            // currently in progress which are backed by this subsection.
            // Note this is the perverse case where one thread in a process
            // has unmapped the relevant VAD even while other threads in the
            // same process are faulting on the addresses in that VAD (if the
            // VAD had not been unmapped then the subsection view count would
            // have been nonzero and caught above).  Clearly this is a bad
            // process, but nonetheless it must be detected and handled here
            // because upon conclusion of the inpage, the thread will compare
            // (unsynchronized) against the prototype PTEs which may in
            // various stages of deletion below and would cause corruption.
            //

            MiSubsectionActions |= 0x20;

            ASSERT (MappedSubsection->NumberOfMappedViews == 1);
            ProtoPtes = MappedSubsection->SubsectionBase;
            NumberOfPtes = MappedSubsection->PtesInSubsection;

            //
            // Note checking the prototype PTEs must be done carefully as
            // they are pagable and the PFN lock is (and must be) held.
            //

            ProtoPtes2 = ProtoPtes;
            LastProtoPte = ProtoPtes + NumberOfPtes;

            while (ProtoPtes2 < LastProtoPte) {

                if ((ProtoPtes2 == ProtoPtes) ||
                    (MiIsPteOnPdeBoundary (ProtoPtes2))) {

                    if (MiCheckProtoPtePageState (ProtoPtes2, TRUE, &DroppedPfnLock) == FALSE) {

                        //
                        // Skip this chunk as it is paged out and thus, cannot
                        // have any valid or transition PTEs within it.
                        //

                        ProtoPtes2 = (PMMPTE)(((ULONG_PTR)ProtoPtes2 | (PAGE_SIZE - 1)) + 1);
                        continue;
                    }
                    else {

                        //
                        // The prototype PTE page is resident right now - but
                        // if the PFN lock was dropped & reacquired to make it
                        // so, then anything could have changed - so everything
                        // must be rechecked.
                        //

                        if (DroppedPfnLock == TRUE) {
                            if ((MappedSubsection->NumberOfMappedViews != 1) ||
                                (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 1) ||
                                (ControlArea->u.Flags.BeingDeleted == 1)) {

                                MiSubsectionActions |= 0x40;
                                goto Requeue;
                            }
                        }
                    }
                }

                PteContents = *ProtoPtes2;
                if (PteContents.u.Hard.Valid == 1) {
                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x3,
                                  (ULONG_PTR)MappedSubsection,
                                  (ULONG_PTR)ProtoPtes2,
                                  (ULONG_PTR)PteContents.u.Long);
                }

                if (PteContents.u.Soft.Prototype == 1) {
                    MiSubsectionActions |= 0x200;
                    NOTHING;        // This is the expected case.
                }
                else if (PteContents.u.Soft.Transition == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);

                    if (Pfn1->u3.e1.Modified == 1) {

                        //
                        // An I/O transfer finished after the last view was
                        // unmapped.  MmUnlockPages can set the modified bit
                        // in this situation so it must be handled properly
                        // here - ie: mark the subsection as needing to be
                        // reprocessed and march on.
                        //

                        MiSubsectionActions |= 0x8000000;
                        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
                        goto Requeue;
                    }

                    if (Pfn1->u3.e2.ReferenceCount != 0) {

                        ASSERT (Pfn1->u4.LockCharged == 1);

                        //
                        // A fault is being satisfied for deleted address space,
                        // so don't eliminate this subsection right now.
                        //

                        MiSubsectionActions |= 0x400;
                        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
                        goto Requeue;
                    }
                    MiSubsectionActions |= 0x800;
                }
                else {
                    if (PteContents.u.Long != 0) {
                        KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                      0x4,
                                      (ULONG_PTR)MappedSubsection,
                                      (ULONG_PTR)ProtoPtes2,
                                      (ULONG_PTR)PteContents.u.Long);
                    }

                    MiSubsectionActions |= 0x1000;
                }

                ProtoPtes2 += 1;
            }

            MiSubsectionActions |= 0x2000;

            //
            // There can be no modified pages in this subsection at this point.
            // Sever the subsection's tie to the prototype PTEs while still
            // holding the lock and then decrement the counts on any resident
            // prototype pages.
            //

            ASSERT (MappedSubsection->NumberOfMappedViews == 1);
            MappedSubsection->NumberOfMappedViews = 0;

            MappedSubsection->SubsectionBase = NULL;

            MiSubsectionActions |= 0x8000;
            ProtoPtes2 = ProtoPtes;

            while (ProtoPtes2 < LastProtoPte) {

                if ((ProtoPtes2 == ProtoPtes) ||
                    (MiIsPteOnPdeBoundary (ProtoPtes2))) {

                    if (MiCheckProtoPtePageState (ProtoPtes2, TRUE, &DroppedPfnLock) == FALSE) {

                        //
                        // Skip this chunk as it is paged out and thus, cannot
                        // have any valid or transition PTEs within it.
                        //

                        ProtoPtes2 = (PMMPTE)(((ULONG_PTR)ProtoPtes2 | (PAGE_SIZE - 1)) + 1);
                        continue;
                    }
                    else {

                        //
                        // The prototype PTE page is resident right now - but
                        // if the PFN lock was dropped & reacquired to make it
                        // so, then anything could have changed - but notice
                        // that the SubsectionBase was zeroed above before
                        // entering this loop, so even if the PFN lock was
                        // dropped & reacquired, nothing needs to be rechecked.
                        //
                    }
                }

                PteContents = *ProtoPtes2;

                ASSERT (PteContents.u.Hard.Valid == 0);

                if (PteContents.u.Soft.Prototype == 1) {
                    MiSubsectionActions |= 0x10000;
                    NOTHING;        // This is the expected case.
                }
                else if (PteContents.u.Soft.Transition == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);
                    ASSERT (Pfn1->u3.e1.Modified == 0);

                    //
                    // If the page is on the standby list, move it to the
                    // freelist.  If it's not on the standby list (ie: I/O
                    // is still in progress), when the Iast I/O completes, the
                    // page will be placed on the freelist as the PFN entry
                    // is always marked as deleted now.
                    //

                    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
                    ASSERT (Pfn1->u4.LockCharged == 0);

                    MI_SET_PFN_DELETED (Pfn1);

                    ControlArea->NumberOfPfnReferences -= 1;
                    ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    ASSERT (Pfn1->u3.e1.PageLocation != FreePageList);

                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (PageFrameIndex);
                    MiSubsectionActions |= 0x20000;
                }
                else {
                    MiSubsectionActions |= 0x80000;
                    ASSERT (PteContents.u.Long == 0);
                }

                ProtoPtes2 += 1;
            }

            //
            // If all the cached pages for this control area have been removed
            // then delete it.  This will actually insert the control
            // area into the dereference segment header list.
            //

            ControlArea->NumberOfMappedViews -= 1;

#if DBG
            if ((ControlArea->NumberOfPfnReferences == 0) &&
                (ControlArea->NumberOfMappedViews == 0) &&
                (ControlArea->NumberOfSectionReferences == 0 )) {
                MiSubsectionActions |= 0x100000;
            }
#endif

            MiCheckForControlAreaDeletion (ControlArea);

            UNLOCK_PFN (OldIrql);

            ExFreePool (ProtoPtes);

            ConsecutiveFileLockFailures = 0;

            continue;
        }

        ASSERT (!IsListEmpty(&MmUnusedSegmentList));

        NextEntry = RemoveHeadList(&MmUnusedSegmentList);

        ControlArea = CONTAINING_RECORD (NextEntry,
                                         CONTROL_AREA,
                                         DereferenceList);

        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

#if DBG
        if (ControlArea->u.Flags.BeingDeleted == 0) {
          if (ControlArea->u.Flags.Image) {
            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject)) != NULL);
          }
          else {
            ASSERT (((PCONTROL_AREA)(ControlArea->FilePointer->SectionObjectPointer->DataSectionObject)) != NULL);
          }
        }
#endif

        //
        // Set the flink to NULL indicating this control area
        // is not on any lists.
        //

        ControlArea->DereferenceList.Flink = NULL;

        if ((ControlArea->NumberOfMappedViews == 0) &&
            (ControlArea->NumberOfSectionReferences == 0) &&
            (ControlArea->u.Flags.BeingDeleted == 0)) {

            //
            // If there is paging I/O in progress on this
            // segment, just put this at the tail of the list, as
            // the call to MiCleanSegment would block waiting
            // for the I/O to complete.  As this could tie up
            // the thread, don't do it.  Check if these are the only
            // types of segments on the dereference list so we don't
            // spin forever and wedge the system.
            //

            if (ControlArea->ModifiedWriteCount > 0) {
                MI_INSERT_UNUSED_SEGMENT (ControlArea);

                UNLOCK_PFN (OldIrql);

                ConsecutivePagingIOs += 1;
                if (ConsecutivePagingIOs > 10) {
                    KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                    ConsecutivePagingIOs = 0;
                }
                continue;
            }
            ConsecutivePagingIOs = 0;

            //
            // Up the number of mapped views to prevent other threads
            // from freeing this.  Clear the accessed bit so we'll know
            // if another thread opens the control area while we're flushing
            // and closes it before we finish the flush - the other thread
            // may have modified some pages which can then cause our
            // MiCleanSection call (which expects no modified pages in this
            // case) to deadlock with the filesystem.
            //

            ControlArea->NumberOfMappedViews = 1;
            ControlArea->u.Flags.Accessed = 0;

            if (ControlArea->u.Flags.Image == 0) {

                ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);
                if (ControlArea->u.Flags.Rom == 0) {
                    Subsection = (PSUBSECTION)(ControlArea + 1);
                }
                else {
                    Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
                }

                MiSubsectionActions |= 0x200000;

                while (Subsection->SubsectionBase == NULL) {

                    Subsection = Subsection->NextSubsection;

                    if (Subsection == NULL) {

                        MiSubsectionActions |= 0x400000;

                        //
                        // All the subsections for this segment have already
                        // been trimmed so nothing left to flush.  Just get rid
                        // of the segment carcass provided no other thread
                        // accessed it while we weren't holding the PFN lock.
                        //

                        UNLOCK_PFN (OldIrql);
                        goto skip_flush;
                    }
                    else {
                        MiSubsectionActions |= 0x800000;
                    }
                }

                PointerPte = &Subsection->SubsectionBase[0];
                LastSubsection = Subsection;
                LastSubsectionWithProtos = Subsection;

                while (LastSubsection->NextSubsection != NULL) {
                    if (LastSubsection->SubsectionBase != NULL) {
                        LastSubsectionWithProtos = LastSubsection;
                        MiSubsectionActions |= 0x1000000;
                    }
                    else {
                        MiSubsectionActions |= 0x2000000;
                    }
                    LastSubsection = LastSubsection->NextSubsection;
                }

                if (LastSubsection->SubsectionBase == NULL) {
                    MiSubsectionActions |= 0x4000000;
                    LastSubsection = LastSubsectionWithProtos;
                }

                UNLOCK_PFN (OldIrql);

                LastPte = &LastSubsection->SubsectionBase
                                [LastSubsection->PtesInSubsection - 1];

                //
                // Preacquire the file to prevent deadlocks with other flushers
                // Also mark ourself as a top level IRP so the filesystem knows
                // we are holding no other resources and that it can unroll if
                // it needs to in order to avoid deadlock.  Don't hold this
                // protection any longer than we need to.
                //

                Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

                if (NT_SUCCESS(Status)) {

                    IoSetTopLevelIrp ((PIRP)FSRTL_FSP_TOP_LEVEL_IRP);

                    Status = MiFlushSectionInternal (PointerPte,
                                                     LastPte,
                                                     Subsection,
                                                     LastSubsection,
                                                     FALSE,
                                                     FALSE,
                                                     &IoStatus);

                    IoSetTopLevelIrp (NULL);

                    //
                    //  Now release the file.
                    //

                    FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);
                }

skip_flush:
                LOCK_PFN (OldIrql);
            }

            //
            // Before checking for any failure codes, see if any other
            // threads accessed the control area while the flush was ongoing.
            //
            // Note that beyond the case of another thread currently using
            // the control area, the more subtle one is where another
            // thread accessed the control area and modified some pages.
            // The flush needs to redone (so the clean is guaranteed to work)
            // before another clean can be issued.
            //
            // If any of these cases have occurred, grant this control area
            // a reprieve.
            //

            if (!((ControlArea->NumberOfMappedViews == 1) &&
                (ControlArea->u.Flags.Accessed == 0) &&
                (ControlArea->NumberOfSectionReferences == 0) &&
                (ControlArea->u.Flags.BeingDeleted == 0))) {

                ControlArea->NumberOfMappedViews -= 1;

                //
                // If the other thread(s) are done with this control area,
                // it MUST be requeued here - otherwise if there are any
                // pages in the control area, when they are reclaimed,
                // MiCheckForControlAreaDeletion checks for and expects
                // the control area to be queued on the unused segment list.
                //
                // Note this must be done very carefully because if the other
                // threads are not done with the control area, it had better
                // not get put on the unused segment list.
                //

                //
                // Need to do the equivalent of a MiCheckControlArea here.
                // or reprocess.  Only iff mappedview & sectref = 0.
                //

                if ((ControlArea->NumberOfMappedViews == 0) &&
                    (ControlArea->NumberOfSectionReferences == 0) &&
                    (ControlArea->u.Flags.BeingDeleted == 0)) {

                    ASSERT (ControlArea->u.Flags.Accessed == 1);
                    ASSERT(ControlArea->DereferenceList.Flink == NULL);

                    MI_INSERT_UNUSED_SEGMENT (ControlArea);
                }

                UNLOCK_PFN (OldIrql);
                continue;
            }

            if (!NT_SUCCESS(Status)) {

                //
                // If the filesystem told us it had to unroll to avoid
                // deadlock OR we hit a mapped writer collision OR
                // the error occurred on a local file:
                //
                // Then requeue this at the end so we can try again later.
                //
                // Any other errors for networked files are assumed to be
                // permanent (ie: the link may have gone down for an indefinite
                // period), so these sections are cleaned regardless.
                //

                if ((Status == STATUS_FILE_LOCK_CONFLICT) ||
                    (Status == STATUS_MAPPED_WRITER_COLLISION) ||
                    (ControlArea->u.Flags.Networked == 0)) {

                    ASSERT(ControlArea->DereferenceList.Flink == NULL);

                    ControlArea->NumberOfMappedViews -= 1;

                    MI_INSERT_UNUSED_SEGMENT (ControlArea);

                    UNLOCK_PFN (OldIrql);

                    if (Status == STATUS_FILE_LOCK_CONFLICT) {
                        ConsecutiveFileLockFailures += 1;
                    }
                    else {
                        ConsecutiveFileLockFailures = 0;
                    }

                    //
                    // 10 consecutive file locking failures means we need to
                    // yield the processor to allow the filesystem to unjam.
                    // Nothing magic about 10, just a number so it
                    // gives the worker threads a chance to run.
                    //

                    if (ConsecutiveFileLockFailures >= 10) {
                        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);
                        ConsecutiveFileLockFailures = 0;
                    }
                    continue;
                }
                DirtyPagesOk = TRUE;
            }
            else {
                ConsecutiveFileLockFailures = 0;
                DirtyPagesOk = FALSE;
            }

            ControlArea->u.Flags.BeingDeleted = 1;

            //
            // Don't let any pages be written by the modified
            // page writer from this point on.
            //

            ControlArea->u.Flags.NoModifiedWriting = 1;
            ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
            UNLOCK_PFN (OldIrql);

            MiCleanSection (ControlArea, DirtyPagesOk);

        }
        else {

            //
            // The segment was not eligible for deletion.  Just leave
            // it off the unused segment list and continue the loop.
            //

            UNLOCK_PFN (OldIrql);
            ConsecutivePagingIOs = 0;
        }
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\sources.inc ===
MAJORCOMP=ntos
MINORCOMP=mm

TARGETNAME=mm
TARGETTYPE=LIBRARY
TARGETPATH=obj

INCLUDES=..;..\..\inc;..\..\ke;..\..\kd64;$(SDKTOOLS_INC_PATH)

MSC_WARNING_LEVEL=/W4 /WX

SOURCES=..\acceschk.c \
        ..\addrsup.c  \
        ..\allocpag.c \
        ..\allocvm.c  \
        ..\buildmdl.c \
        ..\checkpfn.c \
        ..\checkpte.c \
        ..\compress.c \
        ..\crashdmp.c \
        ..\creasect.c \
        ..\debugsup.c \
        ..\deleteva.c \
        ..\dmpaddr.c  \
        ..\dynmem.c   \
        ..\extsect.c  \
        ..\flushbuf.c \
        ..\flushsec.c \
        ..\forksup.c  \
        ..\freevm.c   \
        ..\hypermap.c \
        ..\iosup.c    \
        ..\lockvm.c   \
        ..\mapcache.c \
        ..\mapview.c  \
        ..\miglobal.c \
        ..\mirror.c   \
        ..\mmfault.c  \
        ..\mminit.c   \
        ..\mmsup.c    \
        ..\mmquota.c  \
        ..\modwrite.c \
        ..\nolowmem.c \
        ..\pagfault.c \
        ..\pfndec.c   \
        ..\pfnlist.c  \
        ..\pfsup.c    \
        ..\physical.c \
        ..\procsup.c  \
        ..\protect.c  \
        ..\querysec.c \
        ..\queryvm.c  \
        ..\readwrt.c  \
        ..\sectsup.c  \
        ..\session.c  \
        ..\sessload.c \
        ..\shutdown.c \
        ..\specpool.c \
        ..\sysload.c  \
        ..\sysptes.c  \
        ..\triage.c   \
        ..\umapview.c \
        ..\vadtree.c  \
        ..\verifier.c \
        ..\wslist.c   \
        ..\wsmanage.c \
        ..\wstree.c   \
        ..\wrtfault.c \
        ..\wrtwatch.c \
        ..\zeropage.c

PRECOMPILED_INCLUDE=..\mi.h
PRECOMPILED_PCH=mi.pch
PRECOMPILED_OBJ=mi.obj

SOURCES_USED=..\sources.inc
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\shutdown.c ===
/*++

Copyright (c) 1991  Microsoft Corporation

Module Name:

    shutdown.c

Abstract:

    This module contains the shutdown code for the memory management system.

Author:

    Lou Perazzoli (loup) 21-Aug-1991
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

extern ULONG MmSystemShutdown;

VOID
MiReleaseAllMemory (
    VOID
    );

BOOLEAN
MiShutdownSystem (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGELK,MiShutdownSystem)
#pragma alloc_text(PAGELK,MiReleaseAllMemory)
#pragma alloc_text(PAGELK,MmShutdownSystem)
#endif

ULONG MmZeroPageFile;

extern ULONG MmUnusedSegmentForceFree;
extern LIST_ENTRY MmSessionWideAddressList;
extern LIST_ENTRY MiVerifierDriverAddedThunkListHead;
extern LIST_ENTRY MmLoadedUserImageList;
extern PRTL_BITMAP MiSessionIdBitmap;
extern LOGICAL MiZeroingDisabled;
extern ULONG MmNumberOfMappedMdls;
extern ULONG MmNumberOfMappedMdlsInUse;


BOOLEAN
MiShutdownSystem (
    VOID
    )

/*++

Routine Description:

    This function performs the shutdown of memory management.  This
    is accomplished by writing out all modified pages which are
    destined for files other than the paging file.

    All processes have already been killed, the registry shutdown and
    shutdown IRPs already sent.  On return from this phase all mapped
    file data must be flushed and the unused segment list emptied.
    This releases all the Mm references to file objects, allowing many
    drivers (especially the network) to unload.

Arguments:

    None.

Return Value:

    TRUE if the pages were successfully written, FALSE otherwise.

--*/

{
    LOGICAL PageLk;
    SIZE_T ImportListSize;
    PLOAD_IMPORTS ImportList;
    PLOAD_IMPORTS ImportListNonPaged;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PFN_NUMBER ModifiedPage;
    PMMPFN Pfn1;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PPFN_NUMBER Page;
    PFILE_OBJECT FilePointer;
    ULONG ConsecutiveFileLockFailures;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_WRITE_CLUSTER];
    PMDL Mdl;
    NTSTATUS Status;
    KEVENT IoEvent;
    IO_STATUS_BLOCK IoStatus;
    KIRQL OldIrql;
    LARGE_INTEGER StartingOffset;
    ULONG count;
    PFN_NUMBER j;
    ULONG k;
    PFN_NUMBER first;
    ULONG write;
    PMMPAGING_FILE PagingFile;

    PageLk = FALSE;

    //
    // Don't do this more than once.
    //

    if (MmSystemShutdown == 0) {

        PageLk = TRUE;
        MmLockPagableSectionByHandle (ExPageLockHandle);

        Mdl = (PMDL)&MdlHack;
        Page = (PPFN_NUMBER)(Mdl + 1);

        KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

        MmInitializeMdl(Mdl,
                        NULL,
                        PAGE_SIZE);

        Mdl->MdlFlags |= MDL_PAGES_LOCKED;

        LOCK_PFN (OldIrql);

        ModifiedPage = MmModifiedPageListHead.Flink;
        while (ModifiedPage != MM_EMPTY_LIST) {

            //
            // There are modified pages.
            //

            Pfn1 = MI_PFN_ELEMENT (ModifiedPage);

            if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {

                //
                // This page is destined for a file.
                //

                Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
                ControlArea = Subsection->ControlArea;
                if ((!ControlArea->u.Flags.Image) &&
                   (!ControlArea->u.Flags.NoModifiedWriting)) {

                    MiUnlinkPageFromList (Pfn1);

                    //
                    // Issue the write.
                    //

                    MI_SET_MODIFIED (Pfn1, 0, 0x28);

                    //
                    // Up the reference count for the physical page as there
                    // is I/O in progress.
                    //

                    MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, 26);
                    Pfn1->u3.e2.ReferenceCount += 1;

                    *Page = ModifiedPage;
                    ControlArea->NumberOfMappedViews += 1;
                    ControlArea->NumberOfPfnReferences += 1;

                    UNLOCK_PFN (OldIrql);

                    StartingOffset.QuadPart = MiStartingOffset (Subsection,
                                                                Pfn1->PteAddress);
                    Mdl->StartVa = NULL;

                    ConsecutiveFileLockFailures = 0;
                    FilePointer = ControlArea->FilePointer;

retry:
                    KeClearEvent (&IoEvent);

                    Status = FsRtlAcquireFileForCcFlushEx (FilePointer);

                    if (NT_SUCCESS(Status)) {
                        Status = IoSynchronousPageWrite (FilePointer,
                                                         Mdl,
                                                         &StartingOffset,
                                                         &IoEvent,
                                                         &IoStatus);

                        //
                        // Release the file we acquired.
                        //

                        FsRtlReleaseFileForCcFlush (FilePointer);
                    }

                    if (!NT_SUCCESS(Status)) {

                        //
                        // Only try the request more than once if the
                        // filesystem said it had a deadlock.
                        //

                        if (Status == STATUS_FILE_LOCK_CONFLICT) {
                            ConsecutiveFileLockFailures += 1;
                            if (ConsecutiveFileLockFailures < 5) {
                                KeDelayExecutionThread (KernelMode,
                                                        FALSE,
                                                        (PLARGE_INTEGER)&MmShortTime);
                                goto retry;
                            }
                            goto wait_complete;
                        }

                        //
                        // Ignore all I/O failures - there is nothing that
                        // can be done at this point.
                        //

                        KeSetEvent (&IoEvent, 0, FALSE);
                    }

                    Status = KeWaitForSingleObject (&IoEvent,
                                                    WrPageOut,
                                                    KernelMode,
                                                    FALSE,
                                                    (PLARGE_INTEGER)&MmTwentySeconds);

wait_complete:

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (Status == STATUS_TIMEOUT) {

                        //
                        // The write did not complete in 20 seconds, assume
                        // that the file systems are hung and return an
                        // error.
                        //

                        LOCK_PFN (OldIrql);

                        MI_SET_MODIFIED (Pfn1, 1, 0xF);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 27);
                        ControlArea->NumberOfMappedViews -= 1;
                        ControlArea->NumberOfPfnReferences -= 1;

                        //
                        // This routine returns with the PFN lock released!
                        //

                        MiCheckControlArea (ControlArea, NULL, OldIrql);

                        MmUnlockPagableImageSection (ExPageLockHandle);

                        return FALSE;
                    }

                    LOCK_PFN (OldIrql);
                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 27);
                    ControlArea->NumberOfMappedViews -= 1;
                    ControlArea->NumberOfPfnReferences -= 1;

                    //
                    // This routine returns with the PFN lock released!
                    //

                    MiCheckControlArea (ControlArea, NULL, OldIrql);
                    LOCK_PFN (OldIrql);

                    //
                    // Restart scan at the front of the list.
                    //

                    ModifiedPage = MmModifiedPageListHead.Flink;
                    continue;
                }
            }
            ModifiedPage = Pfn1->u1.Flink;
        }

        UNLOCK_PFN (OldIrql);

        //
        // If a high number of modified pages still exist, start the
        // modified page writer and wait for 5 seconds.
        //

        if (MmAvailablePages < (MmFreeGoal * 2)) {
            LARGE_INTEGER FiveSeconds = {(ULONG)(-5 * 1000 * 1000 * 10), -1};

            KeSetEvent (&MmModifiedPageWriterEvent, 0, FALSE);
            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&FiveSeconds);
        }

        //
        // Indicate to the modified page writer that the system has
        // shutdown.
        //

        MmSystemShutdown = 1;

        //
        // Check to see if the paging file should be overwritten.
        // Only free blocks are written.
        //

        if (MmZeroPageFile) {

            //
            // Get pages to complete the write request.
            //

            Mdl->StartVa = NULL;
            j = 0;
            k = 0;
            Page = (PPFN_NUMBER)(Mdl + 1);

            LOCK_PFN (OldIrql);

            if (MmAvailablePages < (MmModifiedWriteClusterSize + 20)) {
                UNLOCK_PFN(OldIrql);
                goto freecache;
            }

            do {
                *Page = MiRemoveZeroPage ((ULONG)j & MmSecondaryColorMask);
                Pfn1 = MI_PFN_ELEMENT (*Page);
                Pfn1->u3.e2.ReferenceCount = 1;
                ASSERT (Pfn1->u2.ShareCount == 0);
                Pfn1->OriginalPte.u.Long = 0;
                MI_SET_PFN_DELETED (Pfn1);
                Page += 1;
                j += 1;
            } while (j < MmModifiedWriteClusterSize);

            while (k < MmNumberOfPagingFiles) {

                PagingFile = MmPagingFile[k];

                count = 0;
                write = FALSE;

                //
                // Initializing first is not needed for correctness, but
                // without it the compiler cannot compile this code W4 to
                // check for use of uninitialized variables.
                //

                first = 0;

                for (j = 1; j < PagingFile->Size; j += 1) {

                    if (RtlCheckBit (PagingFile->Bitmap, j) == 0) {

                        if (count == 0) {
                            first = j;
                        }
                        count += 1;
                        if (count == MmModifiedWriteClusterSize) {
                            write = TRUE;
                        }
                    } else {
                        if (count != 0) {

                            //
                            // Issue a write.
                            //

                            write = TRUE;
                        }
                    }

                    if ((j == (PagingFile->Size - 1)) &&
                        (count != 0)) {
                        write = TRUE;
                    }

                    if (write) {

                        UNLOCK_PFN (OldIrql);

                        StartingOffset.QuadPart = (LONGLONG)first << PAGE_SHIFT;
                        Mdl->ByteCount = count << PAGE_SHIFT;
                        KeClearEvent (&IoEvent);

                        Status = IoSynchronousPageWrite (PagingFile->File,
                                                         Mdl,
                                                         &StartingOffset,
                                                         &IoEvent,
                                                         &IoStatus);

                        //
                        // Ignore all I/O failures - there is nothing that can
                        // be done at this point.
                        //

                        if (!NT_SUCCESS(Status)) {
                            KeSetEvent (&IoEvent, 0, FALSE);
                        }

                        Status = KeWaitForSingleObject (&IoEvent,
                                                        WrPageOut,
                                                        KernelMode,
                                                        FALSE,
                                                        (PLARGE_INTEGER)&MmTwentySeconds);

                        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                        }

                        if (Status == STATUS_TIMEOUT) {

                            //
                            // The write did not complete in 20 seconds, assume
                            // that the file systems are hung and return an
                            // error.
                            //

                            j = 0;
                            Page = (PPFN_NUMBER)(Mdl + 1);
                            LOCK_PFN (OldIrql);
                            do {
                                MiDecrementReferenceCount (*Page);
                                Page += 1;
                                j += 1;
                            } while (j < MmModifiedWriteClusterSize);
                            UNLOCK_PFN (OldIrql);

                            MmUnlockPagableImageSection (ExPageLockHandle);
                            return FALSE;
                        }

                        count = 0;
                        write = FALSE;
                        LOCK_PFN (OldIrql);
                    }
                }
                k += 1;
            }
            j = 0;
            Page = (PPFN_NUMBER)(Mdl + 1);
            do {
                MiDecrementReferenceCount (*Page);
                Page += 1;
                j += 1;
            } while (j < MmModifiedWriteClusterSize);
            UNLOCK_PFN (OldIrql);
        }
    }

freecache:

    if (PageLk == TRUE) {
        MmUnlockPagableImageSection (ExPageLockHandle);
    }

    if (PoCleanShutdownEnabled ()) {

        //
        // Empty the unused segment list.
        //

        LOCK_PFN (OldIrql);
        MmUnusedSegmentForceFree = (ULONG)-1;
        KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);

        //
        // Give it 5 seconds to empty otherwise assume the filesystems are
        // hung and march on.
        //

        for (count = 0; count < 500; count += 1) {

            if (IsListEmpty(&MmUnusedSegmentList)) {
                break;
            }

            UNLOCK_PFN (OldIrql);

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmShortTime);
            LOCK_PFN (OldIrql);

#if DBG
            if (count == 400) {

                //
                // Everything should have been flushed by now.  Give the
                // filesystem team a chance to debug this on checked builds.
                //

                ASSERT (FALSE);
            }
#endif

            //
            // Resignal if needed in case more closed file objects triggered
            // additional entries.
            //

            if (MmUnusedSegmentForceFree == 0) {
                MmUnusedSegmentForceFree = (ULONG)-1;
                KeSetEvent (&MmUnusedSegmentCleanup, 0, FALSE);
            }
        }

        UNLOCK_PFN (OldIrql);

        //
        // Get rid of any paged pool references as they will be illegal
        // by the time MmShutdownSystem is called again since the filesystems
        // will have shutdown.
        //

        KeWaitForSingleObject (&MmSystemLoadLock,
                               WrVirtualMemory,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)NULL);

        NextEntry = PsLoadedModuleList.Flink;
        while (NextEntry != &PsLoadedModuleList) {

            DataTableEntry = CONTAINING_RECORD (NextEntry,
                                                KLDR_DATA_TABLE_ENTRY,
                                                InLoadOrderLinks);

            ImportList = (PLOAD_IMPORTS)DataTableEntry->LoadedImports;

            if ((ImportList != (PVOID)LOADED_AT_BOOT) &&
                (ImportList != (PVOID)NO_IMPORTS_USED) &&
                (!SINGLE_ENTRY(ImportList))) {

                ImportListSize = ImportList->Count * sizeof(PVOID) + sizeof(SIZE_T);
                ImportListNonPaged = (PLOAD_IMPORTS) ExAllocatePoolWithTag (NonPagedPool,
                                                                    ImportListSize,
                                                                    'TDmM');

                if (ImportListNonPaged != NULL) {
                    RtlCopyMemory (ImportListNonPaged, ImportList, ImportListSize);
                    ExFreePool (ImportList);
                    DataTableEntry->LoadedImports = ImportListNonPaged;
                }
                else {

                    //
                    // Don't bother with the clean shutdown at this point.
                    //

                    PopShutdownCleanly = FALSE;
                    break;
                }
            }

            //
            // Free the full DLL name as it is pagable.
            //

            if (DataTableEntry->FullDllName.Buffer != NULL) {
                ExFreePool (DataTableEntry->FullDllName.Buffer);
                DataTableEntry->FullDllName.Buffer = NULL;
            }

            NextEntry = NextEntry->Flink;
        }

        //
        // Free any Hydra resources.  Note that if any session is still alive
        // (ie: all processes must be gone at this point) then the session
        // drivers will not have unloaded.  This is not supposed to happen
        // so assert here to ensure this.
        //

        if (PoCleanShutdownEnabled()) {
            ASSERT (IsListEmpty (&MmSessionWideAddressList) != 0);
            ExFreePool (MiSessionIdBitmap);
        }

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

        //
        // Close all the pagefile handles, note we still have an object
        // reference to each keeping the underlying object resident.
        // At the end of Phase1 shutdown we'll release those references
        // to trigger the storage stack unload.  The handle close must be
        // done here however as it will reference pagable structures.
        //

        for (k = 0; k < MmNumberOfPagingFiles; k += 1) {

            //
            // Free each pagefile name now as it resides in paged pool and
            // may need to be inpaged to be freed.  Since the paging files
            // are going to be shutdown shortly, now is the time to access
            // pagable stuff and get rid of it.  Zeroing the buffer pointer
            // is sufficient as the only accesses to this are from the
            // try-except-wrapped GetSystemInformation APIs and all the
            // user processes are gone already.
            //
        
            ASSERT (MmPagingFile[k]->PageFileName.Buffer != NULL);
            ExFreePool (MmPagingFile[k]->PageFileName.Buffer);
            MmPagingFile[k]->PageFileName.Buffer = NULL;

            ZwClose (MmPagingFile[k]->FileHandle);
        }
    }

    return TRUE;
}

BOOLEAN
MmShutdownSystem (
    IN ULONG Phase
    )

/*++

Routine Description:

    This function performs the shutdown of memory management.  This
    is accomplished by writing out all modified pages which are
    destined for files other than the paging file.

Arguments:

    Phase - Supplies 0 on the initiation of shutdown.  All processes have
            already been killed, the registry shutdown and shutdown IRPs already
            sent.  On return from this phase all mapped file data must be
            flushed and the unused segment list emptied.  This releases all
            the Mm references to file objects, allowing many drivers (especially
            the network) to unload.

            Supplies 1 on the initiation of shutdown.  The filesystem stack
            has received its shutdown IRPs (the stack must free its paged pool
            allocations here and lock down any pagable code it intends to call)
            as no more references to pagable code or data are allowed on return.
            ie: Any IoPageRead at this point is illegal.
            Close the pagefile handles here so the filesystem stack will be
            dereferenced causing those drivers to unload as well.

            Supplies 2 on final shutdown of the system.  Any resources not
            freed by this point are treated as leaks and cause a bugcheck.

Return Value:

    TRUE if the pages were successfully written, FALSE otherwise.

--*/

{
    ULONG i;

    if (Phase == 0) {
        return MiShutdownSystem ();
    }

    if (Phase == 1) {

        //
        // The filesystem has shutdown.  References to pagable code or data
        // is no longer allowed at this point.
        //
        // Close the pagefile handles here so the filesystem stack will be
        // dereferenced causing those drivers to unload as well.
        //

        if (MmSystemShutdown < 2) {

            MmSystemShutdown = 2;

            if (PoCleanShutdownEnabled() & PO_CLEAN_SHUTDOWN_PAGING) {

                //
                // Make any IoPageRead at this point illegal.  Detect this by
                // purging all system pagable memory.
                //

                MmTrimAllSystemPagableMemory (TRUE);

                //
                // There should be no dirty pages destined for the filesystem.
                // Give the filesystem team a shot to debug this on checked
                // builds.
                //

                ASSERT (MmModifiedPageListHead.Total == MmTotalPagesForPagingFile);
                //
                // Dereference all the pagefile objects to trigger a cascading
                // unload of the storage stack as this should be the last
                // reference to their driver objects.
                //

                for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
                    ObDereferenceObject (MmPagingFile[i]->File);
                }
            }
        }
        return TRUE;
    }

    ASSERT (Phase == 2);

    //
    // Check for resource leaks and bugcheck if any are found.
    //

    if (MmSystemShutdown < 3) {
        MmSystemShutdown = 3;
        if (PoCleanShutdownEnabled ()) {
            MiReleaseAllMemory ();
        }
    }

    return TRUE;
}


VOID
MiReleaseAllMemory (
    VOID
    )

/*++

Routine Description:

    This function performs the final release of memory management allocations.

Arguments:

    None.

Return Value:

    None.

Environment:

    No references to paged pool or pagable code/data are allowed.

--*/

{
    ULONG i;
    PEVENT_COUNTER EventSupport;
    PUNLOADED_DRIVERS Entry;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLOAD_IMPORTS ImportList;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    PMMINPAGE_SUPPORT Support;
    PSINGLE_LIST_ENTRY SingleListEntry;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;
    PMMMOD_WRITER_MDL_ENTRY ModWriterEntry;

    ASSERT (MmUnusedSegmentList.Flink == &MmUnusedSegmentList);

    //
    // Don't clear free pages so problems can be debugged.
    //

    MiZeroingDisabled = TRUE;

    if (MiMirrorBitMap != NULL) {
        ExFreePool (MiMirrorBitMap);
        ASSERT (MiMirrorBitMap2);
        ExFreePool (MiMirrorBitMap2);
    }

    //
    // Free the unloaded driver list.
    //

    if (MmUnloadedDrivers != NULL) {
        Entry = &MmUnloadedDrivers[0];
        for (i = 0; i < MI_UNLOADED_DRIVERS; i += 1) {
            if (Entry->Name.Buffer != NULL) {
                RtlFreeUnicodeString (&Entry->Name);
            }
            Entry += 1;
        }
        ExFreePool (MmUnloadedDrivers);
    }

    NextEntry = MmLoadedUserImageList.Flink;
    while (NextEntry != &MmLoadedUserImageList) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        NextEntry = NextEntry->Flink;

        ExFreePool ((PVOID)DataTableEntry);
    }

    //
    // Release the loaded module list entries.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        ImportList = (PLOAD_IMPORTS)DataTableEntry->LoadedImports;

        if ((ImportList != (PVOID)LOADED_AT_BOOT) &&
            (ImportList != (PVOID)NO_IMPORTS_USED) &&
            (!SINGLE_ENTRY(ImportList))) {

                ExFreePool (ImportList);
        }

        if (DataTableEntry->FullDllName.Buffer != NULL) {
            ASSERT (DataTableEntry->FullDllName.Buffer == DataTableEntry->BaseDllName.Buffer);
        }

        NextEntry = NextEntry->Flink;

        ExFreePool ((PVOID)DataTableEntry);
    }

    //
    // Free the physical memory descriptor block.
    //

    ExFreePool (MmPhysicalMemoryBlock);

    //
    // Free the system views structure.
    //

    if (MmSession.SystemSpaceViewTable != NULL) {
        ExFreePool (MmSession.SystemSpaceViewTable);
    }

    if (MmSession.SystemSpaceBitMap != NULL) {
        ExFreePool (MmSession.SystemSpaceBitMap);
    }

    //
    // Free the pagefile structures - note the PageFileName buffer was freed
    // earlier as it resided in paged pool and may have needed an inpage
    // to be freed.
    //

    for (i = 0; i < MmNumberOfPagingFiles; i += 1) {
        ASSERT (MmPagingFile[i]->PageFileName.Buffer == NULL);
        ExFreePool (MmPagingFile[i]->Entry[0]);
        ExFreePool (MmPagingFile[i]->Entry[1]);
        ExFreePool (MmPagingFile[i]->Bitmap);
        ExFreePool (MmPagingFile[i]);
    }

    ASSERT (MmNumberOfMappedMdlsInUse == 0);

    i = 0;
    while (IsListEmpty (&MmMappedFileHeader.ListHead) != 0) {

        ModWriterEntry = (PMMMOD_WRITER_MDL_ENTRY)RemoveHeadList (
                                    &MmMappedFileHeader.ListHead);

        ExFreePool (ModWriterEntry);
        i += 1;
    }
    ASSERT (i == MmNumberOfMappedMdls);

    //
    // Free the paged pool bitmaps.
    //

    ExFreePool (MmPagedPoolInfo.PagedPoolAllocationMap);
    ExFreePool (MmPagedPoolInfo.EndOfPagedPoolBitmap);

    if (VerifierLargePagedPoolMap != NULL) {
        ExFreePool (VerifierLargePagedPoolMap);
    }

    //
    // Free the inpage structures.
    //

    while (ExQueryDepthSList (&MmInPageSupportSListHead) != 0) {

        SingleListEntry = InterlockedPopEntrySList (&MmInPageSupportSListHead);

        if (SingleListEntry != NULL) {
            Support = CONTAINING_RECORD (SingleListEntry,
                                         MMINPAGE_SUPPORT,
                                         ListEntry);

            ASSERT (Support->u1.e1.PrefetchMdlHighBits == 0);
            ExFreePool (Support);
        }
    }

    while (ExQueryDepthSList (&MmEventCountSListHead) != 0) {

        EventSupport = (PEVENT_COUNTER) InterlockedPopEntrySList (&MmEventCountSListHead);

        if (EventSupport != NULL) {
            ExFreePool (EventSupport);
        }
    }

    //
    // Free the verifier list last because it must be consulted to debug
    // any bugchecks.
    //

    NextEntry = MiVerifierDriverAddedThunkListHead.Flink;
    if (NextEntry != NULL) {
        while (NextEntry != &MiVerifierDriverAddedThunkListHead) {

            ThunkTableBase = CONTAINING_RECORD (NextEntry,
                                                DRIVER_SPECIFIED_VERIFIER_THUNKS,
                                                ListEntry );

            NextEntry = NextEntry->Flink;
            ExFreePool (ThunkTableBase);
        }
    }

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        Verifier = CONTAINING_RECORD(NextEntry,
                                     MI_VERIFIER_DRIVER_ENTRY,
                                     Links);

        NextEntry = NextEntry->Flink;
        ExFreePool (Verifier);
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\specpool.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   specpool.c

Abstract:

    This module contains the routines which allocate and deallocate
    pages from special pool.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifndef NO_POOL_CHECKS
VOID
MiInitializeSpecialPoolCriteria (
    IN VOID
    );

VOID
MiSpecialPoolTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    );
#endif

LOGICAL
MmSetSpecialPool (
    IN LOGICAL Enable
    );

PVOID
MiAllocateSpecialPool (
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN POOL_TYPE PoolType,
    IN ULONG SpecialPoolType
    );

VOID
MmFreeSpecialPool (
    IN PVOID P
    );

LOGICAL
MiProtectSpecialPool (
    IN PVOID VirtualAddress,
    IN ULONG NewProtect
    );

VOID
MiMakeSpecialPoolPagable (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN POOL_TYPE PoolType
    );

LOGICAL
MiExpandSpecialPool (
    IN POOL_TYPE PoolType,
    IN KIRQL OldIrql
    );

#ifdef ALLOC_PRAGMA
#ifndef NO_POOL_CHECKS
#pragma alloc_text(INIT, MiInitializeSpecialPoolCriteria)
#pragma alloc_text(PAGE, MiEnableRandomSpecialPool)
#endif
#if defined (_WIN64)
#pragma alloc_text(PAGESPEC, MiDeleteSessionSpecialPool)
#pragma alloc_text(PAGE, MiInitializeSpecialPool)
#else
#pragma alloc_text(INIT, MiInitializeSpecialPool)
#endif
#pragma alloc_text(PAGESPEC, MiExpandSpecialPool)
#pragma alloc_text(PAGESPEC, MmFreeSpecialPool)
#pragma alloc_text(PAGESPEC, MiAllocateSpecialPool)
#pragma alloc_text(PAGESPEC, MiMakeSpecialPoolPagable)
#pragma alloc_text(PAGESPEC, MiProtectSpecialPool)
#endif

ULONG MmSpecialPoolTag;
PVOID MmSpecialPoolStart;
PVOID MmSpecialPoolEnd;

#if defined (_WIN64)
PVOID MmSessionSpecialPoolStart;
PVOID MmSessionSpecialPoolEnd;
#else
PMMPTE MiSpecialPoolExtra;
ULONG MiSpecialPoolExtraCount;
#endif

ULONG MmSpecialPoolRejected[6];
LOGICAL MmSpecialPoolCatchOverruns = TRUE;


PMMPTE MiSpecialPoolFirstPte;
PMMPTE MiSpecialPoolLastPte;

ULONG MiSpecialPagesNonPaged;
ULONG MiSpecialPagesPagable;
ULONG MmSpecialPagesInUse;      // Used by the debugger

ULONG MiSpecialPagesNonPagedPeak;
ULONG MiSpecialPagesPagablePeak;
ULONG MiSpecialPagesInUsePeak;

ULONG MiSpecialPagesNonPagedMaximum;

LOGICAL MiSpecialPoolEnabled = TRUE;

extern LOGICAL MmPagedPoolMaximumDesired;

extern ULONG MmPteFailures[MaximumPtePoolTypes];

#if defined (_X86_)
extern ULONG MiExtraPtes1;
KSPIN_LOCK MiSpecialPoolLock;
#endif

#if !defined (_WIN64)
LOGICAL
MiInitializeSpecialPool (
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    This routine initializes the special pool used to catch pool corruptors.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no locks held.

--*/

{
    ULONG i;
    PMMPTE PointerPte;
    PMMPTE PointerPteBase;
    ULONG SpecialPoolPtes;

    UNREFERENCED_PARAMETER (PoolType);

    if ((MmVerifyDriverBufferLength == (ULONG)-1) &&
        ((MmSpecialPoolTag == 0) || (MmSpecialPoolTag == (ULONG)-1))) {
            return FALSE;
    }

    //
    // Even though we asked for some number of system PTEs to map special pool,
    // we may not have been given them all.  Large memory systems are
    // autoconfigured so that a large nonpaged pool is the default.
    // x86 systems booted with the 3GB switch don't have enough
    // contiguous virtual address space to support this, so our request may
    // have been trimmed.  Handle that intelligently here so we don't exhaust
    // the system PTE pool and fail to handle thread stacks and I/O.
    //

    if (MmNumberOfSystemPtes < 0x3000) {
        SpecialPoolPtes = MmNumberOfSystemPtes / 6;
    }
    else {
        SpecialPoolPtes = MmNumberOfSystemPtes / 3;
    }

    //
    // 32-bit systems are very cramped on virtual address space.  Apply
    // a cap here to prevent overzealousness.
    //

    if (SpecialPoolPtes > MM_SPECIAL_POOL_PTES) {
        SpecialPoolPtes = MM_SPECIAL_POOL_PTES;
    }

    SpecialPoolPtes = MI_ROUND_TO_SIZE (SpecialPoolPtes, PTE_PER_PAGE);

#if defined (_X86_)

    //
    // For x86, we can actually use an additional range of special PTEs to
    // map memory with and so we can raise the limit from 25000 to approximately
    // 256000.
    //

    if ((MiExtraPtes1 != 0) &&
        (ExpMultiUserTS == FALSE) &&
        (MiRequestedSystemPtes != (ULONG)-1)) {

        if (MmPagedPoolMaximumDesired == TRUE) {

            //
            // The low PTEs between 2 and 3GB virtual must be used
            // for both regular system PTE usage and special pool usage.
            //

            SpecialPoolPtes = (MiNumberOfExtraSystemPdes / 2) * PTE_PER_PAGE;
        }
        else {

            //
            // The low PTEs between 2 and 3GB virtual can be used
            // exclusively for special pool.
            //

            SpecialPoolPtes = MiNumberOfExtraSystemPdes * PTE_PER_PAGE;
        }
    }

    KeInitializeSpinLock (&MiSpecialPoolLock);
#endif

    //
    // A PTE disappears for double mapping the system page directory.
    // When guard paging for system PTEs is enabled, a few more go also.
    // Thus, not being able to get all the PTEs we wanted is not fatal and
    // we just back off a bit and retry.
    //

    //
    // Always request an even number of PTEs so each one can be guard paged.
    //

    ASSERT ((SpecialPoolPtes & (PTE_PER_PAGE - 1)) == 0);

    do {

        PointerPte = MiReserveAlignedSystemPtes (SpecialPoolPtes,
                                                 SystemPteSpace,
                                                 MM_VA_MAPPED_BY_PDE);

        if (PointerPte != NULL) {
            break;
        }

        ASSERT (SpecialPoolPtes >= PTE_PER_PAGE);

        SpecialPoolPtes -= PTE_PER_PAGE;

    } while (SpecialPoolPtes != 0);

    //
    // We deliberately try to get a huge number of system PTEs.  Don't let
    // any of these count as a real failure in our debugging counters.
    //

    MmPteFailures[SystemPteSpace] = 0;

    if (SpecialPoolPtes == 0) {
        return FALSE;
    }

    ASSERT (SpecialPoolPtes >= PTE_PER_PAGE);

    //
    // Build the list of PTE pairs using only the first page table page for
    // now.  Keep the other PTEs in reserve so they can be returned to the
    // PTE pool in case some driver wants a huge amount.
    //

    PointerPteBase = PointerPte;

    MmSpecialPoolStart = MiGetVirtualAddressMappedByPte (PointerPte);
    ASSERT (MiIsVirtualAddressOnPdeBoundary (MmSpecialPoolStart));

    for (i = 0; i < PTE_PER_PAGE; i += 2) {
        PointerPte->u.List.NextEntry = ((PointerPte + 2) - MmSystemPteBase);
        PointerPte += 2;
    }

    MiSpecialPoolExtra = PointerPte;
    MiSpecialPoolExtraCount = SpecialPoolPtes - PTE_PER_PAGE;

    PointerPte -= 2;
    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

    MmSpecialPoolEnd = MiGetVirtualAddressMappedByPte (PointerPte + 1);

    MiSpecialPoolLastPte = PointerPte;
    MiSpecialPoolFirstPte = PointerPteBase;

    //
    // Limit nonpaged special pool based on the memory size.
    //

    MiSpecialPagesNonPagedMaximum = (ULONG)(MmResidentAvailablePages >> 4);

    if (MmNumberOfPhysicalPages > 0x3FFF) {
        MiSpecialPagesNonPagedMaximum = (ULONG)(MmResidentAvailablePages >> 3);
    }

    ExSetPoolFlags (EX_SPECIAL_POOL_ENABLED);

    return TRUE;
}

#else

PMMPTE MiSpecialPoolNextPdeForSpecialPoolExpansion;
PMMPTE MiSpecialPoolLastPdeForSpecialPoolExpansion;

LOGICAL
MiInitializeSpecialPool (
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    This routine initializes special pool used to catch pool corruptors.
    Only NT64 systems have sufficient virtual address space to make use of this.

Arguments:

    PoolType - Supplies the pool type (system global or session) being
               initialized.

Return Value:

    TRUE if the requested special pool was initialized, FALSE if not.

Environment:

    Kernel mode, no locks held.

--*/

{
    PVOID BaseAddress;
    PVOID EndAddress;
    KIRQL OldIrql;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE EndPpe;
    PMMPTE EndPde;
    LOGICAL SpecialPoolCreated;
    SIZE_T AdditionalCommittedPages;
    PFN_NUMBER PageFrameIndex;

    PAGED_CODE ();

    if (PoolType & SESSION_POOL_MASK) {
        ASSERT (MmSessionSpace->SpecialPoolFirstPte == NULL);
        if (MmSessionSpecialPoolStart == 0) {
            return FALSE;
        }
        BaseAddress = MmSessionSpecialPoolStart;
        ASSERT (((ULONG_PTR)BaseAddress & (MM_VA_MAPPED_BY_PDE - 1)) == 0);
        EndAddress = (PVOID)((ULONG_PTR)MmSessionSpecialPoolEnd - 1);
    }
    else {
        if (MmSpecialPoolStart == 0) {
            return FALSE;
        }
        BaseAddress = MmSpecialPoolStart;
        ASSERT (((ULONG_PTR)BaseAddress & (MM_VA_MAPPED_BY_PDE - 1)) == 0);
        EndAddress = (PVOID)((ULONG_PTR)MmSpecialPoolEnd - 1);

        //
        // Construct empty page directory parent mappings as needed.
        //

        PointerPpe = MiGetPpeAddress (BaseAddress);
        EndPpe = MiGetPpeAddress (EndAddress);
        TempPte = ValidKernelPde;
        AdditionalCommittedPages = 0;

        LOCK_PFN (OldIrql);

        while (PointerPpe <= EndPpe) {
            if (PointerPpe->u.Long == 0) {
                PageFrameIndex = MiRemoveZeroPage (
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPpe, TempPte);

                MiInitializePfn (PageFrameIndex, PointerPpe, 1);

                MmResidentAvailablePages -= 1;
                AdditionalCommittedPages += 1;
            }
            PointerPpe += 1;
        }
        UNLOCK_PFN (OldIrql);
        InterlockedExchangeAddSizeT (&MmTotalCommittedPages,
                                     AdditionalCommittedPages);
    }

    //
    // Build just one page table page for session special pool - the rest
    // are built on demand.
    //

    ASSERT (MiGetPpeAddress(BaseAddress)->u.Hard.Valid == 1);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerPde = MiGetPdeAddress (BaseAddress);
    EndPde = MiGetPdeAddress (EndAddress);

#if DBG

    //
    // The special pool address range better be unused.
    //

    while (PointerPde <= EndPde) {
        ASSERT (PointerPde->u.Long == 0);
        PointerPde += 1;
    }
    PointerPde = MiGetPdeAddress (BaseAddress);
#endif

    if (PoolType & SESSION_POOL_MASK) {
        MmSessionSpace->NextPdeForSpecialPoolExpansion = PointerPde;
        MmSessionSpace->LastPdeForSpecialPoolExpansion = EndPde;
    }
    else {
        MiSpecialPoolNextPdeForSpecialPoolExpansion = PointerPde;
        MiSpecialPoolLastPdeForSpecialPoolExpansion = EndPde;

        //
        // Cap nonpaged special pool based on the memory size.
        //

        MiSpecialPagesNonPagedMaximum = (ULONG)(MmResidentAvailablePages >> 4);

        if (MmNumberOfPhysicalPages > 0x3FFF) {
            MiSpecialPagesNonPagedMaximum = (ULONG)(MmResidentAvailablePages >> 3);
        }
    }

    LOCK_PFN (OldIrql);

    SpecialPoolCreated = MiExpandSpecialPool (PoolType, OldIrql);

    UNLOCK_PFN (OldIrql);

    return SpecialPoolCreated;
}

VOID
MiDeleteSessionSpecialPool (
    VOID
    )

/*++

Routine Description:

    This routine deletes the session special pool range used to catch
    pool corruptors.  Only NT64 systems have the extra virtual address
    space in the session to make use of this.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, no locks held.

--*/

{
    PVOID BaseAddress;
    PVOID EndAddress;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE StartPde;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTablePages;
    PMMPTE EndPde;
#if DBG
    PMMPTE StartPte;
    PMMPTE EndPte;
#endif

    PAGED_CODE ();

    //
    // If the initial creation of this session's special pool failed, then
    // there's nothing to delete.
    //

    if (MmSessionSpace->SpecialPoolFirstPte == NULL) {
        return;
    }

    if (MmSessionSpace->SpecialPagesInUse != 0) {
        KeBugCheckEx (SESSION_HAS_VALID_SPECIAL_POOL_ON_EXIT,
                      (ULONG_PTR)MmSessionSpace->SessionId,
                      MmSessionSpace->SpecialPagesInUse,
                      0,
                      0);
    }

    //
    // Special pool page table pages are expanded such that all PDEs after the
    // first blank one must also be blank.
    //

    BaseAddress = MmSessionSpecialPoolStart;
    EndAddress = (PVOID)((ULONG_PTR)MmSessionSpecialPoolEnd - 1);

    ASSERT (((ULONG_PTR)BaseAddress & (MM_VA_MAPPED_BY_PDE - 1)) == 0);
    ASSERT (MiGetPpeAddress(BaseAddress)->u.Hard.Valid == 1);
    ASSERT (MiGetPdeAddress(BaseAddress)->u.Hard.Valid == 1);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerPde = MiGetPdeAddress (BaseAddress);
    EndPde = MiGetPdeAddress (EndAddress);
    StartPde = PointerPde;

    //
    // No need to flush the TB below as the entire TB will be flushed
    // on return when the rest of the session space is destroyed.
    //

    while (PointerPde <= EndPde) {
        if (PointerPde->u.Long == 0) {
            break;
        }

#if DBG
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        StartPte = PointerPte;
        EndPte = PointerPte + PTE_PER_PAGE;

        while (PointerPte < EndPte) {
            ASSERT ((PointerPte + 1)->u.Long == 0);
            PointerPte += 2;
        }
#endif

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
        MiSessionPageTableRelease (PageFrameIndex);
        *PointerPde = ZeroKernelPte;

        PointerPde += 1;
    }

    PageTablePages = PointerPde - StartPde;

#if DBG

    //
    // The remaining session special pool address range better be unused.
    //

    while (PointerPde <= EndPde) {
        ASSERT (PointerPde->u.Long == 0);
        PointerPde += 1;
    }
#endif

    MiReturnCommitment (PageTablePages);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES, 0 - PageTablePages);

    MM_BUMP_COUNTER(42, 0 - PageTablePages);
    MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC,
                         (ULONG)(0 - PageTablePages));

    LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
    MmSessionSpace->NonPagablePages -= PageTablePages;
    UNLOCK_SESSION_SPACE_WS (OldIrql);

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 0 - PageTablePages);

    MmSessionSpace->SpecialPoolFirstPte = NULL;
}
#endif

#if defined (_X86_)
LOGICAL
MiRecoverSpecialPtes (
    IN ULONG NumberOfPtes
    )
{
    KIRQL OldIrql;
    PMMPTE PointerPte;

    if (MiSpecialPoolExtraCount == 0) {
        return FALSE;
    }

    //
    // Round the requested number of PTEs up to a full page table multiple.
    //

    NumberOfPtes = MI_ROUND_TO_SIZE (NumberOfPtes, PTE_PER_PAGE);

    //
    // If the caller needs more than we have, then do nothing and return FALSE.
    //

    ExAcquireSpinLock (&MiSpecialPoolLock, &OldIrql);

    if (NumberOfPtes > MiSpecialPoolExtraCount) {
        ExReleaseSpinLock (&MiSpecialPoolLock, OldIrql);
        return FALSE;
    }

    //
    // Return the tail end of the extra reserve.
    //

    MiSpecialPoolExtraCount -= NumberOfPtes;

    PointerPte = MiSpecialPoolExtra + MiSpecialPoolExtraCount;

    ExReleaseSpinLock (&MiSpecialPoolLock, OldIrql);

    MiReleaseSplitSystemPtes (PointerPte, NumberOfPtes, SystemPteSpace);

    return TRUE;
}
#endif


LOGICAL
MiExpandSpecialPool (
    IN POOL_TYPE PoolType,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine attempts to allocate another page table page for the
    requested special pool.

Arguments:

    PoolType - Supplies the special pool type being expanded.

    OldIrql - Supplies the previous irql the PFN lock was acquired at.

Return Value:

    TRUE if expansion occurred, FALSE if not.

Environment:

    Kernel mode, PFN lock held.  The PFN lock may released and reacquired.

--*/

{
#if defined (_WIN64)

    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER PageFrameIndex;
    NTSTATUS Status;
    PMMPTE SpecialPoolFirstPte;
    PMMPTE SpecialPoolLastPte;
    PMMPTE *NextPde;
    PMMPTE *LastPde;
    PMMPTE PteBase;
    PFN_NUMBER ContainingFrame;
    LOGICAL SessionAllocation;
    PMMPTE *SpecialPoolFirstPteGlobal;
    PMMPTE *SpecialPoolLastPteGlobal;

    if (PoolType & SESSION_POOL_MASK) {
        NextPde = &MmSessionSpace->NextPdeForSpecialPoolExpansion;
        LastPde = &MmSessionSpace->LastPdeForSpecialPoolExpansion;
        PteBase = MI_PTE_BASE_FOR_LOWEST_SESSION_ADDRESS;
        ContainingFrame = MmSessionSpace->SessionPageDirectoryIndex;
        SessionAllocation = TRUE;
        SpecialPoolFirstPteGlobal = &MmSessionSpace->SpecialPoolFirstPte;
        SpecialPoolLastPteGlobal = &MmSessionSpace->SpecialPoolLastPte;
    }
    else {
        NextPde = &MiSpecialPoolNextPdeForSpecialPoolExpansion;
        LastPde = &MiSpecialPoolLastPdeForSpecialPoolExpansion;
        PteBase = MmSystemPteBase;
        ContainingFrame = 0;
        SessionAllocation = FALSE;
        SpecialPoolFirstPteGlobal = &MiSpecialPoolFirstPte;
        SpecialPoolLastPteGlobal = &MiSpecialPoolLastPte;
    }

    PointerPde = *NextPde;

    if (PointerPde > *LastPde) {
        return FALSE;
    }

    UNLOCK_PFN2 (OldIrql);

    //
    // Acquire a page and initialize it.  If no one else has done this in
    // the interim, then insert it into the list.
    //
    // Note that CantExpand commitment charging must be used because this
    // path can get called in the idle thread context while processing DPCs
    // and the normal commitment charging may queue a pagefile extension using
    // an event on the local stack which is illegal.
    //

    if (MiChargeCommitmentCantExpand (1, FALSE) == FALSE) {
        if (PoolType & SESSION_POOL_MASK) {
            MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);
        }
        LOCK_PFN2 (OldIrql);
        return FALSE;
    }

    if ((PoolType & SESSION_POOL_MASK) == 0) {
        ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPde));
    }

    Status = MiInitializeAndChargePfn (&PageFrameIndex,
                                       PointerPde,
                                       ContainingFrame,
                                       SessionAllocation);

    if (!NT_SUCCESS(Status)) {
        MiReturnCommitment (1);
        LOCK_PFN2 (OldIrql);

        //
        // Don't retry even if STATUS_RETRY is returned above because if we
        // preempted the thread that allocated the PDE before he gets a
        // chance to update the PTE chain, we can loop forever.
        //

        return FALSE;
    }

    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

    KeFillEntryTb ((PHARDWARE_PTE) PointerPde, PointerPte, FALSE);

    MM_BUMP_COUNTER(42, 1);

    if (PoolType & SESSION_POOL_MASK) {
        MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES, 1);
        MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC, 1);
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_POOL_CREATE, 1);

        LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
        MmSessionSpace->NonPagablePages += 1;
        UNLOCK_SESSION_SPACE_WS (OldIrql);

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, 1);
    }
    else {
        MM_TRACK_COMMIT (MM_DBG_COMMIT_SPECIAL_POOL_MAPPING_PAGES, 1);
    }

    //
    // Build the list of PTE pairs.
    //

    SpecialPoolFirstPte = PointerPte;

    SpecialPoolLastPte = PointerPte + PTE_PER_PAGE;

    while (PointerPte < SpecialPoolLastPte) {
        PointerPte->u.List.NextEntry = (PointerPte + 2 - PteBase);
        (PointerPte + 1)->u.Long = 0;
        PointerPte += 2;
    }
    PointerPte -= 2;
    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

    ASSERT (PointerPde == *NextPde);
    ASSERT (PointerPde <= *LastPde);

    //
    // Insert the new page table page into the head of the current list (if
    // one exists) so it gets used first.
    //

    if (*SpecialPoolFirstPteGlobal == NULL) {

        //
        // This is the initial creation.
        //

        *SpecialPoolFirstPteGlobal = SpecialPoolFirstPte;
        *SpecialPoolLastPteGlobal = PointerPte;

        ExSetPoolFlags (EX_SPECIAL_POOL_ENABLED);
        LOCK_PFN2 (OldIrql);
    }
    else {

        //
        // This is actually an expansion.
        //

        LOCK_PFN2 (OldIrql);

        PointerPte->u.List.NextEntry = *SpecialPoolFirstPteGlobal - PteBase;

        *SpecialPoolFirstPteGlobal = SpecialPoolFirstPte;
    }
            
    ASSERT ((*SpecialPoolLastPteGlobal)->u.List.NextEntry == MM_EMPTY_PTE_LIST);

    *NextPde = *NextPde + 1;

#else

    ULONG i;
    PMMPTE PointerPte;

    UNREFERENCED_PARAMETER (PoolType);

    if (MiSpecialPoolExtraCount == 0) {
        return FALSE;
    }

    ExAcquireSpinLock (&MiSpecialPoolLock, &OldIrql);

    if (MiSpecialPoolExtraCount == 0) {
        ExReleaseSpinLock (&MiSpecialPoolLock, OldIrql);
        return FALSE;
    }

    ASSERT (MiSpecialPoolExtraCount >= PTE_PER_PAGE);

    PointerPte = MiSpecialPoolExtra;

    for (i = 0; i < PTE_PER_PAGE - 2; i += 2) {
        PointerPte->u.List.NextEntry = ((PointerPte + 2) - MmSystemPteBase);
        PointerPte += 2;
    }

    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

    MmSpecialPoolEnd = MiGetVirtualAddressMappedByPte (PointerPte + 1);

    MiSpecialPoolLastPte = PointerPte;
    MiSpecialPoolFirstPte = MiSpecialPoolExtra;

    MiSpecialPoolExtraCount -= PTE_PER_PAGE;
    MiSpecialPoolExtra += PTE_PER_PAGE;

    ExReleaseSpinLock (&MiSpecialPoolLock, OldIrql);

#endif

    return TRUE;
}
PVOID
MmAllocateSpecialPool (
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN POOL_TYPE PoolType,
    IN ULONG SpecialPoolType
    )

/*++

Routine Description:

    This routine allocates virtual memory from special pool.  This allocation
    is made from the end of a physical page with the next PTE set to no access
    so that any reads or writes will cause an immediate fatal system crash.
    
    This lets us catch components that corrupt pool.

Arguments:

    NumberOfBytes - Supplies the number of bytes to commit.

    Tag - Supplies the tag of the requested allocation.

    PoolType - Supplies the pool type of the requested allocation.

    SpecialPoolType - Supplies the special pool type of the
                      requested allocation.

                      - 0 indicates overruns.
                      - 1 indicates underruns.
                      - 2 indicates use the systemwide pool policy.

Return Value:

    A non-NULL pointer if the requested allocation was fulfilled from special
    pool.  NULL if the allocation was not made.

Environment:

    Kernel mode, no pool locks held.

    Note this is a nonpagable wrapper so that machines without special pool
    can still support drivers allocating nonpaged pool at DISPATCH_LEVEL
    requesting special pool.

--*/

{
    if (MiSpecialPoolFirstPte == NULL) {

        //
        // The special pool allocation code was never initialized.
        //

        return NULL;
    }

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        if (MmSessionSpace->SpecialPoolFirstPte == NULL) {

            //
            // The special pool allocation code was never initialized.
            //

            return NULL;
        }
    }
#endif

    return MiAllocateSpecialPool (NumberOfBytes,
                                  Tag,
                                  PoolType,
                                  SpecialPoolType);
}

PVOID
MiAllocateSpecialPool (
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN POOL_TYPE PoolType,
    IN ULONG SpecialPoolType
    )

/*++

Routine Description:

    This routine allocates virtual memory from special pool.  This allocation
    is made from the end of a physical page with the next PTE set to no access
    so that any reads or writes will cause an immediate fatal system crash.
    
    This lets us catch components that corrupt pool.

Arguments:

    NumberOfBytes - Supplies the number of bytes to commit.

    Tag - Supplies the tag of the requested allocation.

    PoolType - Supplies the pool type of the requested allocation.

    SpecialPoolType - Supplies the special pool type of the
                      requested allocation.

                      - 0 indicates overruns.
                      - 1 indicates underruns.
                      - 2 indicates use the systemwide pool policy.

Return Value:

    A non-NULL pointer if the requested allocation was fulfilled from special
    pool.  NULL if the allocation was not made.

Environment:

    Kernel mode, no locks (not even pool locks) held.

--*/

{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPTE PointerPte;
    KIRQL OldIrql;
    PVOID Entry;
    PPOOL_HEADER Header;
    LARGE_INTEGER CurrentTime;
    LOGICAL CatchOverruns;
    PMMPTE SpecialPoolFirstPte;

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {

        if (KeGetCurrentIrql() > APC_LEVEL) {

            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          KeGetCurrentIrql(),
                          PoolType,
                          NumberOfBytes,
                          0x30);
        }
    }
    else {
        if (KeGetCurrentIrql() > DISPATCH_LEVEL) {

            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          KeGetCurrentIrql(),
                          PoolType,
                          NumberOfBytes,
                          0x30);
        }
    }

#if !defined (_WIN64) && !defined (_X86PAE_)

    if ((MiExtraPtes1 != 0) || (MiUseMaximumSystemSpace != 0)) {

        extern const ULONG MMSECT;

        //
        // Prototype PTEs cannot come from lower special pool because
        // their address is encoded into PTEs and the encoding only covers
        // a max of 1GB from the start of paged pool.  Likewise fork
        // prototype PTEs.
        //

        if (Tag == MMSECT || Tag == 'lCmM') {
            return NULL;
        }
    }

    if (Tag == 'bSmM' || Tag == 'iCmM' || Tag == 'aCmM' || Tag == 'dSmM' || Tag == 'cSmM') {

        //
        // Mm subsections cannot come from this special pool because they
        // get encoded into PTEs - they must come from normal nonpaged pool.
        //

        return NULL;
    }

#endif

    if (MiChargeCommitmentCantExpand (1, FALSE) == FALSE) {
        MmSpecialPoolRejected[5] += 1;
        return NULL;
    }

    TempPte = ValidKernelPte;
    MI_SET_PTE_DIRTY (TempPte);

    LOCK_PFN2 (OldIrql);

restart:

    if (MiSpecialPoolEnabled == FALSE) {

        //
        // The special pool allocation code is currently disabled.
        //

        UNLOCK_PFN2 (OldIrql);
        MiReturnCommitment (1);
        return NULL;
    }

    if (MmAvailablePages < 200) {
        UNLOCK_PFN2 (OldIrql);
        MmSpecialPoolRejected[0] += 1;
        MiReturnCommitment (1);
        return NULL;
    }

    //
    // Don't get too aggressive until a paging file gets set up.
    //

    if (MmNumberOfPagingFiles == 0 && MmSpecialPagesInUse > MmAvailablePages / 2) {
        UNLOCK_PFN2 (OldIrql);
        MmSpecialPoolRejected[3] += 1;
        MiReturnCommitment (1);
        return NULL;
    }

    SpecialPoolFirstPte = MiSpecialPoolFirstPte;

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        SpecialPoolFirstPte = MmSessionSpace->SpecialPoolFirstPte;
    }
#endif

    if (SpecialPoolFirstPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

        //
        // Add another page table page (virtual address space and resources
        // permitting) and then restart the request.  The PFN lock may be
        // released and reacquired during this call.
        //

        if (MiExpandSpecialPool (PoolType, OldIrql) == TRUE) {
            goto restart;
        }

        UNLOCK_PFN2 (OldIrql);
        MmSpecialPoolRejected[2] += 1;
        MiReturnCommitment (1);
        return NULL;
    }

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() < 100) {
        UNLOCK_PFN2 (OldIrql);
        MmSpecialPoolRejected[4] += 1;
        MiReturnCommitment (1);
        return NULL;
    }

    //
    // Cap nonpaged allocations to prevent runaways.
    //

    if ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool) {

        if (MiSpecialPagesNonPaged > MiSpecialPagesNonPagedMaximum) {
            UNLOCK_PFN2 (OldIrql);
            MmSpecialPoolRejected[1] += 1;
            MiReturnCommitment (1);
            return NULL;
        }

        MmResidentAvailablePages -= 1;
        MM_BUMP_COUNTER(31, 1);

        MiSpecialPagesNonPaged += 1;
        if (MiSpecialPagesNonPaged > MiSpecialPagesNonPagedPeak) {
            MiSpecialPagesNonPagedPeak = MiSpecialPagesNonPaged;
        }
    }
    else {
        MiSpecialPagesPagable += 1;
        if (MiSpecialPagesPagable > MiSpecialPagesPagablePeak) {
            MiSpecialPagesPagablePeak = MiSpecialPagesPagable;
        }
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SPECIAL_POOL_PAGES, 1);

    PointerPte = SpecialPoolFirstPte;

    ASSERT (PointerPte->u.List.NextEntry != MM_EMPTY_PTE_LIST);

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {

        MmSessionSpace->SpecialPoolFirstPte = PointerPte->u.List.NextEntry +
                                    MI_PTE_BASE_FOR_LOWEST_SESSION_ADDRESS;
        MmSessionSpace->SpecialPagesInUse += 1;
    }
    else
#endif
    {

        MiSpecialPoolFirstPte = PointerPte->u.List.NextEntry + MmSystemPteBase;
    }

    PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));

    MmSpecialPagesInUse += 1;
    if (MmSpecialPagesInUse > MiSpecialPagesInUsePeak) {
        MiSpecialPagesInUsePeak = MmSpecialPagesInUse;
    }

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MI_WRITE_VALID_PTE (PointerPte, TempPte);
    MiInitializePfn (PageFrameIndex, PointerPte, 1);
    UNLOCK_PFN2 (OldIrql);

    //
    // Fill the page with a random pattern.
    //

    KeQueryTickCount(&CurrentTime);

    Entry = MiGetVirtualAddressMappedByPte (PointerPte);

    RtlFillMemory (Entry, PAGE_SIZE, (UCHAR) (CurrentTime.LowPart | 0x1));

    if (SpecialPoolType == 0) {
        CatchOverruns = TRUE;
    }
    else if (SpecialPoolType == 1) {
        CatchOverruns = FALSE;
    }
    else if (MmSpecialPoolCatchOverruns == TRUE) {
        CatchOverruns = TRUE;
    }
    else {
        CatchOverruns = FALSE;
    }

    if (CatchOverruns == TRUE) {
        Header = (PPOOL_HEADER) Entry;
        Entry = (PVOID)(((LONG_PTR)(((PCHAR)Entry + (PAGE_SIZE - NumberOfBytes)))) & ~((LONG_PTR)POOL_OVERHEAD - 1));
    }
    else {
        Header = (PPOOL_HEADER) ((PCHAR)Entry + PAGE_SIZE - POOL_OVERHEAD);
    }

    //
    // Zero the header and stash any information needed at release time.
    //

    RtlZeroMemory (Header, POOL_OVERHEAD);

    Header->Ulong1 = (ULONG)NumberOfBytes;

    ASSERT (NumberOfBytes <= PAGE_SIZE - POOL_OVERHEAD && PAGE_SIZE <= 32 * 1024);

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {
        Header->Ulong1 |= MI_SPECIAL_POOL_PAGABLE;
        MiMakeSpecialPoolPagable (Entry, PointerPte, PoolType);
        (PointerPte + 1)->u.Soft.PageFileHigh = MI_SPECIAL_POOL_PTE_PAGABLE;
    }
    else {
        (PointerPte + 1)->u.Soft.PageFileHigh = MI_SPECIAL_POOL_PTE_NONPAGABLE;
    }

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        Header->Ulong1 |= MI_SPECIAL_POOL_IN_SESSION;
    }
#endif

    if (PoolType & POOL_VERIFIER_MASK) {
        Header->Ulong1 |= MI_SPECIAL_POOL_VERIFIER;
    }

    Header->BlockSize = (UCHAR) (CurrentTime.LowPart | 0x1);
    Header->PoolTag = Tag;

    ASSERT ((Header->PoolType & POOL_QUOTA_MASK) == 0);

    return Entry;
}

#define SPECIAL_POOL_FREE_TRACE_LENGTH 16

typedef struct _SPECIAL_POOL_FREE_TRACE {

    PVOID StackTrace [SPECIAL_POOL_FREE_TRACE_LENGTH];

} SPECIAL_POOL_FREE_TRACE, *PSPECIAL_POOL_FREE_TRACE;

VOID
MmFreeSpecialPool (
    IN PVOID P
    )

/*++

Routine Description:

    This routine frees a special pool allocation.  The backing page is freed
    and the mapping virtual address is made no access (the next virtual
    address is already no access).

    The virtual address PTE pair is then placed into an LRU queue to provide
    maximum no-access (protection) life to catch components that access
    deallocated pool.

Arguments:

    VirtualAddress - Supplies the special pool virtual address to free.

Return Value:

    None.

Environment:

    Kernel mode, no locks (not even pool locks) held.

--*/

{
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    ULONG SlopBytes;
    ULONG NumberOfBytesCalculated;
    ULONG NumberOfBytesRequested;
    POOL_TYPE PoolType;
    MMPTE LocalNoAccessPte;
    PPOOL_HEADER Header;
    PUCHAR Slop;
    ULONG i;
    LOGICAL BufferAtPageEnd;
    PMI_FREED_SPECIAL_POOL AllocationBase;
    LARGE_INTEGER CurrentTime;
#if defined (_X86_)
    PULONG_PTR StackPointer;
#else
    ULONG Hash;
#endif

    PointerPte = MiGetPteAddress (P);
    PteContents = *PointerPte;

    //
    // Check the PTE now so we can give a more friendly bugcheck rather than
    // crashing below on a bad reference.
    //

    if (PteContents.u.Hard.Valid == 0) {
        if ((PteContents.u.Soft.Protection == 0) ||
            (PteContents.u.Soft.Protection == MM_NOACCESS)) {
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          (ULONG_PTR)P,
                          (ULONG_PTR)PteContents.u.Long,
                          0,
                          0x20);
        }
    }

    if (((ULONG_PTR)P & (PAGE_SIZE - 1))) {
        Header = PAGE_ALIGN (P);
        BufferAtPageEnd = TRUE;
    }
    else {
        Header = (PPOOL_HEADER)((PCHAR)PAGE_ALIGN (P) + PAGE_SIZE - POOL_OVERHEAD);
        BufferAtPageEnd = FALSE;
    }

    if (Header->Ulong1 & MI_SPECIAL_POOL_PAGABLE) {
        ASSERT ((PointerPte + 1)->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_PAGABLE);
        if (KeGetCurrentIrql() > APC_LEVEL) {
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          KeGetCurrentIrql(),
                          PagedPool,
                          (ULONG_PTR)P,
                          0x31);
        }
        PoolType = PagedPool;
    }
    else {
        ASSERT ((PointerPte + 1)->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_NONPAGABLE);
        if (KeGetCurrentIrql() > DISPATCH_LEVEL) {
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          KeGetCurrentIrql(),
                          NonPagedPool,
                          (ULONG_PTR)P,
                          0x31);
        }
        PoolType = NonPagedPool;
    }

#if defined (_WIN64)
    if (Header->Ulong1 & MI_SPECIAL_POOL_IN_SESSION) {
        PoolType |= SESSION_POOL_MASK;
    }
#endif

    NumberOfBytesRequested = (ULONG)(USHORT)(Header->Ulong1 & ~(MI_SPECIAL_POOL_PAGABLE | MI_SPECIAL_POOL_VERIFIER | MI_SPECIAL_POOL_IN_SESSION));

    //
    // We gave the caller pool-header aligned data, so account for
    // that when checking here.
    //

    if (BufferAtPageEnd == TRUE) {

        NumberOfBytesCalculated = PAGE_SIZE - BYTE_OFFSET(P);
    
        if (NumberOfBytesRequested > NumberOfBytesCalculated) {
    
            //
            // Seems like we didn't give the caller enough - this is an error.
            //
    
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          (ULONG_PTR)P,
                          NumberOfBytesRequested,
                          NumberOfBytesCalculated,
                          0x21);
        }
    
        if (NumberOfBytesRequested + POOL_OVERHEAD < NumberOfBytesCalculated) {
    
            //
            // Seems like we gave the caller too much - also an error.
            //
    
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          (ULONG_PTR)P,
                          NumberOfBytesRequested,
                          NumberOfBytesCalculated,
                          0x22);
        }

        //
        // Check the memory before the start of the caller's allocation.
        //
    
        Slop = (PUCHAR)(Header + 1);
        if (Header->Ulong1 & MI_SPECIAL_POOL_VERIFIER) {
            Slop += sizeof(MI_VERIFIER_POOL_HEADER);
        }

        for ( ; Slop < (PUCHAR)P; Slop += 1) {
    
            if (*Slop != Header->BlockSize) {
    
                KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                              (ULONG_PTR)P,
                              (ULONG_PTR)Slop,
                              Header->Ulong1,
                              0x23);
            }
        }
    }
    else {
        NumberOfBytesCalculated = 0;
    }

    //
    // Check the memory after the end of the caller's allocation.
    //

    Slop = (PUCHAR)P + NumberOfBytesRequested;

    SlopBytes = (ULONG)((PUCHAR)(PAGE_ALIGN(P)) + PAGE_SIZE - Slop);

    if (BufferAtPageEnd == FALSE) {
        SlopBytes -= POOL_OVERHEAD;
        if (Header->Ulong1 & MI_SPECIAL_POOL_VERIFIER) {
            SlopBytes -= sizeof(MI_VERIFIER_POOL_HEADER);
        }
    }

    for (i = 0; i < SlopBytes; i += 1) {

        if (*Slop != Header->BlockSize) {

            //
            // The caller wrote slop between the free alignment we gave and the
            // end of the page (this is not detectable from page protection).
            //
    
            KeBugCheckEx (SPECIAL_POOL_DETECTED_MEMORY_CORRUPTION,
                          (ULONG_PTR)P,
                          (ULONG_PTR)Slop,
                          Header->Ulong1,
                          0x24);
        }
        Slop += 1;
    }

    //
    // Note session pool is directly tracked by default already so there is
    // no need to notify the verifier for session special pool allocations.
    //

    if ((Header->Ulong1 & (MI_SPECIAL_POOL_VERIFIER | MI_SPECIAL_POOL_IN_SESSION)) == MI_SPECIAL_POOL_VERIFIER) {
        VerifierFreeTrackedPool (P,
                                 NumberOfBytesRequested,
                                 PoolType,
                                 TRUE);
    }

    AllocationBase = (PMI_FREED_SPECIAL_POOL)(PAGE_ALIGN (P));

    AllocationBase->Signature = MI_FREED_SPECIAL_POOL_SIGNATURE;

    KeQueryTickCount(&CurrentTime);
    AllocationBase->TickCount = CurrentTime.LowPart;

    AllocationBase->NumberOfBytesRequested = NumberOfBytesRequested;
    AllocationBase->Pagable = (ULONG)PoolType;
    AllocationBase->VirtualAddress = P;
    AllocationBase->Thread = PsGetCurrentThread ();

#if defined (_X86_)
    _asm {
        mov StackPointer, esp
    }

    AllocationBase->StackPointer = StackPointer;

    //
    // For now, don't get fancy with copying more than what's in the current
    // stack page.  To do so would require checking the thread stack limits,
    // DPC stack limits, etc.
    //

    AllocationBase->StackBytes = PAGE_SIZE - BYTE_OFFSET(StackPointer);

    if (AllocationBase->StackBytes != 0) {

        if (AllocationBase->StackBytes > MI_STACK_BYTES) {
            AllocationBase->StackBytes = MI_STACK_BYTES;
        }

        RtlCopyMemory (AllocationBase->StackData,
                       StackPointer,
                       AllocationBase->StackBytes);
    }
#else
    AllocationBase->StackPointer = NULL;
    AllocationBase->StackBytes = 0;

    RtlZeroMemory (AllocationBase->StackData, sizeof (SPECIAL_POOL_FREE_TRACE));

    RtlCaptureStackBackTrace (0,
                              SPECIAL_POOL_FREE_TRACE_LENGTH,
                              (PVOID *)AllocationBase->StackData,
                              &Hash);
#endif

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {
        LocalNoAccessPte.u.Long = MM_KERNEL_NOACCESS_PTE;
        MiDeleteSystemPagableVm (PointerPte,
                                 1,
                                 LocalNoAccessPte,
                                 (PoolType & SESSION_POOL_MASK) ? TRUE : FALSE,
                                 NULL);
        LOCK_PFN (OldIrql);
        MiSpecialPagesPagable -= 1;
    }
    else {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        LOCK_PFN2 (OldIrql);
        MiSpecialPagesNonPaged -= 1;
        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (PageFrameIndex);
        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);
        KeFlushSingleTb (PAGE_ALIGN(P),
                         TRUE,
                         TRUE,
                         (PHARDWARE_PTE)PointerPte,
                         ZeroKernelPte.u.Flush);
        MmResidentAvailablePages += 1;
        MM_BUMP_COUNTER(37, 1);
    }

    // 
    // Clear the adjacent PTE to support MmIsSpecialPoolAddressFree().
    // 

    (PointerPte + 1)->u.Long = 0;
    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        ASSERT (MmSessionSpace->SpecialPoolLastPte->u.List.NextEntry == MM_EMPTY_PTE_LIST);
        MmSessionSpace->SpecialPoolLastPte->u.List.NextEntry = PointerPte -
                                        MI_PTE_BASE_FOR_LOWEST_SESSION_ADDRESS;

        MmSessionSpace->SpecialPoolLastPte = PointerPte;
        MmSessionSpace->SpecialPagesInUse -= 1;
    }
    else
#endif
    {
        ASSERT (MiSpecialPoolLastPte->u.List.NextEntry == MM_EMPTY_PTE_LIST);
        MiSpecialPoolLastPte->u.List.NextEntry = PointerPte - MmSystemPteBase;
        MiSpecialPoolLastPte = PointerPte;
    }

    MmSpecialPagesInUse -= 1;

    UNLOCK_PFN2 (OldIrql);

    MiReturnCommitment (1);

    MM_TRACK_COMMIT_REDUCTION (MM_DBG_COMMIT_SPECIAL_POOL_PAGES, 1);

    return;
}

SIZE_T
MmQuerySpecialPoolBlockSize (
    IN PVOID P
    )

/*++

Routine Description:

    This routine returns the size of a special pool allocation.

Arguments:

    VirtualAddress - Supplies the special pool virtual address to query.

Return Value:

    The size in bytes of the allocation.

Environment:

    Kernel mode, APC_LEVEL or below for pagable addresses, DISPATCH_LEVEL or
    below for nonpaged addresses.

--*/

{
    PPOOL_HEADER Header;

#if defined (_WIN64)
    ASSERT (((P >= MmSessionSpecialPoolStart) && (P < MmSessionSpecialPoolEnd)) ||
            ((P >= MmSpecialPoolStart) && (P < MmSpecialPoolEnd)));
#else
    ASSERT ((P >= MmSpecialPoolStart) && (P < MmSpecialPoolEnd));
#endif


    if (((ULONG_PTR)P & (PAGE_SIZE - 1))) {
        Header = PAGE_ALIGN (P);
    }
    else {
        Header = (PPOOL_HEADER)((PCHAR)PAGE_ALIGN (P) + PAGE_SIZE - POOL_OVERHEAD);
    }

    return (SIZE_T)(Header->Ulong1 & ~(MI_SPECIAL_POOL_PAGABLE | MI_SPECIAL_POOL_VERIFIER | MI_SPECIAL_POOL_IN_SESSION));
}

VOID
MiMakeSpecialPoolPagable (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN POOL_TYPE PoolType
    )

/*++

Routine Description:

    Make a special pool allocation pagable.

Arguments:

    VirtualAddress - Supplies the faulting address.

    PointerPte - Supplies the PTE for the faulting address.

    PoolType - Supplies the pool type of the allocation.

Return Value:

    None.

Environment:

    Kernel mode, no locks (not even pool locks) held.

--*/

{
    PMMPFN Pfn1;
    MMPTE TempPte;
    KIRQL PreviousIrql;
    PFN_NUMBER PageFrameIndex;
    PMMSUPPORT VmSupport;
    PETHREAD CurrentThread;

    CurrentThread = PsGetCurrentThread ();

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        VmSupport = &MmSessionSpace->Vm;
        LOCK_SESSION_SPACE_WS (PreviousIrql, CurrentThread);
    }
    else
#endif
    {
        VmSupport = &MmSystemCacheWs;
        PoolType = PoolType;
        LOCK_SYSTEM_WS (PreviousIrql, CurrentThread);
    }

    //
    // As this page is now allocated, add it to the system working set to
    // make it pagable.
    //

    TempPte = *PointerPte;

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    ASSERT (Pfn1->u1.Event == 0);

    Pfn1->u1.Event = (PVOID) CurrentThread;

    MiAddValidPageToWorkingSet (VirtualAddress,
                                PointerPte,
                                Pfn1,
                                0);

    ASSERT (KeGetCurrentIrql() == APC_LEVEL);

    if (VmSupport->Flags.AllowWorkingSetAdjustment == MM_GROW_WSLE_HASH) {
        MiGrowWsleHash (VmSupport);
        VmSupport->Flags.AllowWorkingSetAdjustment = TRUE;
    }

#if defined (_WIN64)
    if (PoolType & SESSION_POOL_MASK) {
        UNLOCK_SESSION_SPACE_WS (PreviousIrql);
    }
    else
#endif
    {
        UNLOCK_SYSTEM_WS (PreviousIrql);
    }
}

LOGICAL
MmIsSpecialPoolAddress (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function returns TRUE if the argument address is in special pool.
    FALSE if not.

Arguments:

    VirtualAddress - Supplies the address in question.

Return Value:

    See above.

Environment:

    Kernel mode.

--*/

{
    if ((VirtualAddress >= MmSpecialPoolStart) &&
        (VirtualAddress < MmSpecialPoolEnd)) {
        return TRUE;
    }

#if defined (_WIN64)
    if ((VirtualAddress >= MmSessionSpecialPoolStart) &&
        (VirtualAddress < MmSessionSpecialPoolEnd)) {
        return TRUE;
    }
#endif

    return FALSE;
}

LOGICAL
MmIsSpecialPoolAddressFree (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function returns TRUE if a special pool address has been freed.
    FALSE is returned if it is inuse (ie: the caller overran).

Arguments:

    VirtualAddress - Supplies the special pool address in question.

Return Value:

    See above.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;

    //
    // Caller must check that the address in in special pool.
    //

    ASSERT (MmIsSpecialPoolAddress (VirtualAddress) == TRUE);

    PointerPte = MiGetPteAddress (VirtualAddress);

    //
    // Take advantage of the fact that adjacent PTEs have the paged/nonpaged
    // bits set when in use and these bits are cleared on free.  Note also
    // that freed pages get their PTEs chained together through PageFileHigh.
    //

    if ((PointerPte->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_PAGABLE) ||
        (PointerPte->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_NONPAGABLE)) {
            return FALSE;
    }

    return TRUE;
}

LOGICAL
MiIsSpecialPoolAddressNonPaged (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function returns TRUE if the special pool address is nonpaged,
    FALSE if not.

Arguments:

    VirtualAddress - Supplies the special pool address in question.

Return Value:

    See above.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;

    //
    // Caller must check that the address in in special pool.
    //

    ASSERT (MmIsSpecialPoolAddress (VirtualAddress) == TRUE);

    PointerPte = MiGetPteAddress (VirtualAddress);

    //
    // Take advantage of the fact that adjacent PTEs have the paged/nonpaged
    // bits set when in use and these bits are cleared on free.  Note also
    // that freed pages get their PTEs chained together through PageFileHigh.
    //

    if ((PointerPte + 1)->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_NONPAGABLE) {
        return TRUE;
    }

    return FALSE;
}

LOGICAL
MmProtectSpecialPool (
    IN PVOID VirtualAddress,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function protects a special pool allocation.

Arguments:

    VirtualAddress - Supplies the special pool address to protect.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    TRUE if the protection was successfully applied, FALSE if not.

Environment:

    Kernel mode, IRQL at APC_LEVEL or below for pagable pool, DISPATCH or
    below for nonpagable pool.

    Note that setting an allocation to NO_ACCESS implies that an accessible
    protection must be applied by the caller prior to this allocation being
    freed.

    Note this is a nonpagable wrapper so that machines without special pool
    can still support code attempting to protect special pool at
    DISPATCH_LEVEL.

--*/

{
    if (MiSpecialPoolFirstPte == NULL) {

        //
        // The special pool allocation code was never initialized.
        //

        return (ULONG)-1;
    }

    return MiProtectSpecialPool (VirtualAddress, NewProtect);
}

LOGICAL
MiProtectSpecialPool (
    IN PVOID VirtualAddress,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function protects a special pool allocation.

Arguments:

    VirtualAddress - Supplies the special pool address to protect.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    TRUE if the protection was successfully applied, FALSE if not.

Environment:

    Kernel mode, IRQL at APC_LEVEL or below for pagable pool, DISPATCH or
    below for nonpagable pool.

    Note that setting an allocation to NO_ACCESS implies that an accessible
    protection must be applied by the caller prior to this allocation being
    freed.

--*/

{
    KIRQL OldIrql;
    KIRQL OldIrql2;
    MMPTE PteContents;
    MMPTE NewPteContents;
    MMPTE PreviousPte;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    ULONG ProtectionMask;
    WSLE_NUMBER WsIndex;
    LOGICAL Pagable;
    LOGICAL SystemWsLocked;
    PMMSUPPORT VmSupport;

#if defined (_WIN64)
    if ((VirtualAddress >= MmSessionSpecialPoolStart) &&
        (VirtualAddress < MmSessionSpecialPoolEnd)) {
        VmSupport = &MmSessionSpace->Vm;
    }
    else
#endif
    if (VirtualAddress >= MmSpecialPoolStart && VirtualAddress < MmSpecialPoolEnd)
    {
        VmSupport = &MmSystemCacheWs;
    }
#if defined (_PROTECT_PAGED_POOL)
    else if ((VirtualAddress >= MmPagedPoolStart) &&
             (VirtualAddress < PagedPoolEnd)) {

        VmSupport = &MmSystemCacheWs;
    }
#endif
    else {
        return (ULONG)-1;
    }

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return (ULONG)-1;
    }

    SystemWsLocked = FALSE;

    PointerPte = MiGetPteAddress (VirtualAddress);

    //
    // Initializing OldIrql is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    OldIrql = PASSIVE_LEVEL;

#if defined (_PROTECT_PAGED_POOL)
    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress < PagedPoolEnd)) {
        Pagable = TRUE;
    }
    else
#endif
    if ((PointerPte + 1)->u.Soft.PageFileHigh == MI_SPECIAL_POOL_PTE_PAGABLE) {
        Pagable = TRUE;
    }
    else {
        Pagable = FALSE;
    }

    if (Pagable == TRUE) {
        if (VmSupport == &MmSystemCacheWs) {
            LOCK_SYSTEM_WS (OldIrql, PsGetCurrentThread ());
        }
        else {
            LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
        }
        SystemWsLocked = TRUE;
    }

    PteContents = *PointerPte;

    if (ProtectionMask == MM_NOACCESS) {

        if (SystemWsLocked == TRUE) {
retry1:
            ASSERT (SystemWsLocked == TRUE);
            if (PteContents.u.Hard.Valid == 1) {

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
                WsIndex = Pfn1->u1.WsIndex;
                ASSERT (WsIndex != 0);
                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                MiRemovePageFromWorkingSet (PointerPte,
                                            Pfn1,
                                            VmSupport);
            }
            else if (PteContents.u.Soft.Transition == 1) {

                LOCK_PFN2 (OldIrql2);

                PteContents = *(volatile MMPTE *)PointerPte;

                if (PteContents.u.Soft.Transition == 0) {
                    UNLOCK_PFN2 (OldIrql2);
                    goto retry1;
                }

                Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
                Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                PointerPte->u.Soft.Protection = ProtectionMask;
                UNLOCK_PFN2(OldIrql2);
            }
            else {
    
                //
                // Must be page file space or demand zero.
                //
    
                PointerPte->u.Soft.Protection = ProtectionMask;
            }
            ASSERT (SystemWsLocked == TRUE);
            if (VmSupport == &MmSystemCacheWs) {
                UNLOCK_SYSTEM_WS (OldIrql);
            }
            else {
                UNLOCK_SESSION_SPACE_WS (OldIrql);
            }
        }
        else {

            ASSERT (SystemWsLocked == FALSE);

            //
            // Make it no access regardless of its previous protection state.
            // Note that the page frame number is preserved.
            //

            PteContents.u.Hard.Valid = 0;
            PteContents.u.Soft.Prototype = 0;
            PteContents.u.Soft.Protection = MM_NOACCESS;
    
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            LOCK_PFN2 (OldIrql2);

            Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

            PreviousPte.u.Flush = KeFlushSingleTb (VirtualAddress,
                                                   TRUE,
                                                   TRUE,
                                                   (PHARDWARE_PTE)PointerPte,
                                                   PteContents.u.Flush);

            MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);

            UNLOCK_PFN2(OldIrql2);
        }

        return TRUE;
    }

    //
    // No guard pages, noncached pages or copy-on-write for special pool.
    //

    if ((ProtectionMask >= MM_NOCACHE) || (ProtectionMask == MM_WRITECOPY) || (ProtectionMask == MM_EXECUTE_WRITECOPY)) {
        if (SystemWsLocked == TRUE) {
            if (VmSupport == &MmSystemCacheWs) {
                UNLOCK_SYSTEM_WS (OldIrql);
            }
            else {
                UNLOCK_SESSION_SPACE_WS (OldIrql);
            }
        }
        return FALSE;
    }

    //
    // Set accessible permissions - the page may already be protected or not.
    //

    if (Pagable == FALSE) {

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

        MI_MAKE_VALID_PTE (NewPteContents,
                           PteContents.u.Hard.PageFrameNumber,
                           ProtectionMask,
                           PointerPte);

        KeFlushSingleTb (VirtualAddress,
                         TRUE,
                         TRUE,
                         (PHARDWARE_PTE)PointerPte,
                         NewPteContents.u.Flush);

        ASSERT (SystemWsLocked == FALSE);
        return TRUE;
    }

retry2:

    ASSERT (SystemWsLocked == TRUE);

    if (PteContents.u.Hard.Valid == 1) {

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
        ASSERT (Pfn1->u1.WsIndex != 0);

        LOCK_PFN2 (OldIrql2);

        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

        MI_MAKE_VALID_PTE (PteContents,
                           PteContents.u.Hard.PageFrameNumber,
                           ProtectionMask,
                           PointerPte);

        PreviousPte.u.Flush = KeFlushSingleTb (VirtualAddress,
                                               TRUE,
                                               TRUE,
                                               (PHARDWARE_PTE)PointerPte,
                                               PteContents.u.Flush);

        MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);

        UNLOCK_PFN2 (OldIrql2);
    }
    else if (PteContents.u.Soft.Transition == 1) {

        LOCK_PFN2 (OldIrql2);

        PteContents = *(volatile MMPTE *)PointerPte;

        if (PteContents.u.Soft.Transition == 0) {
            UNLOCK_PFN2 (OldIrql2);
            goto retry2;
        }

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
        PointerPte->u.Soft.Protection = ProtectionMask;
        UNLOCK_PFN2(OldIrql2);
    }
    else {

        //
        // Must be page file space or demand zero.
        //

        PointerPte->u.Soft.Protection = ProtectionMask;
    }

    if (VmSupport == &MmSystemCacheWs) {
        UNLOCK_SYSTEM_WS (OldIrql);
    }
    else {
        UNLOCK_SESSION_SPACE_WS (OldIrql);
    }
    return TRUE;
}

LOGICAL
MmSetSpecialPool (
    IN LOGICAL Enable
    )

/*++

Routine Description:

    This routine enables/disables special pool.  This allows callers to ensure
    that subsequent allocations do not come from special pool.  It is relied
    upon by callers that require KSEG0 addresses.

Arguments:

    Enable - Supplies TRUE to enable special pool, FALSE to disable it.

Return Value:

    Current special pool state (enabled or disabled).

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    LOGICAL OldEnable;

    LOCK_PFN2 (OldIrql);

    OldEnable = MiSpecialPoolEnabled;

    MiSpecialPoolEnabled = Enable;

    UNLOCK_PFN2 (OldIrql);

    return OldEnable;
}

#ifndef NO_POOL_CHECKS
typedef struct _MI_BAD_TAGS {
    USHORT  Enabled;
    UCHAR   TargetChar;
    UCHAR   AllOthers;
    ULONG   Dispatches;
    ULONG   Allocations;
    ULONG   RandomizerEnabled;
} MI_BAD_TAGS, *PMI_BAD_TAGS;

MI_BAD_TAGS MiBadTags;
KTIMER MiSpecialPoolTimer;
KDPC MiSpecialPoolTimerDpc;
LARGE_INTEGER MiTimerDueTime;

#define MI_THREE_SECONDS     3


VOID
MiSpecialPoolTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )

/*++

Routine Description:

    This routine is executed every 3 seconds.  Just toggle the enable bit.
    If not many squeezed allocations have been made then just leave it
    continuously enabled.  Switch to a different tag if it looks like this
    one isn't getting any hits.

    No locks needed.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    DeferredContext - Optional deferred context;  not used.

    SystemArgument1 - Optional argument 1;  not used.

    SystemArgument2 - Optional argument 2;  not used.

Return Value:

    None.

--*/

{
    UCHAR NewChar;

    UNREFERENCED_PARAMETER (Dpc);
    UNREFERENCED_PARAMETER (DeferredContext);
    UNREFERENCED_PARAMETER (SystemArgument1);
    UNREFERENCED_PARAMETER (SystemArgument2);

    MiBadTags.Dispatches += 1;

    if (MiBadTags.Allocations > 500) {
        MiBadTags.Enabled += 1;
    }
    else if ((MiBadTags.Allocations == 0) && (MiBadTags.Dispatches > 100)) {
        if (MiBadTags.AllOthers == 0) {
            NewChar = (UCHAR)(MiBadTags.TargetChar + 1);
            if (NewChar >= 'a' && NewChar <= 'z') {
                MiBadTags.TargetChar = NewChar;
            }
            else if (NewChar == 'z' + 1) {
                MiBadTags.TargetChar = 'a';
            }
            else if (NewChar >= 'A' && NewChar <= 'Z') {
                MiBadTags.TargetChar = NewChar;
            }
            else {
                MiBadTags.TargetChar = 'A';
            }
        }
    }
}

extern ULONG InitializationPhase;

VOID
MiInitializeSpecialPoolCriteria (
    VOID
    )
{
    LARGE_INTEGER SystemTime;
    TIME_FIELDS TimeFields;

    if (InitializationPhase == 0) {
#if defined (_MI_SPECIAL_POOL_BY_DEFAULT)
        if (MmSpecialPoolTag == 0) {
            MmSpecialPoolTag = (ULONG)-2;
        }
#endif
        return;
    }

    if (MmSpecialPoolTag != (ULONG)-2) {
        return;
    }

    KeQuerySystemTime (&SystemTime);

    RtlTimeToTimeFields (&SystemTime, &TimeFields);

    if (TimeFields.Second <= 25) {
        MiBadTags.TargetChar = (UCHAR)('a' + (UCHAR)TimeFields.Second);
    }
    else if (TimeFields.Second <= 51) {
        MiBadTags.TargetChar = (UCHAR)('A' + (UCHAR)(TimeFields.Second - 26));
    }
    else {
        MiBadTags.AllOthers = 1;
    }

    MiBadTags.RandomizerEnabled = 1;

    //
    // Initialize a periodic timer to go off every three seconds.
    //

    KeInitializeDpc (&MiSpecialPoolTimerDpc, MiSpecialPoolTimerDispatch, NULL);

    KeInitializeTimer (&MiSpecialPoolTimer);

    MiTimerDueTime.QuadPart = Int32x32To64 (MI_THREE_SECONDS, -10000000);

    KeSetTimerEx (&MiSpecialPoolTimer,
                  MiTimerDueTime,
                  MI_THREE_SECONDS * 1000,
                  &MiSpecialPoolTimerDpc);

    MiBadTags.Enabled += 1;
}

LOGICAL
MmSqueezeBadTags (
    IN ULONG Tag
    )

/*++

Routine Description:

    This routine squeezes bad tags by forcing them into special pool in a
    systematic fashion.

Arguments:

    Tag - Supplies the tag of the requested allocation.

Return Value:

    TRUE if the caller should attempt to satisfy the requested allocation from
    special pool, FALSE if not.

Environment:

    Kernel mode, no locks (not even pool locks) held.

--*/

{
    PUCHAR tc;

    if ((MiBadTags.Enabled % 0x10) == 0) {
        return FALSE;
    }

    if (MiBadTags.RandomizerEnabled == 0) {
        return FALSE;
    }

    tc = (PUCHAR)&Tag;
    if (*tc == MiBadTags.TargetChar) {
        ;
    }
    else if (MiBadTags.AllOthers == 1) {
        if (*tc >= 'a' && *tc <= 'z') {
            return FALSE;
        }
        if (*tc >= 'A' && *tc <= 'Z') {
            return FALSE;
        }
    }
    else {
        return FALSE;
    }

    MiBadTags.Allocations += 1;

    return TRUE;
}

VOID
MiEnableRandomSpecialPool (
    IN LOGICAL Enable
    )
{
    MiBadTags.RandomizerEnabled = Enable;
}

#endif

LOGICAL
MiCheckSingleFilter (
    ULONG Tag,
    ULONG Filter
    )

/*++

Routine Description:

    This function checks if a pool tag matches a given pattern.

        ? - matches a single character
        * - terminates match with TRUE

    N.B.: ability inspired by the !poolfind debugger extension.

Arguments:

    Tag - a pool tag

    Filter - a globish pattern (chars and/or ?,*)

Return Value:

    TRUE if a match exists, FALSE otherwise.

--*/

{
    ULONG i;
    PUCHAR tc;
    PUCHAR fc;

    tc = (PUCHAR) &Tag;
    fc = (PUCHAR) &Filter;

    for (i = 0; i < 4; i += 1, tc += 1, fc += 1) {

        if (*fc == '*') {
            break;
        }
        if (*fc == '?') {
            continue;
        }
        if (i == 3 && ((*tc) & ~(PROTECTED_POOL >> 24)) == *fc) {
            continue;
        }
        if (*tc != *fc) {
            return FALSE;
        }
    }
    return TRUE;
}

LOGICAL
MmUseSpecialPool (
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    )

/*++

Routine Description:

    This routine checks whether the specified allocation should be attempted
    from special pool.  Both the tag string and the number of bytes are used
    to match against, if either cause a hit, then special pool is recommended.

Arguments:

    NumberOfBytes - Supplies the number of bytes to commit.

    Tag - Supplies the tag of the requested allocation.

Return Value:

    TRUE if the caller should attempt to satisfy the requested allocation from
    special pool, FALSE if not.

Environment:

    Kernel mode, no locks (not even pool locks) held.

--*/
{
    if ((NumberOfBytes <= POOL_BUDDY_MAX) &&
        (MmSpecialPoolTag != 0) &&
        (NumberOfBytes != 0)) {

#ifndef NO_POOL_CHECKS
        if (MmSqueezeBadTags (Tag) == TRUE) {
            return TRUE;
        }
#endif

        //
        // Check for a special pool tag match by tag string and size ranges.
        //

        if ((MiCheckSingleFilter (Tag, MmSpecialPoolTag)) ||
            ((MmSpecialPoolTag >= (NumberOfBytes + POOL_OVERHEAD)) &&
            (MmSpecialPoolTag < (NumberOfBytes + POOL_OVERHEAD + POOL_SMALLEST_BLOCK)))) {

            return TRUE;
        }
    }

    return FALSE;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\sessload.c ===
/*++

Copyright (c) 1997  Microsoft Corporation

Module Name:

   sessload.c

Abstract:

    This module contains the routines which implement the loading of
    session space drivers.

Author:

    Landy Wang (landyw) 05-Dec-1997

Revision History:

--*/

#include "mi.h"

//
// This tracks allocated group virtual addresses.  The term SESSIONWIDE is used
// to denote data that is the same across all sessions (as opposed to
// per-session data which can vary from session to session).
//
// Since each driver loaded into a session space is linked and fixed up
// against the system image, it must remain at the same virtual address
// across the system regardless of the session.
//
// A list is maintained by the group allocator of which virtual
// addresses are in use and by which DLL.
//
// The reference count tracks the number of sessions that have loaded
// this image.
//
// Access to this structure is guarded by the MmSystemLoadLock.
//
//
// typedef struct _SESSIONWIDE_DRIVER_ADDRESS {
//    LIST_ENTRY Link;
//    ULONG ReferenceCount;
//    PVOID Address;
//    ULONG_PTR Size;
//    ULONG_PTR WritablePages;
//    UNICODE_STRING FullDllName;
// } SESSIONWIDE_DRIVER_ADDRESS, *PSESSIONWIDE_DRIVER_ADDRESS;
//

LIST_ENTRY MmSessionWideAddressList;

//
// External function references
//

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

NTSTATUS
MiSessionInsertImage (
    IN PVOID BaseAddress
    );

NTSTATUS
MiSessionRemoveImage (
    IN PVOID BaseAddress
    );

NTSTATUS
MiSessionWideInsertImageAddress (
    IN PVOID BaseAddress,
    IN ULONG_PTR Size,
    IN ULONG WritablePages,
    IN PUNICODE_STRING ImageName,
    IN LOGICAL AtPreferredAddress
    );

NTSTATUS
MiSessionWideDereferenceImage (
    IN PVOID BaseAddress
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MiSessionWideInitializeAddresses)

#pragma alloc_text(PAGE, MiSessionWideInsertImageAddress)
#pragma alloc_text(PAGE, MiSessionWideDereferenceImage)
#pragma alloc_text(PAGE, MiSessionWideGetImageSize)
#pragma alloc_text(PAGE, MiSessionWideReserveImageAddress)
#pragma alloc_text(PAGE, MiRemoveImageSessionWide)
#pragma alloc_text(PAGE, MiShareSessionImage)

#pragma alloc_text(PAGE, MiSessionInsertImage)
#pragma alloc_text(PAGE, MiSessionRemoveImage)
#pragma alloc_text(PAGE, MiSessionLookupImage)
#pragma alloc_text(PAGE, MiSessionUnloadAllImages)
#endif


LOGICAL
MiMarkControlAreaInSystemSpace (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    Nonpaged wrapper to mark the argument control area properly.

Arguments:

    ControlArea - Supplies a control area to mark as mapped in system space.

Return Value:

    Returns TRUE if this was the first system space mapping of this
    control area.  FALSE otherwise.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    KIRQL OldIrql;
    LOGICAL FirstMapped;

    FirstMapped = FALSE;

    LOCK_PFN (OldIrql);
    if (ControlArea->u.Flags.ImageMappedInSystemSpace == 0) {
        FirstMapped = TRUE;
        ControlArea->u.Flags.ImageMappedInSystemSpace = 1;
    }
    UNLOCK_PFN (OldIrql);

    return FirstMapped;
}


NTSTATUS
MiShareSessionImage (
    IN PSECTION Section,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the given image into the current session space.
    This allows the image to be executed backed by the image file in the
    filesystem and allow code and read-only data to be shared.

Arguments:

    Section - Supplies a pointer to a section.

    ViewSize - Supplies the size in bytes of the view desired.

Return Value:

    Returns STATUS_SUCCESS on success, various NTSTATUS codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    KIRQL WsIrql;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG NumberOfPtes;
    PMMPTE StartPte;
    PMMPTE EndPte;
    PVOID AllocationStart;
    SIZE_T AllocationSize;
    NTSTATUS Status;
    LOGICAL FirstMapped;
    PVOID MappedBase;
    SIZE_T CommittedPages;
    PIMAGE_ENTRY_IN_SESSION DriverImage;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    if (*ViewSize == 0) {
        return STATUS_SUCCESS;
    }

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    MappedBase = Section->Segment->BasedAddress;

    ASSERT (((ULONG_PTR)MappedBase % PAGE_SIZE) == 0);
    ASSERT ((*ViewSize % PAGE_SIZE) == 0);

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    ControlArea = Section->Segment->ControlArea;

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (MiCheckPurgeAndUpMapCount (ControlArea) == FALSE) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (*ViewSize == 0) {

        *ViewSize = Section->SizeOfSection.LowPart;

    }
    else if (*ViewSize > Section->SizeOfSection.LowPart) {

        //
        // Section offset or view size past size of section.
        //

        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    AllocationStart = MappedBase;

    AllocationSize = *ViewSize;

    //
    // Calculate the PTE ranges and amount.
    //

    StartPte = MiGetPteAddress (AllocationStart);

    EndPte = MiGetPteAddress ((PCHAR)AllocationStart + AllocationSize);

    NumberOfPtes = BYTES_TO_PAGES (AllocationSize);

    Status = MiSessionWideGetImageSize (MappedBase, NULL, &CommittedPages);

    if (!NT_SUCCESS(Status)) {
        CommittedPages = NumberOfPtes;
    }

    if (MiChargeCommitment (CommittedPages, NULL) == FALSE) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_COMMIT);

        //
        // Don't bother releasing the page tables or their commit here, another
        // load will happen shortly or the whole session will go away.  On
        // session exit everything will be released automatically.
        //

        MiDereferenceControlArea (ControlArea);
        return STATUS_NO_MEMORY;
    }

    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                 CommittedPages);

    LOCK_SESSION_SPACE_WS (WsIrql, PsGetCurrentThread ());

    //
    // Make sure we have page tables for the PTE
    // entries we must fill in the session space structure.
    //

    Status = MiSessionCommitPageTables (AllocationStart,
                                        (PVOID)((PCHAR)AllocationStart + AllocationSize));

    if (!NT_SUCCESS(Status)) {

        UNLOCK_SESSION_SPACE_WS (WsIrql);

        Status = STATUS_NO_MEMORY;
bail:
        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CommittedPages);

        MiDereferenceControlArea (ControlArea);
        MiReturnCommitment (CommittedPages);

    	return Status;
    }

#if DBG
    while (StartPte < EndPte) {
        ASSERT (StartPte->u.Long == 0);
        StartPte += 1;
    }
    StartPte = MiGetPteAddress (AllocationStart);
#endif

    //
    // Flag that the image is mapped into system space.
    //

    if (Section->u.Flags.Image) {

        FirstMapped = MiMarkControlAreaInSystemSpace (ControlArea);

        //
        // Initialize all of the prototype PTEs as read only - later, copy
        // on write protections will be set on the actual PTEs mapping the
        // data (but not code) pages.
        //
    
        if (FirstMapped == TRUE) {
            MiSetImageProtect (Section->Segment, MM_EXECUTE_READ);
        }
    }

    //
    // Initialize the PTEs to point at the prototype PTEs.
    //

    Status = MiAddMappedPtes (StartPte, NumberOfPtes, ControlArea);

    UNLOCK_SESSION_SPACE_WS (WsIrql);

    if (!NT_SUCCESS (Status)) {

        //
        // Regardless of whether the PTEs were mapped, leave the control area
        // marked as mapped in system space so user applications cannot map the
        // file as an image as clearly the intent is to run it as a driver.
        //

        goto bail;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_SESSION_SHARED_IMAGE, CommittedPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_SYSMAPPED_PAGES_COMMITTED, (ULONG)CommittedPages);

    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_SYSMAPPED_PAGES_ALLOC, NumberOfPtes);

    //
    // No session space image faults may be taken until these fields of the
    // image entry are initialized.
    //

    DriverImage = MiSessionLookupImage (AllocationStart);
    ASSERT (DriverImage);

    DriverImage->LastAddress = (PVOID)((PCHAR)AllocationStart + AllocationSize - 1);
    DriverImage->PrototypePtes = Subsection->SubsectionBase;

    return STATUS_SUCCESS;
}


NTSTATUS
MiSessionInsertImage (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This routine allocates an image entry for the specified address in the
    current session space.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    STATUS_SUCCESS or various NTSTATUS error codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.
    
    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;
    PIMAGE_ENTRY_IN_SESSION NewImage;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    //
    // Create and initialize a new image entry prior to acquiring the session
    // space ws mutex.  This is to reduce the amount of time the mutex is held.
    // If an existing entry is found this allocation is just discarded.
    //

    NewImage = ExAllocatePoolWithTag (NonPagedPool,
                                      sizeof(IMAGE_ENTRY_IN_SESSION),
                                      'iHmM');

    if (NewImage == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (NewImage, sizeof(IMAGE_ENTRY_IN_SESSION));

    NewImage->Address = BaseAddress;
    NewImage->ImageCountInThisSession = 1;

    //
    // Check to see if the address is already loaded.
    //

    LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {
        Image = CONTAINING_RECORD (NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddress) {
            Image->ImageCountInThisSession += 1;
            UNLOCK_SESSION_SPACE_WS (OldIrql);
            ExFreePool (NewImage);
            return STATUS_ALREADY_COMMITTED;
        }
        NextEntry = NextEntry->Flink;
    }

    //
    // Insert the image entry into the session space structure.
    //

    InsertTailList (&MmSessionSpace->ImageList, &NewImage->Link);

    UNLOCK_SESSION_SPACE_WS (OldIrql);
    return STATUS_SUCCESS;
}


NTSTATUS
MiSessionRemoveImage (
    PVOID BaseAddr
    )

/*++

Routine Description:

    This routine removes the given image entry from the current session space.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND if the image is not
    in the current session space.

Environment:

    Kernel mode, APC_LEVEL and below.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;
    KIRQL OldIrql;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    LOCK_SESSION_SPACE_WS (OldIrql, PsGetCurrentThread ());
    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddr) {
            RemoveEntryList (NextEntry);
            UNLOCK_SESSION_SPACE_WS (OldIrql);
            ExFreePool (Image);
            return STATUS_SUCCESS;
        }

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_SESSION_SPACE_WS (OldIrql);
    return STATUS_NOT_FOUND;
}


PIMAGE_ENTRY_IN_SESSION
MiSessionLookupImage (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This routine looks up the image entry within the current session by the 
    specified base address.

Arguments:

    BaseAddress - Supplies the base address for the executable image.

Return Value:

    The image entry within this session on success or NULL on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Image;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Image = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        if (Image->Address == BaseAddress) {
            return Image;
        }

        NextEntry = NextEntry->Flink;
    }

    return NULL;
}


VOID
MiSessionUnloadAllImages (
    VOID
    )

/*++

Routine Description:

    This routine dereferences each image that has been loaded in the
    current session space.

    As each image is dereferenced, checks are made:

    If this session's reference count to the image reaches zero, the VA
    range in this session is deleted.  If the reference count to the image
    in the SESSIONWIDE list drops to zero, then the SESSIONWIDE's VA
    reservation is removed and the address space is made available to any
    new image.

    If this is the last systemwide reference to the driver then the driver
    is deleted from memory.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.  This is called in one of two contexts:
        1. the last thread in the last process of the current session space.
        2. or by any thread in the SMSS process.

    Note both the system load resource and the session working set
    mutex must be held to modify the list of images in this session.
    Either may be held to safely walk the list.

--*/

{
    NTSTATUS Status;
    PLIST_ENTRY NextEntry;
    PIMAGE_ENTRY_IN_SESSION Module;
    PKLDR_DATA_TABLE_ENTRY ImageHandle;

    ASSERT (MmSessionSpace->ReferenceCount == 0);

    //
    // The session's working set lock does not need to be acquired here since
    // no thread can be faulting on these addresses.
    //

    NextEntry = MmSessionSpace->ImageList.Flink;

    while (NextEntry != &MmSessionSpace->ImageList) {

        Module = CONTAINING_RECORD(NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

        //
        // Lookup the image entry in the system PsLoadedModuleList,
        // unload the image and delete it.
        //

        ImageHandle = MiLookupDataTableEntry (Module->Address, FALSE);

        ASSERT (ImageHandle);

        Status = MmUnloadSystemImage (ImageHandle);

        //
        // Restart the search at the beginning since the entry has been deleted.
        //

        ASSERT (MmSessionSpace->ReferenceCount == 0);

        NextEntry = MmSessionSpace->ImageList.Flink;
    }
}


VOID
MiSessionWideInitializeAddresses (
    VOID
    )

/*++

Routine Description:

    This routine is called at system initialization to set up the group
    address list.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    InitializeListHead (&MmSessionWideAddressList);
}


NTSTATUS
MiSessionWideInsertImageAddress (
    IN PVOID BaseAddress,
    IN ULONG_PTR NumberOfBytes,
    IN ULONG WritablePages,
    IN PUNICODE_STRING ImageName,
    IN LOGICAL AtPreferredAddress
    )

/*++

Routine Description:

    Allocate and add a SessionWide Entry reference to the global address
    allocation list for the current process' session space.

Arguments:

    BaseAddress - Supplies the base address to allocate an entry for.

    NumberOfBytes - Supplies the number of bytes the entry spans.

    WritablePages - Supplies the number of pages to charge commit for.

    ImageName - Supplies the name of the image in the PsLoadedModuleList
                that is represented by the virtual region.

    AtPreferredAddress - Supplies TRUE if the image is based at its preferred
                         address.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NO_MEMORY on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    PVOID LastAddress;
    PLIST_ENTRY NextEntry;
    PSESSIONWIDE_DRIVER_ADDRESS Vaddr;
    PSESSIONWIDE_DRIVER_ADDRESS New;
    PWCHAR NewName;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    New = ExAllocatePoolWithTag (NonPagedPool,
                                 sizeof(SESSIONWIDE_DRIVER_ADDRESS),
                                 'vHmM');

    if (New == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        return STATUS_NO_MEMORY;
    }

    RtlZeroMemory (New, sizeof(SESSIONWIDE_DRIVER_ADDRESS));

    New->ReferenceCount = 1;
    New->Address = BaseAddress;
    New->Size = NumberOfBytes;
    if (AtPreferredAddress == TRUE) {
        New->WritablePages = WritablePages;
    }
    else {
        New->WritablePages = (MI_ROUND_TO_SIZE (NumberOfBytes, PAGE_SIZE)) >> PAGE_SHIFT;
    }

    ASSERT (ImageName != NULL);

    NewName = (PWCHAR) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                  ImageName->Length + sizeof(UNICODE_NULL),
                                  'nHmM');

    if (NewName == NULL) {
        ExFreePool (New);
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_PAGED_POOL);
        return STATUS_NO_MEMORY;
    }

    RtlCopyMemory (NewName, ImageName->Buffer, ImageName->Length);
    NewName [ImageName->Length / sizeof(WCHAR)] = UNICODE_NULL;

    New->FullDllName.Buffer = NewName;
    New->FullDllName.Length = ImageName->Length;
    New->FullDllName.MaximumLength = ImageName->Length;

    //
    // Insert the entry in the memory-ordered list.
    //

    LastAddress = NULL;
    NextEntry = MmSessionWideAddressList.Flink;

    while (NextEntry != &MmSessionWideAddressList) {

        Vaddr = CONTAINING_RECORD (NextEntry,
                                   SESSIONWIDE_DRIVER_ADDRESS,
                                   Link);

        if (LastAddress < Vaddr->Address && Vaddr->Address > New->Address) {
            break;
        }

        LastAddress = Vaddr->Address;
        NextEntry = NextEntry->Flink;
    }

    InsertTailList (NextEntry, &New->Link);

    return STATUS_SUCCESS;
}

NTSTATUS
MiSessionWideDereferenceImage (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    Dereference the SessionWide entry for the specified address, potentially
    resulting in a deletion of the image.

Arguments:

    BaseAddress - Supplies the address for the driver to dereference.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    PLIST_ENTRY NextEntry;
    PSESSIONWIDE_DRIVER_ADDRESS SessionWideImageEntry;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (BaseAddress);

    NextEntry = MmSessionWideAddressList.Flink;

    while (NextEntry !=	&MmSessionWideAddressList) {

        SessionWideImageEntry = CONTAINING_RECORD (NextEntry,
                                                   SESSIONWIDE_DRIVER_ADDRESS,
                                                   Link);

        if (BaseAddress == SessionWideImageEntry->Address) {
    
            SessionWideImageEntry->ReferenceCount -= 1;
    
            //
            // If reference count is 0, delete the node.
            //

            if (SessionWideImageEntry->ReferenceCount == 0) {
                RemoveEntryList (NextEntry);
                ASSERT (SessionWideImageEntry->FullDllName.Buffer != NULL);
                ExFreePool (SessionWideImageEntry->FullDllName.Buffer);
                ExFreePool (SessionWideImageEntry);
            }
            return STATUS_SUCCESS;
        }

        NextEntry = NextEntry->Flink;
    }

    return STATUS_NOT_FOUND;
}


NTSTATUS
MiSessionWideGetImageSize (
    IN PVOID BaseAddress,
    OUT PSIZE_T NumberOfBytes OPTIONAL,
    OUT PSIZE_T CommitPages OPTIONAL
    )

/*++

Routine Description:

    Lookup the size allocated and committed for the image at the base address.
    This ensures that we free every page that may have been allocated due
    to rounding up.

Arguments:

    BaseAddress - Supplies the preferred address that the driver has
                  been linked (rebased) at.  If this address is available,
                  the driver will require no relocation.

    NumberOfBytes - Supplies a pointer to store the image size into.

    CommitPages - Supplies a pointer to store the number of committed pages
                  that were charged for this image.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    PLIST_ENTRY NextEntry;
    PSESSIONWIDE_DRIVER_ADDRESS SessionWideEntry;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    NextEntry = MmSessionWideAddressList.Flink;

    while (NextEntry != &MmSessionWideAddressList) {

        SessionWideEntry = CONTAINING_RECORD (NextEntry,
                                              SESSIONWIDE_DRIVER_ADDRESS,
                                              Link);

        if (BaseAddress == SessionWideEntry->Address) {

            if (ARGUMENT_PRESENT (NumberOfBytes)) {
                *NumberOfBytes = SessionWideEntry->Size;
            }

            if (ARGUMENT_PRESENT (CommitPages)) {
                *CommitPages = SessionWideEntry->WritablePages;
            }

            return STATUS_SUCCESS;
        }

        NextEntry = NextEntry->Flink;
    }

    return STATUS_NOT_FOUND;
}


NTSTATUS
MiSessionWideReserveImageAddress (
    IN PUNICODE_STRING ImageName,
    IN PSECTION Section,
    IN ULONG_PTR Alignment,
    OUT PVOID *AssignedAddress,
    OUT PLOGICAL AlreadyLoaded
    )

/*++

Routine Description:

    This routine allocates a range of virtual address space within
    session space.  This address range is unique system-wide and in this
    manner, code and pristine data of session drivers can be shared across
    multiple sessions.

    This routine does not actually commit pages, but reserves the virtual
    address region for the named image.  An entry is created here and attached
    to the current session space to track the loaded image.  Thus if all
    the references to a given range go away, the range can then be reused.

Arguments:

    ImageName - Supplies the name of the driver that will be loaded into
                the allocated space.

    Section - Supplies the section (and thus, the preferred address that the
              driver has been linked (rebased) at.  If this address is
              available, the driver will require no relocation.  The section
              is also used to derive the number of bytes to reserve.

    Alignment - Supplies the virtual address alignment for the address.

    AssignedAddress - Supplies a pointer to a variable that receives the
                      allocated address if the routine succeeds.

    AlreadyLoaded - Supplies a pointer to a variable that receives TRUE if the
                    specified image name has already been loaded.

Return Value:

    Returns STATUS_SUCCESS on success, various NTSTATUS codes on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    PLIST_ENTRY NextEntry;
    PSESSIONWIDE_DRIVER_ADDRESS Vaddr;
    NTSTATUS Status;
    PWCHAR pName;
    PVOID NewAddress;
    ULONG_PTR AvailableAddress;
    ULONG_PTR SessionSpaceEnd;
    PVOID PreferredAddress;
    ULONG_PTR NumberOfBytes;
    ULONG WritablePages;
    LOGICAL AtPreferredAddress;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION);
    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    pName = NULL;
    *AlreadyLoaded = FALSE;
    PreferredAddress = Section->Segment->BasedAddress;
    NumberOfBytes = Section->Segment->TotalNumberOfPtes << PAGE_SHIFT;

    AvailableAddress = MiSessionImageStart;
    NumberOfBytes = MI_ROUND_TO_SIZE (NumberOfBytes, Alignment);
    SessionSpaceEnd = MiSessionImageEnd;

    Status = MiGetWritablePagesInSection(Section, &WritablePages);

    if (!NT_SUCCESS(Status)) {
        WritablePages = Section->Segment->TotalNumberOfPtes;
    }

    //
    // If the requested address is not properly aligned or not in the session
    // space region, pick an address for it.  This image will not be shared.
    //

    if ((ULONG_PTR)PreferredAddress & (Alignment - 1)) {

#if DBG
        DbgPrint("MiSessionWideReserveImageAddress: Bad alignment 0x%x for PreferredAddress 0x%x\n",
            Alignment,
            PreferredAddress);
#endif

        PreferredAddress = NULL;
    }
    else if ((ULONG_PTR)PreferredAddress < AvailableAddress ||
	     ((ULONG_PTR)PreferredAddress + NumberOfBytes >= SessionSpaceEnd)) {

#if DBG
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrint ("MiSessionWideReserveImageAddress: PreferredAddress 0x%x not in session space\n", PreferredAddress);
        }
#endif

        PreferredAddress = NULL;
    }

    //
    // Check the system wide session space image list to see if the
    // image name has already been given a slot.
    //

    NextEntry = MmSessionWideAddressList.Flink;

    while (NextEntry != &MmSessionWideAddressList) {

        Vaddr = CONTAINING_RECORD (NextEntry,
                                   SESSIONWIDE_DRIVER_ADDRESS,
                                   Link);

        if (Vaddr->FullDllName.Buffer != NULL) {

            if (RtlEqualUnicodeString(ImageName, &Vaddr->FullDllName, TRUE)) {

                //
                // The size requested should be the same.
                //

                if (Vaddr->Size < NumberOfBytes) {
#if DBG
                    DbgPrint ("MiSessionWideReserveImageAddress: Size %d Larger than Entry %d, DLL %wZ\n",
                        NumberOfBytes,
                        Vaddr->Size,
                        ImageName);
#endif

                    return STATUS_CONFLICTING_ADDRESSES;
                }
        
                //
                // This image has already been loaded systemwide.  If it's
                // already been loaded in this session space as well, just
                // bump the reference count using the already allocated
                // address.  Otherwise, insert it into this session space.
                //

                Status = MiSessionInsertImage (Vaddr->Address);

                if (Status == STATUS_ALREADY_COMMITTED) {

                    *AlreadyLoaded = TRUE;
                    *AssignedAddress = Vaddr->Address;

                    return STATUS_SUCCESS;
                }

                if (!NT_SUCCESS (Status)) {
                    return Status;
                }

                //
                // Bump the reference count as this is a new entry.
                //

                Vaddr->ReferenceCount += 1;

                *AssignedAddress = Vaddr->Address;

                return STATUS_SUCCESS;
            }
        }

        //
        // Note this list must be sorted by ascending address.
        // See if the PreferredAddress and size collide with any entries.
        //

        if (PreferredAddress) {

            if ((PreferredAddress >= Vaddr->Address) &&
                 (PreferredAddress < (PVOID)((ULONG_PTR)Vaddr->Address + Vaddr->Size))) {
                    PreferredAddress = NULL;
            }
            else if ((PreferredAddress < Vaddr->Address) &&
                    ((PVOID)((ULONG_PTR)PreferredAddress + NumberOfBytes) > Vaddr->Address)) {
                    PreferredAddress = NULL;
            }
        }

        //
        // Check for an available general allocation slot.
        //

        if (((PVOID)AvailableAddress >= Vaddr->Address) &&
            (AvailableAddress <= (ULONG_PTR)Vaddr->Address + Vaddr->Size)) {

            AvailableAddress = (ULONG_PTR)Vaddr->Address + Vaddr->Size;

            if (AvailableAddress & (Alignment - 1)) {
                AvailableAddress = MI_ROUND_TO_SIZE (AvailableAddress, Alignment);
            }
        }
        else if (AvailableAddress + NumberOfBytes > (ULONG_PTR)Vaddr->Address) {

            AvailableAddress = (ULONG_PTR)Vaddr->Address + Vaddr->Size;

            if (AvailableAddress & (Alignment - 1)) {
                AvailableAddress = MI_ROUND_TO_SIZE (AvailableAddress, Alignment);
            }
        }

        NextEntry = NextEntry->Flink;
    }

    if ((PreferredAddress == NULL) &&
        (AvailableAddress + NumberOfBytes > MiSessionImageEnd)) {

        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_IMAGE_VA_SPACE);
        return STATUS_NO_MEMORY;
    }

    //
    // Try to put the module into its requested address so it can be shared.
    //

    if (PreferredAddress) {

#if DBG
        if (MmDebug & MM_DBG_SESSIONS) {
            DbgPrint ("MiSessionWideReserveImageAddress: Code Sharing on %wZ, Address 0x%x\n",ImageName,PreferredAddress);
        }
#endif

        NewAddress = PreferredAddress;
    }
    else {
        ASSERT (AvailableAddress != 0);
        ASSERT ((AvailableAddress & (Alignment - 1)) == 0);

#if DBG
        DbgPrint ("MiSessionWideReserveImageAddress: NO Code Sharing on %wZ, Address 0x%x\n",ImageName,AvailableAddress);
#endif

        NewAddress = (PVOID)AvailableAddress;
    }

    //
    // Create a new node entry for the address range.
    //

    if (NewAddress == PreferredAddress) {
        AtPreferredAddress = TRUE;
    }
    else {
        AtPreferredAddress = FALSE;
    }

    Status = MiSessionWideInsertImageAddress (NewAddress,
                                              NumberOfBytes,
                                              WritablePages,
                                              ImageName,
                                              AtPreferredAddress);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // Create an entry for this image in the current session space.
    //

    Status = MiSessionInsertImage (NewAddress);

    if (!NT_SUCCESS(Status)) {
        MiSessionWideDereferenceImage (NewAddress);
        return Status;
    }

    *AssignedAddress = NewAddress;

    return STATUS_SUCCESS;
}

NTSTATUS
MiRemoveImageSessionWide (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    Delete the image space region from the current session space.
    This dereferences the globally allocated SessionWide region.
    
    The SessionWide region will be deleted if the reference count goes to zero.
    
Arguments:

    BaseAddress - Supplies the address the driver is loaded at.

Return Value:

    Returns STATUS_SUCCESS on success, STATUS_NOT_FOUND on failure.

Environment:

    Kernel mode, APC_LEVEL and below, MmSystemLoadLock held.

--*/

{
    NTSTATUS Status;

    PAGED_CODE();

    SYSLOAD_LOCK_OWNED_BY_ME ();

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);

    Status = MiSessionWideDereferenceImage (BaseAddress);

    ASSERT (NT_SUCCESS(Status));

    //
    // Remove the image reference from the current session space.
    //

    MiSessionRemoveImage (BaseAddress);

    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\sysptes.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   sysptes.c

Abstract:

    This module contains the routines which reserve and release
    system wide PTEs reserved within the non paged portion of the
    system space.  These PTEs are used for mapping I/O devices
    and mapping kernel stacks for threads.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

VOID
MiFeedSysPtePool (
    IN ULONG Index
    );

ULONG
MiGetSystemPteListCount (
    IN ULONG ListSize
    );

VOID
MiPteSListExpansionWorker (
    IN PVOID Context
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSystemPtes)
#pragma alloc_text(PAGE,MiPteSListExpansionWorker)
#pragma alloc_text(MISYSPTE,MiReserveAlignedSystemPtes)
#pragma alloc_text(MISYSPTE,MiReserveSystemPtes)
#pragma alloc_text(MISYSPTE,MiFeedSysPtePool)
#pragma alloc_text(MISYSPTE,MiReleaseSystemPtes)
#pragma alloc_text(MISYSPTE,MiGetSystemPteListCount)
#endif

ULONG MmTotalSystemPtes;
ULONG MmTotalFreeSystemPtes[MaximumPtePoolTypes];
PMMPTE MmSystemPtesStart[MaximumPtePoolTypes];
PMMPTE MmSystemPtesEnd[MaximumPtePoolTypes];
ULONG MmPteFailures[MaximumPtePoolTypes];

PMMPTE MiPteStart;
PRTL_BITMAP MiPteStartBitmap;
PRTL_BITMAP MiPteEndBitmap;
extern KSPIN_LOCK MiPteTrackerLock;

ULONG MiSystemPteAllocationFailed;

#if defined(_IA64_)

//
// IA64 has an 8k page size.
//
// Mm cluster MDLs consume 8 pages.
// Small stacks consume 9 pages (including backing store and guard pages).
// Large stacks consume 22 pages (including backing store and guard pages).
//
// PTEs are binned at sizes 1, 2, 4, 8, 9 and 23.
//

#define MM_SYS_PTE_TABLES_MAX 6

//
// Make sure when changing MM_PTE_TABLE_LIMIT that you also increase the
// number of entries in MmSysPteTables.
//

#define MM_PTE_TABLE_LIMIT 23

ULONG MmSysPteIndex[MM_SYS_PTE_TABLES_MAX] = {1,2,4,8,9,MM_PTE_TABLE_LIMIT};

UCHAR MmSysPteTables[MM_PTE_TABLE_LIMIT+1] = {0,0,1,2,2,3,3,3,3,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5};

ULONG MmSysPteMinimumFree [MM_SYS_PTE_TABLES_MAX] = {100,50,30,20,20,20};

#elif defined (_AMD64_)

//
// AMD64 has a 4k page size.
// Small stacks consume 6 pages (including the guard page).
// Large stacks consume 16 pages (including the guard page).
//
// PTEs are binned at sizes 1, 2, 4, 6, 8, and 16.
//

#define MM_SYS_PTE_TABLES_MAX 6

#define MM_PTE_TABLE_LIMIT 16

ULONG MmSysPteIndex[MM_SYS_PTE_TABLES_MAX] = {1,2,4,6,8,MM_PTE_TABLE_LIMIT};

UCHAR MmSysPteTables[MM_PTE_TABLE_LIMIT+1] = {0,0,1,2,2,3,3,4,4,5,5,5,5,5,5,5,5};

ULONG MmSysPteMinimumFree [MM_SYS_PTE_TABLES_MAX] = {100,50,30,100,20,20};

#else

//
// x86 has a 4k page size.
// Small stacks consume 4 pages (including the guard page).
// Large stacks consume 16 pages (including the guard page).
//
// PTEs are binned at sizes 1, 2, 4, 8, and 16.
//

#define MM_SYS_PTE_TABLES_MAX 5

#define MM_PTE_TABLE_LIMIT 16

ULONG MmSysPteIndex[MM_SYS_PTE_TABLES_MAX] = {1,2,4,8,MM_PTE_TABLE_LIMIT};

UCHAR MmSysPteTables[MM_PTE_TABLE_LIMIT+1] = {0,0,1,2,2,3,3,3,3,4,4,4,4,4,4,4,4};

ULONG MmSysPteMinimumFree [MM_SYS_PTE_TABLES_MAX] = {100,50,30,20,20};

#endif

KSPIN_LOCK MiSystemPteSListHeadLock;
SLIST_HEADER MiSystemPteSListHead;

#define MM_MIN_SYSPTE_FREE 500
#define MM_MAX_SYSPTE_FREE 3000

ULONG MmSysPteListBySizeCount [MM_SYS_PTE_TABLES_MAX];

//
// Initial sizes for PTE lists.
//

#define MM_PTE_LIST_1  400
#define MM_PTE_LIST_2  100
#define MM_PTE_LIST_4   60
#define MM_PTE_LIST_6  100
#define MM_PTE_LIST_8   50
#define MM_PTE_LIST_9   50
#define MM_PTE_LIST_16  40
#define MM_PTE_LIST_18  40

PVOID MiSystemPteNBHead[MM_SYS_PTE_TABLES_MAX];
LONG MiSystemPteFreeCount[MM_SYS_PTE_TABLES_MAX];

#if defined(_WIN64)
#define MI_MAXIMUM_SLIST_PTE_PAGES 16
#else
#define MI_MAXIMUM_SLIST_PTE_PAGES 8
#endif

typedef struct _MM_PTE_SLIST_EXPANSION_WORK_CONTEXT {
    WORK_QUEUE_ITEM WorkItem;
    LONG Active;
    ULONG SListPages;
} MM_PTE_SLIST_EXPANSION_WORK_CONTEXT, *PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT;

MM_PTE_SLIST_EXPANSION_WORK_CONTEXT MiPteSListExpand;

VOID
MiFeedSysPtePool (
    IN ULONG Index
    );

VOID
MiDumpSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

ULONG
MiCountFreeSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

PVOID
MiGetHighestPteConsumer (
    OUT PULONG_PTR NumberOfPtes
    );

VOID
MiCheckPteReserve (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    );

VOID
MiCheckPteRelease (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    );

//
// Define inline functions to pack and unpack pointers in the platform
// specific non-blocking queue pointer structure.
//

typedef struct _PTE_SLIST {
    union {
        struct {
            SINGLE_LIST_ENTRY ListEntry;
        } Slist;
        NBQUEUE_BLOCK QueueBlock;
    } u1;
} PTE_SLIST, *PPTE_SLIST;

#if defined (_AMD64_)

typedef union _PTE_QUEUE_POINTER {
    struct {
        LONG64 PointerPte : 48;
        LONG64 TimeStamp : 16;
    };

    LONG64 Data;
} PTE_QUEUE_POINTER, *PPTE_QUEUE_POINTER;

#elif defined(_X86_)

typedef union _PTE_QUEUE_POINTER {
    struct {
        LONG PointerPte;
        LONG TimeStamp;
    };

    LONG64 Data;
} PTE_QUEUE_POINTER, *PPTE_QUEUE_POINTER;

#elif defined(_IA64_)

typedef union _PTE_QUEUE_POINTER {
    struct {
        ULONG64 PointerPte : 45;
        ULONG64 Region : 3;
        ULONG64 TimeStamp : 16;
    };

    LONG64 Data;
} PTE_QUEUE_POINTER, *PPTE_QUEUE_POINTER;


#else

#error "no target architecture"

#endif



#if defined(_AMD64_)

__inline
VOID
PackPTEValue (
    IN PPTE_QUEUE_POINTER Entry,
    IN PMMPTE PointerPte,
    IN ULONG TimeStamp
    )
{
    Entry->PointerPte = (LONG64)PointerPte;
    Entry->TimeStamp = (LONG64)TimeStamp;
    return;
}

__inline
PMMPTE
UnpackPTEPointer (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (PMMPTE)(Entry->PointerPte);
}

__inline
ULONG
MiReadTbFlushTimeStamp (
    VOID
    )
{
    return (KeReadTbFlushTimeStamp() & (ULONG)0xFFFF);
}

#elif defined(_X86_)

__inline
VOID
PackPTEValue (
    IN PPTE_QUEUE_POINTER Entry,
    IN PMMPTE PointerPte,
    IN ULONG TimeStamp
    )
{
    Entry->PointerPte = (LONG)PointerPte;
    Entry->TimeStamp = (LONG)TimeStamp;
    return;
}

__inline
PMMPTE
UnpackPTEPointer (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (PMMPTE)(Entry->PointerPte);
}

__inline
ULONG
MiReadTbFlushTimeStamp (
    VOID
    )
{
    return (KeReadTbFlushTimeStamp());
}

#elif defined(_IA64_)

__inline
VOID
PackPTEValue (
    IN PPTE_QUEUE_POINTER Entry,
    IN PMMPTE PointerPte,
    IN ULONG TimeStamp
    )
{
    Entry->PointerPte = (ULONG64)PointerPte - PTE_BASE;
    Entry->TimeStamp = (ULONG64)TimeStamp;
    Entry->Region = (ULONG64)PointerPte >> 61;
    return;
}

__inline
PMMPTE
UnpackPTEPointer (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    LONG64 Value;
    Value = (ULONG64)Entry->PointerPte + PTE_BASE;
    Value |= Entry->Region << 61;
    return (PMMPTE)(Value);
}

__inline
ULONG
MiReadTbFlushTimeStamp (
    VOID
    )
{
    return (KeReadTbFlushTimeStamp() & (ULONG)0xFFFF);
}

#else

#error "no target architecture"

#endif

__inline
ULONG
UnpackPTETimeStamp (
    IN PPTE_QUEUE_POINTER Entry
    )
{
    return (ULONG)(Entry->TimeStamp);
}


PMMPTE
MiReserveSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function locates the specified number of unused PTEs
    within the non paged portion of system space.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to locate.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    Returns the address of the first PTE located.
    NULL if no system PTEs can be located.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    ULONG Index;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;
#if DBG
    ULONG j;
    PMMPTE PointerFreedPte;
#endif

    if (SystemPtePoolType == SystemPteSpace) {

        if (NumberOfPtes <= MM_PTE_TABLE_LIMIT) {
            Index = MmSysPteTables [NumberOfPtes];
            ASSERT (NumberOfPtes <= MmSysPteIndex[Index]);

            if (ExRemoveHeadNBQueue (MiSystemPteNBHead[Index], (PULONG64)&Value) == TRUE) {
                InterlockedDecrement ((PLONG)&MmSysPteListBySizeCount[Index]);

                PointerPte = UnpackPTEPointer (&Value);

                TimeStamp = UnpackPTETimeStamp (&Value);

#if DBG
                PointerPte->u.List.NextEntry = 0xABCDE;
                if (MmDebug & MM_DBG_SYS_PTES) {
                    PointerFreedPte = PointerPte;
                    for (j = 0; j < MmSysPteIndex[Index]; j += 1) {
                        ASSERT (PointerFreedPte->u.Hard.Valid == 0);
                        PointerFreedPte += 1;
                    }
                }
#endif

                ASSERT (PointerPte >= MmSystemPtesStart[SystemPtePoolType]);
                ASSERT (PointerPte <= MmSystemPtesEnd[SystemPtePoolType]);

                if (MmSysPteListBySizeCount[Index] < MmSysPteMinimumFree[Index]) {
                    MiFeedSysPtePool (Index);
                }

                //
                // The last thing is to check whether the TB needs flushing.
                //

                if (TimeStamp == MiReadTbFlushTimeStamp()) {
                    KeFlushEntireTb (TRUE, TRUE);
                }

                if (MmTrackPtes & 0x2) {
                    MiCheckPteReserve (PointerPte, MmSysPteIndex[Index]);
                }

                return PointerPte;
            }

            //
            // Fall through and go the long way to satisfy the PTE request.
            //

            NumberOfPtes = MmSysPteIndex [Index];
        }
    }

    //
    // Acquire the system space lock to synchronize access to this
    // routine.
    //

    PointerPte = MiReserveAlignedSystemPtes (NumberOfPtes,
                                             SystemPtePoolType,
                                             0);

#if DBG
    if (MmDebug & MM_DBG_SYS_PTES) {
        if (PointerPte != NULL) {
            PointerFreedPte = PointerPte;
            for (j = 0; j < NumberOfPtes; j += 1) {
                ASSERT (PointerFreedPte->u.Hard.Valid == 0);
                PointerFreedPte += 1;
            }
        }
    }
#endif

    if (PointerPte == NULL) {
        MiSystemPteAllocationFailed += 1;
    }

    return PointerPte;
}

VOID
MiFeedSysPtePool (
    IN ULONG Index
    )

/*++

Routine Description:

    This routine adds PTEs to the nonblocking queue lists.

Arguments:

    Index - Supplies the index for the nonblocking queue list to fill.

Return Value:

    None.

Environment:

    Kernel mode, internal to SysPtes.

--*/

{
    ULONG i;
    PMMPTE PointerPte;

    if (MmTotalFreeSystemPtes[SystemPteSpace] < MM_MIN_SYSPTE_FREE) {
#if defined (_X86_)
        if (MiRecoverExtraPtes () == FALSE) {
            MiRecoverSpecialPtes (PTE_PER_PAGE);
        }
#endif
        return;
    }

    for (i = 0; i < 10 ; i += 1) {
        PointerPte = MiReserveAlignedSystemPtes (MmSysPteIndex [Index],
                                                 SystemPteSpace,
                                                 0);
        if (PointerPte == NULL) {
            return;
        }

        MiReleaseSystemPtes (PointerPte,
                             MmSysPteIndex [Index],
                             SystemPteSpace);
    }

    return;
}


PMMPTE
MiReserveAlignedSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType,
    IN ULONG Alignment
    )

/*++

Routine Description:

    This function locates the specified number of unused PTEs to locate
    within the non paged portion of system space.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to locate.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

    Alignment - Supplies the virtual address alignment for the address
                the returned PTE maps. For example, if the value is 64K,
                the returned PTE will map an address on a 64K boundary.
                An alignment of zero means to align on a page boundary.

Return Value:

    Returns the address of the first PTE located.
    NULL if no system PTEs can be located.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerFollowingPte;
    PMMPTE Previous;
    ULONG_PTR SizeInSet;
    KIRQL OldIrql;
    ULONG MaskSize;
    ULONG NumberOfRequiredPtes;
    ULONG OffsetSum;
    ULONG PtesToObtainAlignment;
    PMMPTE NextSetPointer;
    ULONG_PTR LeftInSet;
    ULONG_PTR PteOffset;
    MMPTE_FLUSH_LIST PteFlushList;

    MaskSize = (Alignment - 1) >> (PAGE_SHIFT - PTE_SHIFT);

    OffsetSum = (Alignment >> (PAGE_SHIFT - PTE_SHIFT));

#if defined (_X86_)
restart:
#endif

    //
    // Initializing PointerFollowingPte is not needed for correctness,
    // but without it the compiler cannot compile this code W4 to
    // check for use of uninitialized variables.
    //

    PointerFollowingPte = NULL;

    //
    // The nonpaged PTE pool uses the invalid PTEs to define the pool
    // structure.   A global pointer points to the first free set
    // in the list, each free set contains the number free and a pointer
    // to the next free set.  The free sets are kept in an ordered list
    // such that the pointer to the next free set is always greater
    // than the address of the current free set.
    //
    // As to not limit the size of this pool, two PTEs are used
    // to define a free region.  If the region is a single PTE, the
    // prototype field within the PTE is set indicating the set
    // consists of a single PTE.
    //
    // The page frame number field is used to define the next set
    // and the number free.  The two flavors are:
    //
    //                           o          V
    //                           n          l
    //                           e          d
    //  +-----------------------+-+----------+
    //  |  next set             |0|0        0|
    //  +-----------------------+-+----------+
    //  |  number in this set   |0|0        0|
    //  +-----------------------+-+----------+
    //
    //
    //  +-----------------------+-+----------+
    //  |  next set             |1|0        0|
    //  +-----------------------+-+----------+
    //  ...
    //

    //
    // Acquire the system space lock to synchronize access.
    //

    MiLockSystemSpace(OldIrql);

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];
    Previous = PointerPte;

    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

        //
        // End of list and none found.
        //

        MiUnlockSystemSpace(OldIrql);
#if defined (_X86_)
        if (MiRecoverExtraPtes () == TRUE) {
            goto restart;
        }
        if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
            goto restart;
        }
#endif
        MmPteFailures[SystemPtePoolType] += 1;
        return NULL;
    }

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    if (Alignment <= PAGE_SIZE) {

        //
        // Don't deal with alignment issues.
        //

        while (TRUE) {

            if (PointerPte->u.List.OneEntry) {
                SizeInSet = 1;

            }
            else {

                PointerFollowingPte = PointerPte + 1;
                SizeInSet = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;
            }

            if (NumberOfPtes < SizeInSet) {

                //
                // Get the PTEs from this set and reduce the size of the
                // set.  Note that the size of the current set cannot be 1.
                //

                if ((SizeInSet - NumberOfPtes) == 1) {

                    //
                    // Collapse to the single PTE format.
                    //

                    PointerPte->u.List.OneEntry = 1;

                }
                else {

                    PointerFollowingPte->u.List.NextEntry = SizeInSet - NumberOfPtes;

                    //
                    // Get the required PTEs from the end of the set.
                    //

#if 0
                    if (MmDebug & MM_DBG_SYS_PTES) {
                        MiDumpSystemPtes(SystemPtePoolType);
                        PointerFollowingPte = PointerPte + (SizeInSet - NumberOfPtes);
                        DbgPrint("allocated 0x%lx Ptes at %p\n",NumberOfPtes,PointerFollowingPte);
                    }
#endif //0
                }

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                MiUnlockSystemSpace(OldIrql);

                PointerPte =  PointerPte + (SizeInSet - NumberOfPtes);
                goto Flush;
            }

            if (NumberOfPtes == SizeInSet) {

                //
                // Satisfy the request with this complete set and change
                // the list to reflect the fact that this set is gone.
                //

                Previous->u.List.NextEntry = PointerPte->u.List.NextEntry;

                //
                // Release the system PTE lock.
                //

#if 0
                if (MmDebug & MM_DBG_SYS_PTES) {
                        MiDumpSystemPtes(SystemPtePoolType);
                        PointerFollowingPte = PointerPte + (SizeInSet - NumberOfPtes);
                        DbgPrint("allocated 0x%lx Ptes at %lx\n",NumberOfPtes,PointerFollowingPte);
                }
#endif //0

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                MiUnlockSystemSpace(OldIrql);
                goto Flush;
            }

            //
            // Point to the next set and try again
            //

            if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

                //
                // End of list and none found.
                //

                MiUnlockSystemSpace(OldIrql);
#if defined (_X86_)
                if (MiRecoverExtraPtes () == TRUE) {
                    goto restart;
                }
                if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
                    goto restart;
                }
#endif
                MmPteFailures[SystemPtePoolType] += 1;
                return NULL;
            }
            Previous = PointerPte;
            PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
            ASSERT (PointerPte > Previous);
        }

    }
    else {

        //
        // Deal with the alignment issues.
        //

        while (TRUE) {

            if (PointerPte->u.List.OneEntry) {
                SizeInSet = 1;

            }
            else {

                PointerFollowingPte = PointerPte + 1;
                SizeInSet = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;
            }

            PtesToObtainAlignment = (ULONG)
                (((OffsetSum - ((ULONG_PTR)PointerPte & MaskSize)) & MaskSize) >>
                    PTE_SHIFT);

            NumberOfRequiredPtes = NumberOfPtes + PtesToObtainAlignment;

            if (NumberOfRequiredPtes < SizeInSet) {

                //
                // Get the PTEs from this set and reduce the size of the
                // set.  Note that the size of the current set cannot be 1.
                //
                // This current block will be slit into 2 blocks if
                // the PointerPte does not match the alignment.
                //

                //
                // Check to see if the first PTE is on the proper
                // alignment, if so, eliminate this block.
                //

                LeftInSet = SizeInSet - NumberOfRequiredPtes;

                //
                // Set up the new set at the end of this block.
                //

                NextSetPointer = PointerPte + NumberOfRequiredPtes;
                NextSetPointer->u.List.NextEntry =
                                       PointerPte->u.List.NextEntry;

                PteOffset = (ULONG_PTR)(NextSetPointer - MmSystemPteBase);

                if (PtesToObtainAlignment == 0) {

                    Previous->u.List.NextEntry += NumberOfRequiredPtes;

                }
                else {

                    //
                    // Point to the new set at the end of the block
                    // we are giving away.
                    //

                    PointerPte->u.List.NextEntry = PteOffset;

                    //
                    // Update the size of the current set.
                    //

                    if (PtesToObtainAlignment == 1) {

                        //
                        // Collapse to the single PTE format.
                        //

                        PointerPte->u.List.OneEntry = 1;

                    }
                    else {

                        //
                        // Set the set size in the next PTE.
                        //

                        PointerFollowingPte->u.List.NextEntry =
                                                        PtesToObtainAlignment;
                    }
                }

                //
                // Set up the new set at the end of the block.
                //

                if (LeftInSet == 1) {
                    NextSetPointer->u.List.OneEntry = 1;
                }
                else {
                    NextSetPointer->u.List.OneEntry = 0;
                    NextSetPointer += 1;
                    NextSetPointer->u.List.NextEntry = LeftInSet;
                }
                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                MiUnlockSystemSpace(OldIrql);

                PointerPte = PointerPte + PtesToObtainAlignment;
                goto Flush;
            }

            if (NumberOfRequiredPtes == SizeInSet) {

                //
                // Satisfy the request with this complete set and change
                // the list to reflect the fact that this set is gone.
                //

                if (PtesToObtainAlignment == 0) {

                    //
                    // This block exactly satisfies the request.
                    //

                    Previous->u.List.NextEntry =
                                            PointerPte->u.List.NextEntry;

                }
                else {

                    //
                    // A portion at the start of this block remains.
                    //

                    if (PtesToObtainAlignment == 1) {

                        //
                        // Collapse to the single PTE format.
                        //

                        PointerPte->u.List.OneEntry = 1;

                    }
                    else {
                      PointerFollowingPte->u.List.NextEntry =
                                                        PtesToObtainAlignment;

                    }
                }

                MmTotalFreeSystemPtes[SystemPtePoolType] -= NumberOfPtes;
#if DBG
                if (MmDebug & MM_DBG_SYS_PTES) {
                    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                             MiCountFreeSystemPtes (SystemPtePoolType));
                }
#endif

                MiUnlockSystemSpace(OldIrql);

                PointerPte = PointerPte + PtesToObtainAlignment;
                goto Flush;
            }

            //
            // Point to the next set and try again.
            //

            if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {

                //
                // End of list and none found.
                //

                MiUnlockSystemSpace(OldIrql);
#if defined (_X86_)
                if (MiRecoverExtraPtes () == TRUE) {
                    goto restart;
                }
                if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
                    goto restart;
                }
#endif
                MmPteFailures[SystemPtePoolType] += 1;
                return NULL;
            }
            Previous = PointerPte;
            PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
            ASSERT (PointerPte > Previous);
        }
    }
Flush:

    if (SystemPtePoolType == SystemPteSpace) {
        PVOID BaseAddress;
        ULONG j;

        PteFlushList.Count = 0;
        Previous = PointerPte;
        BaseAddress = MiGetVirtualAddressMappedByPte (Previous);

        for (j = 0; j < NumberOfPtes; j += 1) {
            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushPte[PteFlushList.Count] = Previous;
                PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
                PteFlushList.Count += 1;
            }

            //
            // PTEs being freed better be invalid.
            //
            ASSERT (Previous->u.Hard.Valid == 0);

            *Previous = ZeroKernelPte;
            BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
            Previous += 1;
        }

        MiFlushPteList (&PteFlushList, TRUE, ZeroKernelPte);

        if (MmTrackPtes & 0x2) {
            MiCheckPteReserve (PointerPte, NumberOfPtes);
        }
    }
    return PointerPte;
}

VOID
MiIssueNoPtesBugcheck (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function bugchecks when no PTEs are left.

Arguments:

    SystemPtePoolType - Supplies the PTE type of the pool that is empty.

    NumberOfPtes - Supplies the number of PTEs requested that failed.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PVOID HighConsumer;
    ULONG_PTR HighPteUse;

    if (SystemPtePoolType == SystemPteSpace) {

        HighConsumer = MiGetHighestPteConsumer (&HighPteUse);

        if (HighConsumer != NULL) {
            KeBugCheckEx (DRIVER_USED_EXCESSIVE_PTES,
                          (ULONG_PTR)HighConsumer,
                          HighPteUse,
                          MmTotalFreeSystemPtes[SystemPtePoolType],
                          MmNumberOfSystemPtes);
        }
    }

    KeBugCheckEx (NO_MORE_SYSTEM_PTES,
                  (ULONG_PTR)SystemPtePoolType,
                  NumberOfPtes,
                  MmTotalFreeSystemPtes[SystemPtePoolType],
                  MmNumberOfSystemPtes);
}

VOID
MiPteSListExpansionWorker (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine is the worker routine to add additional SLISTs for the
    system PTE nonblocking queues.

Arguments:

    Context - Supplies a pointer to the MM_PTE_SLIST_EXPANSION_WORK_CONTEXT.

Return Value:

    None.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    ULONG i;
    ULONG SListEntries;
    PPTE_SLIST SListChunks;
    PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT Expansion;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    Expansion = (PMM_PTE_SLIST_EXPANSION_WORK_CONTEXT) Context;

    ASSERT (Expansion->Active == 1);

    if (Expansion->SListPages < MI_MAXIMUM_SLIST_PTE_PAGES) {

        //
        // Allocate another page of SLIST entries for the
        // nonblocking PTE queues.
        //

        SListChunks = (PPTE_SLIST) ExAllocatePoolWithTag (NonPagedPool,
                                                          PAGE_SIZE,
                                                          'PSmM');

        if (SListChunks != NULL) {

            //
            // Carve up the pages into SLIST entries (with no pool headers).
            //

            Expansion->SListPages += 1;

            SListEntries = PAGE_SIZE / sizeof (PTE_SLIST);

            for (i = 0; i < SListEntries; i += 1) {
                InterlockedPushEntrySList (&MiSystemPteSListHead,
                                           (PSINGLE_LIST_ENTRY)SListChunks);
                SListChunks += 1;
            }
        }
    }

    ASSERT (Expansion->Active == 1);
    InterlockedExchange (&Expansion->Active, 0);
}


VOID
MiReleaseSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function releases the specified number of PTEs
    within the non paged portion of system space.

    Note that the PTEs must be invalid and the page frame number
    must have been set to zero.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

    SystemPtePoolType - Supplies the PTE type of the pool to release PTEs to,
                        one of SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG_PTR Size;
    ULONG i;
    ULONG_PTR PteOffset;
    PMMPTE PointerPte;
    PMMPTE PointerFollowingPte;
    PMMPTE NextPte;
    KIRQL OldIrql;
    ULONG Index;
    ULONG TimeStamp;
    PTE_QUEUE_POINTER Value;
    ULONG ExtensionInProgress;

    //
    // Check to make sure the PTE address is within bounds.
    //

    ASSERT (NumberOfPtes != 0);
    ASSERT (StartingPte >= MmSystemPtesStart[SystemPtePoolType]);
    ASSERT (StartingPte <= MmSystemPtesEnd[SystemPtePoolType]);

    if ((MmTrackPtes & 0x2) && (SystemPtePoolType == SystemPteSpace)) {

        //
        // If the low bit is set, this range was never reserved and therefore
        // should not be validated during the release.
        //

        if ((ULONG_PTR)StartingPte & 0x1) {
            StartingPte = (PMMPTE) ((ULONG_PTR)StartingPte & ~0x1);
        }
        else {
            MiCheckPteRelease (StartingPte, NumberOfPtes);
        }
    }

    //
    // Zero PTEs.
    //

    MiFillMemoryPte (StartingPte,
                     NumberOfPtes * sizeof (MMPTE),
                     ZeroKernelPte.u.Long);

    if ((SystemPtePoolType == SystemPteSpace) &&
        (NumberOfPtes <= MM_PTE_TABLE_LIMIT)) {

        //
        // Encode the PTE pointer and the TB flush counter into Value.
        //

        TimeStamp = KeReadTbFlushTimeStamp();

        PackPTEValue (&Value, StartingPte, TimeStamp);

        Index = MmSysPteTables [NumberOfPtes];

        ASSERT (NumberOfPtes <= MmSysPteIndex [Index]);

        //
        // N.B.  NumberOfPtes must be set here regardless so if this entry
        // is not inserted into the nonblocking list, the PTE count will still
        // be right when we go the long way.
        //

        NumberOfPtes = MmSysPteIndex [Index];

        if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MIN_SYSPTE_FREE) {

            //
            // Add to the pool if the size is less than 15 + the minimum.
            //

            i = MmSysPteMinimumFree[Index];
            if (MmTotalFreeSystemPtes[SystemPteSpace] >= MM_MAX_SYSPTE_FREE) {

                //
                // Lots of free PTEs, quadruple the limit.
                //

                i = i * 4;
            }
            i += 15;
            if (MmSysPteListBySizeCount[Index] <= i) {

                if (ExInsertTailNBQueue (MiSystemPteNBHead[Index], Value.Data) == TRUE) {
                    InterlockedIncrement ((PLONG)&MmSysPteListBySizeCount[Index]);
                    return;
                }

                //
                // No lookasides are left for inserting this PTE allocation
                // into the nonblocking queues.  Queue an extension to a
                // worker thread so it can be done in a deadlock-free
                // manner.
                //

                if (MiPteSListExpand.SListPages < MI_MAXIMUM_SLIST_PTE_PAGES) {

                    //
                    // If an extension is not in progress then queue one now.
                    //

                    ExtensionInProgress = InterlockedCompareExchange (&MiPteSListExpand.Active, 1, 0);

                    if (ExtensionInProgress == 0) {

                        ExInitializeWorkItem (&MiPteSListExpand.WorkItem,
                                              MiPteSListExpansionWorker,
                                              (PVOID)&MiPteSListExpand);

                        ExQueueWorkItem (&MiPteSListExpand.WorkItem, CriticalWorkQueue);
                    }

                }
            }
        }

        //
        // The insert failed - our lookaside list must be empty or we are
        // low on PTEs systemwide or we already had plenty on our list and
        // didn't try to insert.  Fall through to queue this in the long way.
        //
    }

    //
    // Acquire system space spin lock to synchronize access.
    //

    PteOffset = (ULONG_PTR)(StartingPte - MmSystemPteBase);

    MiLockSystemSpace(OldIrql);

    MmTotalFreeSystemPtes[SystemPtePoolType] += NumberOfPtes;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];

    while (TRUE) {
        NextPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
        if (PteOffset < PointerPte->u.List.NextEntry) {

            //
            // Insert in the list at this point.  The
            // previous one should point to the new freed set and
            // the new freed set should point to the place
            // the previous set points to.
            //
            // Attempt to combine the clusters before we
            // insert.
            //
            // Locate the end of the current structure.
            //

            ASSERT (((StartingPte + NumberOfPtes) <= NextPte) ||
                    (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST));

            PointerFollowingPte = PointerPte + 1;
            if (PointerPte->u.List.OneEntry) {
                Size = 1;
            }
            else {
                Size = (ULONG_PTR) PointerFollowingPte->u.List.NextEntry;
            }
            if ((PointerPte + Size) == StartingPte) {

                //
                // We can combine the clusters.
                //

                NumberOfPtes += (ULONG)Size;
                PointerFollowingPte->u.List.NextEntry = NumberOfPtes;
                PointerPte->u.List.OneEntry = 0;

                //
                // Point the starting PTE to the beginning of
                // the new free set and try to combine with the
                // following free cluster.
                //

                StartingPte = PointerPte;

            }
            else {

                //
                // Can't combine with previous. Make this Pte the
                // start of a cluster.
                //

                //
                // Point this cluster to the next cluster.
                //

                StartingPte->u.List.NextEntry = PointerPte->u.List.NextEntry;

                //
                // Point the current cluster to this cluster.
                //

                PointerPte->u.List.NextEntry = PteOffset;

                //
                // Set the size of this cluster.
                //

                if (NumberOfPtes == 1) {
                    StartingPte->u.List.OneEntry = 1;

                }
                else {
                    StartingPte->u.List.OneEntry = 0;
                    PointerFollowingPte = StartingPte + 1;
                    PointerFollowingPte->u.List.NextEntry = NumberOfPtes;
                }
            }

            //
            // Attempt to combine the newly created cluster with
            // the following cluster.
            //

            if ((StartingPte + NumberOfPtes) == NextPte) {

                //
                // Combine with following cluster.
                //

                //
                // Set the next cluster to the value contained in the
                // cluster we are merging into this one.
                //

                StartingPte->u.List.NextEntry = NextPte->u.List.NextEntry;
                StartingPte->u.List.OneEntry = 0;
                PointerFollowingPte = StartingPte + 1;

                if (NextPte->u.List.OneEntry) {
                    Size = 1;

                }
                else {
                    NextPte++;
                    Size = (ULONG_PTR) NextPte->u.List.NextEntry;
                }
                PointerFollowingPte->u.List.NextEntry = NumberOfPtes + Size;
            }
#if 0
            if (MmDebug & MM_DBG_SYS_PTES) {
                MiDumpSystemPtes(SystemPtePoolType);
            }
#endif

#if DBG
            if (MmDebug & MM_DBG_SYS_PTES) {
                ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                         MiCountFreeSystemPtes (SystemPtePoolType));
            }
#endif
            MiUnlockSystemSpace(OldIrql);
            return;
        }

        //
        // Point to next freed cluster.
        //

        PointerPte = NextPte;
    }
}

VOID
MiReleaseSplitSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This function releases the specified number of PTEs
    within the non paged portion of system space.

    Note that the PTEs must be invalid and the page frame number
    must have been set to zero.

    This portion is a split portion from a larger allocation so
    careful updating of the tracking bitmaps must be done here.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

    SystemPtePoolType - Supplies the PTE type of the pool to release PTEs to,
                        one of SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG StartBit;
    KIRQL OldIrql;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
                
    //
    // Check to make sure the PTE address is within bounds.
    //

    ASSERT (NumberOfPtes != 0);
    ASSERT (StartingPte >= MmSystemPtesStart[SystemPtePoolType]);
    ASSERT (StartingPte <= MmSystemPtesEnd[SystemPtePoolType]);

    if ((MmTrackPtes & 0x2) && (SystemPtePoolType == SystemPteSpace)) {

        ASSERT (MmTrackPtes & 0x2);

        VirtualAddress = MiGetVirtualAddressMappedByPte (StartingPte);

        StartBit = (ULONG) (StartingPte - MiPteStart);

        ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

        //
        // Verify start and size of allocation using the tracking bitmaps.
        //

        StartBitMapBuffer = MiPteStartBitmap->Buffer;
        EndBitMapBuffer = MiPteEndBitmap->Buffer;

        //
        // All the start bits better be set.
        //

        for (i = StartBit; i < StartBit + NumberOfPtes; i += 1) {
            ASSERT (MI_CHECK_BIT (StartBitMapBuffer, i) == 1);
        }

        if (StartBit != 0) {

            if (RtlCheckBit (MiPteStartBitmap, StartBit - 1)) {

                if (!RtlCheckBit (MiPteEndBitmap, StartBit - 1)) {

                    //
                    // In the middle of an allocation - update the previous
                    // so it ends here.
                    //

                    MI_SET_BIT (EndBitMapBuffer, StartBit - 1);
                }
                else {

                    //
                    // The range being freed is the start of an allocation.
                    //
                }
            }
        }

        //
        // Unconditionally set the end bit (and clear any others) in case the
        // split chunk crosses multiple allocations.
        //

        MI_SET_BIT (EndBitMapBuffer, StartBit + NumberOfPtes - 1);

        ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
    }

    MiReleaseSystemPtes (StartingPte, NumberOfPtes, SystemPteSpace);
}


VOID
MiInitializeSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This routine initializes the system PTE pool.

Arguments:

    StartingPte - Supplies the address of the first PTE to put in the pool.

    NumberOfPtes - Supplies the number of PTEs to put in the pool.

    SystemPtePoolType - Supplies the PTE type of the pool to initialize, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    none.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG TotalPtes;
    ULONG SListEntries;
    SIZE_T SListBytes;
    ULONG TotalChunks;
    PMMPTE PointerPte;
    PPTE_SLIST Chunk;
    PPTE_SLIST SListChunks;

    //
    // Set the base of the system PTE pool to this PTE.  This takes into
    // account that systems may have additional PTE pools below the PTE_BASE.
    //

    MmSystemPteBase = MI_PTE_BASE_FOR_LOWEST_KERNEL_ADDRESS;

    MmSystemPtesStart[SystemPtePoolType] = StartingPte;
    MmSystemPtesEnd[SystemPtePoolType] = StartingPte + NumberOfPtes - 1;

    //
    // If there are no PTEs specified, then make a valid chain by indicating
    // that the list is empty.
    //

    if (NumberOfPtes == 0) {
        MmFirstFreeSystemPte[SystemPtePoolType] = ZeroKernelPte;
        MmFirstFreeSystemPte[SystemPtePoolType].u.List.NextEntry =
                                                                MM_EMPTY_LIST;
        return;
    }

    //
    // Initialize the specified system PTE pool.
    //

    MiFillMemoryPte (StartingPte,
                     NumberOfPtes * sizeof (MMPTE),
                     ZeroKernelPte.u.Long);

    //
    // The page frame field points to the next cluster.  As we only
    // have one cluster at initialization time, mark it as the last
    // cluster.
    //

    StartingPte->u.List.NextEntry = MM_EMPTY_LIST;

    MmFirstFreeSystemPte[SystemPtePoolType] = ZeroKernelPte;
    MmFirstFreeSystemPte[SystemPtePoolType].u.List.NextEntry =
                                                StartingPte - MmSystemPteBase;

    //
    // If there is only one PTE in the pool, then mark it as a one entry
    // PTE. Otherwise, store the cluster size in the following PTE.
    //

    if (NumberOfPtes == 1) {
        StartingPte->u.List.OneEntry = TRUE;

    }
    else {
        StartingPte += 1;
        MI_WRITE_INVALID_PTE (StartingPte, ZeroKernelPte);
        StartingPte->u.List.NextEntry = NumberOfPtes;
    }

    //
    // Set the total number of free PTEs for the specified type.
    //

    MmTotalFreeSystemPtes[SystemPtePoolType] = NumberOfPtes;

    ASSERT (MmTotalFreeSystemPtes[SystemPtePoolType] ==
                         MiCountFreeSystemPtes (SystemPtePoolType));

    if (SystemPtePoolType == SystemPteSpace) {

        ULONG Lists[MM_SYS_PTE_TABLES_MAX] = {
#if defined(_IA64_)
                MM_PTE_LIST_1,
                MM_PTE_LIST_2,
                MM_PTE_LIST_4,
                MM_PTE_LIST_8,
                MM_PTE_LIST_9,
                MM_PTE_LIST_18
#elif defined(_AMD64_)
                MM_PTE_LIST_1,
                MM_PTE_LIST_2,
                MM_PTE_LIST_4,
                MM_PTE_LIST_6,
                MM_PTE_LIST_8,
                MM_PTE_LIST_16
#else
                MM_PTE_LIST_1,
                MM_PTE_LIST_2,
                MM_PTE_LIST_4,
                MM_PTE_LIST_8,
                MM_PTE_LIST_16
#endif
        };

        MmTotalSystemPtes = NumberOfPtes;

        TotalPtes = 0;
        TotalChunks = 0;

        KeInitializeSpinLock (&MiSystemPteSListHeadLock);
        InitializeSListHead (&MiSystemPteSListHead);

        for (i = 0; i < MM_SYS_PTE_TABLES_MAX ; i += 1) {
            TotalPtes += (Lists[i] * MmSysPteIndex[i]);
            TotalChunks += Lists[i];
        }

        SListBytes = TotalChunks * sizeof (PTE_SLIST);
        SListBytes = MI_ROUND_TO_SIZE (SListBytes, PAGE_SIZE);
        SListEntries = (ULONG)(SListBytes / sizeof (PTE_SLIST));

        SListChunks = (PPTE_SLIST) ExAllocatePoolWithTag (NonPagedPool,
                                                          SListBytes,
                                                          'PSmM');

        if (SListChunks == NULL) {
            MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
        }

        ASSERT (MiPteSListExpand.Active == FALSE);
        ASSERT (MiPteSListExpand.SListPages == 0);

        MiPteSListExpand.SListPages = (ULONG)(SListBytes / PAGE_SIZE);

        ASSERT (MiPteSListExpand.SListPages != 0);

        //
        // Carve up the pages into SLIST entries (with no pool headers).
        //

        Chunk = SListChunks;
        for (i = 0; i < SListEntries; i += 1) {
            InterlockedPushEntrySList (&MiSystemPteSListHead,
                                       (PSINGLE_LIST_ENTRY)Chunk);
            Chunk += 1;
        }

        //
        // Now that the SLIST is populated, initialize the nonblocking heads.
        //

        for (i = 0; i < MM_SYS_PTE_TABLES_MAX ; i += 1) {
            MiSystemPteNBHead[i] = ExInitializeNBQueueHead (&MiSystemPteSListHead);

            if (MiSystemPteNBHead[i] == NULL) {
                MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
            }
        }

        if (MmTrackPtes & 0x2) {

            //
            // Allocate PTE mapping verification bitmaps.
            //

            ULONG BitmapSize;

#if defined(_WIN64)
            BitmapSize = MmNumberOfSystemPtes;
            MiPteStart = MmSystemPtesStart[SystemPteSpace];
#else
	    MiPteStart = MiGetPteAddress (MmSystemRangeStart);
            BitmapSize = ((ULONG_PTR)PTE_TOP + 1) - (ULONG_PTR) MiPteStart;
            BitmapSize /= sizeof (MMPTE);
#endif

            MiCreateBitMap (&MiPteStartBitmap, BitmapSize, NonPagedPool);

            if (MiPteStartBitmap != NULL) {

                MiCreateBitMap (&MiPteEndBitmap, BitmapSize, NonPagedPool);

                if (MiPteEndBitmap == NULL) {
                    ExFreePool (MiPteStartBitmap);
                    MiPteStartBitmap = NULL;
                }
            }

            if ((MiPteStartBitmap != NULL) && (MiPteEndBitmap != NULL)) {
                RtlClearAllBits (MiPteStartBitmap);
                RtlClearAllBits (MiPteEndBitmap);
            }
            MmTrackPtes &= ~0x2;
        }

        //
        // Initialize the by size lists.
        //

        PointerPte = MiReserveSystemPtes (TotalPtes, SystemPteSpace);

        if (PointerPte == NULL) {
            MiIssueNoPtesBugcheck (TotalPtes, SystemPteSpace);
        }

        i = MM_SYS_PTE_TABLES_MAX;
        do {
            i -= 1;
            do {
                Lists[i] -= 1;
                MiReleaseSystemPtes (PointerPte,
                                     MmSysPteIndex[i],
                                     SystemPteSpace);
                PointerPte += MmSysPteIndex[i];
            } while (Lists[i] != 0);
        } while (i != 0);

        //
        // Turn this on after the multiple releases of the binned PTEs (that
        // came from a single reservation) above.
        //

        if (MiPteStartBitmap != NULL) {
            MmTrackPtes |= 0x2;
        }
    }

    return;
}

VOID
MiIncrementSystemPtes (
    IN ULONG  NumberOfPtes
    )

/*++

Routine Description:

    This routine increments the total number of PTEs.  This is done
    separately from actually adding the PTEs to the pool so that
    autoconfiguration can use the high number in advance of the PTEs
    actually getting added.

Arguments:

    NumberOfPtes - Supplies the number of PTEs to increment the total by.

Return Value:

    None.

Environment:

    Kernel mode.  Synchronization provided by the caller.

--*/

{
    MmTotalSystemPtes += NumberOfPtes;
}
VOID
MiAddSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG  NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )

/*++

Routine Description:

    This routine adds newly created PTEs to the specified pool.

Arguments:

    StartingPte - Supplies the address of the first PTE to put in the pool.

    NumberOfPtes - Supplies the number of PTEs to put in the pool.

    SystemPtePoolType - Supplies the PTE type of the pool to expand, one of
                        SystemPteSpace or NonPagedPoolExpansion.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE EndingPte;

    ASSERT (SystemPtePoolType == SystemPteSpace);

    EndingPte = StartingPte + NumberOfPtes - 1;

    if (StartingPte < MmSystemPtesStart[SystemPtePoolType]) {
        MmSystemPtesStart[SystemPtePoolType] = StartingPte;
    }

    if (EndingPte > MmSystemPtesEnd[SystemPtePoolType]) {
        MmSystemPtesEnd[SystemPtePoolType] = EndingPte;
    }

    //
    // Set the low bit to signify this range was never reserved and therefore
    // should not be validated during the release.
    //

    if (MmTrackPtes & 0x2) {
        StartingPte = (PMMPTE) ((ULONG_PTR)StartingPte | 0x1);
    }

    MiReleaseSystemPtes (StartingPte, NumberOfPtes, SystemPtePoolType);
}


ULONG
MiGetSystemPteListCount (
    IN ULONG ListSize
    )

/*++

Routine Description:

    This routine returns the number of free entries of the list which
    covers the specified size.  The size must be less than or equal to the
    largest list index.

Arguments:

    ListSize - Supplies the number of PTEs needed.

Return Value:

    Number of free entries on the list which contains ListSize PTEs.

Environment:

    Kernel mode.

--*/

{
    ULONG Index;

    ASSERT (ListSize <= MM_PTE_TABLE_LIMIT);

    Index = MmSysPteTables [ListSize];

    return MmSysPteListBySizeCount[Index];
}


LOGICAL
MiGetSystemPteAvailability (
    IN ULONG NumberOfPtes,
    IN MM_PAGE_PRIORITY Priority
    )

/*++

Routine Description:

    This routine checks how many SystemPteSpace PTEs are available for the
    requested size.  If plenty are available then TRUE is returned.
    If we are reaching a low resource situation, then the request is evaluated
    based on the argument priority.

Arguments:

    NumberOfPtes - Supplies the number of PTEs needed.

    Priority - Supplies the priority of the request.

Return Value:

    TRUE if the caller should allocate the PTEs, FALSE if not.

Environment:

    Kernel mode.

--*/

{
    ULONG Index;
    ULONG FreePtes;
    ULONG FreeBinnedPtes;

    ASSERT (Priority != HighPagePriority);

    FreePtes = MmTotalFreeSystemPtes[SystemPteSpace];

    if (NumberOfPtes <= MM_PTE_TABLE_LIMIT) {
        Index = MmSysPteTables [NumberOfPtes];
        FreeBinnedPtes = MmSysPteListBySizeCount[Index];

        if (FreeBinnedPtes > MmSysPteMinimumFree[Index]) {
            return TRUE;
        }
        if (FreeBinnedPtes != 0) {
            if (Priority == NormalPagePriority) {
                if (FreeBinnedPtes > 1 || FreePtes > 512) {
                    return TRUE;
                }
#if defined (_X86_)
                if (MiRecoverExtraPtes () == TRUE) {
                    return TRUE;
                }
                if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
                    return TRUE;
                }
#endif
                MmPteFailures[SystemPteSpace] += 1;
                return FALSE;
            }
            if (FreePtes > 2048) {
                return TRUE;
            }
#if defined (_X86_)
            if (MiRecoverExtraPtes () == TRUE) {
                return TRUE;
            }
            if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
                return TRUE;
            }
#endif
            MmPteFailures[SystemPteSpace] += 1;
            return FALSE;
        }
    }

    if (Priority == NormalPagePriority) {
        if ((LONG)NumberOfPtes < (LONG)FreePtes - 512) {
            return TRUE;
        }
#if defined (_X86_)
        if (MiRecoverExtraPtes () == TRUE) {
            return TRUE;
        }
        if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
            return TRUE;
        }
#endif
        MmPteFailures[SystemPteSpace] += 1;
        return FALSE;
    }

    if ((LONG)NumberOfPtes < (LONG)FreePtes - 2048) {
        return TRUE;
    }
#if defined (_X86_)
    if (MiRecoverExtraPtes () == TRUE) {
        return TRUE;
    }
    if (MiRecoverSpecialPtes (NumberOfPtes) == TRUE) {
        return TRUE;
    }
#endif
    MmPteFailures[SystemPteSpace] += 1;
    return FALSE;
}

VOID
MiCheckPteReserve (
    IN PMMPTE PointerPte,
    IN ULONG NumberOfPtes
    )

/*++

Routine Description:

    This function checks the reserve of the specified number of system
    space PTEs.

Arguments:

    StartingPte - Supplies the address of the first PTE to reserve.

    NumberOfPtes - Supplies the number of PTEs to reserve.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    ULONG StartBit;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
        
    ASSERT (MmTrackPtes & 0x2);

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    if (NumberOfPtes == 0) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x200,
                      (ULONG_PTR) VirtualAddress,
                      0,
                      0);
    }

    StartBit = (ULONG) (PointerPte - MiPteStart);

    i = StartBit;

    StartBitMapBuffer = MiPteStartBitmap->Buffer;

    EndBitMapBuffer = MiPteEndBitmap->Buffer;

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    for ( ; i < StartBit + NumberOfPtes; i += 1) {
        if (MI_CHECK_BIT (StartBitMapBuffer, i)) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x201,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          NumberOfPtes);
        }
    }

    RtlSetBits (MiPteStartBitmap, StartBit, NumberOfPtes);

    for (i = StartBit; i < StartBit + NumberOfPtes; i += 1) {
        if (MI_CHECK_BIT (EndBitMapBuffer, i)) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x202,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          NumberOfPtes);
        }
    }

    MI_SET_BIT (EndBitMapBuffer, i - 1);

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}

VOID
MiCheckPteRelease (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes
    )

/*++

Routine Description:

    This function checks the release of the specified number of system
    space PTEs.

Arguments:

    StartingPte - Supplies the address of the first PTE to release.

    NumberOfPtes - Supplies the number of PTEs to release.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    ULONG i;
    ULONG Index;
    ULONG StartBit;
    KIRQL OldIrql;
    ULONG CalculatedPtes;
    ULONG NumberOfPtesRoundedUp;
    PULONG StartBitMapBuffer;
    PULONG EndBitMapBuffer;
    PVOID VirtualAddress;
    PVOID LowestVirtualAddress;
    PVOID HighestVirtualAddress;
            
    ASSERT (MmTrackPtes & 0x2);

    VirtualAddress = MiGetVirtualAddressMappedByPte (StartingPte);

    LowestVirtualAddress = MiGetVirtualAddressMappedByPte (MmSystemPtesStart[SystemPteSpace]);

    HighestVirtualAddress = MiGetVirtualAddressMappedByPte (MmSystemPtesEnd[SystemPteSpace]);

    if (NumberOfPtes == 0) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x300,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    if (StartingPte < MmSystemPtesStart[SystemPteSpace]) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x301,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    if (StartingPte > MmSystemPtesEnd[SystemPteSpace]) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x302,
                      (ULONG_PTR) VirtualAddress,
                      (ULONG_PTR) LowestVirtualAddress,
                      (ULONG_PTR) HighestVirtualAddress);
    }

    StartBit = (ULONG) (StartingPte - MiPteStart);

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    //
    // Verify start and size of allocation using the tracking bitmaps.
    //

    if (!RtlCheckBit (MiPteStartBitmap, StartBit)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x303,
                      (ULONG_PTR) VirtualAddress,
                      NumberOfPtes,
                      0);
    }

    if (StartBit != 0) {

        if (RtlCheckBit (MiPteStartBitmap, StartBit - 1)) {

            if (!RtlCheckBit (MiPteEndBitmap, StartBit - 1)) {

                //
                // In the middle of an allocation... bugcheck.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x304,
                              (ULONG_PTR) VirtualAddress,
                              NumberOfPtes,
                              0);
            }
        }
    }

    //
    // Find the last allocated PTE to calculate the correct size.
    //

    EndBitMapBuffer = MiPteEndBitmap->Buffer;

    i = StartBit;
    while (!MI_CHECK_BIT (EndBitMapBuffer, i)) {
        i += 1;
    }

    CalculatedPtes = i - StartBit + 1;
    NumberOfPtesRoundedUp = NumberOfPtes;

    if (CalculatedPtes <= MM_PTE_TABLE_LIMIT) {
        Index = MmSysPteTables [NumberOfPtes];
        NumberOfPtesRoundedUp = MmSysPteIndex [Index];
    }

    if (CalculatedPtes != NumberOfPtesRoundedUp) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x305,
                      (ULONG_PTR) VirtualAddress,
                      NumberOfPtes,
                      CalculatedPtes);
    }

    StartBitMapBuffer = MiPteStartBitmap->Buffer;

    for (i = StartBit; i < StartBit + CalculatedPtes; i += 1) {
        if (MI_CHECK_BIT (StartBitMapBuffer, i) == 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x306,
                          (ULONG_PTR) VirtualAddress,
                          (ULONG_PTR) VirtualAddress + ((i - StartBit) << PAGE_SHIFT),
                          CalculatedPtes);
        }
    }

    RtlClearBits (MiPteStartBitmap, StartBit, CalculatedPtes);

    MI_CLEAR_BIT (EndBitMapBuffer, i - 1);

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}



#if DBG

VOID
MiDumpSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )
{
    PMMPTE PointerPte;
    PMMPTE PointerNextPte;
    ULONG_PTR ClusterSize;
    PMMPTE EndOfCluster;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];
    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        return;
    }

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    for (;;) {
        if (PointerPte->u.List.OneEntry) {
            ClusterSize = 1;
        }
        else {
            PointerNextPte = PointerPte + 1;
            ClusterSize = (ULONG_PTR) PointerNextPte->u.List.NextEntry;
        }

        EndOfCluster = PointerPte + (ClusterSize - 1);

        DbgPrint("System Pte at %p for %p entries (%p)\n",
                PointerPte, ClusterSize, EndOfCluster);

        if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
            break;
        }

        PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
    }
    return;
}

ULONG
MiCountFreeSystemPtes (
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    )
{
    PMMPTE PointerPte;
    PMMPTE PointerNextPte;
    ULONG_PTR ClusterSize;
    ULONG_PTR FreeCount;

    PointerPte = &MmFirstFreeSystemPte[SystemPtePoolType];
    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        return 0;
    }

    FreeCount = 0;

    PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;

    for (;;) {
        if (PointerPte->u.List.OneEntry) {
            ClusterSize = 1;

        }
        else {
            PointerNextPte = PointerPte + 1;
            ClusterSize = (ULONG_PTR) PointerNextPte->u.List.NextEntry;
        }

        FreeCount += ClusterSize;
        if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
            break;
        }

        PointerPte = MmSystemPteBase + PointerPte->u.List.NextEntry;
    }

    return (ULONG)FreeCount;
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\umapview.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   umapview.c

Abstract:

    This module contains the routines which implement the
    NtUnmapViewOfSection service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtUnmapViewOfSection)
#pragma alloc_text(PAGE,MmUnmapViewOfSection)
#pragma alloc_text(PAGE,MiUnmapViewOfSection)
#endif


NTSTATUS
NtUnmapViewOfSection (
    IN HANDLE ProcessHandle,
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies the base address of the view.

Return Value:

    NTSTATUS.

--*/

{
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;

    PAGED_CODE();

    PreviousMode = KeGetPreviousMode();

    if ((PreviousMode == UserMode) && (BaseAddress > MM_HIGHEST_USER_ADDRESS)) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    Status = MiUnmapViewOfSection ( Process, BaseAddress, FALSE);
    ObDereferenceObject (Process);

    return Status;
}

NTSTATUS
MiUnmapViewOfSection (
    IN PEPROCESS Process,
    IN PVOID BaseAddress,
    IN LOGICAL AddressSpaceMutexHeld
    )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    Process - Supplies a referenced pointer to a process object.

    BaseAddress - Supplies the base address of the view.

    AddressSpaceMutexHeld - Supplies TRUE if the address space mutex is held.

Return Value:

    NTSTATUS.

--*/

{
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    SIZE_T RegionSize;
    PVOID UnMapImageBase;
    PVOID StartingVa;
    PVOID EndingVa;
    NTSTATUS status;
    LOGICAL Attached;
    KAPC_STATE ApcState;

    PAGED_CODE();

    Attached = FALSE;
    UnMapImageBase = NULL;

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (PsGetCurrentProcess() != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be removed.  Raise IRQL to block APCs.
    //

    if (AddressSpaceMutexHeld == FALSE) {
        LOCK_ADDRESS_SPACE (Process);
    }

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    //
    // Find the associated vad.
    //

    Vad = MiLocateAddress (BaseAddress);

    if ((Vad == NULL) || (Vad->u.VadFlags.PrivateMemory)) {

        //
        // No Virtual Address Descriptor located for Base Address.
        //

        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        status = STATUS_NOT_MAPPED_VIEW;
        goto ErrorReturn;
    }

    StartingVa = MI_VPN_TO_VA (Vad->StartingVpn);
    EndingVa = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

    //
    // If this Vad is for an image section, then
    // get the base address of the section.
    //

    ASSERT (Process == PsGetCurrentProcess());

    if (Vad->u.VadFlags.ImageMap == 1) {
        UnMapImageBase = StartingVa;
    }

    RegionSize = PAGE_SIZE + ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT);

    if (Vad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is being made to delete a secured VAD, check
        // the whole VAD to see if this deletion is allowed.
        //

        status = MiCheckSecuredVad (Vad,
                                    StartingVa,
                                    RegionSize - 1,
                                    MM_SECURE_DELETE_CHECK);

        if (!NT_SUCCESS (status)) {
            if (AddressSpaceMutexHeld == FALSE) {
                UNLOCK_ADDRESS_SPACE (Process);
            }
            goto ErrorReturn;
        }
    }

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);

    LOCK_WS_UNSAFE (Process);

    MiRemoveVad (Vad);

    //
    // Return commitment for page table pages if possible.
    //

    MiReturnPageTablePageCommitment (StartingVa,
                                     EndingVa,
                                     Process,
                                     PreviousVad,
                                     NextVad);

    MiRemoveMappedView (Process, Vad);

    UNLOCK_WS_UNSAFE (Process);

#if defined(_MIALT4K_)

    if (Process->Wow64Process != NULL) {
        MiDeleteFor4kPage (StartingVa, EndingVa, Process);
    }

#endif

    //
    // Update the current virtual size in the process header.
    //

    Process->VirtualSize -= RegionSize;
    if (AddressSpaceMutexHeld == FALSE) {
        UNLOCK_ADDRESS_SPACE (Process);
    }

    ExFreePool (Vad);
    status = STATUS_SUCCESS;

ErrorReturn:

    if (UnMapImageBase) {
        DbgkUnMapViewOfSection (UnMapImageBase);
    }
    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    return status;
}

NTSTATUS
MmUnmapViewOfSection (
    IN PEPROCESS Process,
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This function unmaps a previously created view to a section.

Arguments:

    Process - Supplies a referenced pointer to a process object.

    BaseAddress - Supplies the base address of the view.

Return Value:

    NTSTATUS.

--*/

{
    return MiUnmapViewOfSection (Process, BaseAddress, FALSE);
}

VOID
MiDecrementSubsections (
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection OPTIONAL
    )
/*++

Routine Description:

    This function decrements the subsections, inserting them on the unused
    subsection list if they qualify.

Arguments:

    FirstSubsection - Supplies the subsection to start at.

    LastSubsection - Supplies the last subsection to insert.  Supplies NULL
                     to decrement all the subsections in the chain.

Return Value:

    None.

Environment:

    PFN lock held.

--*/
{
    PMSUBSECTION MappedSubsection;

    ASSERT ((FirstSubsection->ControlArea->u.Flags.Image == 0) &&
            (FirstSubsection->ControlArea->FilePointer != NULL) &&
            (FirstSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {
        MappedSubsection = (PMSUBSECTION) FirstSubsection;

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if (ARGUMENT_PRESENT (LastSubsection)) {
            if (FirstSubsection == LastSubsection) {
                break;
            }
        }
        else {
            if (FirstSubsection->NextSubsection == NULL) {
                break;
            }
        }

        FirstSubsection = FirstSubsection->NextSubsection;
    } while (TRUE);
}


VOID
MiRemoveMappedView (
    IN PEPROCESS CurrentProcess,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function removes the mapping from the current process's
    address space.  The physical VAD may be a normal mapping (backed by
    a control area) or it may have no control area (it was mapped by a driver).

Arguments:

    Process - Supplies a referenced pointer to the current process object.

    Vad - Supplies the VAD which maps the view.

Return Value:

    None.

Environment:

    APC level, working set mutex and address creation mutex held.

    NOTE:  THE WORKING SET MUTEXES MAY BE RELEASED THEN REACQUIRED!!!!

           SINCE MiCheckControlArea releases unsafe, the WS mutex must be
           acquired UNSAFE.

--*/

{
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPte;
    PFN_NUMBER PdePage;
    PVOID TempVa;
    MMPTE_FLUSH_LIST PteFlushList;
    PVOID UsedPageTableHandle;
    PMMPFN Pfn2;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
    PVOID UsedPageDirectoryHandle;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PVOID UsedPageDirectoryParentHandle;
#endif

    ControlArea = Vad->ControlArea;

    if (Vad->u.VadFlags.PhysicalMapping == 1) {

#if defined(_MIALT4K_)
        ASSERT (((PMMVAD_LONG)Vad)->AliasInformation == NULL);
#endif

        if (((PMMVAD_LONG)Vad)->u4.Banked != NULL) {
            ExFreePool (((PMMVAD_LONG)Vad)->u4.Banked);
        }

#ifdef LARGE_PAGES
        if (Vad->u.VadFlags.LargePages == 1) {

            //
            // Delete the subsection allocated to hold the large pages.
            //

            ExFreePool (Vad->FirstPrototypePte);
            Vad->FirstPrototypePte = NULL;
            KeFlushEntireTb (TRUE, FALSE);
            LOCK_PFN (OldIrql);
        }
        else {

#endif //LARGE_PAGES

            //
            // This is a physical memory view.  The pages map physical memory
            // and are not accounted for in the working set list or in the PFN
            // database.
            //

            MiPhysicalViewRemover (CurrentProcess, Vad);

            //
            // Set count so only flush entire TB operations are performed.
            //

            PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;

            PointerPde = MiGetPdeAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));

            LOCK_PFN (OldIrql);

            //
            // Remove the PTES from the address space.
            //

            PdePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MI_VPN_TO_VA (Vad->StartingVpn));

            while (PointerPte <= LastPte) {

                if (MiIsPteOnPdeBoundary (PointerPte)) {

                    PointerPde = MiGetPteAddress (PointerPte);
                    PdePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);

                    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));
                }

                //
                // Decrement the count of non-zero page table entries for this
                // page table.
                //

                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

                MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

                Pfn2 = MI_PFN_ELEMENT (PdePage);
                MiDecrementShareCountInline (Pfn2, PdePage);

                //
                // If all the entries have been eliminated from the previous
                // page table page, delete the page table page itself.  And if
                // this results in an empty page directory page, then delete
                // that too.
                //

                if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageTableHandle) == 0) {

                    TempVa = MiGetVirtualAddressMappedByPte(PointerPde);

                    PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;

#if (_MI_PAGING_LEVELS >= 3)
                    UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
    
                    MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif

                    MiDeletePte (PointerPde,
                                 TempVa,
                                 FALSE,
                                 CurrentProcess,
                                 (PMMPTE)NULL,
                                 &PteFlushList);

                    //
                    // Add back in the private page MiDeletePte subtracted.
                    //
    
                    CurrentProcess->NumberOfPrivatePages += 1;

#if (_MI_PAGING_LEVELS >= 3)

                    if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageDirectoryHandle) == 0) {
    
                        PointerPpe = MiGetPdeAddress(PointerPte);
                        TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
    
                        PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;
    
#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageDirectoryParentHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
    
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryParentHandle);
#endif

                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     CurrentProcess,
                                     (PMMPTE)NULL,
                                     &PteFlushList);

                        //
                        // Add back in the private page MiDeletePte subtracted.
                        //
        
                        CurrentProcess->NumberOfPrivatePages += 1;

#if (_MI_PAGING_LEVELS >= 4)

                        if (MI_GET_USED_PTES_FROM_HANDLE(UsedPageDirectoryParentHandle) == 0) {
    
                            PointerPxe = MiGetPpeAddress(PointerPte);
                            TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
    
                            PteFlushList.Count = MM_MAXIMUM_FLUSH_COUNT;
    
                            MiDeletePte (PointerPxe,
                                         TempVa,
                                         FALSE,
                                         CurrentProcess,
                                         NULL,
                                         &PteFlushList);

                            //
                            // Add back in the private page MiDeletePte subtracted.
                            //
        
                            CurrentProcess->NumberOfPrivatePages += 1;
                        }
#endif

                    }
#endif
                }
                PointerPte += 1;
            }
            KeFlushEntireTb (TRUE, FALSE);

#ifdef LARGE_PAGES
        }
#endif //LARGE_PAGES
    } else {

        if (Vad->u2.VadFlags2.ExtendableFile) {
            PMMEXTEND_INFO ExtendedInfo;
            PMMVAD_LONG VadLong;

            ExtendedInfo = NULL;
            VadLong = (PMMVAD_LONG) Vad;

            ExAcquireFastMutexUnsafe (&MmSectionBasedMutex);
            ASSERT (Vad->ControlArea->Segment->ExtendInfo == VadLong->u4.ExtendedInfo);
            VadLong->u4.ExtendedInfo->ReferenceCount -= 1;
            if (VadLong->u4.ExtendedInfo->ReferenceCount == 0) {
                ExtendedInfo = VadLong->u4.ExtendedInfo;
                VadLong->ControlArea->Segment->ExtendInfo = NULL;
            }
            ExReleaseFastMutexUnsafe (&MmSectionBasedMutex);
            if (ExtendedInfo != NULL) {
                ExFreePool (ExtendedInfo);
            }
        }

        FirstSubsection = NULL;

        if (Vad->u.VadFlags.ImageMap == 0) {

#if defined (_MIALT4K_)
            if ((Vad->u2.VadFlags2.LongVad == 1) &&
                (((PMMVAD_LONG)Vad)->AliasInformation != NULL)) {

                MiRemoveAliasedVads (CurrentProcess, Vad);
            }
#endif

            if (ControlArea->FilePointer != NULL) {

                if (Vad->u.VadFlags.Protection & MM_READWRITE) {

                    //
                    // Adjust the count of writable user mappings
                    // to support transactions.
                    //
    
                    InterlockedDecrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
                }

                FirstSubsection = (PSUBSECTION)1;
            }
        }

        LOCK_PFN (OldIrql);

        MiDeleteVirtualAddresses (MI_VPN_TO_VA (Vad->StartingVpn),
                                  MI_VPN_TO_VA_ENDING (Vad->EndingVpn),
                                  FALSE,
                                  Vad);

        if (FirstSubsection != NULL) {

            FirstSubsection = MiLocateSubsection (Vad, Vad->StartingVpn);

            //
            // Note LastSubsection may be NULL for extendable VADs when the
            // EndingVpn is past the end of the section.  In this case,
            // all the subsections can be safely decremented.
            //

            LastSubsection = MiLocateSubsection (Vad, Vad->EndingVpn);

            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            MiDecrementSubsections (FirstSubsection, LastSubsection);
        }
    }

    //
    // Only physical VADs mapped by drivers don't have control areas.
    // If this view has a control area, the view count must be decremented now.
    //

    if (ControlArea) {

        //
        // Decrement the count of the number of views for the
        // Segment object.  This requires the PFN lock to be held (it is
        // already).
        //
    
        ControlArea->NumberOfMappedViews -= 1;
        ControlArea->NumberOfUserReferences -= 1;
    
        //
        // Check to see if the control area (segment) should be deleted.
        // This routine releases the PFN lock.
        //
    
        MiCheckControlArea (ControlArea, CurrentProcess, OldIrql);
    }
    else {

        UNLOCK_PFN (OldIrql);

        //
        // Even though it says short VAD in VadFlags, it better be a long VAD.
        //

        ASSERT (Vad->u.VadFlags.PhysicalMapping == 1);
        ASSERT (((PMMVAD_LONG)Vad)->u4.Banked == NULL);
        ASSERT (Vad->ControlArea == NULL);
        ASSERT (Vad->FirstPrototypePte == NULL);
    }
    
    return;
}

VOID
MiPurgeImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process OPTIONAL
    )

/*++

Routine Description:

    This function locates subsections within an image section that
    contain global memory and resets the global memory back to
    the initial subsection contents.

    Note, that for this routine to be called the section is not
    referenced nor is it mapped in any process.

Arguments:

    ControlArea - Supplies a pointer to the control area for the section.

    Process - Supplies a pointer to the process IFF the working set mutex
              is held, else NULL is supplied.  Note that IFF the working set
              mutex is held, it must always be acquired unsafe.

Return Value:

    None.

Environment:

    PFN LOCK held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageTableFrameIndex;
    MMPTE PteContents;
    MMPTE NewContents;
    MMPTE NewContentsDemandZero;
    KIRQL OldIrql;
    ULONG i;
    ULONG SizeOfRawData;
    ULONG OffsetIntoSubsection;
    PSUBSECTION Subsection;
#if DBG
    ULONG DelayCount = 0;
#endif //DBG

    ASSERT (ControlArea->u.Flags.Image != 0);

    OldIrql = APC_LEVEL;

    i = ControlArea->NumberOfSubsections;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Loop through all the subsections

    while (i > 0) {

        if (Subsection->u.SubsectionFlags.GlobalMemory == 1) {

            NewContents.u.Long = 0;
            NewContentsDemandZero.u.Long = 0;
            SizeOfRawData = 0;
            OffsetIntoSubsection = 0;

            //
            // Purge this section.
            //

            if (Subsection->StartingSector != 0) {

                //
                // This is not a demand zero section.
                //

                NewContents.u.Long = MiGetSubsectionAddressForPte(Subsection);
                NewContents.u.Soft.Prototype = 1;

                SizeOfRawData = (Subsection->NumberOfFullSectors << MMSECTOR_SHIFT) |
                               Subsection->u.SubsectionFlags.SectorEndOffset;
            }

            NewContents.u.Soft.Protection =
                                       Subsection->u.SubsectionFlags.Protection;
            NewContentsDemandZero.u.Soft.Protection =
                                        NewContents.u.Soft.Protection;

            PointerPte = Subsection->SubsectionBase;
            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
            ControlArea = Subsection->ControlArea;

            //
            // The WS lock may be released and reacquired and our callers
            // always acquire it unsafe.
            //

            MiMakeSystemAddressValidPfnWs (PointerPte, Process);

            while (PointerPte < LastPte) {

                if (MiIsPteOnPdeBoundary(PointerPte)) {

                    //
                    // We are on a page boundary, make sure this PTE is resident.
                    //

                    MiMakeSystemAddressValidPfnWs (PointerPte, Process);
                }

                PteContents = *PointerPte;
                if (PteContents.u.Long == 0) {

                    //
                    // No more valid PTEs to deal with.
                    //

                    break;
                }

                ASSERT (PteContents.u.Hard.Valid == 0);

                if ((PteContents.u.Soft.Prototype == 0) &&
                         (PteContents.u.Soft.Transition == 1)) {

                    //
                    // The prototype PTE is in transition format.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                    //
                    // If the prototype PTE is no longer pointing to
                    // the original image page (not in protopte format),
                    // or has been modified, remove it from memory.
                    //

                    if ((Pfn1->u3.e1.Modified == 1) ||
                        (Pfn1->OriginalPte.u.Soft.Prototype == 0)) {
                        ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                        //
                        // This is a transition PTE which has been
                        // modified or is no longer in protopte format.
                        //

                        if (Pfn1->u3.e2.ReferenceCount != 0) {

                            //
                            // There must be an I/O in progress on this
                            // page.  Wait for the I/O operation to complete.
                            //

                            UNLOCK_PFN (OldIrql);

                            //
                            // Drain the deferred lists as these pages may be
                            // sitting in there right now.
                            //

                            MiDeferredUnlockPages (0);

                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

                            //
                            // Redo the loop.
                            //
#if DBG
                            if ((DelayCount % 1024) == 0) {
                                DbgPrint("MMFLUSHSEC: waiting for i/o to complete PFN %p\n",
                                    Pfn1);
                            }
                            DelayCount += 1;
#endif //DBG

                            LOCK_PFN (OldIrql);

                            MiMakeSystemAddressValidPfnWs (PointerPte, Process);
                            continue;
                        }

                        ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                           (Pfn1->OriginalPte.u.Soft.Transition == 1)));

                        MI_WRITE_INVALID_PTE (PointerPte, Pfn1->OriginalPte);
                        ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                        //
                        // Only reduce the number of PFN references if
                        // the original PTE is still in prototype PTE
                        // format.
                        //

                        if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {
                            ControlArea->NumberOfPfnReferences -= 1;
                            ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);
                        }
                        MiUnlinkPageFromList (Pfn1);

                        MI_SET_PFN_DELETED (Pfn1);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // If the reference count for the page is zero, insert
                        // it into the free page list, otherwise leave it alone
                        // and when the reference count is decremented to zero
                        // the page will go to the free list.
                        //

                        if (Pfn1->u3.e2.ReferenceCount == 0) {
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                        }

                        MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                    }
                } else {

                    //
                    // Prototype PTE is not in transition format.
                    //

                    if (PteContents.u.Soft.Prototype == 0) {

                        //
                        // This refers to a page in the paging file,
                        // as it no longer references the image,
                        // restore the PTE contents to what they were
                        // at the initial image creation.
                        //

                        if (PteContents.u.Long != NoAccessPte.u.Long) {
                            MiReleasePageFileSpace (PteContents);
                            MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                        }
                    }
                }
                PointerPte += 1;
                OffsetIntoSubsection += PAGE_SIZE;

                if (OffsetIntoSubsection >= SizeOfRawData) {

                    //
                    // There are trailing demand zero pages in this
                    // subsection, set the PTE contents to be demand
                    // zero for the remainder of the PTEs in this
                    // subsection.
                    //

                    NewContents = NewContentsDemandZero;
                }

#if DBG
                DelayCount = 0;
#endif //DBG

            } //end while
        }

        i -=1;
        Subsection += 1;
     }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\vadtree.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    vadtree.c

Abstract:

    This module contains the routine to manipulate the virtual address
    descriptor tree.

Author:

    Lou Perazzoli (loup) 19-May-1989
    Landy Wang (landyw) 02-June-1997

Environment:

    Kernel mode only, working set mutex held, APCs disabled.

Revision History:

--*/

#include "mi.h"

VOID
VadTreeWalk (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiInsertVad)
#pragma alloc_text(PAGE,MiRemoveVad)
#pragma alloc_text(PAGE,MiFindEmptyAddressRange)
#pragma alloc_text(PAGE, MmPerfVadTreeWalk)
#if DBG
#pragma alloc_text(PAGE,VadTreeWalk)
#endif
#endif

NTSTATUS
MiInsertVad (
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function inserts a virtual address descriptor into the tree and
    reorders the splay tree as appropriate.

Arguments:

    Vad - Supplies a pointer to a virtual address descriptor.

Return Value:

    NTSTATUS.

--*/

{
    ULONG StartBit;
    ULONG EndBit;
    PMMADDRESS_NODE *Root;
    PEPROCESS CurrentProcess;
    SIZE_T RealCharge;
    SIZE_T PageCharge;
    SIZE_T PagesReallyCharged;
    ULONG FirstPage;
    ULONG LastPage;
    SIZE_T PagedPoolCharge;
    LOGICAL ChargedJobCommit;
    NTSTATUS Status;
    RTL_BITMAP VadBitMap;
#if (_MI_PAGING_LEVELS >= 3)
    ULONG FirstPdPage;
    ULONG LastPdPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    ULONG FirstPpPage;
    ULONG LastPpPage;
#endif

    ASSERT (Vad->EndingVpn >= Vad->StartingVpn);

    CurrentProcess = PsGetCurrentProcess();

    //
    // Commit charge of MAX_COMMIT means don't charge quota.
    //

    if (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT) {

        PageCharge = 0;
        PagedPoolCharge = 0;
        ChargedJobCommit = FALSE;

        //
        // Charge quota for the nonpaged pool for the VAD.  This is
        // done here rather than by using ExAllocatePoolWithQuota
        // so the process object is not referenced by the quota charge.
        //

        Status = PsChargeProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));
        if (!NT_SUCCESS(Status)) {
            return STATUS_COMMITMENT_LIMIT;
        }

        //
        // Charge quota for the prototype PTEs if this is a mapped view.
        //

        if ((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) {

            PagedPoolCharge =
              (Vad->EndingVpn - Vad->StartingVpn + 1) << PTE_SHIFT;

            Status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                                    PagedPoolCharge);

            if (!NT_SUCCESS(Status)) {
                PagedPoolCharge = 0;
                RealCharge = 0;
                goto Failed;
            }
        }

        //
        // Add in the charge for page table pages.
        //

        FirstPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPage <= LastPage) {

            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                               FirstPage)) {
                PageCharge += 1;
            }
            FirstPage += 1;
        }

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Add in the charge for page directory parent pages.
        //

        FirstPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPpPage <= LastPpPage) {

            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                               FirstPpPage)) {
                PageCharge += 1;
            }
            FirstPpPage += 1;
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Add in the charge for page directory pages.
        //

        FirstPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->StartingVpn));
        LastPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->EndingVpn));

        while (FirstPdPage <= LastPdPage) {

            if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                               FirstPdPage)) {
                PageCharge += 1;
            }
            FirstPdPage += 1;
        }
#endif

        RealCharge = Vad->u.VadFlags.CommitCharge + PageCharge;

        if (RealCharge != 0) {

            Status = PsChargeProcessPageFileQuota (CurrentProcess, RealCharge);
            if (!NT_SUCCESS (Status)) {
                RealCharge = 0;
                goto Failed;
            }

            if (CurrentProcess->CommitChargeLimit) {
                if (CurrentProcess->CommitCharge + RealCharge > CurrentProcess->CommitChargeLimit) {
                    if (CurrentProcess->Job) {
                        PsReportProcessMemoryLimitViolation ();
                    }
                    goto Failed;
                }
            }
            if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                if (PsChangeJobMemoryUsage(RealCharge) == FALSE) {
                    goto Failed;
                }
                ChargedJobCommit = TRUE;
            }

            if (MiChargeCommitment (RealCharge, CurrentProcess) == FALSE) {
                goto Failed;
            }

            CurrentProcess->CommitCharge += RealCharge;
            if (CurrentProcess->CommitCharge > CurrentProcess->CommitChargePeak) {
                CurrentProcess->CommitChargePeak = CurrentProcess->CommitCharge;
            }

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (RealCharge);

            ASSERT (RealCharge == Vad->u.VadFlags.CommitCharge + PageCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD, Vad->u.VadFlags.CommitCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD_PT, PageCharge);
        }

        if (PageCharge != 0) {

            //
            // Since the commitment was successful, charge the page
            // table pages.
            //

            PagesReallyCharged = 0;

            FirstPage = MiGetPdeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPage <= LastPage) {

                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                                   FirstPage)) {
                    MI_SET_BIT (MmWorkingSetList->CommittedPageTables,
                                FirstPage);
                    MmWorkingSetList->NumberOfCommittedPageTables += 1;

                    ASSERT32 (MmWorkingSetList->NumberOfCommittedPageTables <
                                                 PD_PER_SYSTEM * PDE_PER_PAGE);
                    PagesReallyCharged += 1;
                }
                FirstPage += 1;
            }

#if (_MI_PAGING_LEVELS >= 3)

            //
            // Charge the page directory pages.
            //

            FirstPdPage = MiGetPpeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPdPage <= LastPdPage) {

                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                                   FirstPdPage)) {

                    MI_SET_BIT (MmWorkingSetList->CommittedPageDirectories,
                                FirstPdPage);
                    MmWorkingSetList->NumberOfCommittedPageDirectories += 1;
                    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectories <
                                                                 PDE_PER_PAGE);
                    PagesReallyCharged += 1;
                }
                FirstPdPage += 1;
            }
#endif

#if (_MI_PAGING_LEVELS >= 4)

            //
            // Charge the page directory parent pages.
            //

            FirstPpPage = MiGetPxeIndex (MI_VPN_TO_VA (Vad->StartingVpn));

            while (FirstPpPage <= LastPpPage) {

                if (!MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                   FirstPpPage)) {

                    MI_SET_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                FirstPpPage);
                    MmWorkingSetList->NumberOfCommittedPageDirectoryParents += 1;
                    ASSERT (MmWorkingSetList->NumberOfCommittedPageDirectoryParents <
                                                                 PDE_PER_PAGE);
                    PagesReallyCharged += 1;
                }
                FirstPpPage += 1;
            }
#endif

            ASSERT (PageCharge == PagesReallyCharged);
        }
    }

    Root = (PMMADDRESS_NODE *)&CurrentProcess->VadRoot;

    //
    // Set the relevant fields in the Vad bitmap.
    //

    StartBit = (ULONG)(((ULONG_PTR) MI_64K_ALIGN (MI_VPN_TO_VA (Vad->StartingVpn))) / X64K);
    EndBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (MI_VPN_TO_VA (Vad->EndingVpn))) / X64K);

    //
    // Initialize the bitmap inline for speed.
    //

    VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
    VadBitMap.Buffer = VAD_BITMAP_SPACE;

    //
    // Note VADs like the PEB & TEB start on page (not 64K) boundaries so
    // for these, the relevant bits may already be set.
    //

#if defined (_WIN64) || defined (_X86PAE_)
    if (EndBit > MiLastVadBit) {
        EndBit = MiLastVadBit;
    }

    //
    // Only the first (PAGE_SIZE*8*64K) of VA space on NT64 is bitmapped.
    //

    if (StartBit <= MiLastVadBit) {
        RtlSetBits (&VadBitMap, StartBit, EndBit - StartBit + 1);
    }
#else
    RtlSetBits (&VadBitMap, StartBit, EndBit - StartBit + 1);
#endif

    if (MmWorkingSetList->VadBitMapHint == StartBit) {
        MmWorkingSetList->VadBitMapHint = EndBit + 1;
    }

    //
    // Set the hint field in the process to this Vad.
    //

    CurrentProcess->VadHint = Vad;

    if (CurrentProcess->VadFreeHint != NULL) {
        if (((ULONG)((PMMVAD)CurrentProcess->VadFreeHint)->EndingVpn +
                MI_VA_TO_VPN (X64K)) >=
                Vad->StartingVpn) {
            CurrentProcess->VadFreeHint = Vad;
        }
    }

    MiInsertNode ((PMMADDRESS_NODE)Vad, Root);
    CurrentProcess->NumberOfVads += 1;
    return STATUS_SUCCESS;

Failed:

    //
    // Return any quotas charged thus far.
    //

    PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));

    if (PagedPoolCharge != 0) {
        PsReturnProcessPagedPoolQuota (CurrentProcess, PagedPoolCharge);
    }

    if (RealCharge != 0) {
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
    }

    if (ChargedJobCommit == TRUE) {
        PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
    }

    return STATUS_COMMITMENT_LIMIT;
}


VOID
MiRemoveVad (
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function removes a virtual address descriptor from the tree and
    reorders the splay tree as appropriate.  If any quota or commitment
    was charged by the VAD (as indicated by the CommitCharge field) it
    is released.

Arguments:

    Vad - Supplies a pointer to a virtual address descriptor.

Return Value:

    None.

--*/

{
    PMMADDRESS_NODE *Root;
    PEPROCESS CurrentProcess;
    SIZE_T RealCharge;
    PLIST_ENTRY Next;
    PMMSECURE_ENTRY Entry;

    CurrentProcess = PsGetCurrentProcess();

#if defined(_MIALT4K_)
    if (((Vad->u.VadFlags.PrivateMemory) && (Vad->u.VadFlags.NoChange == 0)) 
        ||
        (Vad->u2.VadFlags2.LongVad == 0)) {

        NOTHING;
    }
    else {
        ASSERT ((((PMMVAD_LONG)Vad)->AliasInformation == NULL) || (CurrentProcess->Wow64Process != NULL));
    }
#endif

    //
    // Commit charge of MAX_COMMIT means don't charge quota.
    //

    if (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT) {

        //
        // Return the quota charge to the process.
        //

        PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMVAD));

        if ((Vad->u.VadFlags.PrivateMemory == 0) &&
            (Vad->ControlArea != NULL)) {
            PsReturnProcessPagedPoolQuota (CurrentProcess,
                                           (Vad->EndingVpn - Vad->StartingVpn + 1) << PTE_SHIFT);
        }

        RealCharge = Vad->u.VadFlags.CommitCharge;

        if (RealCharge != 0) {

            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);

            if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                (Vad->ControlArea != NULL)) {

#if 0 //commented out so page file quota is meaningful.
                if (Vad->ControlArea->FilePointer == NULL) {

                    //
                    // Don't release commitment for the page file space
                    // occupied by a page file section.  This will be charged
                    // as the shared memory is committed.
                    //

                    RealCharge -= BYTES_TO_PAGES ((ULONG)Vad->EndingVa -
                                                   (ULONG)Vad->StartingVa);
                }
#endif
            }

            MiReturnCommitment (RealCharge);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_VAD, RealCharge);
            if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                PsChangeJobMemoryUsage(-(SSIZE_T)RealCharge);
            }
            CurrentProcess->CommitCharge -= RealCharge;

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - RealCharge);
        }
    }

    if (Vad == CurrentProcess->VadFreeHint) {
        CurrentProcess->VadFreeHint = MiGetPreviousVad (Vad);
    }

    Root = (PMMADDRESS_NODE *)&CurrentProcess->VadRoot;

    MiRemoveNode ( (PMMADDRESS_NODE)Vad, Root);

    ASSERT (CurrentProcess->NumberOfVads >= 1);
    CurrentProcess->NumberOfVads -= 1;

    if (Vad->u.VadFlags.NoChange) {
        if (Vad->u2.VadFlags2.MultipleSecured) {

           //
           // Free the oustanding pool allocations.
           //

            Next = ((PMMVAD_LONG) Vad)->u3.List.Flink;
            do {
                Entry = CONTAINING_RECORD( Next,
                                           MMSECURE_ENTRY,
                                           List);

                Next = Entry->List.Flink;
                ExFreePool (Entry);
            } while (Next != &((PMMVAD_LONG)Vad)->u3.List);
        }
    }

    //
    // If the VadHint was the removed Vad, change the Hint.

    if (CurrentProcess->VadHint == Vad) {
        CurrentProcess->VadHint = CurrentProcess->VadRoot;
    }

    return;
}

PMMVAD
FASTCALL
MiLocateAddress (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    The function locates the virtual address descriptor which describes
    a given address.

Arguments:

    VirtualAddress - Supplies the virtual address to locate a descriptor
                     for.

Return Value:

    Returns a pointer to the virtual address descriptor which contains
    the supplied virtual address or NULL if none was located.

--*/

{
    PMMVAD FoundVad;
    PEPROCESS CurrentProcess;
    ULONG_PTR Vpn;

    CurrentProcess = PsGetCurrentProcess();

    if (CurrentProcess->VadHint == NULL) {
        return NULL;
    }

    Vpn = MI_VA_TO_VPN (VirtualAddress);
    if ((Vpn >= ((PMMADDRESS_NODE)CurrentProcess->VadHint)->StartingVpn) &&
        (Vpn <= ((PMMADDRESS_NODE)CurrentProcess->VadHint)->EndingVpn)) {

        return (PMMVAD)CurrentProcess->VadHint;
    }

    FoundVad = (PMMVAD)MiLocateAddressInTree ( Vpn,
                   (PMMADDRESS_NODE *)&(CurrentProcess->VadRoot));

    if (FoundVad != NULL) {
        CurrentProcess->VadHint = (PVOID)FoundVad;
    }
    return FoundVad;
}

NTSTATUS
MiFindEmptyAddressRange (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN ULONG QuickCheck,
    IN PVOID *Base
    )

/*++

Routine Description:

    The function examines the virtual address descriptors to locate
    an unused range of the specified size and returns the starting
    address of the range.

Arguments:

    SizeOfRange - Supplies the size in bytes of the range to locate.

    Alignment - Supplies the alignment for the address.  Must be
                 a power of 2 and greater than the page_size.

    QuickCheck - Supplies a zero if a quick check for free memory
                 after the VadFreeHint exists, non-zero if checking
                 should start at the lowest address.

    Base - Receives the starting address of a suitable range on success.

Return Value:

    NTSTATUS.

--*/

{
    ULONG FirstBitValue;
    ULONG StartPosition;
    ULONG BitsNeeded;
    PMMVAD NextVad;
    PMMVAD FreeHint;
    PEPROCESS CurrentProcess;
    PVOID StartingVa;
    PVOID EndingVa;
    NTSTATUS Status;
    RTL_BITMAP VadBitMap;

    CurrentProcess = PsGetCurrentProcess();

    if (QuickCheck == 0) {
                    
        //
        // Initialize the bitmap inline for speed.
        //

        VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
        VadBitMap.Buffer = VAD_BITMAP_SPACE;

        //
        // Skip the first bit here as we don't generally recommend
        // that applications map virtual address zero.
        //

        FirstBitValue = *((PULONG)VAD_BITMAP_SPACE);

        *((PULONG)VAD_BITMAP_SPACE) = (FirstBitValue | 0x1);

        BitsNeeded = (ULONG) ((MI_ROUND_TO_64K (SizeOfRange)) / X64K);

        StartPosition = RtlFindClearBits (&VadBitMap,
                                          BitsNeeded,
                                          MmWorkingSetList->VadBitMapHint);

        if (FirstBitValue & 0x1) {
            FirstBitValue = (ULONG)-1;
        }
        else {
            FirstBitValue = (ULONG)~0x1;
        }

        *((PULONG)VAD_BITMAP_SPACE) &= FirstBitValue;

        if (StartPosition != NO_BITS_FOUND) {
            *Base = (PVOID) (((ULONG_PTR)StartPosition) * X64K);
#if DBG
            if (MiCheckForConflictingVad (CurrentProcess, *Base, (ULONG_PTR)*Base + SizeOfRange - 1) != NULL) {
                DbgPrint ("MiFindEmptyAddressRange: overlapping VAD %p %p\n", *Base, SizeOfRange);
                DbgBreakPoint ();
            }
#endif
            return STATUS_SUCCESS;
        }

        FreeHint = CurrentProcess->VadFreeHint;

        if (FreeHint != NULL) {

            EndingVa = MI_VPN_TO_VA_ENDING (FreeHint->EndingVpn);
            NextVad = MiGetNextVad (FreeHint);

            if (NextVad == NULL) {

                if (SizeOfRange <
                    (((ULONG_PTR)MM_HIGHEST_USER_ADDRESS + 1) -
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa, Alignment))) {
                    *Base = (PVOID) MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,
                                                         Alignment);
                    return STATUS_SUCCESS;
                }
            }
            else {
                StartingVa = MI_VPN_TO_VA (NextVad->StartingVpn);

                if (SizeOfRange <
                    ((ULONG_PTR)StartingVa -
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa, Alignment))) {

                    //
                    // Check to ensure that the ending address aligned upwards
                    // is not greater than the starting address.
                    //

                    if ((ULONG_PTR)StartingVa >
                         MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,Alignment)) {

                        *Base = (PVOID)MI_ROUND_TO_SIZE((ULONG_PTR)EndingVa,
                                                           Alignment);
                        return STATUS_SUCCESS;
                    }
                }
            }
        }
    }

    Status = MiFindEmptyAddressRangeInTree (
                   SizeOfRange,
                   Alignment,
                   (PMMADDRESS_NODE)(CurrentProcess->VadRoot),
                   (PMMADDRESS_NODE *)&CurrentProcess->VadFreeHint,
                   Base);

    return Status;
}

#if DBG
VOID
VadTreeWalk (
    VOID
    )

{
    NodeTreeWalk ( (PMMADDRESS_NODE)(PsGetCurrentProcess()->VadRoot));

    return;
}
#endif

LOGICAL
MiCheckForConflictingVadExistence (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    The function determines if any addresses between a given starting and
    ending address is contained within a virtual address descriptor.

Arguments:

    StartingAddress - Supplies the virtual address to locate a containing
                      descriptor.

    EndingAddress - Supplies the virtual address to locate a containing
                      descriptor.

Return Value:

    TRUE if the VAD if found, FALSE if not.

Environment:

    Kernel mode, process address creation mutex held.

--*/

{
#if 0
    ULONG StartBit;
    ULONG EndBit;

    if (MiLastVadBit != 0) {

        StartBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (StartingAddress)) / X64K);
        EndBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (EndingAddress)) / X64K);

        ASSERT (StartBit <= EndBit);
        if (EndBit > MiLastVadBit) {
            ASSERT (FALSE);
            EndBit = MiLastVadBit;
            if (StartBit > MiLastVadBit) {
                StartBit = MiLastVadBit;
            }
        }

        while (StartBit <= EndBit) {
            if (MI_CHECK_BIT (((PULONG)VAD_BITMAP_SPACE), StartBit) != 0) {
                return TRUE;
            }
            StartBit += 1;
        }

        ASSERT (MiCheckForConflictingVad (Process, StartingAddress, EndingAddress) == NULL);
        return FALSE;
    }
#endif

    if (MiCheckForConflictingVad (Process, StartingAddress, EndingAddress) != NULL) {
        return TRUE;
    }

    return FALSE;
}

PFILE_OBJECT *
MmPerfVadTreeWalk (
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine walks through the VAD tree to find all files mapped
    into the specified process.  It returns a pointer to a pool allocation 
    containing the referenced file object pointers.

Arguments:

    Process - Supplies the process to walk.

Return Value:

    Returns a pointer to a NULL terminated pool allocation containing 
    the file object pointers which have been referenced in the process, 
    NULL if the memory could not be allocated.

    It is also the responsibility of the caller to dereference each
    file object in the list and then free the returned pool.

Environment:

    PASSIVE_LEVEL, arbitrary thread context.

--*/
{
    PMMVAD Vad;
    PMMVAD NextVad;
    ULONG VadCount;
    PFILE_OBJECT *File;
    PFILE_OBJECT *FileObjects;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    
    LOCK_ADDRESS_SPACE(Process);

    Vad = Process->VadRoot;

    if (Vad == NULL) {
        ASSERT (Process->NumberOfVads == 0);
        UNLOCK_ADDRESS_SPACE (Process);
        return NULL;
    }

    ASSERT (Process->NumberOfVads != 0);

    //
    // Allocate one additional entry for the NULL terminator.
    //

    VadCount = Process->NumberOfVads + 1;

    FileObjects = (PFILE_OBJECT *) ExAllocatePoolWithTag (
                                            PagedPool,
                                            VadCount * sizeof(PFILE_OBJECT),
                                            '01pM');

    if (FileObjects == NULL) {
        UNLOCK_ADDRESS_SPACE (Process);
        return NULL;
    }

    File = FileObjects;

    while (Vad->LeftChild != NULL) {
        Vad = Vad->LeftChild;
    }

    if ((!Vad->u.VadFlags.PrivateMemory) &&
        (Vad->ControlArea != NULL) &&
        (Vad->ControlArea->FilePointer != NULL)) {

        *File = Vad->ControlArea->FilePointer;
        ObReferenceObject (*File);
        File += 1;
    }

    for (;;) {
        NextVad = (PMMVAD) MiGetNextNode ((PMMADDRESS_NODE)Vad);

        if (NextVad == NULL) {
            break;
        }

        Vad = (PMMVAD) NextVad;

        if ((!Vad->u.VadFlags.PrivateMemory) &&
            (Vad->ControlArea != NULL) &&
            (Vad->ControlArea->FilePointer != NULL)) {

            *File = Vad->ControlArea->FilePointer;
            ObReferenceObject (*File);
            File += 1;
        }

        Vad = NextVad;
    }

    ASSERT (File < FileObjects + VadCount);

    UNLOCK_ADDRESS_SPACE(Process);

    *File = NULL;

    return FileObjects;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\wrtfault.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   wrtfault.c

Abstract:

    This module contains the copy on write routine for memory management.

Author:

    Lou Perazzoli (loup) 10-Apr-1989

Revision History:

--*/

#include "mi.h"

LOGICAL
FASTCALL
MiCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    )

/*++

Routine Description:

    This routine performs a copy on write operation for the specified
    virtual address.

Arguments:

    FaultingAddress - Supplies the virtual address which caused the
                      fault.

    PointerPte - Supplies the pointer to the PTE which caused the
                 page fault.


Return Value:

    Returns TRUE if the page was actually split, FALSE if not.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NewPageIndex;
    PULONG CopyTo;
    PULONG CopyFrom;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PEPROCESS CurrentProcess;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    PVOID VirtualAddress;
    WSLE_NUMBER WorkingSetIndex;
    LOGICAL FakeCopyOnWrite;

    FakeCopyOnWrite = FALSE;

    CurrentProcess = PsGetCurrentProcess ();

    //
    // This is called from MmAccessFault, the PointerPte is valid
    // and the working set mutex ensures it cannot change state.
    //
    // Capture the PTE contents to TempPte.
    //

    TempPte = *PointerPte;

    //
    // Check to see if this is a prototype PTE with copy on write
    // enabled.
    //

    if (TempPte.u.Hard.CopyOnWrite == 0) {

        //
        // This is a fork page which is being made private in order
        // to change the protection of the page.
        // Do not make the page writable.
        //

        FakeCopyOnWrite = TRUE;
    }

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
    WorkingSetIndex = MiLocateWsle (VirtualAddress, MmWorkingSetList,
                        Pfn1->u1.WsIndex);

    //
    // The page must be copied into a new page.
    //

    //
    // If a fork operation is in progress, block until the fork is completed,
    // then retry the whole operation as the state of everything may have
    // changed between when the mutexes were released and reacquired.
    //

    if (CurrentProcess->ForkInProgress != NULL) {
        if (MiWaitForForkToComplete (CurrentProcess, FALSE) == TRUE) {
            return FALSE;
        }
    }

    LOCK_PFN (OldIrql);

    if (MiEnsureAvailablePageOrWait (CurrentProcess, NULL)) {

        //
        // A wait operation was performed to obtain an available
        // page and the working set mutex and PFN lock have
        // been released and various things may have changed for
        // the worse.  Rather than examine all the conditions again,
        // return and if things are still proper, the fault will
        // be taken again.
        //

        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

    //
    // Increment the number of private pages.
    //

    CurrentProcess->NumberOfPrivatePages += 1;

    MmInfoCounters.CopyOnWriteCount += 1;

    //
    // A page is being copied and made private, the global state of
    // the shared page needs to be updated at this point on certain
    // hardware.  This is done by ORing the dirty bit into the modify bit in
    // the PFN element.
    //

    MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

    //
    // This must be a prototype PTE.  Perform the copy on write.
    //

#if DBG
    if (Pfn1->u3.e1.PrototypePte == 0) {
        DbgPrint("writefault - PTE indicates cow but not protopte\n");
        MiFormatPte(PointerPte);
        MiFormatPfn(Pfn1);
    }
#endif

    CloneBlock = (PMMCLONE_BLOCK)Pfn1->PteAddress;

    //
    // Get a new page with the same color as this page.
    //

    NewPageIndex = MiRemoveAnyPage (
                    MI_PAGE_COLOR_PTE_PROCESS(PageFrameIndex,
                                              &CurrentProcess->NextPageColor));
    MiInitializeCopyOnWritePfn (NewPageIndex, PointerPte, WorkingSetIndex, NULL);

    UNLOCK_PFN (OldIrql);

    CopyTo = (PULONG)MiMapPageInHyperSpace (CurrentProcess, NewPageIndex, &OldIrql);

#if defined(_MIALT4K_)

    //
    // Avoid accessing user space as it may potentially 
    // cause a page fault on the alternate table.   
    //

    CopyFrom = KSEG_ADDRESS(PointerPte->u.Hard.PageFrameNumber);

#else
    CopyFrom = (PULONG)MiGetVirtualAddressMappedByPte (PointerPte);
#endif

    RtlCopyMemory ( CopyTo, CopyFrom, PAGE_SIZE);

    PERFINFO_PRIVATE_COPY_ON_WRITE(CopyFrom, PAGE_SIZE);

    MiUnmapPageInHyperSpace (CurrentProcess, CopyTo, OldIrql);

    if (!FakeCopyOnWrite) {

        //
        // If the page was really a copy on write page, make it
        // accessed, dirty and writable.  Also, clear the copy-on-write
        // bit in the PTE.
        //

        MI_SET_PTE_DIRTY (TempPte);
        TempPte.u.Hard.Write = 1;
        MI_SET_ACCESSED_IN_PTE (&TempPte, 1);
        TempPte.u.Hard.CopyOnWrite = 0;
        TempPte.u.Hard.PageFrameNumber = NewPageIndex;
    }
    else {

        //
        // The page was not really a copy on write, just change
        // the frame field of the PTE.
        //

        TempPte.u.Hard.PageFrameNumber = NewPageIndex;
    }

    //
    // If the modify bit is set in the PFN database for the
    // page, the data cache must be flushed.  This is due to the
    // fact that this process may have been cloned and the cache
    // still contains stale data destined for the page we are
    // going to remove.
    //

    ASSERT (TempPte.u.Hard.Valid == 1);

    LOCK_PFN (OldIrql);

    //
    // Flush the TB entry for this page.
    //

    KeFlushSingleTb (FaultingAddress,
                     TRUE,
                     FALSE,
                     (PHARDWARE_PTE)PointerPte,
                     TempPte.u.Flush);

    //
    // Decrement the share count for the page which was copied
    // as this PTE no longer refers to it.
    //

    MiDecrementShareCount (PageFrameIndex);

    CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);

    if (CloneDescriptor != NULL) {

        //
        // Decrement the reference count for the clone block,
        // note that this could release and reacquire the mutexes.
        //

        MiDecrementCloneBlockReference (CloneDescriptor,
                                        CloneBlock,
                                        CurrentProcess);
    }

    UNLOCK_PFN (OldIrql);
    return TRUE;
}


#if !defined(NT_UP) || defined (_IA64_)

VOID
MiSetDirtyBit (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN ULONG PfnHeld
    )

/*++

Routine Description:

    This routine sets dirty in the specified PTE and the modify bit in the
    corresponding PFN element.  If any page file space is allocated, it
    is deallocated.

Arguments:

    FaultingAddress - Supplies the faulting address.

    PointerPte - Supplies a pointer to the corresponding valid PTE.

    PfnHeld - Supplies TRUE if the PFN lock is already held.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, Working set mutex held.

--*/

{
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;

    //
    // The page is NOT copy on write, update the PTE setting both the
    // dirty bit and the accessed bit. Note, that as this PTE is in
    // the TB, the TB must be flushed.
    //

    TempPte = *PointerPte;
    MI_SET_PTE_DIRTY (TempPte);
    MI_SET_ACCESSED_IN_PTE (&TempPte, 1);
    MI_WRITE_VALID_PTE_NEW_PROTECTION(PointerPte, TempPte);

    //
    // Check state of PFN lock and if not held, don't update PFN database.
    //

    if (PfnHeld) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Set the modified field in the PFN database, also, if the physical
        // page is currently in a paging file, free up the page file space
        // as the contents are now worthless.
        //

        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                             (Pfn1->u3.e1.WriteInProgress == 0)) {

            //
            // This page is in page file format, deallocate the page file space.
            //

            MiReleasePageFileSpace (Pfn1->OriginalPte);

            //
            // Change original PTE to indicate no page file space is reserved,
            // otherwise the space will be deallocated when the PTE is
            // deleted.
            //

            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        MI_SET_MODIFIED (Pfn1, 1, 0x17);
    }

    //
    // The TB entry must be flushed as the valid PTE with the dirty bit clear
    // has been fetched into the TB. If it isn't flushed, another fault
    // is generated as the dirty bit is not set in the cached TB entry.
    //

    KeFillEntryTb ((PHARDWARE_PTE)PointerPte, FaultingAddress, TRUE);
    return;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\verifier.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   verifier.c

Abstract:

    This module contains the routines to verify the system kernel, HAL and
    drivers.

Author:

    Landy Wang (landyw) 3-Sep-1998

Revision History:

--*/
#include "mi.h"

#define THUNKED_API

THUNKED_API
PVOID
VerifierAllocatePool (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes
    );

THUNKED_API
PVOID
VerifierAllocatePoolWithTag (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    );

THUNKED_API
PVOID
VerifierAllocatePoolWithQuotaTag (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    );

THUNKED_API
PVOID
VerifierAllocatePoolWithTagPriority (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN EX_POOL_PRIORITY Priority
    );

PVOID
VeAllocatePoolWithTagPriority (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN EX_POOL_PRIORITY Priority,
    IN PVOID CallingAddress
    );

VOID
VerifierFreePool (
    IN PVOID P
    );

THUNKED_API
VOID
VerifierFreePoolWithTag (
    IN PVOID P,
    IN ULONG TagToFree
    );

THUNKED_API
LONG
VerifierSetEvent (
    IN PRKEVENT Event,
    IN KPRIORITY Increment,
    IN BOOLEAN Wait
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKfRaiseIrql (
    IN KIRQL NewIrql
    );

THUNKED_API
KIRQL
VerifierKeRaiseIrqlToDpcLevel (
    VOID
    );

THUNKED_API
VOID
FASTCALL
VerifierKfLowerIrql (
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
VerifierKeRaiseIrql (
    IN KIRQL NewIrql,
    OUT PKIRQL OldIrql
    );

THUNKED_API
VOID
VerifierKeLowerIrql (
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
VerifierKeAcquireSpinLock (
    IN PKSPIN_LOCK SpinLock,
    OUT PKIRQL OldIrql
    );

THUNKED_API
VOID
VerifierKeReleaseSpinLock (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
#if defined(_X86_)
FASTCALL
#endif
VerifierKeAcquireSpinLockAtDpcLevel (
    IN PKSPIN_LOCK SpinLock
    );

THUNKED_API
VOID
#if defined(_X86_)
FASTCALL
#endif
VerifierKeReleaseSpinLockFromDpcLevel (
    IN PKSPIN_LOCK SpinLock
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKfAcquireSpinLock (
    IN PKSPIN_LOCK SpinLock
    );

THUNKED_API
VOID
FASTCALL
VerifierKfReleaseSpinLock (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    );

#if !defined(_X86_)
THUNKED_API
KIRQL
VerifierKeAcquireSpinLockRaiseToDpc (
    IN PKSPIN_LOCK SpinLock
    );
#endif


THUNKED_API
VOID
VerifierKeInitializeTimerEx (
    IN PKTIMER Timer,
    IN TIMER_TYPE Type
    );

THUNKED_API
VOID
VerifierKeInitializeTimer (
    IN PKTIMER Timer
    );

THUNKED_API
BOOLEAN
FASTCALL
VerifierExTryToAcquireFastMutex (
    IN PFAST_MUTEX FastMutex
    );

THUNKED_API
VOID
FASTCALL
VerifierExAcquireFastMutex (
    IN PFAST_MUTEX FastMutex
    );

THUNKED_API
VOID
FASTCALL
VerifierExReleaseFastMutex (
    IN PFAST_MUTEX FastMutex
    );

THUNKED_API
VOID
FASTCALL
VerifierExAcquireFastMutexUnsafe (
    IN PFAST_MUTEX FastMutex
    );

THUNKED_API
VOID
FASTCALL
VerifierExReleaseFastMutexUnsafe (
    IN PFAST_MUTEX FastMutex
    );

THUNKED_API
BOOLEAN
VerifierExAcquireResourceExclusiveLite (
    IN PERESOURCE Resource,
    IN BOOLEAN Wait
    );

THUNKED_API
VOID
FASTCALL
VerifierExReleaseResourceLite (
    IN PERESOURCE Resource
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKeAcquireQueuedSpinLock (
    IN KSPIN_LOCK_QUEUE_NUMBER Number
    );

THUNKED_API
VOID
FASTCALL
VerifierKeReleaseQueuedSpinLock (
    IN KSPIN_LOCK_QUEUE_NUMBER Number,
    IN KIRQL OldIrql
    );

THUNKED_API
BOOLEAN
VerifierSynchronizeExecution (
    IN PKINTERRUPT Interrupt,
    IN PKSYNCHRONIZE_ROUTINE SynchronizeRoutine,
    IN PVOID SynchronizeContext
    );

THUNKED_API
VOID
VerifierProbeAndLockPages (
    IN OUT PMDL MemoryDescriptorList,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    );

THUNKED_API
VOID
VerifierProbeAndLockProcessPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PEPROCESS Process,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    );

THUNKED_API
VOID
VerifierProbeAndLockSelectedPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PFILE_SEGMENT_ELEMENT SegmentArray,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    );

VOID
VerifierUnlockPages (
     IN OUT PMDL MemoryDescriptorList
     );

VOID
VerifierUnmapLockedPages (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     );

VOID
VerifierUnmapIoSpace (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     );

THUNKED_API
PVOID
VerifierMapIoSpace (
    IN PHYSICAL_ADDRESS PhysicalAddress,
    IN SIZE_T NumberOfBytes,
    IN MEMORY_CACHING_TYPE CacheType
    );

THUNKED_API
PVOID
VerifierMapLockedPages (
    IN PMDL MemoryDescriptorList,
    IN KPROCESSOR_MODE AccessMode
    );

THUNKED_API
PVOID
VerifierMapLockedPagesSpecifyCache (
    IN PMDL MemoryDescriptorList,
    IN KPROCESSOR_MODE AccessMode,
    IN MEMORY_CACHING_TYPE CacheType,
    IN PVOID RequestedAddress,
    IN ULONG BugCheckOnFailure,
    IN MM_PAGE_PRIORITY Priority
    );

THUNKED_API
NTSTATUS
VerifierKeWaitForSingleObject (
    IN PVOID Object,
    IN KWAIT_REASON WaitReason,
    IN KPROCESSOR_MODE WaitMode,
    IN BOOLEAN Alertable,
    IN PLARGE_INTEGER Timeout OPTIONAL
    );

THUNKED_API
LONG
VerifierKeReleaseMutex (
    IN PRKMUTEX Mutex,
    IN BOOLEAN Wait
    );

THUNKED_API
VOID
VerifierKeInitializeMutex (
    IN PRKMUTEX Mutex,
    IN ULONG Level
    );

THUNKED_API
LONG
VerifierKeReleaseMutant(
    IN PRKMUTANT Mutant,
    IN KPRIORITY Increment,
    IN BOOLEAN Abandoned,
    IN BOOLEAN Wait
    );

THUNKED_API
VOID
VerifierKeInitializeMutant(
    IN PRKMUTANT Mutant,
    IN BOOLEAN InitialOwner
    );

THUNKED_API
VOID
VerifierKeInitializeSpinLock (
    IN PKSPIN_LOCK  SpinLock
    );

VOID
ViCheckMdlPages (
    IN PMDL MemoryDescriptorList,
    IN MEMORY_CACHING_TYPE CacheType
    );

VOID
ViFreeTrackedPool (
    IN PVOID VirtualAddress,
    IN SIZE_T ChargedBytes,
    IN LOGICAL CheckType,
    IN LOGICAL SpecialPool
    );

VOID
VerifierFreeTrackedPool (
    IN PVOID VirtualAddress,
    IN SIZE_T ChargedBytes,
    IN LOGICAL CheckType,
    IN LOGICAL SpecialPool
    );

VOID
ViPrintString (
    IN PUNICODE_STRING DriverName
    );

LOGICAL
ViInjectResourceFailure (
    VOID
    );

VOID
ViTrimAllSystemPagableMemory (
    VOID
    );

VOID
ViInitializeEntry (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN LOGICAL FirstLoad
    );

LOGICAL
ViReservePoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    );

ULONG_PTR
ViInsertPoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN PVOID VirtualAddress,
    IN PVOID CallingAddress,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    );

VOID
ViCancelPoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    );

VOID
ViReleasePoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN PVOID VirtualAddress,
    IN ULONG_PTR ListIndex,
    IN SIZE_T ChargedBytes
    );

VOID
KfSanityCheckRaiseIrql (
    IN KIRQL NewIrql
    );

VOID
KfSanityCheckLowerIrql (
    IN KIRQL NewIrql
    );

NTSTATUS
VerifierReferenceObjectByHandle (
    IN HANDLE Handle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_TYPE ObjectType OPTIONAL,
    IN KPROCESSOR_MODE AccessMode,
    OUT PVOID *Object,
    OUT POBJECT_HANDLE_INFORMATION HandleInformation OPTIONAL
    );

MM_DRIVER_VERIFIER_DATA MmVerifierData;

//
// Any flags which can be modified on the fly without rebooting are set here.
//

ULONG VerifierModifyableOptions;
ULONG VerifierOptionChanges;

LIST_ENTRY MiSuspectDriverList;

LOGICAL MiVerifyAllDrivers;

WCHAR MiVerifyRandomDrivers;

ULONG MiActiveVerifies;

ULONG MiActiveVerifierThunks;

ULONG MiNoPageOnRaiseIrql;

ULONG MiVerifierStackProtectTime;

LOGICAL MmDontVerifyRandomDrivers = TRUE;

LOGICAL VerifierSystemSufficientlyBooted;

LARGE_INTEGER VerifierRequiredTimeSinceBoot = {(ULONG)(40 * 1000 * 1000 * 10), 1};

LOGICAL VerifierIsTrackingPool = FALSE;

KSPIN_LOCK VerifierListLock;

KSPIN_LOCK VerifierPoolLock;

FAST_MUTEX VerifierPoolMutex;

PRTL_BITMAP VerifierLargePagedPoolMap;

LIST_ENTRY MiVerifierDriverAddedThunkListHead;

extern LOGICAL MmSpecialPoolCatchOverruns;

LOGICAL KernelVerifier = FALSE;

ULONG KernelVerifierTickPage = 0x7;

ULONG MiVerifierThunksAdded;

PDRIVER_VERIFIER_THUNK_ROUTINE
MiResolveVerifierExports (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PCHAR PristineName
    );

LOGICAL
MiEnableVerifier (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

LOGICAL
MiReEnableVerifier (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
ViInsertVerifierEntry (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    );

PVOID
ViPostPoolAllocation (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN POOL_TYPE PoolType,
    IN ULONG Tag,
    IN PVOID CallingAddress
    );

PMI_VERIFIER_DRIVER_ENTRY
ViLocateVerifierEntry (
    IN PVOID SystemAddress
    );

VOID
MiVerifierCheckThunks (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

//
// Track irqls functions
//

VOID
ViTrackIrqlInitialize (
    );

VOID
ViTrackIrqlLog (
    IN KIRQL CurrentIrql,
    IN KIRQL NewIrql
    );

//
// Fault injection stack trace log
//

#if defined(_X86_)

VOID
ViFaultTracesInitialize (
    );

VOID
ViFaultTracesLog (
    );

#endif



#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeDriverVerifierList)
#pragma alloc_text(INIT,MiInitializeVerifyingComponents)
#pragma alloc_text(INIT,ViTrackIrqlInitialize)
#pragma alloc_text(INIT,MiResolveVerifierExports)
#if defined(_X86_)
#pragma alloc_text(INIT,MiEnableKernelVerifier)
#pragma alloc_text(INIT,ViFaultTracesInitialize)
#endif
#pragma alloc_text(PAGE,MiApplyDriverVerifier)
#pragma alloc_text(PAGE,MiEnableVerifier)
#pragma alloc_text(INIT,MiReEnableVerifier)
#pragma alloc_text(PAGE,ViPrintString)
#pragma alloc_text(PAGE,MmGetVerifierInformation)
#pragma alloc_text(PAGE,MmSetVerifierInformation)
#pragma alloc_text(PAGE,MmAddVerifierThunks)
#pragma alloc_text(PAGE,MmIsVerifierEnabled)
#pragma alloc_text(PAGE,MiVerifierCheckThunks)
#pragma alloc_text(PAGEVRFY,MiVerifyingDriverUnloading)

#pragma alloc_text(PAGE,MmAddVerifierEntry)
#pragma alloc_text(PAGE,MmRemoveVerifierEntry)
#pragma alloc_text(INIT,MiReApplyVerifierToLoadedModules)
#pragma alloc_text(PAGEVRFY,VerifierProbeAndLockPages)
#pragma alloc_text(PAGEVRFY,VerifierProbeAndLockProcessPages)
#pragma alloc_text(PAGEVRFY,VerifierProbeAndLockSelectedPages)
#pragma alloc_text(PAGEVRFY,VerifierUnlockPages)
#pragma alloc_text(PAGEVRFY,VerifierMapIoSpace)
#pragma alloc_text(PAGEVRFY,VerifierMapLockedPages)
#pragma alloc_text(PAGEVRFY,VerifierMapLockedPagesSpecifyCache)
#pragma alloc_text(PAGEVRFY,VerifierUnmapLockedPages)
#pragma alloc_text(PAGEVRFY,VerifierUnmapIoSpace)
#pragma alloc_text(PAGEVRFY,VerifierAllocatePool)
#pragma alloc_text(PAGEVRFY,VerifierAllocatePoolWithTag)
#pragma alloc_text(PAGEVRFY,VerifierAllocatePoolWithTagPriority)
#pragma alloc_text(PAGEVRFY,VerifierAllocatePoolWithQuotaTag)
#pragma alloc_text(PAGEVRFY,VerifierFreePool)
#pragma alloc_text(PAGEVRFY,VerifierFreePoolWithTag)
#pragma alloc_text(PAGEVRFY,VerifierKeWaitForSingleObject)
#pragma alloc_text(PAGEVRFY,VerifierKfRaiseIrql)
#pragma alloc_text(PAGEVRFY,VerifierKeRaiseIrqlToDpcLevel)
#pragma alloc_text(PAGEVRFY,VerifierKfLowerIrql)
#pragma alloc_text(PAGEVRFY,VerifierKeRaiseIrql)
#pragma alloc_text(PAGEVRFY,VerifierKeLowerIrql)
#pragma alloc_text(PAGEVRFY,VerifierKeAcquireSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKeReleaseSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKeAcquireSpinLockAtDpcLevel)
#pragma alloc_text(PAGEVRFY,VerifierKeReleaseSpinLockFromDpcLevel)
#pragma alloc_text(PAGEVRFY,VerifierKfAcquireSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKfReleaseSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKeInitializeTimer)
#pragma alloc_text(PAGEVRFY,VerifierKeInitializeTimerEx)
#pragma alloc_text(PAGEVRFY,VerifierExTryToAcquireFastMutex)
#pragma alloc_text(PAGEVRFY,VerifierExAcquireFastMutex)
#pragma alloc_text(PAGEVRFY,VerifierExReleaseFastMutex)
#pragma alloc_text(PAGEVRFY,VerifierExAcquireFastMutexUnsafe)
#pragma alloc_text(PAGEVRFY,VerifierExReleaseFastMutexUnsafe)
#pragma alloc_text(PAGEVRFY,VerifierExAcquireResourceExclusiveLite)
#pragma alloc_text(PAGEVRFY,VerifierExReleaseResourceLite)
#pragma alloc_text(PAGEVRFY,VerifierKeAcquireQueuedSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKeReleaseQueuedSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierKeReleaseMutex)
#pragma alloc_text(PAGEVRFY,VerifierKeInitializeMutex)
#pragma alloc_text(PAGEVRFY,VerifierKeReleaseMutant)
#pragma alloc_text(PAGEVRFY,VerifierKeInitializeMutant)
#pragma alloc_text(PAGEVRFY,VerifierKeInitializeSpinLock)
#pragma alloc_text(PAGEVRFY,VerifierSynchronizeExecution)
#pragma alloc_text(PAGEVRFY,VerifierReferenceObjectByHandle)
#pragma alloc_text(PAGEVRFY,VerifierSetEvent)

#pragma alloc_text(PAGEVRFY,ViFreeTrackedPool)

#pragma alloc_text(PAGEVRFY,VeAllocatePoolWithTagPriority)
#pragma alloc_text(PAGEVRFY,ViCheckMdlPages)
#pragma alloc_text(PAGEVRFY,ViInsertVerifierEntry)
#pragma alloc_text(PAGEVRFY,ViLocateVerifierEntry)
#pragma alloc_text(PAGEVRFY,ViPostPoolAllocation)
#pragma alloc_text(PAGEVRFY,ViInjectResourceFailure)
#pragma alloc_text(PAGEVRFY,ViTrimAllSystemPagableMemory)
#pragma alloc_text(PAGEVRFY,ViInitializeEntry)
#pragma alloc_text(PAGEVRFY,ViReservePoolAllocation)
#pragma alloc_text(PAGEVRFY,ViInsertPoolAllocation)
#pragma alloc_text(PAGEVRFY,ViCancelPoolAllocation)
#pragma alloc_text(PAGEVRFY,ViReleasePoolAllocation)
#pragma alloc_text(PAGEVRFY,KfSanityCheckRaiseIrql)
#pragma alloc_text(PAGEVRFY,KfSanityCheckLowerIrql)
#pragma alloc_text(PAGEVRFY,ViTrackIrqlLog)

#if defined(_X86_)
#pragma alloc_text(PAGEVRFY,ViFaultTracesLog)
#endif

#if !defined(_X86_)
#pragma alloc_text(PAGEVRFY,VerifierKeAcquireSpinLockRaiseToDpc)
#endif

#endif

typedef struct _VERIFIER_THUNKS {
    union {
        PCHAR                           PristineRoutineAsciiName;

        //
        // The actual pristine routine address is derived from exports
        //

        PDRIVER_VERIFIER_THUNK_ROUTINE  PristineRoutine;
    };
    PDRIVER_VERIFIER_THUNK_ROUTINE  NewRoutine;
} VERIFIER_THUNKS, *PVERIFIER_THUNKS;

extern const VERIFIER_THUNKS MiVerifierThunks[];
extern const VERIFIER_THUNKS MiVerifierPoolThunks[];

#if defined (_X86_)

#define VI_KE_RAISE_IRQL            0
#define VI_KE_LOWER_IRQL            1
#define VI_KE_ACQUIRE_SPINLOCK      2
#define VI_KE_RELEASE_SPINLOCK      3
#define VI_KF_RAISE_IRQL            4
#define VI_KE_RAISE_IRQL_TO_DPC_LEVEL 5
#define VI_KF_LOWER_IRQL            6
#define VI_KF_ACQUIRE_SPINLOCK      7
#define VI_KF_RELEASE_SPINLOCK      8
#define VI_KE_ACQUIRE_QUEUED_SPINLOCK      9
#define VI_KE_RELEASE_QUEUED_SPINLOCK      10

#define VI_HALMAX                   11

PVOID MiKernelVerifierOriginalCalls[VI_HALMAX];

#endif

//
// Track irql package declarations
//

#define VI_TRACK_IRQL_TRACE_LENGTH 5

typedef struct _VI_TRACK_IRQL {

    PVOID Thread;
    KIRQL OldIrql;
    KIRQL NewIrql;
    UCHAR Processor;
    ULONG TickCount;
    PVOID StackTrace [VI_TRACK_IRQL_TRACE_LENGTH];

} VI_TRACK_IRQL, *PVI_TRACK_IRQL;

PVI_TRACK_IRQL ViTrackIrqlQueue;
ULONG ViTrackIrqlIndex;
ULONG ViTrackIrqlQueueLength = 128;

VOID
ViTrackIrqlInitialize (
    )
{
    ULONG Length;
    ULONG Round;

    //
    // Round up length to a power of two and prepare
    // mask for the length.
    //

    Length = ViTrackIrqlQueueLength;

    if (Length > 0x10000) {
        Length = 0x10000;
    }

    for (Round = 0x10000; Round != 0; Round >>= 1) {

        if (Length == Round) {
            break;
        }
        else if ((Length & Round) == Round) {
            Length = (Round << 1);
            break;
        }
    }

    ViTrackIrqlQueueLength = Length;

    //
    // Note POOL_DRIVER_MASK must be set to stop the recursion loop
    // when using the kernel verifier.
    //

    ViTrackIrqlQueue = ExAllocatePoolWithTagPriority (
        NonPagedPool | POOL_DRIVER_MASK,
        ViTrackIrqlQueueLength * sizeof (VI_TRACK_IRQL),
        'lqrI',
        HighPoolPriority);
}

VOID
ViTrackIrqlLog (
    IN KIRQL CurrentIrql,
    IN KIRQL NewIrql
    )
{
    PVI_TRACK_IRQL Information;
    LARGE_INTEGER TimeStamp;
    ULONG Index;
    ULONG Hash;

    ASSERT (ViTrackIrqlQueue != NULL);

    if (CurrentIrql > DISPATCH_LEVEL || NewIrql > DISPATCH_LEVEL) {
        return;
    }

    //
    // Get a slot to write into.
    //

    Index = InterlockedIncrement((PLONG)&ViTrackIrqlIndex);
    Index &= (ViTrackIrqlQueueLength - 1);

    //
    // Capture information.
    //

    Information = &(ViTrackIrqlQueue[Index]);

    Information->Thread = KeGetCurrentThread();
    Information->OldIrql = CurrentIrql;
    Information->NewIrql = NewIrql;
    Information->Processor = (UCHAR)(KeGetCurrentProcessorNumber());
    KeQueryTickCount(&TimeStamp);
    Information->TickCount = TimeStamp.LowPart;

    RtlCaptureStackBackTrace (2,
                              VI_TRACK_IRQL_TRACE_LENGTH,
                              Information->StackTrace,
                              &Hash);
}

//
// Detect the caller of the current function in an architecture
// dependent way.
//

#define VI_DETECT_RETURN_ADDRESS(Caller)  {                     \
        PVOID CallersCaller;                                    \
        RtlGetCallersAddress(&Caller, &CallersCaller);          \
    }

//
// Fault injection stack trace log.
//

#define VI_FAULT_TRACE_LENGTH 8

typedef struct _VI_FAULT_TRACE {

    PVOID StackTrace [VI_FAULT_TRACE_LENGTH];

} VI_FAULT_TRACE, *PVI_FAULT_TRACE;

PVI_FAULT_TRACE ViFaultTraces;
ULONG ViFaultTracesIndex;
ULONG ViFaultTracesLength = 128;

VOID
ViFaultTracesInitialize (
    VOID
    )
{
    //
    // Note POOL_DRIVER_MASK must be set to stop the recursion loop
    // when using the kernel verifier.
    //

    ViFaultTraces = ExAllocatePoolWithTagPriority (
                                NonPagedPool | POOL_DRIVER_MASK,
                                ViFaultTracesLength * sizeof (VI_FAULT_TRACE),
                                'ttlF',
                                HighPoolPriority);
}

VOID
ViFaultTracesLog (
    VOID
    )
{
    PVI_FAULT_TRACE Information;
    ULONG Hash;
    ULONG Index;

    //
    // Sanity check
    //

    if (ViFaultTraces == NULL) {
        return;
    }

    //
    // Get slot to write into.
    //

    Index = InterlockedIncrement ((PLONG)&ViFaultTracesIndex);
    Index &= (ViFaultTracesLength - 1);

    //
    // Capture information.  Even if we lose performance it is
    // worth zeroing the trace buffer to avoid confusing people
    // if old traces get merged with new ones.  This zeroing
    // will happen only if we actually inject a failure.
    //

    Information = &(ViFaultTraces[Index]);

    RtlZeroMemory (Information, sizeof (VI_FAULT_TRACE));

    RtlCaptureStackBackTrace (2,
                              VI_FAULT_TRACE_LENGTH,
                              Information->StackTrace,
                              &Hash);
}

//
// Don't fail any requests in the first 7 or 8 minutes as we want to
// give the system enough time to boot.
//
#define MI_CHECK_UPTIME()                                       \
    if (VerifierSystemSufficientlyBooted == FALSE) {            \
        LARGE_INTEGER _CurrentTime;                              \
        KeQuerySystemTime (&_CurrentTime);                       \
        if (_CurrentTime.QuadPart > KeBootTime.QuadPart + VerifierRequiredTimeSinceBoot.QuadPart) {                                              \
            VerifierSystemSufficientlyBooted = TRUE;            \
        }                                                       \
    }

THUNKED_API
VOID
VerifierProbeAndLockPages (
     IN OUT PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode,
     IN LOCK_OPERATION Operation
     )
{
    ULONG i;
    KIRQL CurrentIrql;
    PPFN_NUMBER Page;
    PVOID StartingVa;
    LOGICAL ValidPfn;
    PFN_NUMBER NumberOfPages;
    PEPROCESS CurrentProcess;
    PLIST_ENTRY NextEntry;
    PPFN_NUMBER LastPage;
    PMI_PHYSICAL_VIEW PhysicalView;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x70,
                      CurrentIrql,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)AccessMode);
    }

    if (ViInjectResourceFailure () == TRUE) {
        ExRaiseStatus (STATUS_WORKING_SET_QUOTA);
    }

    MmProbeAndLockPages (MemoryDescriptorList, AccessMode, Operation);

    //
    // Every page that has been probed and locked must have a PFN with the
    // exception of physical section mappings.
    //

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

    LastPage = Page + NumberOfPages;

    ASSERT (NumberOfPages != 0);

    LOCK_PFN2 (CurrentIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {

            //
            // There are no more locked pages.
            //

            break;
        }

        ValidPfn = FALSE;
        for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {
            if ((*Page >= MmPhysicalMemoryBlock->Run[i].BasePage) &&
                (*Page <= MmPhysicalMemoryBlock->Run[i].BasePage +
                    MmPhysicalMemoryBlock->Run[i].PageCount)) {

                //
                // A valid PFN exists for this page, march to the next one.
                //

                ValidPfn = TRUE;
                break;
            }
        }

        if (ValidPfn == FALSE) {

            CurrentProcess = PsGetCurrentProcess ();

            //
            // Check for a transfer to/from a physical VAD - these are
            // allowed to have no backing PFNs.
            //

            if ((StartingVa <= MM_HIGHEST_USER_ADDRESS) &&
                (CurrentProcess->Flags & PS_PROCESS_FLAGS_HAS_PHYSICAL_VAD)) {

                //
                // This process has a physical VAD which maps directly to RAM
                // not necessarily present in the PFN database.  See if the
                // MDL request intersects this physical VAD.
                //

                NextEntry = CurrentProcess->PhysicalVadList.Flink;
                while (NextEntry != &CurrentProcess->PhysicalVadList) {

                    PhysicalView = CONTAINING_RECORD(NextEntry,
                                                     MI_PHYSICAL_VIEW,
                                                     ListEntry);

                    if ((PhysicalView->Vad->u.VadFlags.PhysicalMapping == 1) &&
                        (StartingVa >= (PVOID)PhysicalView->StartVa) &&
                        (StartingVa <= (PVOID)PhysicalView->EndVa)) {

                        ValidPfn = TRUE;
                        break;
                    }
                    NextEntry = NextEntry->Flink;
                }
            }

            if (ValidPfn == FALSE) {

                //
                // The MDL being probed has no backing PFNs and it was not
                // a user physical VAD.  This means that PFN entries had their
                // reference counts bumped, but since they don't exist, random
                // physical pages just got what are going to usually look
                // like single bit errors.  This is bad.
                //

                KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                              0x6F,
                              (ULONG_PTR)MemoryDescriptorList,
                              (ULONG_PTR)(*Page),
                              (ULONG_PTR)MmHighestPhysicalPage);
            }
            break;
        }

        Page += 1;
    } while (Page < LastPage);

    UNLOCK_PFN2 (CurrentIrql);
}

THUNKED_API
VOID
VerifierProbeAndLockProcessPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PEPROCESS Process,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x71,
                      CurrentIrql,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)Process);
    }

    if (ViInjectResourceFailure () == TRUE) {
        ExRaiseStatus (STATUS_WORKING_SET_QUOTA);
    }

    MmProbeAndLockProcessPages (MemoryDescriptorList,
                                Process,
                                AccessMode,
                                Operation);
}

THUNKED_API
VOID
VerifierProbeAndLockSelectedPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PFILE_SEGMENT_ELEMENT SegmentArray,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > APC_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x72,
                      CurrentIrql,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)AccessMode);
    }

    if (ViInjectResourceFailure () == TRUE) {
        ExRaiseStatus (STATUS_WORKING_SET_QUOTA);
    }

    MmProbeAndLockSelectedPages (MemoryDescriptorList,
                                 SegmentArray,
                                 AccessMode,
                                 Operation);
}

THUNKED_API
PVOID
VerifierMapIoSpace (
     IN PHYSICAL_ADDRESS PhysicalAddress,
     IN SIZE_T NumberOfBytes,
     IN MEMORY_CACHING_TYPE CacheType
     )
{
    KIRQL CurrentIrql;
    ULONG Hint;
    PMMPFN Pfn1;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;

    CurrentIrql = KeGetCurrentIrql ();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x73,
                      CurrentIrql,
                      (ULONG_PTR)PhysicalAddress.LowPart,
                      NumberOfBytes);
    }

    //
    // See if the first frame is in the PFN database and if so, they all must
    // be.
    //

    Hint = 0;
    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    if (MiIsPhysicalMemoryAddress (PageFrameIndex, &Hint, TRUE) == TRUE) {

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (PhysicalAddress.LowPart,
                                                        NumberOfBytes);

        do {

            //
            // Each frame better be locked down already.  Bugcheck if not.
            //

            if ((Pfn1->u3.e2.ReferenceCount != 0) ||
                ((Pfn1->u3.e1.Rom == 1) && ((CacheType & 0xFF) == MmCached))) {

                NOTHING;
            }
            else {
                KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                              0x83,
                              (ULONG_PTR)PhysicalAddress.LowPart,
                              NumberOfBytes,
                              (ULONG_PTR)(Pfn1 - MmPfnDatabase));
            }

            if (Pfn1->u3.e1.CacheAttribute == MiNotMapped) {

                //
                // This better be for a page allocated with
                // MmAllocatePagesForMdl.  Otherwise it might be a
                // page on the freelist which could subsequently be
                // given out with a different attribute !
                //

                if ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
#if defined (_MI_MORE_THAN_4GB_)
                    (Pfn1->u4.PteFrame == MI_MAGIC_4GB_RECLAIM) ||
#endif
                    (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1))) {

                    NOTHING;
                }
                else {
                    KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                                  0x84,
                                  (ULONG_PTR)PhysicalAddress.LowPart,
                                  NumberOfBytes,
                                  (ULONG_PTR)(Pfn1 - MmPfnDatabase));
                }
            }
            Pfn1 += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);
    }

    if (ViInjectResourceFailure () == TRUE) {
        return NULL;
    }

    return MmMapIoSpace (PhysicalAddress, NumberOfBytes, CacheType);
}

VOID
ViCheckMdlPages (
    IN PMDL MemoryDescriptorList,
    IN MEMORY_CACHING_TYPE CacheType
    )
{
    PMMPFN Pfn1;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PVOID StartingVa;

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                         MemoryDescriptorList->ByteOffset);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               MemoryDescriptorList->ByteCount);
    LastPage = Page + NumberOfPages;

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        Pfn1 = MI_PFN_ELEMENT (*Page);

        //
        // Each frame better be locked down already.  Bugcheck if not.
        //

        if ((Pfn1->u3.e2.ReferenceCount != 0) ||
            ((Pfn1->u3.e1.Rom == 1) && (CacheType == MmCached))) {

            NOTHING;
        }
        else {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x85,
                          (ULONG_PTR)MemoryDescriptorList,
                          NumberOfPages,
                          (ULONG_PTR)(Pfn1 - MmPfnDatabase));
        }

        if (Pfn1->u3.e1.CacheAttribute == MiNotMapped) {

            //
            // This better be for a page allocated with
            // MmAllocatePagesForMdl.  Otherwise it might be a
            // page on the freelist which could subsequently be
            // given out with a different attribute !
            //

            if ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
#if defined (_MI_MORE_THAN_4GB_)
                (Pfn1->u4.PteFrame == MI_MAGIC_4GB_RECLAIM) ||
#endif
                (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1))) {

                NOTHING;
            }
            else {
                KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                              0x86,
                              (ULONG_PTR)MemoryDescriptorList,
                              NumberOfPages,
                              (ULONG_PTR)(Pfn1 - MmPfnDatabase));
            }
        }

        Page += 1;
    } while (Page < LastPage);
}

THUNKED_API
PVOID
VerifierMapLockedPages (
     IN PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode
     )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();

    if (AccessMode == KernelMode) {
        if (CurrentIrql > DISPATCH_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x74,
                          CurrentIrql,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)AccessMode);
        }
    }
    else {
        if (CurrentIrql > APC_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x75,
                          CurrentIrql,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)AccessMode);
        }
    }

    if ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) {
        ViCheckMdlPages (MemoryDescriptorList, MmCached);
    }

    if ((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) == 0) {

        MI_CHECK_UPTIME ();

        if (VerifierSystemSufficientlyBooted == TRUE) {

            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x81,
                          (ULONG_PTR) MemoryDescriptorList,
                          MemoryDescriptorList->MdlFlags,
                          0);
        }
    }

    return MmMapLockedPages (MemoryDescriptorList, AccessMode);
}

THUNKED_API
PVOID
VerifierMapLockedPagesSpecifyCache (
    IN PMDL MemoryDescriptorList,
    IN KPROCESSOR_MODE AccessMode,
    IN MEMORY_CACHING_TYPE CacheType,
    IN PVOID RequestedAddress,
    IN ULONG BugCheckOnFailure,
    IN MM_PAGE_PRIORITY Priority
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();
    if (AccessMode == KernelMode) {
        if (CurrentIrql > DISPATCH_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x76,
                          CurrentIrql,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)AccessMode);
        }
    }
    else {
        if (CurrentIrql > APC_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x77,
                          CurrentIrql,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)AccessMode);
        }
    }

    if ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) {
        ViCheckMdlPages (MemoryDescriptorList, CacheType);
    }

    if ((MemoryDescriptorList->MdlFlags & MDL_MAPPING_CAN_FAIL) ||
        (BugCheckOnFailure == 0)) {

        if (ViInjectResourceFailure () == TRUE) {
            return NULL;
        }
    }
    else {

        //
        // All drivers must specify can fail or don't bugcheck.
        //

        MI_CHECK_UPTIME ();

        if (VerifierSystemSufficientlyBooted == TRUE) {

            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x82,
                          (ULONG_PTR) MemoryDescriptorList,
                          MemoryDescriptorList->MdlFlags,
                          BugCheckOnFailure);
        }
    }

    return MmMapLockedPagesSpecifyCache (MemoryDescriptorList,
                                         AccessMode,
                                         CacheType,
                                         RequestedAddress,
                                         BugCheckOnFailure,
                                         Priority);
}

VOID
VerifierUnlockPages (
     IN OUT PMDL MemoryDescriptorList
     )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x78,
                      CurrentIrql,
                      (ULONG_PTR)MemoryDescriptorList,
                      0);
    }

    if ((MemoryDescriptorList->MdlFlags & MDL_PAGES_LOCKED) == 0) {

        //
        // The caller is trying to unlock an MDL that was never locked down.
        //

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x7C,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)MemoryDescriptorList->MdlFlags,
                      0);
    }

    if (MemoryDescriptorList->MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) {

        //
        // Nonpaged pool should never be locked down.
        //

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x7D,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)MemoryDescriptorList->MdlFlags,
                      0);
    }

    MmUnlockPages (MemoryDescriptorList);
}

VOID
VerifierUnmapLockedPages (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {
        if (CurrentIrql > DISPATCH_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x79,
                          CurrentIrql,
                          (ULONG_PTR)BaseAddress,
                          (ULONG_PTR)MemoryDescriptorList);
        }
    }
    else {
        if (CurrentIrql > APC_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x7A,
                          CurrentIrql,
                          (ULONG_PTR)BaseAddress,
                          (ULONG_PTR)MemoryDescriptorList);
        }
    }

    MmUnmapLockedPages (BaseAddress, MemoryDescriptorList);
}

VOID
VerifierUnmapIoSpace (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x7B,
                      CurrentIrql,
                      (ULONG_PTR)BaseAddress,
                      (ULONG_PTR)NumberOfBytes);
    }

    MmUnmapIoSpace (BaseAddress, NumberOfBytes);
}

THUNKED_API
PVOID
VerifierAllocatePool (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes
    )
{
    PVOID CallingAddress;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    VI_DETECT_RETURN_ADDRESS (CallingAddress);

    if (KernelVerifier == TRUE) {

        Verifier = ViLocateVerifierEntry (CallingAddress);

        if ((Verifier == NULL) ||
            ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0)) {

            return ExAllocatePool (PoolType | POOL_DRIVER_MASK, NumberOfBytes);
        }
        PoolType |= POOL_DRIVER_MASK;
    }

    MmVerifierData.AllocationsWithNoTag += 1;

    return VeAllocatePoolWithTagPriority (PoolType,
                                          NumberOfBytes,
                                          'parW',
                                          HighPoolPriority,
                                          CallingAddress);
}

THUNKED_API
PVOID
VerifierAllocatePoolWithTag (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    )
{
    PVOID CallingAddress;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    VI_DETECT_RETURN_ADDRESS (CallingAddress);

    if (KernelVerifier == TRUE) {
        Verifier = ViLocateVerifierEntry (CallingAddress);

        if ((Verifier == NULL) ||
            ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0)) {

            return ExAllocatePoolWithTag (PoolType | POOL_DRIVER_MASK,
                                          NumberOfBytes,
                                          Tag);
        }
        PoolType |= POOL_DRIVER_MASK;
    }

    return VeAllocatePoolWithTagPriority (PoolType,
                                          NumberOfBytes,
                                          Tag,
                                          HighPoolPriority,
                                          CallingAddress);
}

THUNKED_API
PVOID
VerifierAllocatePoolWithQuota(
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes
    )
{
    PVOID Va;
    LOGICAL RaiseOnQuotaFailure;
    PVOID CallingAddress;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    VI_DETECT_RETURN_ADDRESS (CallingAddress);

    if (KernelVerifier == TRUE) {
        Verifier = ViLocateVerifierEntry (CallingAddress);

        if ((Verifier == NULL) ||
            ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0)) {

            return ExAllocatePoolWithQuota (PoolType | POOL_DRIVER_MASK,
                                            NumberOfBytes);
        }
        PoolType |= POOL_DRIVER_MASK;
    }

    MmVerifierData.AllocationsWithNoTag += 1;

    if (PoolType & POOL_QUOTA_FAIL_INSTEAD_OF_RAISE) {
        RaiseOnQuotaFailure = FALSE;
        PoolType &= ~POOL_QUOTA_FAIL_INSTEAD_OF_RAISE;
    }
    else {
        RaiseOnQuotaFailure = TRUE;
    }

    Va = VeAllocatePoolWithTagPriority (PoolType,
                                        NumberOfBytes,
                                        'parW',
                                        HighPoolPriority,
                                        CallingAddress);

    if (Va == NULL) {
        if (RaiseOnQuotaFailure == TRUE) {
            ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        }
    }

    return Va;
}

THUNKED_API
PVOID
VerifierAllocatePoolWithQuotaTag(
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    )
{
    PVOID Va;
    LOGICAL RaiseOnQuotaFailure;
    PVOID CallingAddress;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    VI_DETECT_RETURN_ADDRESS (CallingAddress);

    if (KernelVerifier == TRUE) {
        Verifier = ViLocateVerifierEntry (CallingAddress);

        if ((Verifier == NULL) ||
            ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0)) {

            return ExAllocatePoolWithQuotaTag (PoolType | POOL_DRIVER_MASK,
                                               NumberOfBytes,
                                               Tag);
        }
        PoolType |= POOL_DRIVER_MASK;
    }

    if (PoolType & POOL_QUOTA_FAIL_INSTEAD_OF_RAISE) {
        RaiseOnQuotaFailure = FALSE;
        PoolType &= ~POOL_QUOTA_FAIL_INSTEAD_OF_RAISE;
    }
    else {
        RaiseOnQuotaFailure = TRUE;
    }

    Va = VeAllocatePoolWithTagPriority (PoolType,
                                        NumberOfBytes,
                                        Tag,
                                        HighPoolPriority,
                                        CallingAddress);

    if (Va == NULL) {
        if (RaiseOnQuotaFailure == TRUE) {
            ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        }
    }

    return Va;
}

THUNKED_API
PVOID
VerifierAllocatePoolWithTagPriority(
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN EX_POOL_PRIORITY Priority
    )

/*++

Routine Description:

    This thunked-in function:

        - Performs sanity checks on the caller.
        - Can optionally provide allocation failures to the caller.
        - Attempts to provide the allocation from special pool.
        - Tracks pool to ensure callers free everything they allocate.

--*/

{
    PVOID CallingAddress;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    VI_DETECT_RETURN_ADDRESS (CallingAddress);

    if (KernelVerifier == TRUE) {
        Verifier = ViLocateVerifierEntry (CallingAddress);

        if ((Verifier == NULL) ||
            ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0)) {

            return ExAllocatePoolWithTagPriority (PoolType | POOL_DRIVER_MASK,
                                                  NumberOfBytes,
                                                  Tag,
                                                  Priority);
        }
        PoolType |= POOL_DRIVER_MASK;
    }

    return VeAllocatePoolWithTagPriority (PoolType,
                                          NumberOfBytes,
                                          Tag,
                                          Priority,
                                          CallingAddress);
}

LOGICAL
ViInjectResourceFailure (
    VOID
    )

/*++

Routine Description:

    This function determines whether a resource allocation should be
    deliberately failed.  This may be a pool allocation, MDL creation,
    system PTE allocation, etc.

Arguments:

    None.

Return Value:

    TRUE if the allocation should be failed.  FALSE otherwise.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.

--*/

{
    ULONG TimeLow;
    LARGE_INTEGER CurrentTime;

    if ((MmVerifierData.Level & DRIVER_VERIFIER_INJECT_ALLOCATION_FAILURES) == 0) {
        return FALSE;
    }

    //
    // Don't fail any requests in the first 7 or 8 minutes as we want to
    // give the system enough time to boot.
    //

    MI_CHECK_UPTIME ();

    if (VerifierSystemSufficientlyBooted == TRUE) {

        KeQueryTickCount(&CurrentTime);

        TimeLow = CurrentTime.LowPart;

        if ((TimeLow & 0xF) == 0) {

            MmVerifierData.AllocationsFailedDeliberately += 1;

            //
            // Deliberately fail this request.
            //

            if (MiFaultRetryMask != 0xFFFFFFFF) {
                MiFaultRetryMask = 0xFFFFFFFF;
                MiUserFaultRetryMask = 0xFFFFFFFF;
            }

#if defined(_X86_)
            ViFaultTracesLog ();
#endif

            return TRUE;
        }

        //
        // Approximately every 5 minutes (on most systems), fail all of this
        // components allocations for a 10 second burst.  This more closely
        // simulates (and exaggerates) the duration of the typical low resource
        // scenario.
        //

        TimeLow &= 0x7FFF;

        if (TimeLow < 0x400) {

            MmVerifierData.BurstAllocationsFailedDeliberately += 1;

            //
            // Deliberately fail this request.
            //

            if (MiFaultRetryMask != 0xFFFFFFFF) {
                MiFaultRetryMask = 0xFFFFFFFF;
                MiUserFaultRetryMask = 0xFFFFFFFF;
            }

#if defined(_X86_)
            ViFaultTracesLog ();
#endif

            return TRUE;
        }
    }

    return FALSE;
}

LOGICAL
ViReservePoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    )
{
    ULONG_PTR OldSize;
    ULONG_PTR NewSize;
    ULONG_PTR NewHashOffset;
    ULONG_PTR Increment;
    ULONG_PTR Entries;
    ULONG_PTR i;
    KIRQL OldIrql;
    PVOID NewHashTable;
    PVI_POOL_ENTRY HashEntry;
    PVI_POOL_ENTRY OldHashTable;

    ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

    while (Verifier->PoolHashSize <= Verifier->CurrentPagedPoolAllocations + Verifier->CurrentNonPagedPoolAllocations + Verifier->PoolHashReserved) {

        //
        // More space is needed.  Try for it now.
        //

#define VI_POOL_ENTRIES_PER_PAGE (PAGE_SIZE / sizeof(VI_POOL_ENTRY))

        OldSize = Verifier->PoolHashSize * sizeof(VI_POOL_ENTRY);

        if (Verifier->PoolHashSize >= VI_POOL_ENTRIES_PER_PAGE) {
            Increment = PAGE_SIZE;
        }
        else {
            Increment = 16 * sizeof (VI_POOL_ENTRY);
        }

        ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

        NewSize = OldSize + Increment;

        if (NewSize < OldSize) {
            return FALSE;
        }

        //
        // Note POOL_DRIVER_MASK must be set to stop the recursion loop
        // when using the kernel verifier.
        //

        NewHashTable = ExAllocatePoolWithTagPriority (NonPagedPool | POOL_DRIVER_MASK,
                                                      NewSize,
                                                      'ppeV',
                                                      HighPoolPriority);

        ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

        OldSize = Verifier->PoolHashSize * sizeof(VI_POOL_ENTRY);

        if (NewHashTable == NULL) {
            if (Verifier->PoolHashSize <= Verifier->CurrentPagedPoolAllocations + Verifier->CurrentNonPagedPoolAllocations + Verifier->PoolHashReserved) {
                ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);
                return FALSE;
            }

            //
            // Another thread got here before us and space is available.
            //

            break;
        }

        if (NewSize != OldSize + Increment) {

            //
            // Another thread got here before us.
            //

            ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);
            ExFreePool (NewHashTable);
            ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);
        }
        else {

            //
            // Rebuild the list into the new table.
            //

            OldHashTable = Verifier->PoolHash;
            if (OldHashTable != NULL) {
                RtlCopyMemory (NewHashTable, OldHashTable, OldSize);
            }

            //
            // Construct the freelist chaining it through any existing
            // list (any free entries must be already reserved).
            //

            HashEntry = (PVI_POOL_ENTRY) ((PCHAR)NewHashTable + OldSize);
            Entries = Increment / sizeof (VI_POOL_ENTRY);
            NewHashOffset = HashEntry - (PVI_POOL_ENTRY)NewHashTable;

            //
            // If list compaction becomes important then chaining it on the
            // end here will need to be revisited.
            //

            for (i = 0; i < Entries; i += 1) {
                HashEntry->FreeListNext = NewHashOffset + i + 1;
                HashEntry += 1;
            }
            HashEntry -= 1;
            HashEntry->FreeListNext = Verifier->PoolHashFree;
            Verifier->PoolHashFree = NewHashOffset;
            Verifier->PoolHash = NewHashTable;
            Verifier->PoolHashSize += Entries;

            //
            // Free the old table.
            //

            if (OldHashTable != NULL) {
                ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);
                ExFreePool (OldHashTable);
                ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);
            }
        }
    }

    Verifier->PoolHashReserved += 1;

    ASSERT (Verifier->PoolHashSize >= Verifier->CurrentPagedPoolAllocations + Verifier->CurrentNonPagedPoolAllocations + Verifier->PoolHashReserved);

    ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

    return TRUE;
}

ULONG_PTR
ViInsertPoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN PVOID VirtualAddress,
    IN PVOID CallingAddress,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag
    )

/*++

Routine Description:

    This function inserts the specified virtual address into the verifier
    list for this driver.

Arguments:

    Verifier - Supplies the verifier entry to update.

    VirtualAddress - Supplies the virtual address to insert.

    CallingAddress - Supplies the caller's address.

    NumberOfBytes - Supplies the number of bytes to allocate.

    Tag - Supplies the tag for the pool being allocated.

Return Value:

    The list index this virtual address was inserted at.

Environment:

    Kernel mode, DISPATCH_LEVEL.  The Verifier->VerifierPoolLock must be held.

--*/

{
    ULONG_PTR Index;
    PVI_POOL_ENTRY HashEntry;

    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);

    //
    // The list entry must be reserved in advance.
    //

    ASSERT (Verifier->PoolHashReserved != 0);

    ASSERT (Verifier->PoolHashSize >= Verifier->CurrentPagedPoolAllocations + Verifier->CurrentNonPagedPoolAllocations + Verifier->PoolHashReserved);

    //
    // Use the next free list entry.
    //

    Index = Verifier->PoolHashFree;
    ASSERT (Index != VI_POOL_FREELIST_END);

    HashEntry = Verifier->PoolHash + Index;

    Verifier->PoolHashFree = HashEntry->FreeListNext;

    Verifier->PoolHashReserved -= 1;

#if defined (_X86_)
    //
    // MiUseMaximumSystemSpace denotes an x86 mode where kernel pointers don't
    // necessarily have the high bit set.
    //
    ASSERT (((HashEntry->FreeListNext & MINLONG_PTR) == 0) ||
            (HashEntry->FreeListNext == VI_POOL_FREELIST_END) ||
            (MiUseMaximumSystemSpace != 0));
#else
    ASSERT (((HashEntry->FreeListNext & MINLONG_PTR) == 0) ||
            (HashEntry->FreeListNext == VI_POOL_FREELIST_END));
#endif

    HashEntry->InUse.VirtualAddress = VirtualAddress;
    HashEntry->InUse.CallingAddress = CallingAddress;
    HashEntry->InUse.NumberOfBytes = NumberOfBytes;
    HashEntry->InUse.Tag = Tag;

#if defined (_X86_)
    //
    // MiUseMaximumSystemSpace denotes an x86 mode where kernel pointers don't
    // necessarily have the high bit set.
    //
    ASSERT (((HashEntry->FreeListNext & MINLONG_PTR) != 0) ||
            (MiUseMaximumSystemSpace != 0));
#else
    ASSERT ((HashEntry->FreeListNext & MINLONG_PTR) != 0);
#endif

    return Index;
}

VOID
ViReleasePoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN PVOID VirtualAddress,
    IN ULONG_PTR ListIndex,
    IN SIZE_T ChargedBytes
    )

/*++

Routine Description:

    This function removes the specified virtual address from the verifier
    list for this driver.

Arguments:

    Verifier - Supplies the verifier entry to update.

    VirtualAddress - Supplies the virtual address to release.

    ListIndex - Supplies the verifier pool hash index for the address being
                released.

    ChargedBytes - Supplies the bytes charged for this allocation.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL.  The Verifier->VerifierPoolLock must be held.

--*/

{
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageFrameIndex2;
    PVI_POOL_ENTRY HashEntry;
    PMMPTE PointerPte;

    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);

    if (Verifier->PoolHash == NULL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x59,
                      (ULONG_PTR)VirtualAddress,
                      ListIndex,
                      (ULONG_PTR)Verifier);
    }

    //
    // Ensure that the list pointer has not been overrun and still
    // points at something decent.
    //

    HashEntry = Verifier->PoolHash + ListIndex;

    if (ListIndex >= Verifier->PoolHashSize) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x54,
                      (ULONG_PTR)VirtualAddress,
                      Verifier->PoolHashSize,
                      ListIndex);
    }

    if (HashEntry->InUse.VirtualAddress != VirtualAddress) {

        PageFrameIndex = 0;
        PageFrameIndex2 = 1;

        if ((!MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) &&
            (MI_IS_PHYSICAL_ADDRESS(HashEntry->InUse.VirtualAddress))) {

            PointerPte = MiGetPteAddress(VirtualAddress);
            if (PointerPte->u.Hard.Valid == 1) {
                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

                PageFrameIndex2 = MI_CONVERT_PHYSICAL_TO_PFN (HashEntry->InUse.VirtualAddress);
            }
        }

        //
        // Caller overran and corrupted the virtual address - the linked
        // list cannot be counted on either.
        //

        if (PageFrameIndex != PageFrameIndex2) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x52,
                          (ULONG_PTR)VirtualAddress,
                          (ULONG_PTR)HashEntry->InUse.VirtualAddress,
                          ChargedBytes);
        }
    }

    if (HashEntry->InUse.NumberOfBytes != ChargedBytes) {

        //
        // Caller overran and corrupted the byte count - the linked
        // list cannot be counted on either.
        //

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x51,
                      (ULONG_PTR)VirtualAddress,
                      (ULONG_PTR)HashEntry,
                      ChargedBytes);
    }

    //
    // Put this list entry into the freelist.
    //

    HashEntry->FreeListNext = Verifier->PoolHashFree;
    Verifier->PoolHashFree = HashEntry - Verifier->PoolHash;
}

VOID
ViCancelPoolAllocation (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    )

/*++

Routine Description:

    This function removes a reservation from the verifier list for this driver.
    All reservations must be made in advance.  This routine is used when an
    earlier reservation is not going to be used (ie: the actual pool
    allocation failed so no reservation will be needed after all).

Arguments:

    Verifier - Supplies the verifier entry to update.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL or below, no verifier mutexes held.

--*/

{
    KIRQL OldIrql;

    ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

    //
    // The hash entry reserved earlier is not going to be used after all.
    //

    ASSERT (Verifier->PoolHashReserved != 0);

    ASSERT (Verifier->PoolHashSize >= Verifier->CurrentPagedPoolAllocations + Verifier->CurrentNonPagedPoolAllocations + Verifier->PoolHashReserved);

    ASSERT (Verifier->PoolHashFree != VI_POOL_FREELIST_END);

    Verifier->PoolHashReserved -= 1;

    ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);
}

PVOID
ViPostPoolAllocation (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN POOL_TYPE PoolType,
    IN ULONG Tag,
    IN PVOID CallingAddress
    )

/*++

Routine Description:

    This function performs verifier book-keeping on the allocation attempt.

Arguments:

    VirtualAddress - Supplies the virtual address that should be allocated.

    NumberOfBytes - Supplies the number of bytes to allocate.

    PoolType - Supplies the type of pool being allocated.

    Tag - Supplies the tag for the pool being allocated.

    CallingAddress - Supplies the caller's address.

Return Value:

    The virtual address the caller should use.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PMI_VERIFIER_POOL_HEADER Header;
    SIZE_T ChargedBytes;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    ULONG_PTR InsertedIndex;
    LOGICAL SpecialPoolAllocation;
    PPOOL_HEADER PoolHeader;

    InterlockedIncrement ((PLONG)&MmVerifierData.AllocationsSucceeded);
    ChargedBytes = EX_REAL_POOL_USAGE(NumberOfBytes);
    SpecialPoolAllocation = FALSE;

    if (MmIsSpecialPoolAddress (VirtualAddress) == TRUE) {
        ChargedBytes = NumberOfBytes;
        InterlockedIncrement ((PLONG)&MmVerifierData.AllocationsSucceededSpecialPool);
        SpecialPoolAllocation = TRUE;
    }
    else if (NumberOfBytes <= POOL_BUDDY_MAX) {
        ChargedBytes -= POOL_OVERHEAD;
    }
    else {

        //
        // This isn't exactly true but it does give the user a way to see
        // if this machine is large enough to support special pool 100%.
        //

        InterlockedIncrement ((PLONG)&MmVerifierData.AllocationsSucceededSpecialPool);
    }

    if ((PoolType & POOL_VERIFIER_MASK) == 0) {
        return VirtualAddress;
    }

    if (NumberOfBytes > POOL_BUDDY_MAX) {
        ASSERT (BYTE_OFFSET(VirtualAddress) == 0);
    }

    Verifier = ViLocateVerifierEntry (CallingAddress);
    ASSERT (Verifier != NULL);
    VerifierIsTrackingPool = TRUE;

    if (SpecialPoolAllocation == TRUE) {

        //
        // Carefully adjust the special pool page to move the verifier tracking
        // header to the front.  This allows the allocation to remain butted
        // against the end of the page so overruns can be detected immediately.
        //

        if (((ULONG_PTR)VirtualAddress & (PAGE_SIZE - 1))) {
            PoolHeader = (PPOOL_HEADER)(PAGE_ALIGN (VirtualAddress));
            Header = (PMI_VERIFIER_POOL_HEADER) (PoolHeader + 1);
            VirtualAddress = (PVOID) ((PCHAR)VirtualAddress + sizeof (MI_VERIFIER_POOL_HEADER));
        }
        else {
            PoolHeader = (PPOOL_HEADER)((PCHAR)PAGE_ALIGN (VirtualAddress) + PAGE_SIZE - POOL_OVERHEAD);
            Header = (PMI_VERIFIER_POOL_HEADER) (PoolHeader - 1);
        }
        // ASSERT (PoolHeader->Ulong1 & MI_SPECIAL_POOL_VERIFIER);
        PoolHeader->Ulong1 -= sizeof (MI_VERIFIER_POOL_HEADER);
        ChargedBytes -= sizeof (MI_VERIFIER_POOL_HEADER);
        PoolHeader->Ulong1 |= MI_SPECIAL_POOL_VERIFIER;
    }
    else {
        Header = (PMI_VERIFIER_POOL_HEADER)((PCHAR)VirtualAddress +
                         ChargedBytes -
                         sizeof(MI_VERIFIER_POOL_HEADER));
    }

    ASSERT (((ULONG_PTR)Header & (sizeof(ULONG) - 1)) == 0);


    Header->Verifier = Verifier;

    //
    // Enqueue the entry and update per-driver counters.
    // Note that paged pool allocations must be chained using nonpaged
    // pool to prevent deadlocks.
    //

    ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

    InsertedIndex = ViInsertPoolAllocation (Verifier,
                                            VirtualAddress,
                                            CallingAddress,
                                            ChargedBytes,
                                            Tag);

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {

        Verifier->PagedBytes += ChargedBytes;
        if (Verifier->PagedBytes > Verifier->PeakPagedBytes) {
            Verifier->PeakPagedBytes = Verifier->PagedBytes;
        }

        Verifier->CurrentPagedPoolAllocations += 1;
        if (Verifier->CurrentPagedPoolAllocations > Verifier->PeakPagedPoolAllocations) {
            Verifier->PeakPagedPoolAllocations = Verifier->CurrentPagedPoolAllocations;
        }
    }
    else {
        Verifier->NonPagedBytes += ChargedBytes;
        if (Verifier->NonPagedBytes > Verifier->PeakNonPagedBytes) {
            Verifier->PeakNonPagedBytes = Verifier->NonPagedBytes;
        }

        Verifier->CurrentNonPagedPoolAllocations += 1;
        if (Verifier->CurrentNonPagedPoolAllocations > Verifier->PeakNonPagedPoolAllocations) {
            Verifier->PeakNonPagedPoolAllocations = Verifier->CurrentNonPagedPoolAllocations;
        }
    }

    ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

    //
    // Since the header for paged pool is paged, don't initialize it until the
    // spinlock above is released.
    //

    Header->ListIndex = InsertedIndex;

    //
    // Update systemwide counters.
    //

    if ((PoolType & BASE_POOL_TYPE_MASK) == PagedPool) {
        ExAcquireFastMutex (&VerifierPoolMutex);

        MmVerifierData.PagedBytes += ChargedBytes;
        if (MmVerifierData.PagedBytes > MmVerifierData.PeakPagedBytes) {
            MmVerifierData.PeakPagedBytes = MmVerifierData.PagedBytes;
        }

        MmVerifierData.CurrentPagedPoolAllocations += 1;
        if (MmVerifierData.CurrentPagedPoolAllocations > MmVerifierData.PeakPagedPoolAllocations) {
            MmVerifierData.PeakPagedPoolAllocations = MmVerifierData.CurrentPagedPoolAllocations;
        }

        ExReleaseFastMutex (&VerifierPoolMutex);
    }
    else {
        ExAcquireSpinLock (&VerifierPoolLock, &OldIrql);

        MmVerifierData.NonPagedBytes += ChargedBytes;
        if (MmVerifierData.NonPagedBytes > MmVerifierData.PeakNonPagedBytes) {
            MmVerifierData.PeakNonPagedBytes = MmVerifierData.NonPagedBytes;
        }

        MmVerifierData.CurrentNonPagedPoolAllocations += 1;
        if (MmVerifierData.CurrentNonPagedPoolAllocations > MmVerifierData.PeakNonPagedPoolAllocations) {
            MmVerifierData.PeakNonPagedPoolAllocations = MmVerifierData.CurrentNonPagedPoolAllocations;
        }

        ExReleaseSpinLock (&VerifierPoolLock, OldIrql);
    }

    return VirtualAddress;
}

PVOID
VeAllocatePoolWithTagPriority (
    IN POOL_TYPE PoolType,
    IN SIZE_T NumberOfBytes,
    IN ULONG Tag,
    IN EX_POOL_PRIORITY Priority,
    IN PVOID CallingAddress
    )

/*++

Routine Description:

    This routine is called both from ex\pool.c and directly within this module.

        - Performs sanity checks on the caller.
        - Can optionally provide allocation failures to the caller.
        - Attempts to provide the allocation from special pool.
        - Tracks pool to ensure callers free everything they allocate.

--*/

{
    PVOID VirtualAddress;
    EX_POOL_PRIORITY AllocationPriority;
    SIZE_T ChargedBytes;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    LOGICAL ReservedHash;
    ULONG HeaderSize;

    ExAllocatePoolSanityChecks (PoolType, NumberOfBytes);

    InterlockedIncrement ((PLONG)&MmVerifierData.AllocationsAttempted);

    if ((PoolType & MUST_SUCCEED_POOL_TYPE_MASK) == 0) {

        if (ViInjectResourceFailure () == TRUE) {

            //
            // Caller requested an exception - throw it here.
            //

            if ((PoolType & POOL_RAISE_IF_ALLOCATION_FAILURE) != 0) {
                ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
            }

            return NULL;
        }
    }
    else {
        MI_CHECK_UPTIME ();

        if (VerifierSystemSufficientlyBooted == TRUE) {

            KeBugCheckEx (BAD_POOL_CALLER,
                          0x9A,
                          PoolType,
                          NumberOfBytes,
                          Tag);
        }
    }

    ASSERT ((PoolType & POOL_VERIFIER_MASK) == 0);

    //
    // Initializing Verifier is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Verifier = NULL;

    AllocationPriority = Priority;

    if (MmVerifierData.Level & DRIVER_VERIFIER_SPECIAL_POOLING) {

        //
        // Try for a special pool overrun allocation unless the caller has
        // explicitly specified otherwise.
        //

        if ((AllocationPriority & (LowPoolPrioritySpecialPoolOverrun | LowPoolPrioritySpecialPoolUnderrun)) == 0) {
            if (MmSpecialPoolCatchOverruns == TRUE) {
                AllocationPriority |= LowPoolPrioritySpecialPoolOverrun;
            }
            else {
                AllocationPriority |= LowPoolPrioritySpecialPoolUnderrun;
            }
        }
    }

    ReservedHash = FALSE;
    if (MmVerifierData.Level & DRIVER_VERIFIER_TRACK_POOL_ALLOCATIONS) {

        if (PoolType & SESSION_POOL_MASK) {

            //
            // Session pool is directly tracked by default already.
            //

            NOTHING;
        }
        else {
            HeaderSize = sizeof(MI_VERIFIER_POOL_HEADER);

            ChargedBytes = MI_ROUND_TO_SIZE (NumberOfBytes, sizeof(ULONG)) + HeaderSize;
            Verifier = ViLocateVerifierEntry (CallingAddress);

            if ((Verifier == NULL) ||
                ((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0) ||
                (Verifier->Flags & VI_DISABLE_VERIFICATION)) {

                //
                // This can happen for many reasons including no framing (which
                // can cause RtlGetCallersAddress to return the wrong address),
                // etc.
                //

                MmVerifierData.UnTrackedPool += 1;
            }
            else if (ChargedBytes <= NumberOfBytes) {

                //
                // Don't let the verifier header transform a bad caller into a
                // good caller.  Fail via the fall through so an exception
                // can be thrown if asked for, etc.
                //

                MmVerifierData.UnTrackedPool += 1;
            }
            else if (((PoolType & MUST_SUCCEED_POOL_TYPE_MASK) == 0) ||
                     (ChargedBytes <= PAGE_SIZE)) {

                //
                // Any pool allocation that is allowed to fail or where the
                // total number of charged bytes fits in a page (nonpaged-
                // must-succeed requires this) is suitable for tracking.
                // Just ensure that the hash list has space for it.
                //

                if (ViReservePoolAllocation (Verifier) == TRUE) {
                    ReservedHash = TRUE;
                    NumberOfBytes = ChargedBytes;
                    PoolType |= POOL_VERIFIER_MASK;
                }
            }
            else {
                ASSERT ((PoolType & BASE_POOL_TYPE_MASK) == NonPagedPool);
                MmVerifierData.UnTrackedPool += 1;
            }
        }
    }

    VirtualAddress = ExAllocatePoolWithTagPriority (PoolType,
                                                    NumberOfBytes,
                                                    Tag,
                                                    AllocationPriority);

    if (VirtualAddress == NULL) {
        MmVerifierData.AllocationsFailed += 1;

        if (ReservedHash == TRUE) {

            //
            // Release the hash table entry now as it's not needed.
            //

            ViCancelPoolAllocation (Verifier);
        }

        if ((PoolType & POOL_RAISE_IF_ALLOCATION_FAILURE) != 0) {
            ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        }
        return NULL;
    }

    VirtualAddress = ViPostPoolAllocation (VirtualAddress,
                                           NumberOfBytes,
                                           PoolType,
                                           Tag,
                                           CallingAddress);



    return VirtualAddress;
}

VOID
ViFreeTrackedPool (
    IN PVOID VirtualAddress,
    IN SIZE_T ChargedBytes,
    IN LOGICAL CheckType,
    IN LOGICAL SpecialPool
    )

/*++

Routine Description:

    Called directly from the pool manager or the memory manager for verifier-
    tracked allocations.  The call to ExFreePool is already in progress.

Arguments:

    VirtualAddress - Supplies the virtual address being freed.

    ChargedBytes - Supplies the number of bytes charged to this allocation.

    CheckType - Supplies PagedPool or NonPagedPool.

    SpecialPool - Supplies TRUE if the allocation is from special pool.

Return Value:

    None.

Environment:

    Kernel mode.

    N.B.

    Callers freeing small pool allocations hold no locks or mutexes on entry.

    Callers freeing special pool hold no locks or mutexes on entry.

    Callers freeing pool of PAGE_SIZE or larger hold the nonpaged pool spinlock
    or the PagedPool mutex (depending on allocation type) on entry.

--*/

{
    KIRQL OldIrql;
    ULONG_PTR Index;
    PPOOL_HEADER PoolHeader;
    PMI_VERIFIER_POOL_HEADER Header;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    ASSERT (VerifierIsTrackingPool == TRUE);

    if (SpecialPool == TRUE) {

        //
        // Special pool allocation.
        //

        if (((ULONG_PTR)VirtualAddress & (PAGE_SIZE - 1))) {
            PoolHeader = PAGE_ALIGN (VirtualAddress);
            Header = (PMI_VERIFIER_POOL_HEADER)(PoolHeader + 1);
        }
        else {
            PoolHeader = (PPOOL_HEADER)((PCHAR)PAGE_ALIGN (VirtualAddress) + PAGE_SIZE - POOL_OVERHEAD);
            Header = (PMI_VERIFIER_POOL_HEADER)(PoolHeader - 1);
        }
    }
    else if (PAGE_ALIGNED(VirtualAddress)) {

        //
        // Large page allocation.
        //

        Header = (PMI_VERIFIER_POOL_HEADER) ((PCHAR)VirtualAddress +
                     ChargedBytes -
                     sizeof(MI_VERIFIER_POOL_HEADER));
    }
    else {
        ChargedBytes -= POOL_OVERHEAD;
        Header = (PMI_VERIFIER_POOL_HEADER) ((PCHAR)VirtualAddress +
                     ChargedBytes -
                     sizeof(MI_VERIFIER_POOL_HEADER));
    }

    Verifier = Header->Verifier;

    //
    // Check the pointer now so we can give a more friendly bugcheck
    // rather than crashing below on a bad reference.
    //

    if ((((ULONG_PTR)Verifier & (sizeof(ULONG) - 1)) != 0) ||
        (!MmIsAddressValid(&Verifier->Signature)) ||
        (Verifier->Signature != MI_VERIFIER_ENTRY_SIGNATURE)) {

        //
        // The caller corrupted the saved verifier field.
        //

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x53,
                      (ULONG_PTR)VirtualAddress,
                      (ULONG_PTR)Header,
                      (ULONG_PTR)Verifier);
    }

    Index = Header->ListIndex;

    ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

    ViReleasePoolAllocation (Verifier,
                             VirtualAddress,
                             Index,
                             ChargedBytes);

    if (CheckType == PagedPool) {
        Verifier->PagedBytes -= ChargedBytes;
        Verifier->CurrentPagedPoolAllocations -= 1;

        ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

        ExAcquireFastMutex (&VerifierPoolMutex);
        MmVerifierData.PagedBytes -= ChargedBytes;
        MmVerifierData.CurrentPagedPoolAllocations -= 1;
        ExReleaseFastMutex (&VerifierPoolMutex);
    }
    else {
        Verifier->NonPagedBytes -= ChargedBytes;
        Verifier->CurrentNonPagedPoolAllocations -= 1;
        ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

        ExAcquireSpinLock (&VerifierPoolLock, &OldIrql);
        MmVerifierData.NonPagedBytes -= ChargedBytes;
        MmVerifierData.CurrentNonPagedPoolAllocations -= 1;
        ExReleaseSpinLock (&VerifierPoolLock, OldIrql);
    }
}

VOID
VerifierFreeTrackedPool (
    IN PVOID VirtualAddress,
    IN SIZE_T ChargedBytes,
    IN LOGICAL CheckType,
    IN LOGICAL SpecialPool
    )

/*++

Routine Description:

    Called directly from the pool manager or the memory manager for verifier-
    tracked allocations.  The call to ExFreePool is already in progress.

Arguments:

    VirtualAddress - Supplies the virtual address being freed.

    ChargedBytes - Supplies the number of bytes charged to this allocation.

    CheckType - Supplies PagedPool or NonPagedPool.

    SpecialPool - Supplies TRUE if the allocation is from special pool.

Return Value:

    None.

Environment:

    Kernel mode.

    N.B.

    Callers freeing small pool allocations hold no locks or mutexes on entry.

    Callers freeing special pool hold no locks or mutexes on entry.

    Callers freeing pool of PAGE_SIZE or larger hold the nonpaged pool spinlock
    or the PagedPool mutex (depending on allocation type) on entry.

--*/

{
    if (VerifierIsTrackingPool == FALSE) {

        //
        // The verifier is not enabled so the only way this routine is being
        // called is because the pool header is mangled or the caller specified
        // a bad address.  Either way it's a bugcheck.
        //

        KeBugCheckEx (BAD_POOL_CALLER,
                      0x99,
                      (ULONG_PTR)VirtualAddress,
                      0,
                      0);
    }

    ViFreeTrackedPool (VirtualAddress, ChargedBytes, CheckType, SpecialPool);
}

THUNKED_API
VOID
VerifierFreePool(
    IN PVOID P
    )
{
    if (KernelVerifier == TRUE) {
        ExFreePool (P);
        return;
    }

    VerifierFreePoolWithTag (P, 0);
}

THUNKED_API
VOID
VerifierFreePoolWithTag(
    IN PVOID P,
    IN ULONG TagToFree
    )
{
    if (KernelVerifier == TRUE) {
        ExFreePoolWithTag (P, TagToFree);
        return;
    }

    ExFreePoolSanityChecks (P);

    ExFreePoolWithTag (P, TagToFree);
}

THUNKED_API
LONG
VerifierSetEvent (
    IN PRKEVENT Event,
    IN KPRIORITY Increment,
    IN BOOLEAN Wait
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql();
    if (CurrentIrql > DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x80,
                      CurrentIrql,
                      (ULONG_PTR)Event,
                      (ULONG_PTR)0);
    }

    return KeSetEvent (Event, Increment, Wait);
}

THUNKED_API
BOOLEAN
VerifierExAcquireResourceExclusiveLite(
    IN PERESOURCE Resource,
    IN BOOLEAN Wait
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    if ((CurrentIrql != APC_LEVEL) &&
        (!IS_SYSTEM_THREAD(PsGetCurrentThread())) &&
        (KeGetCurrentThread()->KernelApcDisable == 0)) {

            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x37,
                          CurrentIrql,
                          (ULONG_PTR)(KeGetCurrentThread()->KernelApcDisable),
                          (ULONG_PTR)Resource);
    }

    return ExAcquireResourceExclusiveLite (Resource, Wait);
}

THUNKED_API
VOID
FASTCALL
VerifierExReleaseResourceLite(
    IN PERESOURCE Resource
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    if ((CurrentIrql != APC_LEVEL) &&
        (!IS_SYSTEM_THREAD(PsGetCurrentThread())) &&
        (KeGetCurrentThread()->KernelApcDisable == 0)) {

            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x38,
                          CurrentIrql,
                          (ULONG_PTR)(KeGetCurrentThread()->KernelApcDisable),
                          (ULONG_PTR)Resource);
    }

    ExReleaseResourceLite (Resource);
}

int VerifierIrqlData[0x10];

VOID
KfSanityCheckRaiseIrql (
    IN KIRQL NewIrql
    )
{
    KIRQL CurrentIrql;

    //
    // Check for the caller inadvertently lowering.
    //

    CurrentIrql = KeGetCurrentIrql ();

    if (CurrentIrql == NewIrql) {
        VerifierIrqlData[0] += 1;
        if (CurrentIrql == APC_LEVEL) {
            VerifierIrqlData[1] += 1;
        }
        else if (CurrentIrql == DISPATCH_LEVEL) {
            VerifierIrqlData[2] += 1;
        }
        else {
            VerifierIrqlData[3] += 1;
        }
    }
    else {
        VerifierIrqlData[4] += 1;
    }

    if (CurrentIrql > NewIrql) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x30,
                      CurrentIrql,
                      NewIrql,
                      0);
    }

    //
    // Check for the caller using an uninitialized variable.
    //

    if (NewIrql > HIGH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x30,
                      CurrentIrql,
                      NewIrql,
                      0);
    }

    if (ViTrackIrqlQueue != NULL) {
        ViTrackIrqlLog (CurrentIrql, NewIrql);
    }
}

VOID
KfSanityCheckLowerIrql (
    IN KIRQL NewIrql
    )
{
    KIRQL CurrentIrql;

    //
    // Check for the caller inadvertently lowering.
    //

    CurrentIrql = KeGetCurrentIrql ();

    if (CurrentIrql == NewIrql) {
        VerifierIrqlData[8] += 1;
        if (CurrentIrql == APC_LEVEL) {
            VerifierIrqlData[9] += 1;
        }
        else if (CurrentIrql == DISPATCH_LEVEL) {
            VerifierIrqlData[10] += 1;
        }
        else {
            VerifierIrqlData[11] += 1;
        }
    }
    else {
        VerifierIrqlData[12] += 1;
    }

    if (CurrentIrql < NewIrql) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x31,
                      CurrentIrql,
                      NewIrql,
                      0);
    }

    //
    // Check for the caller using an uninitialized variable.
    //

    if (NewIrql > HIGH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x31,
                      CurrentIrql,
                      NewIrql,
                      0);
    }

    if (ViTrackIrqlQueue != NULL) {
        ViTrackIrqlLog (CurrentIrql, NewIrql);
    }
}

#define VI_TRIM_KERNEL  0x00000001
#define VI_TRIM_USER    0x00000002
#define VI_TRIM_SESSION 0x00000004
#define VI_TRIM_PURGE   0x80000000

ULONG ViTrimSpaces = VI_TRIM_KERNEL;

VOID
ViTrimAllSystemPagableMemory (
    VOID
    )
{
    LOGICAL PurgeTransition;
    LARGE_INTEGER CurrentTime;
    LOGICAL PageOut;

    PageOut = TRUE;
    if (KernelVerifier == TRUE) {
        KeQueryTickCount(&CurrentTime);
        if ((CurrentTime.LowPart & KernelVerifierTickPage) != 0) {
            PageOut = FALSE;
        }
    }

    if ((PageOut == TRUE) && (MiNoPageOnRaiseIrql == 0)) {
        MmVerifierData.TrimRequests += 1;

        if (ViTrimSpaces & VI_TRIM_PURGE) {
            PurgeTransition = TRUE;
        }
        else {
            PurgeTransition = FALSE;
        }

        if (ViTrimSpaces & VI_TRIM_KERNEL) {
            if (MmTrimAllSystemPagableMemory (PurgeTransition) == TRUE) {
                MmVerifierData.Trims += 1;
            }
        }

        if (ViTrimSpaces & VI_TRIM_USER) {
            if (MmTrimProcessMemory (PurgeTransition) == TRUE) {
                MmVerifierData.UserTrims += 1;
            }
        }

        if (ViTrimSpaces & VI_TRIM_SESSION) {
            if (MmTrimSessionMemory (PurgeTransition) == TRUE) {
                MmVerifierData.SessionTrims += 1;
            }
        }
    }
}

typedef
VOID
(*PKE_ACQUIRE_SPINLOCK) (
    IN PKSPIN_LOCK SpinLock,
    OUT PKIRQL OldIrql
    );

THUNKED_API
VOID
VerifierKeAcquireSpinLock (
    IN PKSPIN_LOCK SpinLock,
    OUT PKIRQL OldIrql
    )
{
    KIRQL CurrentIrql;

#if defined (_X86_)
    PKE_ACQUIRE_SPINLOCK HalRoutine;
#endif

    CurrentIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (DISPATCH_LEVEL);

    MmVerifierData.AcquireSpinLocks += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if (CurrentIrql < DISPATCH_LEVEL) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKE_ACQUIRE_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_ACQUIRE_SPINLOCK];

    if (HalRoutine) {
        (*HalRoutine)(SpinLock, OldIrql);

        VfDeadlockAcquireResource(SpinLock,
                                  VfDeadlockSpinLock,
                                  KeGetCurrentThread(),
                                  FALSE,
                                  _ReturnAddress());
        return;
    }
#endif

    KeAcquireSpinLock (SpinLock, OldIrql);

    VfDeadlockAcquireResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());
}

typedef
VOID
(*PKE_RELEASE_SPINLOCK) (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
VerifierKeReleaseSpinLock (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    )
{
    KIRQL CurrentIrql;
#if defined (_X86_)
    PKE_RELEASE_SPINLOCK HalRoutine;
#endif

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better still be at DISPATCH_LEVEL when releasing the spinlock
    //

    if (CurrentIrql < DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x32,
                      CurrentIrql,
                      (ULONG_PTR)SpinLock,
                      0);
    }

    KfSanityCheckLowerIrql (NewIrql);

#if defined (_X86_)
    HalRoutine = (PKE_RELEASE_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_RELEASE_SPINLOCK];

    if (HalRoutine) {
        VfDeadlockReleaseResource(SpinLock,
                                  VfDeadlockSpinLock,
                                  KeGetCurrentThread(),
                                  _ReturnAddress());
        (*HalRoutine)(SpinLock, NewIrql);

        return;
    }
#endif

    VfDeadlockReleaseResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              _ReturnAddress());

    KeReleaseSpinLock (SpinLock, NewIrql);
}

//
// Verifier thunks for AcquireSpinLockAtDpcLevel and ReleaseSpinLockFromDpcLevel.
//
// On x86 the functions exported by the kernel that are used by the driver are:
// KefAcquire.../KefRelease.... On other platforms the functions used by drivers
// are KeAcquire.../KeRelease. Among other differences the x86 versions use the
// fastcall convention which requires additional precaution.
//

THUNKED_API
VOID
#if defined(_X86_)
FASTCALL
#endif
VerifierKeAcquireSpinLockAtDpcLevel (
    IN PKSPIN_LOCK SpinLock
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at or above DISPATCH_LEVEL.
    //

    if (CurrentIrql < DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x40,
                      CurrentIrql,
                      (ULONG_PTR)SpinLock,
                      0);
    }

    MmVerifierData.AcquireSpinLocks += 1;

    KeAcquireSpinLockAtDpcLevel (SpinLock);

    VfDeadlockAcquireResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());
}

THUNKED_API
VOID
#if defined(_X86_)
FASTCALL
#endif
VerifierKeReleaseSpinLockFromDpcLevel (
    IN PKSPIN_LOCK SpinLock
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at DISPATCH_LEVEL.
    //

    if (CurrentIrql < DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x41,
                      CurrentIrql,
                      (ULONG_PTR)SpinLock,
                      0);
    }

    VfDeadlockReleaseResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              _ReturnAddress());

    KeReleaseSpinLockFromDpcLevel (SpinLock);
}

#if !defined(_X86_)

THUNKED_API
KIRQL
VerifierKeAcquireSpinLockRaiseToDpc (
    IN PKSPIN_LOCK SpinLock
    )
{
    KIRQL NewIrql = KeAcquireSpinLockRaiseToDpc (SpinLock);

    VfDeadlockAcquireResource (SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());

    return NewIrql;
}


#endif




#if defined (_X86_)

typedef
KIRQL
(FASTCALL *PKF_ACQUIRE_SPINLOCK) (
    IN PKSPIN_LOCK SpinLock
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKfAcquireSpinLock (
    IN PKSPIN_LOCK SpinLock
    )
{
    KIRQL CurrentIrql;
    PKF_ACQUIRE_SPINLOCK HalRoutine;

    CurrentIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (DISPATCH_LEVEL);

    MmVerifierData.AcquireSpinLocks += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if (CurrentIrql < DISPATCH_LEVEL) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKF_ACQUIRE_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KF_ACQUIRE_SPINLOCK];

    if (HalRoutine) {
        CurrentIrql = (*HalRoutine)(SpinLock);

        VfDeadlockAcquireResource(SpinLock,
                                  VfDeadlockSpinLock,
                                  KeGetCurrentThread(),
                                  FALSE,
                                  _ReturnAddress());

        return CurrentIrql;
    }
#endif

    CurrentIrql = KfAcquireSpinLock (SpinLock);

    VfDeadlockAcquireResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());

    return CurrentIrql;
}

typedef
VOID
(FASTCALL *PKF_RELEASE_SPINLOCK) (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
FASTCALL
VerifierKfReleaseSpinLock (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    )
{
    KIRQL CurrentIrql;
    PKF_RELEASE_SPINLOCK HalRoutine;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better still be at DISPATCH_LEVEL when releasing the spinlock.
    //

    if (CurrentIrql < DISPATCH_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x35,
                      CurrentIrql,
                      (ULONG_PTR)SpinLock,
                      NewIrql);
    }

    KfSanityCheckLowerIrql (NewIrql);

#if defined (_X86_)
    HalRoutine = (PKF_RELEASE_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KF_RELEASE_SPINLOCK];

    if (HalRoutine) {
        VfDeadlockReleaseResource(SpinLock,
                                  VfDeadlockSpinLock,
                                  KeGetCurrentThread(),
                                  _ReturnAddress());

        (*HalRoutine)(SpinLock, NewIrql);
        return;
    }
#endif

    VfDeadlockReleaseResource(SpinLock,
                              VfDeadlockSpinLock,
                              KeGetCurrentThread(),
                              _ReturnAddress());

    KfReleaseSpinLock (SpinLock, NewIrql);
}


#if !defined(NT_UP)

typedef
KIRQL
(FASTCALL *PKE_ACQUIRE_QUEUED_SPINLOCK) (
    IN KSPIN_LOCK_QUEUE_NUMBER Number
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKeAcquireQueuedSpinLock (
    IN KSPIN_LOCK_QUEUE_NUMBER Number
    )
{
    KIRQL CurrentIrql;
    PKE_ACQUIRE_QUEUED_SPINLOCK HalRoutine;

    CurrentIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (DISPATCH_LEVEL);

    MmVerifierData.AcquireSpinLocks += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if (CurrentIrql < DISPATCH_LEVEL) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKE_ACQUIRE_QUEUED_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_ACQUIRE_QUEUED_SPINLOCK];

    if (HalRoutine) {
        return (*HalRoutine)(Number);
    }
#endif


    CurrentIrql = KeAcquireQueuedSpinLock (Number);

    return CurrentIrql;
}

typedef
VOID
(FASTCALL *PKE_RELEASE_QUEUED_SPINLOCK) (
    IN KSPIN_LOCK_QUEUE_NUMBER Number,
    IN KIRQL OldIrql
    );

THUNKED_API
VOID
FASTCALL
VerifierKeReleaseQueuedSpinLock (
    IN KSPIN_LOCK_QUEUE_NUMBER Number,
    IN KIRQL OldIrql
    )
{
    KIRQL CurrentIrql;
    PKE_RELEASE_QUEUED_SPINLOCK HalRoutine;

    CurrentIrql = KeGetCurrentIrql ();

    if (KernelVerifier == TRUE) {
        if (CurrentIrql < DISPATCH_LEVEL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x36,
                          CurrentIrql,
                          (ULONG_PTR)Number,
                          (ULONG_PTR)OldIrql);
        }
    }

    KfSanityCheckLowerIrql (OldIrql);

#if defined (_X86_)
    HalRoutine = (PKE_RELEASE_QUEUED_SPINLOCK) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_RELEASE_QUEUED_SPINLOCK];

    if (HalRoutine) {
        (*HalRoutine)(Number, OldIrql);
        return;
    }
#endif

    KeReleaseQueuedSpinLock (Number, OldIrql);
}
#endif  // NT_UP

#endif  // _X86_

#if defined(_X86_) || defined(_AMD64_)

typedef
KIRQL
(FASTCALL *PKF_RAISE_IRQL) (
    IN KIRQL NewIrql
    );

THUNKED_API
KIRQL
FASTCALL
VerifierKfRaiseIrql (
    IN KIRQL NewIrql
    )
{
#if defined (_X86_)
    PKF_RAISE_IRQL HalRoutine;
#endif
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (NewIrql);

    MmVerifierData.RaiseIrqls += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if ((CurrentIrql < DISPATCH_LEVEL) && (NewIrql >= DISPATCH_LEVEL)) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKF_RAISE_IRQL) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KF_RAISE_IRQL];
    if (HalRoutine) {
        return (*HalRoutine)(NewIrql);
    }
#endif

    return KfRaiseIrql (NewIrql);
}

typedef
KIRQL
(FASTCALL *PKE_RAISE_IRQL_TO_DPC_LEVEL) (
    VOID
    );

THUNKED_API
KIRQL
VerifierKeRaiseIrqlToDpcLevel (
    VOID
    )
{
#if defined (_X86_)
    PKE_RAISE_IRQL_TO_DPC_LEVEL HalRoutine;
#endif
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (DISPATCH_LEVEL);

    MmVerifierData.RaiseIrqls += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if (CurrentIrql < DISPATCH_LEVEL) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKE_RAISE_IRQL_TO_DPC_LEVEL) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_RAISE_IRQL_TO_DPC_LEVEL];
    if (HalRoutine) {
        return (*HalRoutine)();
    }
#endif

    return KeRaiseIrqlToDpcLevel ();
}

#endif  // _X86_ || _AMD64_

#if defined(_X86_)

typedef
VOID
(FASTCALL *PKF_LOWER_IRQL) (
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
FASTCALL
VerifierKfLowerIrql (
    IN KIRQL NewIrql
    )
{
    PKF_LOWER_IRQL HalRoutine;

    KfSanityCheckLowerIrql (NewIrql);

#if defined (_X86_)
    HalRoutine = (PKF_LOWER_IRQL) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KF_LOWER_IRQL];
    if (HalRoutine) {
        (*HalRoutine)(NewIrql);
        return;
    }
#endif

    KfLowerIrql (NewIrql);
}

#endif

THUNKED_API
BOOLEAN
FASTCALL
VerifierExTryToAcquireFastMutex (
    IN PFAST_MUTEX FastMutex
    )
{
    KIRQL CurrentIrql;
    BOOLEAN Acquired;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at or below APC_LEVEL or have APCs blocked.
    //

    if (CurrentIrql > APC_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x33,
                      CurrentIrql,
                      (ULONG_PTR)FastMutex,
                      0);
    }

    Acquired = ExTryToAcquireFastMutex (FastMutex);
    if (Acquired != FALSE) {
        VfDeadlockAcquireResource(FastMutex,
                                  VfDeadlockFastMutex,
                                  KeGetCurrentThread(),
                                  TRUE,
                                  _ReturnAddress());
    }

    return Acquired;

}

THUNKED_API
VOID
FASTCALL
VerifierExAcquireFastMutex (
    IN PFAST_MUTEX FastMutex
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at or below APC_LEVEL or have APCs blocked.
    //

    if (CurrentIrql > APC_LEVEL) {
        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x33,
                      CurrentIrql,
                      (ULONG_PTR)FastMutex,
                      0);
    }

    ExAcquireFastMutex (FastMutex);

    VfDeadlockAcquireResource(FastMutex,
                              VfDeadlockFastMutex,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());
}

THUNKED_API
VOID
FASTCALL
VerifierExAcquireFastMutexUnsafe (
    IN PFAST_MUTEX FastMutex
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at APC_LEVEL or have APCs blocked.
    //

    if ((CurrentIrql != APC_LEVEL) &&
        (!IS_SYSTEM_THREAD(PsGetCurrentThread())) &&
        (KeGetCurrentThread()->KernelApcDisable == 0)) {

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x39,
                      CurrentIrql,
                      (ULONG_PTR)(KeGetCurrentThread()->KernelApcDisable),
                      (ULONG_PTR)FastMutex);
    }
    ExAcquireFastMutexUnsafe (FastMutex);

    VfDeadlockAcquireResource(FastMutex,
                              VfDeadlockFastMutexUnsafe,
                              KeGetCurrentThread(),
                              FALSE,
                              _ReturnAddress());
}

THUNKED_API
VOID
FASTCALL
VerifierExReleaseFastMutex (
    IN PFAST_MUTEX FastMutex
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at APC_LEVEL or have APCs blocked.
    //

    if ((CurrentIrql != APC_LEVEL) &&
        (!IS_SYSTEM_THREAD(PsGetCurrentThread())) &&
        (KeGetCurrentThread()->KernelApcDisable == 0)) {

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x34,
                      CurrentIrql,
                      (ULONG_PTR)(KeGetCurrentThread()->KernelApcDisable),
                      (ULONG_PTR)FastMutex);
    }

    VfDeadlockReleaseResource(FastMutex,
                              VfDeadlockFastMutex,
                              KeGetCurrentThread(),
                              _ReturnAddress());
    ExReleaseFastMutex (FastMutex);
}

THUNKED_API
VOID
FASTCALL
VerifierExReleaseFastMutexUnsafe (
    IN PFAST_MUTEX FastMutex
    )
{
    KIRQL CurrentIrql;

    CurrentIrql = KeGetCurrentIrql ();

    //
    // Caller better be at APC_LEVEL or have APCs blocked.
    //

    if ((CurrentIrql != APC_LEVEL) &&
        (!IS_SYSTEM_THREAD(PsGetCurrentThread())) &&
        (KeGetCurrentThread()->KernelApcDisable == 0)) {

        KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                      0x3A,
                      CurrentIrql,
                      (ULONG_PTR)(KeGetCurrentThread()->KernelApcDisable),
                      (ULONG_PTR)FastMutex);
    }

    VfDeadlockReleaseResource(FastMutex,
                              VfDeadlockFastMutexUnsafe,
                              KeGetCurrentThread(),
                              _ReturnAddress());
    ExReleaseFastMutexUnsafe (FastMutex);

}

typedef
VOID
(*PKE_RAISE_IRQL) (
    IN KIRQL NewIrql,
    OUT PKIRQL OldIrql
    );

THUNKED_API
VOID
VerifierKeRaiseIrql (
    IN KIRQL NewIrql,
    OUT PKIRQL OldIrql
    )
{
#if defined (_X86_)
    PKE_RAISE_IRQL HalRoutine;
#endif

    *OldIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (NewIrql);

    MmVerifierData.RaiseIrqls += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if ((*OldIrql < DISPATCH_LEVEL) && (NewIrql >= DISPATCH_LEVEL)) {
            ViTrimAllSystemPagableMemory ();
        }
    }

#if defined (_X86_)
    HalRoutine = (PKE_RAISE_IRQL) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_RAISE_IRQL];
    if (HalRoutine) {
        (*HalRoutine)(NewIrql, OldIrql);
        return;
    }
#endif

    KeRaiseIrql (NewIrql, OldIrql);
}

typedef
VOID
(*PKE_LOWER_IRQL) (
    IN KIRQL NewIrql
    );

THUNKED_API
VOID
VerifierKeLowerIrql (
    IN KIRQL NewIrql
    )
{
#if defined (_X86_)
    PKE_LOWER_IRQL HalRoutine;
#endif

    KfSanityCheckLowerIrql (NewIrql);

#if defined (_X86_)
    HalRoutine = (PKE_LOWER_IRQL) (ULONG_PTR) MiKernelVerifierOriginalCalls[VI_KE_LOWER_IRQL];
    if (HalRoutine) {
        (*HalRoutine)(NewIrql);
        return;
    }
#endif

    KeLowerIrql (NewIrql);
}

THUNKED_API
BOOLEAN
VerifierSynchronizeExecution (
    IN PKINTERRUPT Interrupt,
    IN PKSYNCHRONIZE_ROUTINE SynchronizeRoutine,
    IN PVOID SynchronizeContext
    )
{
    KIRQL OldIrql;

    OldIrql = KeGetCurrentIrql ();

    KfSanityCheckRaiseIrql (Interrupt->SynchronizeIrql);

    MmVerifierData.SynchronizeExecutions += 1;

    if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {
        if ((OldIrql < DISPATCH_LEVEL) && (Interrupt->SynchronizeIrql >= DISPATCH_LEVEL)) {
            ViTrimAllSystemPagableMemory ();
        }
    }

    return KeSynchronizeExecution (Interrupt,
                                   SynchronizeRoutine,
                                   SynchronizeContext);
}

THUNKED_API
VOID
VerifierKeInitializeTimerEx(
    IN PKTIMER Timer,
    IN TIMER_TYPE Type
    )
{
    //
    // Check the object being initialized isn't already an
    // active timer.  Make sure the timer table list is initialized.
    //

    if (KiTimerTableListHead[0].Flink != NULL) {
        KeCheckForTimer(Timer, sizeof(KTIMER));
    }

    KeInitializeTimerEx(Timer, Type);
}

THUNKED_API
VOID
VerifierKeInitializeTimer(
    IN PKTIMER Timer
    )
{
    VerifierKeInitializeTimerEx(Timer, NotificationTimer);
}


THUNKED_API
NTSTATUS
VerifierKeWaitForSingleObject (
    IN PVOID Object,
    IN KWAIT_REASON WaitReason,
    IN KPROCESSOR_MODE WaitMode,
    IN BOOLEAN Alertable,
    IN PLARGE_INTEGER Timeout OPTIONAL
    )
{
    KIRQL CurrentIrql;
    PRKTHREAD Thread;
    NTSTATUS Status;
    BOOLEAN TryAcquire;

    if (! ((ARGUMENT_PRESENT(Timeout)) && (Timeout->QuadPart == 0))) {

        CurrentIrql = KeGetCurrentIrql ();

        if (CurrentIrql >= DISPATCH_LEVEL) {

            Thread = KeGetCurrentThread();

            //
            // Skip situations where KeWait is called at DPC level
            // to optimize dispatcher database lock handling.
            //

            if (Thread->WaitNext == FALSE) {

                //
                // Cannot call KeWait at DPC level.
                //

                KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                              0x3B,
                              (ULONG_PTR)CurrentIrql,
                              (ULONG_PTR)Object,
                              (ULONG_PTR)Timeout);
            }
        }
    }


    Status = KeWaitForSingleObject (Object,
                                    WaitReason,
                                    WaitMode,
                                    Alertable,
                                    Timeout);

    if ((STATUS_SUCCESS == Status) &&
        (((PRKMUTANT) Object)->Header.Type == MutantObject)) {

        if (ARGUMENT_PRESENT(Timeout)) {
            TryAcquire = TRUE;
        }
        else {
            TryAcquire = FALSE;
        }

        VfDeadlockAcquireResource (Object,
                                   VfDeadlockMutex,
                                   KeGetCurrentThread (),
                                   TryAcquire,
                                   _ReturnAddress ());
    }

    return Status;

}

THUNKED_API
LONG
VerifierKeReleaseMutex(
    IN PRKMUTEX Mutex,
    IN BOOLEAN Wait
    )
{
    VfDeadlockReleaseResource(Mutex,
                              VfDeadlockMutex,
                              KeGetCurrentThread(),
                              _ReturnAddress());

    return KeReleaseMutex(Mutex, Wait);
}

THUNKED_API
VOID
VerifierKeInitializeMutex(
    IN PRKMUTEX Mutex,
    IN ULONG Level
    )
{
    KeInitializeMutex (Mutex,Level);
    VfDeadlockInitializeResource (Mutex, VfDeadlockMutex, _ReturnAddress(), FALSE);
}

THUNKED_API
LONG
VerifierKeReleaseMutant(
    IN PRKMUTANT Mutant,
    IN KPRIORITY Increment,
    IN BOOLEAN Abandoned,
    IN BOOLEAN Wait
    )
{
    VfDeadlockReleaseResource(Mutant,
                              VfDeadlockMutex,
                              KeGetCurrentThread(),
                              _ReturnAddress());

    return KeReleaseMutant(Mutant, Increment, Abandoned, Wait);
}

THUNKED_API
VOID
VerifierKeInitializeMutant(
    IN PRKMUTANT Mutant,
    IN BOOLEAN InitialOwner
    )
{
    KeInitializeMutant (Mutant, InitialOwner);
    VfDeadlockInitializeResource (Mutant, VfDeadlockMutex, _ReturnAddress(), FALSE);

    if (InitialOwner) {

        VfDeadlockAcquireResource (Mutant,
                                   VfDeadlockMutex,
                                   KeGetCurrentThread(),
                                   FALSE,
                                   _ReturnAddress());
    }
}

THUNKED_API
VOID
VerifierKeInitializeSpinLock(
    IN PKSPIN_LOCK  SpinLock
    )
{
    KeInitializeSpinLock (SpinLock);
    VfDeadlockInitializeResource (SpinLock, VfDeadlockSpinLock, _ReturnAddress(), FALSE);

}

NTSTATUS
VerifierReferenceObjectByHandle (
    IN HANDLE Handle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_TYPE ObjectType OPTIONAL,
    IN KPROCESSOR_MODE AccessMode,
    OUT PVOID *Object,
    OUT POBJECT_HANDLE_INFORMATION HandleInformation OPTIONAL
    )
{
    NTSTATUS Status;

    Status = ObReferenceObjectByHandle (Handle,
                                        DesiredAccess,
                                        ObjectType,
                                        AccessMode,
                                        Object,
                                        HandleInformation);
    if (AccessMode == KernelMode || PsIsSystemThread (PsGetCurrentThread ())) {
        if (Status == STATUS_INVALID_HANDLE || Status == STATUS_OBJECT_TYPE_MISMATCH) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x3C,
                          (ULONG_PTR)Handle,
                          (ULONG_PTR)ObjectType,
                          (ULONG_PTR)0);
        }
    }
    return Status;
}


VOID
ViInitializeEntry (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier,
    IN LOGICAL FirstLoad
    )

/*++

Routine Description:

    Initialize various verifier fields as the driver is being (re)loaded now.

Arguments:

    Verifier - Supplies the verifier entry to be initialized.

    FirstLoad - Supplies TRUE if this is the first load of this driver.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    //
    // Only the BaseName field is initialized on entry.
    //

    KeInitializeSpinLock (&Verifier->VerifierPoolLock);

    Verifier->CurrentPagedPoolAllocations = 0;
    Verifier->CurrentNonPagedPoolAllocations = 0;
    Verifier->PeakPagedPoolAllocations = 0;
    Verifier->PeakNonPagedPoolAllocations = 0;

    Verifier->PagedBytes = 0;
    Verifier->NonPagedBytes = 0;
    Verifier->PeakPagedBytes = 0;
    Verifier->PeakNonPagedBytes = 0;

    Verifier->PoolHash = NULL;
    Verifier->PoolHashSize = 0;
    Verifier->PoolHashFree = VI_POOL_FREELIST_END;
    Verifier->PoolHashReserved = 0;

    Verifier->Signature = MI_VERIFIER_ENTRY_SIGNATURE;

    if (FirstLoad == TRUE) {
        Verifier->Flags = 0;
        Verifier->Loads = 0;
        Verifier->Unloads = 0;
    }

    ExAcquireSpinLock (&VerifierListLock, &OldIrql);
    Verifier->StartAddress = NULL;
    Verifier->EndAddress = NULL;
    ExReleaseSpinLock (&VerifierListLock, OldIrql);
}

#define UNICODE_TAB               0x0009
#define UNICODE_LF                0x000A
#define UNICODE_CR                0x000D
#define UNICODE_SPACE             0x0020
#define UNICODE_CJK_SPACE         0x3000

#define UNICODE_WHITESPACE(_ch)     (((_ch) == UNICODE_TAB) || \
                                     ((_ch) == UNICODE_LF) || \
                                     ((_ch) == UNICODE_CR) || \
                                     ((_ch) == UNICODE_SPACE) || \
                                     ((_ch) == UNICODE_CJK_SPACE) || \
                                     ((_ch) == UNICODE_NULL))

LOGICAL
MiInitializeDriverVerifierList (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    Parse the registry settings and set up the list of driver names that will
    be put through the validation process.

    It is important that this list be parsed early because the machine-specific
    memory management initialization needs to know whether the verifier is
    going to be enabled.

Arguments:

    LoaderBlock - Supplies the loader block used by the system to boot.

Return Value:

    TRUE if successful, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization.

    Nonpaged (but not paged) pool exists.

    The PsLoadedModuleList has not been set up yet AND the boot drivers
    have NOT been relocated to their final resting places.

--*/

{
    PWCHAR Start;
    PWCHAR End;
    PWCHAR Walk;
    ULONG NameLength;
    LARGE_INTEGER CurrentTime;
    UNICODE_STRING KernelString;
    UNICODE_STRING DriverBaseName;

    UNREFERENCED_PARAMETER (LoaderBlock);

    InitializeListHead (&MiSuspectDriverList);

    if (MmVerifyDriverLevel != (ULONG)-1) {
        if (MmVerifyDriverLevel & DRIVER_VERIFIER_IO_CHECKING) {
            if (MmVerifyDriverBufferLength == (ULONG)-1) {
                MmVerifyDriverBufferLength = 0;     // Mm will not page out verifier pages.
            }
        }
    }

    if (MmVerifyDriverBufferLength == (ULONG)-1) {

        if (MmDontVerifyRandomDrivers == TRUE) {
            return FALSE;
        }

        MmVerifyDriverBufferLength = 0;

        CurrentTime = KeQueryPerformanceCounter (NULL);
        CurrentTime.LowPart = (CurrentTime.LowPart % 26);

        MiVerifyRandomDrivers = (WCHAR)('A' + (WCHAR)CurrentTime.LowPart);

        if ((MiVerifyRandomDrivers == (WCHAR)'H') ||
            (MiVerifyRandomDrivers == (WCHAR)'J') ||
            (MiVerifyRandomDrivers == (WCHAR)'X') ||
            (MiVerifyRandomDrivers == (WCHAR)'Y') ||
            (MiVerifyRandomDrivers == (WCHAR)'Z')) {
                MiVerifyRandomDrivers = (WCHAR)'X';
        }
    }

    KeInitializeSpinLock (&VerifierListLock);

    KeInitializeSpinLock (&VerifierPoolLock);
    ExInitializeFastMutex (&VerifierPoolMutex);

    //
    // Initializing this listhead indicates to the rest of this module that
    // the system was booted with verification of some sort configured.
    //

    InitializeListHead (&MiVerifierDriverAddedThunkListHead);

    //
    // If no default is specified, then special pool, pagable code/data
    // flushing and pool leak detection are enabled.
    //

    if (MmVerifyDriverLevel == (ULONG)-1) {
        MmVerifierData.Level = DRIVER_VERIFIER_SPECIAL_POOLING |
                               DRIVER_VERIFIER_FORCE_IRQL_CHECKING |
                               DRIVER_VERIFIER_TRACK_POOL_ALLOCATIONS;
    }
    else {
        MmVerifierData.Level = MmVerifyDriverLevel;
    }

    VerifierModifyableOptions = (DRIVER_VERIFIER_SPECIAL_POOLING |
                                 DRIVER_VERIFIER_FORCE_IRQL_CHECKING |
                                 DRIVER_VERIFIER_INJECT_ALLOCATION_FAILURES);

    //
    // An initial parse of the driver list is needed here to see if it's the
    // kernel as special machine-dependent initialization is needed to fully
    // support kernel verification (ie: no use of large pages, etc).
    //

    if (MiVerifyRandomDrivers == (WCHAR)0) {

        RtlInitUnicodeString (&KernelString, (PUSHORT)L"ntoskrnl.exe");

        Start = MmVerifyDriverBuffer;
        End = MmVerifyDriverBuffer + (MmVerifyDriverBufferLength - sizeof(WCHAR)) / sizeof(WCHAR);

        while (Start < End) {
            if (UNICODE_WHITESPACE(*Start)) {
                Start += 1;
                continue;
            }

            if (*Start == (WCHAR)'*') {
                MiVerifyAllDrivers = TRUE;
                break;
            }

            for (Walk = Start; Walk < End; Walk += 1) {
                if (UNICODE_WHITESPACE(*Walk)) {
                    break;
                }
            }

            //
            // Got a string - see if it indicates the kernel.
            //

            NameLength = (ULONG)(Walk - Start + 1) * sizeof (WCHAR);

            DriverBaseName.Buffer = Start;
            DriverBaseName.Length = (USHORT)(NameLength - sizeof (UNICODE_NULL));
            DriverBaseName.MaximumLength = (USHORT)NameLength;

            if (RtlEqualUnicodeString (&KernelString,
                                       &DriverBaseName,
                                       TRUE)) {

                //
                // AcquireAtDpc/ReleaseFromDpc calls made by the kernel are not 
                // intercepted which confuses the deadlock verifier.  So disable 
                // deadlock verification if we are kernel verifying.
                //

                MmVerifyDriverLevel &= ~DRIVER_VERIFIER_DEADLOCK_DETECTION;
                MmVerifierData.Level &= ~DRIVER_VERIFIER_DEADLOCK_DETECTION;

                //
                //
                // All driver pool allocation calls must be intercepted so
                // they are not mistaken for kernel pool allocations.
                //

                MiVerifyAllDrivers = TRUE;
                KernelVerifier = TRUE;
                ExSetPoolFlags (EX_KERNEL_VERIFIER_ENABLED);
                break;
            }

            Start = Walk + 1;
        }
    }

    return TRUE;
}

VOID
MiReApplyVerifierToLoadedModules(
    IN PLIST_ENTRY ModuleListHead
    )

/*++

Routine Description:

    Walk the supplied module list and re-thunk any drivers that are being
    verified.  This allows the module to pick up any new thunks that have
    been added.

Arguments:

    ModuleListHead - Supplies a pointer to the head of a loaded module list.

Environment:

    Kernel mode, Phase 0 Initialization only.

--*/

{
    LOGICAL Skip;
    PLIST_ENTRY Entry;
    PKLDR_DATA_TABLE_ENTRY TableEntry;
    UNICODE_STRING HalString;
    UNICODE_STRING KernelString;

    //
    // If the thunk listhead is NULL then the verifier is not enabled so
    // don't notify any components.
    //

    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        return;
    }

    //
    // Initialize unicode strings to use to bypass modules
    // in the list.  There's no reason to reapply verifier to
    // the kernel or to the hal.
    //

    RtlInitUnicodeString (&KernelString, (PUSHORT)L"ntoskrnl.exe");
    RtlInitUnicodeString (&HalString, (PUSHORT)L"hal.dll");

    //
    // Walk the list and reapply verifier to all the modules except those
    // selected for exclusion.
    //

    Entry = ModuleListHead->Flink;
    while (Entry != ModuleListHead) {

        TableEntry = CONTAINING_RECORD(Entry,
                                       KLDR_DATA_TABLE_ENTRY,
                                       InLoadOrderLinks);

        Skip = TRUE;

        if (RtlEqualUnicodeString (&KernelString,
                                   &TableEntry->BaseDllName,
                                   TRUE)) {
            NOTHING;
        }
        else if (RtlEqualUnicodeString (&HalString,
                                        &TableEntry->BaseDllName,
                                        TRUE)) {
            NOTHING;
        }
        else {
            Skip = FALSE;
        }

        //
        // Reapply verifier thunks to the image if it is already being
        // verified and if it is not one of the modules we've decided to skip.
        //

        if ((Skip == FALSE) && (TableEntry->Flags & LDRP_IMAGE_VERIFYING)) {
#if DBG
            PLIST_ENTRY NextEntry;
            PMI_VERIFIER_DRIVER_ENTRY Verifier;

            //
            // Initializing Verifier is not needed for correctness, but
            // without it the compiler cannot compile this code W4 to check
            // for use of uninitialized variables.
            //

            Verifier = NULL;

            //
            // Find the entry for this driver in the suspect list.  This is
            // expected to succeed since we are re-applying thunks to a module
            // that has already been verified at least once before.
            //

            NextEntry = MiSuspectDriverList.Flink;
            while (NextEntry != &MiSuspectDriverList) {

                Verifier = CONTAINING_RECORD(NextEntry,
                                             MI_VERIFIER_DRIVER_ENTRY,
                                             Links);

                if (RtlEqualUnicodeString (&Verifier->BaseName,
                                           &TableEntry->BaseDllName,
                                           TRUE)) {

                    break;
                }
                NextEntry = NextEntry->Flink;
            }

            ASSERT (NextEntry != &MiSuspectDriverList);

            //
            // Sanity tests.  We should always find this module in the suspect
            // driver list because it is already being verified.  And the
            // start and end addresses should still match those of the this
            // module.
            //

            ASSERT(NextEntry != &MiSuspectDriverList);
            ASSERT(Verifier->StartAddress == TableEntry->DllBase);
            ASSERT(Verifier->EndAddress ==
                   (PVOID)((ULONG_PTR)TableEntry->DllBase +
                           TableEntry->SizeOfImage));
#endif
            MiReEnableVerifier (TableEntry);
        }

        Entry = Entry->Flink;
    }
}

LOGICAL
MiInitializeVerifyingComponents (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    Walk the loaded module list and thunk any drivers that need/deserve it.

Arguments:

    LoaderBlock - Supplies the loader block used by the system to boot.

Return Value:

    TRUE if successful, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization.

    Both nonpaged and paged pool exist.

    The PsLoadedModuleList has not been set up yet although the boot drivers
    have been relocated to their final resting places.

--*/

{
    ULONG i;
    PWCHAR Start;
    PWCHAR End;
    PWCHAR Walk;
    ULONG NameLength;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    PMI_VERIFIER_DRIVER_ENTRY KernelEntry;
    PMI_VERIFIER_DRIVER_ENTRY HalEntry;
    UNICODE_STRING HalString;
    UNICODE_STRING KernelString;
    PVERIFIER_THUNKS Thunk;
    PDRIVER_VERIFIER_THUNK_ROUTINE PristineRoutine;

    //
    // If the thunk listhead is NULL then the verifier is not enabled so
    // don't notify any components.
    //

    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        return FALSE;
    }

    KernelEntry = NULL;
    HalEntry = NULL;

    if (MiVerifyRandomDrivers == (WCHAR)0) {

        RtlInitUnicodeString (&KernelString, (PUSHORT)L"ntoskrnl.exe");
        RtlInitUnicodeString (&HalString, (PUSHORT)L"hal.dll");

        Start = MmVerifyDriverBuffer;
        End = MmVerifyDriverBuffer + (MmVerifyDriverBufferLength - sizeof(WCHAR)) / sizeof(WCHAR);

        while (Start < End) {
            if (UNICODE_WHITESPACE(*Start)) {
                Start += 1;
                continue;
            }

            if (*Start == (WCHAR)'*') {
                MiVerifyAllDrivers = TRUE;
                break;
            }

            for (Walk = Start; Walk < End; Walk += 1) {
                if (UNICODE_WHITESPACE(*Walk)) {
                    break;
                }
            }

            //
            // Got a string.  Save it.
            //

            NameLength = (ULONG)(Walk - Start + 1) * sizeof (WCHAR);

            Verifier = (PMI_VERIFIER_DRIVER_ENTRY)ExAllocatePoolWithTag (
                                        NonPagedPool,
                                        sizeof (MI_VERIFIER_DRIVER_ENTRY) +
                                                            NameLength,
                                        'dLmM');

            if (Verifier == NULL) {
                break;
            }

            Verifier->BaseName.Buffer = (PWSTR)((PCHAR)Verifier +
                                                sizeof (MI_VERIFIER_DRIVER_ENTRY));
            Verifier->BaseName.Length = (USHORT)(NameLength - sizeof (UNICODE_NULL));
            Verifier->BaseName.MaximumLength = (USHORT)NameLength;

            RtlCopyMemory (Verifier->BaseName.Buffer,
                           Start,
                           NameLength - sizeof (UNICODE_NULL));

            ViInitializeEntry (Verifier, TRUE);

            Verifier->Flags |= VI_VERIFYING_DIRECTLY;

            ViInsertVerifierEntry (Verifier);

            if (RtlEqualUnicodeString (&KernelString,
                                       &Verifier->BaseName,
                                       TRUE)) {

                //
                // All driver pool allocation calls must be intercepted so
                // they are not mistaken for kernel pool allocations.
                //

                ASSERT (MiVerifyAllDrivers == TRUE);
                ASSERT (KernelVerifier == TRUE);

                KernelEntry = Verifier;

            }
            else if (RtlEqualUnicodeString (&HalString,
                                            &Verifier->BaseName,
                                            TRUE)) {

                HalEntry = Verifier;
            }

            Start = Walk + 1;
        }
    }

    //
    // Enable deadlock detection if the deadlock bit was set in the
    // registry.
    //

    if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
        VfDeadlockDetectionInitialize (MiVerifyAllDrivers, KernelVerifier);
        ExSetPoolFlags (EX_VERIFIER_DEADLOCK_DETECTION_ENABLED);
    }

    //
    // Initialize i/o verifier.
    //

    IoVerifierInit (MmVerifierData.Level);

    if (MiTriageAddDrivers (LoaderBlock) == TRUE) {

        //
        // Disable random driver verification if triage has picked driver(s).
        //

        MiVerifyRandomDrivers = (WCHAR)0;
    }

    Thunk = (PVERIFIER_THUNKS) &MiVerifierThunks[0];

    while (Thunk->PristineRoutineAsciiName != NULL) {
        PristineRoutine = MiResolveVerifierExports (LoaderBlock,
                                                    Thunk->PristineRoutineAsciiName);
        ASSERT (PristineRoutine != NULL);
        Thunk->PristineRoutine = PristineRoutine;
        Thunk += 1;
    }

    Thunk = (PVERIFIER_THUNKS) &MiVerifierPoolThunks[0];
    while (Thunk->PristineRoutineAsciiName != NULL) {
        PristineRoutine = MiResolveVerifierExports (LoaderBlock,
                                                    Thunk->PristineRoutineAsciiName);
        ASSERT (PristineRoutine != NULL);
        Thunk->PristineRoutine = PristineRoutine;
        Thunk += 1;
    }

    //
    // Process the boot-loaded drivers now.
    //

    i = 0;
    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    for ( ; NextEntry != &LoaderBlock->LoadOrderListHead; NextEntry = NextEntry->Flink) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        //
        // Process the kernel and HAL specially.
        //

        if (i == 0) {
            if (KernelEntry != NULL) {
                MiApplyDriverVerifier (DataTableEntry, KernelEntry);
            }
        }
        else if (i == 1) {
            if (HalEntry != NULL) {
                MiApplyDriverVerifier (DataTableEntry, HalEntry);
            }
        }
        else {
            MiApplyDriverVerifier (DataTableEntry, NULL);
        }
        i += 1;
    }

    //
    // Initialize irql tracking package. The drivers that will be verified
    // will have automatically tracked all their raise/lower irql operations.
    //

    ViTrackIrqlInitialize ();

    //
    // Initialize fault injection stack trace log package.
    //

#if defined(_X86_)
    ViFaultTracesInitialize ();
#endif

    return TRUE;
}

NTSTATUS
MmAddVerifierEntry (
    IN PUNICODE_STRING ImageFileName
    )

/*++

Routine Description:

    This routine inserts a new verifier entry for the specified driver so that
    when the driver is loaded it will automatically be verified.

    Note that if the driver is already loaded, then no entry is added and
    STATUS_IMAGE_ALREADY_LOADED is returned.

    If the system was booted with an empty verifier list, then no entries can
    be added now as the current system configuration will not support special
    pool, etc.

    Note also that no registry changes are made so any insertions made by this
    routine are lost on reboot.

Arguments:

    ImageFileName - Supplies the name of the desired driver.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, PASSIVE_LEVEL, arbitrary process context.

--*/

{
    PKTHREAD CurrentThread;
    PLIST_ENTRY NextEntry;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    PMI_VERIFIER_DRIVER_ENTRY VerifierEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // If the system was not booted with verification on, then bail.
    //

    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        return STATUS_NOT_SUPPORTED;
    }

    //
    // First build up a verifier entry.
    //

    Verifier = (PMI_VERIFIER_DRIVER_ENTRY)ExAllocatePoolWithTag (
                                NonPagedPool,
                                sizeof (MI_VERIFIER_DRIVER_ENTRY) +
                                    ImageFileName->MaximumLength,
                                'dLmM');

    if (Verifier == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Verifier->BaseName.Buffer = (PWSTR)((PCHAR)Verifier +
                                    sizeof (MI_VERIFIER_DRIVER_ENTRY));
    Verifier->BaseName.Length = ImageFileName->Length;
    Verifier->BaseName.MaximumLength = ImageFileName->MaximumLength;

    RtlCopyMemory (Verifier->BaseName.Buffer,
                   ImageFileName->Buffer,
                   ImageFileName->Length);

    ViInitializeEntry (Verifier, TRUE);

    Verifier->Flags |= VI_VERIFYING_DIRECTLY;

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    //
    // Acquire the load lock so the verifier list can be read.
    // Then ensure that the specified driver is not already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    //
    // Check to make sure the requested entry is not already present in
    // the verifier list and that the driver is not currently loaded.
    //

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        VerifierEntry = CONTAINING_RECORD(NextEntry,
                                          MI_VERIFIER_DRIVER_ENTRY,
                                          Links);

        if (RtlEqualUnicodeString (&Verifier->BaseName,
                                   &VerifierEntry->BaseName,
                                   TRUE)) {

            //
            // The driver is already in the verifier list - just mark the
            // entry as verification-enabled and free the temporary allocation.
            //

            if ((VerifierEntry->Loads > VerifierEntry->Unloads) &&
                (VerifierEntry->Flags & VI_DISABLE_VERIFICATION)) {

                //
                // The driver is loaded and verification is disabled.  Don't
                // turn it on now because we don't want to mislead our caller.
                //

                KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
                KeLeaveCriticalRegionThread (CurrentThread);
                ExFreePool (Verifier);
                return STATUS_IMAGE_ALREADY_LOADED;
            }
            VerifierEntry->Flags &= ~VI_DISABLE_VERIFICATION;
            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);
            ExFreePool (Verifier);
            return STATUS_SUCCESS;
        }
        NextEntry = NextEntry->Flink;
    }

    //
    // A new verifier entry will need to be added so check to
    // make sure the specified driver is not already loaded.
    //

    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        if (RtlEqualUnicodeString (&Verifier->BaseName,
                                   &DataTableEntry->BaseDllName,
                                   TRUE)) {

            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            ExReleaseResourceLite (&PsLoadedModuleResource);
            KeLeaveCriticalRegionThread (CurrentThread);
            ExFreePool (Verifier);
            return STATUS_IMAGE_ALREADY_LOADED;
        }

        NextEntry = NextEntry->Flink;
    }

    //
    // The entry is not already in the verifier list and the driver is not
    // currently loaded.  Proceed to insert it now.
    //

    ViInsertVerifierEntry (Verifier);

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    return STATUS_SUCCESS;
}

NTSTATUS
MmRemoveVerifierEntry (
    IN PUNICODE_STRING ImageFileName
    )

/*++

Routine Description:

    This routine doesn't actually remove the verifier entry for the
    specified driver as we don't want to lose any valuable information
    already gathered on the driver if it was previously loaded.
    Instead, this routine disables verification for this driver for future
    loads.

    Note that if the driver is already loaded, then the removal is not
    performed and STATUS_IMAGE_ALREADY_LOADED is returned.

    Note also that no registry changes are made so any removals made by this
    routine are lost on reboot.

Arguments:

    ImageFileName - Supplies the name of the desired driver.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, PASSIVE_LEVEL, arbitrary process context.

--*/

{
    PKTHREAD CurrentThread;
    PLIST_ENTRY NextEntry;
    PMI_VERIFIER_DRIVER_ENTRY VerifierEntry;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    //
    // If the system was not booted with verification on, then bail.
    //

    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        return STATUS_NOT_SUPPORTED;
    }

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    //
    // Acquire the load lock so the verifier list can be read.
    // Then ensure that the specified driver is not already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    //
    // Check to make sure the requested entry is not already present in
    // the verifier list and that the driver is not currently loaded.
    //

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        VerifierEntry = CONTAINING_RECORD(NextEntry,
                                          MI_VERIFIER_DRIVER_ENTRY,
                                          Links);

        if (RtlEqualUnicodeString (ImageFileName,
                                   &VerifierEntry->BaseName,
                                   TRUE)) {

            //
            // The driver is already in the verifier list - just mark the
            // entry as verification-enabled and free the temporary allocation.
            // No need to check the loaded module list if the entry is already
            // in the verifier list.
            //

            if ((VerifierEntry->Loads > VerifierEntry->Unloads) &&
                ((VerifierEntry->Flags & VI_DISABLE_VERIFICATION) == 0)) {

                //
                // The driver is loaded and verification is enabled.  Don't
                // disable it now because we don't want to mislead our caller.
                //

                KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
                KeLeaveCriticalRegionThread (CurrentThread);
                return STATUS_IMAGE_ALREADY_LOADED;
            }

            VerifierEntry->Flags |= VI_DISABLE_VERIFICATION;
            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);
            return STATUS_SUCCESS;
        }
        NextEntry = NextEntry->Flink;
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    return STATUS_NOT_FOUND;
}

VOID
ViInsertVerifierEntry (
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    )

/*++

Routine Description:

    Nonpagable wrapper to insert a new verifier entry.

    Note that the system load mutant or the verifier load spinlock is sufficient
    for readers to access the list.  This is because the insertion path
    acquires both.

    Lock synchronization is needed because pool allocators walk the
    verifier list at DISPATCH_LEVEL.

Arguments:

    Verifier - Supplies a caller-initialized entry for the driver.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    ExAcquireSpinLock (&VerifierListLock, &OldIrql);
    InsertTailList (&MiSuspectDriverList, &Verifier->Links);
    ExReleaseSpinLock (&VerifierListLock, OldIrql);
}

PMI_VERIFIER_DRIVER_ENTRY
ViLocateVerifierEntry (
    IN PVOID SystemAddress
    )

/*++

Routine Description:

    Locate the Driver Verifier entry for the specified system address.

Arguments:

    SystemAddress - Supplies a code or data address within a driver.

Return Value:

    The Verifier entry corresponding to the driver or NULL.

Environment:

    The caller may be at DISPATCH_LEVEL and does not hold the MmSystemLoadLock.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;

    ExAcquireSpinLock (&VerifierListLock, &OldIrql);

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        Verifier = CONTAINING_RECORD(NextEntry,
                                     MI_VERIFIER_DRIVER_ENTRY,
                                     Links);

        if ((SystemAddress >= Verifier->StartAddress) &&
            (SystemAddress < Verifier->EndAddress)) {

            ExReleaseSpinLock (&VerifierListLock, OldIrql);
            return Verifier;
        }
        NextEntry = NextEntry->Flink;
    }

    ExReleaseSpinLock (&VerifierListLock, OldIrql);
    return NULL;
}

LOGICAL
MiApplyDriverVerifier (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry,
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    )

/*++

Routine Description:

    This function is called as each module is loaded.  If the module being
    loaded is in the suspect list, thunk it here.

Arguments:

    DataTableEntry - Supplies the data table entry for the module.

    Verifier - Non-NULL if verification must be applied.  FALSE indicates
                      that the driver name must match for verification to be
                      applied.

Return Value:

    TRUE if thunking was applied, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization and normal runtime.
    Non paged pool exists in Phase0, but paged pool does not.
    Post-Phase0 serialization is provided by the MmSystemLoadLock.

--*/

{
    WCHAR FirstChar;
    LOGICAL Found;
    PLIST_ENTRY NextEntry;
    ULONG VerifierFlags;

    if (Verifier != NULL) {
        Found = TRUE;
    }
    else {
        Found = FALSE;
        NextEntry = MiSuspectDriverList.Flink;
        while (NextEntry != &MiSuspectDriverList) {

            Verifier = CONTAINING_RECORD(NextEntry,
                                         MI_VERIFIER_DRIVER_ENTRY,
                                         Links);

            if (RtlEqualUnicodeString (&Verifier->BaseName,
                                       &DataTableEntry->BaseDllName,
                                       TRUE)) {

                Found = TRUE;
                ViInitializeEntry (Verifier, FALSE);
                break;
            }
            NextEntry = NextEntry->Flink;
        }
    }

    if (Found == FALSE) {
        VerifierFlags = VI_VERIFYING_DIRECTLY;
        if (MiVerifyAllDrivers == TRUE) {
            if (KernelVerifier == TRUE) {
                VerifierFlags = VI_VERIFYING_INVERSELY;
            }
            Found = TRUE;
        }
        else if (MiVerifyRandomDrivers != (WCHAR)0) {

            //
            // Wildcard match drivers randomly.
            //

            FirstChar = RtlUpcaseUnicodeChar(DataTableEntry->BaseDllName.Buffer[0]);

            if (MiVerifyRandomDrivers == FirstChar) {
                Found = TRUE;
            }
            else if (MiVerifyRandomDrivers == (WCHAR)'X') {
                if ((FirstChar >= (WCHAR)'0') && (FirstChar <= (WCHAR)'9')) {
                    Found = TRUE;
                }
            }
        }

        if (Found == FALSE) {
            return FALSE;
        }

        Verifier = (PMI_VERIFIER_DRIVER_ENTRY)ExAllocatePoolWithTag (
                                    NonPagedPool,
                                    sizeof (MI_VERIFIER_DRIVER_ENTRY) +
                                        DataTableEntry->BaseDllName.MaximumLength,
                                    'dLmM');

        if (Verifier == NULL) {
            return FALSE;
        }

        Verifier->BaseName.Buffer = (PWSTR)((PCHAR)Verifier +
                                        sizeof (MI_VERIFIER_DRIVER_ENTRY));
        Verifier->BaseName.Length = DataTableEntry->BaseDllName.Length;
        Verifier->BaseName.MaximumLength = DataTableEntry->BaseDllName.MaximumLength;

        RtlCopyMemory (Verifier->BaseName.Buffer,
                       DataTableEntry->BaseDllName.Buffer,
                       DataTableEntry->BaseDllName.Length);

        ViInitializeEntry (Verifier, TRUE);

        Verifier->Flags = VerifierFlags;

        ViInsertVerifierEntry (Verifier);
    }

    Verifier->StartAddress = DataTableEntry->DllBase;
    Verifier->EndAddress = (PVOID)((ULONG_PTR)DataTableEntry->DllBase + DataTableEntry->SizeOfImage);

    ASSERT (Found == TRUE);

    if (Verifier->Flags & VI_DISABLE_VERIFICATION) {

        //
        // We've been instructed to not verify this driver.  If kernel
        // verification is enabled, then the driver must still be thunked
        // for "inverse-verification".  If kernel verification is disabled,
        // nothing needs to be done here except load/unload counting.
        //

        if (KernelVerifier == TRUE) {
            Found = MiEnableVerifier (DataTableEntry);
        }
        else {
            Found = FALSE;
        }
    }
    else {
        Found = MiEnableVerifier (DataTableEntry);
    }

    if (Found == TRUE) {

        if (Verifier->Flags & VI_VERIFYING_DIRECTLY &&
            ((DataTableEntry->Flags & LDRP_IMAGE_VERIFYING) == 0)) {
            ViPrintString (&DataTableEntry->BaseDllName);
        }

        MmVerifierData.Loads += 1;
        Verifier->Loads += 1;

        DataTableEntry->Flags |= LDRP_IMAGE_VERIFYING;
        MiActiveVerifies += 1;

        if (MiActiveVerifies == 1) {

#ifndef NO_POOL_CHECKS

            //
            // If a loaded driver(s) is undergoing validation, the default
            // special pool randomizer is disabled as the precious virtual
            // address space and physical memory is being put to specific
            // use.
            //

            MiEnableRandomSpecialPool (FALSE);
#endif
            if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {

                //
                // Page out all thread stacks as soon as possible to
                // catch drivers using local events that do usermode waits.
                //

                if (KernelVerifier == FALSE) {
                    MiVerifierStackProtectTime = KiStackProtectTime;
                    KiStackProtectTime = 0;
                }
            }
        }
    }

    return Found;
}

PUNICODE_STRING ViBadDriver;

VOID
MiVerifyingDriverUnloading (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function is called as a driver that was being verified is now being
    unloaded.

Arguments:

    DataTableEntry - Supplies the data table entry for the driver.

Return Value:

    TRUE if thunking was applied, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization and normal runtime.
    Non paged pool exists in Phase0, but paged pool does not.
    Post-Phase0 serialization is provided by the MmSystemLoadLock.

--*/

{
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    PVI_POOL_ENTRY OldHashTable;

    //
    // Initializing Verifier is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    Verifier = NULL;

    NextEntry = MiSuspectDriverList.Flink;
    while (NextEntry != &MiSuspectDriverList) {

        Verifier = CONTAINING_RECORD(NextEntry,
                                          MI_VERIFIER_DRIVER_ENTRY,
                                          Links);

        if (RtlEqualUnicodeString (&Verifier->BaseName,
                                   &DataTableEntry->BaseDllName,
                                   TRUE)) {

            break;
        }
        NextEntry = NextEntry->Flink;
    }

    ASSERT (NextEntry != &MiSuspectDriverList);

    //
    // Delete any static locks in the driver image.
    //
    // silviuc: might be able to get rid of this call if we get a
    // hook in MmSystemImageUnload.
    //

    VfDeadlockDeleteMemoryRange(DataTableEntry->DllBase,
                                (SIZE_T) DataTableEntry->SizeOfImage);


    if (MmVerifierData.Level & DRIVER_VERIFIER_TRACK_POOL_ALLOCATIONS) {

        //
        // Better not be any pool left that wasn't freed.
        //

        if (Verifier->PagedBytes) {

#if DBG
            DbgPrint ("Driver %wZ leaked %d paged pool allocations (0x%x bytes)\n",
                &DataTableEntry->FullDllName,
                Verifier->CurrentPagedPoolAllocations,
                Verifier->PagedBytes);
#endif

            //
            // It would be nice to fault in the driver's paged pool allocations
            // now to make debugging easier, but this cannot be easily done
            // in a deadlock free manner.
            //
            // At least disable the paging of pool on IRQL raising in attempt
            // to keep some of these allocations resident for debugging.
            // No need to undo the increment as we're about to bugcheck anyway.
            //

            InterlockedIncrement ((PLONG)&MiNoPageOnRaiseIrql);
        }
#if DBG
        if (Verifier->NonPagedBytes) {
            DbgPrint ("Driver %wZ leaked %d nonpaged pool allocations (0x%x bytes)\n",
                &DataTableEntry->FullDllName,
                Verifier->CurrentNonPagedPoolAllocations,
                Verifier->NonPagedBytes);
        }
#endif

        if (Verifier->PagedBytes || Verifier->NonPagedBytes) {
#if 0
            DbgBreakPoint ();
            InterlockedDecrement (&MiNoPageOnRaiseIrql);
#else
            //
            // Snap this so the build/BVT lab can easily triage the culprit.
            //

            ViBadDriver = &Verifier->BaseName;

            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x60,
                          Verifier->PagedBytes,
                          Verifier->NonPagedBytes,
                          Verifier->CurrentPagedPoolAllocations +
                            Verifier->CurrentNonPagedPoolAllocations);
#endif
        }

        ExAcquireSpinLock (&Verifier->VerifierPoolLock, &OldIrql);

        if (Verifier->PoolHashReserved != 0) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x61,
                          Verifier->PagedBytes,
                          Verifier->NonPagedBytes,
                          Verifier->CurrentPagedPoolAllocations +
                            Verifier->CurrentNonPagedPoolAllocations);
        }

        OldHashTable = Verifier->PoolHash;
        if (OldHashTable != NULL) {
            Verifier->PoolHashSize = 0;
            Verifier->PoolHashFree = VI_POOL_FREELIST_END;
            Verifier->PoolHash = NULL;
        }
        else {
            ASSERT (Verifier->PoolHashSize == 0);
            ASSERT (Verifier->PoolHashFree == VI_POOL_FREELIST_END);
        }

        ExReleaseSpinLock (&Verifier->VerifierPoolLock, OldIrql);

        if (OldHashTable != NULL) {
            ExFreePool (OldHashTable);
        }

        //
        // Clear these fields so reuse of stale addresses don't trigger
        // erroneous bucket fills.
        //

        ExAcquireSpinLock (&VerifierListLock, &OldIrql);
        Verifier->StartAddress = NULL;
        Verifier->EndAddress = NULL;
        ExReleaseSpinLock (&VerifierListLock, OldIrql);
    }

    Verifier->Unloads += 1;
    MmVerifierData.Unloads += 1;
    MiActiveVerifies -= 1;

    if (MiActiveVerifies == 0) {

        if (MmVerifierData.Level & DRIVER_VERIFIER_FORCE_IRQL_CHECKING) {

            //
            // Return to normal thread stack protection.
            //

            if (KernelVerifier == FALSE) {
                KiStackProtectTime = MiVerifierStackProtectTime;
            }
        }

#ifndef NO_POOL_CHECKS
        MiEnableRandomSpecialPool (TRUE);
#endif
    }
}

NTKERNELAPI
LOGICAL
MmIsDriverVerifying (
    IN PDRIVER_OBJECT DriverObject
    )

/*++

Routine Description:

    This function informs the caller if the argument driver is being verified.

Arguments:

    DriverObject - Supplies the driver object.

Return Value:

    TRUE if this driver is being verified, FALSE if not.

Environment:

    Kernel mode, any IRQL, any needed synchronization must be provided by the
    caller.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;

    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)DriverObject->DriverSection;

    if (DataTableEntry == NULL) {
        return FALSE;
    }

    if ((DataTableEntry->Flags & LDRP_IMAGE_VERIFYING) == 0) {
        return FALSE;
    }

    return TRUE;
}

NTSTATUS
MmAddVerifierThunks (
    IN PVOID ThunkBuffer,
    IN ULONG ThunkBufferSize
    )

/*++

Routine Description:

    This routine adds another set of thunks to the verifier list.

Arguments:

    ThunkBuffer - Supplies the buffer containing the thunk pairs.

    ThunkBufferSize - Supplies the number of bytes in the thunk buffer.

Return Value:

    Returns the status of the operation.

Environment:

    Kernel mode.  APC_LEVEL and below, arbitrary process context.

--*/

{
    ULONG i;
    PKTHREAD CurrentThread;
    ULONG NumberOfThunkPairs;
    PDRIVER_VERIFIER_THUNK_PAIRS ThunkPairs;
    PDRIVER_VERIFIER_THUNK_PAIRS ThunkTable;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry2;
    PLIST_ENTRY NextEntry;
    PVOID DriverStartAddress;
    PVOID DriverEndAddress;

    PAGED_CODE();

    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        return STATUS_NOT_SUPPORTED;
    }

    ThunkPairs = (PDRIVER_VERIFIER_THUNK_PAIRS)ThunkBuffer;
    NumberOfThunkPairs = ThunkBufferSize / sizeof(DRIVER_VERIFIER_THUNK_PAIRS);

    if (NumberOfThunkPairs == 0) {
        return STATUS_INVALID_PARAMETER_1;
    }

    ThunkTableBase = (PDRIVER_SPECIFIED_VERIFIER_THUNKS) ExAllocatePoolWithTag (
                            PagedPool,
                            sizeof (DRIVER_SPECIFIED_VERIFIER_THUNKS) + NumberOfThunkPairs * sizeof (DRIVER_VERIFIER_THUNK_PAIRS),
                            'tVmM');

    if (ThunkTableBase == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    ThunkTable = (PDRIVER_VERIFIER_THUNK_PAIRS)(ThunkTableBase + 1);

    RtlCopyMemory (ThunkTable,
                   ThunkPairs,
                   NumberOfThunkPairs * sizeof(DRIVER_VERIFIER_THUNK_PAIRS));

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    //
    // Find and validate the image that contains the routines to be thunked.
    //

    DataTableEntry = MiLookupDataTableEntry ((PVOID)(ULONG_PTR)ThunkTable->PristineRoutine,
                                             TRUE);

    if (DataTableEntry == NULL) {
        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        ExFreePool (ThunkTableBase);
        return STATUS_INVALID_PARAMETER_2;
    }

    DriverStartAddress = (PVOID)(DataTableEntry->DllBase);
    DriverEndAddress = (PVOID)((PCHAR)DataTableEntry->DllBase + DataTableEntry->SizeOfImage);

    //
    // Don't let drivers hook calls to kernel or HAL routines.
    //

    i = 0;
    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry2 = CONTAINING_RECORD(NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        if (DataTableEntry == DataTableEntry2) {
            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);
            ExFreePool (ThunkTableBase);
            return STATUS_INVALID_PARAMETER_2;
        }

        NextEntry = NextEntry->Flink;
        i += 1;
        if (i >= 2) {
            break;
        }
    }

    for (i = 0; i < NumberOfThunkPairs; i += 1) {

        //
        // Ensure all the routines being thunked are in the same driver.
        //

        if (((ULONG_PTR)ThunkTable->PristineRoutine < (ULONG_PTR)DriverStartAddress) ||
            ((ULONG_PTR)ThunkTable->PristineRoutine >= (ULONG_PTR)DriverEndAddress) ||
            ((ULONG_PTR)ThunkTable->NewRoutine < (ULONG_PTR)DriverStartAddress) ||
            ((ULONG_PTR)ThunkTable->NewRoutine >= (ULONG_PTR)DriverEndAddress)
        ) {

            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);
            ExFreePool (ThunkTableBase);
            return STATUS_INVALID_PARAMETER_2;
        }
        ThunkTable += 1;
    }

    //
    // Add the validated thunk table to the verifier's global list.
    //

    ThunkTableBase->DataTableEntry = DataTableEntry;
    ThunkTableBase->NumberOfThunks = NumberOfThunkPairs;
    MiActiveVerifierThunks += 1;

    InsertTailList (&MiVerifierDriverAddedThunkListHead,
                    &ThunkTableBase->ListEntry);

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    //
    // Indicate that new thunks have been added to the verifier list.
    //

    MiVerifierThunksAdded += 1;

    return STATUS_SUCCESS;
}

VOID
MiVerifierCheckThunks (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This routine adds another set of thunks to the verifier list.

Arguments:

    DataTableEntry - Supplies the data table entry for the driver.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL and below.
    The system load lock must be held by the caller.

--*/

{
    PLIST_ENTRY NextEntry;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;

    PAGED_CODE ();

    //
    // N.B.  The DataTableEntry can move (see MiInitializeLoadedModuleList),
    //       but this only happens long before IoInitialize so this is safe.
    //

    NextEntry = MiVerifierDriverAddedThunkListHead.Flink;
    while (NextEntry != &MiVerifierDriverAddedThunkListHead) {

        ThunkTableBase = CONTAINING_RECORD(NextEntry,
                                           DRIVER_SPECIFIED_VERIFIER_THUNKS,
                                           ListEntry);

        if (ThunkTableBase->DataTableEntry == DataTableEntry) {
            RemoveEntryList (NextEntry);
            NextEntry = NextEntry->Flink;
            ExFreePool (ThunkTableBase);
            MiActiveVerifierThunks -= 1;

            //
            // Keep looking as the driver may have made multiple calls.
            //

            continue;
        }

        NextEntry = NextEntry->Flink;
    }
}

NTSTATUS
MmIsVerifierEnabled (
    OUT PULONG VerifierFlags
    )

/*++

Routine Description:

    This routine is called by drivers to query whether the Driver Verifier
    is enabled and to find out what the currently enabled options are.

Arguments:

    VerifierFlags - Returns the current driver verifier flags.  Note these
                    flags can change dynamically without rebooting.

Return Value:

    Returns STATUS_SUCCESS if the verifier is enabled, or a failure code if not.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    if (MiVerifierDriverAddedThunkListHead.Flink == NULL) {
        *VerifierFlags = 0;
        return STATUS_NOT_SUPPORTED;
    }

    *VerifierFlags = MmVerifierData.Level;
    return STATUS_SUCCESS;
}

#define ROUND_UP(VALUE,ROUND) ((ULONG)(((ULONG)VALUE + \
                               ((ULONG)ROUND - 1L)) & (~((ULONG)ROUND - 1L))))

NTSTATUS
MmGetVerifierInformation (
    OUT PVOID SystemInformation,
    IN ULONG SystemInformationLength,
    OUT PULONG Length
    )

/*++

Routine Description:

    This routine returns information about drivers undergoing verification.

Arguments:

    SystemInformation - Returns the driver verification information.

    SystemInformationLength - Supplies the length of the SystemInformation
                              buffer.

    Length - Returns the length of the driver verification file information
             placed in the buffer.

Return Value:

    Returns the status of the operation.

Environment:

    The SystemInformation buffer is in user space and our caller has wrapped
    a try-except around this entire routine.  Capture any exceptions here and
    release resources accordingly.

--*/

{
    PKTHREAD CurrentThread;
    PSYSTEM_VERIFIER_INFORMATION UserVerifyBuffer;
    ULONG NextEntryOffset;
    ULONG TotalSize;
    NTSTATUS Status;
    PLIST_ENTRY NextEntry;
    PMI_VERIFIER_DRIVER_ENTRY Verifier;
    UNICODE_STRING UserBufferDriverName;

    PAGED_CODE();

    NextEntryOffset = 0;
    TotalSize = 0;

    *Length = 0;
    UserVerifyBuffer = (PSYSTEM_VERIFIER_INFORMATION)SystemInformation;

    //
    // Capture the number of verifying drivers and the relevant data while
    // synchronized.  Then return it to our caller.
    //

    Status = STATUS_SUCCESS;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    try {

        NextEntry = MiSuspectDriverList.Flink;
        while (NextEntry != &MiSuspectDriverList) {

            Verifier = CONTAINING_RECORD(NextEntry,
                                              MI_VERIFIER_DRIVER_ENTRY,
                                              Links);

            if (((Verifier->Flags & VI_VERIFYING_DIRECTLY) == 0) ||
                (Verifier->Flags & VI_DISABLE_VERIFICATION)) {

                NextEntry = NextEntry->Flink;
                continue;
            }

            UserVerifyBuffer = (PSYSTEM_VERIFIER_INFORMATION)(
                                    (PUCHAR)UserVerifyBuffer + NextEntryOffset);
            NextEntryOffset = sizeof(SYSTEM_VERIFIER_INFORMATION);
            TotalSize += sizeof(SYSTEM_VERIFIER_INFORMATION);

            if (TotalSize > SystemInformationLength) {
                ExRaiseStatus (STATUS_INFO_LENGTH_MISMATCH);
            }

            //
            // This data is cumulative for all drivers.
            //

            UserVerifyBuffer->Level = MmVerifierData.Level;
            UserVerifyBuffer->RaiseIrqls = MmVerifierData.RaiseIrqls;
            UserVerifyBuffer->AcquireSpinLocks = MmVerifierData.AcquireSpinLocks;

            UserVerifyBuffer->UnTrackedPool = MmVerifierData.UnTrackedPool;
            UserVerifyBuffer->SynchronizeExecutions = MmVerifierData.SynchronizeExecutions;

            UserVerifyBuffer->AllocationsAttempted = MmVerifierData.AllocationsAttempted;
            UserVerifyBuffer->AllocationsSucceeded = MmVerifierData.AllocationsSucceeded;
            UserVerifyBuffer->AllocationsSucceededSpecialPool = MmVerifierData.AllocationsSucceededSpecialPool;
            UserVerifyBuffer->AllocationsWithNoTag = MmVerifierData.AllocationsWithNoTag;

            UserVerifyBuffer->TrimRequests = MmVerifierData.TrimRequests;
            UserVerifyBuffer->Trims = MmVerifierData.Trims;
            UserVerifyBuffer->AllocationsFailed = MmVerifierData.AllocationsFailed;
            UserVerifyBuffer->AllocationsFailedDeliberately = MmVerifierData.AllocationsFailedDeliberately;

            //
            // This data is kept on a per-driver basis.
            //

            UserVerifyBuffer->CurrentPagedPoolAllocations = Verifier->CurrentPagedPoolAllocations;
            UserVerifyBuffer->CurrentNonPagedPoolAllocations = Verifier->CurrentNonPagedPoolAllocations;
            UserVerifyBuffer->PeakPagedPoolAllocations = Verifier->PeakPagedPoolAllocations;
            UserVerifyBuffer->PeakNonPagedPoolAllocations = Verifier->PeakNonPagedPoolAllocations;

            UserVerifyBuffer->PagedPoolUsageInBytes = Verifier->PagedBytes;
            UserVerifyBuffer->NonPagedPoolUsageInBytes = Verifier->NonPagedBytes;
            UserVerifyBuffer->PeakPagedPoolUsageInBytes = Verifier->PeakPagedBytes;
            UserVerifyBuffer->PeakNonPagedPoolUsageInBytes = Verifier->PeakNonPagedBytes;

            UserVerifyBuffer->Loads = Verifier->Loads;
            UserVerifyBuffer->Unloads = Verifier->Unloads;

            //
            // The DriverName portion of the UserVerifyBuffer must be saved
            // locally to protect against a malicious thread changing the
            // contents.  This is because we will reference the contents
            // ourselves when the actual string is copied out carefully below.
            //

            UserBufferDriverName.Length = Verifier->BaseName.Length;
            UserBufferDriverName.MaximumLength = (USHORT)(Verifier->BaseName.Length + sizeof (WCHAR));
            UserBufferDriverName.Buffer = (PWCHAR)(UserVerifyBuffer + 1);

            UserVerifyBuffer->DriverName = UserBufferDriverName;

            TotalSize += ROUND_UP (UserBufferDriverName.MaximumLength,
                                   sizeof(PVOID));
            NextEntryOffset += ROUND_UP (UserBufferDriverName.MaximumLength,
                                         sizeof(PVOID));

            if (TotalSize > SystemInformationLength) {
                ExRaiseStatus (STATUS_INFO_LENGTH_MISMATCH);
            }

            //
            // Carefully reference the UserVerifyBuffer here.
            //

            RtlCopyMemory(UserBufferDriverName.Buffer,
                          Verifier->BaseName.Buffer,
                          Verifier->BaseName.Length);

            UserBufferDriverName.Buffer[
                        Verifier->BaseName.Length/sizeof(WCHAR)] = UNICODE_NULL;
            UserVerifyBuffer->NextEntryOffset = NextEntryOffset;

            NextEntry = NextEntry->Flink;
        }
    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (CurrentThread);

    if (Status != STATUS_INFO_LENGTH_MISMATCH) {
        UserVerifyBuffer->NextEntryOffset = 0;
        *Length = TotalSize;
    }

    return Status;
}

NTSTATUS
MmSetVerifierInformation (
    IN OUT PVOID SystemInformation,
    IN ULONG SystemInformationLength
    )

/*++

Routine Description:

    This routine sets any driver verifier flags that can be done without
    rebooting.

Arguments:

    SystemInformation - Gets and returns the driver verification flags.

    SystemInformationLength - Supplies the length of the SystemInformation
                              buffer.

Return Value:

    Returns the status of the operation.

Environment:

    The SystemInformation buffer is in user space and our caller has wrapped
    a try-except around this entire routine.  Capture any exceptions here and
    release resources accordingly.

--*/

{
    PKTHREAD CurrentThread;
    ULONG UserFlags;
    ULONG NewFlags;
    ULONG NewFlagsOn;
    ULONG NewFlagsOff;
    NTSTATUS Status;
    PULONG UserVerifyBuffer;

    PAGED_CODE();

    if (SystemInformationLength < sizeof (ULONG)) {
        ExRaiseStatus (STATUS_INFO_LENGTH_MISMATCH);
    }

    UserVerifyBuffer = (PULONG)SystemInformation;

    //
    // Synchronize all changes to the flags here.
    //

    Status = STATUS_SUCCESS;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    try {

        UserFlags = *UserVerifyBuffer;

        //
        // Ensure nothing is being set or cleared that isn't supported.
        //
        //

        NewFlagsOn = UserFlags & VerifierModifyableOptions;

        NewFlags = MmVerifierData.Level | NewFlagsOn;

        //
        // Any bits set in NewFlagsOff must be zeroed in the NewFlags.
        //

        NewFlagsOff = ((~UserFlags) & VerifierModifyableOptions);

        NewFlags &= ~NewFlagsOff;

        if (NewFlags != MmVerifierData.Level) {
            VerifierOptionChanges += 1;
            MmVerifierData.Level = NewFlags;
            *UserVerifyBuffer = NewFlags;
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (CurrentThread);

    return Status;
}

typedef struct _VERIFIER_STRING_INFO {
   ULONG BuildNumber;
   ULONG DriverVerifierLevel;
   ULONG Flags;
   ULONG Check;
} VERIFIER_STRING_INFO, *PVERIFIER_STRING_INFO;

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("PAGECONST")
#endif

static const WCHAR Printable[] = L"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789";
static const ULONG PrintableChars = sizeof (Printable) / sizeof (Printable[0]) - 1;

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg()
#endif

VOID
ViPrintString (
    IN PUNICODE_STRING DriverName
    )

/*++

Routine Description:

    This routine does a really bad hash of build number, verifier level and
    flags by using the driver name as a stream of bytes to XOR into the flags,
    etc.

    This is a Neill Clift special.

Arguments:

    DriverName - Supplies the name of the driver.

Return Value:

    None.

--*/

{
    VERIFIER_STRING_INFO Bld;
    PUCHAR BufPtr;
    PWCHAR DriverPtr;
    ULONG BufLen;
    ULONG i;
    ULONG j;
    ULONG DriverChars;
    ULONG MaxChars;
    WCHAR OutBuf[sizeof (VERIFIER_STRING_INFO) * 2 + 1];
    UNICODE_STRING OutBufU;
    ULONG Rem;
    ULONG LastRem;
    LOGICAL Done;

    Bld.BuildNumber = NtBuildNumber;
    Bld.DriverVerifierLevel = MmVerifierData.Level;

    //
    // Unloads and other actions could be encoded in the Flags field here.
    //

    Bld.Flags = 0;

    //
    // Make the last ULONG a weird function of the others.
    //

    Bld.Check = ((Bld.Flags + 1) * Bld.BuildNumber * (Bld.DriverVerifierLevel + 1)) * 123456789;

    BufPtr = (PUCHAR) &Bld;
    BufLen = sizeof (Bld);

    DriverChars = DriverName->Length / sizeof (DriverName->Buffer[0]);
    DriverPtr = DriverName->Buffer;
    MaxChars = DriverChars;

    if (DriverChars < sizeof (VERIFIER_STRING_INFO)) {
        MaxChars = sizeof (VERIFIER_STRING_INFO);
    }

    //
    // Xor each character in the driver name into the buffer.
    //

    for (i = 0; i < MaxChars; i += 1) {
        BufPtr[i % BufLen] ^= (UCHAR) RtlUpcaseUnicodeChar(DriverPtr[i % DriverChars]);
    }

    //
    // Produce a base N decoding of the binary buffer using the printable
    // characters defines. Treat the binary as a byte array and do the
    // division for each, tracking the carry.
    //

    j = 0;
    do {
        Done = TRUE;

        for (i = 0, LastRem = 0; i < sizeof (VERIFIER_STRING_INFO); i += 1) {
            Rem = BufPtr[i] + 256 * LastRem;
            BufPtr[i] = (UCHAR) (Rem / PrintableChars);
            LastRem = Rem % PrintableChars;
            if (BufPtr[i]) {
                Done = FALSE;
            }
        }
        OutBuf[j++] = Printable[LastRem];

        if (j >= sizeof (OutBuf) / sizeof (OutBuf[0])) {

            //
            // The stack buffer isn't big enough.
            //

            return;
        }

    } while (Done == FALSE);

    OutBuf[j] = L'\0';

    OutBufU.Length = OutBufU.MaximumLength = (USHORT) (j * sizeof (WCHAR));
    OutBufU.Buffer = OutBuf;

    DbgPrint ("*******************************************************************************\n"
              "*\n"
              "* This is the string you add to your checkin description\n"
              "* Driver Verifier: Enabled for %Z on Build %ld %wZ\n"
              "*\n"
              "*******************************************************************************\n",
              DriverName, NtBuildNumber & 0xFFFFFFF, &OutBufU);

    return;
}

//
// BEWARE: Various kernel macros are undefined here so we can pull in the
// real routines.  This is needed because the real routines are exported for
// driver compatibility.  This module has been carefully laid out so these
// macros are not referenced from this point down and references go to the
// real routines.
//




#undef KeRaiseIrql
#undef KeLowerIrql
#undef KeAcquireSpinLock
#undef KeReleaseSpinLock
#undef KeAcquireSpinLockAtDpcLevel
#undef KeReleaseSpinLockFromDpcLevel
#if 0
#undef ExAcquireResourceExclusive
#endif

#if !defined(_AMD64_)

VOID
KeRaiseIrql (
    IN KIRQL NewIrql,
    OUT PKIRQL OldIrql
    );

#endif

VOID
KeLowerIrql (
    IN KIRQL NewIrql
    );

#if !defined(_AMD64_)

VOID
KeAcquireSpinLock (
    IN PKSPIN_LOCK SpinLock,
    OUT PKIRQL OldIrql
    );

#endif

VOID
KeReleaseSpinLock (
    IN PKSPIN_LOCK SpinLock,
    IN KIRQL NewIrql
    );

VOID
KeAcquireSpinLockAtDpcLevel (
    IN PKSPIN_LOCK SpinLock
    );

VOID
KeReleaseSpinLockFromDpcLevel (
    IN PKSPIN_LOCK SpinLock
    );

#if 0
BOOLEAN
ExAcquireResourceExclusive (
    IN PERESOURCE Resource,
    IN BOOLEAN Wait
    );
#endif

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("PAGECONST")
#endif

const VERIFIER_THUNKS MiVerifierThunks[] = {

    "KeSetEvent",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierSetEvent,

    "ExAcquireFastMutexUnsafe",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExAcquireFastMutexUnsafe,

    "ExReleaseFastMutexUnsafe",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExReleaseFastMutexUnsafe,

    "ExAcquireResourceExclusiveLite",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExAcquireResourceExclusiveLite,

    "ExReleaseResourceLite",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExReleaseResourceLite,

    "MmProbeAndLockPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierProbeAndLockPages,

#if 0
    //
    // Don't bother thunking this API as it appears no drivers use it.
    //
    "MmProbeAndLockSelectedPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierProbeAndLockSelectedPages,
#endif

    "MmProbeAndLockProcessPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierProbeAndLockProcessPages,

    "MmMapIoSpace",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierMapIoSpace,

    "MmMapLockedPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierMapLockedPages,

    "MmMapLockedPagesSpecifyCache",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierMapLockedPagesSpecifyCache,

    "MmUnlockPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierUnlockPages,

    "MmUnmapLockedPages",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierUnmapLockedPages,

    "MmUnmapIoSpace",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierUnmapIoSpace,

    "ExAcquireFastMutex",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExAcquireFastMutex,

    "ExTryToAcquireFastMutex",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExTryToAcquireFastMutex,

    "ExReleaseFastMutex",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierExReleaseFastMutex,

#if !defined(_AMD64_)

    "KeRaiseIrql",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeRaiseIrql,

#endif

    "KeLowerIrql",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeLowerIrql,

#if !defined(_AMD64_)

    "KeAcquireSpinLock",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireSpinLock,

#endif

    "KeReleaseSpinLock",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeReleaseSpinLock,

#if defined(_X86_)
    "KefAcquireSpinLockAtDpcLevel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireSpinLockAtDpcLevel,

    "KefReleaseSpinLockFromDpcLevel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeReleaseSpinLockFromDpcLevel,
#else
    "KeAcquireSpinLockAtDpcLevel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireSpinLockAtDpcLevel,

    "KeReleaseSpinLockFromDpcLevel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeReleaseSpinLockFromDpcLevel,
#endif

    "KeSynchronizeExecution",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierSynchronizeExecution,

    "KeInitializeTimerEx",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeInitializeTimerEx,

    "KeInitializeTimer",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeInitializeTimer,

    "KeWaitForSingleObject",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeWaitForSingleObject,

#if defined(_X86_) || defined(_AMD64_)

    "KfRaiseIrql",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfRaiseIrql,

    "KeRaiseIrqlToDpcLevel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeRaiseIrqlToDpcLevel,

#endif

#if defined(_X86_)

    "KfLowerIrql",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfLowerIrql,

    "KfAcquireSpinLock",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfAcquireSpinLock,

    "KfReleaseSpinLock",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfReleaseSpinLock,

#endif

#if !defined(_X86_)

    "KeAcquireSpinLockRaiseToDpc",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireSpinLockRaiseToDpc,

#endif

    "IoFreeIrp",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovFreeIrp,

    "IofCallDriver",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovCallDriver,

    "IofCompleteRequest",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovCompleteRequest,

    "IoBuildDeviceIoControlRequest",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovBuildDeviceIoControlRequest,

    "IoBuildAsynchronousFsdRequest",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovBuildAsynchronousFsdRequest,

    "IoInitializeTimer",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)IovInitializeTimer,

    "KeQueryPerformanceCounter",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfQueryPerformanceCounter,

    "IoGetDmaAdapter",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfGetDmaAdapter,

    "HalAllocateCrashDumpRegisters",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfAllocateCrashDumpRegisters,

    "ObReferenceObjectByHandle",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierReferenceObjectByHandle,

    "KeReleaseMutex",
    (PDRIVER_VERIFIER_THUNK_ROUTINE) VerifierKeReleaseMutex,

    "KeInitializeMutex",
    (PDRIVER_VERIFIER_THUNK_ROUTINE) VerifierKeInitializeMutex,

    "KeReleaseMutant",
    (PDRIVER_VERIFIER_THUNK_ROUTINE) VerifierKeReleaseMutant,

    "KeInitializeMutant",
    (PDRIVER_VERIFIER_THUNK_ROUTINE) VerifierKeInitializeMutant,

    "KeInitializeSpinLock",
    (PDRIVER_VERIFIER_THUNK_ROUTINE) VerifierKeInitializeSpinLock,

#if !defined(NO_LEGACY_DRIVERS)
    "HalGetAdapter",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfLegacyGetAdapter,

    "IoMapTransfer",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfMapTransfer,

    "IoFlushAdapterBuffers",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfFlushAdapterBuffers,

    "HalAllocateCommonBuffer",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfAllocateCommonBuffer,

    "HalFreeCommonBuffer",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfFreeCommonBuffer,

    "IoAllocateAdapterChannel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfAllocateAdapterChannel,

    "IoFreeAdapterChannel",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfFreeAdapterChannel,

    "IoFreeMapRegisters",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VfFreeMapRegisters,
#endif

    "NtCreateFile",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierNtCreateFile,

    "NtWriteFile",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierNtWriteFile,

    "NtReadFile",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierNtReadFile,

    NULL,
    NULL,
};

const VERIFIER_THUNKS MiVerifierPoolThunks[] = {

    "ExAllocatePool",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierAllocatePool,

    "ExAllocatePoolWithQuota",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierAllocatePoolWithQuota,

    "ExAllocatePoolWithQuotaTag",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierAllocatePoolWithQuotaTag,

    "ExAllocatePoolWithTag",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierAllocatePoolWithTag,

    "ExAllocatePoolWithTagPriority",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierAllocatePoolWithTagPriority,

    "ExFreePool",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierFreePool,

    "ExFreePoolWithTag",
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierFreePoolWithTag,

    NULL,
    NULL,
};

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

PDRIVER_VERIFIER_THUNK_ROUTINE
MiResolveVerifierExports (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PCHAR PristineName
    )

/*++

Routine Description:

    This function scans the kernel & HAL exports for the specified routine name.

Arguments:

    DataTableEntry - Supplies the data table entry for the driver.

Return Value:

    Non-NULL address of routine to thunk or NULL if the routine could not be
    found.

Environment:

    Kernel mode, Phase 0 Initialization only.
    The PsLoadedModuleList has not been initialized yet.
    Non paged pool exists in Phase0, but paged pool does not.

--*/

{
    ULONG i;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PULONG Addr;
    ULONG ExportSize;
    ULONG Low;
    ULONG Middle;
    ULONG High;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    USHORT OrdinalNumber;
    LONG Result;
    PCHAR DllBase;

    i = 0;
    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    for ( ; NextEntry != &LoaderBlock->LoadOrderListHead; NextEntry = NextEntry->Flink) {

        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);

        //
        // Process the kernel and HAL exports so the proper routine
        // addresses can be generated now that relocations are complete.
        //

        DllBase = (PCHAR) DataTableEntry->DllBase;

        ExportDirectory = (PIMAGE_EXPORT_DIRECTORY)RtlImageDirectoryEntryToData(
                                    (PVOID) DllBase,
                                    TRUE,
                                    IMAGE_DIRECTORY_ENTRY_EXPORT,
                                    &ExportSize);

        if (ExportDirectory != NULL) {

            //
            // Lookup the import name in the name table using a binary search.
            //

            NameTableBase = (PULONG)(DllBase + (ULONG)ExportDirectory->AddressOfNames);
            NameOrdinalTableBase = (PUSHORT)(DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

            Low = 0;
            High = ExportDirectory->NumberOfNames - 1;

            //
            // Initializing Middle is not needed for correctness, but without it
            // the compiler cannot compile this code W4 to check for use of
            // uninitialized variables.
            //

            Middle = 0;

            while (High >= Low) {

                //
                // Compute the next probe index and compare the import name
                // with the export name entry.
                //

                Middle = (Low + High) >> 1;
                Result = strcmp (PristineName,
                                 (PCHAR)DllBase + NameTableBase[Middle]);

                if (Result < 0) {
                    High = Middle - 1;

                } else if (Result > 0) {
                    Low = Middle + 1;

                }
                else {
                    break;
                }
            }

            //
            // If the high index is less than the low index, then a matching
            // table entry was not found. Otherwise, get the ordinal number
            // from the ordinal table.
            //

            if ((LONG)High >= (LONG)Low) {
                OrdinalNumber = NameOrdinalTableBase[Middle];

                //
                // If OrdinalNumber is not within the Export Address Table,
                // then DLL does not implement function.  Otherwise we have
                // the export that matches the specified argument routine name.
                //

                if ((ULONG)OrdinalNumber < ExportDirectory->NumberOfFunctions) {

                    Addr = (PULONG)(DllBase + (ULONG)ExportDirectory->AddressOfFunctions);
                    return (PDRIVER_VERIFIER_THUNK_ROUTINE)(ULONG_PTR)(DllBase + Addr[OrdinalNumber]);
                }
            }
        }

        i += 1;
        if (i == 2) {
            break;
        }
    }
    return NULL;
}

LOGICAL
MiEnableVerifier (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function enables the verifier for the argument driver by thunking
    relevant system APIs in the argument driver import table.

Arguments:

    DataTableEntry - Supplies the data table entry for the driver.

Return Value:

    TRUE if thunking was applied, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization and normal runtime.
    Non paged pool exists in Phase0, but paged pool does not.

--*/

{
    ULONG i;
    ULONG j;
    PULONG_PTR ImportThunk;
    ULONG ImportSize;
    VERIFIER_THUNKS const *VerifierThunk;
    LOGICAL Found;
    ULONG_PTR RealRoutine;
    PLIST_ENTRY NextEntry;
    PDRIVER_VERIFIER_THUNK_PAIRS ThunkTable;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;

    ImportThunk = (PULONG_PTR)RtlImageDirectoryEntryToData(
                                               DataTableEntry->DllBase,
                                               TRUE,
                                               IMAGE_DIRECTORY_ENTRY_IAT,
                                               &ImportSize);

    if (ImportThunk == NULL) {
        return FALSE;
    }

    ImportSize /= sizeof(PULONG_PTR);

    for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {

        Found = FALSE;

        if (KernelVerifier == FALSE) {
            VerifierThunk = MiVerifierThunks;

            while (VerifierThunk->PristineRoutineAsciiName != NULL) {

                RealRoutine = (ULONG_PTR)VerifierThunk->PristineRoutine;

                if (*ImportThunk == RealRoutine) {
                    *ImportThunk = (ULONG_PTR)(VerifierThunk->NewRoutine);
                    Found = TRUE;
                    break;
                }
                VerifierThunk += 1;
            }
        }

        if (Found == FALSE) {
            VerifierThunk = MiVerifierPoolThunks;

            while (VerifierThunk->PristineRoutineAsciiName != NULL) {

                RealRoutine = (ULONG_PTR)VerifierThunk->PristineRoutine;

                if (*ImportThunk == RealRoutine) {
                    *ImportThunk = (ULONG_PTR)(VerifierThunk->NewRoutine);
                    Found = TRUE;
                    break;
                }
                VerifierThunk += 1;
            }
        }

        if (Found == FALSE) {

            NextEntry = MiVerifierDriverAddedThunkListHead.Flink;
            while (NextEntry != &MiVerifierDriverAddedThunkListHead) {

                ThunkTableBase = CONTAINING_RECORD(NextEntry,
                                                   DRIVER_SPECIFIED_VERIFIER_THUNKS,
                                                   ListEntry);

                ThunkTable = (PDRIVER_VERIFIER_THUNK_PAIRS)(ThunkTableBase + 1);

                for (j = 0; j < ThunkTableBase->NumberOfThunks; j += 1) {

                    if (*ImportThunk == (ULONG_PTR)ThunkTable->PristineRoutine) {
                        *ImportThunk = (ULONG_PTR)(ThunkTable->NewRoutine);
                        Found = TRUE;
                        break;
                    }
                    ThunkTable += 1;
                }

                if (Found == TRUE) {
                    break;
                }

                NextEntry = NextEntry->Flink;
            }
        }
    }
    return TRUE;
}

LOGICAL
MiReEnableVerifier (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This function thunks DLL-supplied APIs in the argument driver import table.

Arguments:

    DataTableEntry - Supplies the data table entry for the driver.

Return Value:

    TRUE if thunking was applied, FALSE if not.

Environment:

    Kernel mode, Phase 0 Initialization only.
    Non paged pool exists in Phase0, but paged pool does not.

--*/

{
    ULONG i;
    ULONG j;
    PULONG_PTR ImportThunk;
    ULONG ImportSize;
    LOGICAL Found;
    PLIST_ENTRY NextEntry;
    PMMPTE PointerPte;
    PULONG_PTR VirtualThunk;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER VirtualPageFrameIndex;
    PDRIVER_VERIFIER_THUNK_PAIRS ThunkTable;
    PDRIVER_SPECIFIED_VERIFIER_THUNKS ThunkTableBase;
    ULONG Offset;

    ImportThunk = (PULONG_PTR)RtlImageDirectoryEntryToData(
                                               DataTableEntry->DllBase,
                                               TRUE,
                                               IMAGE_DIRECTORY_ENTRY_IAT,
                                               &ImportSize);

    if (ImportThunk == NULL) {
        return FALSE;
    }

    VirtualThunk = NULL;
    ImportSize /= sizeof(PULONG_PTR);

    //
    // Initializing VirtualPageFrameIndex is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    VirtualPageFrameIndex = 0;

    for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {

        Found = FALSE;

        NextEntry = MiVerifierDriverAddedThunkListHead.Flink;
        while (NextEntry != &MiVerifierDriverAddedThunkListHead) {

            ThunkTableBase = CONTAINING_RECORD(NextEntry,
                                               DRIVER_SPECIFIED_VERIFIER_THUNKS,
                                               ListEntry);

            ThunkTable = (PDRIVER_VERIFIER_THUNK_PAIRS)(ThunkTableBase + 1);

            for (j = 0; j < ThunkTableBase->NumberOfThunks; j += 1) {

                if (*ImportThunk == (ULONG_PTR)ThunkTable->PristineRoutine) {

                    ASSERT (MI_IS_PHYSICAL_ADDRESS(ImportThunk) == 0);
                    PointerPte = MiGetPteAddress (ImportThunk);
                    ASSERT (PointerPte->u.Hard.Valid == 1);
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                    Offset = (ULONG) MiGetByteOffset(ImportThunk);

                    if ((VirtualThunk != NULL) &&
                        (VirtualPageFrameIndex == PageFrameIndex)) {

                        NOTHING;
                    }
                    else {

                        VirtualThunk = MiMapSinglePage (VirtualThunk,
                                                        PageFrameIndex,
                                                        MmCached,
                                                        HighPagePriority);

                        if (VirtualThunk == NULL) {
                            return FALSE;
                        }
                        VirtualPageFrameIndex = PageFrameIndex;
                    }

                    *(PULONG_PTR)((PUCHAR)VirtualThunk + Offset) =
                        (ULONG_PTR)(ThunkTable->NewRoutine);

                    Found = TRUE;
                    break;
                }
                ThunkTable += 1;
            }

            if (Found == TRUE) {
                break;
            }

            NextEntry = NextEntry->Flink;
        }
    }

    if (VirtualThunk != NULL) {
        MiUnmapSinglePage (VirtualThunk);
    }

    return TRUE;
}

typedef struct _KERNEL_VERIFIER_THUNK_PAIRS {
    PDRIVER_VERIFIER_THUNK_ROUTINE  PristineRoutine;
    PDRIVER_VERIFIER_THUNK_ROUTINE  NewRoutine;
} KERNEL_VERIFIER_THUNK_PAIRS, *PKERNEL_VERIFIER_THUNK_PAIRS;

#if defined(_X86_)

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("INITCONST")
#endif

const KERNEL_VERIFIER_THUNK_PAIRS MiKernelVerifierThunks[] = {

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeRaiseIrql,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeRaiseIrql,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeLowerIrql,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeLowerIrql,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeAcquireSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireSpinLock,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeReleaseSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeReleaseSpinLock,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KfRaiseIrql,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfRaiseIrql,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeRaiseIrqlToDpcLevel,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeRaiseIrqlToDpcLevel,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KfLowerIrql,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfLowerIrql,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KfAcquireSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfAcquireSpinLock,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KfReleaseSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKfReleaseSpinLock,

#if !defined(NT_UP)
    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeAcquireQueuedSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeAcquireQueuedSpinLock,

    (PDRIVER_VERIFIER_THUNK_ROUTINE)KeReleaseQueuedSpinLock,
    (PDRIVER_VERIFIER_THUNK_ROUTINE)VerifierKeReleaseQueuedSpinLock,
#endif
};

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

VOID
MiEnableKernelVerifier (
    VOID
    )

/*++

Routine Description:

    This function enables the verifier for the kernel by thunking
    relevant HAL APIs in the kernel's import table.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, Phase 1 Initialization.

--*/

{
    ULONG i;
    PULONG_PTR ImportThunk;
    ULONG ImportSize;
    KERNEL_VERIFIER_THUNK_PAIRS const *VerifierThunk;
    ULONG ThunkCount;
    ULONG_PTR RealRoutine;
    PULONG_PTR PointerRealRoutine;

    if (KernelVerifier == FALSE) {
        return;
    }

    ImportThunk = (PULONG_PTR)RtlImageDirectoryEntryToData(
                                               PsNtosImageBase,
                                               TRUE,
                                               IMAGE_DIRECTORY_ENTRY_IAT,
                                               &ImportSize);

    if (ImportThunk == NULL) {
        return;
    }

    ImportSize /= sizeof(PULONG_PTR);

    for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {

        VerifierThunk = MiKernelVerifierThunks;

        for (ThunkCount = 0; ThunkCount < sizeof (MiKernelVerifierThunks) / sizeof (KERNEL_VERIFIER_THUNK_PAIRS); ThunkCount += 1) {

            //
            // Only the x86 has/needs this oddity - take the kernel address,
            // knowing that it points at a 2 byte jmp opcode followed by
            // a 4-byte indirect pointer to a destination address.
            //

            PointerRealRoutine = (PULONG_PTR)*((PULONG_PTR)((ULONG_PTR)VerifierThunk->PristineRoutine + 2));
            RealRoutine = *PointerRealRoutine;

            if (*ImportThunk == RealRoutine) {

                //
                // Order is important here.
                //

                if (MiKernelVerifierOriginalCalls[ThunkCount] == NULL) {
                    MiKernelVerifierOriginalCalls[ThunkCount] = (PVOID)RealRoutine;
                }

                *ImportThunk = (ULONG_PTR)(VerifierThunk->NewRoutine);

                break;
            }
            VerifierThunk += 1;
        }
    }
    return;
}
#endif

//
// BEWARE: Various kernel macros were undefined above so we can pull in the
// real routines.  This is needed because the real routines are exported for
// driver compatibility.  This module has been carefully laid out so these
// macros are not referenced from that point to here and references go to the
// real routines.
//
// BE EXTREMELY CAREFUL IF YOU DECIDE TO ADD ROUTINES BELOW THIS POINT !
//
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\triage.c ===
/*++

Copyright (c) 1999  Microsoft Corporation

Module Name:

   triage.c

Abstract:

    This module contains the Phase 0 code to triage bugchecks and
    automatically enable various system tracing components until the
    guilty party is found.

Author:

    Landy Wang 13-Jan-1999

Revision History:

--*/

#include "mi.h"
#include "ntiodump.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiTriageSystem)
#pragma alloc_text(INIT,MiTriageAddDrivers)
#endif

//
// Always update this macro when adding triage support for additional bugchecks.
//

#define MI_CAN_TRIAGE_BUGCHECK(BugCheckCode) \
        ((BugCheckCode) == PROCESS_HAS_LOCKED_PAGES || \
         (BugCheckCode) == NO_MORE_SYSTEM_PTES || \
         (BugCheckCode) == BAD_POOL_HEADER || \
         (BugCheckCode) == DRIVER_CORRUPTED_SYSPTES || \
         (BugCheckCode) == DRIVER_CORRUPTED_EXPOOL || \
         (BugCheckCode) == DRIVER_CORRUPTED_MMPOOL)

//
// These are bugchecks that were presumably triggered by either autotriage or
// the admin's registry settings - so don't apply any new rules and in addition,
// keep the old ones unaltered so it can reproduce.
//

#define MI_HOLD_TRIAGE_BUGCHECK(BugCheckCode) \
        ((BugCheckCode) == DRIVER_USED_EXCESSIVE_PTES || \
         (BugCheckCode) == DRIVER_LEFT_LOCKED_PAGES_IN_PROCESS || \
         (BugCheckCode) == PAGE_FAULT_IN_FREED_SPECIAL_POOL || \
         (BugCheckCode) == DRIVER_PAGE_FAULT_IN_FREED_SPECIAL_POOL || \
         (BugCheckCode) == PAGE_FAULT_BEYOND_END_OF_ALLOCATION || \
         (BugCheckCode) == DRIVER_PAGE_FAULT_BEYOND_END_OF_ALLOCATION || \
         (BugCheckCode) == DRIVER_CAUGHT_MODIFYING_FREED_POOL || \
         (BugCheckCode) == SYSTEM_PTE_MISUSE)

#define MI_TRACKING_LOCKED_PAGES            0x00000001
#define MI_TRACKING_PTES                    0x00000002
#define MI_PROTECT_FREED_NONPAGED_POOL      0x00000004
#define MI_VERIFYING_PRENT5_DRIVERS         0x00000008
#define MI_KEEPING_PREVIOUS_SETTINGS        0x00000010

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg("INITCONST")
#pragma data_seg("INITDATA")
#endif
const PCHAR MiTriageActionStrings[] = {
    "Locked pages tracking",
    "System PTE usage tracking",
    "Making accesses to freed nonpaged pool cause bugchecks",
    "Driver Verifying Pre-Windows 2000 built drivers",
    "Keeping previous autotriage settings"
};

#if DBG
ULONG MiTriageDebug = 0;
BOOLEAN MiTriageRegardless = FALSE;
#endif

#ifdef ALLOC_DATA_PRAGMA
#pragma const_seg()
#pragma data_seg()
#endif

//
// N.B.  The debugger references this.
//

ULONG MmTriageActionTaken;

//
// The Version number must be incremented whenever the MI_TRIAGE_STORAGE
// structure is changed.  This enables usermode programs to decode the Mm
// portions of triage dumps regardless of which kernel revision created the
// dump.
//

typedef struct _MI_TRIAGE_STORAGE {
    ULONG Version;
    ULONG Size;
    ULONG MmSpecialPoolTag;
    ULONG MiTriageActionTaken;

    ULONG MmVerifyDriverLevel;
    ULONG KernelVerifier;
    ULONG_PTR MmMaximumNonPagedPool;
    ULONG_PTR MmAllocatedNonPagedPool;

    ULONG_PTR PagedPoolMaximum;
    ULONG_PTR PagedPoolAllocated;

    ULONG_PTR CommittedPages;
    ULONG_PTR CommittedPagesPeak;
    ULONG_PTR CommitLimitMaximum;

} MI_TRIAGE_STORAGE, *PMI_TRIAGE_STORAGE;

PKLDR_DATA_TABLE_ENTRY
TriageGetLoaderEntry (
    IN PVOID TriageDumpBlock,
    IN ULONG ModuleIndex
    );

LOGICAL
TriageActUpon(
    IN PVOID TriageDumpBlock
    );

PVOID
TriageGetMmInformation (
    IN PVOID TriageDumpBlock
    );


LOGICAL
MiTriageSystem (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This routine takes the information from the last bugcheck (if any)
    and triages it.  Various debugging options are then automatically
    enabled.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    TRUE if triaging succeeded and options were enabled.  FALSE otherwise.

--*/

{
    PVOID TriageDumpBlock;
    ULONG_PTR BugCheckData[5];
    ULONG i;
    ULONG ModuleCount;
    NTSTATUS Status;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY DumpTableEntry;
    LOGICAL Matched;
    ULONG OldDrivers;
    ULONG OldDriversNotVerifying;
    PMI_TRIAGE_STORAGE TriageInformation;
    
    if (LoaderBlock->Extension == NULL) {
        return FALSE;
    }

    if (LoaderBlock->Extension->Size < sizeof (LOADER_PARAMETER_EXTENSION)) {
        return FALSE;
    }

    TriageDumpBlock = LoaderBlock->Extension->TriageDumpBlock;

    Status = TriageGetBugcheckData (TriageDumpBlock,
                                    (PULONG)&BugCheckData[0],
                                    (PUINT_PTR) &BugCheckData[1],
                                    (PUINT_PTR) &BugCheckData[2],
                                    (PUINT_PTR) &BugCheckData[3],
                                    (PUINT_PTR) &BugCheckData[4]);

    if (!NT_SUCCESS (Status)) {
        return FALSE;
    }

    //
    // Always display at least the bugcheck data from the previous crash.
    //

    DbgPrint ("MiTriageSystem: Previous bugcheck was %x %p %p %p %p\n",
        BugCheckData[0],
        BugCheckData[1],
        BugCheckData[2],
        BugCheckData[3],
        BugCheckData[4]);

    if (TriageActUpon (TriageDumpBlock) == FALSE) {
        DbgPrint ("MiTriageSystem: Triage disabled in registry by administrator\n");
        return FALSE;
    }

    DbgPrint ("MiTriageSystem: Triage ENABLED in registry by administrator\n");

    //
    // See if the previous bugcheck was one where action can be taken.
    // If not, bail now.  If so, then march on and verify all the loaded
    // module checksums before actually taking action on the bugcheck.
    //

    if (!MI_CAN_TRIAGE_BUGCHECK(BugCheckData[0])) {
        return FALSE;
    }

    TriageInformation = (PMI_TRIAGE_STORAGE) TriageGetMmInformation (TriageDumpBlock);

    if (TriageInformation == NULL) {
        return FALSE;
    }

    Status = TriageGetDriverCount (TriageDumpBlock, &ModuleCount);

    if (!NT_SUCCESS (Status)) {
        return FALSE;
    }

    //
    // Process module information from the triage dump.
    //

#if DBG
    if (MiTriageDebug & 0x1) {
        DbgPrint ("MiTriageSystem: printing active drivers from triage crash...\n");
    }
#endif

    OldDrivers = 0;
    OldDriversNotVerifying = 0;

    for (i = 0; i < ModuleCount; i += 1) {

        DumpTableEntry = TriageGetLoaderEntry (TriageDumpBlock, i);

        if (DumpTableEntry != NULL) {

            if ((DumpTableEntry->Flags & LDRP_ENTRY_NATIVE) == 0) {
                OldDrivers += 1;
                if ((DumpTableEntry->Flags & LDRP_IMAGE_VERIFYING) == 0) {

                    //
                    // An NT3 or NT4 driver is in the system and was not
                    // running under the verifier.
                    //

                    OldDriversNotVerifying += 1;
                }
            }
#if DBG
            if (MiTriageDebug & 0x1) {

                DbgPrint (" %wZ: base = %p, size = %lx, flags = %lx\n",
                          &DumpTableEntry->BaseDllName,
                          DumpTableEntry->DllBase,
                          DumpTableEntry->SizeOfImage,
                          DumpTableEntry->Flags);
            }
#endif
        }
    }

    //
    // Ensure that every driver that is currently loaded is identical to
    // the one in the triage dump before proceeding.
    //

    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    while (NextEntry != &LoaderBlock->LoadOrderListHead) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        Matched = FALSE;

        for (i = 0; i < ModuleCount; i += 1) {
    
            DumpTableEntry = TriageGetLoaderEntry (TriageDumpBlock, i);
    
            if (DumpTableEntry != NULL) {
    
                if (DataTableEntry->CheckSum == DumpTableEntry->CheckSum) {
                    Matched = TRUE;
                    break;
                }
            }
        }
    
        if (Matched == FALSE) {
            DbgPrint ("Matching checksum for module %wZ not found in triage dump\n",
                &DataTableEntry->BaseDllName);

#if DBG
            if (MiTriageRegardless == FALSE)
#endif
            return FALSE;
        }

        NextEntry = NextEntry->Flink;
    }

#if DBG
    if (MiTriageDebug & 0x1) {
        DbgPrint ("MiTriageSystem: OldDrivers = %u, without verification =%u\n",
            OldDrivers,
            OldDriversNotVerifying);
    }
#endif

    //
    // All boot loaded drivers matched, take action on the triage dump now.
    //

    if (MI_HOLD_TRIAGE_BUGCHECK(BugCheckData[0])) {

        //
        // The last bugcheck was presumably triggered by either autotriage or
        // the admin's registry settings - so don't apply any new rules
        // and in addition, keep the old ones unaltered so it can reproduce.
        //

        MmTriageActionTaken = TriageInformation->MiTriageActionTaken;
        MmTriageActionTaken |= MI_KEEPING_PREVIOUS_SETTINGS;
    }
    else {
    
        switch (BugCheckData[0]) {
    
            case PROCESS_HAS_LOCKED_PAGES:
    
                //
                // Turn on locked pages tracking so this turns into bugcheck
                // DRIVER_LEFT_LOCKED_PAGES_IN_PROCESS which shows the name
                // of the driver.
                //
    
                MmTriageActionTaken |= MI_TRACKING_LOCKED_PAGES;
                break;
    
            case DRIVER_CORRUPTED_SYSPTES:
    
                //
                // Turn on PTE tracking to trigger a SYSTEM_PTE_MISUSE bugcheck.
                //
    
                MmTriageActionTaken |= MI_TRACKING_PTES;
                break;
    
            case NO_MORE_SYSTEM_PTES:
    
                //
                // Turn on PTE tracking so the driver can be identified via a
                // DRIVER_USED_EXCESSIVE_PTES bugcheck.
                //
    
                if (BugCheckData[1] == SystemPteSpace) {
                    MmTriageActionTaken |= MI_TRACKING_PTES;
                }
                break;
    
            case BAD_POOL_HEADER:
            case DRIVER_CORRUPTED_EXPOOL:
    
                //
                // Turn on the driver verifier and/or special pool.
                // Start by enabling it for every driver that isn't built for NT5.
                // Override any specified driver verifier options so that only
                // special pool is enabled to minimize the performance hit.
                //
    
                if (OldDrivers != 0) {
                    if (OldDriversNotVerifying != 0) {
                        MmTriageActionTaken |= MI_VERIFYING_PRENT5_DRIVERS;
                    }
                }
    
                break;
    
            case DRIVER_CORRUPTED_MMPOOL:
    
                //
                // Protect freed nonpaged pool if the system had less than 128mb
                // of nonpaged pool anyway.  This is to trigger a
                // DRIVER_CAUGHT_MODIFYING_FREED_POOL bugcheck.
                //
    
#define MB128 ((ULONG_PTR)0x80000000 >> PAGE_SHIFT)
    
                if (TriageInformation->MmMaximumNonPagedPool < MB128) {
                    MmTriageActionTaken |= MI_PROTECT_FREED_NONPAGED_POOL;
                }
                break;
    
            case IRQL_NOT_LESS_OR_EQUAL:
            case DRIVER_IRQL_NOT_LESS_OR_EQUAL:
            default:
                break;
        }
    }

    //
    // For now always show if action was taken from the bugcheck
    // data from the crash.  This print and the space for the print strings
    // will be enabled for checked builds only prior to shipping.
    //

    if (MmTriageActionTaken != 0) {

        if (MmTriageActionTaken & MI_TRACKING_LOCKED_PAGES) {
            MmTrackLockedPages = TRUE;
        }
    
        if (MmTriageActionTaken & MI_TRACKING_PTES) {
            MmTrackPtes |= 0x1;
        }
    
        if (MmTriageActionTaken & MI_VERIFYING_PRENT5_DRIVERS) {
            MmVerifyDriverLevel &= ~DRIVER_VERIFIER_FORCE_IRQL_CHECKING;
            MmVerifyDriverLevel |= DRIVER_VERIFIER_SPECIAL_POOLING;
        }
    
        if (MmTriageActionTaken & MI_PROTECT_FREED_NONPAGED_POOL) {
            MmProtectFreedNonPagedPool = TRUE;
        }

        DbgPrint ("MiTriageSystem: enabling options below to find who caused the last crash\n");

        for (i = 0; i < 32; i += 1) {
            if (MmTriageActionTaken & (1 << i)) {
                DbgPrint ("  %s\n", MiTriageActionStrings[i]);
            }
        }
    }

    return TRUE;
}


LOGICAL
MiTriageAddDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This routine moves the names of any drivers that autotriage has determined
    need verifying from the LoaderBlock into pool.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    TRUE if any drivers were added, FALSE if not.

--*/

{
    ULONG i;
    ULONG ModuleCount;
    NTSTATUS Status;
    PKLDR_DATA_TABLE_ENTRY DumpTableEntry;
    PVOID TriageDumpBlock;
    ULONG NameLength;
    LOGICAL Added;
    PMI_VERIFIER_DRIVER_ENTRY VerifierDriverEntry;

    if ((MmTriageActionTaken & MI_VERIFYING_PRENT5_DRIVERS) == 0) {
        return FALSE;
    }

    TriageDumpBlock = LoaderBlock->Extension->TriageDumpBlock;

    Status = TriageGetDriverCount (TriageDumpBlock, &ModuleCount);

    if (!NT_SUCCESS (Status)) {
        return FALSE;
    }

    Added = FALSE;

    for (i = 0; i < ModuleCount; i += 1) {

        DumpTableEntry = TriageGetLoaderEntry (TriageDumpBlock, i);

        if (DumpTableEntry == NULL) {
            continue;
        }

        if (DumpTableEntry->Flags & LDRP_ENTRY_NATIVE) {
            continue;
        }

        DbgPrint ("MiTriageAddDrivers: Marking %wZ for verification when it is loaded\n", &DumpTableEntry->BaseDllName);

        NameLength = DumpTableEntry->BaseDllName.Length;

        VerifierDriverEntry = (PMI_VERIFIER_DRIVER_ENTRY)ExAllocatePoolWithTag (
                                    NonPagedPool,
                                    sizeof (MI_VERIFIER_DRIVER_ENTRY) +
                                                        NameLength,
                                    'dLmM');

        if (VerifierDriverEntry == NULL) {
            continue;
        }

        VerifierDriverEntry->Loads = 0;
        VerifierDriverEntry->Unloads = 0;
        VerifierDriverEntry->BaseName.Buffer = (PWSTR)((PCHAR)VerifierDriverEntry +
                            sizeof (MI_VERIFIER_DRIVER_ENTRY));

        VerifierDriverEntry->BaseName.Length = (USHORT)NameLength;
        VerifierDriverEntry->BaseName.MaximumLength = (USHORT)NameLength;

        RtlCopyMemory (VerifierDriverEntry->BaseName.Buffer,
                       DumpTableEntry->BaseDllName.Buffer,
                       NameLength);

        InsertHeadList (&MiSuspectDriverList, &VerifierDriverEntry->Links);
        Added = TRUE;
    }

    return Added;
}

#define MAX_UNLOADED_NAME_LENGTH    24

typedef struct _DUMP_UNLOADED_DRIVERS {
    UNICODE_STRING Name;
    WCHAR DriverName[MAX_UNLOADED_NAME_LENGTH / sizeof (WCHAR)];
    PVOID StartAddress;
    PVOID EndAddress;
} DUMP_UNLOADED_DRIVERS, *PDUMP_UNLOADED_DRIVERS;


ULONG
MmSizeOfUnloadedDriverInformation (
    VOID
    )

/*++

Routine Description:

    This routine returns the size of the Mm-internal unloaded driver
    information that is stored in the triage dump when (if?) the system crashes.

Arguments:

    None.

Return Value:

    Size of the Mm-internal unloaded driver information.

--*/

{
    if (MmUnloadedDrivers == NULL) {
        return sizeof (ULONG_PTR);
    }

    return sizeof(ULONG_PTR) + MI_UNLOADED_DRIVERS * sizeof(DUMP_UNLOADED_DRIVERS);
}


VOID
MmWriteUnloadedDriverInformation (
    IN PVOID Destination
    )

/*++

Routine Description:

    This routine stores the Mm-internal unloaded driver information into
    the triage dump.

Arguments:

    None.

Return Value:

    None.

--*/

{
    ULONG i;
    ULONG Index;
    PUNLOADED_DRIVERS Unloaded;
    PDUMP_UNLOADED_DRIVERS DumpUnloaded;

    if (MmUnloadedDrivers == NULL) {
        *(PULONG)Destination = 0;
    }
    else {

        DumpUnloaded = (PDUMP_UNLOADED_DRIVERS)((PULONG_PTR)Destination + 1);
        Unloaded = MmUnloadedDrivers;

        //
        // Write the list with the most recently unloaded driver first to the
        // least recently unloaded driver last.
        //

        Index = MmLastUnloadedDriver - 1;

        for (i = 0; i < MI_UNLOADED_DRIVERS; i += 1) {

            if (Index >= MI_UNLOADED_DRIVERS) {
                Index = MI_UNLOADED_DRIVERS - 1;
            }

            Unloaded = &MmUnloadedDrivers[Index];

            DumpUnloaded->Name = Unloaded->Name;

            if (Unloaded->Name.Buffer == NULL) {
                break;
            }

            DumpUnloaded->StartAddress = Unloaded->StartAddress;
            DumpUnloaded->EndAddress = Unloaded->EndAddress;

            if (DumpUnloaded->Name.Length > MAX_UNLOADED_NAME_LENGTH) {
                DumpUnloaded->Name.Length = MAX_UNLOADED_NAME_LENGTH;
            }

            if (DumpUnloaded->Name.MaximumLength > MAX_UNLOADED_NAME_LENGTH) {
                DumpUnloaded->Name.MaximumLength = MAX_UNLOADED_NAME_LENGTH;
            }

            DumpUnloaded->Name.Buffer = DumpUnloaded->DriverName;

            RtlCopyMemory ((PVOID)DumpUnloaded->Name.Buffer,
                           (PVOID)Unloaded->Name.Buffer,
                           DumpUnloaded->Name.MaximumLength);

            DumpUnloaded += 1;
            Index -= 1;
        }

        *(PULONG)Destination = i;
    }
}


ULONG
MmSizeOfTriageInformation (
    VOID
    )

/*++

Routine Description:

    This routine returns the size of the Mm-internal information that is
    stored in the triage dump when (if?) the system crashes.

Arguments:

    None.

Return Value:

    Size of the Mm-internal triage information.

--*/

{
    return sizeof (MI_TRIAGE_STORAGE);
}


VOID
MmWriteTriageInformation (
    IN PVOID Destination
    )

/*++

Routine Description:

    This routine stores the Mm-internal information into the triage dump.

Arguments:

    None.

Return Value:

    None.

--*/

{
    MI_TRIAGE_STORAGE TriageInformation;

    TriageInformation.Version = 1;
    TriageInformation.Size = sizeof (MI_TRIAGE_STORAGE);

    TriageInformation.MmSpecialPoolTag = MmSpecialPoolTag;
    TriageInformation.MiTriageActionTaken = MmTriageActionTaken;

    TriageInformation.MmVerifyDriverLevel = MmVerifierData.Level;
    TriageInformation.KernelVerifier = KernelVerifier;

    TriageInformation.MmMaximumNonPagedPool = MmMaximumNonPagedPoolInBytes >> PAGE_SHIFT;
    TriageInformation.MmAllocatedNonPagedPool = MmAllocatedNonPagedPool;

    TriageInformation.PagedPoolMaximum = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;
    TriageInformation.PagedPoolAllocated = MmPagedPoolInfo.AllocatedPagedPool;

    TriageInformation.CommittedPages = MmTotalCommittedPages;
    TriageInformation.CommittedPagesPeak = MmPeakCommitment;
    TriageInformation.CommitLimitMaximum = MmTotalCommitLimitMaximum;

    RtlCopyMemory (Destination,
                   (PVOID)&TriageInformation,
                   sizeof (MI_TRIAGE_STORAGE));
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\wrtwatch.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   wrtwatch.c

Abstract:

    This module contains the routines to support write watch.

Author:

    Landy Wang (landyw) 28-Jul-1999

Revision History:

--*/

#include "mi.h"

#define COPY_STACK_SIZE 256

//
// This is the number of systemwide currently active write watch VADs.
//

ULONG_PTR MiActiveWriteWatch;


NTSTATUS
NtGetWriteWatch (
    IN HANDLE ProcessHandle,
    IN ULONG Flags,
    IN PVOID BaseAddress,
    IN SIZE_T RegionSize,
    IN OUT PVOID *UserAddressArray,
    IN OUT PULONG_PTR EntriesInUserAddressArray,
    OUT PULONG Granularity
    )

/*++

Routine Description:

    This function returns the write watch status of the argument region.
    UserAddressArray is filled with the base address of each page that has
    been written to since the last NtResetWriteWatch call (or if no
    NtResetWriteWatch calls have been made, then each page written since
    this address space was created).

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    Flags - Supplies WRITE_WATCH_FLAG_RESET or nothing.

    BaseAddress - An address within a region of pages to be queried. This
                  value must lie within a private memory region with the
                  write-watch attribute already set.

    RegionSize - The size of the region in bytes beginning at the base address
                 specified.

    UserAddressArray - Supplies a pointer to user memory to store the user
                       addresses modified since the last reset.

    UserAddressArrayEntries - Supplies a pointer to how many user addresses
                              can be returned in this call.  This is then filled
                              with the exact number of addresses actually
                              returned.

    Granularity - Supplies a pointer to a variable to receive the size of
                  modified granule in bytes.
        
Return Value:

    Various NTSTATUS codes.

--*/

{
    PMMPFN Pfn1;
    LOGICAL First;
    LOGICAL UserWritten;
    PVOID EndAddress;
    PMMVAD Vad;
    KIRQL OldIrql;
    PEPROCESS Process;
    PMMPTE NextPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE EndPte;
    NTSTATUS Status;
    PVOID PoolArea;
    PVOID *PoolAreaPointer;
    ULONG_PTR StackArray[COPY_STACK_SIZE];
    MMPTE PteContents;
    ULONG_PTR NumberOfBytes;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    ULONG NextBitMapIndex;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    ULONG_PTR PagesWritten;
    ULONG_PTR NumberOfPages;
    LOGICAL Attached;
    KPROCESSOR_MODE PreviousMode;
    PFN_NUMBER PageFrameIndex;
    ULONG WorkingSetIndex;
    MMPTE TempPte;
    MMPTE PreviousPte;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if ((Flags & ~WRITE_WATCH_FLAG_RESET) != 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread(CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        if (PreviousMode != KernelMode) {

            //
            // Make sure the specified starting and ending addresses are
            // within the user part of the virtual address space.
            //
        
            if (BaseAddress > MM_HIGHEST_VAD_ADDRESS) {
                return STATUS_INVALID_PARAMETER_2;
            }
        
            if ((((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) - (ULONG_PTR)BaseAddress) <
                    RegionSize) {
                return STATUS_INVALID_PARAMETER_3;
            }

            //
            // Capture the number of pages.
            //

            ProbeForWritePointer (EntriesInUserAddressArray);

            NumberOfPages = *EntriesInUserAddressArray;

            if (NumberOfPages == 0) {
                return STATUS_INVALID_PARAMETER_5;
            }

            if (NumberOfPages > (MAXULONG_PTR / sizeof(ULONG_PTR))) {
                return STATUS_INVALID_PARAMETER_5;
            }

            ProbeForWrite (UserAddressArray,
                           NumberOfPages * sizeof (PVOID),
                           sizeof(PVOID));

            ProbeForWriteUlong (Granularity);
        }
        else {
            NumberOfPages = *EntriesInUserAddressArray;
            ASSERT (NumberOfPages != 0);
        }

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Carefully probe and capture the user virtual address array.
    //

    PoolArea = (PVOID)&StackArray[0];

    NumberOfBytes = NumberOfPages * sizeof(ULONG_PTR);

    if (NumberOfPages > COPY_STACK_SIZE) {
        PoolArea = ExAllocatePoolWithTag (NonPagedPool,
                                                 NumberOfBytes,
                                                 'cGmM');

        if (PoolArea == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    PoolAreaPointer = (PVOID *)PoolArea;

    Attached = FALSE;

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            goto ErrorReturn0;
        }
    }

    EndAddress = (PVOID)((PCHAR)BaseAddress + RegionSize - 1);

    PagesWritten = 0;

    if (BaseAddress > EndAddress) {
        Status = STATUS_INVALID_PARAMETER_4;
        goto ErrorReturn;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Vad = NULL;

    //
    // Initializing PhysicalView is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PhysicalView = NULL;

    First = TRUE;

    LOCK_PFN (OldIrql);

    //
    // The PhysicalVadList should typically have just one entry - the view
    // we're looking for, so this traverse should be quick.
    //

    NextEntry = Process->PhysicalVadList.Flink;
    while (NextEntry != &Process->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {

            if ((BaseAddress >= (PVOID)PhysicalView->StartVa) &&
                (EndAddress <= (PVOID)PhysicalView->EndVa)) {

                    Vad = PhysicalView->Vad;
                    break;
            }
        }

        NextEntry = NextEntry->Flink;
        continue;
    }

    if (Vad == NULL) {

        //
        // No virtual address is marked for write-watch at the specified base
        // address, return an error.
        //

        Status = STATUS_INVALID_PARAMETER_1;
        UNLOCK_PFN (OldIrql);
        goto ErrorReturn;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // Extract the write watch status for each page in the range.
    // Note the PFN lock must be held to ensure atomicity.
    //

    BitMap = PhysicalView->u.BitMap;

    PointerPte = MiGetPteAddress (BaseAddress);
    EndPte = MiGetPteAddress (EndAddress);

    PointerPde = MiGetPdeAddress (BaseAddress);
    PointerPpe = MiGetPpeAddress (BaseAddress);
    PointerPxe = MiGetPxeAddress (BaseAddress);

    BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    BitMapIndex = (ULONG)(((PCHAR)BaseAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);

    ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
    ASSERT (BitMapIndex + (EndPte - PointerPte) < BitMap->SizeOfBitMap);

    while (PointerPte <= EndPte) {

        ASSERT (BitMapIndex < BitMap->SizeOfBitMap);

        UserWritten = FALSE;

        //
        // If the PTE is marked dirty (or writable) OR the BitMap says it's
        // dirtied, then let the caller know.
        //

        if (RtlCheckBit (BitMap, BitMapIndex) == 1) {
            UserWritten = TRUE;

            //
            // Note that a chunk of bits cannot be cleared at once because
            // the user array may overflow at any time.  If the user specifies
            // a bad address and the results cannot be written out, then it's
            // his own fault that he won't know which bits were cleared !
            //

            if (Flags & WRITE_WATCH_FLAG_RESET) {
                RtlClearBit (BitMap, BitMapIndex);
                goto ClearPteIfValid;
            }
        }
        else {

ClearPteIfValid:

            //
            // If the page table page is not present, then the dirty bit
            // has already been captured to the write watch bitmap.
            // Unfortunately all the entries in the page cannot be skipped
            // as the write watch bitmap must be checked for each PTE.
            //
    
#if (_MI_PAGING_LEVELS >= 4)
            if (PointerPxe->u.Hard.Valid == 0) {

                //
                // Skip the entire extended page parent if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif
#if (_MI_PAGING_LEVELS >= 3)
            if (PointerPpe->u.Hard.Valid == 0) {

                //
                // Skip the entire page parent if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif
            if (PointerPde->u.Hard.Valid == 0) {

                //
                // Skip the entire page directory if the bitmap permits.
                // The search starts at BitMapIndex (not BitMapIndex + 1) to
                // avoid wraps.
                //

                NextBitMapIndex = RtlFindSetBits (BitMap, 1, BitMapIndex);

                PointerPde += 1;
                NextPte = MiGetVirtualAddressMappedByPte (PointerPde);

                //
                // Compare the bitmap jump with the PTE jump and take
                // the lesser of the two.
                //

                if ((NextBitMapIndex == NO_BITS_FOUND) ||
                    ((ULONG)(NextPte - PointerPte) < (NextBitMapIndex - BitMapIndex))) {
                    BitMapIndex += (ULONG)(NextPte - PointerPte);
                    PointerPte = NextPte;
                }
                else {
                    PointerPte += (NextBitMapIndex - BitMapIndex);
                    BitMapIndex = NextBitMapIndex;
                }

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPdeAddress (PointerPte);
                PointerPxe = MiGetPpeAddress (PointerPte);

                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }

            PteContents = *PointerPte;

            if ((PteContents.u.Hard.Valid == 1) &&
                (MI_IS_PTE_DIRTY(PteContents))) {

                ASSERT (MI_PFN_ELEMENT(MI_GET_PAGE_FRAME_FROM_PTE(&PteContents))->u3.e1.PrototypePte == 0);

                UserWritten = TRUE;
                if (Flags & WRITE_WATCH_FLAG_RESET) {

                    //
                    // For the uniprocessor x86, just the dirty bit is
                    // cleared.  For all other platforms, the PTE writable
                    // bit must be disabled now so future writes trigger
                    // write watch updates.
                    //
        
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                    Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
                    ASSERT (Pfn1->u3.e1.PrototypePte == 0);
        
                    MI_MAKE_VALID_PTE (TempPte,
                                       PageFrameIndex,
                                       Pfn1->OriginalPte.u.Soft.Protection,
                                       PointerPte);
        
                    WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);
                    MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);
        
                    //
                    // Flush the TB as the protection of a valid PTE is
                    // being changed.
                    //
        
                    PreviousPte.u.Flush = KeFlushSingleTb (BaseAddress,
                                                           FALSE,
                                                           FALSE,
                                                           (PHARDWARE_PTE)PointerPte,
                                                           *(PHARDWARE_PTE)&TempPte.u.Hard);
                
                    ASSERT (PreviousPte.u.Hard.Valid == 1);
                
                    //
                    // A page's protection is being changed, on certain
                    // hardware the dirty bit should be ORed into the
                    // modify bit in the PFN element.
                    //
                    
                    MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);
                }
            }
        }

        if (UserWritten == TRUE) {
            *PoolAreaPointer = BaseAddress;
            PoolAreaPointer += 1;
            PagesWritten += 1;
            if (PagesWritten == NumberOfPages) {

                //
                // User array isn't big enough to take any more.  The API
                // (inherited from Win9x) is defined to return at this point.
                //

                break;
            }
        }

        PointerPte += 1;
        if (MiIsPteOnPdeBoundary(PointerPte)) {
            PointerPde = MiGetPteAddress (PointerPte);
            if (MiIsPteOnPdeBoundary(PointerPde)) {
                PointerPpe = MiGetPdeAddress (PointerPte);
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary(PointerPpe)) {
                    PointerPxe = MiGetPpeAddress (PointerPte);
                }
#endif
            }
        }
        BitMapIndex += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
    }

    UNLOCK_PFN (OldIrql);

    Status = STATUS_SUCCESS;

ErrorReturn:

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    if (Status == STATUS_SUCCESS) {

        //
        // Return all results to the caller.
        //
    
        try {
    
            RtlCopyMemory (UserAddressArray,
                           PoolArea,
                           PagesWritten * sizeof (PVOID));

            *EntriesInUserAddressArray = PagesWritten;

            *Granularity = PAGE_SIZE;
    
        } except (ExSystemExceptionFilter()) {
    
            Status = GetExceptionCode();
        }
    }
    
ErrorReturn0:

    if (PoolArea != (PVOID)&StackArray[0]) {
        ExFreePool (PoolArea);
    }

    return Status;
}

NTSTATUS
NtResetWriteWatch (
    IN HANDLE ProcessHandle,
    IN PVOID BaseAddress,
    IN SIZE_T RegionSize
    )

/*++

Routine Description:

    This function clears the write watch status of the argument region.
    This allows callers to "forget" old writes and only see new ones from
    this point on.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - An address within a region of pages to be reset.  This
                  value must lie within a private memory region with the
                  write-watch attribute already set.

    RegionSize - The size of the region in bytes beginning at the base address
                 specified.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PVOID EndAddress;
    PMMVAD Vad;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PEPROCESS Process;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE EndPte;
    NTSTATUS Status;
    MMPTE PreviousPte;
    MMPTE PteContents;
    MMPTE TempPte;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    LOGICAL First;
    LOGICAL Attached;
    KPROCESSOR_MODE PreviousMode;
    PFN_NUMBER PageFrameIndex;
    ULONG WorkingSetIndex;
    KAPC_STATE ApcState;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (BaseAddress > MM_HIGHEST_VAD_ADDRESS) {
        return STATUS_INVALID_PARAMETER_2;
    }

    if ((((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) - (ULONG_PTR)BaseAddress) <
            RegionSize) {
        return STATUS_INVALID_PARAMETER_3;
    }

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread(CurrentThread);

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {
        PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }
    }

    Attached = FALSE;

    EndAddress = (PVOID)((PCHAR)BaseAddress + RegionSize - 1);
    
    if (BaseAddress > EndAddress) {
        Status = STATUS_INVALID_PARAMETER_3;
        goto ErrorReturn;
    }

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (CurrentProcess != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    Vad = NULL;
    First = TRUE;

    //
    // Initializing PhysicalView is not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PhysicalView = NULL;

    LOCK_PFN (OldIrql);

    //
    // The PhysicalVadList should typically have just one entry - the view
    // we're looking for, so this traverse should be quick.
    //

    NextEntry = Process->PhysicalVadList.Flink;
    while (NextEntry != &Process->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {

            if ((BaseAddress >= (PVOID)PhysicalView->StartVa) &&
                (EndAddress <= (PVOID)PhysicalView->EndVa)) {

                    Vad = PhysicalView->Vad;
                    break;
            }
        }

        NextEntry = NextEntry->Flink;
        continue;
    }

    if (Vad == NULL) {

        //
        // No virtual address is marked for write-watch at the specified base
        // address, return an error.
        //

        Status = STATUS_INVALID_PARAMETER_1;
        UNLOCK_PFN (OldIrql);
        goto ErrorReturn;
    }

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // Clear the write watch status (and PTE writable/dirty bits) for each page
    // in the range.  Note if the PTE is not currently valid, then the write
    // watch bit has already been captured to the bitmap.  Hence only valid PTEs
    // need adjusting.
    //
    // The PFN lock must be held to ensure atomicity.
    //

    BitMap = PhysicalView->u.BitMap;

    PointerPte = MiGetPteAddress (BaseAddress);
    EndPte = MiGetPteAddress (EndAddress);

    BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);

    BitMapIndex = (ULONG)(((PCHAR)BaseAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);

    ASSERT (BitMapIndex < BitMap->SizeOfBitMap);
    ASSERT (BitMapIndex + (EndPte - PointerPte) < BitMap->SizeOfBitMap);

    RtlClearBits (BitMap, BitMapIndex, (ULONG)(EndPte - PointerPte + 1));

    while (PointerPte <= EndPte) {

        //
        // If the page table page is not present, then the dirty bit
        // has already been captured to the write watch bitmap.  So skip it.
        //

        if ((First == TRUE) || MiIsPteOnPdeBoundary(PointerPte)) {
            First = FALSE;

            PointerPpe = MiGetPpeAddress (BaseAddress);
            PointerPxe = MiGetPxeAddress (BaseAddress);

#if (_MI_PAGING_LEVELS >= 4)
            if (PointerPxe->u.Hard.Valid == 0) {
                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif

#if (_MI_PAGING_LEVELS >= 3)
            if (PointerPpe->u.Hard.Valid == 0) {
                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
#endif

            PointerPde = MiGetPdeAddress (BaseAddress);

            if (PointerPde->u.Hard.Valid == 0) {
                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                continue;
            }
        }

        //
        // If the PTE is marked dirty (or writable) OR the BitMap says it's
        // dirtied, then let the caller know.
        //

        PteContents = *PointerPte;

        if ((PteContents.u.Hard.Valid == 1) &&
            (MI_IS_PTE_DIRTY(PteContents))) {

            //
            // For the uniprocessor x86, just the dirty bit is cleared.
            // For all other platforms, the PTE writable bit must be
            // disabled now so future writes trigger write watch updates.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT(PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 0);

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               Pfn1->OriginalPte.u.Soft.Protection,
                               PointerPte);

            WorkingSetIndex = MI_GET_WORKING_SET_FROM_PTE (&PteContents);
            MI_SET_PTE_IN_WORKING_SET (&TempPte, WorkingSetIndex);

            //
            // Flush the TB as the protection of a valid PTE is being changed.
            //

            PreviousPte.u.Flush = KeFlushSingleTb (BaseAddress,
                                                   FALSE,
                                                   FALSE,
                                                   (PHARDWARE_PTE)PointerPte,
                                                   *(PHARDWARE_PTE)&TempPte.u.Hard);
        
            ASSERT (PreviousPte.u.Hard.Valid == 1);
        
            //
            // A page's protection is being changed, on certain
            // hardware the dirty bit should be ORed into the
            // modify bit in the PFN element.
            //
        
            MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn1);
        }

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
    }

    UNLOCK_PFN (OldIrql);

    Status = STATUS_SUCCESS;

ErrorReturn:

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    return Status;
}

VOID
MiCaptureWriteWatchDirtyBit (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine sets the write watch bit corresponding to the argument
    virtual address.

Arguments:

    Process - Supplies a pointer to an executive process structure.

    VirtualAddress - Supplies the modified virtual address.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.
    held.

--*/

{
    PMMVAD Vad;
    PLIST_ENTRY NextEntry;
    PMI_PHYSICAL_VIEW PhysicalView;
    PRTL_BITMAP BitMap;
    ULONG BitMapIndex;

    MM_PFN_LOCK_ASSERT();

    ASSERT (Process->Flags & PS_PROCESS_FLAGS_USING_WRITE_WATCH);

    //
    // This process has (or had) write watch VADs.  Search now
    // for a write watch region encapsulating the PTE being
    // invalidated.
    //

    Vad = NULL;
    NextEntry = Process->PhysicalVadList.Flink;
    while (NextEntry != &Process->PhysicalVadList) {

        PhysicalView = CONTAINING_RECORD(NextEntry,
                                         MI_PHYSICAL_VIEW,
                                         ListEntry);

        if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {

            if ((VirtualAddress >= (PVOID)PhysicalView->StartVa) &&
                (VirtualAddress <= (PVOID)PhysicalView->EndVa)) {

                //
                // The write watch bitmap must be updated.
                //

                Vad = PhysicalView->Vad;
                BitMap = PhysicalView->u.BitMap;

                BitMapIndex = (ULONG)(((PCHAR)VirtualAddress - (PCHAR)(Vad->StartingVpn << PAGE_SHIFT)) >> PAGE_SHIFT);
            
                ASSERT (BitMapIndex < BitMap->SizeOfBitMap);

                RtlSetBit (BitMap, BitMapIndex);
                break;
            }
        }

        NextEntry = NextEntry->Flink;
        continue;
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\XPSP1\NT\base\ntos\mm\sysload.c ===
/*++

Copyright (c) 1991  Microsoft Corporation

Module Name:

   sysload.c

Abstract:

    This module contains the code to load DLLs into the system portion of
    the address space and calls the DLL at its initialization entry point.

Author:

    Lou Perazzoli 21-May-1991
    Landy Wang 02-June-1997

Revision History:

--*/

#include "mi.h"

KMUTANT MmSystemLoadLock;

ULONG MmTotalSystemDriverPages;

ULONG MmDriverCommit;

ULONG MiFirstDriverLoadEver = 0;

//
// This key is set to TRUE to make more memory below 16mb available for drivers.
// It can be cleared via the registry.
//

LOGICAL MmMakeLowMemory = TRUE;

//
// Enabled via the registry to identify drivers which unload without releasing
// resources or still have active timers, etc.
//

PUNLOADED_DRIVERS MmUnloadedDrivers;

ULONG MmLastUnloadedDriver;
ULONG MiTotalUnloads;
ULONG MiUnloadsSkipped;

//
// This can be set by the registry.
//

ULONG MmEnforceWriteProtection = 1;

//
// Referenced by ke\bugcheck.c.
//

PVOID ExPoolCodeStart;
PVOID ExPoolCodeEnd;
PVOID MmPoolCodeStart;
PVOID MmPoolCodeEnd;
PVOID MmPteCodeStart;
PVOID MmPteCodeEnd;

extern LONG MiSessionLeaderExists;

ULONG
CacheImageSymbols (
    IN PVOID ImageBase
    );

NTSTATUS
MiResolveImageReferences (
    PVOID ImageBase,
    IN PUNICODE_STRING ImageFileDirectory,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    OUT PCHAR *MissingProcedureName,
    OUT PWSTR *MissingDriverName,
    OUT PLOAD_IMPORTS *LoadedImports
    );

NTSTATUS
MiSnapThunk (
    IN PVOID DllBase,
    IN PVOID ImageBase,
    IN PIMAGE_THUNK_DATA NameThunk,
    OUT PIMAGE_THUNK_DATA AddrThunk,
    IN PIMAGE_EXPORT_DIRECTORY ExportDirectory,
    IN ULONG ExportSize,
    IN LOGICAL SnapForwarder,
    OUT PCHAR *MissingProcedureName
    );

NTSTATUS
MiLoadImageSection (
    IN PSECTION SectionPointer,
    OUT PVOID *ImageBase,
    IN PUNICODE_STRING ImageFileName,
    IN ULONG LoadInSessionSpace
    );

VOID
MiEnablePagingOfDriver (
    IN PVOID ImageHandle
    );

VOID
MiSetPagingOfDriver (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL SessionSpace
    );

PVOID
MiLookupImageSectionByName (
    IN PVOID Base,
    IN LOGICAL MappedAsImage,
    IN PCHAR SectionName,
    OUT PULONG SectionSize
    );

VOID
MiClearImports (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

NTSTATUS
MiBuildImportsForBootDrivers (
    VOID
    );

NTSTATUS
MmCheckSystemImage (
    IN HANDLE ImageFileHandle,
    IN LOGICAL PurgeSection
    );

LONG
MiMapCacheExceptionFilter (
    OUT PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    );

ULONG
MiSetProtectionOnTransitionPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

NTSTATUS
MiDereferenceImports (
    IN PLOAD_IMPORTS ImportList
    );

LOGICAL
MiCallDllUnloadAndUnloadDll (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

PVOID
MiLocateExportName (
    IN PVOID DllBase,
    IN PCHAR FunctionName
    );

VOID
MiRememberUnloadedDriver (
    IN PUNICODE_STRING DriverName,
    IN PVOID Address,
    IN ULONG Length
    );

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    );

VOID
MiLocateKernelSections (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
MiUpdateThunks (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PVOID OldAddress,
    IN PVOID NewAddress,
    IN ULONG NumberOfBytes
    );

PVOID
MiFindExportedRoutineByName (
    IN PVOID DllBase,
    IN PANSI_STRING AnsiImageRoutineName
    );

#if 0
VOID
MiLockDriverPdata (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );
#endif

VOID
MiSetSystemCodeProtection (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte
    );

LOGICAL
MiChargeResidentAvailable (
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Id
    );

VOID
MiReturnResidentAvailable (
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Id
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmCheckSystemImage)
#pragma alloc_text(PAGE,MmLoadSystemImage)
#pragma alloc_text(PAGE,MiResolveImageReferences)
#pragma alloc_text(PAGE,MiSnapThunk)
#pragma alloc_text(PAGE,MiEnablePagingOfDriver)
#pragma alloc_text(PAGE,MmPageEntireDriver)
#pragma alloc_text(PAGE,MiSetImageProtect)
#pragma alloc_text(PAGE,MiDereferenceImports)
#pragma alloc_text(PAGE,MiCallDllUnloadAndUnloadDll)
#pragma alloc_text(PAGE,MiLocateExportName)
#pragma alloc_text(PAGE,MiClearImports)
#pragma alloc_text(PAGE,MiWriteProtectSystemImage)
#pragma alloc_text(PAGE,MmGetSystemRoutineAddress)
#pragma alloc_text(PAGE,MiFindExportedRoutineByName)
#pragma alloc_text(PAGE,MmCallDllInitialize)
#pragma alloc_text(PAGE,MmFreeDriverInitialization)
#pragma alloc_text(PAGE,MmResetDriverPaging)
#pragma alloc_text(PAGE,MmUnloadSystemImage)
#pragma alloc_text(PAGE,MiLoadImageSection)
#pragma alloc_text(PAGE,MiRememberUnloadedDriver)
#pragma alloc_text(INIT,MiBuildImportsForBootDrivers)
#pragma alloc_text(INIT,MiReloadBootLoadedDrivers)
#pragma alloc_text(INIT,MiUpdateThunks)
#pragma alloc_text(INIT,MiInitializeLoadedModuleList)
#pragma alloc_text(INIT,MiLocateKernelSections)
#if 0
#pragma alloc_text(PAGEKD,MiLockDriverPdata)
#endif

#if !defined(NT_UP)
#pragma alloc_text(PAGE,MmVerifyImageIsOkForMpUse)
#endif

#endif

CHAR MiPteStr[] = "\0";

VOID
MiProcessLoaderEntry (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry,
    IN LOGICAL Insert
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PsLoadedModuleList
    lock to insert a new entry.

Arguments:

    DataTableEntry - Supplies the loaded module list entry to insert/remove.

    Insert - Supplies TRUE if the entry should be inserted, FALSE if the entry
             should be removed.

Return Value:

    None.

Environment:

    Kernel mode.  Normal APCs disabled (critical region held).

--*/

{
    KIRQL OldIrql;

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);
    ExAcquireSpinLock (&PsLoadedModuleSpinLock, &OldIrql);

    if (Insert == TRUE) {
        InsertTailList (&PsLoadedModuleList, &DataTableEntry->InLoadOrderLinks);

#if defined(_AMD64_) // || defined(_IA64_)

        RtlInsertInvertedFunctionTable (&PsInvertedFunctionTable,
                                        DataTableEntry->DllBase,
                                        DataTableEntry->SizeOfImage);

#endif

    }
    else {

#if defined(_AMD64_) // || defined(_IA64_)

        RtlRemoveInvertedFunctionTable (&PsInvertedFunctionTable,
                                        DataTableEntry->DllBase);

#endif

        RemoveEntryList (&DataTableEntry->InLoadOrderLinks);
    }

    ExReleaseSpinLock (&PsLoadedModuleSpinLock, OldIrql);
    ExReleaseResourceLite (&PsLoadedModuleResource);
}

NTSTATUS
MmLoadSystemImage (
    IN PUNICODE_STRING ImageFileName,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    IN PUNICODE_STRING LoadedBaseName OPTIONAL,
    IN ULONG LoadFlags,
    OUT PVOID *ImageHandle,
    OUT PVOID *ImageBaseAddress
    )

/*++

Routine Description:

    This routine reads the image pages from the specified section into
    the system and returns the address of the DLL's header.

    At successful completion, the Section is referenced so it remains
    until the system image is unloaded.

Arguments:

    ImageFileName - Supplies the full path name (including the image name)
                    of the image to load.

    NamePrefix - If present, supplies the prefix to use with the image name on
                 load operations.  This is used to load the same image multiple
                 times, by using different prefixes.

    LoadedBaseName - If present, supplies the base name to use on the
                     loaded image instead of the base name found on the
                     image name.

    LoadFlags - Supplies a combination of bit flags as follows:

        MM_LOAD_IMAGE_IN_SESSION :
                       - Supplies whether to load this image in session space.
                         Each session gets a different copy of this driver with
                         pages shared as much as possible via copy on write.

        MM_LOAD_IMAGE_AND_LOCKDOWN :
                       - Supplies TRUE if the image pages should be made
                         nonpagable.

    ImageHandle - Returns an opaque pointer to the referenced section object
                  of the image that was loaded.

    ImageBaseAddress - Returns the image base within the system.

Return Value:

    Status of the load operation.

Environment:

    Kernel mode, APC_LEVEL or below, arbitrary process context.

--*/

{
    SIZE_T DataTableEntrySize;
    PWSTR BaseDllNameBuffer;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    KLDR_DATA_TABLE_ENTRY TempDataTableEntry;
    NTSTATUS Status;
    PSECTION SectionPointer;
    PIMAGE_NT_HEADERS NtHeaders;
    UNICODE_STRING PrefixedImageName;
    UNICODE_STRING BaseName;
    UNICODE_STRING BaseDirectory;
    OBJECT_ATTRIBUTES ObjectAttributes;
    HANDLE FileHandle;
    HANDLE SectionHandle;
    IO_STATUS_BLOCK IoStatus;
    PCHAR NameBuffer;
    PLIST_ENTRY NextEntry;
    ULONG NumberOfPtes;
    PCHAR MissingProcedureName;
    PWSTR MissingDriverName;
    PWSTR PrintableMissingDriverName;
    PLOAD_IMPORTS LoadedImports;
    PMMSESSION Session;
    LOGICAL AlreadyOpen;
    LOGICAL IssueUnloadOnFailure;
    ULONG SectionAccess;
    PKTHREAD CurrentThread;

    PAGED_CODE();

    LoadedImports = (PLOAD_IMPORTS)NO_IMPORTS_USED;
    SectionPointer = (PVOID)-1;
    FileHandle = (HANDLE)0;
    MissingProcedureName = NULL;
    MissingDriverName = NULL;
    IssueUnloadOnFailure = FALSE;
    MiFirstDriverLoadEver |= 0x1;

    NameBuffer = ExAllocatePoolWithTag (NonPagedPool,
                                        MAXIMUM_FILENAME_LENGTH,
                                        'nLmM');

    if (NameBuffer == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    NumberOfPtes = (ULONG)-1;
    DataTableEntry = NULL;

    if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

        ASSERT (NamePrefix == NULL);
        ASSERT (LoadedBaseName == NULL);

        if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
            ExFreePool (NameBuffer);
            return STATUS_NO_MEMORY;
        }

        ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

        Session = &MmSessionSpace->Session;
    }
    else {
        Session = &MmSession;
    }

    //
    // Get name roots.
    //

    if (ImageFileName->Buffer[0] == OBJ_NAME_PATH_SEPARATOR) {
        PWCHAR p;
        ULONG l;

        p = &ImageFileName->Buffer[ImageFileName->Length>>1];
        while (*(p-1) != OBJ_NAME_PATH_SEPARATOR) {
            p--;
        }
        l = (ULONG)(&ImageFileName->Buffer[ImageFileName->Length>>1] - p);
        l *= sizeof(WCHAR);
        BaseName.Length = (USHORT)l;
        BaseName.Buffer = p;
    }
    else {
        BaseName.Length = ImageFileName->Length;
        BaseName.Buffer = ImageFileName->Buffer;
    }

    BaseName.MaximumLength = BaseName.Length;
    BaseDirectory = *ImageFileName;
    BaseDirectory.Length = (USHORT)(BaseDirectory.Length - BaseName.Length);
    BaseDirectory.MaximumLength = BaseDirectory.Length;
    PrefixedImageName = *ImageFileName;

    //
    // If there's a name prefix, add it to the PrefixedImageName.
    //

    if (NamePrefix) {
        PrefixedImageName.MaximumLength = (USHORT)(BaseDirectory.Length + NamePrefix->Length + BaseName.Length);

        PrefixedImageName.Buffer = ExAllocatePoolWithTag (
                                    NonPagedPool,
                                    PrefixedImageName.MaximumLength,
                                    'dLmM');

        if (!PrefixedImageName.Buffer) {
            ExFreePool (NameBuffer);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        PrefixedImageName.Length = 0;
        RtlAppendUnicodeStringToString(&PrefixedImageName, &BaseDirectory);
        RtlAppendUnicodeStringToString(&PrefixedImageName, NamePrefix);
        RtlAppendUnicodeStringToString(&PrefixedImageName, &BaseName);

        //
        // Alter the basename to match.
        //

        BaseName.Buffer = PrefixedImageName.Buffer + BaseDirectory.Length / sizeof(WCHAR);
        BaseName.Length = (USHORT)(BaseName.Length + NamePrefix->Length);
        BaseName.MaximumLength = (USHORT)(BaseName.MaximumLength + NamePrefix->Length);
    }

    //
    // If there's a loaded base name, use it instead of the base name.
    //

    if (LoadedBaseName) {
        BaseName = *LoadedBaseName;
    }

#if DBG
    if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
        DbgPrint ("MM:SYSLDR Loading %wZ (%wZ) %s\n",
            &PrefixedImageName,
            &BaseName,
            (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) ? "in session space" : " ");
    }
#endif

    AlreadyOpen = FALSE;

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    //
    // Check to see if this name already exists in the loader database.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        if (RtlEqualUnicodeString (&PrefixedImageName,
                                   &DataTableEntry->FullDllName,
                                   TRUE)) {

            if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

                if (MI_IS_SESSION_ADDRESS (DataTableEntry->DllBase) == FALSE) {

                    //
                    // The caller is trying to load a driver in session space
                    // that has already been loaded in system space.  This is
                    // not allowed.
                    //

                    Status = STATUS_CONFLICTING_ADDRESSES;
                    goto return2;
                }

                AlreadyOpen = TRUE;

                //
                // The LoadCount should generally not be 0 here, but it is
                // possible in the case where an attempt has been made to
                // unload a DLL on last dereference, but the DLL refused to
                // unload.
                //

                DataTableEntry->LoadCount += 1;
                SectionPointer = DataTableEntry->SectionPointer;
                break;
            }
            else {
                if (MI_IS_SESSION_ADDRESS (DataTableEntry->DllBase) == TRUE) {

                    //
                    // The caller is trying to load a driver in systemwide space
                    // that has already been loaded in session space.  This is
                    // not allowed.
                    //

                    Status = STATUS_CONFLICTING_ADDRESSES;
                    goto return2;
                }
            }

            *ImageHandle = DataTableEntry;
            *ImageBaseAddress = DataTableEntry->DllBase;
            Status = STATUS_IMAGE_ALREADY_LOADED;
            goto return2;
        }

        NextEntry = NextEntry->Flink;
    }

    ASSERT (AlreadyOpen == TRUE || NextEntry == &PsLoadedModuleList);

    if (AlreadyOpen == FALSE) {

        //
        // Check and see if a user wants to replace this binary
        // via a transfer through the kernel debugger.  If this
        // fails just continue on with the existing file.
        //
        if (KdDebuggerEnabled && KdDebuggerNotPresent == FALSE) {
            Status = KdPullRemoteFile(ImageFileName,
                                      FILE_ATTRIBUTE_NORMAL,
                                      FILE_OVERWRITE_IF,
                                      FILE_SYNCHRONOUS_IO_NONALERT);
            if (NT_SUCCESS(Status)) {
                DbgPrint("MmLoadSystemImage: Pulled %wZ from kd\n",
                         ImageFileName);
            }
        }
        
        DataTableEntry = NULL;

        //
        // Attempt to open the driver image itself.  If this fails, then the
        // driver image cannot be located, so nothing else matters.
        //

        InitializeObjectAttributes (&ObjectAttributes,
                                    ImageFileName,
                                    (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                    NULL,
                                    NULL);

        Status = ZwOpenFile (&FileHandle,
                             FILE_EXECUTE,
                             &ObjectAttributes,
                             &IoStatus,
                             FILE_SHARE_READ | FILE_SHARE_DELETE,
                             0);

        if (!NT_SUCCESS(Status)) {

#if DBG
            if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
                DbgPrint ("MiLoadImageSection: cannot open %wZ\n",
                    ImageFileName);
            }
#endif
            //
            // Don't raise hard error status for file not found.
            //

            goto return2;
        }

        Status = MmCheckSystemImage(FileHandle, FALSE);

        if ((Status == STATUS_IMAGE_CHECKSUM_MISMATCH) ||
            (Status == STATUS_IMAGE_MP_UP_MISMATCH) ||
            (Status == STATUS_INVALID_IMAGE_PROTECT)) {

            goto return1;
        }

        //
        // Now attempt to create an image section for the file.  If this fails,
        // then the driver file is not an image.  Session space drivers are
        // shared text with copy on write data, so don't allow writes here.
        //

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {
            SectionAccess = SECTION_MAP_READ | SECTION_MAP_EXECUTE;
        }
        else {
            SectionAccess = SECTION_ALL_ACCESS;
        }

        InitializeObjectAttributes (&ObjectAttributes,
                                    NULL,
                                    (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                    NULL,
                                    NULL);

        Status = ZwCreateSection (&SectionHandle,
                                  SectionAccess,
                                  &ObjectAttributes,
                                  (PLARGE_INTEGER) NULL,
                                  PAGE_EXECUTE,
                                  SEC_IMAGE,
                                  FileHandle);

        if (!NT_SUCCESS(Status)) {
            goto return1;
        }

        //
        // Now reference the section handle.  If this fails something is
        // very wrong because we are in a privileged process.
        //
        // N.B.  ObRef sets SectionPointer to NULL on failure so it must be
        // reset to -1 in this case.
        //

        Status = ObReferenceObjectByHandle (SectionHandle,
                                        SECTION_MAP_EXECUTE,
                                        MmSectionObjectType,
                                        KernelMode,
                                        (PVOID *) &SectionPointer,
                                        (POBJECT_HANDLE_INFORMATION) NULL );

        ZwClose (SectionHandle);
        if (!NT_SUCCESS (Status)) {
            SectionPointer = (PVOID)-1;     // undo ObRef setting.
            goto return1;
        }

        if (SectionPointer->Segment->ControlArea->NumberOfSubsections == 1) {
            if (((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) &&
                (SectionPointer->Segment->BasedAddress != (PVOID)Session->SystemSpaceViewStart)) {

                PSECTION SectionPointer2;

                //
                // The driver was linked with subsection alignment such that
                // it is mapped with one subsection.  Since the CreateSection
                // above guarantees that the driver image is indeed a
                // satisfactory executable, map it directly now to reuse the
                // cache from the MmCheckSystemImage call above.
                //

                Status = ZwCreateSection (&SectionHandle,
                                          SectionAccess,
                                          (POBJECT_ATTRIBUTES) NULL,
                                          (PLARGE_INTEGER) NULL,
                                          PAGE_EXECUTE,
                                          SEC_COMMIT,
                                          FileHandle );

                if (NT_SUCCESS(Status)) {

                    Status = ObReferenceObjectByHandle (
                                            SectionHandle,
                                            SECTION_MAP_EXECUTE,
                                            MmSectionObjectType,
                                            KernelMode,
                                            (PVOID *) &SectionPointer2,
                                            (POBJECT_HANDLE_INFORMATION) NULL );

                    ZwClose (SectionHandle);

                    if (NT_SUCCESS (Status)) {

                        //
                        // The number of PTEs won't match if the image is
                        // stripped and the debug directory crosses the last
                        // sector boundary of the file.  We could still use the
                        // new section, but these cases are under 2% of all the
                        // drivers loaded so don't bother.
                        //

                        if (SectionPointer->Segment->TotalNumberOfPtes == SectionPointer2->Segment->TotalNumberOfPtes) {
                            ObDereferenceObject (SectionPointer);
                            SectionPointer = SectionPointer2;
                        }
                        else {
                            ObDereferenceObject (SectionPointer2);
                        }
                    }
                }
            }
        }

    }

    //
    // Load the driver from the filesystem and pick a virtual address for it.
    // For Hydra, this means also allocating session virtual space, and
    // after mapping a view of the image, either copying or sharing the
    // driver's code and data in the session virtual space.
    //
    // If it is a share map because the image was loaded at its based address,
    // the disk image will remain busy.
    //

    Status = MiLoadImageSection (SectionPointer,
                                 ImageBaseAddress,
                                 ImageFileName,
                                 LoadFlags & MM_LOAD_IMAGE_IN_SESSION);

    NumberOfPtes = SectionPointer->Segment->TotalNumberOfPtes;

    if (Status == STATUS_ALREADY_COMMITTED) {

        //
        // This is a driver that was relocated that is being loaded into
        // the same session space twice.  Don't increment the overall load
        // count - just the image count in the session which has already been
        // done.
        //

        ASSERT (AlreadyOpen == TRUE);
        ASSERT (LoadFlags & MM_LOAD_IMAGE_IN_SESSION);
        ASSERT (DataTableEntry != NULL);
        ASSERT (DataTableEntry->LoadCount > 1);

        *ImageHandle = DataTableEntry;
        *ImageBaseAddress = DataTableEntry->DllBase;

        DataTableEntry->LoadCount -= 1;
        Status = STATUS_SUCCESS;
        goto return1;
    }

    if ((MiFirstDriverLoadEver & 0x2) == 0) {

        NTSTATUS PagingPathStatus;

        //
        // Check with all of the drivers along the path to win32k.sys to
        // ensure that they are willing to follow the rules required
        // of them and to give them a chance to lock down code and data
        // that needs to be locked.  If any of the drivers along the path
        // refuses to participate, fail the win32k.sys load.
        //
        // It is assumed that all drivers live on the same physical drive, so
        // when the very first driver is loaded, this check can be made.
        // This eliminates the need to check for things like relocated win32ks,
        // Terminal Server systems, etc.
        //

        //
        // In WinPE removable media boot case don't do this since user might
        // be running WinPE in RAM and would like to swap out the boot media.
        //
        if (InitWinPEModeType & INIT_WINPEMODE_REMOVABLE_MEDIA) {
            PagingPathStatus = STATUS_SUCCESS;
        } else {            
            PagingPathStatus = PpPagePathAssign(SectionPointer->Segment->ControlArea->FilePointer);
        }            

        if (!NT_SUCCESS(PagingPathStatus)) {

            KdPrint (("PpPagePathAssign FAILED for win32k.sys: %x\n",
                PagingPathStatus));

            //
            // Failing the insertion of win32k.sys' device in the
            // pagefile path is commented out until the storage drivers have
            // been modified to correctly handle this request.  If this is
            // added later, add code here to release relevant resources for
            // the error path.
            //
        }

        MiFirstDriverLoadEver |= 0x2;
    }

    //
    // Normal drivers are dereferenced here and their images can then be
    // overwritten.  This is ok because we've already read the whole thing
    // into memory and from here until reboot (or unload), we back them
    // with the pagefile.
    //
    // win32k.sys and session space drivers are the exception - these images
    // are inpaged from the filesystem and we need to keep our reference to
    // the file so that it doesn't get overwritten.
    //

    if ((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) {
        ObDereferenceObject (SectionPointer);
        SectionPointer = (PVOID)-1;
    }

    //
    // The module LoadCount will be 1 here if the module was just loaded.
    // The LoadCount will be >1 if it was attached to by a session (as opposed
    // to just loaded).
    //

    if (!NT_SUCCESS(Status)) {
        if (AlreadyOpen == TRUE) {

            //
            // We're failing and we were just attaching to an already loaded
            // driver.  We don't want to go through the forced unload path
            // because we've already deleted the address space.  Simply
            // decrement our reference and null the DataTableEntry
            // so we don't go through the forced unload path.
            //

            ASSERT (DataTableEntry != NULL);
            DataTableEntry->LoadCount -= 1;
            DataTableEntry = NULL;
        }
        goto return1;
    }

    //
    // Error recovery from this point out for sessions works as follows:
    //
    // For sessions, we may or may not have a DataTableEntry at this point.
    // If we do, it's because we're attaching to a driver that has already
    // been loaded - and the DataTableEntry->LoadCount has been bumped - so
    // the error recovery from here on out is to just call
    // MmUnloadSystemImage with the DataTableEntry.
    //
    // If this is the first load of a given driver into a session space, we
    // have no DataTableEntry at this point.  The view has already been mapped
    // and committed and the group/session addresses reserved for this DLL.
    // The error recovery path handles all this because
    // MmUnloadSystemImage will zero the relevant fields in the
    // LDR_DATA_TABLE_ENTRY so that MmUnloadSystemImage will work properly.
    //

    IssueUnloadOnFailure = TRUE;

    if (((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) == 0) ||
        (*ImageBaseAddress != SectionPointer->Segment->BasedAddress)) {

#if DBG

        //
        // Warn users about session images that cannot be shared
        // because they were linked at a bad address.
        //

        if ((LoadFlags & MM_LOAD_IMAGE_IN_SESSION) &&
            (MmSessionSpace->SessionId != 0)) {

            DbgPrint ("MM: Session %d image %wZ is linked at a nonsharable address (%p)\n",
                    MmSessionSpace->SessionId,
                    ImageFileName,
                    SectionPointer->Segment->BasedAddress);
            DbgPrint ("MM: Image %wZ has been moved to address (%p) by the system so it can run,\n",
                    ImageFileName,
                    *ImageBaseAddress);
            DbgPrint (" but this needs to be fixed in the image for sharing to occur.\n");
        }
#endif

        //
        // Apply the fixups to the section.
        //

        try {
            Status = LdrRelocateImage (*ImageBaseAddress,
                                       "SYSLDR",
                                       STATUS_SUCCESS,
                                       STATUS_CONFLICTING_ADDRESSES,
                                       STATUS_INVALID_IMAGE_FORMAT);

        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = GetExceptionCode();
            KdPrint(("MM:sysload - LdrRelocateImage failed status %lx\n",
                      Status));
        }

        if (!NT_SUCCESS(Status)) {

            //
            // Unload the system image and dereference the section.
            //

            goto return1;
        }
    }

    if (AlreadyOpen == FALSE) {

        ULONG DebugInfoSize;
        PIMAGE_DATA_DIRECTORY DataDirectory;
        PIMAGE_DEBUG_DIRECTORY DebugDir;
        PNON_PAGED_DEBUG_INFO ssHeader;
        UCHAR i;

        DebugInfoSize = 0;
        DataDirectory = NULL;
        DebugDir = NULL;

        NtHeaders = RtlImageNtHeader(*ImageBaseAddress);

        //
        // Create a loader table entry for this driver before resolving the
        // references so that any circular references can resolve properly.
        //

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

            DebugInfoSize = sizeof (NON_PAGED_DEBUG_INFO);

            if (IMAGE_DIRECTORY_ENTRY_DEBUG <
                NtHeaders->OptionalHeader.NumberOfRvaAndSizes) {

                DataDirectory = &NtHeaders->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_DEBUG];

                if (DataDirectory->VirtualAddress &&
                    DataDirectory->Size &&
                    (DataDirectory->VirtualAddress + DataDirectory->Size) <
                        NtHeaders->OptionalHeader.SizeOfImage) {

                    DebugDir = (PIMAGE_DEBUG_DIRECTORY)
                               ((PUCHAR)(*ImageBaseAddress) +
                                   DataDirectory->VirtualAddress);

                    DebugInfoSize += DataDirectory->Size;

                    for (i = 0;
                         i < DataDirectory->Size/sizeof(IMAGE_DEBUG_DIRECTORY);
                         i += 1) {

                        if ((DebugDir+i)->PointerToRawData &&
                            (DebugDir+i)->PointerToRawData <
                                NtHeaders->OptionalHeader.SizeOfImage &&
                            ((DebugDir+i)->PointerToRawData +
                                (DebugDir+i)->SizeOfData) <
                                NtHeaders->OptionalHeader.SizeOfImage) {

                            DebugInfoSize += (DebugDir+i)->SizeOfData;
                        }
                    }
                }

                DebugInfoSize = MI_ROUND_TO_SIZE(DebugInfoSize, sizeof(ULONG));
            }
        }

        DataTableEntrySize = sizeof (KLDR_DATA_TABLE_ENTRY) +
                             DebugInfoSize +
                             BaseName.Length + sizeof(UNICODE_NULL);

        DataTableEntry = ExAllocatePoolWithTag (NonPagedPool,
                                                DataTableEntrySize,
                                                'dLmM');

        if (DataTableEntry == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto return1;
        }

        //
        // Initialize the flags and load count.
        //

        DataTableEntry->Flags = LDRP_LOAD_IN_PROGRESS;
        DataTableEntry->LoadCount = 1;
        DataTableEntry->LoadedImports = (PVOID)LoadedImports;

        if ((NtHeaders->OptionalHeader.MajorOperatingSystemVersion >= 5) &&
            (NtHeaders->OptionalHeader.MajorImageVersion >= 5)) {
            DataTableEntry->Flags |= LDRP_ENTRY_NATIVE;
        }

        ssHeader = (PNON_PAGED_DEBUG_INFO) ((ULONG_PTR)DataTableEntry +
                                            sizeof (KLDR_DATA_TABLE_ENTRY));

        BaseDllNameBuffer = (PWSTR) ((ULONG_PTR)ssHeader + DebugInfoSize);

        //
        // If loading a session space image, store away some debug data.
        //

        DataTableEntry->NonPagedDebugInfo = NULL;

        if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {

            DataTableEntry->NonPagedDebugInfo = ssHeader;
            DataTableEntry->Flags |= LDRP_NON_PAGED_DEBUG_INFO;

            ssHeader->Signature = NON_PAGED_DEBUG_SIGNATURE;
            ssHeader->Flags = 1;
            ssHeader->Size = DebugInfoSize;
            ssHeader->Machine = NtHeaders->FileHeader.Machine;
            ssHeader->Characteristics = NtHeaders->FileHeader.Characteristics;
            ssHeader->TimeDateStamp = NtHeaders->FileHeader.TimeDateStamp;
            ssHeader->CheckSum = NtHeaders->OptionalHeader.CheckSum;
            ssHeader->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
            ssHeader->ImageBase = (ULONG_PTR) *ImageBaseAddress;

            if (DebugDir)
            {
                RtlCopyMemory(ssHeader + 1,
                              DebugDir,
                              DataDirectory->Size);

                DebugInfoSize = DataDirectory->Size;

                for (i = 0;
                     i < DataDirectory->Size/sizeof(IMAGE_DEBUG_DIRECTORY);
                     i += 1) {

                    if ((DebugDir + i)->PointerToRawData &&
                        (DebugDir+i)->PointerToRawData <
                            NtHeaders->OptionalHeader.SizeOfImage &&
                        ((DebugDir+i)->PointerToRawData +
                            (DebugDir+i)->SizeOfData) <
                            NtHeaders->OptionalHeader.SizeOfImage) {

                        RtlCopyMemory((PUCHAR)(ssHeader + 1) +
                                          DebugInfoSize,
                                      (PUCHAR)(*ImageBaseAddress) +
                                          (DebugDir + i)->PointerToRawData,
                                      (DebugDir + i)->SizeOfData);

                        //
                        // Reset the offset in the debug directory to point to
                        //

                        (((PIMAGE_DEBUG_DIRECTORY)(ssHeader + 1)) + i)->
                            PointerToRawData = DebugInfoSize;

                        DebugInfoSize += (DebugDir+i)->SizeOfData;
                    }
                    else
                    {
                        (((PIMAGE_DEBUG_DIRECTORY)(ssHeader + 1)) + i)->
                            PointerToRawData = 0;
                    }
                }
            }
        }

        //
        // Initialize the address of the DLL image file header and the entry
        // point address.
        //

        DataTableEntry->DllBase = *ImageBaseAddress;
        DataTableEntry->EntryPoint =
            ((PCHAR)*ImageBaseAddress + NtHeaders->OptionalHeader.AddressOfEntryPoint);
        DataTableEntry->SizeOfImage = NumberOfPtes << PAGE_SHIFT;
        DataTableEntry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
        DataTableEntry->SectionPointer = (PVOID)SectionPointer;

        //
        // Store the DLL name.
        //

        DataTableEntry->BaseDllName.Buffer = BaseDllNameBuffer;

        DataTableEntry->BaseDllName.Length = BaseName.Length;
        DataTableEntry->BaseDllName.MaximumLength = BaseName.Length;
        RtlCopyMemory (DataTableEntry->BaseDllName.Buffer,
                       BaseName.Buffer,
                       BaseName.Length );
        DataTableEntry->BaseDllName.Buffer[BaseName.Length/sizeof(WCHAR)] = UNICODE_NULL;

        DataTableEntry->FullDllName.Buffer = ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                         PrefixedImageName.Length + sizeof(UNICODE_NULL),
                                                         'TDmM');

        if (DataTableEntry->FullDllName.Buffer == NULL) {

            //
            // Pool could not be allocated, just set the length to 0.
            //

            DataTableEntry->FullDllName.Length = 0;
            DataTableEntry->FullDllName.MaximumLength = 0;
        }
        else {
            DataTableEntry->FullDllName.Length = PrefixedImageName.Length;
            DataTableEntry->FullDllName.MaximumLength = PrefixedImageName.Length;
            RtlCopyMemory (DataTableEntry->FullDllName.Buffer,
                           PrefixedImageName.Buffer,
                           PrefixedImageName.Length);
            DataTableEntry->FullDllName.Buffer[PrefixedImageName.Length/sizeof(WCHAR)] = UNICODE_NULL;
        }

        //
        // Acquire the loaded module list resource and insert this entry
        // into the list.
        //

        MiProcessLoaderEntry (DataTableEntry, TRUE);
    }

    MissingProcedureName = NameBuffer;

    try {

        //
        // Resolving the image references results in other DLLs being
        // loaded if they are referenced by the module that was just loaded.
        // An example is when an OEM printer or FAX driver links with
        // other general libraries.  This is not a problem for session space
        // because the general libraries do not have the global data issues
        // that win32k.sys and the video drivers do.  So we just call the
        // standard kernel reference resolver and any referenced libraries
        // get loaded into system global space.  Code in the routine
        // restricts which libraries can be referenced by a driver.
        //

        Status = MiResolveImageReferences (*ImageBaseAddress,
                                           &BaseDirectory,
                                           NamePrefix,
                                           &MissingProcedureName,
                                           &MissingDriverName,
                                           &LoadedImports);

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode ();
        KdPrint(("MM:sysload - ResolveImageReferences failed status %x\n",
                    Status));
    }

    if (!NT_SUCCESS(Status)) {
#if DBG
        if (Status == STATUS_OBJECT_NAME_NOT_FOUND) {
            ASSERT (MissingProcedureName == NULL);
        }

        if ((Status == STATUS_DRIVER_ORDINAL_NOT_FOUND) ||
            (Status == STATUS_OBJECT_NAME_NOT_FOUND) ||
            (Status == STATUS_DRIVER_ENTRYPOINT_NOT_FOUND)) {

            if ((ULONG_PTR)MissingProcedureName & ~((ULONG_PTR) (X64K-1))) {
                //
                // If not an ordinal, print string
                //
                DbgPrint("MissingProcedureName %s\n", MissingProcedureName);
            }
            else {
                DbgPrint("MissingProcedureName 0x%p\n", MissingProcedureName);
            }
        }

        if (MissingDriverName != NULL) {
            PrintableMissingDriverName = (PWSTR)((ULONG_PTR)MissingDriverName & ~0x1);
            DbgPrint("MissingDriverName %ws\n", PrintableMissingDriverName);
        }
#endif
        if (AlreadyOpen == FALSE) {
            MiProcessLoaderEntry (DataTableEntry, FALSE);
            if (DataTableEntry->FullDllName.Buffer != NULL) {
                ExFreePool (DataTableEntry->FullDllName.Buffer);
            }
            ExFreePool (DataTableEntry);
            DataTableEntry = NULL;
        }
        goto return1;
    }

    if (AlreadyOpen == FALSE) {

        PERFINFO_IMAGE_LOAD(DataTableEntry);

        //
        // Reinitialize the flags and update the loaded imports.
        //

        DataTableEntry->Flags |=  LDRP_SYSTEM_MAPPED | LDRP_ENTRY_PROCESSED;
        DataTableEntry->Flags &=  ~LDRP_LOAD_IN_PROGRESS;
        DataTableEntry->LoadedImports = LoadedImports;

        MiApplyDriverVerifier (DataTableEntry, NULL);

        MiWriteProtectSystemImage (DataTableEntry->DllBase);

        if (PsImageNotifyEnabled) {
            IMAGE_INFO ImageInfo;

            ImageInfo.Properties = 0;
            ImageInfo.ImageAddressingMode = IMAGE_ADDRESSING_MODE_32BIT;
            ImageInfo.SystemModeImage = TRUE;
            ImageInfo.ImageSize = DataTableEntry->SizeOfImage;
            ImageInfo.ImageBase = *ImageBaseAddress;
            ImageInfo.ImageSelector = 0;
            ImageInfo.ImageSectionNumber = 0;

            PsCallImageNotifyRoutines(ImageFileName, (HANDLE)NULL, &ImageInfo);
        }

        if (CacheImageSymbols (*ImageBaseAddress)) {

            //
            //  TEMP TEMP TEMP rip out when debugger converted
            //

            ANSI_STRING AnsiName;
            UNICODE_STRING UnicodeName;

            //
            //  \SystemRoot is 11 characters in length
            //
            if (PrefixedImageName.Length > (11 * sizeof (WCHAR )) &&
                !_wcsnicmp (PrefixedImageName.Buffer, (const PUSHORT)L"\\SystemRoot", 11)) {
                UnicodeName = PrefixedImageName;
                UnicodeName.Buffer += 11;
                UnicodeName.Length -= (11 * sizeof (WCHAR));
                sprintf (NameBuffer, "%ws%wZ", &SharedUserData->NtSystemRoot[2], &UnicodeName);
            }
            else {
                sprintf (NameBuffer, "%wZ", &BaseName);
            }
            RtlInitString (&AnsiName, NameBuffer);
            DbgLoadImageSymbols (&AnsiName,
                                 *ImageBaseAddress,
                                 (ULONG_PTR) -1);

            DataTableEntry->Flags |= LDRP_DEBUG_SYMBOLS_LOADED;
        }
    }

    //
    // Flush the instruction cache on all systems in the configuration.
    //

    KeSweepIcache (TRUE);
    *ImageHandle = DataTableEntry;
    Status = STATUS_SUCCESS;

    if (LoadFlags & MM_LOAD_IMAGE_IN_SESSION) {
        MI_LOG_SESSION_DATA_START (DataTableEntry);
        MmPageEntireDriver (DataTableEntry->EntryPoint);
    }
    else if ((SectionPointer == (PVOID)-1) &&
             ((LoadFlags & MM_LOAD_IMAGE_AND_LOCKDOWN) == 0)) {

        MiEnablePagingOfDriver (DataTableEntry);
    }

return1:

    if (!NT_SUCCESS(Status)) {

        if ((AlreadyOpen == FALSE) && (SectionPointer != (PVOID)-1)) {

            //
            // This is needed for failed win32k.sys loads or any session's
            // load of the first instance of a driver.
            //

            ObDereferenceObject (SectionPointer);
        }

        if (IssueUnloadOnFailure == TRUE) {

            if (DataTableEntry == NULL) {
                RtlZeroMemory (&TempDataTableEntry, sizeof(KLDR_DATA_TABLE_ENTRY));

                DataTableEntry = &TempDataTableEntry;

                DataTableEntry->DllBase = *ImageBaseAddress;
                DataTableEntry->SizeOfImage = NumberOfPtes << PAGE_SHIFT;
                DataTableEntry->LoadCount = 1;
                DataTableEntry->LoadedImports = LoadedImports;
            }
#if DBG
            else {

                //
                // If DataTableEntry is NULL, then we are unloading before one
                // got created.  Once a LDR_DATA_TABLE_ENTRY is created, the
                // load cannot fail, so if exists here, at least one other
                // session contains this image as well.
                //

                ASSERT (DataTableEntry->LoadCount > 1);
            }
#endif

            MmUnloadSystemImage ((PVOID)DataTableEntry);
        }
    }

    if (FileHandle) {
        ZwClose (FileHandle);
    }

    if (!NT_SUCCESS(Status)) {

        UNICODE_STRING ErrorStrings[4];
        ULONG UniqueErrorValue;
        ULONG StringSize;
        ULONG StringCount;
        ANSI_STRING AnsiString;
        UNICODE_STRING ProcedureName;
        UNICODE_STRING DriverName;
        ULONG i;
        PWCHAR temp;
        PWCHAR ptr;
        ULONG PacketSize;
        SIZE_T length;
        PIO_ERROR_LOG_PACKET ErrLog;

        //
        // The driver could not be loaded - log an event with the details.
        //

        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);

        StringSize = 0;

        *(&ErrorStrings[0]) = *ImageFileName;
        StringSize += (ImageFileName->Length + sizeof(UNICODE_NULL));
        StringCount = 1;

        UniqueErrorValue = 0;
        RtlInitUnicodeString (&ProcedureName, NULL);

        PrintableMissingDriverName = (PWSTR)((ULONG_PTR)MissingDriverName & ~0x1);
        if ((Status == STATUS_DRIVER_ORDINAL_NOT_FOUND) ||
            (Status == STATUS_DRIVER_ENTRYPOINT_NOT_FOUND) ||
            (Status == STATUS_OBJECT_NAME_NOT_FOUND) ||
            (Status == STATUS_PROCEDURE_NOT_FOUND)) {

            ErrorStrings[1].Buffer = L"cannot find";
            length = wcslen(ErrorStrings[1].Buffer) * sizeof(WCHAR);
            ErrorStrings[1].Length = (USHORT) length;
            StringSize += (ULONG)(length + sizeof (UNICODE_NULL));
            StringCount += 1;

            RtlInitUnicodeString (&DriverName, PrintableMissingDriverName);

            StringSize += (DriverName.Length + sizeof(UNICODE_NULL));
            StringCount += 1;
            *(&ErrorStrings[2]) = *(&DriverName);

            if ((ULONG_PTR)MissingProcedureName & ~((ULONG_PTR) (X64K-1))) {

                //
                // If not an ordinal, pass as a Unicode string
                //

                RtlInitAnsiString (&AnsiString, MissingProcedureName);
                RtlAnsiStringToUnicodeString (&ProcedureName, &AnsiString, TRUE);
                StringSize += (ProcedureName.Length + sizeof(UNICODE_NULL));
                StringCount += 1;
                *(&ErrorStrings[3]) = *(&ProcedureName);
            }
            else {

                //
                // Just pass ordinal values as is in the UniqueErrorValue.
                //

                UniqueErrorValue = PtrToUlong (MissingProcedureName);
            }
        }
        else {
            UniqueErrorValue = (ULONG) Status;
            Status = STATUS_DRIVER_UNABLE_TO_LOAD;

            ErrorStrings[1].Buffer = L"failed to load";
            length = wcslen(ErrorStrings[1].Buffer) * sizeof(WCHAR);
            ErrorStrings[1].Length = (USHORT) length;
            StringSize += (ULONG)(length + sizeof (UNICODE_NULL));
            StringCount += 1;
        }

        PacketSize = sizeof (IO_ERROR_LOG_PACKET) + StringSize;

        //
        // Enforce I/O manager interface (ie: UCHAR) size restrictions.
        //

        if (PacketSize < MAXUCHAR) {

            ErrLog = IoAllocateGenericErrorLogEntry ((UCHAR)PacketSize);

            if (ErrLog != NULL) {

                //
                // Fill it in and write it out as a single string.
                //

                ErrLog->ErrorCode = STATUS_LOG_HARD_ERROR;
                ErrLog->FinalStatus = Status;
                ErrLog->UniqueErrorValue = UniqueErrorValue;

                ErrLog->StringOffset = (USHORT) sizeof (IO_ERROR_LOG_PACKET);

                temp = (PWCHAR) ((PUCHAR) ErrLog + ErrLog->StringOffset);

                for (i = 0; i < StringCount; i += 1) {

                    ptr = ErrorStrings[i].Buffer;

                    RtlCopyMemory (temp, ptr, ErrorStrings[i].Length);
                    temp += (ErrorStrings[i].Length / sizeof (WCHAR));

                    *temp = L' ';
                    temp += 1;
                }

                *(temp - 1) = UNICODE_NULL;
                ErrLog->NumberOfStrings = 1;

                IoWriteErrorLogEntry (ErrLog);
            }
        }

        //
        // The only way this pointer has the low bit set is if we are expected
        // to free the pool containing the name.  Typically the name points at
        // a loaded module list entry and so no one has to free it and in this
        // case the low bit will NOT be set.  If the module could not be found
        // and was therefore not loaded, then we left a piece of pool around
        // containing the name since there is no loaded module entry already -
        // this must be released now.
        //

        if ((ULONG_PTR)MissingDriverName & 0x1) {
            ExFreePool (PrintableMissingDriverName);
        }

        if (ProcedureName.Buffer != NULL) {
            RtlFreeUnicodeString (&ProcedureName);
        }
        ExFreePool (NameBuffer);
        return Status;
    }

return2:

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    if (NamePrefix) {
        ExFreePool (PrefixedImageName.Buffer);
    }

    ExFreePool (NameBuffer);

    return Status;
}

VOID
MiReturnFailedSessionPages (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    This routine is a nonpaged wrapper which undoes session image loads
    that failed midway through reading in the pages.

Arguments:

    PointerPte - Supplies the starting PTE for the range to unload.

    LastPte - Supplies the ending PTE for the range to unload.

    NumberOfPages - Supplies the number of resident available pages that
                    were charged and now need to be returned.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {
        if (PointerPte->u.Hard.Valid == 1) {

            //
            // Delete the page.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

            //
            // Set the pointer to PTE as empty so the page
            // is deleted when the reference count goes to zero.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            MiDecrementShareAndValidCount (Pfn1->u4.PteFrame);
            MI_SET_PFN_DELETED (Pfn1);
            MiDecrementShareCountOnly (PageFrameIndex);

            MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
        }
        PointerPte += 1;
    }

    MmResidentAvailablePages += NumberOfPages;
    MM_BUMP_COUNTER(17, NumberOfPages);

    UNLOCK_PFN (OldIrql);
}


NTSTATUS
MiLoadImageSection (
    IN PSECTION SectionPointer,
    OUT PVOID *ImageBaseAddress,
    IN PUNICODE_STRING ImageFileName,
    IN ULONG LoadInSessionSpace
    )

/*++

Routine Description:

    This routine loads the specified image into the kernel part of the
    address space.

Arguments:

    SectionPointer - Supplies the section object for the image.

    ImageBaseAddress - Returns the address that the image header is at.

    ImageFileName - Supplies the full path name (including the image name)
                    of the image to load.

    LoadInSessionSpace - Supplies nonzero to load this image in session space.
                         Each session gets a different copy of this driver with
                         pages shared as much as possible via copy on write.

                         Supplies zero if this image should be loaded in global
                         space.

Return Value:

    Status of the operation.

--*/

{
    KAPC_STATE ApcState;
    PFN_NUMBER PagesRequired;
    PFN_NUMBER ActualPagesUsed;
    PVOID OpaqueSession;
    PMMPTE ProtoPte;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PEPROCESS Process;
    PEPROCESS TargetProcess;
    ULONG NumberOfPtes;
    MMPTE PteContents;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PVOID UserVa;
    PVOID SystemVa;
    NTSTATUS Status;
    NTSTATUS ExceptionStatus;
    PVOID Base;
    ULONG_PTR ViewSize;
    LARGE_INTEGER SectionOffset;
    LOGICAL LoadSymbols;
    PVOID BaseAddress;
    PFN_NUMBER CommittedPages;
    SIZE_T SectionSize;
    LOGICAL AlreadyLoaded;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;

    PAGED_CODE();

    NumberOfPtes = SectionPointer->Segment->TotalNumberOfPtes;

    if (LoadInSessionSpace != 0) {

        SectionSize = (ULONG_PTR)NumberOfPtes * PAGE_SIZE;

        //
        // Allocate a unique systemwide session space virtual address for
        // the driver.
        //

        Status = MiSessionWideReserveImageAddress (ImageFileName,
                                                   SectionPointer,
                                                   PAGE_SIZE,
                                                   &BaseAddress,
                                                   &AlreadyLoaded);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // This is a request to load an existing driver.  This can
        // occur with printer drivers for example.
        //

        if (AlreadyLoaded == TRUE) {
            *ImageBaseAddress = BaseAddress;
            return STATUS_ALREADY_COMMITTED;
        }

#if DBG
        if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
            DbgPrint ("MM: MiLoadImageSection: Image %wZ, BasedAddress 0x%p, Allocated Session BaseAddress 0x%p\n",
                ImageFileName,
                SectionPointer->Segment->BasedAddress,
                BaseAddress);
        }
#endif

        if (BaseAddress == SectionPointer->Segment->BasedAddress) {

            //
            // We were able to load the image at its based address, so
            // map its image segments as backed directly by the file image.
            // All pristine pages of the image will be shared across all
            // sessions, with each page treated as copy-on-write on first write.
            //
            // NOTE: This makes the file image "busy", a different behavior
            // as normal kernel drivers are backed by the paging file only.
            //
            // Map the image into session space.
            //

            Status = MiShareSessionImage (SectionPointer, &SectionSize);

            if (!NT_SUCCESS(Status)) {
                MiRemoveImageSessionWide (BaseAddress);
                return Status;
            }

            ASSERT (BaseAddress == SectionPointer->Segment->BasedAddress);

            *ImageBaseAddress = BaseAddress;

            //
            // Indicate that this section has been loaded into the system.
            //

            SectionPointer->Segment->SystemImageBase = BaseAddress;

#if DBG
            if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
                DbgPrint ("MM: MiLoadImageSection: Mapped image %wZ at requested session address 0x%p\n",
                    ImageFileName,
                    BaseAddress);
            }
#endif

            return Status;
        }

        //
        // The image could not be loaded at its based address.  It must be
        // copied to its new address using private page file backed pages.
        // Our caller will relocate the internal references and then bind the
        // image.  Allocate the pages and page tables for the image now.
        //

        Status = MiSessionCommitImagePages (BaseAddress, SectionSize);

        if (!NT_SUCCESS(Status)) {
#if DBG
            if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
                DbgPrint ("MM: MiLoadImageSection: Error 0x%x Allocating session space %p Bytes\n", Status, SectionSize);
            }
#endif
            MiRemoveImageSessionWide (BaseAddress);
            return Status;
        }
        SystemVa = BaseAddress;

        //
        // Initializing these is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        PagesRequired = 0;
        ActualPagesUsed = 0;
        PointerPte = NULL;
        FirstPte = NULL;
    }
    else {

        //
        // Initializing SectionSize and BaseAddress is not needed for
        // correctness, but without it the compiler cannot compile this
        // code W4 to check for use of uninitialized variables.
        //

        SectionSize = 0;
        BaseAddress = NULL;

        //
        // Calculate the number of pages required to load this image.
        //
        // Start out by charging for everything and subtract out any gap
        // pages after the image loads successfully.
        //

        PagesRequired = NumberOfPtes;
        ActualPagesUsed = 0;

        //
        // See if ample pages exist to load this image.
        //

        if (MiChargeResidentAvailable (PagesRequired, 14) == FALSE) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        //
        // Reserve the necessary system address space.
        //

        FirstPte = MiReserveSystemPtes (NumberOfPtes, SystemPteSpace);

        if (FirstPte == NULL) {
            MiReturnResidentAvailable (PagesRequired, 15);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        PointerPte = FirstPte;
        SystemVa = MiGetVirtualAddressMappedByPte (PointerPte);

        if (MiChargeCommitment (PagesRequired, NULL) == FALSE) {
            MiReturnResidentAvailable (PagesRequired, 15);
            MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_DRIVER_PAGES, PagesRequired);

        InterlockedExchangeAdd ((PLONG)&MmDriverCommit, (LONG) PagesRequired);
    }

    //
    // Map a view into the user portion of the address space.
    //

    Process = PsGetCurrentProcess();

    //
    // Since callees are not always in the context of the system process,
    // attach here when necessary to guarantee the driver load occurs in a
    // known safe address space to prevent security holes.
    //

    OpaqueSession = NULL;

    if ((Process->Peb != NULL) &&
        (Process->Vm.Flags.SessionLeader == 0) &&
        (MiSessionLeaderExists == 2)) {

        OpaqueSession = MiAttachToSecureProcessInSession (&ApcState);

        if (OpaqueSession == NULL) {

            if (LoadInSessionSpace != 0) {
                CommittedPages = MiDeleteSystemPagableVm (
                                      MiGetPteAddress (BaseAddress),
                                      BYTES_TO_PAGES (SectionSize),
                                      ZeroKernelPte,
                                      TRUE,
                                      NULL);

                InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                             0 - CommittedPages);

                MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED1,
                    (ULONG)CommittedPages);

                //
                // Return the commitment we took out on the pagefile when
                // the page was allocated.  This is needed for collided images
                // since all the pages get committed regardless of writability.
                //

                MiRemoveImageSessionWide (BaseAddress);
            }
            else {
                MiReturnResidentAvailable (PagesRequired, 15);
                MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);
                MiReturnCommitment (PagesRequired);
            }

            return STATUS_PROCESS_IS_TERMINATING;
        }

        //
        // We are now attached to a secure process in the current session.
        //
    }

    ZERO_LARGE (SectionOffset);
    Base = NULL;
    ViewSize = 0;

    if (NtGlobalFlag & FLG_ENABLE_KDEBUG_SYMBOL_LOAD) {
        LoadSymbols = TRUE;
        NtGlobalFlag &= ~FLG_ENABLE_KDEBUG_SYMBOL_LOAD;
    }
    else {
        LoadSymbols = FALSE;
    }

    TargetProcess = PsGetCurrentProcess ();

    Status = MmMapViewOfSection (SectionPointer,
                                 TargetProcess,
                                 &Base,
                                 0,
                                 0,
                                 &SectionOffset,
                                 &ViewSize,
                                 ViewUnmap,
                                 0,
                                 PAGE_EXECUTE);

    if (LoadSymbols) {
        NtGlobalFlag |= FLG_ENABLE_KDEBUG_SYMBOL_LOAD;
    }

    if (Status == STATUS_IMAGE_MACHINE_TYPE_MISMATCH) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
    }

    if (!NT_SUCCESS(Status)) {

        if (OpaqueSession != NULL) {
            MiDetachFromSecureProcessInSession (OpaqueSession, &ApcState);
        }

        if (LoadInSessionSpace != 0) {

#if DBG
            if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
                DbgPrint ("MiLoadImageSection: Error 0x%x in session space mapping via MmMapViewOfSection\n", Status);
            }
#endif

            CommittedPages = MiDeleteSystemPagableVm (
                                      MiGetPteAddress (BaseAddress),
                                      BYTES_TO_PAGES (SectionSize),
                                      ZeroKernelPte,
                                      TRUE,
                                      NULL);

            InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                         0 - CommittedPages);

            MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED1,
                (ULONG)CommittedPages);

            //
            // Return the commitment we took out on the pagefile when
            // the page was allocated.  This is needed for collided images
            // since all the pages get committed regardless of writability.
            //

            MiRemoveImageSessionWide (BaseAddress);
        }
        else {
            MiReturnResidentAvailable (PagesRequired, 16);
            MiReleaseSystemPtes (FirstPte, NumberOfPtes, SystemPteSpace);
            MiReturnCommitment (PagesRequired);
        }

        return Status;
    }

    //
    // Allocate a physical page(s) and copy the image data.
    // Note Hydra has already allocated the physical pages and just does
    // data copying here.
    //

    ControlArea = SectionPointer->Segment->ControlArea;
    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }
    ASSERT (Subsection->SubsectionBase != NULL);
    ProtoPte = Subsection->SubsectionBase;

    *ImageBaseAddress = SystemVa;

    UserVa = Base;
    TempPte = ValidKernelPte;
    TempPte.u.Long |= MM_PTE_EXECUTE;

    LastPte = ProtoPte + NumberOfPtes;

    ExceptionStatus = STATUS_SUCCESS;

    while (ProtoPte < LastPte) {
        PteContents = *ProtoPte;
        if ((PteContents.u.Hard.Valid == 1) ||
            (PteContents.u.Soft.Protection != MM_NOACCESS)) {

            if (LoadInSessionSpace == 0) {
                ActualPagesUsed += 1;

                PageFrameIndex = MiAllocatePfn (PointerPte, MM_EXECUTE);

                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                ASSERT (MI_PFN_ELEMENT (PageFrameIndex)->u1.WsIndex == 0);
            }

            try {

                RtlCopyMemory (SystemVa, UserVa, PAGE_SIZE);

            } except (MiMapCacheExceptionFilter (&ExceptionStatus,
                                                 GetExceptionInformation())) {

                //
                // An exception occurred, unmap the view and
                // return the error to the caller.
                //

#if DBG
                DbgPrint("MiLoadImageSection: Exception 0x%x copying driver SystemVa 0x%p, UserVa 0x%p\n",ExceptionStatus,SystemVa,UserVa);
#endif

                if (LoadInSessionSpace != 0) {
                    CommittedPages = MiDeleteSystemPagableVm (
                                              MiGetPteAddress (BaseAddress),
                                              BYTES_TO_PAGES (SectionSize),
                                              ZeroKernelPte,
                                              TRUE,
                                              NULL);

                    InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                                 0 - CommittedPages);

                    MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED2,
                        (ULONG)CommittedPages);

                    //
                    // Return the commitment we took out on the pagefile when
                    // the page was allocated.  This is needed for collided
                    // images since all the pages get committed regardless
                    // of writability.
                    //

                    MiReturnCommitment (CommittedPages);
                    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_SESSION_DRIVER_LOAD_FAILURE1, CommittedPages);
                }
                else {
                    MiReturnFailedSessionPages (FirstPte, PointerPte, PagesRequired);
                    MiReleaseSystemPtes (FirstPte,
                                         NumberOfPtes,
                                         SystemPteSpace);
                    MiReturnCommitment (PagesRequired);
                }

                Status = MiUnmapViewOfSection (TargetProcess, Base, FALSE);

                ASSERT (NT_SUCCESS (Status));

                //
                // Purge the section as we want these pages on the freelist
                // instead of at the tail of standby, as we're completely
                // done with the section.  This is because other valuable
                // standby pages end up getting reused (especially during
                // bootup) when the section pages are the ones that really
                // will never be referenced again.
                //
                // Note this isn't done for session images as they're
                // inpaged directly from the filesystem via the section.
                //

                if (LoadInSessionSpace == 0) {
                    MmPurgeSection (ControlArea->FilePointer->SectionObjectPointer,
                                    NULL,
                                    0,
                                    FALSE);
                }

                if (OpaqueSession != NULL) {
                    MiDetachFromSecureProcessInSession (OpaqueSession,
                                                        &ApcState);
                }

                if (LoadInSessionSpace != 0) {
                    MiRemoveImageSessionWide (BaseAddress);
                }

                return ExceptionStatus;
            }

        }
        else {

            //
            // The PTE is no access - if this driver is being loaded in session
            // space we already preloaded the page so free it now.  The
            // commitment is returned when the whole image is unmapped.
            //

            if (LoadInSessionSpace != 0) {
                CommittedPages = MiDeleteSystemPagableVm (
                                          MiGetPteAddress (SystemVa),
                                          1,
                                          ZeroKernelPte,
                                          TRUE,
                                          NULL);

                MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGELOAD_NOACCESS,
                    1);
            }
            else {
                MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);
            }
        }

        ProtoPte += 1;
        if (LoadInSessionSpace == 0) {
            PointerPte += 1;
        }
        SystemVa = ((PCHAR)SystemVa + PAGE_SIZE);
        UserVa = ((PCHAR)UserVa + PAGE_SIZE);
    }

    Status = MiUnmapViewOfSection (TargetProcess, Base, FALSE);
    ASSERT (NT_SUCCESS (Status));

    //
    // Purge the section as we want these pages on the freelist instead of
    // at the tail of standby, as we're completely done with the section.
    // This is because other valuable standby pages end up getting reused
    // (especially during bootup) when the section pages are the ones that
    // really will never be referenced again.
    //
    // Note this isn't done for session images as they're inpaged directly
    // from the filesystem via the section.
    //

    if (LoadInSessionSpace == 0) {
        MmPurgeSection (ControlArea->FilePointer->SectionObjectPointer,
                        NULL,
                        0,
                        FALSE);
    }

    if (OpaqueSession != NULL) {
        MiDetachFromSecureProcessInSession (OpaqueSession, &ApcState);
    }

    //
    // Indicate that this section has been loaded into the system.
    //

    SectionPointer->Segment->SystemImageBase = *ImageBaseAddress;

    if (LoadInSessionSpace == 0) {

        //
        // Return any excess resident available and commit.
        //

        if (PagesRequired != ActualPagesUsed) {
            ASSERT (PagesRequired > ActualPagesUsed);
            PagesRequired -= ActualPagesUsed;

            MiReturnResidentAvailable (PagesRequired, 13);
            MiReturnCommitment (PagesRequired);
        }
    }

    return Status;
}

VOID
MmFreeDriverInitialization (
    IN PVOID ImageHandle
    )

/*++

Routine Description:

    This routine removes the pages that relocate and debug information from
    the address space of the driver.

    NOTE:  This routine looks at the last sections defined in the image
           header and if that section is marked as DISCARDABLE in the
           characteristics, it is removed from the image.  This means
           that all discardable sections at the end of the driver are
           deleted.

Arguments:

    SectionObject - Supplies the section object for the image.

Return Value:

    None.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER NumberOfPtes;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    PIMAGE_SECTION_HEADER FoundSection;
    PFN_NUMBER PagesDeleted;

    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)ImageHandle;
    Base = DataTableEntry->DllBase;

    ASSERT (MI_IS_SESSION_ADDRESS (Base) == FALSE);

    NumberOfPtes = DataTableEntry->SizeOfImage >> PAGE_SHIFT;
    LastPte = MiGetPteAddress (Base) + NumberOfPtes;

    NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(Base);

    NtSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    NtSection += NtHeaders->FileHeader.NumberOfSections;

    FoundSection = NULL;
    for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {
        NtSection -= 1;
        if ((NtSection->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0) {
            FoundSection = NtSection;
        }
        else {

            //
            // There was a non discardable section between the this
            // section and the last non discardable section, don't
            // discard this section and don't look any more.
            //

            break;
        }
    }

    if (FoundSection != NULL) {

        PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                    (PCHAR)Base + FoundSection->VirtualAddress)));
        NumberOfPtes = (PFN_NUMBER)(LastPte - PointerPte);

        PagesDeleted = MiDeleteSystemPagableVm (PointerPte,
                                                NumberOfPtes,
                                                ZeroKernelPte,
                                                FALSE,
                                                NULL);

        MiReturnResidentAvailable (PagesDeleted, 18);

        MiReturnCommitment (PagesDeleted);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_INIT_CODE, PagesDeleted);

        InterlockedExchangeAdd ((PLONG)&MmDriverCommit,
                                (LONG) (0 - PagesDeleted));
    }

    return;
}

LOGICAL
MiChargeResidentAvailable (
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Id
    )

/*++

Routine Description:

    This routine is a nonpaged wrapper to charge resident available pages.

Arguments:

    NumberOfPages - Supplies the number of pages to charge.

    Id - Supplies a tracking ID for debugging purposes.

Return Value:

    TRUE if the pages were charged, FALSE if not.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)NumberOfPages) {
        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

    MmResidentAvailablePages -= NumberOfPages;
    MM_BUMP_COUNTER(Id, NumberOfPages);

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MiReturnResidentAvailable (
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Id
    )

/*++

Routine Description:

    This routine is a nonpaged wrapper to return resident available pages.

Arguments:

    NumberOfPages - Supplies the number of pages to return.

    Id - Supplies a tracking ID for debugging purposes.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    MmResidentAvailablePages += NumberOfPages;
    MM_BUMP_COUNTER(Id, NumberOfPages);
    UNLOCK_PFN (OldIrql);
}

VOID
MiEnablePagingOfDriver (
    IN PVOID ImageHandle
    )

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER FoundSection;
    PIMAGE_OPTIONAL_HEADER OptionalHeader;

    //
    // Don't page kernel mode code if customer does not want it paged.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

    //
    // If the driver has pagable code, make it paged.
    //

    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)ImageHandle;
    Base = DataTableEntry->DllBase;

    NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(Base);

    OptionalHeader = (PIMAGE_OPTIONAL_HEADER)((PCHAR)NtHeaders +
#if defined (_WIN64)
                        FIELD_OFFSET (IMAGE_NT_HEADERS64, OptionalHeader));
#else
                        FIELD_OFFSET (IMAGE_NT_HEADERS32, OptionalHeader));
#endif

    FoundSection = IMAGE_FIRST_SECTION (NtHeaders);

    i = NtHeaders->FileHeader.NumberOfSections;

    PointerPte = NULL;

    //
    // Initializing LastPte is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    LastPte = NULL;

    while (i > 0) {
#if DBG
            if ((*(PULONG)FoundSection->Name == 'tini') ||
                (*(PULONG)FoundSection->Name == 'egap')) {
                DbgPrint("driver %wZ has lower case sections (init or pagexxx)\n",
                    &DataTableEntry->FullDllName);
            }
#endif //DBG

        //
        // Mark as pagable any section which starts with the
        // first 4 characters PAGE or .eda (for the .edata section).
        //

        if ((*(PULONG)FoundSection->Name == 'EGAP') ||
           (*(PULONG)FoundSection->Name == 'ade.')) {

            //
            // This section is pagable, save away the start and end.
            //

            if (PointerPte == NULL) {

                //
                // Previous section was NOT pagable, get the start address.
                //

                PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                   (PCHAR)Base + FoundSection->VirtualAddress)));
            }
            LastPte = MiGetPteAddress ((PCHAR)Base +
                                       FoundSection->VirtualAddress +
                                       (OptionalHeader->SectionAlignment - 1) +
                                       FoundSection->SizeOfRawData - PAGE_SIZE);

        }
        else {

            //
            // This section is not pagable, if the previous section was
            // pagable, enable it.
            //

            if (PointerPte != NULL) {
                MiSetPagingOfDriver (PointerPte, LastPte, FALSE);
                PointerPte = NULL;
            }
        }
        i -= 1;
        FoundSection += 1;
    }
    if (PointerPte != NULL) {
        MiSetPagingOfDriver (PointerPte, LastPte, FALSE);
    }
}


VOID
MiSetPagingOfDriver (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL SessionSpace
    )

/*++

Routine Description:

    This routine marks the specified range of PTEs as pagable.

Arguments:

    PointerPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

    SessionSpace - Supplies TRUE if this mapping is in session space,
                   FALSE if not.

Return Value:

    None.

Environment:

    Kernel Mode, IRQL of APC_LEVEL or below.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    PVOID Base;
    PFN_NUMBER PageCount;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    MMPTE TempPte;
    MMPTE PreviousPte;
    KIRQL OldIrql1;
    KIRQL OldIrql;

    PAGED_CODE ();

    ASSERT ((SessionSpace == FALSE) ||
            (MmIsAddressValid(MmSessionSpace) == TRUE));

    if (MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(PointerPte))) {

        //
        // No need to lock physical addresses.
        //

        return;
    }

    PageCount = 0;

    if (SessionSpace == TRUE) {
        LOCK_SESSION_SPACE_WS (OldIrql1, PsGetCurrentThread ());
    }
    else {
        LOCK_SYSTEM_WS (OldIrql1, PsGetCurrentThread ());
    }

    LOCK_PFN (OldIrql);

    Base = MiGetVirtualAddressMappedByPte (PointerPte);

    while (PointerPte <= LastPte) {

        //
        // Check to make sure this PTE has not already been
        // made pagable (or deleted).  It is pagable if it
        // is not valid, or if the PFN database wsindex element
        // is non zero.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn->u2.ShareCount == 1);

            if (Pfn->u1.WsIndex == 0) {

                //
                // Original PTE may need to be set for drivers loaded
                // via ntldr.
                //

                if (Pfn->OriginalPte.u.Long == 0) {
                    Pfn->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
                    Pfn->OriginalPte.u.Soft.Protection |= MM_EXECUTE;
                }

                TempPte = *PointerPte;

                MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                           Pfn->OriginalPte.u.Soft.Protection);

                PreviousPte.u.Flush = KeFlushSingleTb (Base,
                                                       TRUE,
                                                       TRUE,
                                                       (PHARDWARE_PTE)PointerPte,
                                                       TempPte.u.Flush);

                MI_CAPTURE_DIRTY_BIT_TO_PFN (&PreviousPte, Pfn);

                //
                // Flush the translation buffer and decrement the number of valid
                // PTEs within the containing page table page.  Note that for a
                // private page, the page table page is still needed because the
                // page is in transition.
                //

                MiDecrementShareCount (PageFrameIndex);
                MmResidentAvailablePages += 1;
                MM_BUMP_COUNTER(19, 1);
                MmTotalSystemDriverPages += 1;
                PageCount += 1;
            }
            else {
                //
                // This page is already pagable and has a WSLE entry.
                // Ignore it here and let the trimmer take it if memory
                // comes under pressure.
                //
            }
        }
        Base = (PVOID)((PCHAR)Base + PAGE_SIZE);
        PointerPte += 1;
    }

    if (SessionSpace == TRUE) {

        //
        // Session space has no ASN - flush the entire TB.
        //

        MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
    }

    UNLOCK_PFN (OldIrql);

    if (SessionSpace == TRUE) {

        //
        // These pages are no longer locked down.
        //

        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_PAGE_DRIVER, (ULONG)PageCount);
        MmSessionSpace->NonPagablePages -= PageCount;
        MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_DRIVER_PAGES_UNLOCKED, (ULONG)PageCount);

        UNLOCK_SESSION_SPACE_WS (OldIrql1);
    }
    else {
        UNLOCK_SYSTEM_WS (OldIrql1);
    }
}


PVOID
MmPageEntireDriver (
    IN PVOID AddressWithinSection
    )

/*++

Routine Description:

    This routine allows a driver to page out all of its code and
    data regardless of the attributes of the various image sections.

    Note, this routine can be called multiple times with no
    intervening calls to MmResetDriverPaging.

Arguments:

    AddressWithinSection - Supplies an address within the driver, e.g.
                           DriverEntry.

Return Value:

    Base address of driver.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PVOID BaseAddress;
    PSECTION SectionPointer;
    LOGICAL SessionSpace;

    PAGED_CODE();

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, FALSE);

    if (DataTableEntry == NULL) {
        return NULL;
    }

    //
    // Don't page kernel mode code if disabled via registry.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        BaseAddress = DataTableEntry->DllBase;
        goto success;
    }

    SectionPointer = (PSECTION)DataTableEntry->SectionPointer;

    SessionSpace = MI_IS_SESSION_IMAGE_ADDRESS (AddressWithinSection);

    if ((SectionPointer != NULL) && (SectionPointer != (PVOID)-1)) {

        //
        // Driver is mapped as an image (ie: win32k), this is always pagable.
        // For session space, an image that has been loaded at its desired
        // address is also always pagable.  If there was an address collision,
        // then we fall through because we have to explicitly page it.
        //

        if (SessionSpace == TRUE) {
            if (SectionPointer->Segment &&
                SectionPointer->Segment->BasedAddress == SectionPointer->Segment->SystemImageBase) {
                BaseAddress = DataTableEntry->DllBase;
                goto success;
            }
        }
        else {
            BaseAddress = DataTableEntry->DllBase;
            goto success;
        }
    }

    BaseAddress = DataTableEntry->DllBase;
    FirstPte = MiGetPteAddress (BaseAddress);
    LastPte = (FirstPte - 1) + (DataTableEntry->SizeOfImage >> PAGE_SHIFT);

    MiSetPagingOfDriver (FirstPte, LastPte, SessionSpace);

success:

#if 0

    //
    // .rdata and .pdata must be made resident for the kernel debugger to
    // display stack frames properly.  This means these sections must not
    // only be marked nonpagable, but for drivers like win32k.sys (and
    // session space drivers that are not relocated) that are paged in and
    // out directly from the filesystem, these pages must be made explicitly
    // resident now.
    //
    // If the debugger can be pitched, then there is no need to lock down
    // these subsections as no one will be debugging this system anyway.
    //

    if (KdPitchDebugger == 0) {
        MiLockDriverPdata (DataTableEntry);
    }

#endif

    return BaseAddress;
}


VOID
MmResetDriverPaging (
    IN PVOID AddressWithinSection
    )

/*++

Routine Description:

    This routines resets the driver paging to what the image specified.
    Hence image sections such as the IAT, .text, .data will be locked
    down in memory.

    Note, there is no requirement that MmPageEntireDriver was called.

Arguments:

    AddressWithinSection - Supplies an address within the driver, e.g.
                           DriverEntry.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER FoundSection;

    PAGED_CODE();

    //
    // Don't page kernel mode code if disabled via registry.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

    if (MI_IS_PHYSICAL_ADDRESS(AddressWithinSection)) {
        return;
    }

    //
    // If the driver has pagable code, make it paged.
    //

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, FALSE);

    if ((DataTableEntry->SectionPointer != NULL) &&
        (DataTableEntry->SectionPointer != (PVOID)-1)) {

        //
        // Driver is mapped by image hence already paged.
        //

        return;
    }

    Base = DataTableEntry->DllBase;

    NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(Base);

    FoundSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    i = NtHeaders->FileHeader.NumberOfSections;
    PointerPte = NULL;

    while (i > 0) {
#if DBG
            if ((*(PULONG)FoundSection->Name == 'tini') ||
                (*(PULONG)FoundSection->Name == 'egap')) {
                DbgPrint("driver %wZ has lower case sections (init or pagexxx)\n",
                    &DataTableEntry->FullDllName);
            }
#endif

        //
        // Don't lock down code for sections marked as discardable or
        // sections marked with the first 4 characters PAGE or .eda
        // (for the .edata section) or INIT.
        //

        if (((FoundSection->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0) ||
           (*(PULONG)FoundSection->Name == 'EGAP') ||
           (*(PULONG)FoundSection->Name == 'ade.') ||
           (*(PULONG)FoundSection->Name == 'TINI')) {

            NOTHING;

        }
        else {

            //
            // This section is nonpagable.
            //

            PointerPte = MiGetPteAddress (
                                   (PCHAR)Base + FoundSection->VirtualAddress);
            LastPte = MiGetPteAddress ((PCHAR)Base +
                                       FoundSection->VirtualAddress +
                                      (FoundSection->SizeOfRawData - 1));
            ASSERT (PointerPte <= LastPte);

            MiLockCode (PointerPte, LastPte, MM_LOCK_BY_NONPAGE);
        }
        i -= 1;
        FoundSection += 1;
    }
    return;
}

#if 0

VOID
MiLockDriverPdata (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    This routines locks down the .pdata & .rdata subsections within the driver.

Arguments:

    DataTableEntry - Supplies the argument module's data table entry.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSECTION SectionPointer;
    LOGICAL SessionSpace;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PVOID Base;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER FoundSection;
    PETHREAD Thread;
    
    PAGED_CODE();

    //
    // If the debugger can be pitched, then there is no need to lock down
    // these subsections as no one will be debugging this system anyway.
    //

    ASSERT (KdPitchDebugger == 0);

    if (MI_IS_PHYSICAL_ADDRESS(DataTableEntry->DllBase)) {
        return;
    }

    SectionPointer = (PSECTION)DataTableEntry->SectionPointer;

    //
    // Drivers backed by the paging file require no handling as the .pdata
    // .rdata sections are not paged by default and have already been inpaged.
    //

    if ((SectionPointer == NULL) || (SectionPointer == (PVOID)-1)) {
        return;
    }

    //
    // The driver is mapped as an image (ie: win32k) which is always pagable.
    // In fact, it may not even be resident right now because in addition
    // to being always pagable, it is backed directly by the filesystem,
    // not the pagefile.  So it must be inpaged now.
    //

    SessionSpace = MI_IS_SESSION_IMAGE_ADDRESS (DataTableEntry->DllBase);

    Base = DataTableEntry->DllBase;

    NtHeaders = (PIMAGE_NT_HEADERS)RtlImageNtHeader(Base);

    FoundSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader);

    i = NtHeaders->FileHeader.NumberOfSections;
    PointerPte = NULL;
    Thread = PsGetCurrentThread ();

    for ( ; i > 0; i -= 1, FoundSection += 1) {

        //
        // Don't lock down discardable sections.
        //

        if ((FoundSection->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0) {
            NOTHING;
        }

        //
        // Inpage and lock down subsections marked as .pdata & .rdata.
        //

        if (((*(PULONG)FoundSection->Name == 'adp.') &&
            (FoundSection->Name[4] == 't') &&
            (FoundSection->Name[5] == 'a')) ||

            ((*(PULONG)FoundSection->Name == 'adr.') &&
            (FoundSection->Name[4] == 't') &&
            (FoundSection->Name[5] == 'a'))) {

            PointerPte = MiGetPteAddress (
                                   (PCHAR)Base + FoundSection->VirtualAddress);
            LastPte = MiGetPteAddress ((PCHAR)Base +
                                       FoundSection->VirtualAddress +
                                      (FoundSection->SizeOfRawData - 1));
            ASSERT (PointerPte <= LastPte);
        }
    }
    return;
}
#endif


VOID
MiClearImports(
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )
/*++

Routine Description:

    Free up the import list and clear the pointer.  This stops the
    recursion performed in MiDereferenceImports().

Arguments:

    DataTableEntry - provided for the driver.

Return Value:

    Status of the import list construction operation.

--*/

{
    PAGED_CODE();

    if (DataTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {
        return;
    }

    if (DataTableEntry->LoadedImports == (PVOID)NO_IMPORTS_USED) {
        NOTHING;
    }
    else if (SINGLE_ENTRY(DataTableEntry->LoadedImports)) {
        NOTHING;
    }
    else {
        //
        // free the memory
        //
        ExFreePool ((PVOID)DataTableEntry->LoadedImports);
    }

    //
    // stop the recursion
    //
    DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
}

VOID
MiRememberUnloadedDriver (
    IN PUNICODE_STRING DriverName,
    IN PVOID Address,
    IN ULONG Length
    )

/*++

Routine Description:

    This routine saves information about unloaded drivers so that ones that
    forget to delete lookaside lists or queues can be caught.

Arguments:

    DriverName - Supplies a Unicode string containing the driver's name.

    Address - Supplies the address the driver was loaded at.

    Length - Supplies the number of bytes the driver load spanned.

Return Value:

    None.

--*/

{
    PUNLOADED_DRIVERS Entry;
    ULONG NumberOfBytes;

    if (DriverName->Length == 0) {

        //
        // This is an aborted load and the driver name hasn't been filled
        // in yet.  No need to save it.
        //

        return;
    }

    //
    // Serialization is provided by the caller, so just update the list now.
    // Note the allocations are nonpaged so they can be searched at bugcheck
    // time.
    //

    if (MmUnloadedDrivers == NULL) {
        NumberOfBytes = MI_UNLOADED_DRIVERS * sizeof (UNLOADED_DRIVERS);

        MmUnloadedDrivers = (PUNLOADED_DRIVERS)ExAllocatePoolWithTag (NonPagedPool,
                                                                      NumberOfBytes,
                                                                      'TDmM');
        if (MmUnloadedDrivers == NULL) {
            return;
        }
        RtlZeroMemory (MmUnloadedDrivers, NumberOfBytes);
        MmLastUnloadedDriver = 0;
    }
    else if (MmLastUnloadedDriver >= MI_UNLOADED_DRIVERS) {
        MmLastUnloadedDriver = 0;
    }

    Entry = &MmUnloadedDrivers[MmLastUnloadedDriver];

    //
    // Free the old entry as we recycle into the new.
    //

    RtlFreeUnicodeString (&Entry->Name);

    Entry->Name.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                DriverName->Length,
                                                'TDmM');

    if (Entry->Name.Buffer == NULL) {
        Entry->Name.MaximumLength = 0;
        Entry->Name.Length = 0;
        MiUnloadsSkipped += 1;
        return;
    }

    RtlCopyMemory(Entry->Name.Buffer, DriverName->Buffer, DriverName->Length);
    Entry->Name.Length = DriverName->Length;
    Entry->Name.MaximumLength = DriverName->MaximumLength;

    Entry->StartAddress = Address;
    Entry->EndAddress = (PVOID)((PCHAR)Address + Length);

    KeQuerySystemTime (&Entry->CurrentTime);

    MiTotalUnloads += 1;
    MmLastUnloadedDriver += 1;
}

PUNICODE_STRING
MmLocateUnloadedDriver (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine attempts to find the specified virtual address in the
    unloaded driver list.

Arguments:

    VirtualAddress - Supplies a virtual address that might be within a driver
                     that has already unloaded.

Return Value:

    A pointer to a Unicode string containing the unloaded driver's name.

Environment:

    Kernel mode, bugcheck time.

--*/

{
    PUNLOADED_DRIVERS Entry;
    ULONG i;
    ULONG Index;

    //
    // No serialization is needed because we've crashed.
    //

    if (MmUnloadedDrivers == NULL) {
        return NULL;
    }

    Index = MmLastUnloadedDriver - 1;

    for (i = 0; i < MI_UNLOADED_DRIVERS; i += 1) {
        if (Index >= MI_UNLOADED_DRIVERS) {
            Index = MI_UNLOADED_DRIVERS - 1;
        }
        Entry = &MmUnloadedDrivers[Index];
        if (Entry->Name.Buffer != NULL) {
            if ((VirtualAddress >= Entry->StartAddress) &&
                (VirtualAddress < Entry->EndAddress)) {
                    return &Entry->Name;
            }
        }
        Index -= 1;
    }

    return NULL;
}


NTSTATUS
MmUnloadSystemImage (
    IN PVOID ImageHandle
    )

/*++

Routine Description:

    This routine unloads a previously loaded system image and returns
    the allocated resources.

Arguments:

    ImageHandle - Supplies a pointer to the section object of the
                  image to unload.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, APC_LEVEL or below, arbitrary process context.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PMMPTE LastPte;
    PFN_NUMBER PagesRequired;
    PFN_NUMBER ResidentPages;
    PMMPTE PointerPte;
    PFN_NUMBER NumberOfPtes;
    PVOID BasedAddress;
    SIZE_T NumberOfBytes;
    LOGICAL MustFree;
    SIZE_T CommittedPages;
    LOGICAL ViewDeleted;
    PIMAGE_ENTRY_IN_SESSION DriverImage;
    NTSTATUS Status;
    PSECTION SectionPointer;
    PKTHREAD CurrentThread;

    //
    // Arbitrary process context so prevent suspend APCs now.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    ViewDeleted = FALSE;
    DataTableEntry = (PKLDR_DATA_TABLE_ENTRY)ImageHandle;
    BasedAddress = DataTableEntry->DllBase;

#if DBGXX
    //
    // MiUnloadSystemImageByForce violates this check so remove it for now.
    //

    if (PsLoadedModuleList.Flink) {
        LOGICAL Found;
        PLIST_ENTRY NextEntry;
        PKLDR_DATA_TABLE_ENTRY DataTableEntry2;

        Found = FALSE;
        NextEntry = PsLoadedModuleList.Flink;
        while (NextEntry != &PsLoadedModuleList) {

            DataTableEntry2 = CONTAINING_RECORD(NextEntry,
                                                KLDR_DATA_TABLE_ENTRY,
                                                InLoadOrderLinks);
            if (DataTableEntry == DataTableEntry2) {
                Found = TRUE;
                break;
            }
            NextEntry = NextEntry->Flink;
        }
        ASSERT (Found == TRUE);
    }
#endif

#if DBG_SYSLOAD
    if (DataTableEntry->SectionPointer == NULL) {
        DbgPrint ("MM: Called to unload boot driver %wZ\n",
            &DataTableEntry->FullDllName);
    }
    else {
        DbgPrint ("MM: Called to unload non-boot driver %wZ\n",
            &DataTableEntry->FullDllName);
    }
#endif

    //
    // Any driver loaded at boot that did not have its import list
    // and LoadCount reconstructed cannot be unloaded because we don't
    // know how many other drivers may be linked to it.
    //

    if (DataTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {
        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        return STATUS_SUCCESS;
    }

    ASSERT (DataTableEntry->LoadCount != 0);

    if (MI_IS_SESSION_IMAGE_ADDRESS (BasedAddress)) {

        //
        // A printer driver may be referenced multiple times for the
        // same session space.  Only unload the last reference.
        //

        DriverImage = MiSessionLookupImage (BasedAddress);

        ASSERT (DriverImage);

        ASSERT (DriverImage->ImageCountInThisSession);

        if (DriverImage->ImageCountInThisSession > 1) {

            DriverImage->ImageCountInThisSession -= 1;
            KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
            KeLeaveCriticalRegionThread (CurrentThread);

            return STATUS_SUCCESS;
        }

        //
        // The reference count for this image has dropped to zero in this
        // session, so we can delete this session's view of the image.
        //

        Status = MiSessionWideGetImageSize (BasedAddress,
                                            &NumberOfBytes,
                                            &CommittedPages);

        if (!NT_SUCCESS(Status)) {

            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x41286,
                          (ULONG_PTR)MmSessionSpace->SessionId,
                          (ULONG_PTR)BasedAddress,
                          0);
        }

        //
        // Free the session space taken up by the image, unmapping it from
        // the current VA space - note this does not remove page table pages
        // from the session PageTables[].  Each data page is only freed
        // if there are no other references to it (ie: from any other
        // sessions).
        //

        PointerPte = MiGetPteAddress (BasedAddress);
        LastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)BasedAddress + NumberOfBytes));

        PagesRequired = MiDeleteSystemPagableVm (PointerPte,
                                                 (PFN_NUMBER)(LastPte - PointerPte),
                                                 ZeroKernelPte,
                                                 TRUE,
                                                 &ResidentPages);

        //
        // Note resident available is returned here without waiting for load
        // count to reach zero because it is charged each time a session space
        // driver locks down its code or data regardless of whether it is really
        // the same copy-on-write backing page(s) that some other session has
        // already locked down.
        //

        MiReturnResidentAvailable (ResidentPages, 22);

        if ((MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) == 0) {

            SectionPointer = (PSECTION)DataTableEntry->SectionPointer;

            if ((SectionPointer == NULL) ||
                (SectionPointer == (PVOID)-1) ||
                (SectionPointer->Segment == NULL) ||
                (SectionPointer->Segment->BasedAddress != SectionPointer->Segment->SystemImageBase)) {

                MmTotalSystemDriverPages -= (ULONG)(PagesRequired - ResidentPages);
            }
        }

        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages,
                                     0 - CommittedPages);

        MM_BUMP_SESS_COUNTER(MM_DBG_SESSION_COMMIT_IMAGE_UNLOAD,
            (ULONG)CommittedPages);

        ViewDeleted = TRUE;

        //
        // Return the commitment we took out on the pagefile when the
        // image was allocated.
        //

        MiReturnCommitment (CommittedPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD, CommittedPages);

        //
        // Tell the session space image handler that we are releasing
        // our claim to the image.
        //

        Status = MiRemoveImageSessionWide (BasedAddress);

        ASSERT (NT_SUCCESS (Status));
    }

    ASSERT (DataTableEntry->LoadCount != 0);

    DataTableEntry->LoadCount -= 1;

    if (DataTableEntry->LoadCount != 0) {
        KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
        KeLeaveCriticalRegionThread (CurrentThread);
        return STATUS_SUCCESS;
    }

#if DBG
    if (MI_IS_SESSION_IMAGE_ADDRESS (BasedAddress)) {
        ASSERT (MiSessionLookupImage (BasedAddress) == NULL);
    }
#endif

    if (MmSnapUnloads) {
#if 0
        PVOID StillQueued;

        StillQueued = KeCheckForTimer (DataTableEntry->DllBase,
                                       DataTableEntry->SizeOfImage);

        if (StillQueued != NULL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x18,
                          (ULONG_PTR)StillQueued,
                          (ULONG_PTR)-1,
                          (ULONG_PTR)DataTableEntry->DllBase);
        }

        StillQueued = ExpCheckForResource (DataTableEntry->DllBase,
                                           DataTableEntry->SizeOfImage);

        if (StillQueued != NULL) {
            KeBugCheckEx (DRIVER_VERIFIER_DETECTED_VIOLATION,
                          0x19,
                          (ULONG_PTR)StillQueued,
                          (ULONG_PTR)-1,
                          (ULONG_PTR)DataTableEntry->DllBase);
        }
#endif
    }

    if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
        VerifierDeadlockFreePool (DataTableEntry->DllBase, DataTableEntry->SizeOfImage);
    }

    if (DataTableEntry->Flags & LDRP_IMAGE_VERIFYING) {
        MiVerifyingDriverUnloading (DataTableEntry);
    }

    if (MiActiveVerifierThunks != 0) {
        MiVerifierCheckThunks (DataTableEntry);
    }

    //
    // Unload symbols from debugger.
    //

    if (DataTableEntry->Flags & LDRP_DEBUG_SYMBOLS_LOADED) {

        //
        //  TEMP TEMP TEMP rip out when debugger converted
        //

        ANSI_STRING AnsiName;

        Status = RtlUnicodeStringToAnsiString (&AnsiName,
                                               &DataTableEntry->BaseDllName,
                                               TRUE);

        if (NT_SUCCESS (Status)) {
            DbgUnLoadImageSymbols (&AnsiName,
                                   BasedAddress,
                                   (ULONG)-1);
            RtlFreeAnsiString (&AnsiName);
        }
    }

    //
    // No unload can happen till after Mm has finished Phase 1 initialization.
    // Therefore, large pages are already in effect (if this platform supports
    // it).
    //

    if (ViewDeleted == FALSE) {

        NumberOfPtes = DataTableEntry->SizeOfImage >> PAGE_SHIFT;

        if (MmSnapUnloads) {
            MiRememberUnloadedDriver (&DataTableEntry->BaseDllName,
                                      BasedAddress,
                                      (ULONG)(NumberOfPtes << PAGE_SHIFT));
        }

        if (DataTableEntry->Flags & LDRP_SYSTEM_MAPPED) {

            PointerPte = MiGetPteAddress (BasedAddress);

            PagesRequired = MiDeleteSystemPagableVm (PointerPte,
                                                     NumberOfPtes,
                                                     ZeroKernelPte,
                                                     FALSE,
                                                     &ResidentPages);

            MmTotalSystemDriverPages -= (ULONG)(PagesRequired - ResidentPages);

            //
            // Note that drivers loaded at boot that have not been relocated
            // have no system PTEs or commit charged.
            //

            MiReleaseSystemPtes (PointerPte,
                                 (ULONG)NumberOfPtes,
                                 SystemPteSpace);

            MiReturnResidentAvailable (ResidentPages, 21);

            //
            // Only return commitment for drivers that weren't loaded by the
            // boot loader.
            //

            if (DataTableEntry->SectionPointer != NULL) {
                MiReturnCommitment (PagesRequired);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1, PagesRequired);
                InterlockedExchangeAdd ((PLONG)&MmDriverCommit,
                                        (LONG) (0 - PagesRequired));
            }
        }
        else {

            //
            // This must be a boot driver that was not relocated into
            // system PTEs.  If large or super pages are enabled, the
            // image pages must be freed without referencing the
            // non-existent page table pages.  If large/super pages are
            // not enabled, note that system PTEs were not used to map the
            // image and thus, cannot be freed.

            //
            // This is further complicated by the fact that the INIT and/or
            // discardable portions of these images may have already been freed.
            //
        }
    }

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible an entry is not in the
    // list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    if (DataTableEntry->InLoadOrderLinks.Flink != NULL) {
        MiProcessLoaderEntry (DataTableEntry, FALSE);
        MustFree = TRUE;
    }
    else {
        MustFree = FALSE;
    }

    //
    // Handle unloading of any dependent DLLs that we loaded automatically
    // for this image.
    //

    MiDereferenceImports ((PLOAD_IMPORTS)DataTableEntry->LoadedImports);

    MiClearImports (DataTableEntry);

    //
    // Free this loader entry.
    //

    if (MustFree == TRUE) {

        if (DataTableEntry->FullDllName.Buffer != NULL) {
            ExFreePool (DataTableEntry->FullDllName.Buffer);
        }

        //
        // Dereference the section object if there is one.
        // There should only be one for win32k.sys and Hydra session images.
        //

        if ((DataTableEntry->SectionPointer != NULL) &&
            (DataTableEntry->SectionPointer != (PVOID)-1)) {

            ObDereferenceObject (DataTableEntry->SectionPointer);
        }

        ExFreePool((PVOID)DataTableEntry);
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    PERFINFO_IMAGE_UNLOAD(BasedAddress);

    return STATUS_SUCCESS;
}


NTSTATUS
MiBuildImportsForBootDrivers(
    VOID
    )

/*++

Routine Description:

    Construct an import list chain for boot-loaded drivers.
    If this cannot be done for an entry, its chain is set to LOADED_AT_BOOT.

    If a chain can be successfully built, then this driver's DLLs
    will be automatically unloaded if this driver goes away (provided
    no other driver is also using them).  Otherwise, on driver unload,
    its dependent DLLs would have to be explicitly unloaded.

    Note that the incoming LoadCount values are not correct and thus, they
    are reinitialized here.

Arguments:

    None.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry2;
    PLIST_ENTRY NextEntry2;
    ULONG i;
    ULONG j;
    ULONG ImageCount;
    PVOID *ImageReferences;
    PVOID LastImageReference;
    PULONG_PTR ImportThunk;
    ULONG_PTR BaseAddress;
    ULONG_PTR LastAddress;
    ULONG ImportSize;
    ULONG ImportListSize;
    PLOAD_IMPORTS ImportList;
    LOGICAL UndoEverything;
    PKLDR_DATA_TABLE_ENTRY KernelDataTableEntry;
    PKLDR_DATA_TABLE_ENTRY HalDataTableEntry;
    UNICODE_STRING KernelString;
    UNICODE_STRING HalString;

    PAGED_CODE();

    ImageCount = 0;

    KernelDataTableEntry = NULL;
    HalDataTableEntry = NULL;

    RtlInitUnicodeString (&KernelString, (const PUSHORT)L"ntoskrnl.exe");
    RtlInitUnicodeString (&HalString, (const PUSHORT)L"hal.dll");

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        if (RtlEqualUnicodeString (&KernelString,
                                   &DataTableEntry->BaseDllName,
                                   TRUE)) {

            KernelDataTableEntry = CONTAINING_RECORD(NextEntry,
                                                     KLDR_DATA_TABLE_ENTRY,
                                                     InLoadOrderLinks);
        }
        else if (RtlEqualUnicodeString (&HalString,
                                        &DataTableEntry->BaseDllName,
                                        TRUE)) {

            HalDataTableEntry = CONTAINING_RECORD(NextEntry,
                                                  KLDR_DATA_TABLE_ENTRY,
                                                  InLoadOrderLinks);
        }

        //
        // Initialize these properly so error recovery is simplified.
        //

        if (DataTableEntry->Flags & LDRP_DRIVER_DEPENDENT_DLL) {
            if ((DataTableEntry == HalDataTableEntry) || (DataTableEntry == KernelDataTableEntry)) {
                DataTableEntry->LoadCount = 1;
            }
            else {
                DataTableEntry->LoadCount = 0;
            }
        }
        else {
            DataTableEntry->LoadCount = 1;
        }

        DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;

        ImageCount += 1;
        NextEntry = NextEntry->Flink;
    }

    if (KernelDataTableEntry == NULL || HalDataTableEntry == NULL) {
        return STATUS_NOT_FOUND;
    }

    ImageReferences = (PVOID *) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                       ImageCount * sizeof (PVOID),
                                                       'TDmM');

    if (ImageReferences == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    UndoEverything = FALSE;

    NextEntry = PsLoadedModuleList.Flink;

    for ( ; NextEntry != &PsLoadedModuleList; NextEntry = NextEntry->Flink) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        ImportThunk = (PULONG_PTR)RtlImageDirectoryEntryToData(
                                           DataTableEntry->DllBase,
                                           TRUE,
                                           IMAGE_DIRECTORY_ENTRY_IAT,
                                           &ImportSize);

        if (ImportThunk == NULL) {
            DataTableEntry->LoadedImports = NO_IMPORTS_USED;
            continue;
        }

        RtlZeroMemory (ImageReferences, ImageCount * sizeof (PVOID));

        ImportSize /= sizeof(PULONG_PTR);

        BaseAddress = 0;

        //
        // Initializing these locals is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        j = 0;
        LastAddress = 0;

        for (i = 0; i < ImportSize; i += 1, ImportThunk += 1) {

            //
            // Check the hint first.
            //

            if (BaseAddress != 0) {
                if (*ImportThunk >= BaseAddress && *ImportThunk < LastAddress) {
                    ASSERT (ImageReferences[j]);
                    continue;
                }
            }

            j = 0;
            NextEntry2 = PsLoadedModuleList.Flink;

            while (NextEntry2 != &PsLoadedModuleList) {

                DataTableEntry2 = CONTAINING_RECORD(NextEntry2,
                                                    KLDR_DATA_TABLE_ENTRY,
                                                    InLoadOrderLinks);

                BaseAddress = (ULONG_PTR) DataTableEntry2->DllBase;
                LastAddress = BaseAddress + DataTableEntry2->SizeOfImage;

                if (*ImportThunk >= BaseAddress && *ImportThunk < LastAddress) {
                    ImageReferences[j] = DataTableEntry2;
                    break;
                }

                NextEntry2 = NextEntry2->Flink;
                j += 1;
            }

            if (*ImportThunk < BaseAddress || *ImportThunk >= LastAddress) {
                if (*ImportThunk) {
#if DBG
                    DbgPrint ("MM: broken import linkage %p %p %p\n",
                        DataTableEntry,
                        ImportThunk,
                        *ImportThunk);
                    DbgBreakPoint ();
#endif
                    UndoEverything = TRUE;
                    goto finished;
                }

                BaseAddress = 0;
            }
        }

        ImportSize = 0;

        //
        // Initializing LastImageReference is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        LastImageReference = NULL;

        for (i = 0; i < ImageCount; i += 1) {

            if ((ImageReferences[i] != NULL) &&
                (ImageReferences[i] != KernelDataTableEntry) &&
                (ImageReferences[i] != HalDataTableEntry)) {

                    LastImageReference = ImageReferences[i];
                    ImportSize += 1;
            }
        }

        if (ImportSize == 0) {
            DataTableEntry->LoadedImports = NO_IMPORTS_USED;
        }
        else if (ImportSize == 1) {
#if DBG_SYSLOAD
            DbgPrint("driver %wZ imports %wZ\n",
                &DataTableEntry->FullDllName,
                &((PKLDR_DATA_TABLE_ENTRY)LastImageReference)->FullDllName);
#endif

            DataTableEntry->LoadedImports = POINTER_TO_SINGLE_ENTRY (LastImageReference);
            ((PKLDR_DATA_TABLE_ENTRY)LastImageReference)->LoadCount += 1;
        }
        else {
#if DBG_SYSLOAD
            DbgPrint("driver %wZ imports many\n", &DataTableEntry->FullDllName);
#endif

            ImportListSize = ImportSize * sizeof(PVOID) + sizeof(SIZE_T);

            ImportList = (PLOAD_IMPORTS) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                                                ImportListSize,
                                                                'TDmM');

            if (ImportList == NULL) {
                UndoEverything = TRUE;
                break;
            }

            ImportList->Count = ImportSize;

            j = 0;
            for (i = 0; i < ImageCount; i += 1) {

                if ((ImageReferences[i] != NULL) &&
                    (ImageReferences[i] != KernelDataTableEntry) &&
                    (ImageReferences[i] != HalDataTableEntry)) {

#if DBG_SYSLOAD
                        DbgPrint("driver %wZ imports %wZ\n",
                            &DataTableEntry->FullDllName,
                            &((PKLDR_DATA_TABLE_ENTRY)ImageReferences[i])->FullDllName);
#endif

                        ImportList->Entry[j] = ImageReferences[i];
                        ((PKLDR_DATA_TABLE_ENTRY)ImageReferences[i])->LoadCount += 1;
                        j += 1;
                }
            }

            ASSERT (j == ImportSize);

            DataTableEntry->LoadedImports = ImportList;
        }
#if DBG_SYSLOAD
        DbgPrint("\n");
#endif
    }

finished:

    ExFreePool ((PVOID)ImageReferences);

    //
    // The kernel and HAL are never unloaded.
    //

    if ((KernelDataTableEntry->LoadedImports != NO_IMPORTS_USED) &&
        (!POINTER_TO_SINGLE_ENTRY(KernelDataTableEntry->LoadedImports))) {
            ExFreePool ((PVOID)KernelDataTableEntry->LoadedImports);
    }

    if ((HalDataTableEntry->LoadedImports != NO_IMPORTS_USED) &&
        (!POINTER_TO_SINGLE_ENTRY(HalDataTableEntry->LoadedImports))) {
            ExFreePool ((PVOID)HalDataTableEntry->LoadedImports);
    }

    KernelDataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
    HalDataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;

    if (UndoEverything == TRUE) {

#if DBG_SYSLOAD
        DbgPrint("driver %wZ import rebuild failed\n",
            &DataTableEntry->FullDllName);
        DbgBreakPoint();
#endif

        //
        // An error occurred and this is an all or nothing operation so
        // roll everything back.
        //

        NextEntry = PsLoadedModuleList.Flink;
        while (NextEntry != &PsLoadedModuleList) {
            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            ImportList = DataTableEntry->LoadedImports;
            if (ImportList == LOADED_AT_BOOT || ImportList == NO_IMPORTS_USED ||
                SINGLE_ENTRY(ImportList)) {
                    NOTHING;
            }
            else {
                ExFreePool (ImportList);
            }

            DataTableEntry->LoadedImports = (PVOID)LOADED_AT_BOOT;
            DataTableEntry->LoadCount = 1;
            NextEntry = NextEntry->Flink;
        }

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    return STATUS_SUCCESS;
}


LOGICAL
MiCallDllUnloadAndUnloadDll(
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    )

/*++

Routine Description:

    All the references from other drivers to this DLL have been cleared.
    The only remaining issue is that this DLL must support being unloaded.
    This means having no outstanding DPCs, allocated pool, etc.

    If the DLL has an unload routine that returns SUCCESS, then we clean
    it up and free up its memory now.

    Note this routine is NEVER called for drivers - only for DLLs that were
    loaded due to import references from various drivers.

Arguments:

    DataTableEntry - provided for the DLL.

Return Value:

    TRUE if the DLL was successfully unloaded, FALSE if not.

--*/

{
    PMM_DLL_UNLOAD Func;
    NTSTATUS Status;
    LOGICAL Unloaded;

    PAGED_CODE();

    Unloaded = FALSE;

    Func = (PMM_DLL_UNLOAD) (ULONG_PTR) MiLocateExportName (DataTableEntry->DllBase, "DllUnload");

    if (Func) {

        //
        // The unload function was found in the DLL so unload it now.
        //

        Status = Func();

        if (NT_SUCCESS(Status)) {

            //
            // Set up the reference count so the import DLL looks like a regular
            // driver image is being unloaded.
            //

            ASSERT (DataTableEntry->LoadCount == 0);
            DataTableEntry->LoadCount = 1;

            MmUnloadSystemImage ((PVOID)DataTableEntry);
            Unloaded = TRUE;
        }
    }

    return Unloaded;
}


PVOID
MiLocateExportName (
    IN PVOID DllBase,
    IN PCHAR FunctionName
    )

/*++

Routine Description:

    This function is invoked to locate a function name in an export directory.

Arguments:

    DllBase - Supplies the image base.

    FunctionName - Supplies the the name to be located.

Return Value:

    The address of the located function or NULL.

--*/

{
    PVOID Func;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;
    PULONG Addr;
    ULONG ExportSize;
    ULONG Low;
    ULONG Middle;
    ULONG High;
    LONG Result;
    USHORT OrdinalNumber;

    PAGED_CODE();

    Func = NULL;

    //
    // Locate the DLL's export directory.
    //

    ExportDirectory = (PIMAGE_EXPORT_DIRECTORY)RtlImageDirectoryEntryToData(
                                DllBase,
                                TRUE,
                                IMAGE_DIRECTORY_ENTRY_EXPORT,
                                &ExportSize
                                );
    if (ExportDirectory) {

        NameTableBase =  (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNames);
        NameOrdinalTableBase = (PUSHORT)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

        //
        // Look in the export name table for the specified function name.
        //

        Low = 0;
        High = ExportDirectory->NumberOfNames - 1;

        //
        // Initializing Middle is not needed for correctness, but without it
        // the compiler cannot compile this code W4 to check for use of
        // uninitialized variables.
        //

        Middle = 0;

        while (High >= Low && (LONG)High >= 0) {

            //
            // Compute the next probe index and compare the export name entry
            // with the specified function name.
            //

            Middle = (Low + High) >> 1;
            Result = strcmp(FunctionName,
                            (PCHAR)((PCHAR)DllBase + NameTableBase[Middle]));

            if (Result < 0) {
                High = Middle - 1;
            }
            else if (Result > 0) {
                Low = Middle + 1;
            }
            else {
                break;
            }
        }

        //
        // If the high index is less than the low index, then a matching table
        // entry was not found.  Otherwise, get the ordinal number from the
        // ordinal table and location the function address.
        //

        if ((LONG)High >= (LONG)Low) {

            OrdinalNumber = NameOrdinalTableBase[Middle];
            Addr = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfFunctions);
            Func = (PVOID)((ULONG_PTR)DllBase + Addr[OrdinalNumber]);

            //
            // If the function address is w/in range of the export directory,
            // then the function is forwarded, which is not allowed, so ignore
            // it.
            //

            if ((ULONG_PTR)Func > (ULONG_PTR)ExportDirectory &&
                (ULONG_PTR)Func < ((ULONG_PTR)ExportDirectory + ExportSize)) {
                Func = NULL;
            }
        }
    }

    return Func;
}


NTSTATUS
MiDereferenceImports (
    IN PLOAD_IMPORTS ImportList
    )

/*++

Routine Description:

    Decrement the reference count on each DLL specified in the image import
    list.  If any DLL's reference count reaches zero, then free the DLL.

    No locks may be held on entry as MmUnloadSystemImage may be called.

    The parameter list is freed here as well.

Arguments:

    ImportList - Supplies the list of DLLs to dereference.

Return Value:

    Status of the dereference operation.

--*/

{
    ULONG i;
    LOGICAL Unloaded;
    PVOID SavedImports;
    LOAD_IMPORTS SingleTableEntry;
    PKLDR_DATA_TABLE_ENTRY ImportTableEntry;

    PAGED_CODE();

    if (ImportList == LOADED_AT_BOOT || ImportList == NO_IMPORTS_USED) {
        return STATUS_SUCCESS;
    }

    if (SINGLE_ENTRY(ImportList)) {
        SingleTableEntry.Count = 1;
        SingleTableEntry.Entry[0] = SINGLE_ENTRY_TO_POINTER(ImportList);
        ImportList = &SingleTableEntry;
    }

    for (i = 0; i < ImportList->Count && ImportList->Entry[i]; i += 1) {
        ImportTableEntry = ImportList->Entry[i];

        if (ImportTableEntry->LoadedImports == (PVOID)LOADED_AT_BOOT) {

            //
            // Skip this one - it was loaded by ntldr.
            //

            continue;
        }

#if DBG
        {
            ULONG ImageCount;
            PLIST_ENTRY NextEntry;
            PKLDR_DATA_TABLE_ENTRY DataTableEntry;

            //
            // Assert that the first 2 entries are never dereferenced as
            // unloading the kernel or HAL would be fatal.
            //

            NextEntry = PsLoadedModuleList.Flink;

            ImageCount = 0;
            while (NextEntry != &PsLoadedModuleList && ImageCount < 2) {
                DataTableEntry = CONTAINING_RECORD(NextEntry,
                                                   KLDR_DATA_TABLE_ENTRY,
                                                   InLoadOrderLinks);
                ASSERT (ImportTableEntry != DataTableEntry);
                ASSERT (DataTableEntry->LoadCount == 1);
                NextEntry = NextEntry->Flink;
                ImageCount += 1;
            }
        }
#endif

        ASSERT (ImportTableEntry->LoadCount >= 1);

        ImportTableEntry->LoadCount -= 1;

        if (ImportTableEntry->LoadCount == 0) {

            //
            // Unload this dependent DLL - we only do this to non-referenced
            // non-boot-loaded drivers.  Stop the import list recursion prior
            // to unloading - we know we're done at this point.
            //
            // Note we can continue on afterwards without restarting
            // regardless of which locks get released and reacquired
            // because this chain is private.
            //

            SavedImports = ImportTableEntry->LoadedImports;

            ImportTableEntry->LoadedImports = (PVOID)NO_IMPORTS_USED;

            Unloaded = MiCallDllUnloadAndUnloadDll ((PVOID)ImportTableEntry);

            if (Unloaded == TRUE) {

                //
                // This DLL was unloaded so recurse through its imports and
                // attempt to unload all of those too.
                //

                MiDereferenceImports ((PLOAD_IMPORTS)SavedImports);

                if ((SavedImports != (PVOID)LOADED_AT_BOOT) &&
                    (SavedImports != (PVOID)NO_IMPORTS_USED) &&
                    (!SINGLE_ENTRY(SavedImports))) {

                        ExFreePool (SavedImports);
                }
            }
            else {
                ImportTableEntry->LoadedImports = SavedImports;
            }
        }
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MiResolveImageReferences (
    PVOID ImageBase,
    IN PUNICODE_STRING ImageFileDirectory,
    IN PUNICODE_STRING NamePrefix OPTIONAL,
    OUT PCHAR *MissingProcedureName,
    OUT PWSTR *MissingDriverName,
    OUT PLOAD_IMPORTS *LoadedImports
    )

/*++

Routine Description:

    This routine resolves the references from the newly loaded driver
    to the kernel, HAL and other drivers.

Arguments:

    ImageBase - Supplies the address of which the image header resides.

    ImageFileDirectory - Supplies the directory to load referenced DLLs.

Return Value:

    Status of the image reference resolution.

--*/

{
    PCHAR MissingProcedureStorageArea;
    PVOID ImportBase;
    ULONG ImportSize;
    ULONG ImportListSize;
    ULONG Count;
    ULONG i;
    PIMAGE_IMPORT_DESCRIPTOR ImportDescriptor;
    PIMAGE_IMPORT_DESCRIPTOR Imp;
    NTSTATUS st;
    ULONG ExportSize;
    PIMAGE_EXPORT_DIRECTORY ExportDirectory;
    PIMAGE_THUNK_DATA NameThunk;
    PIMAGE_THUNK_DATA AddrThunk;
    PSZ ImportName;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY SingleEntry;
    ANSI_STRING AnsiString;
    UNICODE_STRING ImportName_U;
    UNICODE_STRING ImportDescriptorName_U;
    UNICODE_STRING DllToLoad;
    UNICODE_STRING DllToLoad2;
    PVOID Section;
    PVOID BaseAddress;
    LOGICAL PrefixedNameAllocated;
    LOGICAL ReferenceImport;
    ULONG LinkWin32k = 0;
    ULONG LinkNonWin32k = 0;
    PLOAD_IMPORTS ImportList;
    PLOAD_IMPORTS CompactedImportList;
    LOGICAL Loaded;
    UNICODE_STRING DriverDirectory;

    PAGED_CODE();

    *LoadedImports = NO_IMPORTS_USED;

    MissingProcedureStorageArea = *MissingProcedureName;

    ImportDescriptor = (PIMAGE_IMPORT_DESCRIPTOR)RtlImageDirectoryEntryToData(
                        ImageBase,
                        TRUE,
                        IMAGE_DIRECTORY_ENTRY_IMPORT,
                        &ImportSize);

    if (ImportDescriptor == NULL) {
        return STATUS_SUCCESS;
    }

    // Count the number of imports so we can allocate enough room to
    // store them all chained off this module's LDR_DATA_TABLE_ENTRY.
    //

    Count = 0;
    for (Imp = ImportDescriptor; Imp->Name && Imp->OriginalFirstThunk; Imp += 1) {
        Count += 1;
    }

    if (Count != 0) {
        ImportListSize = Count * sizeof(PVOID) + sizeof(SIZE_T);

        ImportList = (PLOAD_IMPORTS) ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                             ImportListSize,
                                             'TDmM');

        //
        // Zero it so we can recover gracefully if we fail in the middle.
        // If the allocation failed, just don't build the import list.
        //

        if (ImportList != NULL) {
            RtlZeroMemory (ImportList, ImportListSize);
            ImportList->Count = Count;
        }
    }
    else {
        ImportList = NULL;
    }

    Count = 0;
    while (ImportDescriptor->Name && ImportDescriptor->OriginalFirstThunk) {

        ImportName = (PSZ)((PCHAR)ImageBase + ImportDescriptor->Name);

        //
        // A driver can link with win32k.sys if and only if it is a GDI
        // driver.
        // Also display drivers can only link to win32k.sys (and lego ...).
        //
        // So if we get a driver that links to win32k.sys and has more
        // than one set of imports, we will fail to load it.
        //

        LinkWin32k = LinkWin32k |
             (!_strnicmp(ImportName, "win32k", sizeof("win32k") - 1));

        //
        // We don't want to count coverage, win32k and irt (lego) since
        // display drivers CAN link against these.
        //

        LinkNonWin32k = LinkNonWin32k |
            ((_strnicmp(ImportName, "win32k", sizeof("win32k") - 1)) &&
             (_strnicmp(ImportName, "dxapi", sizeof("dxapi") - 1)) &&
             (_strnicmp(ImportName, "coverage", sizeof("coverage") - 1)) &&
             (_strnicmp(ImportName, "irt", sizeof("irt") - 1)));


        if (LinkNonWin32k && LinkWin32k) {
            MiDereferenceImports (ImportList);
            if (ImportList) {
                ExFreePool (ImportList);
            }
            return (STATUS_PROCEDURE_NOT_FOUND);
        }

        if ((!_strnicmp(ImportName, "ntdll",    sizeof("ntdll") - 1))    ||
            (!_strnicmp(ImportName, "winsrv",   sizeof("winsrv") - 1))   ||
            (!_strnicmp(ImportName, "advapi32", sizeof("advapi32") - 1)) ||
            (!_strnicmp(ImportName, "kernel32", sizeof("kernel32") - 1)) ||
            (!_strnicmp(ImportName, "user32",   sizeof("user32") - 1))   ||
            (!_strnicmp(ImportName, "gdi32",    sizeof("gdi32") - 1)) ) {

            MiDereferenceImports (ImportList);

            if (ImportList) {
                ExFreePool (ImportList);
            }
            return (STATUS_PROCEDURE_NOT_FOUND);
        }

        if ((!_strnicmp(ImportName, "ntoskrnl", sizeof("ntoskrnl") - 1)) ||
            (!_strnicmp(ImportName, "win32k", sizeof("win32k") - 1))     ||
            (!_strnicmp(ImportName, "hal",   sizeof("hal") - 1))) {

                //
                // These imports don't get refcounted because we don't
                // ever want to unload them.
                //

                ReferenceImport = FALSE;
        }
        else {
                ReferenceImport = TRUE;
        }

        RtlInitAnsiString (&AnsiString, ImportName);
        st = RtlAnsiStringToUnicodeString (&ImportName_U, &AnsiString, TRUE);

        if (!NT_SUCCESS(st)) {
            MiDereferenceImports (ImportList);
            if (ImportList != NULL) {
                ExFreePool (ImportList);
            }
            return st;
        }

        if (NamePrefix  &&
            (_strnicmp(ImportName, "ntoskrnl", sizeof("ntoskrnl") - 1) &&
             _strnicmp(ImportName, "hal", sizeof("hal") - 1))) {

            ImportDescriptorName_U.MaximumLength = (USHORT)(ImportName_U.Length + NamePrefix->Length);
            ImportDescriptorName_U.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                ImportDescriptorName_U.MaximumLength,
                                                'TDmM');
            if (!ImportDescriptorName_U.Buffer) {
                RtlFreeUnicodeString (&ImportName_U);
                MiDereferenceImports (ImportList);
                if (ImportList != NULL) {
                    ExFreePool (ImportList);
                }
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            ImportDescriptorName_U.Length = 0;
            RtlAppendUnicodeStringToString(&ImportDescriptorName_U, NamePrefix);
            RtlAppendUnicodeStringToString(&ImportDescriptorName_U, &ImportName_U);
            PrefixedNameAllocated = TRUE;
        }
        else {
            ImportDescriptorName_U = ImportName_U;
            PrefixedNameAllocated = FALSE;
        }

        Loaded = FALSE;

ReCheck:
        NextEntry = PsLoadedModuleList.Flink;
        ImportBase = NULL;

        //
        // Initializing DataTableEntry is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        DataTableEntry = NULL;

        while (NextEntry != &PsLoadedModuleList) {

            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            if (RtlEqualUnicodeString (&ImportDescriptorName_U,
                                       &DataTableEntry->BaseDllName,
                                       TRUE)) {

                ImportBase = DataTableEntry->DllBase;

                //
                // Only bump the LoadCount if this thread did not initiate
                // the load below.  If this thread initiated the load, then
                // the LoadCount has already been bumped as part of the
                // load - we only want to increment it here if we are
                // "attaching" to a previously loaded DLL.
                //

                if ((Loaded == FALSE) && (ReferenceImport == TRUE)) {

                    //
                    // Only increment the load count on the import if it is not
                    // circular (ie: the import is not from the original
                    // caller).
                    //

                    if ((DataTableEntry->Flags & LDRP_LOAD_IN_PROGRESS) == 0) {
                        DataTableEntry->LoadCount += 1;
                    }
                }

                break;
            }
            NextEntry = NextEntry->Flink;
        }

        if (ImportBase == NULL) {

            //
            // The DLL name was not located, attempt to load this dll.
            //

            DllToLoad.MaximumLength = (USHORT)(ImportName_U.Length +
                                        ImageFileDirectory->Length +
                                        sizeof(WCHAR));

            DllToLoad.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                               DllToLoad.MaximumLength,
                                               'TDmM');

            if (DllToLoad.Buffer) {
                DllToLoad.Length = ImageFileDirectory->Length;
                RtlCopyMemory (DllToLoad.Buffer,
                               ImageFileDirectory->Buffer,
                               ImageFileDirectory->Length);

                RtlAppendStringToString ((PSTRING)&DllToLoad,
                                         (PSTRING)&ImportName_U);

                //
                // Add NULL termination in case the load fails so the name
                // can be returned as the PWSTR MissingDriverName.
                //

                DllToLoad.Buffer[(DllToLoad.MaximumLength - 1) / sizeof (WCHAR)] =
                    UNICODE_NULL;

                st = MmLoadSystemImage (&DllToLoad,
                                        NamePrefix,
                                        NULL,
                                        FALSE,
                                        &Section,
                                        &BaseAddress);

                if (NT_SUCCESS(st)) {

                    //
                    // No need to keep the temporary name buffer around now
                    // that there is a loaded module list entry for this DLL.
                    //

                    ExFreePool (DllToLoad.Buffer);
                }
                else {

                    if ((st == STATUS_OBJECT_NAME_NOT_FOUND) &&
                        (NamePrefix == NULL) &&
                        (MI_IS_SESSION_ADDRESS (ImageBase))) {

#define DRIVERS_SUBDIR_NAME L"drivers\\"

                        DriverDirectory.Buffer = (const PUSHORT) DRIVERS_SUBDIR_NAME;
                        DriverDirectory.Length = sizeof (DRIVERS_SUBDIR_NAME) - sizeof (WCHAR);
                        DriverDirectory.MaximumLength = sizeof DRIVERS_SUBDIR_NAME;

                        //
                        // The DLL file was not located, attempt to load it
                        // from the drivers subdirectory.  This makes it
                        // possible for drivers like win32k.sys to link to
                        // drivers that reside in the drivers subdirectory
                        // (like dxapi.sys).
                        //

                        DllToLoad2.MaximumLength = (USHORT)(ImportName_U.Length +
                                                    DriverDirectory.Length +
                                                    ImageFileDirectory->Length +
                                                    sizeof(WCHAR));

                        DllToLoad2.Buffer = ExAllocatePoolWithTag (NonPagedPool,
                                                           DllToLoad2.MaximumLength,
                                                           'TDmM');

                        if (DllToLoad2.Buffer) {
                            DllToLoad2.Length = ImageFileDirectory->Length;
                            RtlCopyMemory (DllToLoad2.Buffer,
                                           ImageFileDirectory->Buffer,
                                           ImageFileDirectory->Length);

                            RtlAppendStringToString ((PSTRING)&DllToLoad2,
                                                     (PSTRING)&DriverDirectory);

                            RtlAppendStringToString ((PSTRING)&DllToLoad2,
                                                     (PSTRING)&ImportName_U);

                            //
                            // Add NULL termination in case the load fails
                            // so the name can be returned as the PWSTR
                            // MissingDriverName.
                            //

                            DllToLoad2.Buffer[(DllToLoad2.MaximumLength - 1) / sizeof (WCHAR)] =
                                UNICODE_NULL;

                            st = MmLoadSystemImage (&DllToLoad2,
                                                    NULL,
                                                    NULL,
                                                    FALSE,
                                                    &Section,
                                                    &BaseAddress);

                            ExFreePool (DllToLoad.Buffer);

                            DllToLoad.Buffer = DllToLoad2.Buffer;
                            DllToLoad.Length = DllToLoad2.Length;
                            DllToLoad.MaximumLength = DllToLoad2.MaximumLength;

                            if (NT_SUCCESS(st)) {
                                ExFreePool (DllToLoad.Buffer);
                                goto LoadFinished;
                            }
                        }
                        else {
                            Section = NULL;
                            BaseAddress = NULL;
                            st = STATUS_INSUFFICIENT_RESOURCES;
                            goto LoadFinished;
                        }
                    }

                    //
                    // Return the temporary name buffer to our caller so
                    // the name of the DLL that failed to load can be displayed.
                    // Set the low bit of the pointer so our caller knows to
                    // free this buffer when he's done displaying it (as opposed
                    // to loaded module list entries which should not be freed).
                    //

                    *MissingDriverName = DllToLoad.Buffer;
                    *(PULONG)MissingDriverName |= 0x1;

                    //
                    // Set this to NULL so the hard error prints properly.
                    //

                    *MissingProcedureName = NULL;
                }
            }
            else {

                //
                // Initializing Section and BaseAddress is not needed for
                // correctness but without it the compiler cannot compile
                // this code W4 to check for use of uninitialized variables.
                //

                Section = NULL;
                BaseAddress = NULL;
                st = STATUS_INSUFFICIENT_RESOURCES;
            }

LoadFinished:

            //
            // Call any needed DLL initialization now.
            //

            if (NT_SUCCESS(st)) {
#if DBG
                PLIST_ENTRY Entry;
#endif
                PKLDR_DATA_TABLE_ENTRY TableEntry;

                Loaded = TRUE;

                TableEntry = (PKLDR_DATA_TABLE_ENTRY) Section;
                ASSERT (BaseAddress == TableEntry->DllBase);

#if DBG
                //
                // Lookup the dll's table entry in the loaded module list.
                // This is expected to always succeed.
                //

                Entry = PsLoadedModuleList.Blink;
                while (Entry != &PsLoadedModuleList) {
                    TableEntry = CONTAINING_RECORD (Entry,
                                                    KLDR_DATA_TABLE_ENTRY,
                                                    InLoadOrderLinks);

                    if (BaseAddress == TableEntry->DllBase) {
                        ASSERT (TableEntry == (PKLDR_DATA_TABLE_ENTRY) Section);
                        break;
                    }
                    ASSERT (TableEntry != (PKLDR_DATA_TABLE_ENTRY) Section);
                    Entry = Entry->Blink;
                }

                ASSERT (Entry != &PsLoadedModuleList);
#endif

                //
                // Call the dll's initialization routine if it has
                // one.  This routine will reapply verifier thunks to
                // any modules that link to this one if necessary.
                //

                st = MmCallDllInitialize (TableEntry, &PsLoadedModuleList);

                //
                // If the module could not be properly initialized,
                // unload it.
                //

                if (!NT_SUCCESS(st)) {
                    MmUnloadSystemImage ((PVOID)TableEntry);
                    Loaded = FALSE;
                }
            }

            if (!NT_SUCCESS(st)) {

                RtlFreeUnicodeString (&ImportName_U);
                if (PrefixedNameAllocated == TRUE) {
                    ExFreePool (ImportDescriptorName_U.Buffer);
                }
                MiDereferenceImports (ImportList);
                if (ImportList != NULL) {
                    ExFreePool (ImportList);
                }
                return st;
            }

            goto ReCheck;
        }

        if ((ReferenceImport == TRUE) && (ImportList)) {

            //
            // Only add the image providing satisfying our imports to the
            // import list if the reference is not circular (ie: the import
            // is not from the original caller).
            //

            if ((DataTableEntry->Flags & LDRP_LOAD_IN_PROGRESS) == 0) {
                ImportList->Entry[Count] = DataTableEntry;
                Count += 1;
            }
        }

        RtlFreeUnicodeString (&ImportName_U);
        if (PrefixedNameAllocated) {
            ExFreePool (ImportDescriptorName_U.Buffer);
        }

        *MissingDriverName = DataTableEntry->BaseDllName.Buffer;

        ExportDirectory = (PIMAGE_EXPORT_DIRECTORY)RtlImageDirectoryEntryToData(
                                    ImportBase,
                                    TRUE,
                                    IMAGE_DIRECTORY_ENTRY_EXPORT,
                                    &ExportSize
                                    );

        if (!ExportDirectory) {
            MiDereferenceImports (ImportList);
            if (ImportList) {
                ExFreePool (ImportList);
            }
            return STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;
        }

        //
        // Walk through the IAT and snap all the thunks.
        //

        if (ImportDescriptor->OriginalFirstThunk) {

            NameThunk = (PIMAGE_THUNK_DATA)((PCHAR)ImageBase + (ULONG)ImportDescriptor->OriginalFirstThunk);
            AddrThunk = (PIMAGE_THUNK_DATA)((PCHAR)ImageBase + (ULONG)ImportDescriptor->FirstThunk);

            while (NameThunk->u1.AddressOfData) {

                st = MiSnapThunk (ImportBase,
                                  ImageBase,
                                  NameThunk++,
                                  AddrThunk++,
                                  ExportDirectory,
                                  ExportSize,
                                  FALSE,
                                  MissingProcedureName);

                if (!NT_SUCCESS(st) ) {
                    MiDereferenceImports (ImportList);
                    if (ImportList) {
                        ExFreePool (ImportList);
                    }
                    return st;
                }
                *MissingProcedureName = MissingProcedureStorageArea;
            }
        }

        ImportDescriptor += 1;
    }

    //
    // All the imports are successfully loaded so establish and compact
    // the import unload list.
    //

    if (ImportList) {

        //
        // Blank entries occur for things like the kernel, HAL & win32k.sys
        // that we never want to unload.  Especially for things like
        // win32k.sys where the reference count can really hit 0.
        //

        //
        // Initializing SingleEntry is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        SingleEntry = NULL;

        Count = 0;
        for (i = 0; i < ImportList->Count; i += 1) {
            if (ImportList->Entry[i]) {
                SingleEntry = POINTER_TO_SINGLE_ENTRY(ImportList->Entry[i]);
                Count += 1;
            }
        }

        if (Count == 0) {

            ExFreePool(ImportList);
            ImportList = NO_IMPORTS_USED;
        }
        else if (Count == 1) {
            ExFreePool(ImportList);
            ImportList = (PLOAD_IMPORTS)SingleEntry;
        }
        else if (Count != ImportList->Count) {

            ImportListSize = Count * sizeof(PVOID) + sizeof(SIZE_T);

            CompactedImportList = (PLOAD_IMPORTS)
                                        ExAllocatePoolWithTag (PagedPool | POOL_COLD_ALLOCATION,
                                        ImportListSize,
                                        'TDmM');
            if (CompactedImportList) {
                CompactedImportList->Count = Count;

                Count = 0;
                for (i = 0; i < ImportList->Count; i += 1) {
                    if (ImportList->Entry[i]) {
                        CompactedImportList->Entry[Count] = ImportList->Entry[i];
                        Count += 1;
                    }
                }

                ExFreePool(ImportList);
                ImportList = CompactedImportList;
            }
        }

        *LoadedImports = ImportList;
    }
    return STATUS_SUCCESS;
}


NTSTATUS
MiSnapThunk(
    IN PVOID DllBase,
    IN PVOID ImageBase,
    IN PIMAGE_THUNK_DATA NameThunk,
    OUT PIMAGE_THUNK_DATA AddrThunk,
    IN PIMAGE_EXPORT_DIRECTORY ExportDirectory,
    IN ULONG ExportSize,
    IN LOGICAL SnapForwarder,
    OUT PCHAR *MissingProcedureName
    )

/*++

Routine Description:

    This function snaps a thunk using the specified Export Section data.
    If the section data does not support the thunk, then the thunk is
    partially snapped (Dll field is still non-null, but snap address is
    set).

Arguments:

    DllBase - Base of DLL being snapped to.

    ImageBase - Base of image that contains the thunks to snap.

    Thunk - On input, supplies the thunk to snap.  When successfully
            snapped, the function field is set to point to the address in
            the DLL, and the DLL field is set to NULL.

    ExportDirectory - Supplies the Export Section data from a DLL.

    SnapForwarder - Supplies TRUE if the snap is for a forwarder, and therefore
                    Address of Data is already setup.

Return Value:

    STATUS_SUCCESS or STATUS_DRIVER_ENTRYPOINT_NOT_FOUND or
        STATUS_DRIVER_ORDINAL_NOT_FOUND

--*/

{
    BOOLEAN Ordinal;
    USHORT OrdinalNumber;
    PULONG NameTableBase;
    PUSHORT NameOrdinalTableBase;
    PULONG Addr;
    USHORT HintIndex;
    ULONG High;
    ULONG Low;
    ULONG Middle;
    LONG Result;
    NTSTATUS Status;
    PCHAR MissingProcedureName2;
    CHAR NameBuffer[ MAXIMUM_FILENAME_LENGTH ];

    PAGED_CODE();

    //
    // Determine if snap is by name, or by ordinal
    //

    Ordinal = (BOOLEAN)IMAGE_SNAP_BY_ORDINAL(NameThunk->u1.Ordinal);

    if (Ordinal && !SnapForwarder) {

        OrdinalNumber = (USHORT)(IMAGE_ORDINAL(NameThunk->u1.Ordinal) -
                         ExportDirectory->Base);

        *MissingProcedureName = (PCHAR)(ULONG_PTR)OrdinalNumber;

    }
    else {

        //
        // Change AddressOfData from an RVA to a VA.
        //

        if (!SnapForwarder) {
            NameThunk->u1.AddressOfData = (ULONG_PTR)ImageBase + NameThunk->u1.AddressOfData;
        }

        strncpy (*MissingProcedureName,
                 (const PCHAR)&((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name[0],
                 MAXIMUM_FILENAME_LENGTH - 1);

        //
        // Lookup Name in NameTable
        //

        NameTableBase = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNames);
        NameOrdinalTableBase = (PUSHORT)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfNameOrdinals);

        //
        // Before dropping into binary search, see if
        // the hint index results in a successful
        // match. If the hint index is zero, then
        // drop into binary search.
        //

        HintIndex = ((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Hint;
        if ((ULONG)HintIndex < ExportDirectory->NumberOfNames &&
            !strcmp((PSZ)((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name,
             (PSZ)((PCHAR)DllBase + NameTableBase[HintIndex]))) {
            OrdinalNumber = NameOrdinalTableBase[HintIndex];

        }
        else {

            //
            // Lookup the import name in the name table using a binary search.
            //

            Low = 0;
            High = ExportDirectory->NumberOfNames - 1;

            //
            // Initializing Middle is not needed for correctness, but without it
            // the compiler cannot compile this code W4 to check for use of
            // uninitialized variables.
            //

            Middle = 0;

            while (High >= Low) {

                //
                // Compute the next probe index and compare the import name
                // with the export name entry.
                //

                Middle = (Low + High) >> 1;
                Result = strcmp((const PCHAR)&((PIMAGE_IMPORT_BY_NAME)NameThunk->u1.AddressOfData)->Name[0],
                                (PCHAR)((PCHAR)DllBase + NameTableBase[Middle]));

                if (Result < 0) {
                    High = Middle - 1;

                } else if (Result > 0) {
                    Low = Middle + 1;

                }
                else {
                    break;
                }
            }

            //
            // If the high index is less than the low index, then a matching
            // table entry was not found. Otherwise, get the ordinal number
            // from the ordinal table.
            //

            if ((LONG)High < (LONG)Low) {
                return STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;
            }
            else {
                OrdinalNumber = NameOrdinalTableBase[Middle];
            }
        }
    }

    //
    // If OrdinalNumber is not within the Export Address Table,
    // then DLL does not implement function. Snap to LDRP_BAD_DLL.
    //

    if ((ULONG)OrdinalNumber >= ExportDirectory->NumberOfFunctions) {
        Status = STATUS_DRIVER_ORDINAL_NOT_FOUND;

    }
    else {

        MissingProcedureName2 = NameBuffer;

        Addr = (PULONG)((PCHAR)DllBase + (ULONG)ExportDirectory->AddressOfFunctions);
        *(PULONG_PTR)&AddrThunk->u1.Function = (ULONG_PTR)DllBase + Addr[OrdinalNumber];

        // AddrThunk s/b used from here on.

        Status = STATUS_SUCCESS;

        if (((ULONG_PTR)AddrThunk->u1.Function > (ULONG_PTR)ExportDirectory) &&
            ((ULONG_PTR)AddrThunk->u1.Function < ((ULONG_PTR)ExportDirectory + ExportSize)) ) {

            UNICODE_STRING UnicodeString;
            ANSI_STRING ForwardDllName;

            PLIST_ENTRY NextEntry;
            PKLDR_DATA_TABLE_ENTRY DataTableEntry;
            ULONG LocalExportSize;
            PIMAGE_EXPORT_DIRECTORY LocalExportDirectory;

            Status = STATUS_DRIVER_ENTRYPOINT_NOT_FOUND;

            //
            // Include the dot in the length so we can do prefix later on.
            //

            ForwardDllName.Buffer = (PCHAR)AddrThunk->u1.Function;
            ForwardDllName.Length = (USHORT)(strchr(ForwardDllName.Buffer, '.') -
                                           ForwardDllName.Buffer + 1);
            ForwardDllName.MaximumLength = ForwardDllName.Length;

            if (NT_SUCCESS(RtlAnsiStringToUnicodeString(&UnicodeString,
                                                        &ForwardDllName,
                                                        TRUE))) {

                NextEntry = PsLoadedModuleList.Flink;

                while (NextEntry != &PsLoadedModuleList) {

                    DataTableEntry = CONTAINING_RECORD(NextEntry,
                                                       KLDR_DATA_TABLE_ENTRY,
                                                       InLoadOrderLinks);

                    //
                    // We have to do a case INSENSITIVE comparison for
                    // forwarder because the linker just took what is in the
                    // def file, as opposed to looking in the exporting
                    // image for the name.
                    // we also use the prefix function to ignore the .exe or
                    // .sys or .dll at the end.
                    //

                    if (RtlPrefixString((PSTRING)&UnicodeString,
                                        (PSTRING)&DataTableEntry->BaseDllName,
                                        TRUE)) {

                        LocalExportDirectory = (PIMAGE_EXPORT_DIRECTORY)
                            RtlImageDirectoryEntryToData(DataTableEntry->DllBase,
                                                         TRUE,
                                                         IMAGE_DIRECTORY_ENTRY_EXPORT,
                                                         &LocalExportSize);

                        if (LocalExportDirectory != NULL) {

                            IMAGE_THUNK_DATA thunkData;
                            PIMAGE_IMPORT_BY_NAME addressOfData;
                            SIZE_T length;

                            //
                            // One extra byte for NULL termination.
                            //

                            length = strlen(ForwardDllName.Buffer +
                                                ForwardDllName.Length) + 1;

                            addressOfData = (PIMAGE_IMPORT_BY_NAME)
                                ExAllocatePoolWithTag (PagedPool,
                                                      length +
                                                   sizeof(IMAGE_IMPORT_BY_NAME),
                                                   '  mM');

                            if (addressOfData) {

                                RtlCopyMemory(&(addressOfData->Name[0]),
                                              ForwardDllName.Buffer +
                                                  ForwardDllName.Length,
                                              length);

                                addressOfData->Hint = 0;

                                *(PULONG_PTR)&thunkData.u1.AddressOfData =
                                                    (ULONG_PTR)addressOfData;

                                Status = MiSnapThunk(DataTableEntry->DllBase,
                                                     ImageBase,
                                                     &thunkData,
                                                     &thunkData,
                                                     LocalExportDirectory,
                                                     LocalExportSize,
                                                     TRUE,
                                                     &MissingProcedureName2);

                                ExFreePool(addressOfData);

                                AddrThunk->u1 = thunkData.u1;
                            }
                        }

                        break;
                    }

                    NextEntry = NextEntry->Flink;
                }

                RtlFreeUnicodeString(&UnicodeString);
            }

        }

    }
    return Status;
}
#if 0
PVOID
MiLookupImageSectionByName (
    IN PVOID Base,
    IN LOGICAL MappedAsImage,
    IN PCHAR SectionName,
    OUT PULONG SectionSize
    )

/*++

Routine Description:

    This function locates a Directory Entry within the image header
    and returns either the virtual address or seek address of the
    data the Directory describes.

Arguments:

    Base - Supplies the base of the image or data file.

    MappedAsImage - FALSE if the file is mapped as a data file.
                  - TRUE if the file is mapped as an image.

    SectionName - Supplies the name of the section to lookup.

    SectionSize - Return the size of the section.

Return Value:

    NULL - The file does not contain data for the specified section.

    NON-NULL - Returns the address where the section is mapped in memory.

--*/

{
    ULONG i, j, Match;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;

    NtHeaders = RtlImageNtHeader(Base);
    NtSection = IMAGE_FIRST_SECTION (NtHeaders);
    for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i++) {
        Match = TRUE;
        for (j = 0; j < IMAGE_SIZEOF_SHORT_NAME; j++) {
            if (SectionName[j] != NtSection->Name[j]) {
                Match = FALSE;
                break;
            }
            if (SectionName[j] == '\0') {
                break;
            }
        }
        if (Match) {
            break;
        }
        NtSection += 1;
    }
    if (Match) {
        *SectionSize = NtSection->SizeOfRawData;
        if (MappedAsImage) {
            return (((PCHAR)Base + NtSection->VirtualAddress));
        }
        else {
            return (((PCHAR)Base + NtSection->PointerToRawData));
        }
    }
    return NULL;
}
#endif //0


NTSTATUS
MmCheckSystemImage (
    IN HANDLE ImageFileHandle,
    IN LOGICAL PurgeSection
    )

/*++

Routine Description:

    This function ensures the checksum for a system image is correct
    and matches the data in the image.

Arguments:

    ImageFileHandle - Supplies the file handle of the image.

    PurgeSection - Supplies TRUE if the data section mapping the image should
                   be purged prior to returning.  Note that the first page
                   could be used to speed up subsequent image section creation,
                   but generally the cost of useless data pages sitting in
                   transition is costly.  Better to put the pages immediately
                   on the free list to preserve the transition cache for more
                   useful pages.

Return Value:

    Status value.

--*/

{
    NTSTATUS Status;
    NTSTATUS Status2;
    HANDLE Section;
    PVOID ViewBase;
    SIZE_T ViewSize;
    IO_STATUS_BLOCK IoStatusBlock;
    PIMAGE_FILE_HEADER FileHeader;
    PIMAGE_NT_HEADERS NtHeaders;
    FILE_STANDARD_INFORMATION StandardInfo;
    PSECTION SectionPointer;
    OBJECT_ATTRIBUTES ObjectAttributes;
    KAPC_STATE ApcState;

    PAGED_CODE();

    InitializeObjectAttributes (&ObjectAttributes,
                                NULL,
                                (OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE),
                                NULL,
                                NULL);

    Status = ZwCreateSection (&Section,
                              SECTION_MAP_EXECUTE,
                              &ObjectAttributes,
                              NULL,
                              PAGE_EXECUTE,
                              SEC_COMMIT,
                              ImageFileHandle);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    ViewBase = NULL;
    ViewSize = 0;

    //
    // Since callees are not always in the context of the system process,
    // attach here when necessary to guarantee the driver load occurs in a
    // known safe address space to prevent security holes.
    //

    KeStackAttachProcess (&PsInitialSystemProcess->Pcb, &ApcState);

    Status = ZwMapViewOfSection (Section,
                                 NtCurrentProcess(),
                                 (PVOID *)&ViewBase,
                                 0L,
                                 0L,
                                 NULL,
                                 &ViewSize,
                                 ViewShare,
                                 0L,
                                 PAGE_EXECUTE);

    if (!NT_SUCCESS(Status)) {
        KeUnstackDetachProcess (&ApcState);
        ZwClose(Section);
        return Status;
    }

    //
    // Now the image is mapped as a data file... Calculate its size and then
    // check its checksum.
    //

    Status = ZwQueryInformationFile (ImageFileHandle,
                                     &IoStatusBlock,
                                     &StandardInfo,
                                     sizeof(StandardInfo),
                                     FileStandardInformation);

    if (NT_SUCCESS(Status)) {

        try {

            if (!LdrVerifyMappedImageMatchesChecksum (ViewBase, StandardInfo.EndOfFile.LowPart)) {
                Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
                goto out;
            }

            NtHeaders = RtlImageNtHeader (ViewBase);

            if (NtHeaders == NULL) {
                Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
                goto out;
            }

            FileHeader = &NtHeaders->FileHeader;

            //
            // Detect configurations inadvertently trying to load 32-bit
            // drivers on NT64 or mismatched platform architectures, etc.
            //

            if ((FileHeader->Machine != IMAGE_FILE_MACHINE_NATIVE) ||
                (NtHeaders->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR_MAGIC)) {
                Status = STATUS_INVALID_IMAGE_PROTECT;
                goto out;
            }

#if !defined(NT_UP)
            if (!MmVerifyImageIsOkForMpUse (ViewBase)) {
                Status = STATUS_IMAGE_MP_UP_MISMATCH;
                goto out;
            }
#endif
        } except (EXCEPTION_EXECUTE_HANDLER) {
            Status = STATUS_IMAGE_CHECKSUM_MISMATCH;
        }
    }

out:

    ZwUnmapViewOfSection (NtCurrentProcess(), ViewBase);

    KeUnstackDetachProcess (&ApcState);

    if (PurgeSection == TRUE) {

        Status2 = ObReferenceObjectByHandle (Section,
                                             SECTION_MAP_EXECUTE,
                                             MmSectionObjectType,
                                             KernelMode,
                                             (PVOID *) &SectionPointer,
                                             (POBJECT_HANDLE_INFORMATION) NULL);

        if (NT_SUCCESS (Status2)) {

            MmPurgeSection (SectionPointer->Segment->ControlArea->FilePointer->SectionObjectPointer,
                            NULL,
                            0,
                            FALSE);
            ObDereferenceObject (SectionPointer);
        }
    }

    ZwClose (Section);
    return Status;
}

#if !defined(NT_UP)
BOOLEAN
MmVerifyImageIsOkForMpUse(
    IN PVOID BaseAddress
    )
{
    PIMAGE_NT_HEADERS NtHeaders;

    PAGED_CODE();

    NtHeaders = RtlImageNtHeader(BaseAddress);
    if (NtHeaders != NULL) {
        if ((KeNumberProcessors > 1) &&
            (NtHeaders->FileHeader.Characteristics & IMAGE_FILE_UP_SYSTEM_ONLY)) {
            return FALSE;
        }
    }

    return TRUE;
}
#endif


PFN_NUMBER
MiDeleteSystemPagableVm (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER NumberOfPtes,
    IN MMPTE NewPteValue,
    IN LOGICAL SessionAllocation,
    OUT PPFN_NUMBER ResidentPages
    )

/*++

Routine Description:

    This function deletes pagable system address space (paged pool
    or driver pagable sections).

Arguments:

    PointerPte - Supplies the start of the PTE range to delete.

    NumberOfPtes - Supplies the number of PTEs in the range.

    NewPteValue - Supplies the new value for the PTE.

    SessionAllocation - Supplies TRUE if this is a range in session space.  If
                        TRUE is specified, it is assumed that the caller has
                        already attached to the relevant session.

                        If FALSE is supplied, then it is assumed that the range
                        is in the systemwide global space instead.

    ResidentPages - If not NULL, the number of resident pages freed is
                    returned here.

Return Value:

    Returns the number of pages actually freed.

--*/

{
    PVOID VirtualAddress;
    PFN_NUMBER PageFrameIndex;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER ValidPages;
    PFN_NUMBER PagesRequired;
    MMPTE NewContents;
    WSLE_NUMBER WsIndex;
    KIRQL OldIrql;
    KIRQL OldIrqlWs;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE JunkPte;
    MMWSLENTRY Locked;
    PFN_NUMBER PageTableFrameIndex;
    PETHREAD CurrentThread;
    LOGICAL WsHeld;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    ValidPages = 0;
    PagesRequired = 0;
    PteFlushList.Count = 0;
    WsHeld = FALSE;
    OldIrqlWs = PASSIVE_LEVEL;
    NewContents = NewPteValue;
    CurrentThread = PsGetCurrentThread ();

    while (NumberOfPtes != 0) {
        PteContents = *PointerPte;

        if (PteContents.u.Long != ZeroKernelPte.u.Long) {

            if (PteContents.u.Hard.Valid == 1) {

                //
                // Once the working set mutex is acquired, it is deliberately
                // held until all the pages have been freed.  This is because
                // when paged pool is running low on large servers, we need the
                // segment dereference thread to be able to free large amounts
                // quickly.  Typically this thread will free 64k chunks and we
                // don't want to have to contend for the mutex 16 times to do
                // this as there may be thousands of other threads also trying
                // for it.
                //

                if (WsHeld == FALSE) {
                    WsHeld = TRUE;
                    if (SessionAllocation == TRUE) {
                        LOCK_SESSION_SPACE_WS (OldIrqlWs, CurrentThread);
                    }
                    else {
                        LOCK_SYSTEM_WS (OldIrqlWs, CurrentThread);
                    }
                }

                PteContents = *(volatile MMPTE *)PointerPte;
                if (PteContents.u.Hard.Valid == 0) {
                    continue;
                }

                //
                // Delete the page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                //
                // Check to see if this is a pagable page in which
                // case it needs to be removed from the working set list.
                //

                WsIndex = Pfn1->u1.WsIndex;
                if (WsIndex == 0) {
                    ValidPages += 1;
                    if (SessionAllocation == TRUE) {
                        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_DELVA, 1);
                        MmSessionSpace->NonPagablePages -= 1;
                    }
                }
                else {
                    if (SessionAllocation == FALSE) {
                        MiRemoveWsle (WsIndex, MmSystemCacheWorkingSetList);
                        MiReleaseWsle (WsIndex, &MmSystemCacheWs);
                    }
                    else {
                        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                        WsIndex = MiLocateWsle (VirtualAddress,
                                              MmSessionSpace->Vm.VmWorkingSetList,
                                              WsIndex);

                        ASSERT (WsIndex != WSLE_NULL_INDEX);

                        //
                        // Check to see if this entry is locked in
                        // the working set or locked in memory.
                        //

                        Locked = MmSessionSpace->Wsle[WsIndex].u1.e1;

                        MiRemoveWsle (WsIndex, MmSessionSpace->Vm.VmWorkingSetList);

                        MiReleaseWsle (WsIndex, &MmSessionSpace->Vm);

                        if (Locked.LockedInWs == 1 || Locked.LockedInMemory == 1) {

                            //
                            // This entry is locked.
                            //

                            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_DELVA, 1);
                            MmSessionSpace->NonPagablePages -= 1;
                            ValidPages += 1;

                            ASSERT (WsIndex < MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic);
                            MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic -= 1;

                            if (WsIndex != MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic) {
                                WSLE_NUMBER Entry;
                                PVOID SwapVa;

                                Entry = MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic;
                                ASSERT (MmSessionSpace->Wsle[Entry].u1.e1.Valid);
                                SwapVa = MmSessionSpace->Wsle[Entry].u1.VirtualAddress;
                                SwapVa = PAGE_ALIGN (SwapVa);

                                MiSwapWslEntries (Entry,
                                                  WsIndex,
                                                  &MmSessionSpace->Vm);
                            }
                        }
                        else {
                            ASSERT (WsIndex >= MmSessionSpace->Vm.VmWorkingSetList->FirstDynamic);
                        }
                    }
                }

                LOCK_PFN (OldIrql);
#if DBG0
                if ((Pfn1->u3.e2.ReferenceCount > 1) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {
                    DbgPrint ("MM:SYSLOAD - deleting pool locked for I/O PTE %p, pfn %p, share=%x, refcount=%x, wsindex=%x\n",
                             PointerPte,
                             PageFrameIndex,
                             Pfn1->u2.ShareCount,
                             Pfn1->u3.e2.ReferenceCount,
                             Pfn1->u1.WsIndex);
                    //
                    // This case is valid only if the page being deleted
                    // contained a lookaside free list entry that wasn't mapped
                    // and multiple threads faulted on it and waited together.
                    // Some of the faulted threads are still on the ready
                    // list but haven't run yet, and so still have references
                    // to this page that they picked up during the fault.
                    // But this current thread has already allocated the
                    // lookaside entry and is now freeing the entire page.
                    //
                    // BUT - if it is NOT the above case, we really should
                    // trap here.  However, we don't have a good way to
                    // distinguish between the two cases.  Note
                    // that this complication was inserted when we went to
                    // cmpxchg8 because using locks would prevent anyone from
                    // accessing the lookaside freelist flinks like this.
                    //
                    // So, the ASSERT below comes out, but we leave the print
                    // above in (with more data added) for the case where it's
                    // not a lookaside contender with the reference count, but
                    // is instead a truly bad reference that needs to be
                    // debugged.  The system should crash shortly thereafter
                    // and we'll at least have the above print to help us out.
                    //
                    // ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
                }
#endif //DBG
                //
                // Check if this is a prototype PTE.
                //
                if (Pfn1->u3.e1.PrototypePte == 1) {

                    PMMPTE PointerPde;

                    ASSERT (SessionAllocation == TRUE);

                    //
                    // Capture the state of the modified bit for this
                    // PTE.
                    //

                    MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

                    //
                    // Decrement the share and valid counts of the page table
                    // page which maps this PTE.
                    //

                    PointerPde = MiGetPteAddress (PointerPte);
                    if (PointerPde->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                        if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
                            KeBugCheckEx (MEMORY_MANAGEMENT,
                                          0x61940,
                                          (ULONG_PTR)PointerPte,
                                          (ULONG_PTR)PointerPde->u.Long,
                                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(PointerPte));
#if (_MI_PAGING_LEVELS < 3)
                        }
#endif
                    }

                    PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    //
                    // Decrement the share count for the physical page.
                    //

                    MiDecrementShareCount (PageFrameIndex);

                    //
                    // No need to worry about fork prototype PTEs
                    // for kernel addresses.
                    //

                    ASSERT (PointerPte > MiHighestUserPte);

                }
                else {
                    PageTableFrameIndex = Pfn1->u4.PteFrame;
                    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                    MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                    MI_SET_PFN_DELETED (Pfn1);
                    MiDecrementShareCountOnly (PageFrameIndex);
                }

                MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                UNLOCK_PFN (OldIrql);

                //
                // Flush the TB for this page.
                //

                if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {

                    //
                    // We cannot rewrite the PTE later without creating a race
                    // condition.  So point the FlushPte at a harmless
                    // location so MiFlushPteList does the right thing.
                    //

                    PteFlushList.FlushPte[PteFlushList.Count] =
                        (PMMPTE)&JunkPte;

                    PteFlushList.FlushVa[PteFlushList.Count] =
                                    MiGetVirtualAddressMappedByPte (PointerPte);
                    PteFlushList.Count += 1;
                }

            } else if (PteContents.u.Soft.Prototype) {

                ASSERT (SessionAllocation == TRUE);

                //
                // No need to worry about fork prototype PTEs
                // for kernel addresses.
                //

                ASSERT (PointerPte >= MiHighestUserPte);

                MI_WRITE_INVALID_PTE (PointerPte, NewContents);

                //
                // We currently commit for all prototype kernel mappings since
                // we could copy-on-write.
                //

            } else if (PteContents.u.Soft.Transition == 1) {

                LOCK_PFN (OldIrql);

                PteContents = *(volatile MMPTE *)PointerPte;

                if (PteContents.u.Soft.Transition == 0) {
                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                //
                // Transition, release page.
                //

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

                //
                // Set the pointer to PTE as empty so the page
                // is deleted when the reference count goes to zero.
                //

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // Check the reference count for the page, if the reference
                // count is zero, move the page to the free list, if the
                // reference count is not zero, ignore this page.  When the
                // reference count goes to zero, it will be placed on the
                // free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiUnlinkPageFromList (Pfn1);
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (PageFrameIndex);
                }
#if 0
                //
                // This assert is not valid since pool may now be the deferred
                // MmUnlockPages queue in which case the reference count
                // will be nonzero with no write in progress pending.
                //

                if ((Pfn1->u3.e2.ReferenceCount > 1) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {
                    DbgPrint ("MM:SYSLOAD - deleting pool locked for I/O %p\n",
                             PageFrameIndex);
                    DbgBreakPoint();
                }
#endif //DBG

                MI_WRITE_INVALID_PTE (PointerPte, NewContents);
                UNLOCK_PFN (OldIrql);
            }
            else {

                //
                // Demand zero, release page file space.
                //
                if (PteContents.u.Soft.PageFileHigh != 0) {
                    LOCK_PFN (OldIrql);
                    MiReleasePageFileSpace (PteContents);
                    UNLOCK_PFN (OldIrql);
                }

                MI_WRITE_INVALID_PTE (PointerPte, NewContents);
            }

            PagesRequired += 1;
        }

        NumberOfPtes -= 1;
        PointerPte += 1;
    }

    if (WsHeld == TRUE) {
        if (SessionAllocation == TRUE) {
            UNLOCK_SESSION_SPACE_WS (OldIrqlWs);
        }
        else {
            UNLOCK_SYSTEM_WS (OldIrqlWs);
        }
    }

    //
    // There is the thorny case where one of the pages we just deleted could
    // get faulted back in when we released the PFN lock above within the loop.
    // The only time when this can happen is if a thread faulted during an
    // interlocked pool allocation for an address that we've just deleted.
    //
    // If this thread sees the NewContents in the PTE, it will treat it as
    // demand zero, and incorrectly allocate a page, PTE and WSL.  It will
    // reference it once and realize it needs to reread the lookaside listhead
    // and restart the operation.  But this page would live on in paged pool
    // as modified (but unused) until the paged pool allocator chose to give
    // out its virtual address again.
    //
    // The code below rewrites the PTEs which is really bad if another thread
    // gets a zero page between our first setting of the PTEs above and our
    // second setting below - because we'll reset the PTE to demand zero, but
    // we'll still have a WSL entry that's valid, and we spiral from there.
    //
    // We really should remove the writing of the PTE below since we've already
    // done it above.  But for now, we're leaving it in - it's harmless because
    // we've chosen to fix this problem by checking for this case when we
    // materialize demand zero pages.  Note that we have to fix this problem
    // by checking in the demand zero path because a thread could be coming into
    // that path any time before or after we flush the PTE list and any fixes
    // here could only address the before case, not the after.
    //

    if (SessionAllocation == TRUE) {

        //
        // Session space has no ASN - flush the entire TB.
        //

        MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
    }

    MiFlushPteList (&PteFlushList, TRUE, NewContents);

    if (ARGUMENT_PRESENT(ResidentPages)) {
        *ResidentPages = ValidPages;
    }

    return PagesRequired;
}

VOID
MiSetImageProtect (
    IN PSEGMENT Segment,
    IN ULONG Protection
    )

/*++

Routine Description:

    This function sets the protection of all prototype PTEs to the specified
    protection.

Arguments:

     Segment - Supplies a pointer to the segment to protect.

     Protection - Supplies the protection value to set.

Return Value:

     None.

--*/

{
    PMMPTE PointerPte;
    PMMPTE StartPte;
    PMMPTE LastPte;
    MMPTE PteContents;
    PSUBSECTION Subsection;

    //
    // Set the subsection protections.
    //

    ASSERT (Segment->ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if ((Segment->ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (Segment->ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(Segment->ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)(Segment->ControlArea) + 1);
    }

    if ((Protection & MM_PROTECTION_WRITE_MASK) == 0) {
        Subsection->u.SubsectionFlags.Protection = Protection;
        Subsection->u.SubsectionFlags.ReadOnly = 1;
    }

    StartPte = Subsection->SubsectionBase;

    PointerPte = StartPte;
    LastPte = PointerPte + Segment->NonExtendedPtes;

    MmLockPagedPool (PointerPte, (LastPte - PointerPte) * sizeof (MMPTE));

    do {
        PteContents = *PointerPte;
        ASSERT (PteContents.u.Hard.Valid == 0);
        if (PteContents.u.Long != ZeroPte.u.Long) {
            if ((PteContents.u.Soft.Prototype == 0) &&
                (PteContents.u.Soft.Transition == 1)) {
                if (MiSetProtectionOnTransitionPte (PointerPte, Protection)) {
                    continue;
                }
            }
            else {
                PointerPte->u.Soft.Protection = Protection;
            }
        }
        PointerPte += 1;
    } while (PointerPte < LastPte);

    MmUnlockPagedPool (StartPte, (LastPte - StartPte) * sizeof (MMPTE));

    return;
}


VOID
MiSetSystemCodeProtection (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This function sets the protection of system code to read only.
    Note this is different from protecting section-backed code like win32k.

Arguments:

    FirstPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

Return Value:

    None.

Environment:

    Kernel Mode, IRQL of APC_LEVEL or below.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    KIRQL OldIrql;
    MMPTE PteContents;
    MMPTE TempPte;
    MMPTE PreviousPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerProtoPte;
    ULONG ProtectionMask;
    PMMPFN Pfn1;
    PMMPFN ProtoPfn;
    LOGICAL SessionAddress;

#if defined(_X86_)
    ASSERT (MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(FirstPte)) == 0);
#endif

    SessionAddress = FALSE;

    if (MI_IS_SESSION_ADDRESS(MiGetVirtualAddressMappedByPte(FirstPte))) {
        SessionAddress = TRUE;
    }

    ProtectionMask = MM_EXECUTE_READ;

    //
    // Make these PTEs read only.
    //
    // Note that the write bit may already be off (in the valid PTE) if the
    // page has already been inpaged from the paging file and has not since
    // been dirtied.
    //

    PointerPte = FirstPte;

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {

        PteContents = *PointerPte;

        if ((PteContents.u.Long == 0) || (!*MiPteStr)) {
            PointerPte += 1;
            continue;
        }

        if (PteContents.u.Hard.Valid == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;

            //
            // Note the dirty and write bits get turned off here.
            // Any existing pagefile addresses for clean pages are preserved.
            //

            if (MI_IS_PTE_DIRTY(PteContents)) {
                MI_CAPTURE_DIRTY_BIT_TO_PFN (&PteContents, Pfn1);
            }

            MI_MAKE_VALID_PTE (TempPte,
                               PteContents.u.Hard.PageFrameNumber,
                               Pfn1->OriginalPte.u.Soft.Protection,
                               PointerPte);

            if (SessionAddress == TRUE) {

                //
                // Session space has no ASN - flush the entire TB.
                //

                MI_FLUSH_SINGLE_SESSION_TB (MiGetVirtualAddressMappedByPte (PointerPte),
                             TRUE,
                             TRUE,
                             (PHARDWARE_PTE)PointerPte,
                             TempPte.u.Flush,
                             PreviousPte);
            }
            else {
                KeFlushSingleTb (MiGetVirtualAddressMappedByPte (PointerPte),
                                 TRUE,
                                 TRUE,
                                 (PHARDWARE_PTE)PointerPte,
                                 TempPte.u.Flush);
            }
        }
        else if (PteContents.u.Soft.Prototype == 1) {

            if (SessionAddress == TRUE) {
                PointerPte->u.Proto.ReadOnly = 1;
            }
            else {
                PointerProtoPte = MiPteToProto(PointerPte);

                ASSERT (!MI_IS_PHYSICAL_ADDRESS(PointerProtoPte));
                PointerPde = MiGetPteAddress (PointerProtoPte);
                if (PointerPde->u.Hard.Valid == 0) {
                    MiMakeSystemAddressValidPfn (PointerProtoPte);

                    //
                    // The world may change if we had to wait.
                    //

                    PteContents = *PointerPte;
                    if ((PteContents.u.Hard.Valid == 1) ||
                        (PteContents.u.Soft.Prototype == 0)) {
                            continue;
                    }
                }

                ProtoPfn = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
                MI_ADD_LOCKED_PAGE_CHARGE(ProtoPfn, 12);
                ProtoPfn->u3.e2.ReferenceCount += 1;
                ASSERT (ProtoPfn->u3.e2.ReferenceCount > 1);

                PteContents = *PointerProtoPte;

                if (PteContents.u.Long != ZeroPte.u.Long) {

                    ASSERT (PteContents.u.Hard.Valid == 0);

                    PointerProtoPte->u.Soft.Protection = ProtectionMask;

                    if ((PteContents.u.Soft.Prototype == 0) &&
                        (PteContents.u.Soft.Transition == 1)) {
                        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
                        Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
                    }
                }

                ASSERT (ProtoPfn->u3.e2.ReferenceCount > 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(ProtoPfn, 13);
            }
        }
        else if (PteContents.u.Soft.Transition == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);
            Pfn1->OriginalPte.u.Soft.Protection = ProtectionMask;
            PointerPte->u.Soft.Protection = ProtectionMask;

        }
        else {

            //
            // Must be page file space or demand zero.
            //

            PointerPte->u.Soft.Protection = ProtectionMask;
        }
        PointerPte += 1;
    }

    UNLOCK_PFN (OldIrql);
}


VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    )

/*++

Routine Description:

    This function sets the protection of a system component to read only.

Arguments:

    DllBase - Supplies the base address of the system component.

Return Value:

    None.

--*/

{
    ULONG SectionProtection;
    ULONG NumberOfSubsections;
    ULONG SectionVirtualSize;
    ULONG ImageAlignment;
    ULONG OffsetToSectionTable;
    ULONG NumberOfPtes;
    ULONG_PTR VirtualAddress;
    PVOID LastVirtualAddress;
    PMMPTE PointerPte;
    PMMPTE FirstPte;
    PMMPTE LastPte;
    PMMPTE LastImagePte;
    PMMPTE WritablePte;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_FILE_HEADER FileHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;

    PAGED_CODE();

    if (MI_IS_PHYSICAL_ADDRESS(DllBase)) {
        return;
    }

    NtHeader = RtlImageNtHeader (DllBase);

    ASSERT (NtHeader);

    ImageAlignment = NtHeader->OptionalHeader.SectionAlignment;

    //
    // All session drivers must be one way or the other - no mixing is allowed
    // within multiple copy-on-write drivers.
    //

    if (MI_IS_SESSION_ADDRESS(DllBase) == 0) {

        //
        // Images prior to NT5 were not protected from stepping all over
        // their (and others) code and readonly data.  Here we somewhat
        // preserve that behavior, but don't allow them to step on anyone else.
        //

        if (NtHeader->OptionalHeader.MajorOperatingSystemVersion < 5) {
            return;
        }

        if (NtHeader->OptionalHeader.MajorImageVersion < 5) {
            return;
        }
    }

    NumberOfPtes = BYTES_TO_PAGES (NtHeader->OptionalHeader.SizeOfImage);

    FileHeader = &NtHeader->FileHeader;

    NumberOfSubsections = FileHeader->NumberOfSections;

    ASSERT (NumberOfSubsections != 0);

    OffsetToSectionTable = sizeof(ULONG) +
                              sizeof(IMAGE_FILE_HEADER) +
                              FileHeader->SizeOfOptionalHeader;

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                            OffsetToSectionTable);

    //
    // Verify the image contains subsections ordered by increasing virtual
    // address and that there are no overlaps.
    //

    FirstPte = NULL;
    LastVirtualAddress = DllBase;

    for ( ; NumberOfSubsections > 0; NumberOfSubsections -= 1, SectionTableEntry += 1) {

        if (SectionTableEntry->Misc.VirtualSize == 0) {
            SectionVirtualSize = SectionTableEntry->SizeOfRawData;
        }
        else {
            SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
        }

        VirtualAddress = (ULONG_PTR)DllBase + SectionTableEntry->VirtualAddress;
        if ((PVOID)VirtualAddress <= LastVirtualAddress) {

            //
            // Subsections are not in an increasing virtual address ordering.
            // No protection is provided for such a poorly linked image.
            //

            KdPrint (("MM:sysload - Image at %p is badly linked\n", DllBase));
            return;
        }
        LastVirtualAddress = (PVOID)((PCHAR)VirtualAddress + SectionVirtualSize - 1);
    }

    NumberOfSubsections = FileHeader->NumberOfSections;
    ASSERT (NumberOfSubsections != 0);

    SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                            OffsetToSectionTable);

    LastVirtualAddress = NULL;

    //
    // Set writable PTE here so the image headers are excluded.  This is
    // needed so that locking down of sections can continue to edit the
    // image headers for counts.
    //

    WritablePte = MiGetPteAddress ((PVOID)((ULONG_PTR)(SectionTableEntry + NumberOfSubsections) - 1));
    LastImagePte = MiGetPteAddress(DllBase) + NumberOfPtes;

    for ( ; NumberOfSubsections > 0; NumberOfSubsections -= 1, SectionTableEntry += 1) {

        if (SectionTableEntry->Misc.VirtualSize == 0) {
            SectionVirtualSize = SectionTableEntry->SizeOfRawData;
        }
        else {
            SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
        }

        VirtualAddress = (ULONG_PTR)DllBase + SectionTableEntry->VirtualAddress;

        PointerPte = MiGetPteAddress ((PVOID)VirtualAddress);

        if (PointerPte >= LastImagePte) {

            //
            // Skip relocation subsections (which aren't given VA space).
            //

            break;
        }

        SectionProtection = (SectionTableEntry->Characteristics & (IMAGE_SCN_MEM_WRITE | IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_EXECUTE));

        if (SectionProtection & IMAGE_SCN_MEM_WRITE) {

            //
            // This is a writable subsection, skip it.  Make sure if it's
            // sharing a PTE (and update the linker so this doesn't happen
            // for the kernel at least) that the last PTE isn't made
            // read only.
            //

            WritablePte = MiGetPteAddress ((PVOID)(VirtualAddress + SectionVirtualSize - 1));

            if (LastVirtualAddress != NULL) {
                LastPte = (PVOID) MiGetPteAddress (LastVirtualAddress);

                if (LastPte == PointerPte) {
                    LastPte -= 1;
                }

                if (FirstPte <= LastPte) {

                    ASSERT (PointerPte < LastImagePte);

                    if (LastPte >= LastImagePte) {
                        LastPte = LastImagePte - 1;
                    }

                    MiSetSystemCodeProtection (FirstPte, LastPte);
                }

                LastVirtualAddress = NULL;
            }
            continue;
        }

        if (LastVirtualAddress == NULL) {

            //
            // There is no previous subsection or the previous
            // subsection was writable.  Thus the current starting PTE
            // could be mapping both a readonly and a readwrite
            // subsection if the image alignment is less than PAGE_SIZE.
            // These cases (in either order) are handled here.
            //

            if (PointerPte == WritablePte) {
                LastPte = MiGetPteAddress ((PVOID)(VirtualAddress + SectionVirtualSize - 1));
                if (PointerPte == LastPte) {

                    //
                    // Nothing can be protected in this subsection
                    // due to the image alignment specified for the executable.
                    //

                    continue;
                }
                PointerPte += 1;
            }
            FirstPte = PointerPte;
        }

        LastVirtualAddress = (PVOID)((PCHAR)VirtualAddress + SectionVirtualSize - 1);
    }

    if (LastVirtualAddress != NULL) {
        LastPte = (PVOID) MiGetPteAddress (LastVirtualAddress);

        if ((FirstPte <= LastPte) && (FirstPte < LastImagePte)) {

            if (LastPte >= LastImagePte) {
                LastPte = LastImagePte - 1;
            }

            MiSetSystemCodeProtection (FirstPte, LastPte);
        }
    }
}


VOID
MiUpdateThunks (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PVOID OldAddress,
    IN PVOID NewAddress,
    IN ULONG NumberOfBytes
    )

/*++

Routine Description:

    This function updates the IATs of all the loaded modules in the system
    to handle a newly relocated image.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

    OldAddress - Supplies the old address of the DLL which was just relocated.

    NewAddress - Supplies the new address of the DLL which was just relocated.

    NumberOfBytes - Supplies the number of bytes spanned by the DLL.

Return Value:

    None.

--*/

{
    PULONG_PTR ImportThunk;
    ULONG_PTR OldAddressHigh;
    ULONG_PTR AddressDifference;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PLIST_ENTRY NextEntry;
    ULONG_PTR i;
    ULONG ImportSize;

    //
    // Note this routine must not call any modules outside the kernel.
    // This is because that module may itself be the one being relocated right
    // now.
    //

    OldAddressHigh = (ULONG_PTR)((PCHAR)OldAddress + NumberOfBytes - 1);
    AddressDifference = (ULONG_PTR)NewAddress - (ULONG_PTR)OldAddress;

    NextEntry = LoaderBlock->LoadOrderListHead.Flink;

    for ( ; NextEntry != &LoaderBlock->LoadOrderListHead; NextEntry = NextEn